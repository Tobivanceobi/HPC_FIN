no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  12
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.52213
[1mStep[0m  [2/26], [94mLoss[0m : 10.42400
[1mStep[0m  [4/26], [94mLoss[0m : 9.86058
[1mStep[0m  [6/26], [94mLoss[0m : 9.09100
[1mStep[0m  [8/26], [94mLoss[0m : 9.08420
[1mStep[0m  [10/26], [94mLoss[0m : 8.22243
[1mStep[0m  [12/26], [94mLoss[0m : 8.26433
[1mStep[0m  [14/26], [94mLoss[0m : 7.58976
[1mStep[0m  [16/26], [94mLoss[0m : 7.03970
[1mStep[0m  [18/26], [94mLoss[0m : 6.26081
[1mStep[0m  [20/26], [94mLoss[0m : 6.25817
[1mStep[0m  [22/26], [94mLoss[0m : 5.98236
[1mStep[0m  [24/26], [94mLoss[0m : 5.56219

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.922, [92mTest[0m: 10.565, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.10036
[1mStep[0m  [2/26], [94mLoss[0m : 4.91478
[1mStep[0m  [4/26], [94mLoss[0m : 4.46602
[1mStep[0m  [6/26], [94mLoss[0m : 4.20918
[1mStep[0m  [8/26], [94mLoss[0m : 4.08885
[1mStep[0m  [10/26], [94mLoss[0m : 3.47309
[1mStep[0m  [12/26], [94mLoss[0m : 3.49413
[1mStep[0m  [14/26], [94mLoss[0m : 3.23528
[1mStep[0m  [16/26], [94mLoss[0m : 3.16014
[1mStep[0m  [18/26], [94mLoss[0m : 3.16651
[1mStep[0m  [20/26], [94mLoss[0m : 3.17274
[1mStep[0m  [22/26], [94mLoss[0m : 3.15996
[1mStep[0m  [24/26], [94mLoss[0m : 2.74182

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.653, [92mTest[0m: 5.024, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.94453
[1mStep[0m  [2/26], [94mLoss[0m : 2.85420
[1mStep[0m  [4/26], [94mLoss[0m : 2.68111
[1mStep[0m  [6/26], [94mLoss[0m : 2.58824
[1mStep[0m  [8/26], [94mLoss[0m : 2.58455
[1mStep[0m  [10/26], [94mLoss[0m : 2.67629
[1mStep[0m  [12/26], [94mLoss[0m : 2.76263
[1mStep[0m  [14/26], [94mLoss[0m : 2.59019
[1mStep[0m  [16/26], [94mLoss[0m : 2.69462
[1mStep[0m  [18/26], [94mLoss[0m : 2.66176
[1mStep[0m  [20/26], [94mLoss[0m : 2.45025
[1mStep[0m  [22/26], [94mLoss[0m : 2.68667
[1mStep[0m  [24/26], [94mLoss[0m : 2.58592

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.796, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59588
[1mStep[0m  [2/26], [94mLoss[0m : 2.63603
[1mStep[0m  [4/26], [94mLoss[0m : 2.44954
[1mStep[0m  [6/26], [94mLoss[0m : 2.54485
[1mStep[0m  [8/26], [94mLoss[0m : 2.46610
[1mStep[0m  [10/26], [94mLoss[0m : 2.69076
[1mStep[0m  [12/26], [94mLoss[0m : 2.76760
[1mStep[0m  [14/26], [94mLoss[0m : 2.75512
[1mStep[0m  [16/26], [94mLoss[0m : 2.53986
[1mStep[0m  [18/26], [94mLoss[0m : 2.45237
[1mStep[0m  [20/26], [94mLoss[0m : 2.50330
[1mStep[0m  [22/26], [94mLoss[0m : 2.48878
[1mStep[0m  [24/26], [94mLoss[0m : 2.52355

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.514, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64558
[1mStep[0m  [2/26], [94mLoss[0m : 2.67186
[1mStep[0m  [4/26], [94mLoss[0m : 2.41406
[1mStep[0m  [6/26], [94mLoss[0m : 2.45882
[1mStep[0m  [8/26], [94mLoss[0m : 2.69334
[1mStep[0m  [10/26], [94mLoss[0m : 2.50611
[1mStep[0m  [12/26], [94mLoss[0m : 2.58699
[1mStep[0m  [14/26], [94mLoss[0m : 2.56757
[1mStep[0m  [16/26], [94mLoss[0m : 2.63451
[1mStep[0m  [18/26], [94mLoss[0m : 2.58791
[1mStep[0m  [20/26], [94mLoss[0m : 2.47094
[1mStep[0m  [22/26], [94mLoss[0m : 2.45952
[1mStep[0m  [24/26], [94mLoss[0m : 2.35843

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38111
[1mStep[0m  [2/26], [94mLoss[0m : 2.44933
[1mStep[0m  [4/26], [94mLoss[0m : 2.53015
[1mStep[0m  [6/26], [94mLoss[0m : 2.47172
[1mStep[0m  [8/26], [94mLoss[0m : 2.47630
[1mStep[0m  [10/26], [94mLoss[0m : 2.66698
[1mStep[0m  [12/26], [94mLoss[0m : 2.63947
[1mStep[0m  [14/26], [94mLoss[0m : 2.58492
[1mStep[0m  [16/26], [94mLoss[0m : 2.52910
[1mStep[0m  [18/26], [94mLoss[0m : 2.44456
[1mStep[0m  [20/26], [94mLoss[0m : 2.53067
[1mStep[0m  [22/26], [94mLoss[0m : 2.65497
[1mStep[0m  [24/26], [94mLoss[0m : 2.58734

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59653
[1mStep[0m  [2/26], [94mLoss[0m : 2.59290
[1mStep[0m  [4/26], [94mLoss[0m : 2.48025
[1mStep[0m  [6/26], [94mLoss[0m : 2.46248
[1mStep[0m  [8/26], [94mLoss[0m : 2.65037
[1mStep[0m  [10/26], [94mLoss[0m : 2.60328
[1mStep[0m  [12/26], [94mLoss[0m : 2.64516
[1mStep[0m  [14/26], [94mLoss[0m : 2.47845
[1mStep[0m  [16/26], [94mLoss[0m : 2.49096
[1mStep[0m  [18/26], [94mLoss[0m : 2.58775
[1mStep[0m  [20/26], [94mLoss[0m : 2.66720
[1mStep[0m  [22/26], [94mLoss[0m : 2.31273
[1mStep[0m  [24/26], [94mLoss[0m : 2.47827

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36804
[1mStep[0m  [2/26], [94mLoss[0m : 2.49166
[1mStep[0m  [4/26], [94mLoss[0m : 2.54075
[1mStep[0m  [6/26], [94mLoss[0m : 2.29517
[1mStep[0m  [8/26], [94mLoss[0m : 2.49062
[1mStep[0m  [10/26], [94mLoss[0m : 2.43396
[1mStep[0m  [12/26], [94mLoss[0m : 2.42510
[1mStep[0m  [14/26], [94mLoss[0m : 2.59475
[1mStep[0m  [16/26], [94mLoss[0m : 2.47982
[1mStep[0m  [18/26], [94mLoss[0m : 2.40535
[1mStep[0m  [20/26], [94mLoss[0m : 2.51874
[1mStep[0m  [22/26], [94mLoss[0m : 2.47310
[1mStep[0m  [24/26], [94mLoss[0m : 2.55968

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45984
[1mStep[0m  [2/26], [94mLoss[0m : 2.58089
[1mStep[0m  [4/26], [94mLoss[0m : 2.55447
[1mStep[0m  [6/26], [94mLoss[0m : 2.60075
[1mStep[0m  [8/26], [94mLoss[0m : 2.68902
[1mStep[0m  [10/26], [94mLoss[0m : 2.45879
[1mStep[0m  [12/26], [94mLoss[0m : 2.67103
[1mStep[0m  [14/26], [94mLoss[0m : 2.57582
[1mStep[0m  [16/26], [94mLoss[0m : 2.48466
[1mStep[0m  [18/26], [94mLoss[0m : 2.42972
[1mStep[0m  [20/26], [94mLoss[0m : 2.44280
[1mStep[0m  [22/26], [94mLoss[0m : 2.44042
[1mStep[0m  [24/26], [94mLoss[0m : 2.62219

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46476
[1mStep[0m  [2/26], [94mLoss[0m : 2.33982
[1mStep[0m  [4/26], [94mLoss[0m : 2.54494
[1mStep[0m  [6/26], [94mLoss[0m : 2.38776
[1mStep[0m  [8/26], [94mLoss[0m : 2.56037
[1mStep[0m  [10/26], [94mLoss[0m : 2.57130
[1mStep[0m  [12/26], [94mLoss[0m : 2.56787
[1mStep[0m  [14/26], [94mLoss[0m : 2.54512
[1mStep[0m  [16/26], [94mLoss[0m : 2.63373
[1mStep[0m  [18/26], [94mLoss[0m : 2.56604
[1mStep[0m  [20/26], [94mLoss[0m : 2.55007
[1mStep[0m  [22/26], [94mLoss[0m : 2.46941
[1mStep[0m  [24/26], [94mLoss[0m : 2.46125

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43448
[1mStep[0m  [2/26], [94mLoss[0m : 2.59523
[1mStep[0m  [4/26], [94mLoss[0m : 2.54486
[1mStep[0m  [6/26], [94mLoss[0m : 2.56440
[1mStep[0m  [8/26], [94mLoss[0m : 2.50608
[1mStep[0m  [10/26], [94mLoss[0m : 2.41476
[1mStep[0m  [12/26], [94mLoss[0m : 2.47048
[1mStep[0m  [14/26], [94mLoss[0m : 2.39251
[1mStep[0m  [16/26], [94mLoss[0m : 2.47798
[1mStep[0m  [18/26], [94mLoss[0m : 2.34054
[1mStep[0m  [20/26], [94mLoss[0m : 2.59854
[1mStep[0m  [22/26], [94mLoss[0m : 2.71157
[1mStep[0m  [24/26], [94mLoss[0m : 2.31066

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54910
[1mStep[0m  [2/26], [94mLoss[0m : 2.54247
[1mStep[0m  [4/26], [94mLoss[0m : 2.46710
[1mStep[0m  [6/26], [94mLoss[0m : 2.53365
[1mStep[0m  [8/26], [94mLoss[0m : 2.59636
[1mStep[0m  [10/26], [94mLoss[0m : 2.51693
[1mStep[0m  [12/26], [94mLoss[0m : 2.48822
[1mStep[0m  [14/26], [94mLoss[0m : 2.62308
[1mStep[0m  [16/26], [94mLoss[0m : 2.49729
[1mStep[0m  [18/26], [94mLoss[0m : 2.48866
[1mStep[0m  [20/26], [94mLoss[0m : 2.51992
[1mStep[0m  [22/26], [94mLoss[0m : 2.55195
[1mStep[0m  [24/26], [94mLoss[0m : 2.45006

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53929
[1mStep[0m  [2/26], [94mLoss[0m : 2.61289
[1mStep[0m  [4/26], [94mLoss[0m : 2.32983
[1mStep[0m  [6/26], [94mLoss[0m : 2.40020
[1mStep[0m  [8/26], [94mLoss[0m : 2.58354
[1mStep[0m  [10/26], [94mLoss[0m : 2.57591
[1mStep[0m  [12/26], [94mLoss[0m : 2.47431
[1mStep[0m  [14/26], [94mLoss[0m : 2.34998
[1mStep[0m  [16/26], [94mLoss[0m : 2.41679
[1mStep[0m  [18/26], [94mLoss[0m : 2.55226
[1mStep[0m  [20/26], [94mLoss[0m : 2.57773
[1mStep[0m  [22/26], [94mLoss[0m : 2.41438
[1mStep[0m  [24/26], [94mLoss[0m : 2.70061

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51407
[1mStep[0m  [2/26], [94mLoss[0m : 2.40975
[1mStep[0m  [4/26], [94mLoss[0m : 2.51278
[1mStep[0m  [6/26], [94mLoss[0m : 2.49297
[1mStep[0m  [8/26], [94mLoss[0m : 2.28435
[1mStep[0m  [10/26], [94mLoss[0m : 2.42026
[1mStep[0m  [12/26], [94mLoss[0m : 2.38622
[1mStep[0m  [14/26], [94mLoss[0m : 2.51862
[1mStep[0m  [16/26], [94mLoss[0m : 2.59802
[1mStep[0m  [18/26], [94mLoss[0m : 2.48969
[1mStep[0m  [20/26], [94mLoss[0m : 2.47327
[1mStep[0m  [22/26], [94mLoss[0m : 2.34686
[1mStep[0m  [24/26], [94mLoss[0m : 2.52166

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48754
[1mStep[0m  [2/26], [94mLoss[0m : 2.49422
[1mStep[0m  [4/26], [94mLoss[0m : 2.49060
[1mStep[0m  [6/26], [94mLoss[0m : 2.54573
[1mStep[0m  [8/26], [94mLoss[0m : 2.55243
[1mStep[0m  [10/26], [94mLoss[0m : 2.48105
[1mStep[0m  [12/26], [94mLoss[0m : 2.57628
[1mStep[0m  [14/26], [94mLoss[0m : 2.23994
[1mStep[0m  [16/26], [94mLoss[0m : 2.52204
[1mStep[0m  [18/26], [94mLoss[0m : 2.51069
[1mStep[0m  [20/26], [94mLoss[0m : 2.53123
[1mStep[0m  [22/26], [94mLoss[0m : 2.54052
[1mStep[0m  [24/26], [94mLoss[0m : 2.45050

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40709
[1mStep[0m  [2/26], [94mLoss[0m : 2.36768
[1mStep[0m  [4/26], [94mLoss[0m : 2.47130
[1mStep[0m  [6/26], [94mLoss[0m : 2.50905
[1mStep[0m  [8/26], [94mLoss[0m : 2.58976
[1mStep[0m  [10/26], [94mLoss[0m : 2.40523
[1mStep[0m  [12/26], [94mLoss[0m : 2.47160
[1mStep[0m  [14/26], [94mLoss[0m : 2.51048
[1mStep[0m  [16/26], [94mLoss[0m : 2.44283
[1mStep[0m  [18/26], [94mLoss[0m : 2.50781
[1mStep[0m  [20/26], [94mLoss[0m : 2.48244
[1mStep[0m  [22/26], [94mLoss[0m : 2.44280
[1mStep[0m  [24/26], [94mLoss[0m : 2.52459

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32543
[1mStep[0m  [2/26], [94mLoss[0m : 2.47161
[1mStep[0m  [4/26], [94mLoss[0m : 2.38600
[1mStep[0m  [6/26], [94mLoss[0m : 2.55543
[1mStep[0m  [8/26], [94mLoss[0m : 2.34188
[1mStep[0m  [10/26], [94mLoss[0m : 2.27476
[1mStep[0m  [12/26], [94mLoss[0m : 2.61414
[1mStep[0m  [14/26], [94mLoss[0m : 2.49255
[1mStep[0m  [16/26], [94mLoss[0m : 2.52719
[1mStep[0m  [18/26], [94mLoss[0m : 2.59737
[1mStep[0m  [20/26], [94mLoss[0m : 2.41607
[1mStep[0m  [22/26], [94mLoss[0m : 2.46054
[1mStep[0m  [24/26], [94mLoss[0m : 2.44013

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55896
[1mStep[0m  [2/26], [94mLoss[0m : 2.50010
[1mStep[0m  [4/26], [94mLoss[0m : 2.57964
[1mStep[0m  [6/26], [94mLoss[0m : 2.35448
[1mStep[0m  [8/26], [94mLoss[0m : 2.57202
[1mStep[0m  [10/26], [94mLoss[0m : 2.23066
[1mStep[0m  [12/26], [94mLoss[0m : 2.49307
[1mStep[0m  [14/26], [94mLoss[0m : 2.51007
[1mStep[0m  [16/26], [94mLoss[0m : 2.50853
[1mStep[0m  [18/26], [94mLoss[0m : 2.55545
[1mStep[0m  [20/26], [94mLoss[0m : 2.26401
[1mStep[0m  [22/26], [94mLoss[0m : 2.43947
[1mStep[0m  [24/26], [94mLoss[0m : 2.49265

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43848
[1mStep[0m  [2/26], [94mLoss[0m : 2.42323
[1mStep[0m  [4/26], [94mLoss[0m : 2.47499
[1mStep[0m  [6/26], [94mLoss[0m : 2.49133
[1mStep[0m  [8/26], [94mLoss[0m : 2.32118
[1mStep[0m  [10/26], [94mLoss[0m : 2.57044
[1mStep[0m  [12/26], [94mLoss[0m : 2.67381
[1mStep[0m  [14/26], [94mLoss[0m : 2.29998
[1mStep[0m  [16/26], [94mLoss[0m : 2.49647
[1mStep[0m  [18/26], [94mLoss[0m : 2.35559
[1mStep[0m  [20/26], [94mLoss[0m : 2.49847
[1mStep[0m  [22/26], [94mLoss[0m : 2.49375
[1mStep[0m  [24/26], [94mLoss[0m : 2.49342

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31033
[1mStep[0m  [2/26], [94mLoss[0m : 2.57605
[1mStep[0m  [4/26], [94mLoss[0m : 2.51673
[1mStep[0m  [6/26], [94mLoss[0m : 2.41216
[1mStep[0m  [8/26], [94mLoss[0m : 2.53972
[1mStep[0m  [10/26], [94mLoss[0m : 2.46537
[1mStep[0m  [12/26], [94mLoss[0m : 2.44792
[1mStep[0m  [14/26], [94mLoss[0m : 2.45689
[1mStep[0m  [16/26], [94mLoss[0m : 2.58536
[1mStep[0m  [18/26], [94mLoss[0m : 2.50025
[1mStep[0m  [20/26], [94mLoss[0m : 2.49895
[1mStep[0m  [22/26], [94mLoss[0m : 2.33824
[1mStep[0m  [24/26], [94mLoss[0m : 2.35290

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.417, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48984
[1mStep[0m  [2/26], [94mLoss[0m : 2.50504
[1mStep[0m  [4/26], [94mLoss[0m : 2.45845
[1mStep[0m  [6/26], [94mLoss[0m : 2.49523
[1mStep[0m  [8/26], [94mLoss[0m : 2.47982
[1mStep[0m  [10/26], [94mLoss[0m : 2.35987
[1mStep[0m  [12/26], [94mLoss[0m : 2.52136
[1mStep[0m  [14/26], [94mLoss[0m : 2.31890
[1mStep[0m  [16/26], [94mLoss[0m : 2.44416
[1mStep[0m  [18/26], [94mLoss[0m : 2.41713
[1mStep[0m  [20/26], [94mLoss[0m : 2.66851
[1mStep[0m  [22/26], [94mLoss[0m : 2.41060
[1mStep[0m  [24/26], [94mLoss[0m : 2.49121

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.420, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45939
[1mStep[0m  [2/26], [94mLoss[0m : 2.41800
[1mStep[0m  [4/26], [94mLoss[0m : 2.46669
[1mStep[0m  [6/26], [94mLoss[0m : 2.54482
[1mStep[0m  [8/26], [94mLoss[0m : 2.48760
[1mStep[0m  [10/26], [94mLoss[0m : 2.59770
[1mStep[0m  [12/26], [94mLoss[0m : 2.49551
[1mStep[0m  [14/26], [94mLoss[0m : 2.41844
[1mStep[0m  [16/26], [94mLoss[0m : 2.46383
[1mStep[0m  [18/26], [94mLoss[0m : 2.50879
[1mStep[0m  [20/26], [94mLoss[0m : 2.31191
[1mStep[0m  [22/26], [94mLoss[0m : 2.34240
[1mStep[0m  [24/26], [94mLoss[0m : 2.39684

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.423, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34680
[1mStep[0m  [2/26], [94mLoss[0m : 2.50477
[1mStep[0m  [4/26], [94mLoss[0m : 2.47363
[1mStep[0m  [6/26], [94mLoss[0m : 2.44347
[1mStep[0m  [8/26], [94mLoss[0m : 2.47169
[1mStep[0m  [10/26], [94mLoss[0m : 2.54362
[1mStep[0m  [12/26], [94mLoss[0m : 2.49147
[1mStep[0m  [14/26], [94mLoss[0m : 2.29021
[1mStep[0m  [16/26], [94mLoss[0m : 2.38823
[1mStep[0m  [18/26], [94mLoss[0m : 2.45027
[1mStep[0m  [20/26], [94mLoss[0m : 2.36334
[1mStep[0m  [22/26], [94mLoss[0m : 2.44118
[1mStep[0m  [24/26], [94mLoss[0m : 2.46141

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.418, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48574
[1mStep[0m  [2/26], [94mLoss[0m : 2.41777
[1mStep[0m  [4/26], [94mLoss[0m : 2.35213
[1mStep[0m  [6/26], [94mLoss[0m : 2.49850
[1mStep[0m  [8/26], [94mLoss[0m : 2.40960
[1mStep[0m  [10/26], [94mLoss[0m : 2.49615
[1mStep[0m  [12/26], [94mLoss[0m : 2.51888
[1mStep[0m  [14/26], [94mLoss[0m : 2.37523
[1mStep[0m  [16/26], [94mLoss[0m : 2.33677
[1mStep[0m  [18/26], [94mLoss[0m : 2.44448
[1mStep[0m  [20/26], [94mLoss[0m : 2.35512
[1mStep[0m  [22/26], [94mLoss[0m : 2.53265
[1mStep[0m  [24/26], [94mLoss[0m : 2.45167

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.413, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42290
[1mStep[0m  [2/26], [94mLoss[0m : 2.52185
[1mStep[0m  [4/26], [94mLoss[0m : 2.34184
[1mStep[0m  [6/26], [94mLoss[0m : 2.39733
[1mStep[0m  [8/26], [94mLoss[0m : 2.38509
[1mStep[0m  [10/26], [94mLoss[0m : 2.45637
[1mStep[0m  [12/26], [94mLoss[0m : 2.38104
[1mStep[0m  [14/26], [94mLoss[0m : 2.59696
[1mStep[0m  [16/26], [94mLoss[0m : 2.54757
[1mStep[0m  [18/26], [94mLoss[0m : 2.41075
[1mStep[0m  [20/26], [94mLoss[0m : 2.69476
[1mStep[0m  [22/26], [94mLoss[0m : 2.44649
[1mStep[0m  [24/26], [94mLoss[0m : 2.49004

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.419, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46386
[1mStep[0m  [2/26], [94mLoss[0m : 2.42792
[1mStep[0m  [4/26], [94mLoss[0m : 2.37194
[1mStep[0m  [6/26], [94mLoss[0m : 2.50288
[1mStep[0m  [8/26], [94mLoss[0m : 2.51619
[1mStep[0m  [10/26], [94mLoss[0m : 2.54108
[1mStep[0m  [12/26], [94mLoss[0m : 2.46168
[1mStep[0m  [14/26], [94mLoss[0m : 2.48369
[1mStep[0m  [16/26], [94mLoss[0m : 2.50076
[1mStep[0m  [18/26], [94mLoss[0m : 2.41459
[1mStep[0m  [20/26], [94mLoss[0m : 2.61821
[1mStep[0m  [22/26], [94mLoss[0m : 2.45285
[1mStep[0m  [24/26], [94mLoss[0m : 2.56288

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.408, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42755
[1mStep[0m  [2/26], [94mLoss[0m : 2.44193
[1mStep[0m  [4/26], [94mLoss[0m : 2.40697
[1mStep[0m  [6/26], [94mLoss[0m : 2.44735
[1mStep[0m  [8/26], [94mLoss[0m : 2.35188
[1mStep[0m  [10/26], [94mLoss[0m : 2.46704
[1mStep[0m  [12/26], [94mLoss[0m : 2.54775
[1mStep[0m  [14/26], [94mLoss[0m : 2.44297
[1mStep[0m  [16/26], [94mLoss[0m : 2.47317
[1mStep[0m  [18/26], [94mLoss[0m : 2.48973
[1mStep[0m  [20/26], [94mLoss[0m : 2.43759
[1mStep[0m  [22/26], [94mLoss[0m : 2.37817
[1mStep[0m  [24/26], [94mLoss[0m : 2.53198

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.406, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53663
[1mStep[0m  [2/26], [94mLoss[0m : 2.27888
[1mStep[0m  [4/26], [94mLoss[0m : 2.53316
[1mStep[0m  [6/26], [94mLoss[0m : 2.52352
[1mStep[0m  [8/26], [94mLoss[0m : 2.58040
[1mStep[0m  [10/26], [94mLoss[0m : 2.33736
[1mStep[0m  [12/26], [94mLoss[0m : 2.48456
[1mStep[0m  [14/26], [94mLoss[0m : 2.39185
[1mStep[0m  [16/26], [94mLoss[0m : 2.49502
[1mStep[0m  [18/26], [94mLoss[0m : 2.36350
[1mStep[0m  [20/26], [94mLoss[0m : 2.43563
[1mStep[0m  [22/26], [94mLoss[0m : 2.45995
[1mStep[0m  [24/26], [94mLoss[0m : 2.51098

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.415, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50875
[1mStep[0m  [2/26], [94mLoss[0m : 2.47959
[1mStep[0m  [4/26], [94mLoss[0m : 2.36104
[1mStep[0m  [6/26], [94mLoss[0m : 2.49286
[1mStep[0m  [8/26], [94mLoss[0m : 2.40576
[1mStep[0m  [10/26], [94mLoss[0m : 2.48304
[1mStep[0m  [12/26], [94mLoss[0m : 2.43575
[1mStep[0m  [14/26], [94mLoss[0m : 2.58168
[1mStep[0m  [16/26], [94mLoss[0m : 2.52752
[1mStep[0m  [18/26], [94mLoss[0m : 2.42751
[1mStep[0m  [20/26], [94mLoss[0m : 2.58830
[1mStep[0m  [22/26], [94mLoss[0m : 2.54296
[1mStep[0m  [24/26], [94mLoss[0m : 2.44274

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.409, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38425
[1mStep[0m  [2/26], [94mLoss[0m : 2.43317
[1mStep[0m  [4/26], [94mLoss[0m : 2.47142
[1mStep[0m  [6/26], [94mLoss[0m : 2.57016
[1mStep[0m  [8/26], [94mLoss[0m : 2.32081
[1mStep[0m  [10/26], [94mLoss[0m : 2.35187
[1mStep[0m  [12/26], [94mLoss[0m : 2.51384
[1mStep[0m  [14/26], [94mLoss[0m : 2.46910
[1mStep[0m  [16/26], [94mLoss[0m : 2.35167
[1mStep[0m  [18/26], [94mLoss[0m : 2.53987
[1mStep[0m  [20/26], [94mLoss[0m : 2.50846
[1mStep[0m  [22/26], [94mLoss[0m : 2.35805
[1mStep[0m  [24/26], [94mLoss[0m : 2.29905

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.413, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.409
====================================

Phase 1 - Evaluation MAE:  2.409041844881498
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.43810
[1mStep[0m  [2/26], [94mLoss[0m : 2.74388
[1mStep[0m  [4/26], [94mLoss[0m : 2.51641
[1mStep[0m  [6/26], [94mLoss[0m : 2.42343
[1mStep[0m  [8/26], [94mLoss[0m : 2.51918
[1mStep[0m  [10/26], [94mLoss[0m : 2.38615
[1mStep[0m  [12/26], [94mLoss[0m : 2.54982
[1mStep[0m  [14/26], [94mLoss[0m : 2.43388
[1mStep[0m  [16/26], [94mLoss[0m : 2.57166
[1mStep[0m  [18/26], [94mLoss[0m : 2.43256
[1mStep[0m  [20/26], [94mLoss[0m : 2.62855
[1mStep[0m  [22/26], [94mLoss[0m : 2.41766
[1mStep[0m  [24/26], [94mLoss[0m : 2.51440

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57645
[1mStep[0m  [2/26], [94mLoss[0m : 2.42727
[1mStep[0m  [4/26], [94mLoss[0m : 2.53181
[1mStep[0m  [6/26], [94mLoss[0m : 2.48308
[1mStep[0m  [8/26], [94mLoss[0m : 2.37561
[1mStep[0m  [10/26], [94mLoss[0m : 2.55825
[1mStep[0m  [12/26], [94mLoss[0m : 2.42920
[1mStep[0m  [14/26], [94mLoss[0m : 2.52792
[1mStep[0m  [16/26], [94mLoss[0m : 2.38861
[1mStep[0m  [18/26], [94mLoss[0m : 2.44403
[1mStep[0m  [20/26], [94mLoss[0m : 2.52808
[1mStep[0m  [22/26], [94mLoss[0m : 2.51442
[1mStep[0m  [24/26], [94mLoss[0m : 2.58406

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33547
[1mStep[0m  [2/26], [94mLoss[0m : 2.42765
[1mStep[0m  [4/26], [94mLoss[0m : 2.55619
[1mStep[0m  [6/26], [94mLoss[0m : 2.53006
[1mStep[0m  [8/26], [94mLoss[0m : 2.43239
[1mStep[0m  [10/26], [94mLoss[0m : 2.53026
[1mStep[0m  [12/26], [94mLoss[0m : 2.52860
[1mStep[0m  [14/26], [94mLoss[0m : 2.71557
[1mStep[0m  [16/26], [94mLoss[0m : 2.38337
[1mStep[0m  [18/26], [94mLoss[0m : 2.50963
[1mStep[0m  [20/26], [94mLoss[0m : 2.57763
[1mStep[0m  [22/26], [94mLoss[0m : 2.46015
[1mStep[0m  [24/26], [94mLoss[0m : 2.37407

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43191
[1mStep[0m  [2/26], [94mLoss[0m : 2.49390
[1mStep[0m  [4/26], [94mLoss[0m : 2.49472
[1mStep[0m  [6/26], [94mLoss[0m : 2.42938
[1mStep[0m  [8/26], [94mLoss[0m : 2.20117
[1mStep[0m  [10/26], [94mLoss[0m : 2.39507
[1mStep[0m  [12/26], [94mLoss[0m : 2.57097
[1mStep[0m  [14/26], [94mLoss[0m : 2.47628
[1mStep[0m  [16/26], [94mLoss[0m : 2.45457
[1mStep[0m  [18/26], [94mLoss[0m : 2.44731
[1mStep[0m  [20/26], [94mLoss[0m : 2.52108
[1mStep[0m  [22/26], [94mLoss[0m : 2.51949
[1mStep[0m  [24/26], [94mLoss[0m : 2.39678

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43757
[1mStep[0m  [2/26], [94mLoss[0m : 2.24163
[1mStep[0m  [4/26], [94mLoss[0m : 2.42194
[1mStep[0m  [6/26], [94mLoss[0m : 2.51222
[1mStep[0m  [8/26], [94mLoss[0m : 2.54064
[1mStep[0m  [10/26], [94mLoss[0m : 2.39042
[1mStep[0m  [12/26], [94mLoss[0m : 2.26073
[1mStep[0m  [14/26], [94mLoss[0m : 2.36469
[1mStep[0m  [16/26], [94mLoss[0m : 2.38289
[1mStep[0m  [18/26], [94mLoss[0m : 2.44153
[1mStep[0m  [20/26], [94mLoss[0m : 2.53780
[1mStep[0m  [22/26], [94mLoss[0m : 2.44615
[1mStep[0m  [24/26], [94mLoss[0m : 2.35320

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55358
[1mStep[0m  [2/26], [94mLoss[0m : 2.51551
[1mStep[0m  [4/26], [94mLoss[0m : 2.44829
[1mStep[0m  [6/26], [94mLoss[0m : 2.39468
[1mStep[0m  [8/26], [94mLoss[0m : 2.44237
[1mStep[0m  [10/26], [94mLoss[0m : 2.42488
[1mStep[0m  [12/26], [94mLoss[0m : 2.36610
[1mStep[0m  [14/26], [94mLoss[0m : 2.33282
[1mStep[0m  [16/26], [94mLoss[0m : 2.44458
[1mStep[0m  [18/26], [94mLoss[0m : 2.38948
[1mStep[0m  [20/26], [94mLoss[0m : 2.39189
[1mStep[0m  [22/26], [94mLoss[0m : 2.32806
[1mStep[0m  [24/26], [94mLoss[0m : 2.47798

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37138
[1mStep[0m  [2/26], [94mLoss[0m : 2.42993
[1mStep[0m  [4/26], [94mLoss[0m : 2.32654
[1mStep[0m  [6/26], [94mLoss[0m : 2.40526
[1mStep[0m  [8/26], [94mLoss[0m : 2.28755
[1mStep[0m  [10/26], [94mLoss[0m : 2.41388
[1mStep[0m  [12/26], [94mLoss[0m : 2.43177
[1mStep[0m  [14/26], [94mLoss[0m : 2.52636
[1mStep[0m  [16/26], [94mLoss[0m : 2.27714
[1mStep[0m  [18/26], [94mLoss[0m : 2.34387
[1mStep[0m  [20/26], [94mLoss[0m : 2.41338
[1mStep[0m  [22/26], [94mLoss[0m : 2.40135
[1mStep[0m  [24/26], [94mLoss[0m : 2.35473

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36823
[1mStep[0m  [2/26], [94mLoss[0m : 2.29408
[1mStep[0m  [4/26], [94mLoss[0m : 2.37939
[1mStep[0m  [6/26], [94mLoss[0m : 2.24717
[1mStep[0m  [8/26], [94mLoss[0m : 2.34470
[1mStep[0m  [10/26], [94mLoss[0m : 2.52831
[1mStep[0m  [12/26], [94mLoss[0m : 2.58621
[1mStep[0m  [14/26], [94mLoss[0m : 2.53127
[1mStep[0m  [16/26], [94mLoss[0m : 2.34968
[1mStep[0m  [18/26], [94mLoss[0m : 2.36992
[1mStep[0m  [20/26], [94mLoss[0m : 2.28328
[1mStep[0m  [22/26], [94mLoss[0m : 2.41585
[1mStep[0m  [24/26], [94mLoss[0m : 2.33247

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34109
[1mStep[0m  [2/26], [94mLoss[0m : 2.35091
[1mStep[0m  [4/26], [94mLoss[0m : 2.30722
[1mStep[0m  [6/26], [94mLoss[0m : 2.19725
[1mStep[0m  [8/26], [94mLoss[0m : 2.44348
[1mStep[0m  [10/26], [94mLoss[0m : 2.33632
[1mStep[0m  [12/26], [94mLoss[0m : 2.44154
[1mStep[0m  [14/26], [94mLoss[0m : 2.24787
[1mStep[0m  [16/26], [94mLoss[0m : 2.42128
[1mStep[0m  [18/26], [94mLoss[0m : 2.45873
[1mStep[0m  [20/26], [94mLoss[0m : 2.48132
[1mStep[0m  [22/26], [94mLoss[0m : 2.44329
[1mStep[0m  [24/26], [94mLoss[0m : 2.26838

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38710
[1mStep[0m  [2/26], [94mLoss[0m : 2.38792
[1mStep[0m  [4/26], [94mLoss[0m : 2.29143
[1mStep[0m  [6/26], [94mLoss[0m : 2.49000
[1mStep[0m  [8/26], [94mLoss[0m : 2.27369
[1mStep[0m  [10/26], [94mLoss[0m : 2.25734
[1mStep[0m  [12/26], [94mLoss[0m : 2.45089
[1mStep[0m  [14/26], [94mLoss[0m : 2.39358
[1mStep[0m  [16/26], [94mLoss[0m : 2.36275
[1mStep[0m  [18/26], [94mLoss[0m : 2.32329
[1mStep[0m  [20/26], [94mLoss[0m : 2.31233
[1mStep[0m  [22/26], [94mLoss[0m : 2.29475
[1mStep[0m  [24/26], [94mLoss[0m : 2.39503

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.531, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43904
[1mStep[0m  [2/26], [94mLoss[0m : 2.35980
[1mStep[0m  [4/26], [94mLoss[0m : 2.17485
[1mStep[0m  [6/26], [94mLoss[0m : 2.35339
[1mStep[0m  [8/26], [94mLoss[0m : 2.26682
[1mStep[0m  [10/26], [94mLoss[0m : 2.26369
[1mStep[0m  [12/26], [94mLoss[0m : 2.17654
[1mStep[0m  [14/26], [94mLoss[0m : 2.25764
[1mStep[0m  [16/26], [94mLoss[0m : 2.34094
[1mStep[0m  [18/26], [94mLoss[0m : 2.28713
[1mStep[0m  [20/26], [94mLoss[0m : 2.38238
[1mStep[0m  [22/26], [94mLoss[0m : 2.38711
[1mStep[0m  [24/26], [94mLoss[0m : 2.34030

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.552, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21422
[1mStep[0m  [2/26], [94mLoss[0m : 2.55050
[1mStep[0m  [4/26], [94mLoss[0m : 2.28884
[1mStep[0m  [6/26], [94mLoss[0m : 2.24639
[1mStep[0m  [8/26], [94mLoss[0m : 2.45231
[1mStep[0m  [10/26], [94mLoss[0m : 2.50836
[1mStep[0m  [12/26], [94mLoss[0m : 2.29981
[1mStep[0m  [14/26], [94mLoss[0m : 2.40057
[1mStep[0m  [16/26], [94mLoss[0m : 2.43062
[1mStep[0m  [18/26], [94mLoss[0m : 2.39008
[1mStep[0m  [20/26], [94mLoss[0m : 2.29218
[1mStep[0m  [22/26], [94mLoss[0m : 2.24841
[1mStep[0m  [24/26], [94mLoss[0m : 2.22727

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.545, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26595
[1mStep[0m  [2/26], [94mLoss[0m : 2.30027
[1mStep[0m  [4/26], [94mLoss[0m : 2.36112
[1mStep[0m  [6/26], [94mLoss[0m : 2.39773
[1mStep[0m  [8/26], [94mLoss[0m : 2.29695
[1mStep[0m  [10/26], [94mLoss[0m : 2.22978
[1mStep[0m  [12/26], [94mLoss[0m : 2.34813
[1mStep[0m  [14/26], [94mLoss[0m : 2.47292
[1mStep[0m  [16/26], [94mLoss[0m : 2.24621
[1mStep[0m  [18/26], [94mLoss[0m : 2.38153
[1mStep[0m  [20/26], [94mLoss[0m : 2.26730
[1mStep[0m  [22/26], [94mLoss[0m : 2.33219
[1mStep[0m  [24/26], [94mLoss[0m : 2.32170

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30903
[1mStep[0m  [2/26], [94mLoss[0m : 2.27739
[1mStep[0m  [4/26], [94mLoss[0m : 2.16627
[1mStep[0m  [6/26], [94mLoss[0m : 2.42803
[1mStep[0m  [8/26], [94mLoss[0m : 2.33944
[1mStep[0m  [10/26], [94mLoss[0m : 2.35541
[1mStep[0m  [12/26], [94mLoss[0m : 2.32486
[1mStep[0m  [14/26], [94mLoss[0m : 2.26008
[1mStep[0m  [16/26], [94mLoss[0m : 2.28955
[1mStep[0m  [18/26], [94mLoss[0m : 2.17308
[1mStep[0m  [20/26], [94mLoss[0m : 2.30096
[1mStep[0m  [22/26], [94mLoss[0m : 2.31418
[1mStep[0m  [24/26], [94mLoss[0m : 2.28643

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.518, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32319
[1mStep[0m  [2/26], [94mLoss[0m : 2.27459
[1mStep[0m  [4/26], [94mLoss[0m : 2.33246
[1mStep[0m  [6/26], [94mLoss[0m : 2.25306
[1mStep[0m  [8/26], [94mLoss[0m : 2.40767
[1mStep[0m  [10/26], [94mLoss[0m : 2.27986
[1mStep[0m  [12/26], [94mLoss[0m : 2.29176
[1mStep[0m  [14/26], [94mLoss[0m : 2.43967
[1mStep[0m  [16/26], [94mLoss[0m : 2.33391
[1mStep[0m  [18/26], [94mLoss[0m : 2.34369
[1mStep[0m  [20/26], [94mLoss[0m : 2.27480
[1mStep[0m  [22/26], [94mLoss[0m : 2.27975
[1mStep[0m  [24/26], [94mLoss[0m : 2.37694

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.530, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20795
[1mStep[0m  [2/26], [94mLoss[0m : 2.43251
[1mStep[0m  [4/26], [94mLoss[0m : 2.23058
[1mStep[0m  [6/26], [94mLoss[0m : 2.25118
[1mStep[0m  [8/26], [94mLoss[0m : 2.35145
[1mStep[0m  [10/26], [94mLoss[0m : 2.14438
[1mStep[0m  [12/26], [94mLoss[0m : 2.33036
[1mStep[0m  [14/26], [94mLoss[0m : 2.19925
[1mStep[0m  [16/26], [94mLoss[0m : 2.35485
[1mStep[0m  [18/26], [94mLoss[0m : 2.30544
[1mStep[0m  [20/26], [94mLoss[0m : 2.22425
[1mStep[0m  [22/26], [94mLoss[0m : 2.21417
[1mStep[0m  [24/26], [94mLoss[0m : 2.37816

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25834
[1mStep[0m  [2/26], [94mLoss[0m : 2.35881
[1mStep[0m  [4/26], [94mLoss[0m : 2.26609
[1mStep[0m  [6/26], [94mLoss[0m : 2.26290
[1mStep[0m  [8/26], [94mLoss[0m : 2.16869
[1mStep[0m  [10/26], [94mLoss[0m : 2.27186
[1mStep[0m  [12/26], [94mLoss[0m : 2.41976
[1mStep[0m  [14/26], [94mLoss[0m : 2.28001
[1mStep[0m  [16/26], [94mLoss[0m : 2.24907
[1mStep[0m  [18/26], [94mLoss[0m : 2.21010
[1mStep[0m  [20/26], [94mLoss[0m : 2.30395
[1mStep[0m  [22/26], [94mLoss[0m : 2.25674
[1mStep[0m  [24/26], [94mLoss[0m : 2.29950

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26917
[1mStep[0m  [2/26], [94mLoss[0m : 2.25101
[1mStep[0m  [4/26], [94mLoss[0m : 2.18739
[1mStep[0m  [6/26], [94mLoss[0m : 2.25778
[1mStep[0m  [8/26], [94mLoss[0m : 2.16416
[1mStep[0m  [10/26], [94mLoss[0m : 2.31614
[1mStep[0m  [12/26], [94mLoss[0m : 2.15307
[1mStep[0m  [14/26], [94mLoss[0m : 2.17046
[1mStep[0m  [16/26], [94mLoss[0m : 2.22993
[1mStep[0m  [18/26], [94mLoss[0m : 2.27925
[1mStep[0m  [20/26], [94mLoss[0m : 2.24350
[1mStep[0m  [22/26], [94mLoss[0m : 2.23861
[1mStep[0m  [24/26], [94mLoss[0m : 2.22608

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33128
[1mStep[0m  [2/26], [94mLoss[0m : 2.31759
[1mStep[0m  [4/26], [94mLoss[0m : 2.08254
[1mStep[0m  [6/26], [94mLoss[0m : 2.25293
[1mStep[0m  [8/26], [94mLoss[0m : 2.38999
[1mStep[0m  [10/26], [94mLoss[0m : 2.33326
[1mStep[0m  [12/26], [94mLoss[0m : 2.22415
[1mStep[0m  [14/26], [94mLoss[0m : 2.20161
[1mStep[0m  [16/26], [94mLoss[0m : 2.15311
[1mStep[0m  [18/26], [94mLoss[0m : 2.30072
[1mStep[0m  [20/26], [94mLoss[0m : 2.24503
[1mStep[0m  [22/26], [94mLoss[0m : 2.20388
[1mStep[0m  [24/26], [94mLoss[0m : 2.24132

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09660
[1mStep[0m  [2/26], [94mLoss[0m : 2.33045
[1mStep[0m  [4/26], [94mLoss[0m : 2.31326
[1mStep[0m  [6/26], [94mLoss[0m : 2.30773
[1mStep[0m  [8/26], [94mLoss[0m : 2.22971
[1mStep[0m  [10/26], [94mLoss[0m : 2.26320
[1mStep[0m  [12/26], [94mLoss[0m : 2.21467
[1mStep[0m  [14/26], [94mLoss[0m : 2.09286
[1mStep[0m  [16/26], [94mLoss[0m : 2.25572
[1mStep[0m  [18/26], [94mLoss[0m : 2.13715
[1mStep[0m  [20/26], [94mLoss[0m : 2.17689
[1mStep[0m  [22/26], [94mLoss[0m : 2.17063
[1mStep[0m  [24/26], [94mLoss[0m : 2.25895

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.483, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.13879
[1mStep[0m  [2/26], [94mLoss[0m : 2.10009
[1mStep[0m  [4/26], [94mLoss[0m : 2.03689
[1mStep[0m  [6/26], [94mLoss[0m : 2.24237
[1mStep[0m  [8/26], [94mLoss[0m : 2.16917
[1mStep[0m  [10/26], [94mLoss[0m : 2.08043
[1mStep[0m  [12/26], [94mLoss[0m : 2.26054
[1mStep[0m  [14/26], [94mLoss[0m : 2.10482
[1mStep[0m  [16/26], [94mLoss[0m : 2.15464
[1mStep[0m  [18/26], [94mLoss[0m : 2.20953
[1mStep[0m  [20/26], [94mLoss[0m : 2.16471
[1mStep[0m  [22/26], [94mLoss[0m : 2.30084
[1mStep[0m  [24/26], [94mLoss[0m : 2.10381

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.505, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.12682
[1mStep[0m  [2/26], [94mLoss[0m : 2.12628
[1mStep[0m  [4/26], [94mLoss[0m : 2.10420
[1mStep[0m  [6/26], [94mLoss[0m : 2.34359
[1mStep[0m  [8/26], [94mLoss[0m : 2.08144
[1mStep[0m  [10/26], [94mLoss[0m : 2.12193
[1mStep[0m  [12/26], [94mLoss[0m : 2.21495
[1mStep[0m  [14/26], [94mLoss[0m : 2.17047
[1mStep[0m  [16/26], [94mLoss[0m : 2.15386
[1mStep[0m  [18/26], [94mLoss[0m : 2.25120
[1mStep[0m  [20/26], [94mLoss[0m : 2.18054
[1mStep[0m  [22/26], [94mLoss[0m : 2.14439
[1mStep[0m  [24/26], [94mLoss[0m : 2.07267

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03779
[1mStep[0m  [2/26], [94mLoss[0m : 2.20701
[1mStep[0m  [4/26], [94mLoss[0m : 2.25676
[1mStep[0m  [6/26], [94mLoss[0m : 2.09237
[1mStep[0m  [8/26], [94mLoss[0m : 2.07296
[1mStep[0m  [10/26], [94mLoss[0m : 2.21441
[1mStep[0m  [12/26], [94mLoss[0m : 2.22958
[1mStep[0m  [14/26], [94mLoss[0m : 2.20018
[1mStep[0m  [16/26], [94mLoss[0m : 2.14200
[1mStep[0m  [18/26], [94mLoss[0m : 2.19776
[1mStep[0m  [20/26], [94mLoss[0m : 2.25996
[1mStep[0m  [22/26], [94mLoss[0m : 2.01730
[1mStep[0m  [24/26], [94mLoss[0m : 2.14365

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.468, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21394
[1mStep[0m  [2/26], [94mLoss[0m : 2.04499
[1mStep[0m  [4/26], [94mLoss[0m : 2.07131
[1mStep[0m  [6/26], [94mLoss[0m : 2.05744
[1mStep[0m  [8/26], [94mLoss[0m : 2.17016
[1mStep[0m  [10/26], [94mLoss[0m : 2.15690
[1mStep[0m  [12/26], [94mLoss[0m : 2.00652
[1mStep[0m  [14/26], [94mLoss[0m : 2.13713
[1mStep[0m  [16/26], [94mLoss[0m : 2.23240
[1mStep[0m  [18/26], [94mLoss[0m : 2.24169
[1mStep[0m  [20/26], [94mLoss[0m : 2.15086
[1mStep[0m  [22/26], [94mLoss[0m : 2.10845
[1mStep[0m  [24/26], [94mLoss[0m : 2.08805

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.490, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06911
[1mStep[0m  [2/26], [94mLoss[0m : 1.99375
[1mStep[0m  [4/26], [94mLoss[0m : 1.94034
[1mStep[0m  [6/26], [94mLoss[0m : 2.06640
[1mStep[0m  [8/26], [94mLoss[0m : 2.11402
[1mStep[0m  [10/26], [94mLoss[0m : 2.08236
[1mStep[0m  [12/26], [94mLoss[0m : 2.11195
[1mStep[0m  [14/26], [94mLoss[0m : 2.12246
[1mStep[0m  [16/26], [94mLoss[0m : 2.05087
[1mStep[0m  [18/26], [94mLoss[0m : 2.18466
[1mStep[0m  [20/26], [94mLoss[0m : 1.94580
[1mStep[0m  [22/26], [94mLoss[0m : 2.26126
[1mStep[0m  [24/26], [94mLoss[0m : 2.12512

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.436, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84283
[1mStep[0m  [2/26], [94mLoss[0m : 2.14786
[1mStep[0m  [4/26], [94mLoss[0m : 2.04316
[1mStep[0m  [6/26], [94mLoss[0m : 2.21412
[1mStep[0m  [8/26], [94mLoss[0m : 2.12462
[1mStep[0m  [10/26], [94mLoss[0m : 2.04651
[1mStep[0m  [12/26], [94mLoss[0m : 2.09639
[1mStep[0m  [14/26], [94mLoss[0m : 2.02714
[1mStep[0m  [16/26], [94mLoss[0m : 2.19283
[1mStep[0m  [18/26], [94mLoss[0m : 2.12248
[1mStep[0m  [20/26], [94mLoss[0m : 2.12226
[1mStep[0m  [22/26], [94mLoss[0m : 1.98028
[1mStep[0m  [24/26], [94mLoss[0m : 2.06032

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.461, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88758
[1mStep[0m  [2/26], [94mLoss[0m : 2.10709
[1mStep[0m  [4/26], [94mLoss[0m : 2.08567
[1mStep[0m  [6/26], [94mLoss[0m : 2.07491
[1mStep[0m  [8/26], [94mLoss[0m : 2.02135
[1mStep[0m  [10/26], [94mLoss[0m : 1.93806
[1mStep[0m  [12/26], [94mLoss[0m : 2.11802
[1mStep[0m  [14/26], [94mLoss[0m : 2.10102
[1mStep[0m  [16/26], [94mLoss[0m : 2.05069
[1mStep[0m  [18/26], [94mLoss[0m : 2.13809
[1mStep[0m  [20/26], [94mLoss[0m : 2.06298
[1mStep[0m  [22/26], [94mLoss[0m : 2.14313
[1mStep[0m  [24/26], [94mLoss[0m : 2.15110

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.00525
[1mStep[0m  [2/26], [94mLoss[0m : 2.07989
[1mStep[0m  [4/26], [94mLoss[0m : 1.87776
[1mStep[0m  [6/26], [94mLoss[0m : 2.05928
[1mStep[0m  [8/26], [94mLoss[0m : 2.02569
[1mStep[0m  [10/26], [94mLoss[0m : 2.10383
[1mStep[0m  [12/26], [94mLoss[0m : 2.01575
[1mStep[0m  [14/26], [94mLoss[0m : 2.11102
[1mStep[0m  [16/26], [94mLoss[0m : 2.15305
[1mStep[0m  [18/26], [94mLoss[0m : 2.00159
[1mStep[0m  [20/26], [94mLoss[0m : 2.05573
[1mStep[0m  [22/26], [94mLoss[0m : 2.02990
[1mStep[0m  [24/26], [94mLoss[0m : 2.00869

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.453, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86932
[1mStep[0m  [2/26], [94mLoss[0m : 1.98569
[1mStep[0m  [4/26], [94mLoss[0m : 2.05253
[1mStep[0m  [6/26], [94mLoss[0m : 2.17997
[1mStep[0m  [8/26], [94mLoss[0m : 1.93854
[1mStep[0m  [10/26], [94mLoss[0m : 1.90453
[1mStep[0m  [12/26], [94mLoss[0m : 1.87258
[1mStep[0m  [14/26], [94mLoss[0m : 1.98238
[1mStep[0m  [16/26], [94mLoss[0m : 2.07948
[1mStep[0m  [18/26], [94mLoss[0m : 2.00594
[1mStep[0m  [20/26], [94mLoss[0m : 2.12767
[1mStep[0m  [22/26], [94mLoss[0m : 2.01720
[1mStep[0m  [24/26], [94mLoss[0m : 2.00793

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.995, [92mTest[0m: 2.418, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05265
[1mStep[0m  [2/26], [94mLoss[0m : 2.07630
[1mStep[0m  [4/26], [94mLoss[0m : 1.98695
[1mStep[0m  [6/26], [94mLoss[0m : 1.89468
[1mStep[0m  [8/26], [94mLoss[0m : 2.03598
[1mStep[0m  [10/26], [94mLoss[0m : 2.01779
[1mStep[0m  [12/26], [94mLoss[0m : 1.93008
[1mStep[0m  [14/26], [94mLoss[0m : 2.10941
[1mStep[0m  [16/26], [94mLoss[0m : 2.00080
[1mStep[0m  [18/26], [94mLoss[0m : 2.10400
[1mStep[0m  [20/26], [94mLoss[0m : 2.00350
[1mStep[0m  [22/26], [94mLoss[0m : 1.92547
[1mStep[0m  [24/26], [94mLoss[0m : 1.95158

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.401, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.5099106018359842
MAE score P1      2.409042
MAE score P2      2.509911
loss              1.984064
learning_rate     0.007525
batch_size             512
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.52161
[1mStep[0m  [2/26], [94mLoss[0m : 10.71966
[1mStep[0m  [4/26], [94mLoss[0m : 10.65297
[1mStep[0m  [6/26], [94mLoss[0m : 10.85031
[1mStep[0m  [8/26], [94mLoss[0m : 10.59253
[1mStep[0m  [10/26], [94mLoss[0m : 11.12305
[1mStep[0m  [12/26], [94mLoss[0m : 10.55739
[1mStep[0m  [14/26], [94mLoss[0m : 10.77473
[1mStep[0m  [16/26], [94mLoss[0m : 10.63942
[1mStep[0m  [18/26], [94mLoss[0m : 10.66910
[1mStep[0m  [20/26], [94mLoss[0m : 10.70300
[1mStep[0m  [22/26], [94mLoss[0m : 10.55126
[1mStep[0m  [24/26], [94mLoss[0m : 10.47653

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.649, [92mTest[0m: 10.867, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.59972
[1mStep[0m  [2/26], [94mLoss[0m : 10.35826
[1mStep[0m  [4/26], [94mLoss[0m : 10.32514
[1mStep[0m  [6/26], [94mLoss[0m : 10.27009
[1mStep[0m  [8/26], [94mLoss[0m : 10.31645
[1mStep[0m  [10/26], [94mLoss[0m : 10.56932
[1mStep[0m  [12/26], [94mLoss[0m : 10.45397
[1mStep[0m  [14/26], [94mLoss[0m : 9.92069
[1mStep[0m  [16/26], [94mLoss[0m : 10.27834
[1mStep[0m  [18/26], [94mLoss[0m : 10.49882
[1mStep[0m  [20/26], [94mLoss[0m : 10.46462
[1mStep[0m  [22/26], [94mLoss[0m : 10.05964
[1mStep[0m  [24/26], [94mLoss[0m : 10.34758

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.300, [92mTest[0m: 10.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.18866
[1mStep[0m  [2/26], [94mLoss[0m : 10.20007
[1mStep[0m  [4/26], [94mLoss[0m : 9.91103
[1mStep[0m  [6/26], [94mLoss[0m : 9.97440
[1mStep[0m  [8/26], [94mLoss[0m : 10.00114
[1mStep[0m  [10/26], [94mLoss[0m : 10.13283
[1mStep[0m  [12/26], [94mLoss[0m : 9.86400
[1mStep[0m  [14/26], [94mLoss[0m : 9.87457
[1mStep[0m  [16/26], [94mLoss[0m : 9.95984
[1mStep[0m  [18/26], [94mLoss[0m : 9.66834
[1mStep[0m  [20/26], [94mLoss[0m : 10.12138
[1mStep[0m  [22/26], [94mLoss[0m : 9.73660
[1mStep[0m  [24/26], [94mLoss[0m : 9.57517

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.932, [92mTest[0m: 10.042, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.73953
[1mStep[0m  [2/26], [94mLoss[0m : 9.86721
[1mStep[0m  [4/26], [94mLoss[0m : 9.79046
[1mStep[0m  [6/26], [94mLoss[0m : 9.76045
[1mStep[0m  [8/26], [94mLoss[0m : 9.59187
[1mStep[0m  [10/26], [94mLoss[0m : 9.59084
[1mStep[0m  [12/26], [94mLoss[0m : 9.48632
[1mStep[0m  [14/26], [94mLoss[0m : 9.70121
[1mStep[0m  [16/26], [94mLoss[0m : 9.63566
[1mStep[0m  [18/26], [94mLoss[0m : 9.43738
[1mStep[0m  [20/26], [94mLoss[0m : 9.49872
[1mStep[0m  [22/26], [94mLoss[0m : 9.43587
[1mStep[0m  [24/26], [94mLoss[0m : 9.27532

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.554, [92mTest[0m: 9.577, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.35195
[1mStep[0m  [2/26], [94mLoss[0m : 9.09216
[1mStep[0m  [4/26], [94mLoss[0m : 9.21374
[1mStep[0m  [6/26], [94mLoss[0m : 9.13324
[1mStep[0m  [8/26], [94mLoss[0m : 9.39324
[1mStep[0m  [10/26], [94mLoss[0m : 9.10488
[1mStep[0m  [12/26], [94mLoss[0m : 9.38270
[1mStep[0m  [14/26], [94mLoss[0m : 8.99110
[1mStep[0m  [16/26], [94mLoss[0m : 8.66588
[1mStep[0m  [18/26], [94mLoss[0m : 8.97132
[1mStep[0m  [20/26], [94mLoss[0m : 8.94468
[1mStep[0m  [22/26], [94mLoss[0m : 8.91064
[1mStep[0m  [24/26], [94mLoss[0m : 8.92869

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.080, [92mTest[0m: 9.075, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.10186
[1mStep[0m  [2/26], [94mLoss[0m : 8.74446
[1mStep[0m  [4/26], [94mLoss[0m : 8.80218
[1mStep[0m  [6/26], [94mLoss[0m : 8.51942
[1mStep[0m  [8/26], [94mLoss[0m : 8.91515
[1mStep[0m  [10/26], [94mLoss[0m : 8.70563
[1mStep[0m  [12/26], [94mLoss[0m : 8.53671
[1mStep[0m  [14/26], [94mLoss[0m : 8.40529
[1mStep[0m  [16/26], [94mLoss[0m : 8.41040
[1mStep[0m  [18/26], [94mLoss[0m : 8.65942
[1mStep[0m  [20/26], [94mLoss[0m : 8.34901
[1mStep[0m  [22/26], [94mLoss[0m : 8.44594
[1mStep[0m  [24/26], [94mLoss[0m : 8.23432

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.549, [92mTest[0m: 8.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.35201
[1mStep[0m  [2/26], [94mLoss[0m : 8.22289
[1mStep[0m  [4/26], [94mLoss[0m : 8.00988
[1mStep[0m  [6/26], [94mLoss[0m : 7.99810
[1mStep[0m  [8/26], [94mLoss[0m : 8.08070
[1mStep[0m  [10/26], [94mLoss[0m : 7.85676
[1mStep[0m  [12/26], [94mLoss[0m : 7.93031
[1mStep[0m  [14/26], [94mLoss[0m : 7.95180
[1mStep[0m  [16/26], [94mLoss[0m : 7.84482
[1mStep[0m  [18/26], [94mLoss[0m : 7.82204
[1mStep[0m  [20/26], [94mLoss[0m : 7.83590
[1mStep[0m  [22/26], [94mLoss[0m : 7.94847
[1mStep[0m  [24/26], [94mLoss[0m : 7.87864

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.932, [92mTest[0m: 7.793, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.62739
[1mStep[0m  [2/26], [94mLoss[0m : 7.51155
[1mStep[0m  [4/26], [94mLoss[0m : 7.69371
[1mStep[0m  [6/26], [94mLoss[0m : 7.28174
[1mStep[0m  [8/26], [94mLoss[0m : 7.41069
[1mStep[0m  [10/26], [94mLoss[0m : 7.60907
[1mStep[0m  [12/26], [94mLoss[0m : 7.47271
[1mStep[0m  [14/26], [94mLoss[0m : 7.19920
[1mStep[0m  [16/26], [94mLoss[0m : 7.37435
[1mStep[0m  [18/26], [94mLoss[0m : 7.12010
[1mStep[0m  [20/26], [94mLoss[0m : 7.18159
[1mStep[0m  [22/26], [94mLoss[0m : 7.11683
[1mStep[0m  [24/26], [94mLoss[0m : 7.32059

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.339, [92mTest[0m: 7.178, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.89813
[1mStep[0m  [2/26], [94mLoss[0m : 6.96390
[1mStep[0m  [4/26], [94mLoss[0m : 7.15643
[1mStep[0m  [6/26], [94mLoss[0m : 6.95542
[1mStep[0m  [8/26], [94mLoss[0m : 6.82942
[1mStep[0m  [10/26], [94mLoss[0m : 6.69324
[1mStep[0m  [12/26], [94mLoss[0m : 6.90857
[1mStep[0m  [14/26], [94mLoss[0m : 6.79534
[1mStep[0m  [16/26], [94mLoss[0m : 6.74299
[1mStep[0m  [18/26], [94mLoss[0m : 6.79528
[1mStep[0m  [20/26], [94mLoss[0m : 6.61448
[1mStep[0m  [22/26], [94mLoss[0m : 6.84572
[1mStep[0m  [24/26], [94mLoss[0m : 6.47665

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.821, [92mTest[0m: 6.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.65814
[1mStep[0m  [2/26], [94mLoss[0m : 6.75478
[1mStep[0m  [4/26], [94mLoss[0m : 6.48094
[1mStep[0m  [6/26], [94mLoss[0m : 6.55550
[1mStep[0m  [8/26], [94mLoss[0m : 6.38929
[1mStep[0m  [10/26], [94mLoss[0m : 6.33766
[1mStep[0m  [12/26], [94mLoss[0m : 6.27368
[1mStep[0m  [14/26], [94mLoss[0m : 6.12667
[1mStep[0m  [16/26], [94mLoss[0m : 6.15410
[1mStep[0m  [18/26], [94mLoss[0m : 6.31319
[1mStep[0m  [20/26], [94mLoss[0m : 6.32858
[1mStep[0m  [22/26], [94mLoss[0m : 6.40350
[1mStep[0m  [24/26], [94mLoss[0m : 6.30148

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.385, [92mTest[0m: 5.988, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.34342
[1mStep[0m  [2/26], [94mLoss[0m : 6.20889
[1mStep[0m  [4/26], [94mLoss[0m : 6.04835
[1mStep[0m  [6/26], [94mLoss[0m : 5.86398
[1mStep[0m  [8/26], [94mLoss[0m : 5.94109
[1mStep[0m  [10/26], [94mLoss[0m : 5.96112
[1mStep[0m  [12/26], [94mLoss[0m : 6.26126
[1mStep[0m  [14/26], [94mLoss[0m : 5.90631
[1mStep[0m  [16/26], [94mLoss[0m : 5.92702
[1mStep[0m  [18/26], [94mLoss[0m : 5.72037
[1mStep[0m  [20/26], [94mLoss[0m : 5.78444
[1mStep[0m  [22/26], [94mLoss[0m : 5.87301
[1mStep[0m  [24/26], [94mLoss[0m : 5.83312

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.964, [92mTest[0m: 5.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.07226
[1mStep[0m  [2/26], [94mLoss[0m : 5.53190
[1mStep[0m  [4/26], [94mLoss[0m : 5.66507
[1mStep[0m  [6/26], [94mLoss[0m : 5.69987
[1mStep[0m  [8/26], [94mLoss[0m : 5.84072
[1mStep[0m  [10/26], [94mLoss[0m : 5.55019
[1mStep[0m  [12/26], [94mLoss[0m : 5.54318
[1mStep[0m  [14/26], [94mLoss[0m : 5.38802
[1mStep[0m  [16/26], [94mLoss[0m : 5.45860
[1mStep[0m  [18/26], [94mLoss[0m : 5.39382
[1mStep[0m  [20/26], [94mLoss[0m : 5.42551
[1mStep[0m  [22/26], [94mLoss[0m : 5.49496
[1mStep[0m  [24/26], [94mLoss[0m : 5.67688

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.579, [92mTest[0m: 5.050, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.29880
[1mStep[0m  [2/26], [94mLoss[0m : 5.45206
[1mStep[0m  [4/26], [94mLoss[0m : 5.06292
[1mStep[0m  [6/26], [94mLoss[0m : 5.52833
[1mStep[0m  [8/26], [94mLoss[0m : 5.21720
[1mStep[0m  [10/26], [94mLoss[0m : 5.03934
[1mStep[0m  [12/26], [94mLoss[0m : 5.08793
[1mStep[0m  [14/26], [94mLoss[0m : 5.13648
[1mStep[0m  [16/26], [94mLoss[0m : 5.46578
[1mStep[0m  [18/26], [94mLoss[0m : 5.11246
[1mStep[0m  [20/26], [94mLoss[0m : 4.89830
[1mStep[0m  [22/26], [94mLoss[0m : 5.24962
[1mStep[0m  [24/26], [94mLoss[0m : 5.17843

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.220, [92mTest[0m: 4.646, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.17905
[1mStep[0m  [2/26], [94mLoss[0m : 5.00904
[1mStep[0m  [4/26], [94mLoss[0m : 4.87863
[1mStep[0m  [6/26], [94mLoss[0m : 4.81501
[1mStep[0m  [8/26], [94mLoss[0m : 4.96275
[1mStep[0m  [10/26], [94mLoss[0m : 4.59588
[1mStep[0m  [12/26], [94mLoss[0m : 5.08863
[1mStep[0m  [14/26], [94mLoss[0m : 4.73192
[1mStep[0m  [16/26], [94mLoss[0m : 4.65738
[1mStep[0m  [18/26], [94mLoss[0m : 4.79513
[1mStep[0m  [20/26], [94mLoss[0m : 4.49874
[1mStep[0m  [22/26], [94mLoss[0m : 4.42936
[1mStep[0m  [24/26], [94mLoss[0m : 4.57812

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.825, [92mTest[0m: 4.275, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.79243
[1mStep[0m  [2/26], [94mLoss[0m : 4.73748
[1mStep[0m  [4/26], [94mLoss[0m : 4.56450
[1mStep[0m  [6/26], [94mLoss[0m : 4.37129
[1mStep[0m  [8/26], [94mLoss[0m : 4.45479
[1mStep[0m  [10/26], [94mLoss[0m : 4.56960
[1mStep[0m  [12/26], [94mLoss[0m : 4.62877
[1mStep[0m  [14/26], [94mLoss[0m : 4.40090
[1mStep[0m  [16/26], [94mLoss[0m : 4.28775
[1mStep[0m  [18/26], [94mLoss[0m : 4.09037
[1mStep[0m  [20/26], [94mLoss[0m : 4.47438
[1mStep[0m  [22/26], [94mLoss[0m : 4.38370
[1mStep[0m  [24/26], [94mLoss[0m : 4.10916

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.429, [92mTest[0m: 3.910, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.53474
[1mStep[0m  [2/26], [94mLoss[0m : 4.11354
[1mStep[0m  [4/26], [94mLoss[0m : 4.00087
[1mStep[0m  [6/26], [94mLoss[0m : 4.07397
[1mStep[0m  [8/26], [94mLoss[0m : 4.21730
[1mStep[0m  [10/26], [94mLoss[0m : 3.99533
[1mStep[0m  [12/26], [94mLoss[0m : 4.05491
[1mStep[0m  [14/26], [94mLoss[0m : 4.02979
[1mStep[0m  [16/26], [94mLoss[0m : 4.21163
[1mStep[0m  [18/26], [94mLoss[0m : 4.03370
[1mStep[0m  [20/26], [94mLoss[0m : 3.87017
[1mStep[0m  [22/26], [94mLoss[0m : 3.88247
[1mStep[0m  [24/26], [94mLoss[0m : 3.85958

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.025, [92mTest[0m: 3.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.01439
[1mStep[0m  [2/26], [94mLoss[0m : 4.00680
[1mStep[0m  [4/26], [94mLoss[0m : 3.65155
[1mStep[0m  [6/26], [94mLoss[0m : 4.01281
[1mStep[0m  [8/26], [94mLoss[0m : 3.70522
[1mStep[0m  [10/26], [94mLoss[0m : 3.76490
[1mStep[0m  [12/26], [94mLoss[0m : 3.59045
[1mStep[0m  [14/26], [94mLoss[0m : 3.62815
[1mStep[0m  [16/26], [94mLoss[0m : 3.93409
[1mStep[0m  [18/26], [94mLoss[0m : 3.30251
[1mStep[0m  [20/26], [94mLoss[0m : 3.76466
[1mStep[0m  [22/26], [94mLoss[0m : 3.20630
[1mStep[0m  [24/26], [94mLoss[0m : 3.43444

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.664, [92mTest[0m: 3.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.27701
[1mStep[0m  [2/26], [94mLoss[0m : 3.45212
[1mStep[0m  [4/26], [94mLoss[0m : 3.55401
[1mStep[0m  [6/26], [94mLoss[0m : 3.38877
[1mStep[0m  [8/26], [94mLoss[0m : 3.53800
[1mStep[0m  [10/26], [94mLoss[0m : 3.45143
[1mStep[0m  [12/26], [94mLoss[0m : 3.34720
[1mStep[0m  [14/26], [94mLoss[0m : 3.28677
[1mStep[0m  [16/26], [94mLoss[0m : 3.16874
[1mStep[0m  [18/26], [94mLoss[0m : 3.33128
[1mStep[0m  [20/26], [94mLoss[0m : 3.04559
[1mStep[0m  [22/26], [94mLoss[0m : 3.31641
[1mStep[0m  [24/26], [94mLoss[0m : 3.40515

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.347, [92mTest[0m: 3.040, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.12440
[1mStep[0m  [2/26], [94mLoss[0m : 3.19896
[1mStep[0m  [4/26], [94mLoss[0m : 3.21290
[1mStep[0m  [6/26], [94mLoss[0m : 3.08500
[1mStep[0m  [8/26], [94mLoss[0m : 3.09373
[1mStep[0m  [10/26], [94mLoss[0m : 3.08559
[1mStep[0m  [12/26], [94mLoss[0m : 3.05348
[1mStep[0m  [14/26], [94mLoss[0m : 3.05444
[1mStep[0m  [16/26], [94mLoss[0m : 2.96725
[1mStep[0m  [18/26], [94mLoss[0m : 3.00734
[1mStep[0m  [20/26], [94mLoss[0m : 3.00122
[1mStep[0m  [22/26], [94mLoss[0m : 2.99016
[1mStep[0m  [24/26], [94mLoss[0m : 3.17484

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.078, [92mTest[0m: 2.820, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68008
[1mStep[0m  [2/26], [94mLoss[0m : 3.15939
[1mStep[0m  [4/26], [94mLoss[0m : 2.87191
[1mStep[0m  [6/26], [94mLoss[0m : 2.95026
[1mStep[0m  [8/26], [94mLoss[0m : 2.97997
[1mStep[0m  [10/26], [94mLoss[0m : 2.91050
[1mStep[0m  [12/26], [94mLoss[0m : 2.95307
[1mStep[0m  [14/26], [94mLoss[0m : 2.98874
[1mStep[0m  [16/26], [94mLoss[0m : 2.96936
[1mStep[0m  [18/26], [94mLoss[0m : 2.83447
[1mStep[0m  [20/26], [94mLoss[0m : 2.73330
[1mStep[0m  [22/26], [94mLoss[0m : 2.80536
[1mStep[0m  [24/26], [94mLoss[0m : 2.77324

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.894, [92mTest[0m: 2.626, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82845
[1mStep[0m  [2/26], [94mLoss[0m : 2.75079
[1mStep[0m  [4/26], [94mLoss[0m : 2.94688
[1mStep[0m  [6/26], [94mLoss[0m : 2.75721
[1mStep[0m  [8/26], [94mLoss[0m : 2.86290
[1mStep[0m  [10/26], [94mLoss[0m : 2.94728
[1mStep[0m  [12/26], [94mLoss[0m : 2.70893
[1mStep[0m  [14/26], [94mLoss[0m : 2.95779
[1mStep[0m  [16/26], [94mLoss[0m : 2.72438
[1mStep[0m  [18/26], [94mLoss[0m : 2.80918
[1mStep[0m  [20/26], [94mLoss[0m : 2.84307
[1mStep[0m  [22/26], [94mLoss[0m : 2.88610
[1mStep[0m  [24/26], [94mLoss[0m : 2.54446

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.774, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49981
[1mStep[0m  [2/26], [94mLoss[0m : 2.58143
[1mStep[0m  [4/26], [94mLoss[0m : 2.81793
[1mStep[0m  [6/26], [94mLoss[0m : 2.71047
[1mStep[0m  [8/26], [94mLoss[0m : 2.73463
[1mStep[0m  [10/26], [94mLoss[0m : 2.62280
[1mStep[0m  [12/26], [94mLoss[0m : 2.62595
[1mStep[0m  [14/26], [94mLoss[0m : 2.69611
[1mStep[0m  [16/26], [94mLoss[0m : 2.67380
[1mStep[0m  [18/26], [94mLoss[0m : 2.90659
[1mStep[0m  [20/26], [94mLoss[0m : 2.69662
[1mStep[0m  [22/26], [94mLoss[0m : 2.70202
[1mStep[0m  [24/26], [94mLoss[0m : 2.69351

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.73399
[1mStep[0m  [2/26], [94mLoss[0m : 2.87870
[1mStep[0m  [4/26], [94mLoss[0m : 2.82284
[1mStep[0m  [6/26], [94mLoss[0m : 2.91745
[1mStep[0m  [8/26], [94mLoss[0m : 2.70072
[1mStep[0m  [10/26], [94mLoss[0m : 2.79365
[1mStep[0m  [12/26], [94mLoss[0m : 2.62237
[1mStep[0m  [14/26], [94mLoss[0m : 2.64237
[1mStep[0m  [16/26], [94mLoss[0m : 2.73647
[1mStep[0m  [18/26], [94mLoss[0m : 2.71329
[1mStep[0m  [20/26], [94mLoss[0m : 2.86366
[1mStep[0m  [22/26], [94mLoss[0m : 2.52898
[1mStep[0m  [24/26], [94mLoss[0m : 2.81009

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58253
[1mStep[0m  [2/26], [94mLoss[0m : 2.58264
[1mStep[0m  [4/26], [94mLoss[0m : 2.58890
[1mStep[0m  [6/26], [94mLoss[0m : 2.74474
[1mStep[0m  [8/26], [94mLoss[0m : 2.57747
[1mStep[0m  [10/26], [94mLoss[0m : 2.58348
[1mStep[0m  [12/26], [94mLoss[0m : 2.71490
[1mStep[0m  [14/26], [94mLoss[0m : 2.79914
[1mStep[0m  [16/26], [94mLoss[0m : 2.59493
[1mStep[0m  [18/26], [94mLoss[0m : 2.59898
[1mStep[0m  [20/26], [94mLoss[0m : 2.71411
[1mStep[0m  [22/26], [94mLoss[0m : 2.52213
[1mStep[0m  [24/26], [94mLoss[0m : 2.67511

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.448, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58514
[1mStep[0m  [2/26], [94mLoss[0m : 2.46375
[1mStep[0m  [4/26], [94mLoss[0m : 2.74807
[1mStep[0m  [6/26], [94mLoss[0m : 2.76542
[1mStep[0m  [8/26], [94mLoss[0m : 2.55264
[1mStep[0m  [10/26], [94mLoss[0m : 2.61501
[1mStep[0m  [12/26], [94mLoss[0m : 2.56036
[1mStep[0m  [14/26], [94mLoss[0m : 2.60796
[1mStep[0m  [16/26], [94mLoss[0m : 2.62247
[1mStep[0m  [18/26], [94mLoss[0m : 2.64456
[1mStep[0m  [20/26], [94mLoss[0m : 2.78051
[1mStep[0m  [22/26], [94mLoss[0m : 2.50023
[1mStep[0m  [24/26], [94mLoss[0m : 2.50316

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.439, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82802
[1mStep[0m  [2/26], [94mLoss[0m : 2.60808
[1mStep[0m  [4/26], [94mLoss[0m : 2.78438
[1mStep[0m  [6/26], [94mLoss[0m : 2.62014
[1mStep[0m  [8/26], [94mLoss[0m : 2.56235
[1mStep[0m  [10/26], [94mLoss[0m : 2.63344
[1mStep[0m  [12/26], [94mLoss[0m : 2.73269
[1mStep[0m  [14/26], [94mLoss[0m : 2.63074
[1mStep[0m  [16/26], [94mLoss[0m : 2.61017
[1mStep[0m  [18/26], [94mLoss[0m : 2.73909
[1mStep[0m  [20/26], [94mLoss[0m : 2.55368
[1mStep[0m  [22/26], [94mLoss[0m : 2.73039
[1mStep[0m  [24/26], [94mLoss[0m : 2.54341

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59456
[1mStep[0m  [2/26], [94mLoss[0m : 2.62603
[1mStep[0m  [4/26], [94mLoss[0m : 2.81333
[1mStep[0m  [6/26], [94mLoss[0m : 2.76140
[1mStep[0m  [8/26], [94mLoss[0m : 2.69754
[1mStep[0m  [10/26], [94mLoss[0m : 2.68840
[1mStep[0m  [12/26], [94mLoss[0m : 2.55289
[1mStep[0m  [14/26], [94mLoss[0m : 2.58482
[1mStep[0m  [16/26], [94mLoss[0m : 2.65073
[1mStep[0m  [18/26], [94mLoss[0m : 2.72754
[1mStep[0m  [20/26], [94mLoss[0m : 2.76991
[1mStep[0m  [22/26], [94mLoss[0m : 2.40427
[1mStep[0m  [24/26], [94mLoss[0m : 2.76854

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.431, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58101
[1mStep[0m  [2/26], [94mLoss[0m : 2.65452
[1mStep[0m  [4/26], [94mLoss[0m : 2.66483
[1mStep[0m  [6/26], [94mLoss[0m : 2.58104
[1mStep[0m  [8/26], [94mLoss[0m : 2.55643
[1mStep[0m  [10/26], [94mLoss[0m : 2.71612
[1mStep[0m  [12/26], [94mLoss[0m : 2.55093
[1mStep[0m  [14/26], [94mLoss[0m : 2.78310
[1mStep[0m  [16/26], [94mLoss[0m : 2.68456
[1mStep[0m  [18/26], [94mLoss[0m : 2.52878
[1mStep[0m  [20/26], [94mLoss[0m : 2.53033
[1mStep[0m  [22/26], [94mLoss[0m : 2.58376
[1mStep[0m  [24/26], [94mLoss[0m : 2.61566

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.411, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56000
[1mStep[0m  [2/26], [94mLoss[0m : 2.70048
[1mStep[0m  [4/26], [94mLoss[0m : 2.53930
[1mStep[0m  [6/26], [94mLoss[0m : 2.49297
[1mStep[0m  [8/26], [94mLoss[0m : 2.72589
[1mStep[0m  [10/26], [94mLoss[0m : 2.65500
[1mStep[0m  [12/26], [94mLoss[0m : 2.62871
[1mStep[0m  [14/26], [94mLoss[0m : 2.58628
[1mStep[0m  [16/26], [94mLoss[0m : 2.56046
[1mStep[0m  [18/26], [94mLoss[0m : 2.61213
[1mStep[0m  [20/26], [94mLoss[0m : 2.66444
[1mStep[0m  [22/26], [94mLoss[0m : 2.67596
[1mStep[0m  [24/26], [94mLoss[0m : 2.68833

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.407, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64479
[1mStep[0m  [2/26], [94mLoss[0m : 2.60878
[1mStep[0m  [4/26], [94mLoss[0m : 2.64665
[1mStep[0m  [6/26], [94mLoss[0m : 2.67412
[1mStep[0m  [8/26], [94mLoss[0m : 2.54022
[1mStep[0m  [10/26], [94mLoss[0m : 2.67506
[1mStep[0m  [12/26], [94mLoss[0m : 2.51216
[1mStep[0m  [14/26], [94mLoss[0m : 2.48156
[1mStep[0m  [16/26], [94mLoss[0m : 2.47491
[1mStep[0m  [18/26], [94mLoss[0m : 2.59064
[1mStep[0m  [20/26], [94mLoss[0m : 2.64330
[1mStep[0m  [22/26], [94mLoss[0m : 2.73935
[1mStep[0m  [24/26], [94mLoss[0m : 2.54441

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.421, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.423
====================================

Phase 1 - Evaluation MAE:  2.4226792592268724
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.68344
[1mStep[0m  [2/26], [94mLoss[0m : 2.59735
[1mStep[0m  [4/26], [94mLoss[0m : 2.57432
[1mStep[0m  [6/26], [94mLoss[0m : 2.58097
[1mStep[0m  [8/26], [94mLoss[0m : 2.45011
[1mStep[0m  [10/26], [94mLoss[0m : 2.76750
[1mStep[0m  [12/26], [94mLoss[0m : 2.58403
[1mStep[0m  [14/26], [94mLoss[0m : 2.76560
[1mStep[0m  [16/26], [94mLoss[0m : 2.65236
[1mStep[0m  [18/26], [94mLoss[0m : 2.54738
[1mStep[0m  [20/26], [94mLoss[0m : 2.60896
[1mStep[0m  [22/26], [94mLoss[0m : 2.58912
[1mStep[0m  [24/26], [94mLoss[0m : 2.54356

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72099
[1mStep[0m  [2/26], [94mLoss[0m : 2.52301
[1mStep[0m  [4/26], [94mLoss[0m : 2.48553
[1mStep[0m  [6/26], [94mLoss[0m : 2.63963
[1mStep[0m  [8/26], [94mLoss[0m : 2.66675
[1mStep[0m  [10/26], [94mLoss[0m : 2.69980
[1mStep[0m  [12/26], [94mLoss[0m : 2.77575
[1mStep[0m  [14/26], [94mLoss[0m : 2.69834
[1mStep[0m  [16/26], [94mLoss[0m : 2.56258
[1mStep[0m  [18/26], [94mLoss[0m : 2.61071
[1mStep[0m  [20/26], [94mLoss[0m : 2.68655
[1mStep[0m  [22/26], [94mLoss[0m : 2.60657
[1mStep[0m  [24/26], [94mLoss[0m : 2.70748

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53425
[1mStep[0m  [2/26], [94mLoss[0m : 2.45980
[1mStep[0m  [4/26], [94mLoss[0m : 2.63929
[1mStep[0m  [6/26], [94mLoss[0m : 2.64056
[1mStep[0m  [8/26], [94mLoss[0m : 2.56128
[1mStep[0m  [10/26], [94mLoss[0m : 2.50915
[1mStep[0m  [12/26], [94mLoss[0m : 2.61108
[1mStep[0m  [14/26], [94mLoss[0m : 2.51986
[1mStep[0m  [16/26], [94mLoss[0m : 2.71387
[1mStep[0m  [18/26], [94mLoss[0m : 2.57363
[1mStep[0m  [20/26], [94mLoss[0m : 2.63355
[1mStep[0m  [22/26], [94mLoss[0m : 2.45869
[1mStep[0m  [24/26], [94mLoss[0m : 2.75997

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53003
[1mStep[0m  [2/26], [94mLoss[0m : 2.62928
[1mStep[0m  [4/26], [94mLoss[0m : 2.49241
[1mStep[0m  [6/26], [94mLoss[0m : 2.50781
[1mStep[0m  [8/26], [94mLoss[0m : 2.53228
[1mStep[0m  [10/26], [94mLoss[0m : 2.50931
[1mStep[0m  [12/26], [94mLoss[0m : 2.49117
[1mStep[0m  [14/26], [94mLoss[0m : 2.74016
[1mStep[0m  [16/26], [94mLoss[0m : 2.54499
[1mStep[0m  [18/26], [94mLoss[0m : 2.57371
[1mStep[0m  [20/26], [94mLoss[0m : 2.63142
[1mStep[0m  [22/26], [94mLoss[0m : 2.65692
[1mStep[0m  [24/26], [94mLoss[0m : 2.55428

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60682
[1mStep[0m  [2/26], [94mLoss[0m : 2.62517
[1mStep[0m  [4/26], [94mLoss[0m : 2.45463
[1mStep[0m  [6/26], [94mLoss[0m : 2.47548
[1mStep[0m  [8/26], [94mLoss[0m : 2.59564
[1mStep[0m  [10/26], [94mLoss[0m : 2.44220
[1mStep[0m  [12/26], [94mLoss[0m : 2.60838
[1mStep[0m  [14/26], [94mLoss[0m : 2.50771
[1mStep[0m  [16/26], [94mLoss[0m : 2.57392
[1mStep[0m  [18/26], [94mLoss[0m : 2.63826
[1mStep[0m  [20/26], [94mLoss[0m : 2.45415
[1mStep[0m  [22/26], [94mLoss[0m : 2.54838
[1mStep[0m  [24/26], [94mLoss[0m : 2.58034

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38564
[1mStep[0m  [2/26], [94mLoss[0m : 2.60468
[1mStep[0m  [4/26], [94mLoss[0m : 2.52007
[1mStep[0m  [6/26], [94mLoss[0m : 2.59316
[1mStep[0m  [8/26], [94mLoss[0m : 2.49879
[1mStep[0m  [10/26], [94mLoss[0m : 2.50960
[1mStep[0m  [12/26], [94mLoss[0m : 2.48351
[1mStep[0m  [14/26], [94mLoss[0m : 2.59427
[1mStep[0m  [16/26], [94mLoss[0m : 2.40997
[1mStep[0m  [18/26], [94mLoss[0m : 2.39419
[1mStep[0m  [20/26], [94mLoss[0m : 2.48269
[1mStep[0m  [22/26], [94mLoss[0m : 2.36412
[1mStep[0m  [24/26], [94mLoss[0m : 2.53672

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53559
[1mStep[0m  [2/26], [94mLoss[0m : 2.45625
[1mStep[0m  [4/26], [94mLoss[0m : 2.41195
[1mStep[0m  [6/26], [94mLoss[0m : 2.40992
[1mStep[0m  [8/26], [94mLoss[0m : 2.47795
[1mStep[0m  [10/26], [94mLoss[0m : 2.52549
[1mStep[0m  [12/26], [94mLoss[0m : 2.39027
[1mStep[0m  [14/26], [94mLoss[0m : 2.42037
[1mStep[0m  [16/26], [94mLoss[0m : 2.58890
[1mStep[0m  [18/26], [94mLoss[0m : 2.54890
[1mStep[0m  [20/26], [94mLoss[0m : 2.44107
[1mStep[0m  [22/26], [94mLoss[0m : 2.49812
[1mStep[0m  [24/26], [94mLoss[0m : 2.51449

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48050
[1mStep[0m  [2/26], [94mLoss[0m : 2.37998
[1mStep[0m  [4/26], [94mLoss[0m : 2.43599
[1mStep[0m  [6/26], [94mLoss[0m : 2.59022
[1mStep[0m  [8/26], [94mLoss[0m : 2.43854
[1mStep[0m  [10/26], [94mLoss[0m : 2.56877
[1mStep[0m  [12/26], [94mLoss[0m : 2.54059
[1mStep[0m  [14/26], [94mLoss[0m : 2.71482
[1mStep[0m  [16/26], [94mLoss[0m : 2.51292
[1mStep[0m  [18/26], [94mLoss[0m : 2.50168
[1mStep[0m  [20/26], [94mLoss[0m : 2.40087
[1mStep[0m  [22/26], [94mLoss[0m : 2.46069
[1mStep[0m  [24/26], [94mLoss[0m : 2.61265

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36696
[1mStep[0m  [2/26], [94mLoss[0m : 2.46529
[1mStep[0m  [4/26], [94mLoss[0m : 2.55316
[1mStep[0m  [6/26], [94mLoss[0m : 2.44170
[1mStep[0m  [8/26], [94mLoss[0m : 2.51394
[1mStep[0m  [10/26], [94mLoss[0m : 2.50070
[1mStep[0m  [12/26], [94mLoss[0m : 2.60181
[1mStep[0m  [14/26], [94mLoss[0m : 2.34813
[1mStep[0m  [16/26], [94mLoss[0m : 2.51466
[1mStep[0m  [18/26], [94mLoss[0m : 2.54190
[1mStep[0m  [20/26], [94mLoss[0m : 2.49041
[1mStep[0m  [22/26], [94mLoss[0m : 2.26322
[1mStep[0m  [24/26], [94mLoss[0m : 2.47576

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41052
[1mStep[0m  [2/26], [94mLoss[0m : 2.31647
[1mStep[0m  [4/26], [94mLoss[0m : 2.40248
[1mStep[0m  [6/26], [94mLoss[0m : 2.55252
[1mStep[0m  [8/26], [94mLoss[0m : 2.28085
[1mStep[0m  [10/26], [94mLoss[0m : 2.50713
[1mStep[0m  [12/26], [94mLoss[0m : 2.51800
[1mStep[0m  [14/26], [94mLoss[0m : 2.39868
[1mStep[0m  [16/26], [94mLoss[0m : 2.49560
[1mStep[0m  [18/26], [94mLoss[0m : 2.32938
[1mStep[0m  [20/26], [94mLoss[0m : 2.41811
[1mStep[0m  [22/26], [94mLoss[0m : 2.37132
[1mStep[0m  [24/26], [94mLoss[0m : 2.52385

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45374
[1mStep[0m  [2/26], [94mLoss[0m : 2.32352
[1mStep[0m  [4/26], [94mLoss[0m : 2.53420
[1mStep[0m  [6/26], [94mLoss[0m : 2.52644
[1mStep[0m  [8/26], [94mLoss[0m : 2.35282
[1mStep[0m  [10/26], [94mLoss[0m : 2.40568
[1mStep[0m  [12/26], [94mLoss[0m : 2.39352
[1mStep[0m  [14/26], [94mLoss[0m : 2.42498
[1mStep[0m  [16/26], [94mLoss[0m : 2.42018
[1mStep[0m  [18/26], [94mLoss[0m : 2.31116
[1mStep[0m  [20/26], [94mLoss[0m : 2.33588
[1mStep[0m  [22/26], [94mLoss[0m : 2.44730
[1mStep[0m  [24/26], [94mLoss[0m : 2.40632

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32507
[1mStep[0m  [2/26], [94mLoss[0m : 2.48942
[1mStep[0m  [4/26], [94mLoss[0m : 2.36988
[1mStep[0m  [6/26], [94mLoss[0m : 2.34527
[1mStep[0m  [8/26], [94mLoss[0m : 2.42757
[1mStep[0m  [10/26], [94mLoss[0m : 2.44079
[1mStep[0m  [12/26], [94mLoss[0m : 2.48342
[1mStep[0m  [14/26], [94mLoss[0m : 2.38021
[1mStep[0m  [16/26], [94mLoss[0m : 2.39956
[1mStep[0m  [18/26], [94mLoss[0m : 2.45486
[1mStep[0m  [20/26], [94mLoss[0m : 2.51770
[1mStep[0m  [22/26], [94mLoss[0m : 2.29024
[1mStep[0m  [24/26], [94mLoss[0m : 2.25099

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42804
[1mStep[0m  [2/26], [94mLoss[0m : 2.28579
[1mStep[0m  [4/26], [94mLoss[0m : 2.42945
[1mStep[0m  [6/26], [94mLoss[0m : 2.48691
[1mStep[0m  [8/26], [94mLoss[0m : 2.53492
[1mStep[0m  [10/26], [94mLoss[0m : 2.26309
[1mStep[0m  [12/26], [94mLoss[0m : 2.55007
[1mStep[0m  [14/26], [94mLoss[0m : 2.37211
[1mStep[0m  [16/26], [94mLoss[0m : 2.30755
[1mStep[0m  [18/26], [94mLoss[0m : 2.44331
[1mStep[0m  [20/26], [94mLoss[0m : 2.34452
[1mStep[0m  [22/26], [94mLoss[0m : 2.37569
[1mStep[0m  [24/26], [94mLoss[0m : 2.30145

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28956
[1mStep[0m  [2/26], [94mLoss[0m : 2.44900
[1mStep[0m  [4/26], [94mLoss[0m : 2.42269
[1mStep[0m  [6/26], [94mLoss[0m : 2.28570
[1mStep[0m  [8/26], [94mLoss[0m : 2.30992
[1mStep[0m  [10/26], [94mLoss[0m : 2.34796
[1mStep[0m  [12/26], [94mLoss[0m : 2.33445
[1mStep[0m  [14/26], [94mLoss[0m : 2.26711
[1mStep[0m  [16/26], [94mLoss[0m : 2.43476
[1mStep[0m  [18/26], [94mLoss[0m : 2.33396
[1mStep[0m  [20/26], [94mLoss[0m : 2.38367
[1mStep[0m  [22/26], [94mLoss[0m : 2.25203
[1mStep[0m  [24/26], [94mLoss[0m : 2.41645

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30395
[1mStep[0m  [2/26], [94mLoss[0m : 2.35368
[1mStep[0m  [4/26], [94mLoss[0m : 2.32496
[1mStep[0m  [6/26], [94mLoss[0m : 2.42762
[1mStep[0m  [8/26], [94mLoss[0m : 2.27180
[1mStep[0m  [10/26], [94mLoss[0m : 2.21225
[1mStep[0m  [12/26], [94mLoss[0m : 2.23994
[1mStep[0m  [14/26], [94mLoss[0m : 2.14584
[1mStep[0m  [16/26], [94mLoss[0m : 2.29655
[1mStep[0m  [18/26], [94mLoss[0m : 2.49616
[1mStep[0m  [20/26], [94mLoss[0m : 2.20443
[1mStep[0m  [22/26], [94mLoss[0m : 2.37724
[1mStep[0m  [24/26], [94mLoss[0m : 2.25640

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14877
[1mStep[0m  [2/26], [94mLoss[0m : 2.30225
[1mStep[0m  [4/26], [94mLoss[0m : 2.29987
[1mStep[0m  [6/26], [94mLoss[0m : 2.25623
[1mStep[0m  [8/26], [94mLoss[0m : 2.33095
[1mStep[0m  [10/26], [94mLoss[0m : 2.29012
[1mStep[0m  [12/26], [94mLoss[0m : 2.36968
[1mStep[0m  [14/26], [94mLoss[0m : 2.33666
[1mStep[0m  [16/26], [94mLoss[0m : 2.14763
[1mStep[0m  [18/26], [94mLoss[0m : 2.26455
[1mStep[0m  [20/26], [94mLoss[0m : 2.32759
[1mStep[0m  [22/26], [94mLoss[0m : 2.31087
[1mStep[0m  [24/26], [94mLoss[0m : 2.23311

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33739
[1mStep[0m  [2/26], [94mLoss[0m : 2.19794
[1mStep[0m  [4/26], [94mLoss[0m : 2.23300
[1mStep[0m  [6/26], [94mLoss[0m : 2.05322
[1mStep[0m  [8/26], [94mLoss[0m : 2.29647
[1mStep[0m  [10/26], [94mLoss[0m : 2.45458
[1mStep[0m  [12/26], [94mLoss[0m : 2.25401
[1mStep[0m  [14/26], [94mLoss[0m : 2.28474
[1mStep[0m  [16/26], [94mLoss[0m : 2.28945
[1mStep[0m  [18/26], [94mLoss[0m : 2.24357
[1mStep[0m  [20/26], [94mLoss[0m : 2.30086
[1mStep[0m  [22/26], [94mLoss[0m : 2.32833
[1mStep[0m  [24/26], [94mLoss[0m : 2.32506

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17417
[1mStep[0m  [2/26], [94mLoss[0m : 2.22191
[1mStep[0m  [4/26], [94mLoss[0m : 2.26943
[1mStep[0m  [6/26], [94mLoss[0m : 2.21168
[1mStep[0m  [8/26], [94mLoss[0m : 2.19263
[1mStep[0m  [10/26], [94mLoss[0m : 2.13096
[1mStep[0m  [12/26], [94mLoss[0m : 2.20986
[1mStep[0m  [14/26], [94mLoss[0m : 2.21463
[1mStep[0m  [16/26], [94mLoss[0m : 2.18242
[1mStep[0m  [18/26], [94mLoss[0m : 2.26285
[1mStep[0m  [20/26], [94mLoss[0m : 2.27170
[1mStep[0m  [22/26], [94mLoss[0m : 2.33005
[1mStep[0m  [24/26], [94mLoss[0m : 2.32408

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32162
[1mStep[0m  [2/26], [94mLoss[0m : 2.15708
[1mStep[0m  [4/26], [94mLoss[0m : 2.41805
[1mStep[0m  [6/26], [94mLoss[0m : 2.12389
[1mStep[0m  [8/26], [94mLoss[0m : 2.32560
[1mStep[0m  [10/26], [94mLoss[0m : 2.17142
[1mStep[0m  [12/26], [94mLoss[0m : 2.15590
[1mStep[0m  [14/26], [94mLoss[0m : 2.22119
[1mStep[0m  [16/26], [94mLoss[0m : 2.32849
[1mStep[0m  [18/26], [94mLoss[0m : 2.20285
[1mStep[0m  [20/26], [94mLoss[0m : 2.22321
[1mStep[0m  [22/26], [94mLoss[0m : 2.21533
[1mStep[0m  [24/26], [94mLoss[0m : 2.24880

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19711
[1mStep[0m  [2/26], [94mLoss[0m : 2.03606
[1mStep[0m  [4/26], [94mLoss[0m : 2.31711
[1mStep[0m  [6/26], [94mLoss[0m : 2.13438
[1mStep[0m  [8/26], [94mLoss[0m : 2.15770
[1mStep[0m  [10/26], [94mLoss[0m : 2.20073
[1mStep[0m  [12/26], [94mLoss[0m : 2.22001
[1mStep[0m  [14/26], [94mLoss[0m : 2.24874
[1mStep[0m  [16/26], [94mLoss[0m : 2.19637
[1mStep[0m  [18/26], [94mLoss[0m : 2.13212
[1mStep[0m  [20/26], [94mLoss[0m : 2.40713
[1mStep[0m  [22/26], [94mLoss[0m : 2.10251
[1mStep[0m  [24/26], [94mLoss[0m : 2.20947

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.380, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26101
[1mStep[0m  [2/26], [94mLoss[0m : 2.16821
[1mStep[0m  [4/26], [94mLoss[0m : 2.27899
[1mStep[0m  [6/26], [94mLoss[0m : 2.11228
[1mStep[0m  [8/26], [94mLoss[0m : 2.31645
[1mStep[0m  [10/26], [94mLoss[0m : 2.11284
[1mStep[0m  [12/26], [94mLoss[0m : 2.03827
[1mStep[0m  [14/26], [94mLoss[0m : 2.19528
[1mStep[0m  [16/26], [94mLoss[0m : 2.14091
[1mStep[0m  [18/26], [94mLoss[0m : 2.06339
[1mStep[0m  [20/26], [94mLoss[0m : 2.14623
[1mStep[0m  [22/26], [94mLoss[0m : 2.16579
[1mStep[0m  [24/26], [94mLoss[0m : 2.25055

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.390, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.18509
[1mStep[0m  [2/26], [94mLoss[0m : 2.16773
[1mStep[0m  [4/26], [94mLoss[0m : 2.16534
[1mStep[0m  [6/26], [94mLoss[0m : 2.09142
[1mStep[0m  [8/26], [94mLoss[0m : 2.14345
[1mStep[0m  [10/26], [94mLoss[0m : 2.12518
[1mStep[0m  [12/26], [94mLoss[0m : 2.19905
[1mStep[0m  [14/26], [94mLoss[0m : 1.95845
[1mStep[0m  [16/26], [94mLoss[0m : 1.95005
[1mStep[0m  [18/26], [94mLoss[0m : 2.16585
[1mStep[0m  [20/26], [94mLoss[0m : 2.14949
[1mStep[0m  [22/26], [94mLoss[0m : 2.01842
[1mStep[0m  [24/26], [94mLoss[0m : 2.10432

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23339
[1mStep[0m  [2/26], [94mLoss[0m : 2.26685
[1mStep[0m  [4/26], [94mLoss[0m : 2.03881
[1mStep[0m  [6/26], [94mLoss[0m : 2.02057
[1mStep[0m  [8/26], [94mLoss[0m : 2.11446
[1mStep[0m  [10/26], [94mLoss[0m : 2.22242
[1mStep[0m  [12/26], [94mLoss[0m : 2.12003
[1mStep[0m  [14/26], [94mLoss[0m : 2.15163
[1mStep[0m  [16/26], [94mLoss[0m : 2.20259
[1mStep[0m  [18/26], [94mLoss[0m : 2.18099
[1mStep[0m  [20/26], [94mLoss[0m : 2.17778
[1mStep[0m  [22/26], [94mLoss[0m : 2.20423
[1mStep[0m  [24/26], [94mLoss[0m : 2.21936

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97280
[1mStep[0m  [2/26], [94mLoss[0m : 2.06891
[1mStep[0m  [4/26], [94mLoss[0m : 2.03021
[1mStep[0m  [6/26], [94mLoss[0m : 2.24792
[1mStep[0m  [8/26], [94mLoss[0m : 2.08879
[1mStep[0m  [10/26], [94mLoss[0m : 2.11792
[1mStep[0m  [12/26], [94mLoss[0m : 2.10963
[1mStep[0m  [14/26], [94mLoss[0m : 2.02810
[1mStep[0m  [16/26], [94mLoss[0m : 2.20200
[1mStep[0m  [18/26], [94mLoss[0m : 2.15322
[1mStep[0m  [20/26], [94mLoss[0m : 1.86744
[1mStep[0m  [22/26], [94mLoss[0m : 2.13051
[1mStep[0m  [24/26], [94mLoss[0m : 2.05047

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.376, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.87166
[1mStep[0m  [2/26], [94mLoss[0m : 2.02326
[1mStep[0m  [4/26], [94mLoss[0m : 2.08784
[1mStep[0m  [6/26], [94mLoss[0m : 2.01329
[1mStep[0m  [8/26], [94mLoss[0m : 2.11370
[1mStep[0m  [10/26], [94mLoss[0m : 2.12235
[1mStep[0m  [12/26], [94mLoss[0m : 1.97563
[1mStep[0m  [14/26], [94mLoss[0m : 2.15180
[1mStep[0m  [16/26], [94mLoss[0m : 2.08233
[1mStep[0m  [18/26], [94mLoss[0m : 2.11308
[1mStep[0m  [20/26], [94mLoss[0m : 2.22582
[1mStep[0m  [22/26], [94mLoss[0m : 2.25284
[1mStep[0m  [24/26], [94mLoss[0m : 2.19442

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.067, [92mTest[0m: 2.405, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03491
[1mStep[0m  [2/26], [94mLoss[0m : 2.13044
[1mStep[0m  [4/26], [94mLoss[0m : 2.01407
[1mStep[0m  [6/26], [94mLoss[0m : 2.10214
[1mStep[0m  [8/26], [94mLoss[0m : 1.96027
[1mStep[0m  [10/26], [94mLoss[0m : 1.83741
[1mStep[0m  [12/26], [94mLoss[0m : 2.12532
[1mStep[0m  [14/26], [94mLoss[0m : 2.05637
[1mStep[0m  [16/26], [94mLoss[0m : 2.07811
[1mStep[0m  [18/26], [94mLoss[0m : 1.98012
[1mStep[0m  [20/26], [94mLoss[0m : 2.06308
[1mStep[0m  [22/26], [94mLoss[0m : 2.17589
[1mStep[0m  [24/26], [94mLoss[0m : 1.97879

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.376, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.94331
[1mStep[0m  [2/26], [94mLoss[0m : 2.16898
[1mStep[0m  [4/26], [94mLoss[0m : 1.93885
[1mStep[0m  [6/26], [94mLoss[0m : 2.03404
[1mStep[0m  [8/26], [94mLoss[0m : 2.02497
[1mStep[0m  [10/26], [94mLoss[0m : 1.96998
[1mStep[0m  [12/26], [94mLoss[0m : 2.11150
[1mStep[0m  [14/26], [94mLoss[0m : 2.02563
[1mStep[0m  [16/26], [94mLoss[0m : 2.12626
[1mStep[0m  [18/26], [94mLoss[0m : 2.05365
[1mStep[0m  [20/26], [94mLoss[0m : 2.06688
[1mStep[0m  [22/26], [94mLoss[0m : 1.97127
[1mStep[0m  [24/26], [94mLoss[0m : 1.97422

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17155
[1mStep[0m  [2/26], [94mLoss[0m : 2.00835
[1mStep[0m  [4/26], [94mLoss[0m : 1.93438
[1mStep[0m  [6/26], [94mLoss[0m : 1.97334
[1mStep[0m  [8/26], [94mLoss[0m : 2.02833
[1mStep[0m  [10/26], [94mLoss[0m : 2.00744
[1mStep[0m  [12/26], [94mLoss[0m : 2.05462
[1mStep[0m  [14/26], [94mLoss[0m : 1.99661
[1mStep[0m  [16/26], [94mLoss[0m : 2.09822
[1mStep[0m  [18/26], [94mLoss[0m : 2.21711
[1mStep[0m  [20/26], [94mLoss[0m : 1.98020
[1mStep[0m  [22/26], [94mLoss[0m : 1.94030
[1mStep[0m  [24/26], [94mLoss[0m : 2.08714

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.402, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.99781
[1mStep[0m  [2/26], [94mLoss[0m : 1.86205
[1mStep[0m  [4/26], [94mLoss[0m : 2.13607
[1mStep[0m  [6/26], [94mLoss[0m : 2.01235
[1mStep[0m  [8/26], [94mLoss[0m : 1.94264
[1mStep[0m  [10/26], [94mLoss[0m : 1.88677
[1mStep[0m  [12/26], [94mLoss[0m : 1.87422
[1mStep[0m  [14/26], [94mLoss[0m : 1.96373
[1mStep[0m  [16/26], [94mLoss[0m : 1.96902
[1mStep[0m  [18/26], [94mLoss[0m : 1.98259
[1mStep[0m  [20/26], [94mLoss[0m : 2.02111
[1mStep[0m  [22/26], [94mLoss[0m : 2.12461
[1mStep[0m  [24/26], [94mLoss[0m : 2.04913

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.414, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86316
[1mStep[0m  [2/26], [94mLoss[0m : 1.90763
[1mStep[0m  [4/26], [94mLoss[0m : 1.88250
[1mStep[0m  [6/26], [94mLoss[0m : 2.01889
[1mStep[0m  [8/26], [94mLoss[0m : 1.83313
[1mStep[0m  [10/26], [94mLoss[0m : 1.99804
[1mStep[0m  [12/26], [94mLoss[0m : 1.94571
[1mStep[0m  [14/26], [94mLoss[0m : 2.03878
[1mStep[0m  [16/26], [94mLoss[0m : 1.97087
[1mStep[0m  [18/26], [94mLoss[0m : 2.03330
[1mStep[0m  [20/26], [94mLoss[0m : 1.96720
[1mStep[0m  [22/26], [94mLoss[0m : 1.99137
[1mStep[0m  [24/26], [94mLoss[0m : 2.05684

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.970, [92mTest[0m: 2.409, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.405
====================================

Phase 2 - Evaluation MAE:  2.405369465167706
MAE score P1      2.422679
MAE score P2      2.405369
loss              1.970239
learning_rate     0.007525
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.99731
[1mStep[0m  [2/26], [94mLoss[0m : 10.72763
[1mStep[0m  [4/26], [94mLoss[0m : 10.53740
[1mStep[0m  [6/26], [94mLoss[0m : 10.44144
[1mStep[0m  [8/26], [94mLoss[0m : 10.29558
[1mStep[0m  [10/26], [94mLoss[0m : 10.27566
[1mStep[0m  [12/26], [94mLoss[0m : 9.74566
[1mStep[0m  [14/26], [94mLoss[0m : 9.74998
[1mStep[0m  [16/26], [94mLoss[0m : 9.10208
[1mStep[0m  [18/26], [94mLoss[0m : 9.24437
[1mStep[0m  [20/26], [94mLoss[0m : 8.88899
[1mStep[0m  [22/26], [94mLoss[0m : 8.64703
[1mStep[0m  [24/26], [94mLoss[0m : 8.48146

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.745, [92mTest[0m: 11.109, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.30757
[1mStep[0m  [2/26], [94mLoss[0m : 7.87041
[1mStep[0m  [4/26], [94mLoss[0m : 7.56257
[1mStep[0m  [6/26], [94mLoss[0m : 7.69481
[1mStep[0m  [8/26], [94mLoss[0m : 7.34690
[1mStep[0m  [10/26], [94mLoss[0m : 7.03038
[1mStep[0m  [12/26], [94mLoss[0m : 6.94946
[1mStep[0m  [14/26], [94mLoss[0m : 6.63587
[1mStep[0m  [16/26], [94mLoss[0m : 6.23531
[1mStep[0m  [18/26], [94mLoss[0m : 6.42986
[1mStep[0m  [20/26], [94mLoss[0m : 6.08785
[1mStep[0m  [22/26], [94mLoss[0m : 5.61320
[1mStep[0m  [24/26], [94mLoss[0m : 5.72237

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.805, [92mTest[0m: 8.212, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.74118
[1mStep[0m  [2/26], [94mLoss[0m : 4.98657
[1mStep[0m  [4/26], [94mLoss[0m : 4.91965
[1mStep[0m  [6/26], [94mLoss[0m : 4.63136
[1mStep[0m  [8/26], [94mLoss[0m : 4.36009
[1mStep[0m  [10/26], [94mLoss[0m : 4.12252
[1mStep[0m  [12/26], [94mLoss[0m : 4.47365
[1mStep[0m  [14/26], [94mLoss[0m : 4.20555
[1mStep[0m  [16/26], [94mLoss[0m : 3.95568
[1mStep[0m  [18/26], [94mLoss[0m : 3.95810
[1mStep[0m  [20/26], [94mLoss[0m : 3.84485
[1mStep[0m  [22/26], [94mLoss[0m : 3.96835
[1mStep[0m  [24/26], [94mLoss[0m : 3.68059

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.385, [92mTest[0m: 5.287, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.61079
[1mStep[0m  [2/26], [94mLoss[0m : 3.56155
[1mStep[0m  [4/26], [94mLoss[0m : 3.47696
[1mStep[0m  [6/26], [94mLoss[0m : 3.26220
[1mStep[0m  [8/26], [94mLoss[0m : 3.19291
[1mStep[0m  [10/26], [94mLoss[0m : 3.28248
[1mStep[0m  [12/26], [94mLoss[0m : 3.22385
[1mStep[0m  [14/26], [94mLoss[0m : 3.30038
[1mStep[0m  [16/26], [94mLoss[0m : 3.28091
[1mStep[0m  [18/26], [94mLoss[0m : 3.22397
[1mStep[0m  [20/26], [94mLoss[0m : 2.98855
[1mStep[0m  [22/26], [94mLoss[0m : 3.31896
[1mStep[0m  [24/26], [94mLoss[0m : 2.99193

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.301, [92mTest[0m: 3.511, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.89125
[1mStep[0m  [2/26], [94mLoss[0m : 3.06127
[1mStep[0m  [4/26], [94mLoss[0m : 3.04597
[1mStep[0m  [6/26], [94mLoss[0m : 2.99162
[1mStep[0m  [8/26], [94mLoss[0m : 3.13128
[1mStep[0m  [10/26], [94mLoss[0m : 3.00372
[1mStep[0m  [12/26], [94mLoss[0m : 3.16963
[1mStep[0m  [14/26], [94mLoss[0m : 2.88813
[1mStep[0m  [16/26], [94mLoss[0m : 2.80001
[1mStep[0m  [18/26], [94mLoss[0m : 2.79884
[1mStep[0m  [20/26], [94mLoss[0m : 2.74920
[1mStep[0m  [22/26], [94mLoss[0m : 2.70271
[1mStep[0m  [24/26], [94mLoss[0m : 2.85805

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.913, [92mTest[0m: 2.828, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.86333
[1mStep[0m  [2/26], [94mLoss[0m : 2.72900
[1mStep[0m  [4/26], [94mLoss[0m : 2.87489
[1mStep[0m  [6/26], [94mLoss[0m : 2.91330
[1mStep[0m  [8/26], [94mLoss[0m : 2.75233
[1mStep[0m  [10/26], [94mLoss[0m : 2.79331
[1mStep[0m  [12/26], [94mLoss[0m : 2.61773
[1mStep[0m  [14/26], [94mLoss[0m : 2.81028
[1mStep[0m  [16/26], [94mLoss[0m : 2.89141
[1mStep[0m  [18/26], [94mLoss[0m : 2.74415
[1mStep[0m  [20/26], [94mLoss[0m : 2.66472
[1mStep[0m  [22/26], [94mLoss[0m : 2.80206
[1mStep[0m  [24/26], [94mLoss[0m : 2.82188

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.781, [92mTest[0m: 2.589, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74411
[1mStep[0m  [2/26], [94mLoss[0m : 2.79351
[1mStep[0m  [4/26], [94mLoss[0m : 2.75372
[1mStep[0m  [6/26], [94mLoss[0m : 2.78264
[1mStep[0m  [8/26], [94mLoss[0m : 2.65279
[1mStep[0m  [10/26], [94mLoss[0m : 2.73805
[1mStep[0m  [12/26], [94mLoss[0m : 2.65970
[1mStep[0m  [14/26], [94mLoss[0m : 2.82992
[1mStep[0m  [16/26], [94mLoss[0m : 2.72085
[1mStep[0m  [18/26], [94mLoss[0m : 2.88002
[1mStep[0m  [20/26], [94mLoss[0m : 2.80976
[1mStep[0m  [22/26], [94mLoss[0m : 2.60735
[1mStep[0m  [24/26], [94mLoss[0m : 2.64427

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65592
[1mStep[0m  [2/26], [94mLoss[0m : 2.64387
[1mStep[0m  [4/26], [94mLoss[0m : 2.70634
[1mStep[0m  [6/26], [94mLoss[0m : 2.63018
[1mStep[0m  [8/26], [94mLoss[0m : 2.68935
[1mStep[0m  [10/26], [94mLoss[0m : 2.67059
[1mStep[0m  [12/26], [94mLoss[0m : 2.83708
[1mStep[0m  [14/26], [94mLoss[0m : 2.65005
[1mStep[0m  [16/26], [94mLoss[0m : 2.60904
[1mStep[0m  [18/26], [94mLoss[0m : 2.68644
[1mStep[0m  [20/26], [94mLoss[0m : 2.70313
[1mStep[0m  [22/26], [94mLoss[0m : 2.68501
[1mStep[0m  [24/26], [94mLoss[0m : 2.80503

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.73658
[1mStep[0m  [2/26], [94mLoss[0m : 2.53513
[1mStep[0m  [4/26], [94mLoss[0m : 2.70754
[1mStep[0m  [6/26], [94mLoss[0m : 2.75347
[1mStep[0m  [8/26], [94mLoss[0m : 2.66458
[1mStep[0m  [10/26], [94mLoss[0m : 2.60544
[1mStep[0m  [12/26], [94mLoss[0m : 2.84178
[1mStep[0m  [14/26], [94mLoss[0m : 2.74340
[1mStep[0m  [16/26], [94mLoss[0m : 2.40795
[1mStep[0m  [18/26], [94mLoss[0m : 2.64298
[1mStep[0m  [20/26], [94mLoss[0m : 2.48135
[1mStep[0m  [22/26], [94mLoss[0m : 2.70359
[1mStep[0m  [24/26], [94mLoss[0m : 2.85907

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59565
[1mStep[0m  [2/26], [94mLoss[0m : 2.80967
[1mStep[0m  [4/26], [94mLoss[0m : 2.70135
[1mStep[0m  [6/26], [94mLoss[0m : 2.63631
[1mStep[0m  [8/26], [94mLoss[0m : 2.83064
[1mStep[0m  [10/26], [94mLoss[0m : 2.67423
[1mStep[0m  [12/26], [94mLoss[0m : 2.69231
[1mStep[0m  [14/26], [94mLoss[0m : 2.59879
[1mStep[0m  [16/26], [94mLoss[0m : 2.58302
[1mStep[0m  [18/26], [94mLoss[0m : 2.65657
[1mStep[0m  [20/26], [94mLoss[0m : 2.57455
[1mStep[0m  [22/26], [94mLoss[0m : 2.64639
[1mStep[0m  [24/26], [94mLoss[0m : 2.96648

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75076
[1mStep[0m  [2/26], [94mLoss[0m : 2.56117
[1mStep[0m  [4/26], [94mLoss[0m : 2.61817
[1mStep[0m  [6/26], [94mLoss[0m : 2.60217
[1mStep[0m  [8/26], [94mLoss[0m : 2.66429
[1mStep[0m  [10/26], [94mLoss[0m : 2.77370
[1mStep[0m  [12/26], [94mLoss[0m : 2.52184
[1mStep[0m  [14/26], [94mLoss[0m : 2.73977
[1mStep[0m  [16/26], [94mLoss[0m : 2.78621
[1mStep[0m  [18/26], [94mLoss[0m : 2.63399
[1mStep[0m  [20/26], [94mLoss[0m : 2.65358
[1mStep[0m  [22/26], [94mLoss[0m : 2.70591
[1mStep[0m  [24/26], [94mLoss[0m : 2.71677

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68240
[1mStep[0m  [2/26], [94mLoss[0m : 2.82240
[1mStep[0m  [4/26], [94mLoss[0m : 2.79404
[1mStep[0m  [6/26], [94mLoss[0m : 2.86476
[1mStep[0m  [8/26], [94mLoss[0m : 2.65862
[1mStep[0m  [10/26], [94mLoss[0m : 2.58787
[1mStep[0m  [12/26], [94mLoss[0m : 2.84547
[1mStep[0m  [14/26], [94mLoss[0m : 2.65736
[1mStep[0m  [16/26], [94mLoss[0m : 2.64159
[1mStep[0m  [18/26], [94mLoss[0m : 2.76413
[1mStep[0m  [20/26], [94mLoss[0m : 2.70354
[1mStep[0m  [22/26], [94mLoss[0m : 2.77403
[1mStep[0m  [24/26], [94mLoss[0m : 2.74500

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69714
[1mStep[0m  [2/26], [94mLoss[0m : 2.55830
[1mStep[0m  [4/26], [94mLoss[0m : 2.75157
[1mStep[0m  [6/26], [94mLoss[0m : 2.77004
[1mStep[0m  [8/26], [94mLoss[0m : 2.66352
[1mStep[0m  [10/26], [94mLoss[0m : 2.79236
[1mStep[0m  [12/26], [94mLoss[0m : 2.60051
[1mStep[0m  [14/26], [94mLoss[0m : 2.60237
[1mStep[0m  [16/26], [94mLoss[0m : 2.59452
[1mStep[0m  [18/26], [94mLoss[0m : 2.61057
[1mStep[0m  [20/26], [94mLoss[0m : 2.65141
[1mStep[0m  [22/26], [94mLoss[0m : 2.62705
[1mStep[0m  [24/26], [94mLoss[0m : 2.58975

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68974
[1mStep[0m  [2/26], [94mLoss[0m : 2.57179
[1mStep[0m  [4/26], [94mLoss[0m : 2.86823
[1mStep[0m  [6/26], [94mLoss[0m : 2.67559
[1mStep[0m  [8/26], [94mLoss[0m : 2.57859
[1mStep[0m  [10/26], [94mLoss[0m : 2.67175
[1mStep[0m  [12/26], [94mLoss[0m : 2.66511
[1mStep[0m  [14/26], [94mLoss[0m : 2.63060
[1mStep[0m  [16/26], [94mLoss[0m : 2.81772
[1mStep[0m  [18/26], [94mLoss[0m : 2.58207
[1mStep[0m  [20/26], [94mLoss[0m : 2.61489
[1mStep[0m  [22/26], [94mLoss[0m : 2.62928
[1mStep[0m  [24/26], [94mLoss[0m : 2.64021

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81281
[1mStep[0m  [2/26], [94mLoss[0m : 2.81009
[1mStep[0m  [4/26], [94mLoss[0m : 2.50981
[1mStep[0m  [6/26], [94mLoss[0m : 2.61695
[1mStep[0m  [8/26], [94mLoss[0m : 2.67597
[1mStep[0m  [10/26], [94mLoss[0m : 2.67405
[1mStep[0m  [12/26], [94mLoss[0m : 2.71574
[1mStep[0m  [14/26], [94mLoss[0m : 2.59787
[1mStep[0m  [16/26], [94mLoss[0m : 2.55549
[1mStep[0m  [18/26], [94mLoss[0m : 2.78437
[1mStep[0m  [20/26], [94mLoss[0m : 2.54338
[1mStep[0m  [22/26], [94mLoss[0m : 2.54735
[1mStep[0m  [24/26], [94mLoss[0m : 2.56776

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68746
[1mStep[0m  [2/26], [94mLoss[0m : 2.73552
[1mStep[0m  [4/26], [94mLoss[0m : 2.74642
[1mStep[0m  [6/26], [94mLoss[0m : 2.61716
[1mStep[0m  [8/26], [94mLoss[0m : 2.73117
[1mStep[0m  [10/26], [94mLoss[0m : 2.61961
[1mStep[0m  [12/26], [94mLoss[0m : 2.78017
[1mStep[0m  [14/26], [94mLoss[0m : 2.69600
[1mStep[0m  [16/26], [94mLoss[0m : 2.61824
[1mStep[0m  [18/26], [94mLoss[0m : 2.77297
[1mStep[0m  [20/26], [94mLoss[0m : 2.56211
[1mStep[0m  [22/26], [94mLoss[0m : 2.71944
[1mStep[0m  [24/26], [94mLoss[0m : 2.64875

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72924
[1mStep[0m  [2/26], [94mLoss[0m : 2.62304
[1mStep[0m  [4/26], [94mLoss[0m : 2.84127
[1mStep[0m  [6/26], [94mLoss[0m : 2.49059
[1mStep[0m  [8/26], [94mLoss[0m : 2.44352
[1mStep[0m  [10/26], [94mLoss[0m : 2.75558
[1mStep[0m  [12/26], [94mLoss[0m : 2.70960
[1mStep[0m  [14/26], [94mLoss[0m : 2.85789
[1mStep[0m  [16/26], [94mLoss[0m : 2.69718
[1mStep[0m  [18/26], [94mLoss[0m : 2.76116
[1mStep[0m  [20/26], [94mLoss[0m : 2.72086
[1mStep[0m  [22/26], [94mLoss[0m : 2.68461
[1mStep[0m  [24/26], [94mLoss[0m : 2.60602

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72339
[1mStep[0m  [2/26], [94mLoss[0m : 2.63458
[1mStep[0m  [4/26], [94mLoss[0m : 2.66057
[1mStep[0m  [6/26], [94mLoss[0m : 2.77711
[1mStep[0m  [8/26], [94mLoss[0m : 2.59871
[1mStep[0m  [10/26], [94mLoss[0m : 2.69774
[1mStep[0m  [12/26], [94mLoss[0m : 2.60034
[1mStep[0m  [14/26], [94mLoss[0m : 2.80399
[1mStep[0m  [16/26], [94mLoss[0m : 2.65517
[1mStep[0m  [18/26], [94mLoss[0m : 2.51177
[1mStep[0m  [20/26], [94mLoss[0m : 2.62104
[1mStep[0m  [22/26], [94mLoss[0m : 2.59769
[1mStep[0m  [24/26], [94mLoss[0m : 2.56750

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68946
[1mStep[0m  [2/26], [94mLoss[0m : 2.60234
[1mStep[0m  [4/26], [94mLoss[0m : 2.53597
[1mStep[0m  [6/26], [94mLoss[0m : 2.67019
[1mStep[0m  [8/26], [94mLoss[0m : 2.65163
[1mStep[0m  [10/26], [94mLoss[0m : 2.80527
[1mStep[0m  [12/26], [94mLoss[0m : 2.56817
[1mStep[0m  [14/26], [94mLoss[0m : 2.53308
[1mStep[0m  [16/26], [94mLoss[0m : 2.77420
[1mStep[0m  [18/26], [94mLoss[0m : 2.61168
[1mStep[0m  [20/26], [94mLoss[0m : 2.63504
[1mStep[0m  [22/26], [94mLoss[0m : 2.65865
[1mStep[0m  [24/26], [94mLoss[0m : 2.66127

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50991
[1mStep[0m  [2/26], [94mLoss[0m : 2.57923
[1mStep[0m  [4/26], [94mLoss[0m : 2.53354
[1mStep[0m  [6/26], [94mLoss[0m : 2.70780
[1mStep[0m  [8/26], [94mLoss[0m : 2.65643
[1mStep[0m  [10/26], [94mLoss[0m : 2.66664
[1mStep[0m  [12/26], [94mLoss[0m : 2.74009
[1mStep[0m  [14/26], [94mLoss[0m : 2.78329
[1mStep[0m  [16/26], [94mLoss[0m : 2.66018
[1mStep[0m  [18/26], [94mLoss[0m : 2.62491
[1mStep[0m  [20/26], [94mLoss[0m : 2.62307
[1mStep[0m  [22/26], [94mLoss[0m : 2.46960
[1mStep[0m  [24/26], [94mLoss[0m : 2.71014

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.445, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70358
[1mStep[0m  [2/26], [94mLoss[0m : 2.74044
[1mStep[0m  [4/26], [94mLoss[0m : 2.54513
[1mStep[0m  [6/26], [94mLoss[0m : 2.68839
[1mStep[0m  [8/26], [94mLoss[0m : 2.61109
[1mStep[0m  [10/26], [94mLoss[0m : 2.61618
[1mStep[0m  [12/26], [94mLoss[0m : 2.76162
[1mStep[0m  [14/26], [94mLoss[0m : 2.71436
[1mStep[0m  [16/26], [94mLoss[0m : 2.61391
[1mStep[0m  [18/26], [94mLoss[0m : 2.59314
[1mStep[0m  [20/26], [94mLoss[0m : 2.56678
[1mStep[0m  [22/26], [94mLoss[0m : 2.68000
[1mStep[0m  [24/26], [94mLoss[0m : 2.86309

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67161
[1mStep[0m  [2/26], [94mLoss[0m : 2.62543
[1mStep[0m  [4/26], [94mLoss[0m : 2.70839
[1mStep[0m  [6/26], [94mLoss[0m : 2.62331
[1mStep[0m  [8/26], [94mLoss[0m : 2.63734
[1mStep[0m  [10/26], [94mLoss[0m : 2.67574
[1mStep[0m  [12/26], [94mLoss[0m : 2.64898
[1mStep[0m  [14/26], [94mLoss[0m : 2.73267
[1mStep[0m  [16/26], [94mLoss[0m : 2.57412
[1mStep[0m  [18/26], [94mLoss[0m : 2.59990
[1mStep[0m  [20/26], [94mLoss[0m : 2.59011
[1mStep[0m  [22/26], [94mLoss[0m : 2.73083
[1mStep[0m  [24/26], [94mLoss[0m : 2.67857

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.443, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74417
[1mStep[0m  [2/26], [94mLoss[0m : 2.64163
[1mStep[0m  [4/26], [94mLoss[0m : 2.59556
[1mStep[0m  [6/26], [94mLoss[0m : 2.81653
[1mStep[0m  [8/26], [94mLoss[0m : 2.54982
[1mStep[0m  [10/26], [94mLoss[0m : 2.58569
[1mStep[0m  [12/26], [94mLoss[0m : 2.73208
[1mStep[0m  [14/26], [94mLoss[0m : 2.74769
[1mStep[0m  [16/26], [94mLoss[0m : 2.54176
[1mStep[0m  [18/26], [94mLoss[0m : 2.61050
[1mStep[0m  [20/26], [94mLoss[0m : 2.83517
[1mStep[0m  [22/26], [94mLoss[0m : 2.67201
[1mStep[0m  [24/26], [94mLoss[0m : 2.66760

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.441, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62741
[1mStep[0m  [2/26], [94mLoss[0m : 2.58495
[1mStep[0m  [4/26], [94mLoss[0m : 2.62728
[1mStep[0m  [6/26], [94mLoss[0m : 2.55783
[1mStep[0m  [8/26], [94mLoss[0m : 2.87349
[1mStep[0m  [10/26], [94mLoss[0m : 2.58952
[1mStep[0m  [12/26], [94mLoss[0m : 2.69656
[1mStep[0m  [14/26], [94mLoss[0m : 2.71992
[1mStep[0m  [16/26], [94mLoss[0m : 2.60104
[1mStep[0m  [18/26], [94mLoss[0m : 2.80550
[1mStep[0m  [20/26], [94mLoss[0m : 2.71404
[1mStep[0m  [22/26], [94mLoss[0m : 2.51257
[1mStep[0m  [24/26], [94mLoss[0m : 2.74491

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57854
[1mStep[0m  [2/26], [94mLoss[0m : 2.70386
[1mStep[0m  [4/26], [94mLoss[0m : 2.72141
[1mStep[0m  [6/26], [94mLoss[0m : 2.74919
[1mStep[0m  [8/26], [94mLoss[0m : 2.69882
[1mStep[0m  [10/26], [94mLoss[0m : 2.65948
[1mStep[0m  [12/26], [94mLoss[0m : 2.63027
[1mStep[0m  [14/26], [94mLoss[0m : 2.80848
[1mStep[0m  [16/26], [94mLoss[0m : 2.67070
[1mStep[0m  [18/26], [94mLoss[0m : 2.65570
[1mStep[0m  [20/26], [94mLoss[0m : 2.49533
[1mStep[0m  [22/26], [94mLoss[0m : 2.40344
[1mStep[0m  [24/26], [94mLoss[0m : 2.82025

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.449, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59714
[1mStep[0m  [2/26], [94mLoss[0m : 2.56403
[1mStep[0m  [4/26], [94mLoss[0m : 2.48654
[1mStep[0m  [6/26], [94mLoss[0m : 2.76650
[1mStep[0m  [8/26], [94mLoss[0m : 2.76955
[1mStep[0m  [10/26], [94mLoss[0m : 2.58746
[1mStep[0m  [12/26], [94mLoss[0m : 2.69041
[1mStep[0m  [14/26], [94mLoss[0m : 2.67438
[1mStep[0m  [16/26], [94mLoss[0m : 2.57118
[1mStep[0m  [18/26], [94mLoss[0m : 2.72127
[1mStep[0m  [20/26], [94mLoss[0m : 2.49394
[1mStep[0m  [22/26], [94mLoss[0m : 2.65122
[1mStep[0m  [24/26], [94mLoss[0m : 2.63675

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.440, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65569
[1mStep[0m  [2/26], [94mLoss[0m : 2.57926
[1mStep[0m  [4/26], [94mLoss[0m : 2.69225
[1mStep[0m  [6/26], [94mLoss[0m : 2.79034
[1mStep[0m  [8/26], [94mLoss[0m : 2.67335
[1mStep[0m  [10/26], [94mLoss[0m : 2.78183
[1mStep[0m  [12/26], [94mLoss[0m : 2.76331
[1mStep[0m  [14/26], [94mLoss[0m : 2.43831
[1mStep[0m  [16/26], [94mLoss[0m : 2.71010
[1mStep[0m  [18/26], [94mLoss[0m : 2.56783
[1mStep[0m  [20/26], [94mLoss[0m : 2.61051
[1mStep[0m  [22/26], [94mLoss[0m : 2.72451
[1mStep[0m  [24/26], [94mLoss[0m : 2.67251

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.435, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51946
[1mStep[0m  [2/26], [94mLoss[0m : 2.67460
[1mStep[0m  [4/26], [94mLoss[0m : 2.50523
[1mStep[0m  [6/26], [94mLoss[0m : 2.73897
[1mStep[0m  [8/26], [94mLoss[0m : 2.54510
[1mStep[0m  [10/26], [94mLoss[0m : 2.67818
[1mStep[0m  [12/26], [94mLoss[0m : 2.65579
[1mStep[0m  [14/26], [94mLoss[0m : 2.69282
[1mStep[0m  [16/26], [94mLoss[0m : 2.51322
[1mStep[0m  [18/26], [94mLoss[0m : 2.65935
[1mStep[0m  [20/26], [94mLoss[0m : 2.58533
[1mStep[0m  [22/26], [94mLoss[0m : 2.71182
[1mStep[0m  [24/26], [94mLoss[0m : 2.67399

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.441, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54898
[1mStep[0m  [2/26], [94mLoss[0m : 2.55402
[1mStep[0m  [4/26], [94mLoss[0m : 2.64437
[1mStep[0m  [6/26], [94mLoss[0m : 2.63506
[1mStep[0m  [8/26], [94mLoss[0m : 2.66145
[1mStep[0m  [10/26], [94mLoss[0m : 2.67995
[1mStep[0m  [12/26], [94mLoss[0m : 2.70365
[1mStep[0m  [14/26], [94mLoss[0m : 2.76038
[1mStep[0m  [16/26], [94mLoss[0m : 2.70146
[1mStep[0m  [18/26], [94mLoss[0m : 2.73809
[1mStep[0m  [20/26], [94mLoss[0m : 2.63338
[1mStep[0m  [22/26], [94mLoss[0m : 2.70508
[1mStep[0m  [24/26], [94mLoss[0m : 2.50911

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.441, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70234
[1mStep[0m  [2/26], [94mLoss[0m : 2.60124
[1mStep[0m  [4/26], [94mLoss[0m : 2.67636
[1mStep[0m  [6/26], [94mLoss[0m : 2.75592
[1mStep[0m  [8/26], [94mLoss[0m : 2.77424
[1mStep[0m  [10/26], [94mLoss[0m : 2.63986
[1mStep[0m  [12/26], [94mLoss[0m : 2.44773
[1mStep[0m  [14/26], [94mLoss[0m : 2.51874
[1mStep[0m  [16/26], [94mLoss[0m : 2.60836
[1mStep[0m  [18/26], [94mLoss[0m : 2.53100
[1mStep[0m  [20/26], [94mLoss[0m : 2.84678
[1mStep[0m  [22/26], [94mLoss[0m : 2.58173
[1mStep[0m  [24/26], [94mLoss[0m : 2.52710

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.442, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.441
====================================

Phase 1 - Evaluation MAE:  2.440997307117169
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 2.57745
[1mStep[0m  [2/26], [94mLoss[0m : 2.57842
[1mStep[0m  [4/26], [94mLoss[0m : 2.75296
[1mStep[0m  [6/26], [94mLoss[0m : 2.63412
[1mStep[0m  [8/26], [94mLoss[0m : 2.57894
[1mStep[0m  [10/26], [94mLoss[0m : 2.84052
[1mStep[0m  [12/26], [94mLoss[0m : 2.70563
[1mStep[0m  [14/26], [94mLoss[0m : 2.74925
[1mStep[0m  [16/26], [94mLoss[0m : 2.50729
[1mStep[0m  [18/26], [94mLoss[0m : 2.51126
[1mStep[0m  [20/26], [94mLoss[0m : 2.51775
[1mStep[0m  [22/26], [94mLoss[0m : 2.73865
[1mStep[0m  [24/26], [94mLoss[0m : 2.70229

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64598
[1mStep[0m  [2/26], [94mLoss[0m : 2.58691
[1mStep[0m  [4/26], [94mLoss[0m : 2.69648
[1mStep[0m  [6/26], [94mLoss[0m : 2.69491
[1mStep[0m  [8/26], [94mLoss[0m : 2.75964
[1mStep[0m  [10/26], [94mLoss[0m : 2.61579
[1mStep[0m  [12/26], [94mLoss[0m : 2.62999
[1mStep[0m  [14/26], [94mLoss[0m : 2.71344
[1mStep[0m  [16/26], [94mLoss[0m : 2.53848
[1mStep[0m  [18/26], [94mLoss[0m : 2.72900
[1mStep[0m  [20/26], [94mLoss[0m : 2.56798
[1mStep[0m  [22/26], [94mLoss[0m : 2.54629
[1mStep[0m  [24/26], [94mLoss[0m : 2.70663

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68210
[1mStep[0m  [2/26], [94mLoss[0m : 2.69144
[1mStep[0m  [4/26], [94mLoss[0m : 2.65638
[1mStep[0m  [6/26], [94mLoss[0m : 2.60384
[1mStep[0m  [8/26], [94mLoss[0m : 2.78675
[1mStep[0m  [10/26], [94mLoss[0m : 2.67107
[1mStep[0m  [12/26], [94mLoss[0m : 2.60922
[1mStep[0m  [14/26], [94mLoss[0m : 2.68684
[1mStep[0m  [16/26], [94mLoss[0m : 2.61402
[1mStep[0m  [18/26], [94mLoss[0m : 2.63170
[1mStep[0m  [20/26], [94mLoss[0m : 2.63490
[1mStep[0m  [22/26], [94mLoss[0m : 2.68983
[1mStep[0m  [24/26], [94mLoss[0m : 2.60409

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50653
[1mStep[0m  [2/26], [94mLoss[0m : 2.61223
[1mStep[0m  [4/26], [94mLoss[0m : 2.69031
[1mStep[0m  [6/26], [94mLoss[0m : 2.55192
[1mStep[0m  [8/26], [94mLoss[0m : 2.65656
[1mStep[0m  [10/26], [94mLoss[0m : 2.81063
[1mStep[0m  [12/26], [94mLoss[0m : 2.55493
[1mStep[0m  [14/26], [94mLoss[0m : 2.71506
[1mStep[0m  [16/26], [94mLoss[0m : 2.63099
[1mStep[0m  [18/26], [94mLoss[0m : 2.59252
[1mStep[0m  [20/26], [94mLoss[0m : 2.60372
[1mStep[0m  [22/26], [94mLoss[0m : 2.69737
[1mStep[0m  [24/26], [94mLoss[0m : 2.50636

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63285
[1mStep[0m  [2/26], [94mLoss[0m : 2.63344
[1mStep[0m  [4/26], [94mLoss[0m : 2.68974
[1mStep[0m  [6/26], [94mLoss[0m : 2.65329
[1mStep[0m  [8/26], [94mLoss[0m : 2.67451
[1mStep[0m  [10/26], [94mLoss[0m : 2.61018
[1mStep[0m  [12/26], [94mLoss[0m : 2.47009
[1mStep[0m  [14/26], [94mLoss[0m : 2.66638
[1mStep[0m  [16/26], [94mLoss[0m : 2.50311
[1mStep[0m  [18/26], [94mLoss[0m : 2.50620
[1mStep[0m  [20/26], [94mLoss[0m : 2.69381
[1mStep[0m  [22/26], [94mLoss[0m : 2.57667
[1mStep[0m  [24/26], [94mLoss[0m : 2.51373

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61945
[1mStep[0m  [2/26], [94mLoss[0m : 2.56267
[1mStep[0m  [4/26], [94mLoss[0m : 2.48501
[1mStep[0m  [6/26], [94mLoss[0m : 2.52026
[1mStep[0m  [8/26], [94mLoss[0m : 2.47315
[1mStep[0m  [10/26], [94mLoss[0m : 2.69676
[1mStep[0m  [12/26], [94mLoss[0m : 2.75003
[1mStep[0m  [14/26], [94mLoss[0m : 2.55953
[1mStep[0m  [16/26], [94mLoss[0m : 2.61649
[1mStep[0m  [18/26], [94mLoss[0m : 2.65111
[1mStep[0m  [20/26], [94mLoss[0m : 2.55803
[1mStep[0m  [22/26], [94mLoss[0m : 2.60715
[1mStep[0m  [24/26], [94mLoss[0m : 2.65846

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58612
[1mStep[0m  [2/26], [94mLoss[0m : 2.63236
[1mStep[0m  [4/26], [94mLoss[0m : 2.65415
[1mStep[0m  [6/26], [94mLoss[0m : 2.61767
[1mStep[0m  [8/26], [94mLoss[0m : 2.56748
[1mStep[0m  [10/26], [94mLoss[0m : 2.61998
[1mStep[0m  [12/26], [94mLoss[0m : 2.51946
[1mStep[0m  [14/26], [94mLoss[0m : 2.56979
[1mStep[0m  [16/26], [94mLoss[0m : 2.61002
[1mStep[0m  [18/26], [94mLoss[0m : 2.56815
[1mStep[0m  [20/26], [94mLoss[0m : 2.65464
[1mStep[0m  [22/26], [94mLoss[0m : 2.69185
[1mStep[0m  [24/26], [94mLoss[0m : 2.42517

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62322
[1mStep[0m  [2/26], [94mLoss[0m : 2.68087
[1mStep[0m  [4/26], [94mLoss[0m : 2.65817
[1mStep[0m  [6/26], [94mLoss[0m : 2.37196
[1mStep[0m  [8/26], [94mLoss[0m : 2.52929
[1mStep[0m  [10/26], [94mLoss[0m : 2.65652
[1mStep[0m  [12/26], [94mLoss[0m : 2.55569
[1mStep[0m  [14/26], [94mLoss[0m : 2.64338
[1mStep[0m  [16/26], [94mLoss[0m : 2.49018
[1mStep[0m  [18/26], [94mLoss[0m : 2.60145
[1mStep[0m  [20/26], [94mLoss[0m : 2.62160
[1mStep[0m  [22/26], [94mLoss[0m : 2.65676
[1mStep[0m  [24/26], [94mLoss[0m : 2.44893

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53100
[1mStep[0m  [2/26], [94mLoss[0m : 2.54172
[1mStep[0m  [4/26], [94mLoss[0m : 2.48827
[1mStep[0m  [6/26], [94mLoss[0m : 2.62587
[1mStep[0m  [8/26], [94mLoss[0m : 2.47582
[1mStep[0m  [10/26], [94mLoss[0m : 2.52726
[1mStep[0m  [12/26], [94mLoss[0m : 2.56492
[1mStep[0m  [14/26], [94mLoss[0m : 2.48334
[1mStep[0m  [16/26], [94mLoss[0m : 2.69413
[1mStep[0m  [18/26], [94mLoss[0m : 2.51715
[1mStep[0m  [20/26], [94mLoss[0m : 2.65073
[1mStep[0m  [22/26], [94mLoss[0m : 2.50498
[1mStep[0m  [24/26], [94mLoss[0m : 2.74599

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74371
[1mStep[0m  [2/26], [94mLoss[0m : 2.55802
[1mStep[0m  [4/26], [94mLoss[0m : 2.44556
[1mStep[0m  [6/26], [94mLoss[0m : 2.56142
[1mStep[0m  [8/26], [94mLoss[0m : 2.41287
[1mStep[0m  [10/26], [94mLoss[0m : 2.65382
[1mStep[0m  [12/26], [94mLoss[0m : 2.84245
[1mStep[0m  [14/26], [94mLoss[0m : 2.58097
[1mStep[0m  [16/26], [94mLoss[0m : 2.54243
[1mStep[0m  [18/26], [94mLoss[0m : 2.53284
[1mStep[0m  [20/26], [94mLoss[0m : 2.60513
[1mStep[0m  [22/26], [94mLoss[0m : 2.51871
[1mStep[0m  [24/26], [94mLoss[0m : 2.45630

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58599
[1mStep[0m  [2/26], [94mLoss[0m : 2.57862
[1mStep[0m  [4/26], [94mLoss[0m : 2.59898
[1mStep[0m  [6/26], [94mLoss[0m : 2.62112
[1mStep[0m  [8/26], [94mLoss[0m : 2.62141
[1mStep[0m  [10/26], [94mLoss[0m : 2.61358
[1mStep[0m  [12/26], [94mLoss[0m : 2.60306
[1mStep[0m  [14/26], [94mLoss[0m : 2.56685
[1mStep[0m  [16/26], [94mLoss[0m : 2.56806
[1mStep[0m  [18/26], [94mLoss[0m : 2.55359
[1mStep[0m  [20/26], [94mLoss[0m : 2.73420
[1mStep[0m  [22/26], [94mLoss[0m : 2.56156
[1mStep[0m  [24/26], [94mLoss[0m : 2.57151

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.561, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57496
[1mStep[0m  [2/26], [94mLoss[0m : 2.36671
[1mStep[0m  [4/26], [94mLoss[0m : 2.66403
[1mStep[0m  [6/26], [94mLoss[0m : 2.55097
[1mStep[0m  [8/26], [94mLoss[0m : 2.51300
[1mStep[0m  [10/26], [94mLoss[0m : 2.61211
[1mStep[0m  [12/26], [94mLoss[0m : 2.53462
[1mStep[0m  [14/26], [94mLoss[0m : 2.40626
[1mStep[0m  [16/26], [94mLoss[0m : 2.49732
[1mStep[0m  [18/26], [94mLoss[0m : 2.62911
[1mStep[0m  [20/26], [94mLoss[0m : 2.57172
[1mStep[0m  [22/26], [94mLoss[0m : 2.58909
[1mStep[0m  [24/26], [94mLoss[0m : 2.49899

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66849
[1mStep[0m  [2/26], [94mLoss[0m : 2.61906
[1mStep[0m  [4/26], [94mLoss[0m : 2.51014
[1mStep[0m  [6/26], [94mLoss[0m : 2.51129
[1mStep[0m  [8/26], [94mLoss[0m : 2.61281
[1mStep[0m  [10/26], [94mLoss[0m : 2.44079
[1mStep[0m  [12/26], [94mLoss[0m : 2.40507
[1mStep[0m  [14/26], [94mLoss[0m : 2.54221
[1mStep[0m  [16/26], [94mLoss[0m : 2.64982
[1mStep[0m  [18/26], [94mLoss[0m : 2.63816
[1mStep[0m  [20/26], [94mLoss[0m : 2.62287
[1mStep[0m  [22/26], [94mLoss[0m : 2.47449
[1mStep[0m  [24/26], [94mLoss[0m : 2.56607

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49641
[1mStep[0m  [2/26], [94mLoss[0m : 2.52731
[1mStep[0m  [4/26], [94mLoss[0m : 2.72959
[1mStep[0m  [6/26], [94mLoss[0m : 2.43372
[1mStep[0m  [8/26], [94mLoss[0m : 2.35218
[1mStep[0m  [10/26], [94mLoss[0m : 2.69601
[1mStep[0m  [12/26], [94mLoss[0m : 2.52518
[1mStep[0m  [14/26], [94mLoss[0m : 2.41364
[1mStep[0m  [16/26], [94mLoss[0m : 2.64746
[1mStep[0m  [18/26], [94mLoss[0m : 2.53688
[1mStep[0m  [20/26], [94mLoss[0m : 2.60417
[1mStep[0m  [22/26], [94mLoss[0m : 2.62995
[1mStep[0m  [24/26], [94mLoss[0m : 2.65493

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44456
[1mStep[0m  [2/26], [94mLoss[0m : 2.50675
[1mStep[0m  [4/26], [94mLoss[0m : 2.44545
[1mStep[0m  [6/26], [94mLoss[0m : 2.40373
[1mStep[0m  [8/26], [94mLoss[0m : 2.55966
[1mStep[0m  [10/26], [94mLoss[0m : 2.46878
[1mStep[0m  [12/26], [94mLoss[0m : 2.27904
[1mStep[0m  [14/26], [94mLoss[0m : 2.38728
[1mStep[0m  [16/26], [94mLoss[0m : 2.52543
[1mStep[0m  [18/26], [94mLoss[0m : 2.57929
[1mStep[0m  [20/26], [94mLoss[0m : 2.70707
[1mStep[0m  [22/26], [94mLoss[0m : 2.56469
[1mStep[0m  [24/26], [94mLoss[0m : 2.73384

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.609, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44492
[1mStep[0m  [2/26], [94mLoss[0m : 2.41788
[1mStep[0m  [4/26], [94mLoss[0m : 2.43724
[1mStep[0m  [6/26], [94mLoss[0m : 2.42008
[1mStep[0m  [8/26], [94mLoss[0m : 2.44138
[1mStep[0m  [10/26], [94mLoss[0m : 2.51238
[1mStep[0m  [12/26], [94mLoss[0m : 2.51496
[1mStep[0m  [14/26], [94mLoss[0m : 2.57955
[1mStep[0m  [16/26], [94mLoss[0m : 2.39899
[1mStep[0m  [18/26], [94mLoss[0m : 2.53398
[1mStep[0m  [20/26], [94mLoss[0m : 2.63180
[1mStep[0m  [22/26], [94mLoss[0m : 2.54369
[1mStep[0m  [24/26], [94mLoss[0m : 2.64464

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.620, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66554
[1mStep[0m  [2/26], [94mLoss[0m : 2.56719
[1mStep[0m  [4/26], [94mLoss[0m : 2.40662
[1mStep[0m  [6/26], [94mLoss[0m : 2.49442
[1mStep[0m  [8/26], [94mLoss[0m : 2.45860
[1mStep[0m  [10/26], [94mLoss[0m : 2.57528
[1mStep[0m  [12/26], [94mLoss[0m : 2.52835
[1mStep[0m  [14/26], [94mLoss[0m : 2.46371
[1mStep[0m  [16/26], [94mLoss[0m : 2.49639
[1mStep[0m  [18/26], [94mLoss[0m : 2.53763
[1mStep[0m  [20/26], [94mLoss[0m : 2.50993
[1mStep[0m  [22/26], [94mLoss[0m : 2.41098
[1mStep[0m  [24/26], [94mLoss[0m : 2.36747

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.643, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48932
[1mStep[0m  [2/26], [94mLoss[0m : 2.33310
[1mStep[0m  [4/26], [94mLoss[0m : 2.47235
[1mStep[0m  [6/26], [94mLoss[0m : 2.33977
[1mStep[0m  [8/26], [94mLoss[0m : 2.41664
[1mStep[0m  [10/26], [94mLoss[0m : 2.59608
[1mStep[0m  [12/26], [94mLoss[0m : 2.44091
[1mStep[0m  [14/26], [94mLoss[0m : 2.66493
[1mStep[0m  [16/26], [94mLoss[0m : 2.58767
[1mStep[0m  [18/26], [94mLoss[0m : 2.61022
[1mStep[0m  [20/26], [94mLoss[0m : 2.44558
[1mStep[0m  [22/26], [94mLoss[0m : 2.43349
[1mStep[0m  [24/26], [94mLoss[0m : 2.48970

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.694, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45450
[1mStep[0m  [2/26], [94mLoss[0m : 2.43355
[1mStep[0m  [4/26], [94mLoss[0m : 2.50563
[1mStep[0m  [6/26], [94mLoss[0m : 2.44459
[1mStep[0m  [8/26], [94mLoss[0m : 2.59209
[1mStep[0m  [10/26], [94mLoss[0m : 2.66457
[1mStep[0m  [12/26], [94mLoss[0m : 2.58285
[1mStep[0m  [14/26], [94mLoss[0m : 2.51043
[1mStep[0m  [16/26], [94mLoss[0m : 2.58700
[1mStep[0m  [18/26], [94mLoss[0m : 2.65416
[1mStep[0m  [20/26], [94mLoss[0m : 2.49222
[1mStep[0m  [22/26], [94mLoss[0m : 2.48988
[1mStep[0m  [24/26], [94mLoss[0m : 2.51957

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.655, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38429
[1mStep[0m  [2/26], [94mLoss[0m : 2.63011
[1mStep[0m  [4/26], [94mLoss[0m : 2.47758
[1mStep[0m  [6/26], [94mLoss[0m : 2.49687
[1mStep[0m  [8/26], [94mLoss[0m : 2.31029
[1mStep[0m  [10/26], [94mLoss[0m : 2.34560
[1mStep[0m  [12/26], [94mLoss[0m : 2.63603
[1mStep[0m  [14/26], [94mLoss[0m : 2.60725
[1mStep[0m  [16/26], [94mLoss[0m : 2.52122
[1mStep[0m  [18/26], [94mLoss[0m : 2.57976
[1mStep[0m  [20/26], [94mLoss[0m : 2.55895
[1mStep[0m  [22/26], [94mLoss[0m : 2.41373
[1mStep[0m  [24/26], [94mLoss[0m : 2.32360

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.714, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53806
[1mStep[0m  [2/26], [94mLoss[0m : 2.60176
[1mStep[0m  [4/26], [94mLoss[0m : 2.43027
[1mStep[0m  [6/26], [94mLoss[0m : 2.40619
[1mStep[0m  [8/26], [94mLoss[0m : 2.50176
[1mStep[0m  [10/26], [94mLoss[0m : 2.56529
[1mStep[0m  [12/26], [94mLoss[0m : 2.54743
[1mStep[0m  [14/26], [94mLoss[0m : 2.44003
[1mStep[0m  [16/26], [94mLoss[0m : 2.49491
[1mStep[0m  [18/26], [94mLoss[0m : 2.55511
[1mStep[0m  [20/26], [94mLoss[0m : 2.57533
[1mStep[0m  [22/26], [94mLoss[0m : 2.49013
[1mStep[0m  [24/26], [94mLoss[0m : 2.44332

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.631, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41914
[1mStep[0m  [2/26], [94mLoss[0m : 2.33132
[1mStep[0m  [4/26], [94mLoss[0m : 2.43161
[1mStep[0m  [6/26], [94mLoss[0m : 2.38436
[1mStep[0m  [8/26], [94mLoss[0m : 2.51840
[1mStep[0m  [10/26], [94mLoss[0m : 2.55811
[1mStep[0m  [12/26], [94mLoss[0m : 2.57422
[1mStep[0m  [14/26], [94mLoss[0m : 2.42638
[1mStep[0m  [16/26], [94mLoss[0m : 2.41580
[1mStep[0m  [18/26], [94mLoss[0m : 2.49368
[1mStep[0m  [20/26], [94mLoss[0m : 2.52239
[1mStep[0m  [22/26], [94mLoss[0m : 2.41983
[1mStep[0m  [24/26], [94mLoss[0m : 2.45595

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.674, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42199
[1mStep[0m  [2/26], [94mLoss[0m : 2.48626
[1mStep[0m  [4/26], [94mLoss[0m : 2.44901
[1mStep[0m  [6/26], [94mLoss[0m : 2.39648
[1mStep[0m  [8/26], [94mLoss[0m : 2.46840
[1mStep[0m  [10/26], [94mLoss[0m : 2.35681
[1mStep[0m  [12/26], [94mLoss[0m : 2.38466
[1mStep[0m  [14/26], [94mLoss[0m : 2.36433
[1mStep[0m  [16/26], [94mLoss[0m : 2.44074
[1mStep[0m  [18/26], [94mLoss[0m : 2.60146
[1mStep[0m  [20/26], [94mLoss[0m : 2.37120
[1mStep[0m  [22/26], [94mLoss[0m : 2.45544
[1mStep[0m  [24/26], [94mLoss[0m : 2.36492

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.616, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43386
[1mStep[0m  [2/26], [94mLoss[0m : 2.34519
[1mStep[0m  [4/26], [94mLoss[0m : 2.52576
[1mStep[0m  [6/26], [94mLoss[0m : 2.52078
[1mStep[0m  [8/26], [94mLoss[0m : 2.46472
[1mStep[0m  [10/26], [94mLoss[0m : 2.41051
[1mStep[0m  [12/26], [94mLoss[0m : 2.57668
[1mStep[0m  [14/26], [94mLoss[0m : 2.42684
[1mStep[0m  [16/26], [94mLoss[0m : 2.40074
[1mStep[0m  [18/26], [94mLoss[0m : 2.47309
[1mStep[0m  [20/26], [94mLoss[0m : 2.45852
[1mStep[0m  [22/26], [94mLoss[0m : 2.50982
[1mStep[0m  [24/26], [94mLoss[0m : 2.47629

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.638, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40150
[1mStep[0m  [2/26], [94mLoss[0m : 2.53659
[1mStep[0m  [4/26], [94mLoss[0m : 2.36514
[1mStep[0m  [6/26], [94mLoss[0m : 2.40411
[1mStep[0m  [8/26], [94mLoss[0m : 2.38511
[1mStep[0m  [10/26], [94mLoss[0m : 2.31770
[1mStep[0m  [12/26], [94mLoss[0m : 2.42758
[1mStep[0m  [14/26], [94mLoss[0m : 2.42489
[1mStep[0m  [16/26], [94mLoss[0m : 2.47901
[1mStep[0m  [18/26], [94mLoss[0m : 2.43291
[1mStep[0m  [20/26], [94mLoss[0m : 2.51477
[1mStep[0m  [22/26], [94mLoss[0m : 2.54621
[1mStep[0m  [24/26], [94mLoss[0m : 2.48418

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.604, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34538
[1mStep[0m  [2/26], [94mLoss[0m : 2.28477
[1mStep[0m  [4/26], [94mLoss[0m : 2.47302
[1mStep[0m  [6/26], [94mLoss[0m : 2.29874
[1mStep[0m  [8/26], [94mLoss[0m : 2.44101
[1mStep[0m  [10/26], [94mLoss[0m : 2.37678
[1mStep[0m  [12/26], [94mLoss[0m : 2.45097
[1mStep[0m  [14/26], [94mLoss[0m : 2.38460
[1mStep[0m  [16/26], [94mLoss[0m : 2.38333
[1mStep[0m  [18/26], [94mLoss[0m : 2.44403
[1mStep[0m  [20/26], [94mLoss[0m : 2.42965
[1mStep[0m  [22/26], [94mLoss[0m : 2.39052
[1mStep[0m  [24/26], [94mLoss[0m : 2.59291

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.609, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42507
[1mStep[0m  [2/26], [94mLoss[0m : 2.26826
[1mStep[0m  [4/26], [94mLoss[0m : 2.33052
[1mStep[0m  [6/26], [94mLoss[0m : 2.57075
[1mStep[0m  [8/26], [94mLoss[0m : 2.39413
[1mStep[0m  [10/26], [94mLoss[0m : 2.33990
[1mStep[0m  [12/26], [94mLoss[0m : 2.41329
[1mStep[0m  [14/26], [94mLoss[0m : 2.29320
[1mStep[0m  [16/26], [94mLoss[0m : 2.38048
[1mStep[0m  [18/26], [94mLoss[0m : 2.30725
[1mStep[0m  [20/26], [94mLoss[0m : 2.39835
[1mStep[0m  [22/26], [94mLoss[0m : 2.39043
[1mStep[0m  [24/26], [94mLoss[0m : 2.53906

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.606, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.20888
[1mStep[0m  [2/26], [94mLoss[0m : 2.30891
[1mStep[0m  [4/26], [94mLoss[0m : 2.41047
[1mStep[0m  [6/26], [94mLoss[0m : 2.48087
[1mStep[0m  [8/26], [94mLoss[0m : 2.39701
[1mStep[0m  [10/26], [94mLoss[0m : 2.42521
[1mStep[0m  [12/26], [94mLoss[0m : 2.24853
[1mStep[0m  [14/26], [94mLoss[0m : 2.55247
[1mStep[0m  [16/26], [94mLoss[0m : 2.43260
[1mStep[0m  [18/26], [94mLoss[0m : 2.39987
[1mStep[0m  [20/26], [94mLoss[0m : 2.48916
[1mStep[0m  [22/26], [94mLoss[0m : 2.31763
[1mStep[0m  [24/26], [94mLoss[0m : 2.39094

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34442
[1mStep[0m  [2/26], [94mLoss[0m : 2.50816
[1mStep[0m  [4/26], [94mLoss[0m : 2.27357
[1mStep[0m  [6/26], [94mLoss[0m : 2.49149
[1mStep[0m  [8/26], [94mLoss[0m : 2.32286
[1mStep[0m  [10/26], [94mLoss[0m : 2.25571
[1mStep[0m  [12/26], [94mLoss[0m : 2.39697
[1mStep[0m  [14/26], [94mLoss[0m : 2.41211
[1mStep[0m  [16/26], [94mLoss[0m : 2.17889
[1mStep[0m  [18/26], [94mLoss[0m : 2.54855
[1mStep[0m  [20/26], [94mLoss[0m : 2.37729
[1mStep[0m  [22/26], [94mLoss[0m : 2.33567
[1mStep[0m  [24/26], [94mLoss[0m : 2.40954

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.557, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26468
[1mStep[0m  [2/26], [94mLoss[0m : 2.46315
[1mStep[0m  [4/26], [94mLoss[0m : 2.39451
[1mStep[0m  [6/26], [94mLoss[0m : 2.35670
[1mStep[0m  [8/26], [94mLoss[0m : 2.41218
[1mStep[0m  [10/26], [94mLoss[0m : 2.30246
[1mStep[0m  [12/26], [94mLoss[0m : 2.41135
[1mStep[0m  [14/26], [94mLoss[0m : 2.38525
[1mStep[0m  [16/26], [94mLoss[0m : 2.27710
[1mStep[0m  [18/26], [94mLoss[0m : 2.35468
[1mStep[0m  [20/26], [94mLoss[0m : 2.34223
[1mStep[0m  [22/26], [94mLoss[0m : 2.42874
[1mStep[0m  [24/26], [94mLoss[0m : 2.50240

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.551, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.571
====================================

Phase 2 - Evaluation MAE:  2.570568378155048
MAE score P1       2.440997
MAE score P2       2.570568
loss               2.364477
learning_rate      0.007525
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay         0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.83197
[1mStep[0m  [2/26], [94mLoss[0m : 10.73166
[1mStep[0m  [4/26], [94mLoss[0m : 10.93928
[1mStep[0m  [6/26], [94mLoss[0m : 10.68828
[1mStep[0m  [8/26], [94mLoss[0m : 10.78896
[1mStep[0m  [10/26], [94mLoss[0m : 10.45828
[1mStep[0m  [12/26], [94mLoss[0m : 10.57321
[1mStep[0m  [14/26], [94mLoss[0m : 10.06785
[1mStep[0m  [16/26], [94mLoss[0m : 9.60505
[1mStep[0m  [18/26], [94mLoss[0m : 9.67366
[1mStep[0m  [20/26], [94mLoss[0m : 9.37510
[1mStep[0m  [22/26], [94mLoss[0m : 8.83835
[1mStep[0m  [24/26], [94mLoss[0m : 8.80937

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.038, [92mTest[0m: 10.906, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.50851
[1mStep[0m  [2/26], [94mLoss[0m : 8.36672
[1mStep[0m  [4/26], [94mLoss[0m : 7.79075
[1mStep[0m  [6/26], [94mLoss[0m : 7.55739
[1mStep[0m  [8/26], [94mLoss[0m : 7.34589
[1mStep[0m  [10/26], [94mLoss[0m : 7.07054
[1mStep[0m  [12/26], [94mLoss[0m : 6.92463
[1mStep[0m  [14/26], [94mLoss[0m : 6.85544
[1mStep[0m  [16/26], [94mLoss[0m : 6.46797
[1mStep[0m  [18/26], [94mLoss[0m : 6.52143
[1mStep[0m  [20/26], [94mLoss[0m : 6.18836
[1mStep[0m  [22/26], [94mLoss[0m : 6.05860
[1mStep[0m  [24/26], [94mLoss[0m : 5.86381

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.970, [92mTest[0m: 8.545, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.60497
[1mStep[0m  [2/26], [94mLoss[0m : 5.10348
[1mStep[0m  [4/26], [94mLoss[0m : 4.95292
[1mStep[0m  [6/26], [94mLoss[0m : 4.96651
[1mStep[0m  [8/26], [94mLoss[0m : 4.52935
[1mStep[0m  [10/26], [94mLoss[0m : 4.07559
[1mStep[0m  [12/26], [94mLoss[0m : 4.15878
[1mStep[0m  [14/26], [94mLoss[0m : 3.81721
[1mStep[0m  [16/26], [94mLoss[0m : 3.58079
[1mStep[0m  [18/26], [94mLoss[0m : 3.63452
[1mStep[0m  [20/26], [94mLoss[0m : 3.18279
[1mStep[0m  [22/26], [94mLoss[0m : 3.26473
[1mStep[0m  [24/26], [94mLoss[0m : 2.98221

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.079, [92mTest[0m: 4.742, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.90280
[1mStep[0m  [2/26], [94mLoss[0m : 3.03694
[1mStep[0m  [4/26], [94mLoss[0m : 2.72306
[1mStep[0m  [6/26], [94mLoss[0m : 2.90502
[1mStep[0m  [8/26], [94mLoss[0m : 2.69504
[1mStep[0m  [10/26], [94mLoss[0m : 2.66066
[1mStep[0m  [12/26], [94mLoss[0m : 2.71385
[1mStep[0m  [14/26], [94mLoss[0m : 2.80477
[1mStep[0m  [16/26], [94mLoss[0m : 2.71983
[1mStep[0m  [18/26], [94mLoss[0m : 2.85994
[1mStep[0m  [20/26], [94mLoss[0m : 2.82237
[1mStep[0m  [22/26], [94mLoss[0m : 2.73220
[1mStep[0m  [24/26], [94mLoss[0m : 2.66027

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.781, [92mTest[0m: 3.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58910
[1mStep[0m  [2/26], [94mLoss[0m : 2.69979
[1mStep[0m  [4/26], [94mLoss[0m : 2.71355
[1mStep[0m  [6/26], [94mLoss[0m : 2.70501
[1mStep[0m  [8/26], [94mLoss[0m : 2.60933
[1mStep[0m  [10/26], [94mLoss[0m : 2.65565
[1mStep[0m  [12/26], [94mLoss[0m : 2.59720
[1mStep[0m  [14/26], [94mLoss[0m : 2.69311
[1mStep[0m  [16/26], [94mLoss[0m : 2.79097
[1mStep[0m  [18/26], [94mLoss[0m : 2.47761
[1mStep[0m  [20/26], [94mLoss[0m : 2.62880
[1mStep[0m  [22/26], [94mLoss[0m : 2.66351
[1mStep[0m  [24/26], [94mLoss[0m : 2.57015

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56862
[1mStep[0m  [2/26], [94mLoss[0m : 2.58166
[1mStep[0m  [4/26], [94mLoss[0m : 2.55862
[1mStep[0m  [6/26], [94mLoss[0m : 2.61964
[1mStep[0m  [8/26], [94mLoss[0m : 2.62740
[1mStep[0m  [10/26], [94mLoss[0m : 2.62876
[1mStep[0m  [12/26], [94mLoss[0m : 2.55188
[1mStep[0m  [14/26], [94mLoss[0m : 2.51160
[1mStep[0m  [16/26], [94mLoss[0m : 2.60063
[1mStep[0m  [18/26], [94mLoss[0m : 2.66432
[1mStep[0m  [20/26], [94mLoss[0m : 2.70620
[1mStep[0m  [22/26], [94mLoss[0m : 2.43701
[1mStep[0m  [24/26], [94mLoss[0m : 2.51085

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53065
[1mStep[0m  [2/26], [94mLoss[0m : 2.59510
[1mStep[0m  [4/26], [94mLoss[0m : 2.60120
[1mStep[0m  [6/26], [94mLoss[0m : 2.48405
[1mStep[0m  [8/26], [94mLoss[0m : 2.49443
[1mStep[0m  [10/26], [94mLoss[0m : 2.54943
[1mStep[0m  [12/26], [94mLoss[0m : 2.70210
[1mStep[0m  [14/26], [94mLoss[0m : 2.54784
[1mStep[0m  [16/26], [94mLoss[0m : 2.60925
[1mStep[0m  [18/26], [94mLoss[0m : 2.47244
[1mStep[0m  [20/26], [94mLoss[0m : 2.56212
[1mStep[0m  [22/26], [94mLoss[0m : 2.40267
[1mStep[0m  [24/26], [94mLoss[0m : 2.53610

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61003
[1mStep[0m  [2/26], [94mLoss[0m : 2.53338
[1mStep[0m  [4/26], [94mLoss[0m : 2.54213
[1mStep[0m  [6/26], [94mLoss[0m : 2.50151
[1mStep[0m  [8/26], [94mLoss[0m : 2.55295
[1mStep[0m  [10/26], [94mLoss[0m : 2.55331
[1mStep[0m  [12/26], [94mLoss[0m : 2.53388
[1mStep[0m  [14/26], [94mLoss[0m : 2.43381
[1mStep[0m  [16/26], [94mLoss[0m : 2.64274
[1mStep[0m  [18/26], [94mLoss[0m : 2.71217
[1mStep[0m  [20/26], [94mLoss[0m : 2.46469
[1mStep[0m  [22/26], [94mLoss[0m : 2.52221
[1mStep[0m  [24/26], [94mLoss[0m : 2.53336

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59975
[1mStep[0m  [2/26], [94mLoss[0m : 2.48839
[1mStep[0m  [4/26], [94mLoss[0m : 2.49202
[1mStep[0m  [6/26], [94mLoss[0m : 2.45171
[1mStep[0m  [8/26], [94mLoss[0m : 2.44968
[1mStep[0m  [10/26], [94mLoss[0m : 2.66610
[1mStep[0m  [12/26], [94mLoss[0m : 2.53141
[1mStep[0m  [14/26], [94mLoss[0m : 2.66418
[1mStep[0m  [16/26], [94mLoss[0m : 2.74278
[1mStep[0m  [18/26], [94mLoss[0m : 2.66537
[1mStep[0m  [20/26], [94mLoss[0m : 2.52103
[1mStep[0m  [22/26], [94mLoss[0m : 2.62995
[1mStep[0m  [24/26], [94mLoss[0m : 2.42798

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51273
[1mStep[0m  [2/26], [94mLoss[0m : 2.61917
[1mStep[0m  [4/26], [94mLoss[0m : 2.56446
[1mStep[0m  [6/26], [94mLoss[0m : 2.34964
[1mStep[0m  [8/26], [94mLoss[0m : 2.40148
[1mStep[0m  [10/26], [94mLoss[0m : 2.74565
[1mStep[0m  [12/26], [94mLoss[0m : 2.49222
[1mStep[0m  [14/26], [94mLoss[0m : 2.51443
[1mStep[0m  [16/26], [94mLoss[0m : 2.55001
[1mStep[0m  [18/26], [94mLoss[0m : 2.59646
[1mStep[0m  [20/26], [94mLoss[0m : 2.56855
[1mStep[0m  [22/26], [94mLoss[0m : 2.52405
[1mStep[0m  [24/26], [94mLoss[0m : 2.51967

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67470
[1mStep[0m  [2/26], [94mLoss[0m : 2.51592
[1mStep[0m  [4/26], [94mLoss[0m : 2.49626
[1mStep[0m  [6/26], [94mLoss[0m : 2.49979
[1mStep[0m  [8/26], [94mLoss[0m : 2.43398
[1mStep[0m  [10/26], [94mLoss[0m : 2.52395
[1mStep[0m  [12/26], [94mLoss[0m : 2.52163
[1mStep[0m  [14/26], [94mLoss[0m : 2.48553
[1mStep[0m  [16/26], [94mLoss[0m : 2.47448
[1mStep[0m  [18/26], [94mLoss[0m : 2.44419
[1mStep[0m  [20/26], [94mLoss[0m : 2.51696
[1mStep[0m  [22/26], [94mLoss[0m : 2.73034
[1mStep[0m  [24/26], [94mLoss[0m : 2.46778

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40295
[1mStep[0m  [2/26], [94mLoss[0m : 2.43079
[1mStep[0m  [4/26], [94mLoss[0m : 2.50330
[1mStep[0m  [6/26], [94mLoss[0m : 2.44572
[1mStep[0m  [8/26], [94mLoss[0m : 2.39761
[1mStep[0m  [10/26], [94mLoss[0m : 2.65769
[1mStep[0m  [12/26], [94mLoss[0m : 2.55609
[1mStep[0m  [14/26], [94mLoss[0m : 2.56245
[1mStep[0m  [16/26], [94mLoss[0m : 2.49082
[1mStep[0m  [18/26], [94mLoss[0m : 2.54455
[1mStep[0m  [20/26], [94mLoss[0m : 2.57821
[1mStep[0m  [22/26], [94mLoss[0m : 2.43894
[1mStep[0m  [24/26], [94mLoss[0m : 2.51356

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59079
[1mStep[0m  [2/26], [94mLoss[0m : 2.69254
[1mStep[0m  [4/26], [94mLoss[0m : 2.37056
[1mStep[0m  [6/26], [94mLoss[0m : 2.40548
[1mStep[0m  [8/26], [94mLoss[0m : 2.47798
[1mStep[0m  [10/26], [94mLoss[0m : 2.52923
[1mStep[0m  [12/26], [94mLoss[0m : 2.47174
[1mStep[0m  [14/26], [94mLoss[0m : 2.60872
[1mStep[0m  [16/26], [94mLoss[0m : 2.34224
[1mStep[0m  [18/26], [94mLoss[0m : 2.53569
[1mStep[0m  [20/26], [94mLoss[0m : 2.38775
[1mStep[0m  [22/26], [94mLoss[0m : 2.52604
[1mStep[0m  [24/26], [94mLoss[0m : 2.55150

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50985
[1mStep[0m  [2/26], [94mLoss[0m : 2.53878
[1mStep[0m  [4/26], [94mLoss[0m : 2.42644
[1mStep[0m  [6/26], [94mLoss[0m : 2.65951
[1mStep[0m  [8/26], [94mLoss[0m : 2.55703
[1mStep[0m  [10/26], [94mLoss[0m : 2.39793
[1mStep[0m  [12/26], [94mLoss[0m : 2.59888
[1mStep[0m  [14/26], [94mLoss[0m : 2.58408
[1mStep[0m  [16/26], [94mLoss[0m : 2.60580
[1mStep[0m  [18/26], [94mLoss[0m : 2.44452
[1mStep[0m  [20/26], [94mLoss[0m : 2.44702
[1mStep[0m  [22/26], [94mLoss[0m : 2.47326
[1mStep[0m  [24/26], [94mLoss[0m : 2.52505

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25661
[1mStep[0m  [2/26], [94mLoss[0m : 2.57401
[1mStep[0m  [4/26], [94mLoss[0m : 2.57896
[1mStep[0m  [6/26], [94mLoss[0m : 2.59484
[1mStep[0m  [8/26], [94mLoss[0m : 2.40047
[1mStep[0m  [10/26], [94mLoss[0m : 2.47927
[1mStep[0m  [12/26], [94mLoss[0m : 2.55063
[1mStep[0m  [14/26], [94mLoss[0m : 2.48875
[1mStep[0m  [16/26], [94mLoss[0m : 2.48710
[1mStep[0m  [18/26], [94mLoss[0m : 2.46170
[1mStep[0m  [20/26], [94mLoss[0m : 2.38934
[1mStep[0m  [22/26], [94mLoss[0m : 2.51450
[1mStep[0m  [24/26], [94mLoss[0m : 2.41021

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41416
[1mStep[0m  [2/26], [94mLoss[0m : 2.49987
[1mStep[0m  [4/26], [94mLoss[0m : 2.42728
[1mStep[0m  [6/26], [94mLoss[0m : 2.37673
[1mStep[0m  [8/26], [94mLoss[0m : 2.49784
[1mStep[0m  [10/26], [94mLoss[0m : 2.51483
[1mStep[0m  [12/26], [94mLoss[0m : 2.30089
[1mStep[0m  [14/26], [94mLoss[0m : 2.42708
[1mStep[0m  [16/26], [94mLoss[0m : 2.51519
[1mStep[0m  [18/26], [94mLoss[0m : 2.54094
[1mStep[0m  [20/26], [94mLoss[0m : 2.39731
[1mStep[0m  [22/26], [94mLoss[0m : 2.45914
[1mStep[0m  [24/26], [94mLoss[0m : 2.46312

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41648
[1mStep[0m  [2/26], [94mLoss[0m : 2.36728
[1mStep[0m  [4/26], [94mLoss[0m : 2.42768
[1mStep[0m  [6/26], [94mLoss[0m : 2.49187
[1mStep[0m  [8/26], [94mLoss[0m : 2.52942
[1mStep[0m  [10/26], [94mLoss[0m : 2.30807
[1mStep[0m  [12/26], [94mLoss[0m : 2.43107
[1mStep[0m  [14/26], [94mLoss[0m : 2.48340
[1mStep[0m  [16/26], [94mLoss[0m : 2.53650
[1mStep[0m  [18/26], [94mLoss[0m : 2.42449
[1mStep[0m  [20/26], [94mLoss[0m : 2.58586
[1mStep[0m  [22/26], [94mLoss[0m : 2.65995
[1mStep[0m  [24/26], [94mLoss[0m : 2.41161

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55955
[1mStep[0m  [2/26], [94mLoss[0m : 2.42984
[1mStep[0m  [4/26], [94mLoss[0m : 2.49075
[1mStep[0m  [6/26], [94mLoss[0m : 2.35188
[1mStep[0m  [8/26], [94mLoss[0m : 2.42827
[1mStep[0m  [10/26], [94mLoss[0m : 2.41397
[1mStep[0m  [12/26], [94mLoss[0m : 2.57686
[1mStep[0m  [14/26], [94mLoss[0m : 2.40583
[1mStep[0m  [16/26], [94mLoss[0m : 2.50952
[1mStep[0m  [18/26], [94mLoss[0m : 2.44772
[1mStep[0m  [20/26], [94mLoss[0m : 2.41618
[1mStep[0m  [22/26], [94mLoss[0m : 2.49525
[1mStep[0m  [24/26], [94mLoss[0m : 2.44042

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44012
[1mStep[0m  [2/26], [94mLoss[0m : 2.54952
[1mStep[0m  [4/26], [94mLoss[0m : 2.45950
[1mStep[0m  [6/26], [94mLoss[0m : 2.41810
[1mStep[0m  [8/26], [94mLoss[0m : 2.42266
[1mStep[0m  [10/26], [94mLoss[0m : 2.48580
[1mStep[0m  [12/26], [94mLoss[0m : 2.37062
[1mStep[0m  [14/26], [94mLoss[0m : 2.33038
[1mStep[0m  [16/26], [94mLoss[0m : 2.43093
[1mStep[0m  [18/26], [94mLoss[0m : 2.56731
[1mStep[0m  [20/26], [94mLoss[0m : 2.40354
[1mStep[0m  [22/26], [94mLoss[0m : 2.43931
[1mStep[0m  [24/26], [94mLoss[0m : 2.40008

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42075
[1mStep[0m  [2/26], [94mLoss[0m : 2.41524
[1mStep[0m  [4/26], [94mLoss[0m : 2.47183
[1mStep[0m  [6/26], [94mLoss[0m : 2.47634
[1mStep[0m  [8/26], [94mLoss[0m : 2.46702
[1mStep[0m  [10/26], [94mLoss[0m : 2.47619
[1mStep[0m  [12/26], [94mLoss[0m : 2.54023
[1mStep[0m  [14/26], [94mLoss[0m : 2.34384
[1mStep[0m  [16/26], [94mLoss[0m : 2.45435
[1mStep[0m  [18/26], [94mLoss[0m : 2.45388
[1mStep[0m  [20/26], [94mLoss[0m : 2.52672
[1mStep[0m  [22/26], [94mLoss[0m : 2.37973
[1mStep[0m  [24/26], [94mLoss[0m : 2.27748

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.393, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42698
[1mStep[0m  [2/26], [94mLoss[0m : 2.47724
[1mStep[0m  [4/26], [94mLoss[0m : 2.32570
[1mStep[0m  [6/26], [94mLoss[0m : 2.51560
[1mStep[0m  [8/26], [94mLoss[0m : 2.40895
[1mStep[0m  [10/26], [94mLoss[0m : 2.35363
[1mStep[0m  [12/26], [94mLoss[0m : 2.46777
[1mStep[0m  [14/26], [94mLoss[0m : 2.50609
[1mStep[0m  [16/26], [94mLoss[0m : 2.43556
[1mStep[0m  [18/26], [94mLoss[0m : 2.24904
[1mStep[0m  [20/26], [94mLoss[0m : 2.36017
[1mStep[0m  [22/26], [94mLoss[0m : 2.49411
[1mStep[0m  [24/26], [94mLoss[0m : 2.49686

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43056
[1mStep[0m  [2/26], [94mLoss[0m : 2.44865
[1mStep[0m  [4/26], [94mLoss[0m : 2.48055
[1mStep[0m  [6/26], [94mLoss[0m : 2.40182
[1mStep[0m  [8/26], [94mLoss[0m : 2.40833
[1mStep[0m  [10/26], [94mLoss[0m : 2.53046
[1mStep[0m  [12/26], [94mLoss[0m : 2.53772
[1mStep[0m  [14/26], [94mLoss[0m : 2.36437
[1mStep[0m  [16/26], [94mLoss[0m : 2.51534
[1mStep[0m  [18/26], [94mLoss[0m : 2.31640
[1mStep[0m  [20/26], [94mLoss[0m : 2.41719
[1mStep[0m  [22/26], [94mLoss[0m : 2.54145
[1mStep[0m  [24/26], [94mLoss[0m : 2.45514

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.398, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42551
[1mStep[0m  [2/26], [94mLoss[0m : 2.37148
[1mStep[0m  [4/26], [94mLoss[0m : 2.26314
[1mStep[0m  [6/26], [94mLoss[0m : 2.42171
[1mStep[0m  [8/26], [94mLoss[0m : 2.38572
[1mStep[0m  [10/26], [94mLoss[0m : 2.37125
[1mStep[0m  [12/26], [94mLoss[0m : 2.34726
[1mStep[0m  [14/26], [94mLoss[0m : 2.37821
[1mStep[0m  [16/26], [94mLoss[0m : 2.39694
[1mStep[0m  [18/26], [94mLoss[0m : 2.40407
[1mStep[0m  [20/26], [94mLoss[0m : 2.52776
[1mStep[0m  [22/26], [94mLoss[0m : 2.38941
[1mStep[0m  [24/26], [94mLoss[0m : 2.46014

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.382, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37196
[1mStep[0m  [2/26], [94mLoss[0m : 2.30718
[1mStep[0m  [4/26], [94mLoss[0m : 2.49932
[1mStep[0m  [6/26], [94mLoss[0m : 2.52840
[1mStep[0m  [8/26], [94mLoss[0m : 2.42426
[1mStep[0m  [10/26], [94mLoss[0m : 2.37172
[1mStep[0m  [12/26], [94mLoss[0m : 2.43877
[1mStep[0m  [14/26], [94mLoss[0m : 2.30672
[1mStep[0m  [16/26], [94mLoss[0m : 2.24832
[1mStep[0m  [18/26], [94mLoss[0m : 2.31430
[1mStep[0m  [20/26], [94mLoss[0m : 2.35018
[1mStep[0m  [22/26], [94mLoss[0m : 2.50155
[1mStep[0m  [24/26], [94mLoss[0m : 2.52805

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.381, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39083
[1mStep[0m  [2/26], [94mLoss[0m : 2.43886
[1mStep[0m  [4/26], [94mLoss[0m : 2.47543
[1mStep[0m  [6/26], [94mLoss[0m : 2.53070
[1mStep[0m  [8/26], [94mLoss[0m : 2.52161
[1mStep[0m  [10/26], [94mLoss[0m : 2.42381
[1mStep[0m  [12/26], [94mLoss[0m : 2.40664
[1mStep[0m  [14/26], [94mLoss[0m : 2.52235
[1mStep[0m  [16/26], [94mLoss[0m : 2.35758
[1mStep[0m  [18/26], [94mLoss[0m : 2.41408
[1mStep[0m  [20/26], [94mLoss[0m : 2.48105
[1mStep[0m  [22/26], [94mLoss[0m : 2.42030
[1mStep[0m  [24/26], [94mLoss[0m : 2.38428

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34023
[1mStep[0m  [2/26], [94mLoss[0m : 2.54746
[1mStep[0m  [4/26], [94mLoss[0m : 2.43226
[1mStep[0m  [6/26], [94mLoss[0m : 2.51623
[1mStep[0m  [8/26], [94mLoss[0m : 2.57952
[1mStep[0m  [10/26], [94mLoss[0m : 2.39729
[1mStep[0m  [12/26], [94mLoss[0m : 2.40529
[1mStep[0m  [14/26], [94mLoss[0m : 2.52551
[1mStep[0m  [16/26], [94mLoss[0m : 2.40618
[1mStep[0m  [18/26], [94mLoss[0m : 2.44747
[1mStep[0m  [20/26], [94mLoss[0m : 2.49229
[1mStep[0m  [22/26], [94mLoss[0m : 2.32588
[1mStep[0m  [24/26], [94mLoss[0m : 2.46408

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.365, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55645
[1mStep[0m  [2/26], [94mLoss[0m : 2.43139
[1mStep[0m  [4/26], [94mLoss[0m : 2.27817
[1mStep[0m  [6/26], [94mLoss[0m : 2.36754
[1mStep[0m  [8/26], [94mLoss[0m : 2.49506
[1mStep[0m  [10/26], [94mLoss[0m : 2.36767
[1mStep[0m  [12/26], [94mLoss[0m : 2.39745
[1mStep[0m  [14/26], [94mLoss[0m : 2.46919
[1mStep[0m  [16/26], [94mLoss[0m : 2.31917
[1mStep[0m  [18/26], [94mLoss[0m : 2.42435
[1mStep[0m  [20/26], [94mLoss[0m : 2.54566
[1mStep[0m  [22/26], [94mLoss[0m : 2.30988
[1mStep[0m  [24/26], [94mLoss[0m : 2.34154

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.374, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35471
[1mStep[0m  [2/26], [94mLoss[0m : 2.36693
[1mStep[0m  [4/26], [94mLoss[0m : 2.30862
[1mStep[0m  [6/26], [94mLoss[0m : 2.42476
[1mStep[0m  [8/26], [94mLoss[0m : 2.53507
[1mStep[0m  [10/26], [94mLoss[0m : 2.46134
[1mStep[0m  [12/26], [94mLoss[0m : 2.36911
[1mStep[0m  [14/26], [94mLoss[0m : 2.49227
[1mStep[0m  [16/26], [94mLoss[0m : 2.41932
[1mStep[0m  [18/26], [94mLoss[0m : 2.38874
[1mStep[0m  [20/26], [94mLoss[0m : 2.28357
[1mStep[0m  [22/26], [94mLoss[0m : 2.32909
[1mStep[0m  [24/26], [94mLoss[0m : 2.43269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.370, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44598
[1mStep[0m  [2/26], [94mLoss[0m : 2.34117
[1mStep[0m  [4/26], [94mLoss[0m : 2.42218
[1mStep[0m  [6/26], [94mLoss[0m : 2.35731
[1mStep[0m  [8/26], [94mLoss[0m : 2.40048
[1mStep[0m  [10/26], [94mLoss[0m : 2.34297
[1mStep[0m  [12/26], [94mLoss[0m : 2.27200
[1mStep[0m  [14/26], [94mLoss[0m : 2.40630
[1mStep[0m  [16/26], [94mLoss[0m : 2.25709
[1mStep[0m  [18/26], [94mLoss[0m : 2.57126
[1mStep[0m  [20/26], [94mLoss[0m : 2.53532
[1mStep[0m  [22/26], [94mLoss[0m : 2.47653
[1mStep[0m  [24/26], [94mLoss[0m : 2.44069

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.370, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37574
[1mStep[0m  [2/26], [94mLoss[0m : 2.33391
[1mStep[0m  [4/26], [94mLoss[0m : 2.44420
[1mStep[0m  [6/26], [94mLoss[0m : 2.30675
[1mStep[0m  [8/26], [94mLoss[0m : 2.36025
[1mStep[0m  [10/26], [94mLoss[0m : 2.31801
[1mStep[0m  [12/26], [94mLoss[0m : 2.44061
[1mStep[0m  [14/26], [94mLoss[0m : 2.42105
[1mStep[0m  [16/26], [94mLoss[0m : 2.32233
[1mStep[0m  [18/26], [94mLoss[0m : 2.52677
[1mStep[0m  [20/26], [94mLoss[0m : 2.33876
[1mStep[0m  [22/26], [94mLoss[0m : 2.45400
[1mStep[0m  [24/26], [94mLoss[0m : 2.40381

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.371
====================================

Phase 1 - Evaluation MAE:  2.3707656126755934
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.20795
[1mStep[0m  [2/26], [94mLoss[0m : 2.46766
[1mStep[0m  [4/26], [94mLoss[0m : 2.32017
[1mStep[0m  [6/26], [94mLoss[0m : 2.39132
[1mStep[0m  [8/26], [94mLoss[0m : 2.54253
[1mStep[0m  [10/26], [94mLoss[0m : 2.48234
[1mStep[0m  [12/26], [94mLoss[0m : 2.51876
[1mStep[0m  [14/26], [94mLoss[0m : 2.58213
[1mStep[0m  [16/26], [94mLoss[0m : 2.67561
[1mStep[0m  [18/26], [94mLoss[0m : 2.39268
[1mStep[0m  [20/26], [94mLoss[0m : 2.41587
[1mStep[0m  [22/26], [94mLoss[0m : 2.32412
[1mStep[0m  [24/26], [94mLoss[0m : 2.53288

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32730
[1mStep[0m  [2/26], [94mLoss[0m : 2.41263
[1mStep[0m  [4/26], [94mLoss[0m : 2.41525
[1mStep[0m  [6/26], [94mLoss[0m : 2.55044
[1mStep[0m  [8/26], [94mLoss[0m : 2.52287
[1mStep[0m  [10/26], [94mLoss[0m : 2.28107
[1mStep[0m  [12/26], [94mLoss[0m : 2.25200
[1mStep[0m  [14/26], [94mLoss[0m : 2.41058
[1mStep[0m  [16/26], [94mLoss[0m : 2.23713
[1mStep[0m  [18/26], [94mLoss[0m : 2.36330
[1mStep[0m  [20/26], [94mLoss[0m : 2.27393
[1mStep[0m  [22/26], [94mLoss[0m : 2.46922
[1mStep[0m  [24/26], [94mLoss[0m : 2.49736

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39994
[1mStep[0m  [2/26], [94mLoss[0m : 2.36780
[1mStep[0m  [4/26], [94mLoss[0m : 2.32800
[1mStep[0m  [6/26], [94mLoss[0m : 2.12890
[1mStep[0m  [8/26], [94mLoss[0m : 2.39064
[1mStep[0m  [10/26], [94mLoss[0m : 2.17126
[1mStep[0m  [12/26], [94mLoss[0m : 2.15072
[1mStep[0m  [14/26], [94mLoss[0m : 2.30226
[1mStep[0m  [16/26], [94mLoss[0m : 2.34720
[1mStep[0m  [18/26], [94mLoss[0m : 2.38733
[1mStep[0m  [20/26], [94mLoss[0m : 2.37717
[1mStep[0m  [22/26], [94mLoss[0m : 2.35681
[1mStep[0m  [24/26], [94mLoss[0m : 2.33566

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30064
[1mStep[0m  [2/26], [94mLoss[0m : 2.18363
[1mStep[0m  [4/26], [94mLoss[0m : 2.20681
[1mStep[0m  [6/26], [94mLoss[0m : 2.28114
[1mStep[0m  [8/26], [94mLoss[0m : 2.17426
[1mStep[0m  [10/26], [94mLoss[0m : 2.12172
[1mStep[0m  [12/26], [94mLoss[0m : 2.30485
[1mStep[0m  [14/26], [94mLoss[0m : 2.25520
[1mStep[0m  [16/26], [94mLoss[0m : 2.00306
[1mStep[0m  [18/26], [94mLoss[0m : 2.23769
[1mStep[0m  [20/26], [94mLoss[0m : 2.38648
[1mStep[0m  [22/26], [94mLoss[0m : 2.15319
[1mStep[0m  [24/26], [94mLoss[0m : 2.25841

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.00407
[1mStep[0m  [2/26], [94mLoss[0m : 2.08703
[1mStep[0m  [4/26], [94mLoss[0m : 1.94218
[1mStep[0m  [6/26], [94mLoss[0m : 2.12653
[1mStep[0m  [8/26], [94mLoss[0m : 2.16036
[1mStep[0m  [10/26], [94mLoss[0m : 2.15441
[1mStep[0m  [12/26], [94mLoss[0m : 2.16924
[1mStep[0m  [14/26], [94mLoss[0m : 2.34258
[1mStep[0m  [16/26], [94mLoss[0m : 2.26395
[1mStep[0m  [18/26], [94mLoss[0m : 2.07767
[1mStep[0m  [20/26], [94mLoss[0m : 2.14611
[1mStep[0m  [22/26], [94mLoss[0m : 2.07421
[1mStep[0m  [24/26], [94mLoss[0m : 2.22964

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19377
[1mStep[0m  [2/26], [94mLoss[0m : 2.04143
[1mStep[0m  [4/26], [94mLoss[0m : 2.08928
[1mStep[0m  [6/26], [94mLoss[0m : 1.92813
[1mStep[0m  [8/26], [94mLoss[0m : 2.03653
[1mStep[0m  [10/26], [94mLoss[0m : 2.16263
[1mStep[0m  [12/26], [94mLoss[0m : 1.97423
[1mStep[0m  [14/26], [94mLoss[0m : 2.02730
[1mStep[0m  [16/26], [94mLoss[0m : 2.17456
[1mStep[0m  [18/26], [94mLoss[0m : 2.09967
[1mStep[0m  [20/26], [94mLoss[0m : 2.15210
[1mStep[0m  [22/26], [94mLoss[0m : 2.17927
[1mStep[0m  [24/26], [94mLoss[0m : 1.98613

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01191
[1mStep[0m  [2/26], [94mLoss[0m : 2.18060
[1mStep[0m  [4/26], [94mLoss[0m : 1.93043
[1mStep[0m  [6/26], [94mLoss[0m : 2.00714
[1mStep[0m  [8/26], [94mLoss[0m : 2.16187
[1mStep[0m  [10/26], [94mLoss[0m : 2.19311
[1mStep[0m  [12/26], [94mLoss[0m : 2.13478
[1mStep[0m  [14/26], [94mLoss[0m : 1.90683
[1mStep[0m  [16/26], [94mLoss[0m : 2.11145
[1mStep[0m  [18/26], [94mLoss[0m : 2.04845
[1mStep[0m  [20/26], [94mLoss[0m : 1.98223
[1mStep[0m  [22/26], [94mLoss[0m : 2.05163
[1mStep[0m  [24/26], [94mLoss[0m : 1.86756

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.87586
[1mStep[0m  [2/26], [94mLoss[0m : 1.93364
[1mStep[0m  [4/26], [94mLoss[0m : 1.85308
[1mStep[0m  [6/26], [94mLoss[0m : 1.82301
[1mStep[0m  [8/26], [94mLoss[0m : 2.00865
[1mStep[0m  [10/26], [94mLoss[0m : 1.96911
[1mStep[0m  [12/26], [94mLoss[0m : 2.05312
[1mStep[0m  [14/26], [94mLoss[0m : 1.89564
[1mStep[0m  [16/26], [94mLoss[0m : 1.98165
[1mStep[0m  [18/26], [94mLoss[0m : 2.07474
[1mStep[0m  [20/26], [94mLoss[0m : 2.09730
[1mStep[0m  [22/26], [94mLoss[0m : 1.91928
[1mStep[0m  [24/26], [94mLoss[0m : 1.99609

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.942, [92mTest[0m: 2.499, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.00514
[1mStep[0m  [2/26], [94mLoss[0m : 1.93434
[1mStep[0m  [4/26], [94mLoss[0m : 1.71759
[1mStep[0m  [6/26], [94mLoss[0m : 1.87779
[1mStep[0m  [8/26], [94mLoss[0m : 1.92284
[1mStep[0m  [10/26], [94mLoss[0m : 1.92410
[1mStep[0m  [12/26], [94mLoss[0m : 1.80087
[1mStep[0m  [14/26], [94mLoss[0m : 1.89055
[1mStep[0m  [16/26], [94mLoss[0m : 1.81367
[1mStep[0m  [18/26], [94mLoss[0m : 1.87612
[1mStep[0m  [20/26], [94mLoss[0m : 1.90787
[1mStep[0m  [22/26], [94mLoss[0m : 1.91104
[1mStep[0m  [24/26], [94mLoss[0m : 1.90462

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.83857
[1mStep[0m  [2/26], [94mLoss[0m : 1.80439
[1mStep[0m  [4/26], [94mLoss[0m : 1.74896
[1mStep[0m  [6/26], [94mLoss[0m : 1.79958
[1mStep[0m  [8/26], [94mLoss[0m : 1.82778
[1mStep[0m  [10/26], [94mLoss[0m : 1.78509
[1mStep[0m  [12/26], [94mLoss[0m : 1.78959
[1mStep[0m  [14/26], [94mLoss[0m : 1.84608
[1mStep[0m  [16/26], [94mLoss[0m : 1.91056
[1mStep[0m  [18/26], [94mLoss[0m : 1.98478
[1mStep[0m  [20/26], [94mLoss[0m : 1.83499
[1mStep[0m  [22/26], [94mLoss[0m : 1.93067
[1mStep[0m  [24/26], [94mLoss[0m : 1.89982

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.78329
[1mStep[0m  [2/26], [94mLoss[0m : 1.79928
[1mStep[0m  [4/26], [94mLoss[0m : 1.73831
[1mStep[0m  [6/26], [94mLoss[0m : 1.84130
[1mStep[0m  [8/26], [94mLoss[0m : 1.74037
[1mStep[0m  [10/26], [94mLoss[0m : 1.83272
[1mStep[0m  [12/26], [94mLoss[0m : 1.91623
[1mStep[0m  [14/26], [94mLoss[0m : 1.87999
[1mStep[0m  [16/26], [94mLoss[0m : 1.69569
[1mStep[0m  [18/26], [94mLoss[0m : 1.70209
[1mStep[0m  [20/26], [94mLoss[0m : 1.89378
[1mStep[0m  [22/26], [94mLoss[0m : 1.83156
[1mStep[0m  [24/26], [94mLoss[0m : 1.85150

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.79020
[1mStep[0m  [2/26], [94mLoss[0m : 1.74830
[1mStep[0m  [4/26], [94mLoss[0m : 1.64588
[1mStep[0m  [6/26], [94mLoss[0m : 1.73306
[1mStep[0m  [8/26], [94mLoss[0m : 1.64784
[1mStep[0m  [10/26], [94mLoss[0m : 1.91574
[1mStep[0m  [12/26], [94mLoss[0m : 1.69078
[1mStep[0m  [14/26], [94mLoss[0m : 1.64889
[1mStep[0m  [16/26], [94mLoss[0m : 1.77686
[1mStep[0m  [18/26], [94mLoss[0m : 1.78114
[1mStep[0m  [20/26], [94mLoss[0m : 1.85085
[1mStep[0m  [22/26], [94mLoss[0m : 1.75387
[1mStep[0m  [24/26], [94mLoss[0m : 1.71981

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.499, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.65757
[1mStep[0m  [2/26], [94mLoss[0m : 1.64214
[1mStep[0m  [4/26], [94mLoss[0m : 1.67100
[1mStep[0m  [6/26], [94mLoss[0m : 1.67366
[1mStep[0m  [8/26], [94mLoss[0m : 1.61686
[1mStep[0m  [10/26], [94mLoss[0m : 1.70838
[1mStep[0m  [12/26], [94mLoss[0m : 1.77005
[1mStep[0m  [14/26], [94mLoss[0m : 1.81669
[1mStep[0m  [16/26], [94mLoss[0m : 1.59458
[1mStep[0m  [18/26], [94mLoss[0m : 1.82732
[1mStep[0m  [20/26], [94mLoss[0m : 1.70640
[1mStep[0m  [22/26], [94mLoss[0m : 1.74740
[1mStep[0m  [24/26], [94mLoss[0m : 1.71904

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.63728
[1mStep[0m  [2/26], [94mLoss[0m : 1.67075
[1mStep[0m  [4/26], [94mLoss[0m : 1.68429
[1mStep[0m  [6/26], [94mLoss[0m : 1.65370
[1mStep[0m  [8/26], [94mLoss[0m : 1.60484
[1mStep[0m  [10/26], [94mLoss[0m : 1.57592
[1mStep[0m  [12/26], [94mLoss[0m : 1.62239
[1mStep[0m  [14/26], [94mLoss[0m : 1.75051
[1mStep[0m  [16/26], [94mLoss[0m : 1.70556
[1mStep[0m  [18/26], [94mLoss[0m : 1.66815
[1mStep[0m  [20/26], [94mLoss[0m : 1.70242
[1mStep[0m  [22/26], [94mLoss[0m : 1.76205
[1mStep[0m  [24/26], [94mLoss[0m : 1.67758

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61492
[1mStep[0m  [2/26], [94mLoss[0m : 1.54903
[1mStep[0m  [4/26], [94mLoss[0m : 1.56876
[1mStep[0m  [6/26], [94mLoss[0m : 1.54998
[1mStep[0m  [8/26], [94mLoss[0m : 1.57133
[1mStep[0m  [10/26], [94mLoss[0m : 1.60982
[1mStep[0m  [12/26], [94mLoss[0m : 1.55061
[1mStep[0m  [14/26], [94mLoss[0m : 1.68361
[1mStep[0m  [16/26], [94mLoss[0m : 1.60277
[1mStep[0m  [18/26], [94mLoss[0m : 1.65150
[1mStep[0m  [20/26], [94mLoss[0m : 1.56932
[1mStep[0m  [22/26], [94mLoss[0m : 1.58430
[1mStep[0m  [24/26], [94mLoss[0m : 1.76904

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49942
[1mStep[0m  [2/26], [94mLoss[0m : 1.60561
[1mStep[0m  [4/26], [94mLoss[0m : 1.47922
[1mStep[0m  [6/26], [94mLoss[0m : 1.64091
[1mStep[0m  [8/26], [94mLoss[0m : 1.61784
[1mStep[0m  [10/26], [94mLoss[0m : 1.66130
[1mStep[0m  [12/26], [94mLoss[0m : 1.48772
[1mStep[0m  [14/26], [94mLoss[0m : 1.56025
[1mStep[0m  [16/26], [94mLoss[0m : 1.64690
[1mStep[0m  [18/26], [94mLoss[0m : 1.57456
[1mStep[0m  [20/26], [94mLoss[0m : 1.52753
[1mStep[0m  [22/26], [94mLoss[0m : 1.52168
[1mStep[0m  [24/26], [94mLoss[0m : 1.54952

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.547, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53246
[1mStep[0m  [2/26], [94mLoss[0m : 1.41721
[1mStep[0m  [4/26], [94mLoss[0m : 1.60299
[1mStep[0m  [6/26], [94mLoss[0m : 1.53870
[1mStep[0m  [8/26], [94mLoss[0m : 1.59527
[1mStep[0m  [10/26], [94mLoss[0m : 1.56726
[1mStep[0m  [12/26], [94mLoss[0m : 1.48481
[1mStep[0m  [14/26], [94mLoss[0m : 1.57438
[1mStep[0m  [16/26], [94mLoss[0m : 1.72099
[1mStep[0m  [18/26], [94mLoss[0m : 1.67646
[1mStep[0m  [20/26], [94mLoss[0m : 1.56860
[1mStep[0m  [22/26], [94mLoss[0m : 1.52372
[1mStep[0m  [24/26], [94mLoss[0m : 1.65536

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.39144
[1mStep[0m  [2/26], [94mLoss[0m : 1.51531
[1mStep[0m  [4/26], [94mLoss[0m : 1.42984
[1mStep[0m  [6/26], [94mLoss[0m : 1.72317
[1mStep[0m  [8/26], [94mLoss[0m : 1.50207
[1mStep[0m  [10/26], [94mLoss[0m : 1.59657
[1mStep[0m  [12/26], [94mLoss[0m : 1.60431
[1mStep[0m  [14/26], [94mLoss[0m : 1.65499
[1mStep[0m  [16/26], [94mLoss[0m : 1.60233
[1mStep[0m  [18/26], [94mLoss[0m : 1.52519
[1mStep[0m  [20/26], [94mLoss[0m : 1.52553
[1mStep[0m  [22/26], [94mLoss[0m : 1.47896
[1mStep[0m  [24/26], [94mLoss[0m : 1.56944

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52401
[1mStep[0m  [2/26], [94mLoss[0m : 1.41988
[1mStep[0m  [4/26], [94mLoss[0m : 1.48237
[1mStep[0m  [6/26], [94mLoss[0m : 1.58828
[1mStep[0m  [8/26], [94mLoss[0m : 1.44462
[1mStep[0m  [10/26], [94mLoss[0m : 1.45896
[1mStep[0m  [12/26], [94mLoss[0m : 1.54064
[1mStep[0m  [14/26], [94mLoss[0m : 1.50617
[1mStep[0m  [16/26], [94mLoss[0m : 1.50141
[1mStep[0m  [18/26], [94mLoss[0m : 1.31667
[1mStep[0m  [20/26], [94mLoss[0m : 1.44919
[1mStep[0m  [22/26], [94mLoss[0m : 1.41301
[1mStep[0m  [24/26], [94mLoss[0m : 1.55652

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.50605
[1mStep[0m  [2/26], [94mLoss[0m : 1.43485
[1mStep[0m  [4/26], [94mLoss[0m : 1.42265
[1mStep[0m  [6/26], [94mLoss[0m : 1.42886
[1mStep[0m  [8/26], [94mLoss[0m : 1.42414
[1mStep[0m  [10/26], [94mLoss[0m : 1.46739
[1mStep[0m  [12/26], [94mLoss[0m : 1.44316
[1mStep[0m  [14/26], [94mLoss[0m : 1.58743
[1mStep[0m  [16/26], [94mLoss[0m : 1.52937
[1mStep[0m  [18/26], [94mLoss[0m : 1.45639
[1mStep[0m  [20/26], [94mLoss[0m : 1.47431
[1mStep[0m  [22/26], [94mLoss[0m : 1.50278
[1mStep[0m  [24/26], [94mLoss[0m : 1.57958

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.479, [92mTest[0m: 2.453, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.45396
[1mStep[0m  [2/26], [94mLoss[0m : 1.33187
[1mStep[0m  [4/26], [94mLoss[0m : 1.42016
[1mStep[0m  [6/26], [94mLoss[0m : 1.37488
[1mStep[0m  [8/26], [94mLoss[0m : 1.47070
[1mStep[0m  [10/26], [94mLoss[0m : 1.40865
[1mStep[0m  [12/26], [94mLoss[0m : 1.37054
[1mStep[0m  [14/26], [94mLoss[0m : 1.50234
[1mStep[0m  [16/26], [94mLoss[0m : 1.36819
[1mStep[0m  [18/26], [94mLoss[0m : 1.43522
[1mStep[0m  [20/26], [94mLoss[0m : 1.53058
[1mStep[0m  [22/26], [94mLoss[0m : 1.42539
[1mStep[0m  [24/26], [94mLoss[0m : 1.38432

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.40519
[1mStep[0m  [2/26], [94mLoss[0m : 1.36502
[1mStep[0m  [4/26], [94mLoss[0m : 1.33346
[1mStep[0m  [6/26], [94mLoss[0m : 1.29412
[1mStep[0m  [8/26], [94mLoss[0m : 1.37272
[1mStep[0m  [10/26], [94mLoss[0m : 1.44296
[1mStep[0m  [12/26], [94mLoss[0m : 1.35542
[1mStep[0m  [14/26], [94mLoss[0m : 1.35256
[1mStep[0m  [16/26], [94mLoss[0m : 1.36341
[1mStep[0m  [18/26], [94mLoss[0m : 1.45213
[1mStep[0m  [20/26], [94mLoss[0m : 1.26517
[1mStep[0m  [22/26], [94mLoss[0m : 1.36106
[1mStep[0m  [24/26], [94mLoss[0m : 1.44798

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.388, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.23432
[1mStep[0m  [2/26], [94mLoss[0m : 1.34232
[1mStep[0m  [4/26], [94mLoss[0m : 1.37947
[1mStep[0m  [6/26], [94mLoss[0m : 1.36300
[1mStep[0m  [8/26], [94mLoss[0m : 1.22609
[1mStep[0m  [10/26], [94mLoss[0m : 1.32717
[1mStep[0m  [12/26], [94mLoss[0m : 1.33371
[1mStep[0m  [14/26], [94mLoss[0m : 1.37404
[1mStep[0m  [16/26], [94mLoss[0m : 1.35713
[1mStep[0m  [18/26], [94mLoss[0m : 1.36355
[1mStep[0m  [20/26], [94mLoss[0m : 1.28155
[1mStep[0m  [22/26], [94mLoss[0m : 1.51196
[1mStep[0m  [24/26], [94mLoss[0m : 1.39386

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.352, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.482
====================================

Phase 2 - Evaluation MAE:  2.481875236217792
MAE score P1      2.370766
MAE score P2      2.481875
loss              1.352151
learning_rate     0.007525
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.88852
[1mStep[0m  [2/26], [94mLoss[0m : 10.97078
[1mStep[0m  [4/26], [94mLoss[0m : 11.12932
[1mStep[0m  [6/26], [94mLoss[0m : 10.83531
[1mStep[0m  [8/26], [94mLoss[0m : 10.52297
[1mStep[0m  [10/26], [94mLoss[0m : 10.84724
[1mStep[0m  [12/26], [94mLoss[0m : 10.36672
[1mStep[0m  [14/26], [94mLoss[0m : 10.19354
[1mStep[0m  [16/26], [94mLoss[0m : 9.86788
[1mStep[0m  [18/26], [94mLoss[0m : 9.80231
[1mStep[0m  [20/26], [94mLoss[0m : 9.87218
[1mStep[0m  [22/26], [94mLoss[0m : 9.67531
[1mStep[0m  [24/26], [94mLoss[0m : 9.47282

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.282, [92mTest[0m: 10.775, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.08512
[1mStep[0m  [2/26], [94mLoss[0m : 8.73670
[1mStep[0m  [4/26], [94mLoss[0m : 8.51167
[1mStep[0m  [6/26], [94mLoss[0m : 8.57718
[1mStep[0m  [8/26], [94mLoss[0m : 8.39117
[1mStep[0m  [10/26], [94mLoss[0m : 8.15240
[1mStep[0m  [12/26], [94mLoss[0m : 8.07803
[1mStep[0m  [14/26], [94mLoss[0m : 7.68760
[1mStep[0m  [16/26], [94mLoss[0m : 7.34553
[1mStep[0m  [18/26], [94mLoss[0m : 7.08980
[1mStep[0m  [20/26], [94mLoss[0m : 6.66494
[1mStep[0m  [22/26], [94mLoss[0m : 6.67351
[1mStep[0m  [24/26], [94mLoss[0m : 5.90421

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.704, [92mTest[0m: 8.931, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.88047
[1mStep[0m  [2/26], [94mLoss[0m : 5.57273
[1mStep[0m  [4/26], [94mLoss[0m : 5.36565
[1mStep[0m  [6/26], [94mLoss[0m : 5.33152
[1mStep[0m  [8/26], [94mLoss[0m : 4.82805
[1mStep[0m  [10/26], [94mLoss[0m : 4.57741
[1mStep[0m  [12/26], [94mLoss[0m : 4.33109
[1mStep[0m  [14/26], [94mLoss[0m : 3.99606
[1mStep[0m  [16/26], [94mLoss[0m : 3.43042
[1mStep[0m  [18/26], [94mLoss[0m : 3.27420
[1mStep[0m  [20/26], [94mLoss[0m : 3.15823
[1mStep[0m  [22/26], [94mLoss[0m : 3.03540
[1mStep[0m  [24/26], [94mLoss[0m : 2.88796

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.216, [92mTest[0m: 6.180, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71506
[1mStep[0m  [2/26], [94mLoss[0m : 2.86436
[1mStep[0m  [4/26], [94mLoss[0m : 2.85664
[1mStep[0m  [6/26], [94mLoss[0m : 2.84716
[1mStep[0m  [8/26], [94mLoss[0m : 2.64547
[1mStep[0m  [10/26], [94mLoss[0m : 2.74157
[1mStep[0m  [12/26], [94mLoss[0m : 2.92010
[1mStep[0m  [14/26], [94mLoss[0m : 2.76266
[1mStep[0m  [16/26], [94mLoss[0m : 2.85883
[1mStep[0m  [18/26], [94mLoss[0m : 2.83310
[1mStep[0m  [20/26], [94mLoss[0m : 2.80974
[1mStep[0m  [22/26], [94mLoss[0m : 2.71026
[1mStep[0m  [24/26], [94mLoss[0m : 2.81955

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.787, [92mTest[0m: 2.660, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55960
[1mStep[0m  [2/26], [94mLoss[0m : 2.54636
[1mStep[0m  [4/26], [94mLoss[0m : 2.68490
[1mStep[0m  [6/26], [94mLoss[0m : 2.61171
[1mStep[0m  [8/26], [94mLoss[0m : 2.72036
[1mStep[0m  [10/26], [94mLoss[0m : 2.63391
[1mStep[0m  [12/26], [94mLoss[0m : 2.77645
[1mStep[0m  [14/26], [94mLoss[0m : 2.71213
[1mStep[0m  [16/26], [94mLoss[0m : 2.68146
[1mStep[0m  [18/26], [94mLoss[0m : 2.57311
[1mStep[0m  [20/26], [94mLoss[0m : 2.81706
[1mStep[0m  [22/26], [94mLoss[0m : 2.90021
[1mStep[0m  [24/26], [94mLoss[0m : 2.72595

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70115
[1mStep[0m  [2/26], [94mLoss[0m : 2.70845
[1mStep[0m  [4/26], [94mLoss[0m : 2.54326
[1mStep[0m  [6/26], [94mLoss[0m : 2.73110
[1mStep[0m  [8/26], [94mLoss[0m : 2.58158
[1mStep[0m  [10/26], [94mLoss[0m : 2.41845
[1mStep[0m  [12/26], [94mLoss[0m : 2.70551
[1mStep[0m  [14/26], [94mLoss[0m : 2.77191
[1mStep[0m  [16/26], [94mLoss[0m : 2.63760
[1mStep[0m  [18/26], [94mLoss[0m : 2.80621
[1mStep[0m  [20/26], [94mLoss[0m : 2.69195
[1mStep[0m  [22/26], [94mLoss[0m : 2.74931
[1mStep[0m  [24/26], [94mLoss[0m : 2.62366

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75811
[1mStep[0m  [2/26], [94mLoss[0m : 2.61505
[1mStep[0m  [4/26], [94mLoss[0m : 2.64213
[1mStep[0m  [6/26], [94mLoss[0m : 2.60147
[1mStep[0m  [8/26], [94mLoss[0m : 2.63215
[1mStep[0m  [10/26], [94mLoss[0m : 2.70459
[1mStep[0m  [12/26], [94mLoss[0m : 2.47045
[1mStep[0m  [14/26], [94mLoss[0m : 2.71008
[1mStep[0m  [16/26], [94mLoss[0m : 2.58746
[1mStep[0m  [18/26], [94mLoss[0m : 2.66697
[1mStep[0m  [20/26], [94mLoss[0m : 2.50977
[1mStep[0m  [22/26], [94mLoss[0m : 2.58557
[1mStep[0m  [24/26], [94mLoss[0m : 2.51204

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49706
[1mStep[0m  [2/26], [94mLoss[0m : 2.60782
[1mStep[0m  [4/26], [94mLoss[0m : 2.53988
[1mStep[0m  [6/26], [94mLoss[0m : 2.52807
[1mStep[0m  [8/26], [94mLoss[0m : 2.53367
[1mStep[0m  [10/26], [94mLoss[0m : 2.51777
[1mStep[0m  [12/26], [94mLoss[0m : 2.73207
[1mStep[0m  [14/26], [94mLoss[0m : 2.54142
[1mStep[0m  [16/26], [94mLoss[0m : 2.47110
[1mStep[0m  [18/26], [94mLoss[0m : 2.59008
[1mStep[0m  [20/26], [94mLoss[0m : 2.60660
[1mStep[0m  [22/26], [94mLoss[0m : 2.53622
[1mStep[0m  [24/26], [94mLoss[0m : 2.76763

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72427
[1mStep[0m  [2/26], [94mLoss[0m : 2.64002
[1mStep[0m  [4/26], [94mLoss[0m : 2.47949
[1mStep[0m  [6/26], [94mLoss[0m : 2.55541
[1mStep[0m  [8/26], [94mLoss[0m : 2.49834
[1mStep[0m  [10/26], [94mLoss[0m : 2.56015
[1mStep[0m  [12/26], [94mLoss[0m : 2.53447
[1mStep[0m  [14/26], [94mLoss[0m : 2.66154
[1mStep[0m  [16/26], [94mLoss[0m : 2.68712
[1mStep[0m  [18/26], [94mLoss[0m : 2.65772
[1mStep[0m  [20/26], [94mLoss[0m : 2.55057
[1mStep[0m  [22/26], [94mLoss[0m : 2.60087
[1mStep[0m  [24/26], [94mLoss[0m : 2.60009

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40434
[1mStep[0m  [2/26], [94mLoss[0m : 2.60576
[1mStep[0m  [4/26], [94mLoss[0m : 2.52737
[1mStep[0m  [6/26], [94mLoss[0m : 2.55072
[1mStep[0m  [8/26], [94mLoss[0m : 2.58971
[1mStep[0m  [10/26], [94mLoss[0m : 2.50433
[1mStep[0m  [12/26], [94mLoss[0m : 2.52637
[1mStep[0m  [14/26], [94mLoss[0m : 2.65891
[1mStep[0m  [16/26], [94mLoss[0m : 2.58363
[1mStep[0m  [18/26], [94mLoss[0m : 2.45561
[1mStep[0m  [20/26], [94mLoss[0m : 2.53330
[1mStep[0m  [22/26], [94mLoss[0m : 2.58714
[1mStep[0m  [24/26], [94mLoss[0m : 2.44967

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44972
[1mStep[0m  [2/26], [94mLoss[0m : 2.56084
[1mStep[0m  [4/26], [94mLoss[0m : 2.56754
[1mStep[0m  [6/26], [94mLoss[0m : 2.63897
[1mStep[0m  [8/26], [94mLoss[0m : 2.57336
[1mStep[0m  [10/26], [94mLoss[0m : 2.65933
[1mStep[0m  [12/26], [94mLoss[0m : 2.46259
[1mStep[0m  [14/26], [94mLoss[0m : 2.48461
[1mStep[0m  [16/26], [94mLoss[0m : 2.63602
[1mStep[0m  [18/26], [94mLoss[0m : 2.68832
[1mStep[0m  [20/26], [94mLoss[0m : 2.37951
[1mStep[0m  [22/26], [94mLoss[0m : 2.38907
[1mStep[0m  [24/26], [94mLoss[0m : 2.65266

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68955
[1mStep[0m  [2/26], [94mLoss[0m : 2.61940
[1mStep[0m  [4/26], [94mLoss[0m : 2.62056
[1mStep[0m  [6/26], [94mLoss[0m : 2.61004
[1mStep[0m  [8/26], [94mLoss[0m : 2.55865
[1mStep[0m  [10/26], [94mLoss[0m : 2.45991
[1mStep[0m  [12/26], [94mLoss[0m : 2.56003
[1mStep[0m  [14/26], [94mLoss[0m : 2.45514
[1mStep[0m  [16/26], [94mLoss[0m : 2.64620
[1mStep[0m  [18/26], [94mLoss[0m : 2.64971
[1mStep[0m  [20/26], [94mLoss[0m : 2.44431
[1mStep[0m  [22/26], [94mLoss[0m : 2.52417
[1mStep[0m  [24/26], [94mLoss[0m : 2.31846

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62214
[1mStep[0m  [2/26], [94mLoss[0m : 2.56444
[1mStep[0m  [4/26], [94mLoss[0m : 2.39605
[1mStep[0m  [6/26], [94mLoss[0m : 2.67526
[1mStep[0m  [8/26], [94mLoss[0m : 2.41626
[1mStep[0m  [10/26], [94mLoss[0m : 2.52741
[1mStep[0m  [12/26], [94mLoss[0m : 2.50132
[1mStep[0m  [14/26], [94mLoss[0m : 2.43123
[1mStep[0m  [16/26], [94mLoss[0m : 2.45632
[1mStep[0m  [18/26], [94mLoss[0m : 2.44017
[1mStep[0m  [20/26], [94mLoss[0m : 2.45936
[1mStep[0m  [22/26], [94mLoss[0m : 2.61118
[1mStep[0m  [24/26], [94mLoss[0m : 2.68680

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60118
[1mStep[0m  [2/26], [94mLoss[0m : 2.64721
[1mStep[0m  [4/26], [94mLoss[0m : 2.39918
[1mStep[0m  [6/26], [94mLoss[0m : 2.53083
[1mStep[0m  [8/26], [94mLoss[0m : 2.59937
[1mStep[0m  [10/26], [94mLoss[0m : 2.65174
[1mStep[0m  [12/26], [94mLoss[0m : 2.61587
[1mStep[0m  [14/26], [94mLoss[0m : 2.52405
[1mStep[0m  [16/26], [94mLoss[0m : 2.52352
[1mStep[0m  [18/26], [94mLoss[0m : 2.49317
[1mStep[0m  [20/26], [94mLoss[0m : 2.40120
[1mStep[0m  [22/26], [94mLoss[0m : 2.54855
[1mStep[0m  [24/26], [94mLoss[0m : 2.37915

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56121
[1mStep[0m  [2/26], [94mLoss[0m : 2.52217
[1mStep[0m  [4/26], [94mLoss[0m : 2.51996
[1mStep[0m  [6/26], [94mLoss[0m : 2.47260
[1mStep[0m  [8/26], [94mLoss[0m : 2.59631
[1mStep[0m  [10/26], [94mLoss[0m : 2.52701
[1mStep[0m  [12/26], [94mLoss[0m : 2.58493
[1mStep[0m  [14/26], [94mLoss[0m : 2.50054
[1mStep[0m  [16/26], [94mLoss[0m : 2.53253
[1mStep[0m  [18/26], [94mLoss[0m : 2.35353
[1mStep[0m  [20/26], [94mLoss[0m : 2.35032
[1mStep[0m  [22/26], [94mLoss[0m : 2.52230
[1mStep[0m  [24/26], [94mLoss[0m : 2.60762

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33251
[1mStep[0m  [2/26], [94mLoss[0m : 2.35654
[1mStep[0m  [4/26], [94mLoss[0m : 2.44530
[1mStep[0m  [6/26], [94mLoss[0m : 2.54397
[1mStep[0m  [8/26], [94mLoss[0m : 2.53141
[1mStep[0m  [10/26], [94mLoss[0m : 2.49195
[1mStep[0m  [12/26], [94mLoss[0m : 2.48760
[1mStep[0m  [14/26], [94mLoss[0m : 2.55181
[1mStep[0m  [16/26], [94mLoss[0m : 2.46242
[1mStep[0m  [18/26], [94mLoss[0m : 2.39606
[1mStep[0m  [20/26], [94mLoss[0m : 2.47403
[1mStep[0m  [22/26], [94mLoss[0m : 2.53502
[1mStep[0m  [24/26], [94mLoss[0m : 2.61763

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42551
[1mStep[0m  [2/26], [94mLoss[0m : 2.49963
[1mStep[0m  [4/26], [94mLoss[0m : 2.54509
[1mStep[0m  [6/26], [94mLoss[0m : 2.43703
[1mStep[0m  [8/26], [94mLoss[0m : 2.38716
[1mStep[0m  [10/26], [94mLoss[0m : 2.49374
[1mStep[0m  [12/26], [94mLoss[0m : 2.49437
[1mStep[0m  [14/26], [94mLoss[0m : 2.41535
[1mStep[0m  [16/26], [94mLoss[0m : 2.50684
[1mStep[0m  [18/26], [94mLoss[0m : 2.41763
[1mStep[0m  [20/26], [94mLoss[0m : 2.54923
[1mStep[0m  [22/26], [94mLoss[0m : 2.45003
[1mStep[0m  [24/26], [94mLoss[0m : 2.60643

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30362
[1mStep[0m  [2/26], [94mLoss[0m : 2.46940
[1mStep[0m  [4/26], [94mLoss[0m : 2.51829
[1mStep[0m  [6/26], [94mLoss[0m : 2.49528
[1mStep[0m  [8/26], [94mLoss[0m : 2.32005
[1mStep[0m  [10/26], [94mLoss[0m : 2.70683
[1mStep[0m  [12/26], [94mLoss[0m : 2.41231
[1mStep[0m  [14/26], [94mLoss[0m : 2.59339
[1mStep[0m  [16/26], [94mLoss[0m : 2.55683
[1mStep[0m  [18/26], [94mLoss[0m : 2.52631
[1mStep[0m  [20/26], [94mLoss[0m : 2.46294
[1mStep[0m  [22/26], [94mLoss[0m : 2.62447
[1mStep[0m  [24/26], [94mLoss[0m : 2.51339

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41233
[1mStep[0m  [2/26], [94mLoss[0m : 2.58147
[1mStep[0m  [4/26], [94mLoss[0m : 2.56433
[1mStep[0m  [6/26], [94mLoss[0m : 2.37334
[1mStep[0m  [8/26], [94mLoss[0m : 2.59946
[1mStep[0m  [10/26], [94mLoss[0m : 2.54785
[1mStep[0m  [12/26], [94mLoss[0m : 2.40083
[1mStep[0m  [14/26], [94mLoss[0m : 2.41210
[1mStep[0m  [16/26], [94mLoss[0m : 2.39224
[1mStep[0m  [18/26], [94mLoss[0m : 2.54627
[1mStep[0m  [20/26], [94mLoss[0m : 2.42227
[1mStep[0m  [22/26], [94mLoss[0m : 2.52207
[1mStep[0m  [24/26], [94mLoss[0m : 2.44302

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35453
[1mStep[0m  [2/26], [94mLoss[0m : 2.61096
[1mStep[0m  [4/26], [94mLoss[0m : 2.36722
[1mStep[0m  [6/26], [94mLoss[0m : 2.42425
[1mStep[0m  [8/26], [94mLoss[0m : 2.49139
[1mStep[0m  [10/26], [94mLoss[0m : 2.53267
[1mStep[0m  [12/26], [94mLoss[0m : 2.50869
[1mStep[0m  [14/26], [94mLoss[0m : 2.47973
[1mStep[0m  [16/26], [94mLoss[0m : 2.46980
[1mStep[0m  [18/26], [94mLoss[0m : 2.63482
[1mStep[0m  [20/26], [94mLoss[0m : 2.55971
[1mStep[0m  [22/26], [94mLoss[0m : 2.41499
[1mStep[0m  [24/26], [94mLoss[0m : 2.44720

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.376, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32590
[1mStep[0m  [2/26], [94mLoss[0m : 2.40466
[1mStep[0m  [4/26], [94mLoss[0m : 2.37618
[1mStep[0m  [6/26], [94mLoss[0m : 2.41818
[1mStep[0m  [8/26], [94mLoss[0m : 2.55861
[1mStep[0m  [10/26], [94mLoss[0m : 2.34456
[1mStep[0m  [12/26], [94mLoss[0m : 2.59796
[1mStep[0m  [14/26], [94mLoss[0m : 2.40857
[1mStep[0m  [16/26], [94mLoss[0m : 2.47294
[1mStep[0m  [18/26], [94mLoss[0m : 2.48164
[1mStep[0m  [20/26], [94mLoss[0m : 2.46433
[1mStep[0m  [22/26], [94mLoss[0m : 2.41670
[1mStep[0m  [24/26], [94mLoss[0m : 2.39841

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.385, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35721
[1mStep[0m  [2/26], [94mLoss[0m : 2.24450
[1mStep[0m  [4/26], [94mLoss[0m : 2.40183
[1mStep[0m  [6/26], [94mLoss[0m : 2.41942
[1mStep[0m  [8/26], [94mLoss[0m : 2.41290
[1mStep[0m  [10/26], [94mLoss[0m : 2.56718
[1mStep[0m  [12/26], [94mLoss[0m : 2.37533
[1mStep[0m  [14/26], [94mLoss[0m : 2.42979
[1mStep[0m  [16/26], [94mLoss[0m : 2.37224
[1mStep[0m  [18/26], [94mLoss[0m : 2.52474
[1mStep[0m  [20/26], [94mLoss[0m : 2.33083
[1mStep[0m  [22/26], [94mLoss[0m : 2.53798
[1mStep[0m  [24/26], [94mLoss[0m : 2.44742

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.390, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36315
[1mStep[0m  [2/26], [94mLoss[0m : 2.49550
[1mStep[0m  [4/26], [94mLoss[0m : 2.49729
[1mStep[0m  [6/26], [94mLoss[0m : 2.42690
[1mStep[0m  [8/26], [94mLoss[0m : 2.50379
[1mStep[0m  [10/26], [94mLoss[0m : 2.49758
[1mStep[0m  [12/26], [94mLoss[0m : 2.36393
[1mStep[0m  [14/26], [94mLoss[0m : 2.39350
[1mStep[0m  [16/26], [94mLoss[0m : 2.52171
[1mStep[0m  [18/26], [94mLoss[0m : 2.40265
[1mStep[0m  [20/26], [94mLoss[0m : 2.54644
[1mStep[0m  [22/26], [94mLoss[0m : 2.44710
[1mStep[0m  [24/26], [94mLoss[0m : 2.42503

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.393, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42626
[1mStep[0m  [2/26], [94mLoss[0m : 2.41013
[1mStep[0m  [4/26], [94mLoss[0m : 2.38408
[1mStep[0m  [6/26], [94mLoss[0m : 2.35655
[1mStep[0m  [8/26], [94mLoss[0m : 2.56210
[1mStep[0m  [10/26], [94mLoss[0m : 2.37341
[1mStep[0m  [12/26], [94mLoss[0m : 2.51795
[1mStep[0m  [14/26], [94mLoss[0m : 2.49599
[1mStep[0m  [16/26], [94mLoss[0m : 2.39107
[1mStep[0m  [18/26], [94mLoss[0m : 2.40548
[1mStep[0m  [20/26], [94mLoss[0m : 2.42413
[1mStep[0m  [22/26], [94mLoss[0m : 2.51339
[1mStep[0m  [24/26], [94mLoss[0m : 2.47816

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.390, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40251
[1mStep[0m  [2/26], [94mLoss[0m : 2.49546
[1mStep[0m  [4/26], [94mLoss[0m : 2.60265
[1mStep[0m  [6/26], [94mLoss[0m : 2.34826
[1mStep[0m  [8/26], [94mLoss[0m : 2.51640
[1mStep[0m  [10/26], [94mLoss[0m : 2.50273
[1mStep[0m  [12/26], [94mLoss[0m : 2.36574
[1mStep[0m  [14/26], [94mLoss[0m : 2.46288
[1mStep[0m  [16/26], [94mLoss[0m : 2.44025
[1mStep[0m  [18/26], [94mLoss[0m : 2.38362
[1mStep[0m  [20/26], [94mLoss[0m : 2.42269
[1mStep[0m  [22/26], [94mLoss[0m : 2.31392
[1mStep[0m  [24/26], [94mLoss[0m : 2.39141

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.380, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27758
[1mStep[0m  [2/26], [94mLoss[0m : 2.53832
[1mStep[0m  [4/26], [94mLoss[0m : 2.53293
[1mStep[0m  [6/26], [94mLoss[0m : 2.40144
[1mStep[0m  [8/26], [94mLoss[0m : 2.49060
[1mStep[0m  [10/26], [94mLoss[0m : 2.36410
[1mStep[0m  [12/26], [94mLoss[0m : 2.41438
[1mStep[0m  [14/26], [94mLoss[0m : 2.50781
[1mStep[0m  [16/26], [94mLoss[0m : 2.30823
[1mStep[0m  [18/26], [94mLoss[0m : 2.48475
[1mStep[0m  [20/26], [94mLoss[0m : 2.56419
[1mStep[0m  [22/26], [94mLoss[0m : 2.44529
[1mStep[0m  [24/26], [94mLoss[0m : 2.40388

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.386, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51184
[1mStep[0m  [2/26], [94mLoss[0m : 2.37492
[1mStep[0m  [4/26], [94mLoss[0m : 2.49340
[1mStep[0m  [6/26], [94mLoss[0m : 2.38957
[1mStep[0m  [8/26], [94mLoss[0m : 2.42731
[1mStep[0m  [10/26], [94mLoss[0m : 2.60130
[1mStep[0m  [12/26], [94mLoss[0m : 2.41086
[1mStep[0m  [14/26], [94mLoss[0m : 2.44328
[1mStep[0m  [16/26], [94mLoss[0m : 2.41619
[1mStep[0m  [18/26], [94mLoss[0m : 2.58178
[1mStep[0m  [20/26], [94mLoss[0m : 2.35604
[1mStep[0m  [22/26], [94mLoss[0m : 2.46297
[1mStep[0m  [24/26], [94mLoss[0m : 2.36772

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.386, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46539
[1mStep[0m  [2/26], [94mLoss[0m : 2.35313
[1mStep[0m  [4/26], [94mLoss[0m : 2.32858
[1mStep[0m  [6/26], [94mLoss[0m : 2.34498
[1mStep[0m  [8/26], [94mLoss[0m : 2.53066
[1mStep[0m  [10/26], [94mLoss[0m : 2.49987
[1mStep[0m  [12/26], [94mLoss[0m : 2.55432
[1mStep[0m  [14/26], [94mLoss[0m : 2.24910
[1mStep[0m  [16/26], [94mLoss[0m : 2.45092
[1mStep[0m  [18/26], [94mLoss[0m : 2.46829
[1mStep[0m  [20/26], [94mLoss[0m : 2.48691
[1mStep[0m  [22/26], [94mLoss[0m : 2.50464
[1mStep[0m  [24/26], [94mLoss[0m : 2.36488

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19879
[1mStep[0m  [2/26], [94mLoss[0m : 2.42629
[1mStep[0m  [4/26], [94mLoss[0m : 2.49543
[1mStep[0m  [6/26], [94mLoss[0m : 2.50040
[1mStep[0m  [8/26], [94mLoss[0m : 2.47544
[1mStep[0m  [10/26], [94mLoss[0m : 2.39174
[1mStep[0m  [12/26], [94mLoss[0m : 2.41436
[1mStep[0m  [14/26], [94mLoss[0m : 2.40132
[1mStep[0m  [16/26], [94mLoss[0m : 2.37769
[1mStep[0m  [18/26], [94mLoss[0m : 2.47913
[1mStep[0m  [20/26], [94mLoss[0m : 2.43740
[1mStep[0m  [22/26], [94mLoss[0m : 2.53349
[1mStep[0m  [24/26], [94mLoss[0m : 2.48839

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.376, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47453
[1mStep[0m  [2/26], [94mLoss[0m : 2.43932
[1mStep[0m  [4/26], [94mLoss[0m : 2.46956
[1mStep[0m  [6/26], [94mLoss[0m : 2.34815
[1mStep[0m  [8/26], [94mLoss[0m : 2.36726
[1mStep[0m  [10/26], [94mLoss[0m : 2.38343
[1mStep[0m  [12/26], [94mLoss[0m : 2.49183
[1mStep[0m  [14/26], [94mLoss[0m : 2.46269
[1mStep[0m  [16/26], [94mLoss[0m : 2.35260
[1mStep[0m  [18/26], [94mLoss[0m : 2.47793
[1mStep[0m  [20/26], [94mLoss[0m : 2.50348
[1mStep[0m  [22/26], [94mLoss[0m : 2.39316
[1mStep[0m  [24/26], [94mLoss[0m : 2.54292

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.384
====================================

Phase 1 - Evaluation MAE:  2.3837287425994873
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.46937
[1mStep[0m  [2/26], [94mLoss[0m : 2.46849
[1mStep[0m  [4/26], [94mLoss[0m : 2.44165
[1mStep[0m  [6/26], [94mLoss[0m : 2.44596
[1mStep[0m  [8/26], [94mLoss[0m : 2.41661
[1mStep[0m  [10/26], [94mLoss[0m : 2.41963
[1mStep[0m  [12/26], [94mLoss[0m : 2.33993
[1mStep[0m  [14/26], [94mLoss[0m : 2.43359
[1mStep[0m  [16/26], [94mLoss[0m : 2.45935
[1mStep[0m  [18/26], [94mLoss[0m : 2.55119
[1mStep[0m  [20/26], [94mLoss[0m : 2.53938
[1mStep[0m  [22/26], [94mLoss[0m : 2.52956
[1mStep[0m  [24/26], [94mLoss[0m : 2.59308

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32101
[1mStep[0m  [2/26], [94mLoss[0m : 2.44796
[1mStep[0m  [4/26], [94mLoss[0m : 2.43598
[1mStep[0m  [6/26], [94mLoss[0m : 2.29426
[1mStep[0m  [8/26], [94mLoss[0m : 2.56090
[1mStep[0m  [10/26], [94mLoss[0m : 2.45180
[1mStep[0m  [12/26], [94mLoss[0m : 2.38648
[1mStep[0m  [14/26], [94mLoss[0m : 2.40889
[1mStep[0m  [16/26], [94mLoss[0m : 2.52952
[1mStep[0m  [18/26], [94mLoss[0m : 2.47670
[1mStep[0m  [20/26], [94mLoss[0m : 2.56730
[1mStep[0m  [22/26], [94mLoss[0m : 2.34464
[1mStep[0m  [24/26], [94mLoss[0m : 2.41232

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36757
[1mStep[0m  [2/26], [94mLoss[0m : 2.37665
[1mStep[0m  [4/26], [94mLoss[0m : 2.29313
[1mStep[0m  [6/26], [94mLoss[0m : 2.29009
[1mStep[0m  [8/26], [94mLoss[0m : 2.34503
[1mStep[0m  [10/26], [94mLoss[0m : 2.28024
[1mStep[0m  [12/26], [94mLoss[0m : 2.39143
[1mStep[0m  [14/26], [94mLoss[0m : 2.25306
[1mStep[0m  [16/26], [94mLoss[0m : 2.33662
[1mStep[0m  [18/26], [94mLoss[0m : 2.43744
[1mStep[0m  [20/26], [94mLoss[0m : 2.27911
[1mStep[0m  [22/26], [94mLoss[0m : 2.21643
[1mStep[0m  [24/26], [94mLoss[0m : 2.43762

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27997
[1mStep[0m  [2/26], [94mLoss[0m : 2.29606
[1mStep[0m  [4/26], [94mLoss[0m : 2.33675
[1mStep[0m  [6/26], [94mLoss[0m : 2.36744
[1mStep[0m  [8/26], [94mLoss[0m : 2.25553
[1mStep[0m  [10/26], [94mLoss[0m : 2.30364
[1mStep[0m  [12/26], [94mLoss[0m : 2.18822
[1mStep[0m  [14/26], [94mLoss[0m : 2.16096
[1mStep[0m  [16/26], [94mLoss[0m : 2.09645
[1mStep[0m  [18/26], [94mLoss[0m : 2.22618
[1mStep[0m  [20/26], [94mLoss[0m : 2.29349
[1mStep[0m  [22/26], [94mLoss[0m : 2.23145
[1mStep[0m  [24/26], [94mLoss[0m : 2.16618

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15420
[1mStep[0m  [2/26], [94mLoss[0m : 2.16228
[1mStep[0m  [4/26], [94mLoss[0m : 1.95506
[1mStep[0m  [6/26], [94mLoss[0m : 2.19513
[1mStep[0m  [8/26], [94mLoss[0m : 2.16288
[1mStep[0m  [10/26], [94mLoss[0m : 2.11180
[1mStep[0m  [12/26], [94mLoss[0m : 2.46417
[1mStep[0m  [14/26], [94mLoss[0m : 2.17012
[1mStep[0m  [16/26], [94mLoss[0m : 2.10095
[1mStep[0m  [18/26], [94mLoss[0m : 2.10745
[1mStep[0m  [20/26], [94mLoss[0m : 2.24564
[1mStep[0m  [22/26], [94mLoss[0m : 2.07748
[1mStep[0m  [24/26], [94mLoss[0m : 2.07931

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.174, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.04154
[1mStep[0m  [2/26], [94mLoss[0m : 1.98380
[1mStep[0m  [4/26], [94mLoss[0m : 1.98327
[1mStep[0m  [6/26], [94mLoss[0m : 2.08626
[1mStep[0m  [8/26], [94mLoss[0m : 2.23375
[1mStep[0m  [10/26], [94mLoss[0m : 2.14799
[1mStep[0m  [12/26], [94mLoss[0m : 1.94752
[1mStep[0m  [14/26], [94mLoss[0m : 2.08960
[1mStep[0m  [16/26], [94mLoss[0m : 2.22433
[1mStep[0m  [18/26], [94mLoss[0m : 2.28096
[1mStep[0m  [20/26], [94mLoss[0m : 2.11712
[1mStep[0m  [22/26], [94mLoss[0m : 2.06742
[1mStep[0m  [24/26], [94mLoss[0m : 2.07593

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.115, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.02463
[1mStep[0m  [2/26], [94mLoss[0m : 2.09171
[1mStep[0m  [4/26], [94mLoss[0m : 2.02661
[1mStep[0m  [6/26], [94mLoss[0m : 1.98724
[1mStep[0m  [8/26], [94mLoss[0m : 1.97598
[1mStep[0m  [10/26], [94mLoss[0m : 2.19836
[1mStep[0m  [12/26], [94mLoss[0m : 2.03723
[1mStep[0m  [14/26], [94mLoss[0m : 2.09192
[1mStep[0m  [16/26], [94mLoss[0m : 1.90263
[1mStep[0m  [18/26], [94mLoss[0m : 2.15042
[1mStep[0m  [20/26], [94mLoss[0m : 2.17799
[1mStep[0m  [22/26], [94mLoss[0m : 2.15492
[1mStep[0m  [24/26], [94mLoss[0m : 2.00327

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97129
[1mStep[0m  [2/26], [94mLoss[0m : 2.10327
[1mStep[0m  [4/26], [94mLoss[0m : 2.05170
[1mStep[0m  [6/26], [94mLoss[0m : 1.99690
[1mStep[0m  [8/26], [94mLoss[0m : 2.09041
[1mStep[0m  [10/26], [94mLoss[0m : 2.07747
[1mStep[0m  [12/26], [94mLoss[0m : 2.08776
[1mStep[0m  [14/26], [94mLoss[0m : 1.98777
[1mStep[0m  [16/26], [94mLoss[0m : 2.00548
[1mStep[0m  [18/26], [94mLoss[0m : 2.12200
[1mStep[0m  [20/26], [94mLoss[0m : 2.10461
[1mStep[0m  [22/26], [94mLoss[0m : 1.98894
[1mStep[0m  [24/26], [94mLoss[0m : 2.05196

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03774
[1mStep[0m  [2/26], [94mLoss[0m : 2.05826
[1mStep[0m  [4/26], [94mLoss[0m : 1.83684
[1mStep[0m  [6/26], [94mLoss[0m : 1.95678
[1mStep[0m  [8/26], [94mLoss[0m : 1.96185
[1mStep[0m  [10/26], [94mLoss[0m : 1.94033
[1mStep[0m  [12/26], [94mLoss[0m : 1.99942
[1mStep[0m  [14/26], [94mLoss[0m : 1.90353
[1mStep[0m  [16/26], [94mLoss[0m : 1.90643
[1mStep[0m  [18/26], [94mLoss[0m : 1.92341
[1mStep[0m  [20/26], [94mLoss[0m : 1.92016
[1mStep[0m  [22/26], [94mLoss[0m : 1.92015
[1mStep[0m  [24/26], [94mLoss[0m : 1.97772

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84548
[1mStep[0m  [2/26], [94mLoss[0m : 1.92121
[1mStep[0m  [4/26], [94mLoss[0m : 1.77116
[1mStep[0m  [6/26], [94mLoss[0m : 1.84574
[1mStep[0m  [8/26], [94mLoss[0m : 1.81671
[1mStep[0m  [10/26], [94mLoss[0m : 1.90907
[1mStep[0m  [12/26], [94mLoss[0m : 1.72492
[1mStep[0m  [14/26], [94mLoss[0m : 1.88720
[1mStep[0m  [16/26], [94mLoss[0m : 1.95251
[1mStep[0m  [18/26], [94mLoss[0m : 2.03783
[1mStep[0m  [20/26], [94mLoss[0m : 1.86352
[1mStep[0m  [22/26], [94mLoss[0m : 1.82726
[1mStep[0m  [24/26], [94mLoss[0m : 1.96891

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81674
[1mStep[0m  [2/26], [94mLoss[0m : 1.90907
[1mStep[0m  [4/26], [94mLoss[0m : 1.82670
[1mStep[0m  [6/26], [94mLoss[0m : 1.83372
[1mStep[0m  [8/26], [94mLoss[0m : 1.83774
[1mStep[0m  [10/26], [94mLoss[0m : 1.98144
[1mStep[0m  [12/26], [94mLoss[0m : 1.75764
[1mStep[0m  [14/26], [94mLoss[0m : 1.78206
[1mStep[0m  [16/26], [94mLoss[0m : 1.93501
[1mStep[0m  [18/26], [94mLoss[0m : 1.78796
[1mStep[0m  [20/26], [94mLoss[0m : 1.79153
[1mStep[0m  [22/26], [94mLoss[0m : 1.80179
[1mStep[0m  [24/26], [94mLoss[0m : 1.82222

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.83794
[1mStep[0m  [2/26], [94mLoss[0m : 1.78301
[1mStep[0m  [4/26], [94mLoss[0m : 1.70213
[1mStep[0m  [6/26], [94mLoss[0m : 1.82462
[1mStep[0m  [8/26], [94mLoss[0m : 1.86413
[1mStep[0m  [10/26], [94mLoss[0m : 1.78288
[1mStep[0m  [12/26], [94mLoss[0m : 1.72044
[1mStep[0m  [14/26], [94mLoss[0m : 1.82209
[1mStep[0m  [16/26], [94mLoss[0m : 1.79286
[1mStep[0m  [18/26], [94mLoss[0m : 1.83497
[1mStep[0m  [20/26], [94mLoss[0m : 1.79408
[1mStep[0m  [22/26], [94mLoss[0m : 1.82217
[1mStep[0m  [24/26], [94mLoss[0m : 1.98726

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.77800
[1mStep[0m  [2/26], [94mLoss[0m : 1.68690
[1mStep[0m  [4/26], [94mLoss[0m : 1.74781
[1mStep[0m  [6/26], [94mLoss[0m : 1.77292
[1mStep[0m  [8/26], [94mLoss[0m : 1.76281
[1mStep[0m  [10/26], [94mLoss[0m : 1.77089
[1mStep[0m  [12/26], [94mLoss[0m : 1.81307
[1mStep[0m  [14/26], [94mLoss[0m : 1.78154
[1mStep[0m  [16/26], [94mLoss[0m : 1.75906
[1mStep[0m  [18/26], [94mLoss[0m : 1.81272
[1mStep[0m  [20/26], [94mLoss[0m : 1.80810
[1mStep[0m  [22/26], [94mLoss[0m : 1.75457
[1mStep[0m  [24/26], [94mLoss[0m : 1.77896

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.77238
[1mStep[0m  [2/26], [94mLoss[0m : 1.74971
[1mStep[0m  [4/26], [94mLoss[0m : 1.74268
[1mStep[0m  [6/26], [94mLoss[0m : 1.71022
[1mStep[0m  [8/26], [94mLoss[0m : 1.67484
[1mStep[0m  [10/26], [94mLoss[0m : 1.70283
[1mStep[0m  [12/26], [94mLoss[0m : 1.68518
[1mStep[0m  [14/26], [94mLoss[0m : 1.72447
[1mStep[0m  [16/26], [94mLoss[0m : 1.74691
[1mStep[0m  [18/26], [94mLoss[0m : 1.69615
[1mStep[0m  [20/26], [94mLoss[0m : 1.81448
[1mStep[0m  [22/26], [94mLoss[0m : 1.82550
[1mStep[0m  [24/26], [94mLoss[0m : 1.81788

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.721, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54071
[1mStep[0m  [2/26], [94mLoss[0m : 1.63107
[1mStep[0m  [4/26], [94mLoss[0m : 1.71330
[1mStep[0m  [6/26], [94mLoss[0m : 1.63435
[1mStep[0m  [8/26], [94mLoss[0m : 1.71888
[1mStep[0m  [10/26], [94mLoss[0m : 1.64837
[1mStep[0m  [12/26], [94mLoss[0m : 1.59886
[1mStep[0m  [14/26], [94mLoss[0m : 1.67160
[1mStep[0m  [16/26], [94mLoss[0m : 1.65349
[1mStep[0m  [18/26], [94mLoss[0m : 1.67562
[1mStep[0m  [20/26], [94mLoss[0m : 1.85482
[1mStep[0m  [22/26], [94mLoss[0m : 1.70182
[1mStep[0m  [24/26], [94mLoss[0m : 1.76529

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.71434
[1mStep[0m  [2/26], [94mLoss[0m : 1.52099
[1mStep[0m  [4/26], [94mLoss[0m : 1.57629
[1mStep[0m  [6/26], [94mLoss[0m : 1.56500
[1mStep[0m  [8/26], [94mLoss[0m : 1.72731
[1mStep[0m  [10/26], [94mLoss[0m : 1.69515
[1mStep[0m  [12/26], [94mLoss[0m : 1.54777
[1mStep[0m  [14/26], [94mLoss[0m : 1.68187
[1mStep[0m  [16/26], [94mLoss[0m : 1.60314
[1mStep[0m  [18/26], [94mLoss[0m : 1.85401
[1mStep[0m  [20/26], [94mLoss[0m : 1.61973
[1mStep[0m  [22/26], [94mLoss[0m : 1.73454
[1mStep[0m  [24/26], [94mLoss[0m : 1.59031

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.58531
[1mStep[0m  [2/26], [94mLoss[0m : 1.58944
[1mStep[0m  [4/26], [94mLoss[0m : 1.63483
[1mStep[0m  [6/26], [94mLoss[0m : 1.53555
[1mStep[0m  [8/26], [94mLoss[0m : 1.72173
[1mStep[0m  [10/26], [94mLoss[0m : 1.79761
[1mStep[0m  [12/26], [94mLoss[0m : 1.57245
[1mStep[0m  [14/26], [94mLoss[0m : 1.55905
[1mStep[0m  [16/26], [94mLoss[0m : 1.61258
[1mStep[0m  [18/26], [94mLoss[0m : 1.64075
[1mStep[0m  [20/26], [94mLoss[0m : 1.62446
[1mStep[0m  [22/26], [94mLoss[0m : 1.70414
[1mStep[0m  [24/26], [94mLoss[0m : 1.55528

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.51845
[1mStep[0m  [2/26], [94mLoss[0m : 1.71405
[1mStep[0m  [4/26], [94mLoss[0m : 1.61701
[1mStep[0m  [6/26], [94mLoss[0m : 1.61415
[1mStep[0m  [8/26], [94mLoss[0m : 1.60256
[1mStep[0m  [10/26], [94mLoss[0m : 1.65609
[1mStep[0m  [12/26], [94mLoss[0m : 1.60447
[1mStep[0m  [14/26], [94mLoss[0m : 1.65295
[1mStep[0m  [16/26], [94mLoss[0m : 1.62086
[1mStep[0m  [18/26], [94mLoss[0m : 1.62586
[1mStep[0m  [20/26], [94mLoss[0m : 1.55771
[1mStep[0m  [22/26], [94mLoss[0m : 1.62171
[1mStep[0m  [24/26], [94mLoss[0m : 1.64475

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56624
[1mStep[0m  [2/26], [94mLoss[0m : 1.46693
[1mStep[0m  [4/26], [94mLoss[0m : 1.50399
[1mStep[0m  [6/26], [94mLoss[0m : 1.59664
[1mStep[0m  [8/26], [94mLoss[0m : 1.54712
[1mStep[0m  [10/26], [94mLoss[0m : 1.54760
[1mStep[0m  [12/26], [94mLoss[0m : 1.57187
[1mStep[0m  [14/26], [94mLoss[0m : 1.53420
[1mStep[0m  [16/26], [94mLoss[0m : 1.56982
[1mStep[0m  [18/26], [94mLoss[0m : 1.57326
[1mStep[0m  [20/26], [94mLoss[0m : 1.63890
[1mStep[0m  [22/26], [94mLoss[0m : 1.55505
[1mStep[0m  [24/26], [94mLoss[0m : 1.69830

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61883
[1mStep[0m  [2/26], [94mLoss[0m : 1.42560
[1mStep[0m  [4/26], [94mLoss[0m : 1.54228
[1mStep[0m  [6/26], [94mLoss[0m : 1.50863
[1mStep[0m  [8/26], [94mLoss[0m : 1.51667
[1mStep[0m  [10/26], [94mLoss[0m : 1.54196
[1mStep[0m  [12/26], [94mLoss[0m : 1.45587
[1mStep[0m  [14/26], [94mLoss[0m : 1.54156
[1mStep[0m  [16/26], [94mLoss[0m : 1.58954
[1mStep[0m  [18/26], [94mLoss[0m : 1.45921
[1mStep[0m  [20/26], [94mLoss[0m : 1.55996
[1mStep[0m  [22/26], [94mLoss[0m : 1.42947
[1mStep[0m  [24/26], [94mLoss[0m : 1.45657

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.449, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53445
[1mStep[0m  [2/26], [94mLoss[0m : 1.52011
[1mStep[0m  [4/26], [94mLoss[0m : 1.46364
[1mStep[0m  [6/26], [94mLoss[0m : 1.57177
[1mStep[0m  [8/26], [94mLoss[0m : 1.55229
[1mStep[0m  [10/26], [94mLoss[0m : 1.54590
[1mStep[0m  [12/26], [94mLoss[0m : 1.51053
[1mStep[0m  [14/26], [94mLoss[0m : 1.55859
[1mStep[0m  [16/26], [94mLoss[0m : 1.45967
[1mStep[0m  [18/26], [94mLoss[0m : 1.49966
[1mStep[0m  [20/26], [94mLoss[0m : 1.46066
[1mStep[0m  [22/26], [94mLoss[0m : 1.48290
[1mStep[0m  [24/26], [94mLoss[0m : 1.52634

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.529, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.51004
[1mStep[0m  [2/26], [94mLoss[0m : 1.40278
[1mStep[0m  [4/26], [94mLoss[0m : 1.39622
[1mStep[0m  [6/26], [94mLoss[0m : 1.44261
[1mStep[0m  [8/26], [94mLoss[0m : 1.50161
[1mStep[0m  [10/26], [94mLoss[0m : 1.44672
[1mStep[0m  [12/26], [94mLoss[0m : 1.47357
[1mStep[0m  [14/26], [94mLoss[0m : 1.44554
[1mStep[0m  [16/26], [94mLoss[0m : 1.47740
[1mStep[0m  [18/26], [94mLoss[0m : 1.39555
[1mStep[0m  [20/26], [94mLoss[0m : 1.43168
[1mStep[0m  [22/26], [94mLoss[0m : 1.56679
[1mStep[0m  [24/26], [94mLoss[0m : 1.44034

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.29809
[1mStep[0m  [2/26], [94mLoss[0m : 1.42666
[1mStep[0m  [4/26], [94mLoss[0m : 1.34037
[1mStep[0m  [6/26], [94mLoss[0m : 1.36871
[1mStep[0m  [8/26], [94mLoss[0m : 1.47938
[1mStep[0m  [10/26], [94mLoss[0m : 1.49168
[1mStep[0m  [12/26], [94mLoss[0m : 1.41705
[1mStep[0m  [14/26], [94mLoss[0m : 1.43188
[1mStep[0m  [16/26], [94mLoss[0m : 1.50427
[1mStep[0m  [18/26], [94mLoss[0m : 1.55135
[1mStep[0m  [20/26], [94mLoss[0m : 1.47511
[1mStep[0m  [22/26], [94mLoss[0m : 1.41964
[1mStep[0m  [24/26], [94mLoss[0m : 1.48525

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.439, [92mTest[0m: 2.445, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.43671
[1mStep[0m  [2/26], [94mLoss[0m : 1.38076
[1mStep[0m  [4/26], [94mLoss[0m : 1.46341
[1mStep[0m  [6/26], [94mLoss[0m : 1.43542
[1mStep[0m  [8/26], [94mLoss[0m : 1.41494
[1mStep[0m  [10/26], [94mLoss[0m : 1.51624
[1mStep[0m  [12/26], [94mLoss[0m : 1.39970
[1mStep[0m  [14/26], [94mLoss[0m : 1.54563
[1mStep[0m  [16/26], [94mLoss[0m : 1.45767
[1mStep[0m  [18/26], [94mLoss[0m : 1.48768
[1mStep[0m  [20/26], [94mLoss[0m : 1.42106
[1mStep[0m  [22/26], [94mLoss[0m : 1.45575
[1mStep[0m  [24/26], [94mLoss[0m : 1.42218

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.430, [92mTest[0m: 2.491, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.491762693111713
MAE score P1        2.383729
MAE score P2        2.491763
loss                1.430401
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.9
weight_decay           0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.64013
[1mStep[0m  [2/26], [94mLoss[0m : 10.88161
[1mStep[0m  [4/26], [94mLoss[0m : 10.76797
[1mStep[0m  [6/26], [94mLoss[0m : 10.82485
[1mStep[0m  [8/26], [94mLoss[0m : 10.62484
[1mStep[0m  [10/26], [94mLoss[0m : 10.55290
[1mStep[0m  [12/26], [94mLoss[0m : 10.52016
[1mStep[0m  [14/26], [94mLoss[0m : 10.55708
[1mStep[0m  [16/26], [94mLoss[0m : 10.49304
[1mStep[0m  [18/26], [94mLoss[0m : 10.17199
[1mStep[0m  [20/26], [94mLoss[0m : 10.60385
[1mStep[0m  [22/26], [94mLoss[0m : 10.44242
[1mStep[0m  [24/26], [94mLoss[0m : 10.38052

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.602, [92mTest[0m: 10.814, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.33715
[1mStep[0m  [2/26], [94mLoss[0m : 10.57284
[1mStep[0m  [4/26], [94mLoss[0m : 10.39580
[1mStep[0m  [6/26], [94mLoss[0m : 10.34761
[1mStep[0m  [8/26], [94mLoss[0m : 10.15413
[1mStep[0m  [10/26], [94mLoss[0m : 10.26262
[1mStep[0m  [12/26], [94mLoss[0m : 10.05515
[1mStep[0m  [14/26], [94mLoss[0m : 10.03011
[1mStep[0m  [16/26], [94mLoss[0m : 10.02845
[1mStep[0m  [18/26], [94mLoss[0m : 10.27918
[1mStep[0m  [20/26], [94mLoss[0m : 9.92109
[1mStep[0m  [22/26], [94mLoss[0m : 9.97455
[1mStep[0m  [24/26], [94mLoss[0m : 9.76128

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.113, [92mTest[0m: 10.250, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.81972
[1mStep[0m  [2/26], [94mLoss[0m : 9.83702
[1mStep[0m  [4/26], [94mLoss[0m : 9.81033
[1mStep[0m  [6/26], [94mLoss[0m : 9.56886
[1mStep[0m  [8/26], [94mLoss[0m : 9.55931
[1mStep[0m  [10/26], [94mLoss[0m : 9.72799
[1mStep[0m  [12/26], [94mLoss[0m : 9.54174
[1mStep[0m  [14/26], [94mLoss[0m : 9.86076
[1mStep[0m  [16/26], [94mLoss[0m : 9.39680
[1mStep[0m  [18/26], [94mLoss[0m : 9.50733
[1mStep[0m  [20/26], [94mLoss[0m : 9.34320
[1mStep[0m  [22/26], [94mLoss[0m : 9.02355
[1mStep[0m  [24/26], [94mLoss[0m : 9.39947

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.580, [92mTest[0m: 9.643, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.20458
[1mStep[0m  [2/26], [94mLoss[0m : 9.06447
[1mStep[0m  [4/26], [94mLoss[0m : 9.39204
[1mStep[0m  [6/26], [94mLoss[0m : 9.07131
[1mStep[0m  [8/26], [94mLoss[0m : 9.05899
[1mStep[0m  [10/26], [94mLoss[0m : 9.02541
[1mStep[0m  [12/26], [94mLoss[0m : 8.90660
[1mStep[0m  [14/26], [94mLoss[0m : 8.79003
[1mStep[0m  [16/26], [94mLoss[0m : 8.98569
[1mStep[0m  [18/26], [94mLoss[0m : 8.41174
[1mStep[0m  [20/26], [94mLoss[0m : 8.87416
[1mStep[0m  [22/26], [94mLoss[0m : 8.68727
[1mStep[0m  [24/26], [94mLoss[0m : 8.61791

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.941, [92mTest[0m: 8.928, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.54678
[1mStep[0m  [2/26], [94mLoss[0m : 8.39023
[1mStep[0m  [4/26], [94mLoss[0m : 8.61694
[1mStep[0m  [6/26], [94mLoss[0m : 8.19079
[1mStep[0m  [8/26], [94mLoss[0m : 8.29904
[1mStep[0m  [10/26], [94mLoss[0m : 8.63076
[1mStep[0m  [12/26], [94mLoss[0m : 8.12377
[1mStep[0m  [14/26], [94mLoss[0m : 8.20257
[1mStep[0m  [16/26], [94mLoss[0m : 8.23098
[1mStep[0m  [18/26], [94mLoss[0m : 7.88698
[1mStep[0m  [20/26], [94mLoss[0m : 8.18196
[1mStep[0m  [22/26], [94mLoss[0m : 7.73352
[1mStep[0m  [24/26], [94mLoss[0m : 7.59931

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.161, [92mTest[0m: 8.103, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.52412
[1mStep[0m  [2/26], [94mLoss[0m : 7.59328
[1mStep[0m  [4/26], [94mLoss[0m : 7.60572
[1mStep[0m  [6/26], [94mLoss[0m : 7.31669
[1mStep[0m  [8/26], [94mLoss[0m : 7.21357
[1mStep[0m  [10/26], [94mLoss[0m : 7.54276
[1mStep[0m  [12/26], [94mLoss[0m : 7.50179
[1mStep[0m  [14/26], [94mLoss[0m : 7.25498
[1mStep[0m  [16/26], [94mLoss[0m : 7.00588
[1mStep[0m  [18/26], [94mLoss[0m : 7.29310
[1mStep[0m  [20/26], [94mLoss[0m : 6.99646
[1mStep[0m  [22/26], [94mLoss[0m : 6.69004
[1mStep[0m  [24/26], [94mLoss[0m : 7.02124

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.258, [92mTest[0m: 7.165, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.88145
[1mStep[0m  [2/26], [94mLoss[0m : 6.86270
[1mStep[0m  [4/26], [94mLoss[0m : 6.45348
[1mStep[0m  [6/26], [94mLoss[0m : 6.51008
[1mStep[0m  [8/26], [94mLoss[0m : 6.34999
[1mStep[0m  [10/26], [94mLoss[0m : 6.42806
[1mStep[0m  [12/26], [94mLoss[0m : 6.41309
[1mStep[0m  [14/26], [94mLoss[0m : 6.04568
[1mStep[0m  [16/26], [94mLoss[0m : 6.08742
[1mStep[0m  [18/26], [94mLoss[0m : 6.28832
[1mStep[0m  [20/26], [94mLoss[0m : 6.17059
[1mStep[0m  [22/26], [94mLoss[0m : 6.13748
[1mStep[0m  [24/26], [94mLoss[0m : 5.93048

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.374, [92mTest[0m: 6.118, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.77683
[1mStep[0m  [2/26], [94mLoss[0m : 5.96831
[1mStep[0m  [4/26], [94mLoss[0m : 5.90845
[1mStep[0m  [6/26], [94mLoss[0m : 5.66533
[1mStep[0m  [8/26], [94mLoss[0m : 5.59068
[1mStep[0m  [10/26], [94mLoss[0m : 5.75647
[1mStep[0m  [12/26], [94mLoss[0m : 5.56635
[1mStep[0m  [14/26], [94mLoss[0m : 5.30433
[1mStep[0m  [16/26], [94mLoss[0m : 5.24819
[1mStep[0m  [18/26], [94mLoss[0m : 5.15223
[1mStep[0m  [20/26], [94mLoss[0m : 5.32451
[1mStep[0m  [22/26], [94mLoss[0m : 5.07088
[1mStep[0m  [24/26], [94mLoss[0m : 4.98553

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.475, [92mTest[0m: 5.026, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.98080
[1mStep[0m  [2/26], [94mLoss[0m : 4.91468
[1mStep[0m  [4/26], [94mLoss[0m : 4.68226
[1mStep[0m  [6/26], [94mLoss[0m : 4.76424
[1mStep[0m  [8/26], [94mLoss[0m : 4.80013
[1mStep[0m  [10/26], [94mLoss[0m : 4.68429
[1mStep[0m  [12/26], [94mLoss[0m : 4.49395
[1mStep[0m  [14/26], [94mLoss[0m : 4.54378
[1mStep[0m  [16/26], [94mLoss[0m : 4.79969
[1mStep[0m  [18/26], [94mLoss[0m : 4.28707
[1mStep[0m  [20/26], [94mLoss[0m : 4.28090
[1mStep[0m  [22/26], [94mLoss[0m : 4.13520
[1mStep[0m  [24/26], [94mLoss[0m : 3.89212

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.521, [92mTest[0m: 4.095, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.09671
[1mStep[0m  [2/26], [94mLoss[0m : 4.11741
[1mStep[0m  [4/26], [94mLoss[0m : 4.00016
[1mStep[0m  [6/26], [94mLoss[0m : 3.73725
[1mStep[0m  [8/26], [94mLoss[0m : 3.79826
[1mStep[0m  [10/26], [94mLoss[0m : 3.51401
[1mStep[0m  [12/26], [94mLoss[0m : 3.77378
[1mStep[0m  [14/26], [94mLoss[0m : 3.62189
[1mStep[0m  [16/26], [94mLoss[0m : 3.66894
[1mStep[0m  [18/26], [94mLoss[0m : 3.53693
[1mStep[0m  [20/26], [94mLoss[0m : 3.36705
[1mStep[0m  [22/26], [94mLoss[0m : 3.39327
[1mStep[0m  [24/26], [94mLoss[0m : 3.13449

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.657, [92mTest[0m: 3.311, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.27808
[1mStep[0m  [2/26], [94mLoss[0m : 3.42942
[1mStep[0m  [4/26], [94mLoss[0m : 3.27952
[1mStep[0m  [6/26], [94mLoss[0m : 3.19897
[1mStep[0m  [8/26], [94mLoss[0m : 3.16882
[1mStep[0m  [10/26], [94mLoss[0m : 2.78140
[1mStep[0m  [12/26], [94mLoss[0m : 3.03288
[1mStep[0m  [14/26], [94mLoss[0m : 3.01437
[1mStep[0m  [16/26], [94mLoss[0m : 2.75692
[1mStep[0m  [18/26], [94mLoss[0m : 2.89469
[1mStep[0m  [20/26], [94mLoss[0m : 2.99952
[1mStep[0m  [22/26], [94mLoss[0m : 2.98269
[1mStep[0m  [24/26], [94mLoss[0m : 2.97841

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.060, [92mTest[0m: 2.792, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.96688
[1mStep[0m  [2/26], [94mLoss[0m : 2.85861
[1mStep[0m  [4/26], [94mLoss[0m : 2.80153
[1mStep[0m  [6/26], [94mLoss[0m : 2.82952
[1mStep[0m  [8/26], [94mLoss[0m : 2.73293
[1mStep[0m  [10/26], [94mLoss[0m : 2.67507
[1mStep[0m  [12/26], [94mLoss[0m : 2.82198
[1mStep[0m  [14/26], [94mLoss[0m : 2.86971
[1mStep[0m  [16/26], [94mLoss[0m : 2.63884
[1mStep[0m  [18/26], [94mLoss[0m : 2.73175
[1mStep[0m  [20/26], [94mLoss[0m : 2.60089
[1mStep[0m  [22/26], [94mLoss[0m : 2.65789
[1mStep[0m  [24/26], [94mLoss[0m : 2.83855

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.802, [92mTest[0m: 2.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66704
[1mStep[0m  [2/26], [94mLoss[0m : 2.81160
[1mStep[0m  [4/26], [94mLoss[0m : 2.90225
[1mStep[0m  [6/26], [94mLoss[0m : 2.67669
[1mStep[0m  [8/26], [94mLoss[0m : 2.83989
[1mStep[0m  [10/26], [94mLoss[0m : 2.74599
[1mStep[0m  [12/26], [94mLoss[0m : 2.73765
[1mStep[0m  [14/26], [94mLoss[0m : 2.54820
[1mStep[0m  [16/26], [94mLoss[0m : 2.68928
[1mStep[0m  [18/26], [94mLoss[0m : 2.65347
[1mStep[0m  [20/26], [94mLoss[0m : 2.78278
[1mStep[0m  [22/26], [94mLoss[0m : 2.69849
[1mStep[0m  [24/26], [94mLoss[0m : 2.63250

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69875
[1mStep[0m  [2/26], [94mLoss[0m : 2.67071
[1mStep[0m  [4/26], [94mLoss[0m : 2.70107
[1mStep[0m  [6/26], [94mLoss[0m : 2.61592
[1mStep[0m  [8/26], [94mLoss[0m : 2.69311
[1mStep[0m  [10/26], [94mLoss[0m : 2.72539
[1mStep[0m  [12/26], [94mLoss[0m : 2.63556
[1mStep[0m  [14/26], [94mLoss[0m : 2.61872
[1mStep[0m  [16/26], [94mLoss[0m : 2.74367
[1mStep[0m  [18/26], [94mLoss[0m : 2.57977
[1mStep[0m  [20/26], [94mLoss[0m : 2.61241
[1mStep[0m  [22/26], [94mLoss[0m : 2.70150
[1mStep[0m  [24/26], [94mLoss[0m : 2.78248

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.688, [92mTest[0m: 2.504, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68003
[1mStep[0m  [2/26], [94mLoss[0m : 2.53132
[1mStep[0m  [4/26], [94mLoss[0m : 2.74712
[1mStep[0m  [6/26], [94mLoss[0m : 2.65940
[1mStep[0m  [8/26], [94mLoss[0m : 2.84173
[1mStep[0m  [10/26], [94mLoss[0m : 2.67305
[1mStep[0m  [12/26], [94mLoss[0m : 2.73738
[1mStep[0m  [14/26], [94mLoss[0m : 2.53888
[1mStep[0m  [16/26], [94mLoss[0m : 2.68723
[1mStep[0m  [18/26], [94mLoss[0m : 2.76175
[1mStep[0m  [20/26], [94mLoss[0m : 2.71684
[1mStep[0m  [22/26], [94mLoss[0m : 2.70963
[1mStep[0m  [24/26], [94mLoss[0m : 2.58682

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52631
[1mStep[0m  [2/26], [94mLoss[0m : 2.50270
[1mStep[0m  [4/26], [94mLoss[0m : 2.61887
[1mStep[0m  [6/26], [94mLoss[0m : 2.69256
[1mStep[0m  [8/26], [94mLoss[0m : 2.51491
[1mStep[0m  [10/26], [94mLoss[0m : 2.76284
[1mStep[0m  [12/26], [94mLoss[0m : 2.75418
[1mStep[0m  [14/26], [94mLoss[0m : 2.71298
[1mStep[0m  [16/26], [94mLoss[0m : 2.61781
[1mStep[0m  [18/26], [94mLoss[0m : 2.64520
[1mStep[0m  [20/26], [94mLoss[0m : 2.70218
[1mStep[0m  [22/26], [94mLoss[0m : 2.64253
[1mStep[0m  [24/26], [94mLoss[0m : 2.86493

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65245
[1mStep[0m  [2/26], [94mLoss[0m : 2.50497
[1mStep[0m  [4/26], [94mLoss[0m : 2.82413
[1mStep[0m  [6/26], [94mLoss[0m : 2.69903
[1mStep[0m  [8/26], [94mLoss[0m : 2.70535
[1mStep[0m  [10/26], [94mLoss[0m : 2.69392
[1mStep[0m  [12/26], [94mLoss[0m : 2.47991
[1mStep[0m  [14/26], [94mLoss[0m : 2.77524
[1mStep[0m  [16/26], [94mLoss[0m : 2.69822
[1mStep[0m  [18/26], [94mLoss[0m : 2.67552
[1mStep[0m  [20/26], [94mLoss[0m : 2.57528
[1mStep[0m  [22/26], [94mLoss[0m : 2.65465
[1mStep[0m  [24/26], [94mLoss[0m : 2.69754

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66544
[1mStep[0m  [2/26], [94mLoss[0m : 2.75819
[1mStep[0m  [4/26], [94mLoss[0m : 2.59455
[1mStep[0m  [6/26], [94mLoss[0m : 2.70067
[1mStep[0m  [8/26], [94mLoss[0m : 2.67445
[1mStep[0m  [10/26], [94mLoss[0m : 2.65218
[1mStep[0m  [12/26], [94mLoss[0m : 2.67160
[1mStep[0m  [14/26], [94mLoss[0m : 2.62682
[1mStep[0m  [16/26], [94mLoss[0m : 2.57538
[1mStep[0m  [18/26], [94mLoss[0m : 2.54145
[1mStep[0m  [20/26], [94mLoss[0m : 2.51990
[1mStep[0m  [22/26], [94mLoss[0m : 2.65295
[1mStep[0m  [24/26], [94mLoss[0m : 2.55954

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66408
[1mStep[0m  [2/26], [94mLoss[0m : 2.61116
[1mStep[0m  [4/26], [94mLoss[0m : 2.72376
[1mStep[0m  [6/26], [94mLoss[0m : 2.66382
[1mStep[0m  [8/26], [94mLoss[0m : 2.70349
[1mStep[0m  [10/26], [94mLoss[0m : 2.61598
[1mStep[0m  [12/26], [94mLoss[0m : 2.51361
[1mStep[0m  [14/26], [94mLoss[0m : 2.68293
[1mStep[0m  [16/26], [94mLoss[0m : 2.45261
[1mStep[0m  [18/26], [94mLoss[0m : 2.70395
[1mStep[0m  [20/26], [94mLoss[0m : 2.62318
[1mStep[0m  [22/26], [94mLoss[0m : 2.79015
[1mStep[0m  [24/26], [94mLoss[0m : 2.59494

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57704
[1mStep[0m  [2/26], [94mLoss[0m : 2.56173
[1mStep[0m  [4/26], [94mLoss[0m : 2.67635
[1mStep[0m  [6/26], [94mLoss[0m : 2.63565
[1mStep[0m  [8/26], [94mLoss[0m : 2.60706
[1mStep[0m  [10/26], [94mLoss[0m : 2.67989
[1mStep[0m  [12/26], [94mLoss[0m : 2.68160
[1mStep[0m  [14/26], [94mLoss[0m : 2.62758
[1mStep[0m  [16/26], [94mLoss[0m : 2.55496
[1mStep[0m  [18/26], [94mLoss[0m : 2.54790
[1mStep[0m  [20/26], [94mLoss[0m : 2.57959
[1mStep[0m  [22/26], [94mLoss[0m : 2.65646
[1mStep[0m  [24/26], [94mLoss[0m : 2.38498

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.451, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74817
[1mStep[0m  [2/26], [94mLoss[0m : 2.61110
[1mStep[0m  [4/26], [94mLoss[0m : 2.64836
[1mStep[0m  [6/26], [94mLoss[0m : 2.50015
[1mStep[0m  [8/26], [94mLoss[0m : 2.48009
[1mStep[0m  [10/26], [94mLoss[0m : 2.76564
[1mStep[0m  [12/26], [94mLoss[0m : 2.71019
[1mStep[0m  [14/26], [94mLoss[0m : 2.44568
[1mStep[0m  [16/26], [94mLoss[0m : 2.52765
[1mStep[0m  [18/26], [94mLoss[0m : 2.50810
[1mStep[0m  [20/26], [94mLoss[0m : 2.42878
[1mStep[0m  [22/26], [94mLoss[0m : 2.67158
[1mStep[0m  [24/26], [94mLoss[0m : 2.73577

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.440, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55226
[1mStep[0m  [2/26], [94mLoss[0m : 2.72436
[1mStep[0m  [4/26], [94mLoss[0m : 2.52670
[1mStep[0m  [6/26], [94mLoss[0m : 2.56780
[1mStep[0m  [8/26], [94mLoss[0m : 2.56076
[1mStep[0m  [10/26], [94mLoss[0m : 2.59219
[1mStep[0m  [12/26], [94mLoss[0m : 2.56807
[1mStep[0m  [14/26], [94mLoss[0m : 2.78854
[1mStep[0m  [16/26], [94mLoss[0m : 2.39777
[1mStep[0m  [18/26], [94mLoss[0m : 2.49946
[1mStep[0m  [20/26], [94mLoss[0m : 2.66153
[1mStep[0m  [22/26], [94mLoss[0m : 2.65536
[1mStep[0m  [24/26], [94mLoss[0m : 2.54479

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52294
[1mStep[0m  [2/26], [94mLoss[0m : 2.68391
[1mStep[0m  [4/26], [94mLoss[0m : 2.62812
[1mStep[0m  [6/26], [94mLoss[0m : 2.52700
[1mStep[0m  [8/26], [94mLoss[0m : 2.68864
[1mStep[0m  [10/26], [94mLoss[0m : 2.54546
[1mStep[0m  [12/26], [94mLoss[0m : 2.55558
[1mStep[0m  [14/26], [94mLoss[0m : 2.64261
[1mStep[0m  [16/26], [94mLoss[0m : 2.61435
[1mStep[0m  [18/26], [94mLoss[0m : 2.71367
[1mStep[0m  [20/26], [94mLoss[0m : 2.61311
[1mStep[0m  [22/26], [94mLoss[0m : 2.75739
[1mStep[0m  [24/26], [94mLoss[0m : 2.61984

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.435, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68809
[1mStep[0m  [2/26], [94mLoss[0m : 2.68904
[1mStep[0m  [4/26], [94mLoss[0m : 2.60867
[1mStep[0m  [6/26], [94mLoss[0m : 2.64176
[1mStep[0m  [8/26], [94mLoss[0m : 2.74155
[1mStep[0m  [10/26], [94mLoss[0m : 2.59031
[1mStep[0m  [12/26], [94mLoss[0m : 2.39134
[1mStep[0m  [14/26], [94mLoss[0m : 2.69653
[1mStep[0m  [16/26], [94mLoss[0m : 2.57985
[1mStep[0m  [18/26], [94mLoss[0m : 2.50800
[1mStep[0m  [20/26], [94mLoss[0m : 2.60863
[1mStep[0m  [22/26], [94mLoss[0m : 2.51757
[1mStep[0m  [24/26], [94mLoss[0m : 2.58377

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.446, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54410
[1mStep[0m  [2/26], [94mLoss[0m : 2.58195
[1mStep[0m  [4/26], [94mLoss[0m : 2.63490
[1mStep[0m  [6/26], [94mLoss[0m : 2.54403
[1mStep[0m  [8/26], [94mLoss[0m : 2.62783
[1mStep[0m  [10/26], [94mLoss[0m : 2.65188
[1mStep[0m  [12/26], [94mLoss[0m : 2.61026
[1mStep[0m  [14/26], [94mLoss[0m : 2.55403
[1mStep[0m  [16/26], [94mLoss[0m : 2.58072
[1mStep[0m  [18/26], [94mLoss[0m : 2.75274
[1mStep[0m  [20/26], [94mLoss[0m : 2.45422
[1mStep[0m  [22/26], [94mLoss[0m : 2.54239
[1mStep[0m  [24/26], [94mLoss[0m : 2.66713

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.439, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.68269
[1mStep[0m  [2/26], [94mLoss[0m : 2.60120
[1mStep[0m  [4/26], [94mLoss[0m : 2.62873
[1mStep[0m  [6/26], [94mLoss[0m : 2.61480
[1mStep[0m  [8/26], [94mLoss[0m : 2.61139
[1mStep[0m  [10/26], [94mLoss[0m : 2.61216
[1mStep[0m  [12/26], [94mLoss[0m : 2.72845
[1mStep[0m  [14/26], [94mLoss[0m : 2.48299
[1mStep[0m  [16/26], [94mLoss[0m : 2.55241
[1mStep[0m  [18/26], [94mLoss[0m : 2.81973
[1mStep[0m  [20/26], [94mLoss[0m : 2.56547
[1mStep[0m  [22/26], [94mLoss[0m : 2.63486
[1mStep[0m  [24/26], [94mLoss[0m : 2.54159

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.429, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55959
[1mStep[0m  [2/26], [94mLoss[0m : 2.63180
[1mStep[0m  [4/26], [94mLoss[0m : 2.50721
[1mStep[0m  [6/26], [94mLoss[0m : 2.52056
[1mStep[0m  [8/26], [94mLoss[0m : 2.67467
[1mStep[0m  [10/26], [94mLoss[0m : 2.75884
[1mStep[0m  [12/26], [94mLoss[0m : 2.43051
[1mStep[0m  [14/26], [94mLoss[0m : 2.57461
[1mStep[0m  [16/26], [94mLoss[0m : 2.60527
[1mStep[0m  [18/26], [94mLoss[0m : 2.73573
[1mStep[0m  [20/26], [94mLoss[0m : 2.50818
[1mStep[0m  [22/26], [94mLoss[0m : 2.57088
[1mStep[0m  [24/26], [94mLoss[0m : 2.54286

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.434, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66910
[1mStep[0m  [2/26], [94mLoss[0m : 2.57124
[1mStep[0m  [4/26], [94mLoss[0m : 2.73195
[1mStep[0m  [6/26], [94mLoss[0m : 2.58980
[1mStep[0m  [8/26], [94mLoss[0m : 2.63164
[1mStep[0m  [10/26], [94mLoss[0m : 2.42591
[1mStep[0m  [12/26], [94mLoss[0m : 2.53203
[1mStep[0m  [14/26], [94mLoss[0m : 2.53114
[1mStep[0m  [16/26], [94mLoss[0m : 2.56194
[1mStep[0m  [18/26], [94mLoss[0m : 2.59292
[1mStep[0m  [20/26], [94mLoss[0m : 2.39720
[1mStep[0m  [22/26], [94mLoss[0m : 2.61956
[1mStep[0m  [24/26], [94mLoss[0m : 2.59281

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.413, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59409
[1mStep[0m  [2/26], [94mLoss[0m : 2.54485
[1mStep[0m  [4/26], [94mLoss[0m : 2.69362
[1mStep[0m  [6/26], [94mLoss[0m : 2.65364
[1mStep[0m  [8/26], [94mLoss[0m : 2.60557
[1mStep[0m  [10/26], [94mLoss[0m : 2.51431
[1mStep[0m  [12/26], [94mLoss[0m : 2.47497
[1mStep[0m  [14/26], [94mLoss[0m : 2.44119
[1mStep[0m  [16/26], [94mLoss[0m : 2.62346
[1mStep[0m  [18/26], [94mLoss[0m : 2.63001
[1mStep[0m  [20/26], [94mLoss[0m : 2.57853
[1mStep[0m  [22/26], [94mLoss[0m : 2.76602
[1mStep[0m  [24/26], [94mLoss[0m : 2.68769

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.420, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56452
[1mStep[0m  [2/26], [94mLoss[0m : 2.58817
[1mStep[0m  [4/26], [94mLoss[0m : 2.51532
[1mStep[0m  [6/26], [94mLoss[0m : 2.53888
[1mStep[0m  [8/26], [94mLoss[0m : 2.59948
[1mStep[0m  [10/26], [94mLoss[0m : 2.59484
[1mStep[0m  [12/26], [94mLoss[0m : 2.79795
[1mStep[0m  [14/26], [94mLoss[0m : 2.65146
[1mStep[0m  [16/26], [94mLoss[0m : 2.44849
[1mStep[0m  [18/26], [94mLoss[0m : 2.59478
[1mStep[0m  [20/26], [94mLoss[0m : 2.44302
[1mStep[0m  [22/26], [94mLoss[0m : 2.61345
[1mStep[0m  [24/26], [94mLoss[0m : 2.61810

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.412
====================================

Phase 1 - Evaluation MAE:  2.4118857567126932
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.42009
[1mStep[0m  [2/26], [94mLoss[0m : 2.66378
[1mStep[0m  [4/26], [94mLoss[0m : 2.56793
[1mStep[0m  [6/26], [94mLoss[0m : 2.61498
[1mStep[0m  [8/26], [94mLoss[0m : 2.62447
[1mStep[0m  [10/26], [94mLoss[0m : 2.65720
[1mStep[0m  [12/26], [94mLoss[0m : 2.38212
[1mStep[0m  [14/26], [94mLoss[0m : 2.60451
[1mStep[0m  [16/26], [94mLoss[0m : 2.48216
[1mStep[0m  [18/26], [94mLoss[0m : 2.65918
[1mStep[0m  [20/26], [94mLoss[0m : 2.69928
[1mStep[0m  [22/26], [94mLoss[0m : 2.58808
[1mStep[0m  [24/26], [94mLoss[0m : 2.56224

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62838
[1mStep[0m  [2/26], [94mLoss[0m : 2.65984
[1mStep[0m  [4/26], [94mLoss[0m : 2.58935
[1mStep[0m  [6/26], [94mLoss[0m : 2.61244
[1mStep[0m  [8/26], [94mLoss[0m : 2.63386
[1mStep[0m  [10/26], [94mLoss[0m : 2.69670
[1mStep[0m  [12/26], [94mLoss[0m : 2.51496
[1mStep[0m  [14/26], [94mLoss[0m : 2.55595
[1mStep[0m  [16/26], [94mLoss[0m : 2.65139
[1mStep[0m  [18/26], [94mLoss[0m : 2.55780
[1mStep[0m  [20/26], [94mLoss[0m : 2.55256
[1mStep[0m  [22/26], [94mLoss[0m : 2.50209
[1mStep[0m  [24/26], [94mLoss[0m : 2.77459

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36551
[1mStep[0m  [2/26], [94mLoss[0m : 2.64126
[1mStep[0m  [4/26], [94mLoss[0m : 2.65283
[1mStep[0m  [6/26], [94mLoss[0m : 2.63652
[1mStep[0m  [8/26], [94mLoss[0m : 2.44083
[1mStep[0m  [10/26], [94mLoss[0m : 2.64426
[1mStep[0m  [12/26], [94mLoss[0m : 2.58844
[1mStep[0m  [14/26], [94mLoss[0m : 2.55810
[1mStep[0m  [16/26], [94mLoss[0m : 2.62944
[1mStep[0m  [18/26], [94mLoss[0m : 2.63103
[1mStep[0m  [20/26], [94mLoss[0m : 2.58906
[1mStep[0m  [22/26], [94mLoss[0m : 2.73681
[1mStep[0m  [24/26], [94mLoss[0m : 2.48343

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.649, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50899
[1mStep[0m  [2/26], [94mLoss[0m : 2.43345
[1mStep[0m  [4/26], [94mLoss[0m : 2.64612
[1mStep[0m  [6/26], [94mLoss[0m : 2.37609
[1mStep[0m  [8/26], [94mLoss[0m : 2.46502
[1mStep[0m  [10/26], [94mLoss[0m : 2.55811
[1mStep[0m  [12/26], [94mLoss[0m : 2.44410
[1mStep[0m  [14/26], [94mLoss[0m : 2.46408
[1mStep[0m  [16/26], [94mLoss[0m : 2.54982
[1mStep[0m  [18/26], [94mLoss[0m : 2.61625
[1mStep[0m  [20/26], [94mLoss[0m : 2.51708
[1mStep[0m  [22/26], [94mLoss[0m : 2.69804
[1mStep[0m  [24/26], [94mLoss[0m : 2.46959

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.630, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54758
[1mStep[0m  [2/26], [94mLoss[0m : 2.58363
[1mStep[0m  [4/26], [94mLoss[0m : 2.50137
[1mStep[0m  [6/26], [94mLoss[0m : 2.48220
[1mStep[0m  [8/26], [94mLoss[0m : 2.26391
[1mStep[0m  [10/26], [94mLoss[0m : 2.49790
[1mStep[0m  [12/26], [94mLoss[0m : 2.57011
[1mStep[0m  [14/26], [94mLoss[0m : 2.58326
[1mStep[0m  [16/26], [94mLoss[0m : 2.52790
[1mStep[0m  [18/26], [94mLoss[0m : 2.54075
[1mStep[0m  [20/26], [94mLoss[0m : 2.38049
[1mStep[0m  [22/26], [94mLoss[0m : 2.43755
[1mStep[0m  [24/26], [94mLoss[0m : 2.58579

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.701, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51190
[1mStep[0m  [2/26], [94mLoss[0m : 2.41213
[1mStep[0m  [4/26], [94mLoss[0m : 2.42453
[1mStep[0m  [6/26], [94mLoss[0m : 2.43188
[1mStep[0m  [8/26], [94mLoss[0m : 2.27632
[1mStep[0m  [10/26], [94mLoss[0m : 2.53292
[1mStep[0m  [12/26], [94mLoss[0m : 2.53218
[1mStep[0m  [14/26], [94mLoss[0m : 2.55048
[1mStep[0m  [16/26], [94mLoss[0m : 2.55059
[1mStep[0m  [18/26], [94mLoss[0m : 2.35113
[1mStep[0m  [20/26], [94mLoss[0m : 2.55457
[1mStep[0m  [22/26], [94mLoss[0m : 2.55459
[1mStep[0m  [24/26], [94mLoss[0m : 2.51887

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.615, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38755
[1mStep[0m  [2/26], [94mLoss[0m : 2.34848
[1mStep[0m  [4/26], [94mLoss[0m : 2.42074
[1mStep[0m  [6/26], [94mLoss[0m : 2.40711
[1mStep[0m  [8/26], [94mLoss[0m : 2.42073
[1mStep[0m  [10/26], [94mLoss[0m : 2.25156
[1mStep[0m  [12/26], [94mLoss[0m : 2.50553
[1mStep[0m  [14/26], [94mLoss[0m : 2.51923
[1mStep[0m  [16/26], [94mLoss[0m : 2.55047
[1mStep[0m  [18/26], [94mLoss[0m : 2.50755
[1mStep[0m  [20/26], [94mLoss[0m : 2.33922
[1mStep[0m  [22/26], [94mLoss[0m : 2.40888
[1mStep[0m  [24/26], [94mLoss[0m : 2.29432

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.671, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40396
[1mStep[0m  [2/26], [94mLoss[0m : 2.39113
[1mStep[0m  [4/26], [94mLoss[0m : 2.57575
[1mStep[0m  [6/26], [94mLoss[0m : 2.46605
[1mStep[0m  [8/26], [94mLoss[0m : 2.59083
[1mStep[0m  [10/26], [94mLoss[0m : 2.45073
[1mStep[0m  [12/26], [94mLoss[0m : 2.35282
[1mStep[0m  [14/26], [94mLoss[0m : 2.38366
[1mStep[0m  [16/26], [94mLoss[0m : 2.33753
[1mStep[0m  [18/26], [94mLoss[0m : 2.48206
[1mStep[0m  [20/26], [94mLoss[0m : 2.55717
[1mStep[0m  [22/26], [94mLoss[0m : 2.40473
[1mStep[0m  [24/26], [94mLoss[0m : 2.52589

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.699, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24129
[1mStep[0m  [2/26], [94mLoss[0m : 2.50323
[1mStep[0m  [4/26], [94mLoss[0m : 2.36819
[1mStep[0m  [6/26], [94mLoss[0m : 2.37149
[1mStep[0m  [8/26], [94mLoss[0m : 2.44937
[1mStep[0m  [10/26], [94mLoss[0m : 2.64364
[1mStep[0m  [12/26], [94mLoss[0m : 2.42032
[1mStep[0m  [14/26], [94mLoss[0m : 2.34630
[1mStep[0m  [16/26], [94mLoss[0m : 2.44791
[1mStep[0m  [18/26], [94mLoss[0m : 2.29533
[1mStep[0m  [20/26], [94mLoss[0m : 2.43423
[1mStep[0m  [22/26], [94mLoss[0m : 2.22159
[1mStep[0m  [24/26], [94mLoss[0m : 2.47383

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.594, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35015
[1mStep[0m  [2/26], [94mLoss[0m : 2.43748
[1mStep[0m  [4/26], [94mLoss[0m : 2.40810
[1mStep[0m  [6/26], [94mLoss[0m : 2.41703
[1mStep[0m  [8/26], [94mLoss[0m : 2.43562
[1mStep[0m  [10/26], [94mLoss[0m : 2.28679
[1mStep[0m  [12/26], [94mLoss[0m : 2.29395
[1mStep[0m  [14/26], [94mLoss[0m : 2.42127
[1mStep[0m  [16/26], [94mLoss[0m : 2.17332
[1mStep[0m  [18/26], [94mLoss[0m : 2.44655
[1mStep[0m  [20/26], [94mLoss[0m : 2.34807
[1mStep[0m  [22/26], [94mLoss[0m : 2.45081
[1mStep[0m  [24/26], [94mLoss[0m : 2.40050

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.520, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.21115
[1mStep[0m  [2/26], [94mLoss[0m : 2.17673
[1mStep[0m  [4/26], [94mLoss[0m : 2.35980
[1mStep[0m  [6/26], [94mLoss[0m : 2.32785
[1mStep[0m  [8/26], [94mLoss[0m : 2.15769
[1mStep[0m  [10/26], [94mLoss[0m : 2.32835
[1mStep[0m  [12/26], [94mLoss[0m : 2.37685
[1mStep[0m  [14/26], [94mLoss[0m : 2.28411
[1mStep[0m  [16/26], [94mLoss[0m : 2.46921
[1mStep[0m  [18/26], [94mLoss[0m : 2.36278
[1mStep[0m  [20/26], [94mLoss[0m : 2.31677
[1mStep[0m  [22/26], [94mLoss[0m : 2.44159
[1mStep[0m  [24/26], [94mLoss[0m : 2.26472

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22717
[1mStep[0m  [2/26], [94mLoss[0m : 2.31398
[1mStep[0m  [4/26], [94mLoss[0m : 2.34583
[1mStep[0m  [6/26], [94mLoss[0m : 2.14438
[1mStep[0m  [8/26], [94mLoss[0m : 2.18523
[1mStep[0m  [10/26], [94mLoss[0m : 2.25498
[1mStep[0m  [12/26], [94mLoss[0m : 2.40323
[1mStep[0m  [14/26], [94mLoss[0m : 2.24568
[1mStep[0m  [16/26], [94mLoss[0m : 2.38394
[1mStep[0m  [18/26], [94mLoss[0m : 2.34694
[1mStep[0m  [20/26], [94mLoss[0m : 2.12032
[1mStep[0m  [22/26], [94mLoss[0m : 2.32599
[1mStep[0m  [24/26], [94mLoss[0m : 2.21634

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25603
[1mStep[0m  [2/26], [94mLoss[0m : 2.49454
[1mStep[0m  [4/26], [94mLoss[0m : 2.40290
[1mStep[0m  [6/26], [94mLoss[0m : 2.39561
[1mStep[0m  [8/26], [94mLoss[0m : 2.19289
[1mStep[0m  [10/26], [94mLoss[0m : 2.32679
[1mStep[0m  [12/26], [94mLoss[0m : 2.12670
[1mStep[0m  [14/26], [94mLoss[0m : 2.32847
[1mStep[0m  [16/26], [94mLoss[0m : 2.16035
[1mStep[0m  [18/26], [94mLoss[0m : 2.28744
[1mStep[0m  [20/26], [94mLoss[0m : 2.20771
[1mStep[0m  [22/26], [94mLoss[0m : 2.23466
[1mStep[0m  [24/26], [94mLoss[0m : 2.29735

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.590, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23464
[1mStep[0m  [2/26], [94mLoss[0m : 2.12998
[1mStep[0m  [4/26], [94mLoss[0m : 2.23590
[1mStep[0m  [6/26], [94mLoss[0m : 2.10822
[1mStep[0m  [8/26], [94mLoss[0m : 2.22897
[1mStep[0m  [10/26], [94mLoss[0m : 2.28296
[1mStep[0m  [12/26], [94mLoss[0m : 2.21014
[1mStep[0m  [14/26], [94mLoss[0m : 2.22007
[1mStep[0m  [16/26], [94mLoss[0m : 2.27990
[1mStep[0m  [18/26], [94mLoss[0m : 2.40425
[1mStep[0m  [20/26], [94mLoss[0m : 2.31043
[1mStep[0m  [22/26], [94mLoss[0m : 2.40480
[1mStep[0m  [24/26], [94mLoss[0m : 2.18035

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.557, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08452
[1mStep[0m  [2/26], [94mLoss[0m : 2.27324
[1mStep[0m  [4/26], [94mLoss[0m : 2.17873
[1mStep[0m  [6/26], [94mLoss[0m : 2.20865
[1mStep[0m  [8/26], [94mLoss[0m : 2.07327
[1mStep[0m  [10/26], [94mLoss[0m : 2.20825
[1mStep[0m  [12/26], [94mLoss[0m : 2.23043
[1mStep[0m  [14/26], [94mLoss[0m : 2.20120
[1mStep[0m  [16/26], [94mLoss[0m : 2.26897
[1mStep[0m  [18/26], [94mLoss[0m : 2.14005
[1mStep[0m  [20/26], [94mLoss[0m : 2.12985
[1mStep[0m  [22/26], [94mLoss[0m : 2.23832
[1mStep[0m  [24/26], [94mLoss[0m : 2.07896

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19322
[1mStep[0m  [2/26], [94mLoss[0m : 2.15191
[1mStep[0m  [4/26], [94mLoss[0m : 2.17743
[1mStep[0m  [6/26], [94mLoss[0m : 2.15776
[1mStep[0m  [8/26], [94mLoss[0m : 2.13250
[1mStep[0m  [10/26], [94mLoss[0m : 2.19195
[1mStep[0m  [12/26], [94mLoss[0m : 2.26685
[1mStep[0m  [14/26], [94mLoss[0m : 2.22330
[1mStep[0m  [16/26], [94mLoss[0m : 2.22890
[1mStep[0m  [18/26], [94mLoss[0m : 2.20479
[1mStep[0m  [20/26], [94mLoss[0m : 2.32644
[1mStep[0m  [22/26], [94mLoss[0m : 2.15404
[1mStep[0m  [24/26], [94mLoss[0m : 2.11718

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96346
[1mStep[0m  [2/26], [94mLoss[0m : 2.10983
[1mStep[0m  [4/26], [94mLoss[0m : 2.22390
[1mStep[0m  [6/26], [94mLoss[0m : 2.03982
[1mStep[0m  [8/26], [94mLoss[0m : 2.05685
[1mStep[0m  [10/26], [94mLoss[0m : 2.13569
[1mStep[0m  [12/26], [94mLoss[0m : 2.13313
[1mStep[0m  [14/26], [94mLoss[0m : 2.10661
[1mStep[0m  [16/26], [94mLoss[0m : 2.04298
[1mStep[0m  [18/26], [94mLoss[0m : 2.10998
[1mStep[0m  [20/26], [94mLoss[0m : 2.11664
[1mStep[0m  [22/26], [94mLoss[0m : 2.31430
[1mStep[0m  [24/26], [94mLoss[0m : 2.26411

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19478
[1mStep[0m  [2/26], [94mLoss[0m : 2.09392
[1mStep[0m  [4/26], [94mLoss[0m : 2.18155
[1mStep[0m  [6/26], [94mLoss[0m : 1.96289
[1mStep[0m  [8/26], [94mLoss[0m : 2.16074
[1mStep[0m  [10/26], [94mLoss[0m : 2.21044
[1mStep[0m  [12/26], [94mLoss[0m : 2.13353
[1mStep[0m  [14/26], [94mLoss[0m : 2.14122
[1mStep[0m  [16/26], [94mLoss[0m : 2.08531
[1mStep[0m  [18/26], [94mLoss[0m : 2.05629
[1mStep[0m  [20/26], [94mLoss[0m : 2.12474
[1mStep[0m  [22/26], [94mLoss[0m : 2.09056
[1mStep[0m  [24/26], [94mLoss[0m : 2.12791

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07170
[1mStep[0m  [2/26], [94mLoss[0m : 2.14663
[1mStep[0m  [4/26], [94mLoss[0m : 2.02643
[1mStep[0m  [6/26], [94mLoss[0m : 2.12145
[1mStep[0m  [8/26], [94mLoss[0m : 1.92922
[1mStep[0m  [10/26], [94mLoss[0m : 2.07928
[1mStep[0m  [12/26], [94mLoss[0m : 2.12236
[1mStep[0m  [14/26], [94mLoss[0m : 2.09436
[1mStep[0m  [16/26], [94mLoss[0m : 2.23590
[1mStep[0m  [18/26], [94mLoss[0m : 2.17963
[1mStep[0m  [20/26], [94mLoss[0m : 2.01058
[1mStep[0m  [22/26], [94mLoss[0m : 2.07356
[1mStep[0m  [24/26], [94mLoss[0m : 2.17691

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.04283
[1mStep[0m  [2/26], [94mLoss[0m : 2.23172
[1mStep[0m  [4/26], [94mLoss[0m : 2.03956
[1mStep[0m  [6/26], [94mLoss[0m : 1.93406
[1mStep[0m  [8/26], [94mLoss[0m : 2.09085
[1mStep[0m  [10/26], [94mLoss[0m : 1.93127
[1mStep[0m  [12/26], [94mLoss[0m : 2.07488
[1mStep[0m  [14/26], [94mLoss[0m : 2.04476
[1mStep[0m  [16/26], [94mLoss[0m : 2.19571
[1mStep[0m  [18/26], [94mLoss[0m : 2.05987
[1mStep[0m  [20/26], [94mLoss[0m : 2.10267
[1mStep[0m  [22/26], [94mLoss[0m : 2.03703
[1mStep[0m  [24/26], [94mLoss[0m : 2.06371

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.08083
[1mStep[0m  [2/26], [94mLoss[0m : 1.99194
[1mStep[0m  [4/26], [94mLoss[0m : 2.07805
[1mStep[0m  [6/26], [94mLoss[0m : 2.04939
[1mStep[0m  [8/26], [94mLoss[0m : 2.08202
[1mStep[0m  [10/26], [94mLoss[0m : 2.08963
[1mStep[0m  [12/26], [94mLoss[0m : 2.14314
[1mStep[0m  [14/26], [94mLoss[0m : 2.14175
[1mStep[0m  [16/26], [94mLoss[0m : 2.01581
[1mStep[0m  [18/26], [94mLoss[0m : 2.04091
[1mStep[0m  [20/26], [94mLoss[0m : 2.02023
[1mStep[0m  [22/26], [94mLoss[0m : 1.96210
[1mStep[0m  [24/26], [94mLoss[0m : 2.15086

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.047, [92mTest[0m: 2.558, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93616
[1mStep[0m  [2/26], [94mLoss[0m : 2.07950
[1mStep[0m  [4/26], [94mLoss[0m : 2.05744
[1mStep[0m  [6/26], [94mLoss[0m : 2.08889
[1mStep[0m  [8/26], [94mLoss[0m : 1.89855
[1mStep[0m  [10/26], [94mLoss[0m : 2.04647
[1mStep[0m  [12/26], [94mLoss[0m : 1.98828
[1mStep[0m  [14/26], [94mLoss[0m : 2.08406
[1mStep[0m  [16/26], [94mLoss[0m : 1.91629
[1mStep[0m  [18/26], [94mLoss[0m : 1.97383
[1mStep[0m  [20/26], [94mLoss[0m : 2.04369
[1mStep[0m  [22/26], [94mLoss[0m : 2.01084
[1mStep[0m  [24/26], [94mLoss[0m : 2.05621

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.564, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.00555
[1mStep[0m  [2/26], [94mLoss[0m : 1.83270
[1mStep[0m  [4/26], [94mLoss[0m : 1.95533
[1mStep[0m  [6/26], [94mLoss[0m : 1.89910
[1mStep[0m  [8/26], [94mLoss[0m : 2.00660
[1mStep[0m  [10/26], [94mLoss[0m : 1.95138
[1mStep[0m  [12/26], [94mLoss[0m : 2.06669
[1mStep[0m  [14/26], [94mLoss[0m : 1.93359
[1mStep[0m  [16/26], [94mLoss[0m : 1.95657
[1mStep[0m  [18/26], [94mLoss[0m : 1.98704
[1mStep[0m  [20/26], [94mLoss[0m : 2.05615
[1mStep[0m  [22/26], [94mLoss[0m : 2.01983
[1mStep[0m  [24/26], [94mLoss[0m : 2.16165

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.525, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07323
[1mStep[0m  [2/26], [94mLoss[0m : 1.88076
[1mStep[0m  [4/26], [94mLoss[0m : 1.89756
[1mStep[0m  [6/26], [94mLoss[0m : 1.85649
[1mStep[0m  [8/26], [94mLoss[0m : 1.94159
[1mStep[0m  [10/26], [94mLoss[0m : 1.92078
[1mStep[0m  [12/26], [94mLoss[0m : 1.98454
[1mStep[0m  [14/26], [94mLoss[0m : 2.06533
[1mStep[0m  [16/26], [94mLoss[0m : 2.05420
[1mStep[0m  [18/26], [94mLoss[0m : 1.95701
[1mStep[0m  [20/26], [94mLoss[0m : 1.96499
[1mStep[0m  [22/26], [94mLoss[0m : 1.91116
[1mStep[0m  [24/26], [94mLoss[0m : 2.09878

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.525, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.86355
[1mStep[0m  [2/26], [94mLoss[0m : 1.94865
[1mStep[0m  [4/26], [94mLoss[0m : 1.95973
[1mStep[0m  [6/26], [94mLoss[0m : 1.97223
[1mStep[0m  [8/26], [94mLoss[0m : 1.86456
[1mStep[0m  [10/26], [94mLoss[0m : 1.89215
[1mStep[0m  [12/26], [94mLoss[0m : 1.91878
[1mStep[0m  [14/26], [94mLoss[0m : 2.02178
[1mStep[0m  [16/26], [94mLoss[0m : 2.02679
[1mStep[0m  [18/26], [94mLoss[0m : 2.01343
[1mStep[0m  [20/26], [94mLoss[0m : 1.84949
[1mStep[0m  [22/26], [94mLoss[0m : 1.95555
[1mStep[0m  [24/26], [94mLoss[0m : 1.97465

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.92353
[1mStep[0m  [2/26], [94mLoss[0m : 2.02282
[1mStep[0m  [4/26], [94mLoss[0m : 1.89315
[1mStep[0m  [6/26], [94mLoss[0m : 1.86578
[1mStep[0m  [8/26], [94mLoss[0m : 1.88143
[1mStep[0m  [10/26], [94mLoss[0m : 1.99326
[1mStep[0m  [12/26], [94mLoss[0m : 1.92172
[1mStep[0m  [14/26], [94mLoss[0m : 2.03842
[1mStep[0m  [16/26], [94mLoss[0m : 1.96282
[1mStep[0m  [18/26], [94mLoss[0m : 1.96048
[1mStep[0m  [20/26], [94mLoss[0m : 1.97665
[1mStep[0m  [22/26], [94mLoss[0m : 1.90539
[1mStep[0m  [24/26], [94mLoss[0m : 1.89886

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82107
[1mStep[0m  [2/26], [94mLoss[0m : 1.88596
[1mStep[0m  [4/26], [94mLoss[0m : 2.00862
[1mStep[0m  [6/26], [94mLoss[0m : 1.85695
[1mStep[0m  [8/26], [94mLoss[0m : 1.71672
[1mStep[0m  [10/26], [94mLoss[0m : 1.89176
[1mStep[0m  [12/26], [94mLoss[0m : 1.89837
[1mStep[0m  [14/26], [94mLoss[0m : 1.96346
[1mStep[0m  [16/26], [94mLoss[0m : 1.79547
[1mStep[0m  [18/26], [94mLoss[0m : 1.95378
[1mStep[0m  [20/26], [94mLoss[0m : 1.89150
[1mStep[0m  [22/26], [94mLoss[0m : 1.89119
[1mStep[0m  [24/26], [94mLoss[0m : 1.91391

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75511
[1mStep[0m  [2/26], [94mLoss[0m : 1.67659
[1mStep[0m  [4/26], [94mLoss[0m : 1.75148
[1mStep[0m  [6/26], [94mLoss[0m : 1.97096
[1mStep[0m  [8/26], [94mLoss[0m : 1.77604
[1mStep[0m  [10/26], [94mLoss[0m : 2.00550
[1mStep[0m  [12/26], [94mLoss[0m : 1.87852
[1mStep[0m  [14/26], [94mLoss[0m : 1.78198
[1mStep[0m  [16/26], [94mLoss[0m : 1.82573
[1mStep[0m  [18/26], [94mLoss[0m : 1.94039
[1mStep[0m  [20/26], [94mLoss[0m : 1.93015
[1mStep[0m  [22/26], [94mLoss[0m : 1.88907
[1mStep[0m  [24/26], [94mLoss[0m : 1.77727

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.85955
[1mStep[0m  [2/26], [94mLoss[0m : 1.80257
[1mStep[0m  [4/26], [94mLoss[0m : 1.85763
[1mStep[0m  [6/26], [94mLoss[0m : 1.92532
[1mStep[0m  [8/26], [94mLoss[0m : 1.81522
[1mStep[0m  [10/26], [94mLoss[0m : 1.72093
[1mStep[0m  [12/26], [94mLoss[0m : 1.92541
[1mStep[0m  [14/26], [94mLoss[0m : 1.91408
[1mStep[0m  [16/26], [94mLoss[0m : 1.84681
[1mStep[0m  [18/26], [94mLoss[0m : 1.76298
[1mStep[0m  [20/26], [94mLoss[0m : 1.83318
[1mStep[0m  [22/26], [94mLoss[0m : 2.04409
[1mStep[0m  [24/26], [94mLoss[0m : 1.92164

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.78990
[1mStep[0m  [2/26], [94mLoss[0m : 1.80349
[1mStep[0m  [4/26], [94mLoss[0m : 1.79128
[1mStep[0m  [6/26], [94mLoss[0m : 1.91930
[1mStep[0m  [8/26], [94mLoss[0m : 1.84670
[1mStep[0m  [10/26], [94mLoss[0m : 1.81437
[1mStep[0m  [12/26], [94mLoss[0m : 1.85159
[1mStep[0m  [14/26], [94mLoss[0m : 1.75852
[1mStep[0m  [16/26], [94mLoss[0m : 1.86182
[1mStep[0m  [18/26], [94mLoss[0m : 1.87346
[1mStep[0m  [20/26], [94mLoss[0m : 1.74876
[1mStep[0m  [22/26], [94mLoss[0m : 1.86692
[1mStep[0m  [24/26], [94mLoss[0m : 1.89460

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.460, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.501
====================================

Phase 2 - Evaluation MAE:  2.50057629438547
MAE score P1        2.411886
MAE score P2        2.500576
loss                1.827355
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay           0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.97867
[1mStep[0m  [2/26], [94mLoss[0m : 11.02302
[1mStep[0m  [4/26], [94mLoss[0m : 11.07521
[1mStep[0m  [6/26], [94mLoss[0m : 10.87216
[1mStep[0m  [8/26], [94mLoss[0m : 10.61577
[1mStep[0m  [10/26], [94mLoss[0m : 10.73365
[1mStep[0m  [12/26], [94mLoss[0m : 10.65661
[1mStep[0m  [14/26], [94mLoss[0m : 10.40928
[1mStep[0m  [16/26], [94mLoss[0m : 10.99348
[1mStep[0m  [18/26], [94mLoss[0m : 10.63003
[1mStep[0m  [20/26], [94mLoss[0m : 10.89809
[1mStep[0m  [22/26], [94mLoss[0m : 10.59720
[1mStep[0m  [24/26], [94mLoss[0m : 10.53900

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.763, [92mTest[0m: 10.785, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.50165
[1mStep[0m  [2/26], [94mLoss[0m : 10.64600
[1mStep[0m  [4/26], [94mLoss[0m : 10.78717
[1mStep[0m  [6/26], [94mLoss[0m : 10.35471
[1mStep[0m  [8/26], [94mLoss[0m : 10.56021
[1mStep[0m  [10/26], [94mLoss[0m : 10.46240
[1mStep[0m  [12/26], [94mLoss[0m : 10.61217
[1mStep[0m  [14/26], [94mLoss[0m : 10.65274
[1mStep[0m  [16/26], [94mLoss[0m : 10.06116
[1mStep[0m  [18/26], [94mLoss[0m : 10.65933
[1mStep[0m  [20/26], [94mLoss[0m : 10.54111
[1mStep[0m  [22/26], [94mLoss[0m : 10.34787
[1mStep[0m  [24/26], [94mLoss[0m : 10.40865

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.502, [92mTest[0m: 10.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.19927
[1mStep[0m  [2/26], [94mLoss[0m : 10.16505
[1mStep[0m  [4/26], [94mLoss[0m : 10.19373
[1mStep[0m  [6/26], [94mLoss[0m : 10.31339
[1mStep[0m  [8/26], [94mLoss[0m : 10.24854
[1mStep[0m  [10/26], [94mLoss[0m : 10.16458
[1mStep[0m  [12/26], [94mLoss[0m : 10.28416
[1mStep[0m  [14/26], [94mLoss[0m : 10.32798
[1mStep[0m  [16/26], [94mLoss[0m : 10.42787
[1mStep[0m  [18/26], [94mLoss[0m : 10.12545
[1mStep[0m  [20/26], [94mLoss[0m : 10.28461
[1mStep[0m  [22/26], [94mLoss[0m : 10.16667
[1mStep[0m  [24/26], [94mLoss[0m : 10.02661

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.233, [92mTest[0m: 10.239, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.85357
[1mStep[0m  [2/26], [94mLoss[0m : 10.24345
[1mStep[0m  [4/26], [94mLoss[0m : 10.07956
[1mStep[0m  [6/26], [94mLoss[0m : 9.73243
[1mStep[0m  [8/26], [94mLoss[0m : 9.90938
[1mStep[0m  [10/26], [94mLoss[0m : 10.03818
[1mStep[0m  [12/26], [94mLoss[0m : 9.99023
[1mStep[0m  [14/26], [94mLoss[0m : 9.76015
[1mStep[0m  [16/26], [94mLoss[0m : 9.90642
[1mStep[0m  [18/26], [94mLoss[0m : 9.67025
[1mStep[0m  [20/26], [94mLoss[0m : 9.90539
[1mStep[0m  [22/26], [94mLoss[0m : 10.02325
[1mStep[0m  [24/26], [94mLoss[0m : 9.98346

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.955, [92mTest[0m: 9.919, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.88132
[1mStep[0m  [2/26], [94mLoss[0m : 9.43854
[1mStep[0m  [4/26], [94mLoss[0m : 9.64094
[1mStep[0m  [6/26], [94mLoss[0m : 10.08216
[1mStep[0m  [8/26], [94mLoss[0m : 9.50009
[1mStep[0m  [10/26], [94mLoss[0m : 9.62042
[1mStep[0m  [12/26], [94mLoss[0m : 9.87209
[1mStep[0m  [14/26], [94mLoss[0m : 9.75748
[1mStep[0m  [16/26], [94mLoss[0m : 9.30425
[1mStep[0m  [18/26], [94mLoss[0m : 9.82778
[1mStep[0m  [20/26], [94mLoss[0m : 9.30763
[1mStep[0m  [22/26], [94mLoss[0m : 9.55154
[1mStep[0m  [24/26], [94mLoss[0m : 9.36938

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.652, [92mTest[0m: 9.617, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.55628
[1mStep[0m  [2/26], [94mLoss[0m : 9.69980
[1mStep[0m  [4/26], [94mLoss[0m : 9.34794
[1mStep[0m  [6/26], [94mLoss[0m : 9.38087
[1mStep[0m  [8/26], [94mLoss[0m : 9.40172
[1mStep[0m  [10/26], [94mLoss[0m : 9.12155
[1mStep[0m  [12/26], [94mLoss[0m : 9.07434
[1mStep[0m  [14/26], [94mLoss[0m : 9.55384
[1mStep[0m  [16/26], [94mLoss[0m : 9.43047
[1mStep[0m  [18/26], [94mLoss[0m : 9.21553
[1mStep[0m  [20/26], [94mLoss[0m : 9.33148
[1mStep[0m  [22/26], [94mLoss[0m : 9.15803
[1mStep[0m  [24/26], [94mLoss[0m : 9.15448

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.327, [92mTest[0m: 9.234, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.14595
[1mStep[0m  [2/26], [94mLoss[0m : 9.08455
[1mStep[0m  [4/26], [94mLoss[0m : 9.32335
[1mStep[0m  [6/26], [94mLoss[0m : 8.88552
[1mStep[0m  [8/26], [94mLoss[0m : 9.17247
[1mStep[0m  [10/26], [94mLoss[0m : 9.07714
[1mStep[0m  [12/26], [94mLoss[0m : 8.87327
[1mStep[0m  [14/26], [94mLoss[0m : 8.97826
[1mStep[0m  [16/26], [94mLoss[0m : 9.04483
[1mStep[0m  [18/26], [94mLoss[0m : 9.16783
[1mStep[0m  [20/26], [94mLoss[0m : 8.81142
[1mStep[0m  [22/26], [94mLoss[0m : 8.69547
[1mStep[0m  [24/26], [94mLoss[0m : 8.85197

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.987, [92mTest[0m: 8.823, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.86937
[1mStep[0m  [2/26], [94mLoss[0m : 8.92421
[1mStep[0m  [4/26], [94mLoss[0m : 8.89262
[1mStep[0m  [6/26], [94mLoss[0m : 8.68712
[1mStep[0m  [8/26], [94mLoss[0m : 8.88692
[1mStep[0m  [10/26], [94mLoss[0m : 8.63732
[1mStep[0m  [12/26], [94mLoss[0m : 8.81133
[1mStep[0m  [14/26], [94mLoss[0m : 8.20378
[1mStep[0m  [16/26], [94mLoss[0m : 8.47543
[1mStep[0m  [18/26], [94mLoss[0m : 8.31924
[1mStep[0m  [20/26], [94mLoss[0m : 8.35890
[1mStep[0m  [22/26], [94mLoss[0m : 8.41716
[1mStep[0m  [24/26], [94mLoss[0m : 8.41034

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.581, [92mTest[0m: 8.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.34605
[1mStep[0m  [2/26], [94mLoss[0m : 8.22974
[1mStep[0m  [4/26], [94mLoss[0m : 7.93900
[1mStep[0m  [6/26], [94mLoss[0m : 8.24876
[1mStep[0m  [8/26], [94mLoss[0m : 8.36164
[1mStep[0m  [10/26], [94mLoss[0m : 8.14937
[1mStep[0m  [12/26], [94mLoss[0m : 8.14688
[1mStep[0m  [14/26], [94mLoss[0m : 8.05993
[1mStep[0m  [16/26], [94mLoss[0m : 8.29533
[1mStep[0m  [18/26], [94mLoss[0m : 8.01315
[1mStep[0m  [20/26], [94mLoss[0m : 7.87941
[1mStep[0m  [22/26], [94mLoss[0m : 7.83833
[1mStep[0m  [24/26], [94mLoss[0m : 7.86495

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.112, [92mTest[0m: 7.919, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.97611
[1mStep[0m  [2/26], [94mLoss[0m : 7.98786
[1mStep[0m  [4/26], [94mLoss[0m : 7.70282
[1mStep[0m  [6/26], [94mLoss[0m : 7.82104
[1mStep[0m  [8/26], [94mLoss[0m : 7.88983
[1mStep[0m  [10/26], [94mLoss[0m : 7.51277
[1mStep[0m  [12/26], [94mLoss[0m : 7.73920
[1mStep[0m  [14/26], [94mLoss[0m : 7.36450
[1mStep[0m  [16/26], [94mLoss[0m : 7.52648
[1mStep[0m  [18/26], [94mLoss[0m : 7.24673
[1mStep[0m  [20/26], [94mLoss[0m : 7.49168
[1mStep[0m  [22/26], [94mLoss[0m : 7.12194
[1mStep[0m  [24/26], [94mLoss[0m : 7.29709

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.583, [92mTest[0m: 7.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.39079
[1mStep[0m  [2/26], [94mLoss[0m : 7.24249
[1mStep[0m  [4/26], [94mLoss[0m : 7.36367
[1mStep[0m  [6/26], [94mLoss[0m : 7.39418
[1mStep[0m  [8/26], [94mLoss[0m : 7.13817
[1mStep[0m  [10/26], [94mLoss[0m : 7.39886
[1mStep[0m  [12/26], [94mLoss[0m : 6.89231
[1mStep[0m  [14/26], [94mLoss[0m : 7.06585
[1mStep[0m  [16/26], [94mLoss[0m : 6.79791
[1mStep[0m  [18/26], [94mLoss[0m : 7.15630
[1mStep[0m  [20/26], [94mLoss[0m : 6.81601
[1mStep[0m  [22/26], [94mLoss[0m : 6.70444
[1mStep[0m  [24/26], [94mLoss[0m : 6.93393

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.031, [92mTest[0m: 6.723, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.82291
[1mStep[0m  [2/26], [94mLoss[0m : 6.70789
[1mStep[0m  [4/26], [94mLoss[0m : 6.64617
[1mStep[0m  [6/26], [94mLoss[0m : 6.65976
[1mStep[0m  [8/26], [94mLoss[0m : 6.78102
[1mStep[0m  [10/26], [94mLoss[0m : 6.60056
[1mStep[0m  [12/26], [94mLoss[0m : 6.54924
[1mStep[0m  [14/26], [94mLoss[0m : 6.21269
[1mStep[0m  [16/26], [94mLoss[0m : 6.63185
[1mStep[0m  [18/26], [94mLoss[0m : 6.32809
[1mStep[0m  [20/26], [94mLoss[0m : 6.13290
[1mStep[0m  [22/26], [94mLoss[0m : 6.25048
[1mStep[0m  [24/26], [94mLoss[0m : 6.36852

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.476, [92mTest[0m: 6.113, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.03719
[1mStep[0m  [2/26], [94mLoss[0m : 6.44319
[1mStep[0m  [4/26], [94mLoss[0m : 5.90752
[1mStep[0m  [6/26], [94mLoss[0m : 6.18476
[1mStep[0m  [8/26], [94mLoss[0m : 5.91317
[1mStep[0m  [10/26], [94mLoss[0m : 5.96786
[1mStep[0m  [12/26], [94mLoss[0m : 6.01762
[1mStep[0m  [14/26], [94mLoss[0m : 6.07235
[1mStep[0m  [16/26], [94mLoss[0m : 6.01306
[1mStep[0m  [18/26], [94mLoss[0m : 6.10694
[1mStep[0m  [20/26], [94mLoss[0m : 5.47107
[1mStep[0m  [22/26], [94mLoss[0m : 5.88929
[1mStep[0m  [24/26], [94mLoss[0m : 5.74200

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.939, [92mTest[0m: 5.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.56845
[1mStep[0m  [2/26], [94mLoss[0m : 5.78187
[1mStep[0m  [4/26], [94mLoss[0m : 5.71169
[1mStep[0m  [6/26], [94mLoss[0m : 5.63509
[1mStep[0m  [8/26], [94mLoss[0m : 5.61275
[1mStep[0m  [10/26], [94mLoss[0m : 5.55432
[1mStep[0m  [12/26], [94mLoss[0m : 5.51668
[1mStep[0m  [14/26], [94mLoss[0m : 5.15307
[1mStep[0m  [16/26], [94mLoss[0m : 5.31642
[1mStep[0m  [18/26], [94mLoss[0m : 5.38661
[1mStep[0m  [20/26], [94mLoss[0m : 5.05195
[1mStep[0m  [22/26], [94mLoss[0m : 5.41306
[1mStep[0m  [24/26], [94mLoss[0m : 5.58696

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.420, [92mTest[0m: 4.945, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.32355
[1mStep[0m  [2/26], [94mLoss[0m : 5.00311
[1mStep[0m  [4/26], [94mLoss[0m : 5.07777
[1mStep[0m  [6/26], [94mLoss[0m : 5.07511
[1mStep[0m  [8/26], [94mLoss[0m : 5.11720
[1mStep[0m  [10/26], [94mLoss[0m : 4.78456
[1mStep[0m  [12/26], [94mLoss[0m : 5.10325
[1mStep[0m  [14/26], [94mLoss[0m : 4.80889
[1mStep[0m  [16/26], [94mLoss[0m : 4.75697
[1mStep[0m  [18/26], [94mLoss[0m : 4.85001
[1mStep[0m  [20/26], [94mLoss[0m : 4.60439
[1mStep[0m  [22/26], [94mLoss[0m : 4.57568
[1mStep[0m  [24/26], [94mLoss[0m : 4.54078

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.907, [92mTest[0m: 4.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.92063
[1mStep[0m  [2/26], [94mLoss[0m : 4.41642
[1mStep[0m  [4/26], [94mLoss[0m : 4.42681
[1mStep[0m  [6/26], [94mLoss[0m : 4.63991
[1mStep[0m  [8/26], [94mLoss[0m : 4.63093
[1mStep[0m  [10/26], [94mLoss[0m : 4.55027
[1mStep[0m  [12/26], [94mLoss[0m : 4.36916
[1mStep[0m  [14/26], [94mLoss[0m : 4.64593
[1mStep[0m  [16/26], [94mLoss[0m : 4.47759
[1mStep[0m  [18/26], [94mLoss[0m : 4.30188
[1mStep[0m  [20/26], [94mLoss[0m : 4.38026
[1mStep[0m  [22/26], [94mLoss[0m : 4.13951
[1mStep[0m  [24/26], [94mLoss[0m : 4.33318

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.461, [92mTest[0m: 3.983, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.26617
[1mStep[0m  [2/26], [94mLoss[0m : 4.17062
[1mStep[0m  [4/26], [94mLoss[0m : 4.06430
[1mStep[0m  [6/26], [94mLoss[0m : 3.91439
[1mStep[0m  [8/26], [94mLoss[0m : 4.03481
[1mStep[0m  [10/26], [94mLoss[0m : 4.13671
[1mStep[0m  [12/26], [94mLoss[0m : 3.82924
[1mStep[0m  [14/26], [94mLoss[0m : 4.01113
[1mStep[0m  [16/26], [94mLoss[0m : 3.97752
[1mStep[0m  [18/26], [94mLoss[0m : 4.29563
[1mStep[0m  [20/26], [94mLoss[0m : 4.04967
[1mStep[0m  [22/26], [94mLoss[0m : 3.71163
[1mStep[0m  [24/26], [94mLoss[0m : 3.76051

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.007, [92mTest[0m: 3.590, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.93622
[1mStep[0m  [2/26], [94mLoss[0m : 3.82206
[1mStep[0m  [4/26], [94mLoss[0m : 3.98378
[1mStep[0m  [6/26], [94mLoss[0m : 3.83706
[1mStep[0m  [8/26], [94mLoss[0m : 3.83004
[1mStep[0m  [10/26], [94mLoss[0m : 3.68263
[1mStep[0m  [12/26], [94mLoss[0m : 3.84298
[1mStep[0m  [14/26], [94mLoss[0m : 3.61988
[1mStep[0m  [16/26], [94mLoss[0m : 3.75304
[1mStep[0m  [18/26], [94mLoss[0m : 3.23262
[1mStep[0m  [20/26], [94mLoss[0m : 3.61269
[1mStep[0m  [22/26], [94mLoss[0m : 3.67933
[1mStep[0m  [24/26], [94mLoss[0m : 3.76316

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.693, [92mTest[0m: 3.281, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.41413
[1mStep[0m  [2/26], [94mLoss[0m : 3.43430
[1mStep[0m  [4/26], [94mLoss[0m : 3.58073
[1mStep[0m  [6/26], [94mLoss[0m : 3.53565
[1mStep[0m  [8/26], [94mLoss[0m : 3.49053
[1mStep[0m  [10/26], [94mLoss[0m : 3.39304
[1mStep[0m  [12/26], [94mLoss[0m : 3.38464
[1mStep[0m  [14/26], [94mLoss[0m : 3.32770
[1mStep[0m  [16/26], [94mLoss[0m : 3.38822
[1mStep[0m  [18/26], [94mLoss[0m : 3.31778
[1mStep[0m  [20/26], [94mLoss[0m : 3.19017
[1mStep[0m  [22/26], [94mLoss[0m : 3.36106
[1mStep[0m  [24/26], [94mLoss[0m : 3.23406

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.358, [92mTest[0m: 3.023, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.22634
[1mStep[0m  [2/26], [94mLoss[0m : 3.34046
[1mStep[0m  [4/26], [94mLoss[0m : 2.94114
[1mStep[0m  [6/26], [94mLoss[0m : 3.21481
[1mStep[0m  [8/26], [94mLoss[0m : 3.27257
[1mStep[0m  [10/26], [94mLoss[0m : 3.08765
[1mStep[0m  [12/26], [94mLoss[0m : 3.16323
[1mStep[0m  [14/26], [94mLoss[0m : 3.17546
[1mStep[0m  [16/26], [94mLoss[0m : 2.98105
[1mStep[0m  [18/26], [94mLoss[0m : 3.05699
[1mStep[0m  [20/26], [94mLoss[0m : 3.01995
[1mStep[0m  [22/26], [94mLoss[0m : 3.26167
[1mStep[0m  [24/26], [94mLoss[0m : 3.14458

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.156, [92mTest[0m: 2.807, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.05539
[1mStep[0m  [2/26], [94mLoss[0m : 3.14424
[1mStep[0m  [4/26], [94mLoss[0m : 3.01762
[1mStep[0m  [6/26], [94mLoss[0m : 3.11701
[1mStep[0m  [8/26], [94mLoss[0m : 3.21655
[1mStep[0m  [10/26], [94mLoss[0m : 3.05394
[1mStep[0m  [12/26], [94mLoss[0m : 2.91785
[1mStep[0m  [14/26], [94mLoss[0m : 3.16681
[1mStep[0m  [16/26], [94mLoss[0m : 2.95827
[1mStep[0m  [18/26], [94mLoss[0m : 2.77020
[1mStep[0m  [20/26], [94mLoss[0m : 2.84529
[1mStep[0m  [22/26], [94mLoss[0m : 2.80226
[1mStep[0m  [24/26], [94mLoss[0m : 3.11366

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.014, [92mTest[0m: 2.656, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79655
[1mStep[0m  [2/26], [94mLoss[0m : 3.18401
[1mStep[0m  [4/26], [94mLoss[0m : 2.97347
[1mStep[0m  [6/26], [94mLoss[0m : 2.90118
[1mStep[0m  [8/26], [94mLoss[0m : 3.00513
[1mStep[0m  [10/26], [94mLoss[0m : 3.06521
[1mStep[0m  [12/26], [94mLoss[0m : 2.90815
[1mStep[0m  [14/26], [94mLoss[0m : 2.88602
[1mStep[0m  [16/26], [94mLoss[0m : 2.88921
[1mStep[0m  [18/26], [94mLoss[0m : 3.08259
[1mStep[0m  [20/26], [94mLoss[0m : 2.88749
[1mStep[0m  [22/26], [94mLoss[0m : 3.10719
[1mStep[0m  [24/26], [94mLoss[0m : 3.00735

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.960, [92mTest[0m: 2.584, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75848
[1mStep[0m  [2/26], [94mLoss[0m : 2.85147
[1mStep[0m  [4/26], [94mLoss[0m : 3.02810
[1mStep[0m  [6/26], [94mLoss[0m : 2.93009
[1mStep[0m  [8/26], [94mLoss[0m : 2.82984
[1mStep[0m  [10/26], [94mLoss[0m : 2.85374
[1mStep[0m  [12/26], [94mLoss[0m : 3.02277
[1mStep[0m  [14/26], [94mLoss[0m : 2.91269
[1mStep[0m  [16/26], [94mLoss[0m : 2.91639
[1mStep[0m  [18/26], [94mLoss[0m : 2.92408
[1mStep[0m  [20/26], [94mLoss[0m : 2.93469
[1mStep[0m  [22/26], [94mLoss[0m : 2.78370
[1mStep[0m  [24/26], [94mLoss[0m : 2.88610

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.899, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.90415
[1mStep[0m  [2/26], [94mLoss[0m : 2.98315
[1mStep[0m  [4/26], [94mLoss[0m : 2.90160
[1mStep[0m  [6/26], [94mLoss[0m : 3.04042
[1mStep[0m  [8/26], [94mLoss[0m : 2.83577
[1mStep[0m  [10/26], [94mLoss[0m : 2.80659
[1mStep[0m  [12/26], [94mLoss[0m : 2.80046
[1mStep[0m  [14/26], [94mLoss[0m : 2.81472
[1mStep[0m  [16/26], [94mLoss[0m : 2.95110
[1mStep[0m  [18/26], [94mLoss[0m : 2.97948
[1mStep[0m  [20/26], [94mLoss[0m : 2.73998
[1mStep[0m  [22/26], [94mLoss[0m : 3.06617
[1mStep[0m  [24/26], [94mLoss[0m : 2.79579

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.875, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.88040
[1mStep[0m  [2/26], [94mLoss[0m : 2.74302
[1mStep[0m  [4/26], [94mLoss[0m : 2.82571
[1mStep[0m  [6/26], [94mLoss[0m : 2.83704
[1mStep[0m  [8/26], [94mLoss[0m : 2.84409
[1mStep[0m  [10/26], [94mLoss[0m : 2.90083
[1mStep[0m  [12/26], [94mLoss[0m : 2.88348
[1mStep[0m  [14/26], [94mLoss[0m : 2.77015
[1mStep[0m  [16/26], [94mLoss[0m : 2.85092
[1mStep[0m  [18/26], [94mLoss[0m : 2.79580
[1mStep[0m  [20/26], [94mLoss[0m : 2.92284
[1mStep[0m  [22/26], [94mLoss[0m : 2.75799
[1mStep[0m  [24/26], [94mLoss[0m : 2.83877

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.852, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.87130
[1mStep[0m  [2/26], [94mLoss[0m : 2.64883
[1mStep[0m  [4/26], [94mLoss[0m : 2.73347
[1mStep[0m  [6/26], [94mLoss[0m : 2.82729
[1mStep[0m  [8/26], [94mLoss[0m : 2.83432
[1mStep[0m  [10/26], [94mLoss[0m : 2.81888
[1mStep[0m  [12/26], [94mLoss[0m : 2.79033
[1mStep[0m  [14/26], [94mLoss[0m : 2.82916
[1mStep[0m  [16/26], [94mLoss[0m : 2.93166
[1mStep[0m  [18/26], [94mLoss[0m : 2.78878
[1mStep[0m  [20/26], [94mLoss[0m : 2.91336
[1mStep[0m  [22/26], [94mLoss[0m : 2.95912
[1mStep[0m  [24/26], [94mLoss[0m : 2.80581

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.826, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85812
[1mStep[0m  [2/26], [94mLoss[0m : 2.79037
[1mStep[0m  [4/26], [94mLoss[0m : 3.01257
[1mStep[0m  [6/26], [94mLoss[0m : 2.86763
[1mStep[0m  [8/26], [94mLoss[0m : 2.89976
[1mStep[0m  [10/26], [94mLoss[0m : 2.84927
[1mStep[0m  [12/26], [94mLoss[0m : 2.86576
[1mStep[0m  [14/26], [94mLoss[0m : 2.63182
[1mStep[0m  [16/26], [94mLoss[0m : 2.83981
[1mStep[0m  [18/26], [94mLoss[0m : 2.71147
[1mStep[0m  [20/26], [94mLoss[0m : 2.64281
[1mStep[0m  [22/26], [94mLoss[0m : 2.87779
[1mStep[0m  [24/26], [94mLoss[0m : 2.98426

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.824, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71772
[1mStep[0m  [2/26], [94mLoss[0m : 2.87299
[1mStep[0m  [4/26], [94mLoss[0m : 2.66839
[1mStep[0m  [6/26], [94mLoss[0m : 2.99007
[1mStep[0m  [8/26], [94mLoss[0m : 2.87350
[1mStep[0m  [10/26], [94mLoss[0m : 2.91906
[1mStep[0m  [12/26], [94mLoss[0m : 2.63570
[1mStep[0m  [14/26], [94mLoss[0m : 2.72463
[1mStep[0m  [16/26], [94mLoss[0m : 2.86750
[1mStep[0m  [18/26], [94mLoss[0m : 2.83046
[1mStep[0m  [20/26], [94mLoss[0m : 2.83164
[1mStep[0m  [22/26], [94mLoss[0m : 2.93759
[1mStep[0m  [24/26], [94mLoss[0m : 2.84755

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.835, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66589
[1mStep[0m  [2/26], [94mLoss[0m : 2.81816
[1mStep[0m  [4/26], [94mLoss[0m : 2.76023
[1mStep[0m  [6/26], [94mLoss[0m : 2.55221
[1mStep[0m  [8/26], [94mLoss[0m : 2.65822
[1mStep[0m  [10/26], [94mLoss[0m : 2.70337
[1mStep[0m  [12/26], [94mLoss[0m : 2.80228
[1mStep[0m  [14/26], [94mLoss[0m : 2.78629
[1mStep[0m  [16/26], [94mLoss[0m : 2.93051
[1mStep[0m  [18/26], [94mLoss[0m : 2.94177
[1mStep[0m  [20/26], [94mLoss[0m : 2.75800
[1mStep[0m  [22/26], [94mLoss[0m : 2.80329
[1mStep[0m  [24/26], [94mLoss[0m : 2.66088

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.801, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.91390
[1mStep[0m  [2/26], [94mLoss[0m : 2.74256
[1mStep[0m  [4/26], [94mLoss[0m : 2.64020
[1mStep[0m  [6/26], [94mLoss[0m : 2.96231
[1mStep[0m  [8/26], [94mLoss[0m : 2.72489
[1mStep[0m  [10/26], [94mLoss[0m : 2.97030
[1mStep[0m  [12/26], [94mLoss[0m : 2.83793
[1mStep[0m  [14/26], [94mLoss[0m : 2.71975
[1mStep[0m  [16/26], [94mLoss[0m : 2.54889
[1mStep[0m  [18/26], [94mLoss[0m : 2.87679
[1mStep[0m  [20/26], [94mLoss[0m : 2.89899
[1mStep[0m  [22/26], [94mLoss[0m : 2.81268
[1mStep[0m  [24/26], [94mLoss[0m : 2.72726

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.803, [92mTest[0m: 2.492, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.482
====================================

Phase 1 - Evaluation MAE:  2.4819393708155704
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.86249
[1mStep[0m  [2/26], [94mLoss[0m : 2.77586
[1mStep[0m  [4/26], [94mLoss[0m : 2.86114
[1mStep[0m  [6/26], [94mLoss[0m : 2.87801
[1mStep[0m  [8/26], [94mLoss[0m : 2.78723
[1mStep[0m  [10/26], [94mLoss[0m : 2.72725
[1mStep[0m  [12/26], [94mLoss[0m : 2.74634
[1mStep[0m  [14/26], [94mLoss[0m : 2.80165
[1mStep[0m  [16/26], [94mLoss[0m : 2.78998
[1mStep[0m  [18/26], [94mLoss[0m : 2.72089
[1mStep[0m  [20/26], [94mLoss[0m : 2.77738
[1mStep[0m  [22/26], [94mLoss[0m : 2.77972
[1mStep[0m  [24/26], [94mLoss[0m : 2.76887

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78041
[1mStep[0m  [2/26], [94mLoss[0m : 2.80785
[1mStep[0m  [4/26], [94mLoss[0m : 2.92213
[1mStep[0m  [6/26], [94mLoss[0m : 2.79871
[1mStep[0m  [8/26], [94mLoss[0m : 2.85147
[1mStep[0m  [10/26], [94mLoss[0m : 2.64382
[1mStep[0m  [12/26], [94mLoss[0m : 2.96807
[1mStep[0m  [14/26], [94mLoss[0m : 2.85978
[1mStep[0m  [16/26], [94mLoss[0m : 2.86715
[1mStep[0m  [18/26], [94mLoss[0m : 2.70318
[1mStep[0m  [20/26], [94mLoss[0m : 2.93682
[1mStep[0m  [22/26], [94mLoss[0m : 2.64225
[1mStep[0m  [24/26], [94mLoss[0m : 2.65007

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.806, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.07553
[1mStep[0m  [2/26], [94mLoss[0m : 2.56845
[1mStep[0m  [4/26], [94mLoss[0m : 2.71684
[1mStep[0m  [6/26], [94mLoss[0m : 2.79192
[1mStep[0m  [8/26], [94mLoss[0m : 2.69184
[1mStep[0m  [10/26], [94mLoss[0m : 2.68973
[1mStep[0m  [12/26], [94mLoss[0m : 2.79859
[1mStep[0m  [14/26], [94mLoss[0m : 2.73389
[1mStep[0m  [16/26], [94mLoss[0m : 2.75048
[1mStep[0m  [18/26], [94mLoss[0m : 2.92407
[1mStep[0m  [20/26], [94mLoss[0m : 2.75907
[1mStep[0m  [22/26], [94mLoss[0m : 2.72639
[1mStep[0m  [24/26], [94mLoss[0m : 2.74057

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.786, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72402
[1mStep[0m  [2/26], [94mLoss[0m : 2.73756
[1mStep[0m  [4/26], [94mLoss[0m : 2.63563
[1mStep[0m  [6/26], [94mLoss[0m : 2.82619
[1mStep[0m  [8/26], [94mLoss[0m : 2.86099
[1mStep[0m  [10/26], [94mLoss[0m : 2.75862
[1mStep[0m  [12/26], [94mLoss[0m : 2.82953
[1mStep[0m  [14/26], [94mLoss[0m : 2.60688
[1mStep[0m  [16/26], [94mLoss[0m : 2.53575
[1mStep[0m  [18/26], [94mLoss[0m : 2.78498
[1mStep[0m  [20/26], [94mLoss[0m : 2.79865
[1mStep[0m  [22/26], [94mLoss[0m : 2.78934
[1mStep[0m  [24/26], [94mLoss[0m : 2.69373

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.575, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70377
[1mStep[0m  [2/26], [94mLoss[0m : 2.98117
[1mStep[0m  [4/26], [94mLoss[0m : 2.69105
[1mStep[0m  [6/26], [94mLoss[0m : 2.71498
[1mStep[0m  [8/26], [94mLoss[0m : 2.67274
[1mStep[0m  [10/26], [94mLoss[0m : 2.95310
[1mStep[0m  [12/26], [94mLoss[0m : 2.65176
[1mStep[0m  [14/26], [94mLoss[0m : 2.96287
[1mStep[0m  [16/26], [94mLoss[0m : 2.61730
[1mStep[0m  [18/26], [94mLoss[0m : 2.74735
[1mStep[0m  [20/26], [94mLoss[0m : 2.80257
[1mStep[0m  [22/26], [94mLoss[0m : 2.66742
[1mStep[0m  [24/26], [94mLoss[0m : 2.85806

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.590, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60503
[1mStep[0m  [2/26], [94mLoss[0m : 2.63091
[1mStep[0m  [4/26], [94mLoss[0m : 2.73156
[1mStep[0m  [6/26], [94mLoss[0m : 2.78738
[1mStep[0m  [8/26], [94mLoss[0m : 2.84073
[1mStep[0m  [10/26], [94mLoss[0m : 2.77756
[1mStep[0m  [12/26], [94mLoss[0m : 2.79600
[1mStep[0m  [14/26], [94mLoss[0m : 2.68148
[1mStep[0m  [16/26], [94mLoss[0m : 2.79517
[1mStep[0m  [18/26], [94mLoss[0m : 2.75673
[1mStep[0m  [20/26], [94mLoss[0m : 2.96104
[1mStep[0m  [22/26], [94mLoss[0m : 2.78315
[1mStep[0m  [24/26], [94mLoss[0m : 2.80556

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.744, [92mTest[0m: 2.586, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66138
[1mStep[0m  [2/26], [94mLoss[0m : 2.74762
[1mStep[0m  [4/26], [94mLoss[0m : 2.86527
[1mStep[0m  [6/26], [94mLoss[0m : 2.61496
[1mStep[0m  [8/26], [94mLoss[0m : 2.76101
[1mStep[0m  [10/26], [94mLoss[0m : 2.65247
[1mStep[0m  [12/26], [94mLoss[0m : 2.69466
[1mStep[0m  [14/26], [94mLoss[0m : 2.76742
[1mStep[0m  [16/26], [94mLoss[0m : 2.69925
[1mStep[0m  [18/26], [94mLoss[0m : 2.90586
[1mStep[0m  [20/26], [94mLoss[0m : 2.72035
[1mStep[0m  [22/26], [94mLoss[0m : 2.61922
[1mStep[0m  [24/26], [94mLoss[0m : 2.71209

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.82294
[1mStep[0m  [2/26], [94mLoss[0m : 2.71713
[1mStep[0m  [4/26], [94mLoss[0m : 2.71037
[1mStep[0m  [6/26], [94mLoss[0m : 2.70257
[1mStep[0m  [8/26], [94mLoss[0m : 2.71956
[1mStep[0m  [10/26], [94mLoss[0m : 2.70447
[1mStep[0m  [12/26], [94mLoss[0m : 2.64711
[1mStep[0m  [14/26], [94mLoss[0m : 2.59813
[1mStep[0m  [16/26], [94mLoss[0m : 2.74704
[1mStep[0m  [18/26], [94mLoss[0m : 2.61263
[1mStep[0m  [20/26], [94mLoss[0m : 2.70852
[1mStep[0m  [22/26], [94mLoss[0m : 2.75598
[1mStep[0m  [24/26], [94mLoss[0m : 2.63670

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85386
[1mStep[0m  [2/26], [94mLoss[0m : 2.64056
[1mStep[0m  [4/26], [94mLoss[0m : 2.63356
[1mStep[0m  [6/26], [94mLoss[0m : 2.64497
[1mStep[0m  [8/26], [94mLoss[0m : 2.42277
[1mStep[0m  [10/26], [94mLoss[0m : 2.73197
[1mStep[0m  [12/26], [94mLoss[0m : 2.71984
[1mStep[0m  [14/26], [94mLoss[0m : 2.56526
[1mStep[0m  [16/26], [94mLoss[0m : 2.52960
[1mStep[0m  [18/26], [94mLoss[0m : 2.74853
[1mStep[0m  [20/26], [94mLoss[0m : 2.63972
[1mStep[0m  [22/26], [94mLoss[0m : 2.58544
[1mStep[0m  [24/26], [94mLoss[0m : 2.77052

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65003
[1mStep[0m  [2/26], [94mLoss[0m : 2.73514
[1mStep[0m  [4/26], [94mLoss[0m : 2.63157
[1mStep[0m  [6/26], [94mLoss[0m : 2.54029
[1mStep[0m  [8/26], [94mLoss[0m : 2.50810
[1mStep[0m  [10/26], [94mLoss[0m : 2.68268
[1mStep[0m  [12/26], [94mLoss[0m : 2.75798
[1mStep[0m  [14/26], [94mLoss[0m : 2.80377
[1mStep[0m  [16/26], [94mLoss[0m : 2.56675
[1mStep[0m  [18/26], [94mLoss[0m : 2.77611
[1mStep[0m  [20/26], [94mLoss[0m : 2.71986
[1mStep[0m  [22/26], [94mLoss[0m : 2.57462
[1mStep[0m  [24/26], [94mLoss[0m : 2.74383

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.530, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64584
[1mStep[0m  [2/26], [94mLoss[0m : 2.72744
[1mStep[0m  [4/26], [94mLoss[0m : 2.61958
[1mStep[0m  [6/26], [94mLoss[0m : 2.80725
[1mStep[0m  [8/26], [94mLoss[0m : 2.43999
[1mStep[0m  [10/26], [94mLoss[0m : 2.59484
[1mStep[0m  [12/26], [94mLoss[0m : 2.48548
[1mStep[0m  [14/26], [94mLoss[0m : 2.64232
[1mStep[0m  [16/26], [94mLoss[0m : 2.68762
[1mStep[0m  [18/26], [94mLoss[0m : 2.67700
[1mStep[0m  [20/26], [94mLoss[0m : 2.63546
[1mStep[0m  [22/26], [94mLoss[0m : 2.67367
[1mStep[0m  [24/26], [94mLoss[0m : 2.65069

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50921
[1mStep[0m  [2/26], [94mLoss[0m : 2.62172
[1mStep[0m  [4/26], [94mLoss[0m : 2.57842
[1mStep[0m  [6/26], [94mLoss[0m : 2.77624
[1mStep[0m  [8/26], [94mLoss[0m : 2.63053
[1mStep[0m  [10/26], [94mLoss[0m : 2.67059
[1mStep[0m  [12/26], [94mLoss[0m : 2.64498
[1mStep[0m  [14/26], [94mLoss[0m : 2.68825
[1mStep[0m  [16/26], [94mLoss[0m : 2.59454
[1mStep[0m  [18/26], [94mLoss[0m : 2.64677
[1mStep[0m  [20/26], [94mLoss[0m : 2.47769
[1mStep[0m  [22/26], [94mLoss[0m : 2.45055
[1mStep[0m  [24/26], [94mLoss[0m : 2.67422

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53550
[1mStep[0m  [2/26], [94mLoss[0m : 2.49981
[1mStep[0m  [4/26], [94mLoss[0m : 2.62378
[1mStep[0m  [6/26], [94mLoss[0m : 2.69871
[1mStep[0m  [8/26], [94mLoss[0m : 2.62912
[1mStep[0m  [10/26], [94mLoss[0m : 2.66234
[1mStep[0m  [12/26], [94mLoss[0m : 2.65451
[1mStep[0m  [14/26], [94mLoss[0m : 2.63289
[1mStep[0m  [16/26], [94mLoss[0m : 2.68495
[1mStep[0m  [18/26], [94mLoss[0m : 2.66122
[1mStep[0m  [20/26], [94mLoss[0m : 2.59830
[1mStep[0m  [22/26], [94mLoss[0m : 2.68732
[1mStep[0m  [24/26], [94mLoss[0m : 2.57479

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45078
[1mStep[0m  [2/26], [94mLoss[0m : 2.57749
[1mStep[0m  [4/26], [94mLoss[0m : 2.49463
[1mStep[0m  [6/26], [94mLoss[0m : 2.65335
[1mStep[0m  [8/26], [94mLoss[0m : 2.54740
[1mStep[0m  [10/26], [94mLoss[0m : 2.64213
[1mStep[0m  [12/26], [94mLoss[0m : 2.57202
[1mStep[0m  [14/26], [94mLoss[0m : 2.66546
[1mStep[0m  [16/26], [94mLoss[0m : 2.66601
[1mStep[0m  [18/26], [94mLoss[0m : 2.74064
[1mStep[0m  [20/26], [94mLoss[0m : 2.58771
[1mStep[0m  [22/26], [94mLoss[0m : 2.60974
[1mStep[0m  [24/26], [94mLoss[0m : 2.59620

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64098
[1mStep[0m  [2/26], [94mLoss[0m : 2.52627
[1mStep[0m  [4/26], [94mLoss[0m : 2.52730
[1mStep[0m  [6/26], [94mLoss[0m : 2.51354
[1mStep[0m  [8/26], [94mLoss[0m : 2.40201
[1mStep[0m  [10/26], [94mLoss[0m : 2.53923
[1mStep[0m  [12/26], [94mLoss[0m : 2.57747
[1mStep[0m  [14/26], [94mLoss[0m : 2.55040
[1mStep[0m  [16/26], [94mLoss[0m : 2.66241
[1mStep[0m  [18/26], [94mLoss[0m : 2.45658
[1mStep[0m  [20/26], [94mLoss[0m : 2.61911
[1mStep[0m  [22/26], [94mLoss[0m : 2.62408
[1mStep[0m  [24/26], [94mLoss[0m : 2.68729

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48075
[1mStep[0m  [2/26], [94mLoss[0m : 2.43962
[1mStep[0m  [4/26], [94mLoss[0m : 2.52926
[1mStep[0m  [6/26], [94mLoss[0m : 2.65193
[1mStep[0m  [8/26], [94mLoss[0m : 2.70750
[1mStep[0m  [10/26], [94mLoss[0m : 2.47105
[1mStep[0m  [12/26], [94mLoss[0m : 2.57139
[1mStep[0m  [14/26], [94mLoss[0m : 2.53861
[1mStep[0m  [16/26], [94mLoss[0m : 2.66075
[1mStep[0m  [18/26], [94mLoss[0m : 2.55325
[1mStep[0m  [20/26], [94mLoss[0m : 2.57764
[1mStep[0m  [22/26], [94mLoss[0m : 2.67598
[1mStep[0m  [24/26], [94mLoss[0m : 2.37798

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55986
[1mStep[0m  [2/26], [94mLoss[0m : 2.50729
[1mStep[0m  [4/26], [94mLoss[0m : 2.44307
[1mStep[0m  [6/26], [94mLoss[0m : 2.43686
[1mStep[0m  [8/26], [94mLoss[0m : 2.52817
[1mStep[0m  [10/26], [94mLoss[0m : 2.52136
[1mStep[0m  [12/26], [94mLoss[0m : 2.53256
[1mStep[0m  [14/26], [94mLoss[0m : 2.53215
[1mStep[0m  [16/26], [94mLoss[0m : 2.74793
[1mStep[0m  [18/26], [94mLoss[0m : 2.55104
[1mStep[0m  [20/26], [94mLoss[0m : 2.65741
[1mStep[0m  [22/26], [94mLoss[0m : 2.60459
[1mStep[0m  [24/26], [94mLoss[0m : 2.48559

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48162
[1mStep[0m  [2/26], [94mLoss[0m : 2.36848
[1mStep[0m  [4/26], [94mLoss[0m : 2.45749
[1mStep[0m  [6/26], [94mLoss[0m : 2.59057
[1mStep[0m  [8/26], [94mLoss[0m : 2.54292
[1mStep[0m  [10/26], [94mLoss[0m : 2.64375
[1mStep[0m  [12/26], [94mLoss[0m : 2.52695
[1mStep[0m  [14/26], [94mLoss[0m : 2.45327
[1mStep[0m  [16/26], [94mLoss[0m : 2.39386
[1mStep[0m  [18/26], [94mLoss[0m : 2.46417
[1mStep[0m  [20/26], [94mLoss[0m : 2.71876
[1mStep[0m  [22/26], [94mLoss[0m : 2.52171
[1mStep[0m  [24/26], [94mLoss[0m : 2.54989

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43920
[1mStep[0m  [2/26], [94mLoss[0m : 2.39277
[1mStep[0m  [4/26], [94mLoss[0m : 2.52286
[1mStep[0m  [6/26], [94mLoss[0m : 2.48794
[1mStep[0m  [8/26], [94mLoss[0m : 2.48453
[1mStep[0m  [10/26], [94mLoss[0m : 2.75505
[1mStep[0m  [12/26], [94mLoss[0m : 2.49988
[1mStep[0m  [14/26], [94mLoss[0m : 2.64268
[1mStep[0m  [16/26], [94mLoss[0m : 2.42782
[1mStep[0m  [18/26], [94mLoss[0m : 2.53994
[1mStep[0m  [20/26], [94mLoss[0m : 2.61755
[1mStep[0m  [22/26], [94mLoss[0m : 2.48542
[1mStep[0m  [24/26], [94mLoss[0m : 2.40946

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52224
[1mStep[0m  [2/26], [94mLoss[0m : 2.46106
[1mStep[0m  [4/26], [94mLoss[0m : 2.45647
[1mStep[0m  [6/26], [94mLoss[0m : 2.32506
[1mStep[0m  [8/26], [94mLoss[0m : 2.45917
[1mStep[0m  [10/26], [94mLoss[0m : 2.65513
[1mStep[0m  [12/26], [94mLoss[0m : 2.44874
[1mStep[0m  [14/26], [94mLoss[0m : 2.49530
[1mStep[0m  [16/26], [94mLoss[0m : 2.52810
[1mStep[0m  [18/26], [94mLoss[0m : 2.47864
[1mStep[0m  [20/26], [94mLoss[0m : 2.56510
[1mStep[0m  [22/26], [94mLoss[0m : 2.41579
[1mStep[0m  [24/26], [94mLoss[0m : 2.52676

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.423, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31931
[1mStep[0m  [2/26], [94mLoss[0m : 2.56595
[1mStep[0m  [4/26], [94mLoss[0m : 2.46503
[1mStep[0m  [6/26], [94mLoss[0m : 2.52999
[1mStep[0m  [8/26], [94mLoss[0m : 2.54264
[1mStep[0m  [10/26], [94mLoss[0m : 2.36100
[1mStep[0m  [12/26], [94mLoss[0m : 2.37320
[1mStep[0m  [14/26], [94mLoss[0m : 2.34741
[1mStep[0m  [16/26], [94mLoss[0m : 2.47522
[1mStep[0m  [18/26], [94mLoss[0m : 2.44747
[1mStep[0m  [20/26], [94mLoss[0m : 2.43565
[1mStep[0m  [22/26], [94mLoss[0m : 2.47881
[1mStep[0m  [24/26], [94mLoss[0m : 2.50573

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.409, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41160
[1mStep[0m  [2/26], [94mLoss[0m : 2.34244
[1mStep[0m  [4/26], [94mLoss[0m : 2.59033
[1mStep[0m  [6/26], [94mLoss[0m : 2.43110
[1mStep[0m  [8/26], [94mLoss[0m : 2.41731
[1mStep[0m  [10/26], [94mLoss[0m : 2.22828
[1mStep[0m  [12/26], [94mLoss[0m : 2.44657
[1mStep[0m  [14/26], [94mLoss[0m : 2.48466
[1mStep[0m  [16/26], [94mLoss[0m : 2.62311
[1mStep[0m  [18/26], [94mLoss[0m : 2.43428
[1mStep[0m  [20/26], [94mLoss[0m : 2.23119
[1mStep[0m  [22/26], [94mLoss[0m : 2.37694
[1mStep[0m  [24/26], [94mLoss[0m : 2.53611

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.426, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37503
[1mStep[0m  [2/26], [94mLoss[0m : 2.54364
[1mStep[0m  [4/26], [94mLoss[0m : 2.47684
[1mStep[0m  [6/26], [94mLoss[0m : 2.46325
[1mStep[0m  [8/26], [94mLoss[0m : 2.51629
[1mStep[0m  [10/26], [94mLoss[0m : 2.55794
[1mStep[0m  [12/26], [94mLoss[0m : 2.26612
[1mStep[0m  [14/26], [94mLoss[0m : 2.44699
[1mStep[0m  [16/26], [94mLoss[0m : 2.32341
[1mStep[0m  [18/26], [94mLoss[0m : 2.36305
[1mStep[0m  [20/26], [94mLoss[0m : 2.36990
[1mStep[0m  [22/26], [94mLoss[0m : 2.39419
[1mStep[0m  [24/26], [94mLoss[0m : 2.44189

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38385
[1mStep[0m  [2/26], [94mLoss[0m : 2.42453
[1mStep[0m  [4/26], [94mLoss[0m : 2.46598
[1mStep[0m  [6/26], [94mLoss[0m : 2.28086
[1mStep[0m  [8/26], [94mLoss[0m : 2.34026
[1mStep[0m  [10/26], [94mLoss[0m : 2.35531
[1mStep[0m  [12/26], [94mLoss[0m : 2.45677
[1mStep[0m  [14/26], [94mLoss[0m : 2.36792
[1mStep[0m  [16/26], [94mLoss[0m : 2.46254
[1mStep[0m  [18/26], [94mLoss[0m : 2.39021
[1mStep[0m  [20/26], [94mLoss[0m : 2.27642
[1mStep[0m  [22/26], [94mLoss[0m : 2.39185
[1mStep[0m  [24/26], [94mLoss[0m : 2.41896

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49249
[1mStep[0m  [2/26], [94mLoss[0m : 2.39880
[1mStep[0m  [4/26], [94mLoss[0m : 2.33246
[1mStep[0m  [6/26], [94mLoss[0m : 2.45711
[1mStep[0m  [8/26], [94mLoss[0m : 2.22981
[1mStep[0m  [10/26], [94mLoss[0m : 2.39409
[1mStep[0m  [12/26], [94mLoss[0m : 2.27095
[1mStep[0m  [14/26], [94mLoss[0m : 2.29104
[1mStep[0m  [16/26], [94mLoss[0m : 2.21926
[1mStep[0m  [18/26], [94mLoss[0m : 2.10670
[1mStep[0m  [20/26], [94mLoss[0m : 2.37524
[1mStep[0m  [22/26], [94mLoss[0m : 2.49740
[1mStep[0m  [24/26], [94mLoss[0m : 2.49945

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.421, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28172
[1mStep[0m  [2/26], [94mLoss[0m : 2.39384
[1mStep[0m  [4/26], [94mLoss[0m : 2.37762
[1mStep[0m  [6/26], [94mLoss[0m : 2.31004
[1mStep[0m  [8/26], [94mLoss[0m : 2.36773
[1mStep[0m  [10/26], [94mLoss[0m : 2.38164
[1mStep[0m  [12/26], [94mLoss[0m : 2.55157
[1mStep[0m  [14/26], [94mLoss[0m : 2.32593
[1mStep[0m  [16/26], [94mLoss[0m : 2.28466
[1mStep[0m  [18/26], [94mLoss[0m : 2.32262
[1mStep[0m  [20/26], [94mLoss[0m : 2.32442
[1mStep[0m  [22/26], [94mLoss[0m : 2.28184
[1mStep[0m  [24/26], [94mLoss[0m : 2.58283

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28673
[1mStep[0m  [2/26], [94mLoss[0m : 2.20721
[1mStep[0m  [4/26], [94mLoss[0m : 2.26633
[1mStep[0m  [6/26], [94mLoss[0m : 2.33979
[1mStep[0m  [8/26], [94mLoss[0m : 2.44268
[1mStep[0m  [10/26], [94mLoss[0m : 2.42123
[1mStep[0m  [12/26], [94mLoss[0m : 2.35292
[1mStep[0m  [14/26], [94mLoss[0m : 2.42504
[1mStep[0m  [16/26], [94mLoss[0m : 2.40349
[1mStep[0m  [18/26], [94mLoss[0m : 2.33753
[1mStep[0m  [20/26], [94mLoss[0m : 2.30375
[1mStep[0m  [22/26], [94mLoss[0m : 2.39694
[1mStep[0m  [24/26], [94mLoss[0m : 2.31893

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.445, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32273
[1mStep[0m  [2/26], [94mLoss[0m : 2.20199
[1mStep[0m  [4/26], [94mLoss[0m : 2.46857
[1mStep[0m  [6/26], [94mLoss[0m : 2.48606
[1mStep[0m  [8/26], [94mLoss[0m : 2.16808
[1mStep[0m  [10/26], [94mLoss[0m : 2.30127
[1mStep[0m  [12/26], [94mLoss[0m : 2.45401
[1mStep[0m  [14/26], [94mLoss[0m : 2.51118
[1mStep[0m  [16/26], [94mLoss[0m : 2.40723
[1mStep[0m  [18/26], [94mLoss[0m : 2.34850
[1mStep[0m  [20/26], [94mLoss[0m : 2.36233
[1mStep[0m  [22/26], [94mLoss[0m : 2.33852
[1mStep[0m  [24/26], [94mLoss[0m : 2.28828

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37851
[1mStep[0m  [2/26], [94mLoss[0m : 2.39542
[1mStep[0m  [4/26], [94mLoss[0m : 2.40895
[1mStep[0m  [6/26], [94mLoss[0m : 2.36141
[1mStep[0m  [8/26], [94mLoss[0m : 2.33897
[1mStep[0m  [10/26], [94mLoss[0m : 2.31455
[1mStep[0m  [12/26], [94mLoss[0m : 2.34239
[1mStep[0m  [14/26], [94mLoss[0m : 2.17714
[1mStep[0m  [16/26], [94mLoss[0m : 2.14509
[1mStep[0m  [18/26], [94mLoss[0m : 2.41945
[1mStep[0m  [20/26], [94mLoss[0m : 2.28898
[1mStep[0m  [22/26], [94mLoss[0m : 2.36387
[1mStep[0m  [24/26], [94mLoss[0m : 2.39085

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.437, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.19404
[1mStep[0m  [2/26], [94mLoss[0m : 2.20093
[1mStep[0m  [4/26], [94mLoss[0m : 2.30058
[1mStep[0m  [6/26], [94mLoss[0m : 2.36130
[1mStep[0m  [8/26], [94mLoss[0m : 2.31689
[1mStep[0m  [10/26], [94mLoss[0m : 2.22265
[1mStep[0m  [12/26], [94mLoss[0m : 2.21382
[1mStep[0m  [14/26], [94mLoss[0m : 2.24349
[1mStep[0m  [16/26], [94mLoss[0m : 2.36163
[1mStep[0m  [18/26], [94mLoss[0m : 2.40831
[1mStep[0m  [20/26], [94mLoss[0m : 2.38347
[1mStep[0m  [22/26], [94mLoss[0m : 2.25938
[1mStep[0m  [24/26], [94mLoss[0m : 2.23755

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.430, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.424
====================================

Phase 2 - Evaluation MAE:  2.4239431894742527
MAE score P1       2.481939
MAE score P2       2.423943
loss               2.288709
learning_rate      0.007525
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/427], [94mLoss[0m : 10.69632
[1mStep[0m  [42/427], [94mLoss[0m : 9.25954
[1mStep[0m  [84/427], [94mLoss[0m : 9.47698
[1mStep[0m  [126/427], [94mLoss[0m : 4.95536
[1mStep[0m  [168/427], [94mLoss[0m : 4.05024
[1mStep[0m  [210/427], [94mLoss[0m : 2.74238
[1mStep[0m  [252/427], [94mLoss[0m : 3.54017
[1mStep[0m  [294/427], [94mLoss[0m : 2.93087
[1mStep[0m  [336/427], [94mLoss[0m : 2.46329
[1mStep[0m  [378/427], [94mLoss[0m : 2.78113
[1mStep[0m  [420/427], [94mLoss[0m : 2.51693

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.815, [92mTest[0m: 10.784, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.91376
[1mStep[0m  [42/427], [94mLoss[0m : 3.39073
[1mStep[0m  [84/427], [94mLoss[0m : 2.00866
[1mStep[0m  [126/427], [94mLoss[0m : 2.67515
[1mStep[0m  [168/427], [94mLoss[0m : 2.69298
[1mStep[0m  [210/427], [94mLoss[0m : 2.95310
[1mStep[0m  [252/427], [94mLoss[0m : 2.54824
[1mStep[0m  [294/427], [94mLoss[0m : 2.95860
[1mStep[0m  [336/427], [94mLoss[0m : 2.96197
[1mStep[0m  [378/427], [94mLoss[0m : 2.74971
[1mStep[0m  [420/427], [94mLoss[0m : 2.43058

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.61142
[1mStep[0m  [42/427], [94mLoss[0m : 2.61610
[1mStep[0m  [84/427], [94mLoss[0m : 1.58926
[1mStep[0m  [126/427], [94mLoss[0m : 2.54968
[1mStep[0m  [168/427], [94mLoss[0m : 2.28190
[1mStep[0m  [210/427], [94mLoss[0m : 2.47896
[1mStep[0m  [252/427], [94mLoss[0m : 2.64787
[1mStep[0m  [294/427], [94mLoss[0m : 2.72861
[1mStep[0m  [336/427], [94mLoss[0m : 2.96086
[1mStep[0m  [378/427], [94mLoss[0m : 2.12163
[1mStep[0m  [420/427], [94mLoss[0m : 2.43118

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.552, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.25749
[1mStep[0m  [42/427], [94mLoss[0m : 3.06079
[1mStep[0m  [84/427], [94mLoss[0m : 2.94819
[1mStep[0m  [126/427], [94mLoss[0m : 2.41743
[1mStep[0m  [168/427], [94mLoss[0m : 2.62531
[1mStep[0m  [210/427], [94mLoss[0m : 2.69271
[1mStep[0m  [252/427], [94mLoss[0m : 2.32777
[1mStep[0m  [294/427], [94mLoss[0m : 2.58151
[1mStep[0m  [336/427], [94mLoss[0m : 3.00672
[1mStep[0m  [378/427], [94mLoss[0m : 2.61685
[1mStep[0m  [420/427], [94mLoss[0m : 2.42187

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.78811
[1mStep[0m  [42/427], [94mLoss[0m : 2.65508
[1mStep[0m  [84/427], [94mLoss[0m : 3.10923
[1mStep[0m  [126/427], [94mLoss[0m : 2.63889
[1mStep[0m  [168/427], [94mLoss[0m : 2.83849
[1mStep[0m  [210/427], [94mLoss[0m : 2.25993
[1mStep[0m  [252/427], [94mLoss[0m : 2.08573
[1mStep[0m  [294/427], [94mLoss[0m : 2.91034
[1mStep[0m  [336/427], [94mLoss[0m : 3.21519
[1mStep[0m  [378/427], [94mLoss[0m : 2.39981
[1mStep[0m  [420/427], [94mLoss[0m : 2.48019

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.55272
[1mStep[0m  [42/427], [94mLoss[0m : 2.09278
[1mStep[0m  [84/427], [94mLoss[0m : 2.85296
[1mStep[0m  [126/427], [94mLoss[0m : 2.34524
[1mStep[0m  [168/427], [94mLoss[0m : 2.03408
[1mStep[0m  [210/427], [94mLoss[0m : 2.64968
[1mStep[0m  [252/427], [94mLoss[0m : 3.05757
[1mStep[0m  [294/427], [94mLoss[0m : 2.59098
[1mStep[0m  [336/427], [94mLoss[0m : 2.32796
[1mStep[0m  [378/427], [94mLoss[0m : 1.94577
[1mStep[0m  [420/427], [94mLoss[0m : 2.35143

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.422, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.22143
[1mStep[0m  [42/427], [94mLoss[0m : 2.29182
[1mStep[0m  [84/427], [94mLoss[0m : 2.53077
[1mStep[0m  [126/427], [94mLoss[0m : 2.51231
[1mStep[0m  [168/427], [94mLoss[0m : 3.25853
[1mStep[0m  [210/427], [94mLoss[0m : 1.87579
[1mStep[0m  [252/427], [94mLoss[0m : 2.46988
[1mStep[0m  [294/427], [94mLoss[0m : 2.37318
[1mStep[0m  [336/427], [94mLoss[0m : 2.36584
[1mStep[0m  [378/427], [94mLoss[0m : 2.69551
[1mStep[0m  [420/427], [94mLoss[0m : 2.71382

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.96079
[1mStep[0m  [42/427], [94mLoss[0m : 2.64870
[1mStep[0m  [84/427], [94mLoss[0m : 2.71078
[1mStep[0m  [126/427], [94mLoss[0m : 2.12365
[1mStep[0m  [168/427], [94mLoss[0m : 2.51094
[1mStep[0m  [210/427], [94mLoss[0m : 1.90111
[1mStep[0m  [252/427], [94mLoss[0m : 3.03132
[1mStep[0m  [294/427], [94mLoss[0m : 2.43707
[1mStep[0m  [336/427], [94mLoss[0m : 3.08667
[1mStep[0m  [378/427], [94mLoss[0m : 2.12335
[1mStep[0m  [420/427], [94mLoss[0m : 3.01269

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32857
[1mStep[0m  [42/427], [94mLoss[0m : 2.75633
[1mStep[0m  [84/427], [94mLoss[0m : 2.83974
[1mStep[0m  [126/427], [94mLoss[0m : 2.45830
[1mStep[0m  [168/427], [94mLoss[0m : 2.36627
[1mStep[0m  [210/427], [94mLoss[0m : 2.83218
[1mStep[0m  [252/427], [94mLoss[0m : 2.19994
[1mStep[0m  [294/427], [94mLoss[0m : 2.42997
[1mStep[0m  [336/427], [94mLoss[0m : 2.43826
[1mStep[0m  [378/427], [94mLoss[0m : 2.15162
[1mStep[0m  [420/427], [94mLoss[0m : 2.29571

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.30026
[1mStep[0m  [42/427], [94mLoss[0m : 1.94877
[1mStep[0m  [84/427], [94mLoss[0m : 1.98213
[1mStep[0m  [126/427], [94mLoss[0m : 2.15753
[1mStep[0m  [168/427], [94mLoss[0m : 2.28612
[1mStep[0m  [210/427], [94mLoss[0m : 2.50605
[1mStep[0m  [252/427], [94mLoss[0m : 2.28391
[1mStep[0m  [294/427], [94mLoss[0m : 2.11325
[1mStep[0m  [336/427], [94mLoss[0m : 3.20874
[1mStep[0m  [378/427], [94mLoss[0m : 2.88505
[1mStep[0m  [420/427], [94mLoss[0m : 3.18040

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.10687
[1mStep[0m  [42/427], [94mLoss[0m : 2.59350
[1mStep[0m  [84/427], [94mLoss[0m : 2.70859
[1mStep[0m  [126/427], [94mLoss[0m : 2.13408
[1mStep[0m  [168/427], [94mLoss[0m : 2.76551
[1mStep[0m  [210/427], [94mLoss[0m : 2.55956
[1mStep[0m  [252/427], [94mLoss[0m : 3.15507
[1mStep[0m  [294/427], [94mLoss[0m : 2.11560
[1mStep[0m  [336/427], [94mLoss[0m : 2.17522
[1mStep[0m  [378/427], [94mLoss[0m : 2.23048
[1mStep[0m  [420/427], [94mLoss[0m : 2.24744

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.411, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.91135
[1mStep[0m  [42/427], [94mLoss[0m : 2.35963
[1mStep[0m  [84/427], [94mLoss[0m : 2.68725
[1mStep[0m  [126/427], [94mLoss[0m : 2.81058
[1mStep[0m  [168/427], [94mLoss[0m : 2.86558
[1mStep[0m  [210/427], [94mLoss[0m : 2.36513
[1mStep[0m  [252/427], [94mLoss[0m : 2.03290
[1mStep[0m  [294/427], [94mLoss[0m : 2.63142
[1mStep[0m  [336/427], [94mLoss[0m : 2.69203
[1mStep[0m  [378/427], [94mLoss[0m : 2.84419
[1mStep[0m  [420/427], [94mLoss[0m : 2.01261

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.83383
[1mStep[0m  [42/427], [94mLoss[0m : 2.29825
[1mStep[0m  [84/427], [94mLoss[0m : 2.13308
[1mStep[0m  [126/427], [94mLoss[0m : 1.88210
[1mStep[0m  [168/427], [94mLoss[0m : 2.79950
[1mStep[0m  [210/427], [94mLoss[0m : 2.20066
[1mStep[0m  [252/427], [94mLoss[0m : 2.54339
[1mStep[0m  [294/427], [94mLoss[0m : 2.13435
[1mStep[0m  [336/427], [94mLoss[0m : 2.85794
[1mStep[0m  [378/427], [94mLoss[0m : 2.35839
[1mStep[0m  [420/427], [94mLoss[0m : 2.26621

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.17107
[1mStep[0m  [42/427], [94mLoss[0m : 2.27697
[1mStep[0m  [84/427], [94mLoss[0m : 2.73018
[1mStep[0m  [126/427], [94mLoss[0m : 2.14210
[1mStep[0m  [168/427], [94mLoss[0m : 2.28145
[1mStep[0m  [210/427], [94mLoss[0m : 3.08158
[1mStep[0m  [252/427], [94mLoss[0m : 2.41319
[1mStep[0m  [294/427], [94mLoss[0m : 2.24163
[1mStep[0m  [336/427], [94mLoss[0m : 2.75337
[1mStep[0m  [378/427], [94mLoss[0m : 2.82025
[1mStep[0m  [420/427], [94mLoss[0m : 1.95184

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.77636
[1mStep[0m  [42/427], [94mLoss[0m : 3.58574
[1mStep[0m  [84/427], [94mLoss[0m : 2.18563
[1mStep[0m  [126/427], [94mLoss[0m : 2.21097
[1mStep[0m  [168/427], [94mLoss[0m : 2.12010
[1mStep[0m  [210/427], [94mLoss[0m : 2.41807
[1mStep[0m  [252/427], [94mLoss[0m : 2.35214
[1mStep[0m  [294/427], [94mLoss[0m : 2.16039
[1mStep[0m  [336/427], [94mLoss[0m : 2.09852
[1mStep[0m  [378/427], [94mLoss[0m : 2.41970
[1mStep[0m  [420/427], [94mLoss[0m : 2.49498

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.13719
[1mStep[0m  [42/427], [94mLoss[0m : 2.36115
[1mStep[0m  [84/427], [94mLoss[0m : 2.61584
[1mStep[0m  [126/427], [94mLoss[0m : 2.30862
[1mStep[0m  [168/427], [94mLoss[0m : 2.01796
[1mStep[0m  [210/427], [94mLoss[0m : 2.56026
[1mStep[0m  [252/427], [94mLoss[0m : 2.44530
[1mStep[0m  [294/427], [94mLoss[0m : 1.81110
[1mStep[0m  [336/427], [94mLoss[0m : 2.34163
[1mStep[0m  [378/427], [94mLoss[0m : 2.62835
[1mStep[0m  [420/427], [94mLoss[0m : 2.29949

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.52457
[1mStep[0m  [42/427], [94mLoss[0m : 2.20550
[1mStep[0m  [84/427], [94mLoss[0m : 1.89906
[1mStep[0m  [126/427], [94mLoss[0m : 2.45701
[1mStep[0m  [168/427], [94mLoss[0m : 2.10007
[1mStep[0m  [210/427], [94mLoss[0m : 2.62094
[1mStep[0m  [252/427], [94mLoss[0m : 2.65999
[1mStep[0m  [294/427], [94mLoss[0m : 2.37610
[1mStep[0m  [336/427], [94mLoss[0m : 2.20498
[1mStep[0m  [378/427], [94mLoss[0m : 2.75939
[1mStep[0m  [420/427], [94mLoss[0m : 2.32460

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.64233
[1mStep[0m  [42/427], [94mLoss[0m : 2.62668
[1mStep[0m  [84/427], [94mLoss[0m : 3.21018
[1mStep[0m  [126/427], [94mLoss[0m : 2.53145
[1mStep[0m  [168/427], [94mLoss[0m : 2.59791
[1mStep[0m  [210/427], [94mLoss[0m : 2.26616
[1mStep[0m  [252/427], [94mLoss[0m : 3.15935
[1mStep[0m  [294/427], [94mLoss[0m : 2.32550
[1mStep[0m  [336/427], [94mLoss[0m : 2.63728
[1mStep[0m  [378/427], [94mLoss[0m : 2.36878
[1mStep[0m  [420/427], [94mLoss[0m : 2.37045

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.50058
[1mStep[0m  [42/427], [94mLoss[0m : 2.10385
[1mStep[0m  [84/427], [94mLoss[0m : 2.32568
[1mStep[0m  [126/427], [94mLoss[0m : 2.65563
[1mStep[0m  [168/427], [94mLoss[0m : 2.26272
[1mStep[0m  [210/427], [94mLoss[0m : 2.53575
[1mStep[0m  [252/427], [94mLoss[0m : 2.89383
[1mStep[0m  [294/427], [94mLoss[0m : 2.10049
[1mStep[0m  [336/427], [94mLoss[0m : 2.33866
[1mStep[0m  [378/427], [94mLoss[0m : 2.07670
[1mStep[0m  [420/427], [94mLoss[0m : 2.24362

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.36591
[1mStep[0m  [42/427], [94mLoss[0m : 2.49165
[1mStep[0m  [84/427], [94mLoss[0m : 1.59802
[1mStep[0m  [126/427], [94mLoss[0m : 2.92910
[1mStep[0m  [168/427], [94mLoss[0m : 1.90675
[1mStep[0m  [210/427], [94mLoss[0m : 1.69479
[1mStep[0m  [252/427], [94mLoss[0m : 2.13727
[1mStep[0m  [294/427], [94mLoss[0m : 2.52551
[1mStep[0m  [336/427], [94mLoss[0m : 2.64291
[1mStep[0m  [378/427], [94mLoss[0m : 2.13652
[1mStep[0m  [420/427], [94mLoss[0m : 2.97022

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.49174
[1mStep[0m  [42/427], [94mLoss[0m : 2.25149
[1mStep[0m  [84/427], [94mLoss[0m : 2.90485
[1mStep[0m  [126/427], [94mLoss[0m : 2.28396
[1mStep[0m  [168/427], [94mLoss[0m : 2.33817
[1mStep[0m  [210/427], [94mLoss[0m : 2.77524
[1mStep[0m  [252/427], [94mLoss[0m : 2.35699
[1mStep[0m  [294/427], [94mLoss[0m : 2.20911
[1mStep[0m  [336/427], [94mLoss[0m : 2.06995
[1mStep[0m  [378/427], [94mLoss[0m : 2.29058
[1mStep[0m  [420/427], [94mLoss[0m : 2.53262

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.381, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.14403
[1mStep[0m  [42/427], [94mLoss[0m : 2.20454
[1mStep[0m  [84/427], [94mLoss[0m : 2.86897
[1mStep[0m  [126/427], [94mLoss[0m : 2.34882
[1mStep[0m  [168/427], [94mLoss[0m : 2.65149
[1mStep[0m  [210/427], [94mLoss[0m : 2.13294
[1mStep[0m  [252/427], [94mLoss[0m : 2.18649
[1mStep[0m  [294/427], [94mLoss[0m : 2.19277
[1mStep[0m  [336/427], [94mLoss[0m : 2.05580
[1mStep[0m  [378/427], [94mLoss[0m : 2.30514
[1mStep[0m  [420/427], [94mLoss[0m : 2.86514

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.418, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.00210
[1mStep[0m  [42/427], [94mLoss[0m : 2.20441
[1mStep[0m  [84/427], [94mLoss[0m : 2.30624
[1mStep[0m  [126/427], [94mLoss[0m : 2.17699
[1mStep[0m  [168/427], [94mLoss[0m : 2.29049
[1mStep[0m  [210/427], [94mLoss[0m : 2.17238
[1mStep[0m  [252/427], [94mLoss[0m : 2.24429
[1mStep[0m  [294/427], [94mLoss[0m : 2.24314
[1mStep[0m  [336/427], [94mLoss[0m : 2.45020
[1mStep[0m  [378/427], [94mLoss[0m : 2.19693
[1mStep[0m  [420/427], [94mLoss[0m : 2.59937

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.379, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.82543
[1mStep[0m  [42/427], [94mLoss[0m : 2.51675
[1mStep[0m  [84/427], [94mLoss[0m : 2.66801
[1mStep[0m  [126/427], [94mLoss[0m : 2.30877
[1mStep[0m  [168/427], [94mLoss[0m : 2.44090
[1mStep[0m  [210/427], [94mLoss[0m : 2.21484
[1mStep[0m  [252/427], [94mLoss[0m : 2.53971
[1mStep[0m  [294/427], [94mLoss[0m : 2.35287
[1mStep[0m  [336/427], [94mLoss[0m : 2.10299
[1mStep[0m  [378/427], [94mLoss[0m : 2.99075
[1mStep[0m  [420/427], [94mLoss[0m : 1.98933

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.378, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.31411
[1mStep[0m  [42/427], [94mLoss[0m : 1.80065
[1mStep[0m  [84/427], [94mLoss[0m : 2.77174
[1mStep[0m  [126/427], [94mLoss[0m : 3.10404
[1mStep[0m  [168/427], [94mLoss[0m : 1.89255
[1mStep[0m  [210/427], [94mLoss[0m : 2.69458
[1mStep[0m  [252/427], [94mLoss[0m : 2.56961
[1mStep[0m  [294/427], [94mLoss[0m : 2.48608
[1mStep[0m  [336/427], [94mLoss[0m : 2.55294
[1mStep[0m  [378/427], [94mLoss[0m : 2.15025
[1mStep[0m  [420/427], [94mLoss[0m : 2.32193

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.369, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.23956
[1mStep[0m  [42/427], [94mLoss[0m : 2.13825
[1mStep[0m  [84/427], [94mLoss[0m : 2.35373
[1mStep[0m  [126/427], [94mLoss[0m : 2.24466
[1mStep[0m  [168/427], [94mLoss[0m : 2.25823
[1mStep[0m  [210/427], [94mLoss[0m : 2.25974
[1mStep[0m  [252/427], [94mLoss[0m : 1.84053
[1mStep[0m  [294/427], [94mLoss[0m : 2.16306
[1mStep[0m  [336/427], [94mLoss[0m : 2.21609
[1mStep[0m  [378/427], [94mLoss[0m : 2.43759
[1mStep[0m  [420/427], [94mLoss[0m : 2.08730

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.406, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.50181
[1mStep[0m  [42/427], [94mLoss[0m : 3.00235
[1mStep[0m  [84/427], [94mLoss[0m : 2.80198
[1mStep[0m  [126/427], [94mLoss[0m : 2.81025
[1mStep[0m  [168/427], [94mLoss[0m : 1.69825
[1mStep[0m  [210/427], [94mLoss[0m : 2.70951
[1mStep[0m  [252/427], [94mLoss[0m : 2.19358
[1mStep[0m  [294/427], [94mLoss[0m : 2.85153
[1mStep[0m  [336/427], [94mLoss[0m : 2.38802
[1mStep[0m  [378/427], [94mLoss[0m : 2.27234
[1mStep[0m  [420/427], [94mLoss[0m : 2.49467

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.384, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.67784
[1mStep[0m  [42/427], [94mLoss[0m : 2.00471
[1mStep[0m  [84/427], [94mLoss[0m : 2.45553
[1mStep[0m  [126/427], [94mLoss[0m : 2.50344
[1mStep[0m  [168/427], [94mLoss[0m : 2.24484
[1mStep[0m  [210/427], [94mLoss[0m : 2.42374
[1mStep[0m  [252/427], [94mLoss[0m : 2.23419
[1mStep[0m  [294/427], [94mLoss[0m : 2.24941
[1mStep[0m  [336/427], [94mLoss[0m : 3.05322
[1mStep[0m  [378/427], [94mLoss[0m : 2.14919
[1mStep[0m  [420/427], [94mLoss[0m : 1.98938

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.24645
[1mStep[0m  [42/427], [94mLoss[0m : 2.38449
[1mStep[0m  [84/427], [94mLoss[0m : 2.25135
[1mStep[0m  [126/427], [94mLoss[0m : 2.63108
[1mStep[0m  [168/427], [94mLoss[0m : 2.76944
[1mStep[0m  [210/427], [94mLoss[0m : 2.46440
[1mStep[0m  [252/427], [94mLoss[0m : 2.31510
[1mStep[0m  [294/427], [94mLoss[0m : 2.03681
[1mStep[0m  [336/427], [94mLoss[0m : 2.38886
[1mStep[0m  [378/427], [94mLoss[0m : 2.45421
[1mStep[0m  [420/427], [94mLoss[0m : 1.79966

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.386, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.24686
[1mStep[0m  [42/427], [94mLoss[0m : 2.64590
[1mStep[0m  [84/427], [94mLoss[0m : 2.05149
[1mStep[0m  [126/427], [94mLoss[0m : 2.34283
[1mStep[0m  [168/427], [94mLoss[0m : 1.95409
[1mStep[0m  [210/427], [94mLoss[0m : 2.61155
[1mStep[0m  [252/427], [94mLoss[0m : 2.16961
[1mStep[0m  [294/427], [94mLoss[0m : 2.74501
[1mStep[0m  [336/427], [94mLoss[0m : 1.98992
[1mStep[0m  [378/427], [94mLoss[0m : 2.42944
[1mStep[0m  [420/427], [94mLoss[0m : 2.99512

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.405
====================================

Phase 1 - Evaluation MAE:  2.4045042594273887
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/427], [94mLoss[0m : 2.62180
[1mStep[0m  [42/427], [94mLoss[0m : 2.37429
[1mStep[0m  [84/427], [94mLoss[0m : 2.50110
[1mStep[0m  [126/427], [94mLoss[0m : 2.29702
[1mStep[0m  [168/427], [94mLoss[0m : 1.76180
[1mStep[0m  [210/427], [94mLoss[0m : 2.88000
[1mStep[0m  [252/427], [94mLoss[0m : 1.83709
[1mStep[0m  [294/427], [94mLoss[0m : 2.67193
[1mStep[0m  [336/427], [94mLoss[0m : 2.28316
[1mStep[0m  [378/427], [94mLoss[0m : 2.68594
[1mStep[0m  [420/427], [94mLoss[0m : 2.11337

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.27893
[1mStep[0m  [42/427], [94mLoss[0m : 2.52390
[1mStep[0m  [84/427], [94mLoss[0m : 2.36387
[1mStep[0m  [126/427], [94mLoss[0m : 1.89056
[1mStep[0m  [168/427], [94mLoss[0m : 2.29740
[1mStep[0m  [210/427], [94mLoss[0m : 2.06530
[1mStep[0m  [252/427], [94mLoss[0m : 1.62995
[1mStep[0m  [294/427], [94mLoss[0m : 2.01379
[1mStep[0m  [336/427], [94mLoss[0m : 1.87532
[1mStep[0m  [378/427], [94mLoss[0m : 2.49114
[1mStep[0m  [420/427], [94mLoss[0m : 2.15795

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.71000
[1mStep[0m  [42/427], [94mLoss[0m : 2.54367
[1mStep[0m  [84/427], [94mLoss[0m : 2.60893
[1mStep[0m  [126/427], [94mLoss[0m : 2.45370
[1mStep[0m  [168/427], [94mLoss[0m : 2.28583
[1mStep[0m  [210/427], [94mLoss[0m : 2.88154
[1mStep[0m  [252/427], [94mLoss[0m : 3.05324
[1mStep[0m  [294/427], [94mLoss[0m : 2.72073
[1mStep[0m  [336/427], [94mLoss[0m : 2.39128
[1mStep[0m  [378/427], [94mLoss[0m : 2.61134
[1mStep[0m  [420/427], [94mLoss[0m : 2.37894

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.52513
[1mStep[0m  [42/427], [94mLoss[0m : 2.52624
[1mStep[0m  [84/427], [94mLoss[0m : 2.21488
[1mStep[0m  [126/427], [94mLoss[0m : 2.27748
[1mStep[0m  [168/427], [94mLoss[0m : 2.70047
[1mStep[0m  [210/427], [94mLoss[0m : 2.51403
[1mStep[0m  [252/427], [94mLoss[0m : 2.32909
[1mStep[0m  [294/427], [94mLoss[0m : 2.27487
[1mStep[0m  [336/427], [94mLoss[0m : 2.07133
[1mStep[0m  [378/427], [94mLoss[0m : 2.54276
[1mStep[0m  [420/427], [94mLoss[0m : 1.72178

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.36045
[1mStep[0m  [42/427], [94mLoss[0m : 2.56863
[1mStep[0m  [84/427], [94mLoss[0m : 2.47114
[1mStep[0m  [126/427], [94mLoss[0m : 1.98988
[1mStep[0m  [168/427], [94mLoss[0m : 1.89742
[1mStep[0m  [210/427], [94mLoss[0m : 2.07472
[1mStep[0m  [252/427], [94mLoss[0m : 1.97318
[1mStep[0m  [294/427], [94mLoss[0m : 2.16800
[1mStep[0m  [336/427], [94mLoss[0m : 1.99032
[1mStep[0m  [378/427], [94mLoss[0m : 1.55725
[1mStep[0m  [420/427], [94mLoss[0m : 2.30580

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.22574
[1mStep[0m  [42/427], [94mLoss[0m : 2.49686
[1mStep[0m  [84/427], [94mLoss[0m : 1.94376
[1mStep[0m  [126/427], [94mLoss[0m : 2.50644
[1mStep[0m  [168/427], [94mLoss[0m : 2.24913
[1mStep[0m  [210/427], [94mLoss[0m : 2.24771
[1mStep[0m  [252/427], [94mLoss[0m : 2.57940
[1mStep[0m  [294/427], [94mLoss[0m : 1.76295
[1mStep[0m  [336/427], [94mLoss[0m : 2.09175
[1mStep[0m  [378/427], [94mLoss[0m : 2.33372
[1mStep[0m  [420/427], [94mLoss[0m : 2.45275

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.54081
[1mStep[0m  [42/427], [94mLoss[0m : 2.04162
[1mStep[0m  [84/427], [94mLoss[0m : 2.36471
[1mStep[0m  [126/427], [94mLoss[0m : 2.36790
[1mStep[0m  [168/427], [94mLoss[0m : 2.08233
[1mStep[0m  [210/427], [94mLoss[0m : 2.41192
[1mStep[0m  [252/427], [94mLoss[0m : 2.13232
[1mStep[0m  [294/427], [94mLoss[0m : 2.48244
[1mStep[0m  [336/427], [94mLoss[0m : 1.61704
[1mStep[0m  [378/427], [94mLoss[0m : 2.42655
[1mStep[0m  [420/427], [94mLoss[0m : 2.04877

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32202
[1mStep[0m  [42/427], [94mLoss[0m : 2.15500
[1mStep[0m  [84/427], [94mLoss[0m : 1.92774
[1mStep[0m  [126/427], [94mLoss[0m : 2.37697
[1mStep[0m  [168/427], [94mLoss[0m : 2.08436
[1mStep[0m  [210/427], [94mLoss[0m : 2.29788
[1mStep[0m  [252/427], [94mLoss[0m : 1.37026
[1mStep[0m  [294/427], [94mLoss[0m : 2.35610
[1mStep[0m  [336/427], [94mLoss[0m : 2.17759
[1mStep[0m  [378/427], [94mLoss[0m : 1.55406
[1mStep[0m  [420/427], [94mLoss[0m : 2.03537

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.46632
[1mStep[0m  [42/427], [94mLoss[0m : 1.91784
[1mStep[0m  [84/427], [94mLoss[0m : 2.03674
[1mStep[0m  [126/427], [94mLoss[0m : 2.03241
[1mStep[0m  [168/427], [94mLoss[0m : 1.76577
[1mStep[0m  [210/427], [94mLoss[0m : 2.80201
[1mStep[0m  [252/427], [94mLoss[0m : 2.53138
[1mStep[0m  [294/427], [94mLoss[0m : 2.10382
[1mStep[0m  [336/427], [94mLoss[0m : 2.12128
[1mStep[0m  [378/427], [94mLoss[0m : 2.29203
[1mStep[0m  [420/427], [94mLoss[0m : 1.62793

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.80883
[1mStep[0m  [42/427], [94mLoss[0m : 2.19983
[1mStep[0m  [84/427], [94mLoss[0m : 2.34958
[1mStep[0m  [126/427], [94mLoss[0m : 1.92474
[1mStep[0m  [168/427], [94mLoss[0m : 2.32189
[1mStep[0m  [210/427], [94mLoss[0m : 2.28261
[1mStep[0m  [252/427], [94mLoss[0m : 2.11306
[1mStep[0m  [294/427], [94mLoss[0m : 2.24032
[1mStep[0m  [336/427], [94mLoss[0m : 1.53413
[1mStep[0m  [378/427], [94mLoss[0m : 2.12807
[1mStep[0m  [420/427], [94mLoss[0m : 2.42808

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.53518
[1mStep[0m  [42/427], [94mLoss[0m : 2.49120
[1mStep[0m  [84/427], [94mLoss[0m : 2.36236
[1mStep[0m  [126/427], [94mLoss[0m : 2.15129
[1mStep[0m  [168/427], [94mLoss[0m : 1.90994
[1mStep[0m  [210/427], [94mLoss[0m : 2.18846
[1mStep[0m  [252/427], [94mLoss[0m : 2.13189
[1mStep[0m  [294/427], [94mLoss[0m : 1.54273
[1mStep[0m  [336/427], [94mLoss[0m : 1.54036
[1mStep[0m  [378/427], [94mLoss[0m : 1.99154
[1mStep[0m  [420/427], [94mLoss[0m : 1.90600

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.79931
[1mStep[0m  [42/427], [94mLoss[0m : 1.33105
[1mStep[0m  [84/427], [94mLoss[0m : 1.92904
[1mStep[0m  [126/427], [94mLoss[0m : 1.71064
[1mStep[0m  [168/427], [94mLoss[0m : 1.83281
[1mStep[0m  [210/427], [94mLoss[0m : 1.51800
[1mStep[0m  [252/427], [94mLoss[0m : 1.70665
[1mStep[0m  [294/427], [94mLoss[0m : 2.49377
[1mStep[0m  [336/427], [94mLoss[0m : 2.29355
[1mStep[0m  [378/427], [94mLoss[0m : 2.00296
[1mStep[0m  [420/427], [94mLoss[0m : 1.67263

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.70136
[1mStep[0m  [42/427], [94mLoss[0m : 1.78818
[1mStep[0m  [84/427], [94mLoss[0m : 2.10266
[1mStep[0m  [126/427], [94mLoss[0m : 1.64699
[1mStep[0m  [168/427], [94mLoss[0m : 1.89768
[1mStep[0m  [210/427], [94mLoss[0m : 1.53342
[1mStep[0m  [252/427], [94mLoss[0m : 1.51666
[1mStep[0m  [294/427], [94mLoss[0m : 1.63445
[1mStep[0m  [336/427], [94mLoss[0m : 1.51644
[1mStep[0m  [378/427], [94mLoss[0m : 2.11586
[1mStep[0m  [420/427], [94mLoss[0m : 1.78066

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.57968
[1mStep[0m  [42/427], [94mLoss[0m : 2.04821
[1mStep[0m  [84/427], [94mLoss[0m : 1.82480
[1mStep[0m  [126/427], [94mLoss[0m : 2.42879
[1mStep[0m  [168/427], [94mLoss[0m : 2.03530
[1mStep[0m  [210/427], [94mLoss[0m : 2.02236
[1mStep[0m  [252/427], [94mLoss[0m : 1.58372
[1mStep[0m  [294/427], [94mLoss[0m : 2.01458
[1mStep[0m  [336/427], [94mLoss[0m : 1.73738
[1mStep[0m  [378/427], [94mLoss[0m : 1.88191
[1mStep[0m  [420/427], [94mLoss[0m : 2.39605

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.44078
[1mStep[0m  [42/427], [94mLoss[0m : 1.60201
[1mStep[0m  [84/427], [94mLoss[0m : 1.84345
[1mStep[0m  [126/427], [94mLoss[0m : 1.96166
[1mStep[0m  [168/427], [94mLoss[0m : 1.94402
[1mStep[0m  [210/427], [94mLoss[0m : 1.82354
[1mStep[0m  [252/427], [94mLoss[0m : 2.15215
[1mStep[0m  [294/427], [94mLoss[0m : 2.61224
[1mStep[0m  [336/427], [94mLoss[0m : 2.28649
[1mStep[0m  [378/427], [94mLoss[0m : 2.08496
[1mStep[0m  [420/427], [94mLoss[0m : 1.82262

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.75130
[1mStep[0m  [42/427], [94mLoss[0m : 1.69610
[1mStep[0m  [84/427], [94mLoss[0m : 1.98979
[1mStep[0m  [126/427], [94mLoss[0m : 1.57334
[1mStep[0m  [168/427], [94mLoss[0m : 1.78265
[1mStep[0m  [210/427], [94mLoss[0m : 1.75533
[1mStep[0m  [252/427], [94mLoss[0m : 1.81691
[1mStep[0m  [294/427], [94mLoss[0m : 1.99407
[1mStep[0m  [336/427], [94mLoss[0m : 1.61689
[1mStep[0m  [378/427], [94mLoss[0m : 1.75700
[1mStep[0m  [420/427], [94mLoss[0m : 1.46741

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.81239
[1mStep[0m  [42/427], [94mLoss[0m : 1.74001
[1mStep[0m  [84/427], [94mLoss[0m : 1.71507
[1mStep[0m  [126/427], [94mLoss[0m : 1.66945
[1mStep[0m  [168/427], [94mLoss[0m : 2.24517
[1mStep[0m  [210/427], [94mLoss[0m : 1.50853
[1mStep[0m  [252/427], [94mLoss[0m : 1.96115
[1mStep[0m  [294/427], [94mLoss[0m : 1.24396
[1mStep[0m  [336/427], [94mLoss[0m : 2.49846
[1mStep[0m  [378/427], [94mLoss[0m : 1.63399
[1mStep[0m  [420/427], [94mLoss[0m : 1.86720

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.50960
[1mStep[0m  [42/427], [94mLoss[0m : 2.51376
[1mStep[0m  [84/427], [94mLoss[0m : 1.80572
[1mStep[0m  [126/427], [94mLoss[0m : 1.85983
[1mStep[0m  [168/427], [94mLoss[0m : 1.55689
[1mStep[0m  [210/427], [94mLoss[0m : 1.89408
[1mStep[0m  [252/427], [94mLoss[0m : 1.73067
[1mStep[0m  [294/427], [94mLoss[0m : 1.90048
[1mStep[0m  [336/427], [94mLoss[0m : 1.82672
[1mStep[0m  [378/427], [94mLoss[0m : 2.16739
[1mStep[0m  [420/427], [94mLoss[0m : 1.49756

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.98651
[1mStep[0m  [42/427], [94mLoss[0m : 1.79463
[1mStep[0m  [84/427], [94mLoss[0m : 1.97648
[1mStep[0m  [126/427], [94mLoss[0m : 1.56529
[1mStep[0m  [168/427], [94mLoss[0m : 1.48415
[1mStep[0m  [210/427], [94mLoss[0m : 1.48168
[1mStep[0m  [252/427], [94mLoss[0m : 1.45118
[1mStep[0m  [294/427], [94mLoss[0m : 1.97130
[1mStep[0m  [336/427], [94mLoss[0m : 1.82234
[1mStep[0m  [378/427], [94mLoss[0m : 2.43109
[1mStep[0m  [420/427], [94mLoss[0m : 1.91968

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.745, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.61255
[1mStep[0m  [42/427], [94mLoss[0m : 2.01484
[1mStep[0m  [84/427], [94mLoss[0m : 2.18028
[1mStep[0m  [126/427], [94mLoss[0m : 2.16956
[1mStep[0m  [168/427], [94mLoss[0m : 1.77306
[1mStep[0m  [210/427], [94mLoss[0m : 1.62198
[1mStep[0m  [252/427], [94mLoss[0m : 1.75473
[1mStep[0m  [294/427], [94mLoss[0m : 1.85447
[1mStep[0m  [336/427], [94mLoss[0m : 1.35449
[1mStep[0m  [378/427], [94mLoss[0m : 1.96593
[1mStep[0m  [420/427], [94mLoss[0m : 1.85162

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.516, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.46985
[1mStep[0m  [42/427], [94mLoss[0m : 1.83804
[1mStep[0m  [84/427], [94mLoss[0m : 2.15251
[1mStep[0m  [126/427], [94mLoss[0m : 1.32097
[1mStep[0m  [168/427], [94mLoss[0m : 1.71755
[1mStep[0m  [210/427], [94mLoss[0m : 1.91783
[1mStep[0m  [252/427], [94mLoss[0m : 1.56500
[1mStep[0m  [294/427], [94mLoss[0m : 2.15569
[1mStep[0m  [336/427], [94mLoss[0m : 1.49236
[1mStep[0m  [378/427], [94mLoss[0m : 1.14013
[1mStep[0m  [420/427], [94mLoss[0m : 1.80890

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.565, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.64300
[1mStep[0m  [42/427], [94mLoss[0m : 1.04433
[1mStep[0m  [84/427], [94mLoss[0m : 1.46806
[1mStep[0m  [126/427], [94mLoss[0m : 1.40634
[1mStep[0m  [168/427], [94mLoss[0m : 1.53667
[1mStep[0m  [210/427], [94mLoss[0m : 1.81132
[1mStep[0m  [252/427], [94mLoss[0m : 1.84430
[1mStep[0m  [294/427], [94mLoss[0m : 1.37324
[1mStep[0m  [336/427], [94mLoss[0m : 1.91167
[1mStep[0m  [378/427], [94mLoss[0m : 1.44125
[1mStep[0m  [420/427], [94mLoss[0m : 1.84390

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.538, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.63549
[1mStep[0m  [42/427], [94mLoss[0m : 1.43379
[1mStep[0m  [84/427], [94mLoss[0m : 1.42254
[1mStep[0m  [126/427], [94mLoss[0m : 1.32367
[1mStep[0m  [168/427], [94mLoss[0m : 1.71932
[1mStep[0m  [210/427], [94mLoss[0m : 1.62447
[1mStep[0m  [252/427], [94mLoss[0m : 1.67396
[1mStep[0m  [294/427], [94mLoss[0m : 1.29464
[1mStep[0m  [336/427], [94mLoss[0m : 1.21291
[1mStep[0m  [378/427], [94mLoss[0m : 1.67688
[1mStep[0m  [420/427], [94mLoss[0m : 1.45345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.35225
[1mStep[0m  [42/427], [94mLoss[0m : 1.74754
[1mStep[0m  [84/427], [94mLoss[0m : 1.39555
[1mStep[0m  [126/427], [94mLoss[0m : 1.14031
[1mStep[0m  [168/427], [94mLoss[0m : 1.66599
[1mStep[0m  [210/427], [94mLoss[0m : 1.94991
[1mStep[0m  [252/427], [94mLoss[0m : 1.32538
[1mStep[0m  [294/427], [94mLoss[0m : 1.99520
[1mStep[0m  [336/427], [94mLoss[0m : 1.12028
[1mStep[0m  [378/427], [94mLoss[0m : 1.66482
[1mStep[0m  [420/427], [94mLoss[0m : 1.57174

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.50237
[1mStep[0m  [42/427], [94mLoss[0m : 1.54855
[1mStep[0m  [84/427], [94mLoss[0m : 1.09938
[1mStep[0m  [126/427], [94mLoss[0m : 1.85980
[1mStep[0m  [168/427], [94mLoss[0m : 1.52389
[1mStep[0m  [210/427], [94mLoss[0m : 1.50071
[1mStep[0m  [252/427], [94mLoss[0m : 1.78524
[1mStep[0m  [294/427], [94mLoss[0m : 1.48482
[1mStep[0m  [336/427], [94mLoss[0m : 1.32004
[1mStep[0m  [378/427], [94mLoss[0m : 1.24952
[1mStep[0m  [420/427], [94mLoss[0m : 1.68662

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.17776
[1mStep[0m  [42/427], [94mLoss[0m : 1.37765
[1mStep[0m  [84/427], [94mLoss[0m : 1.91308
[1mStep[0m  [126/427], [94mLoss[0m : 1.64046
[1mStep[0m  [168/427], [94mLoss[0m : 1.27694
[1mStep[0m  [210/427], [94mLoss[0m : 1.45535
[1mStep[0m  [252/427], [94mLoss[0m : 1.56311
[1mStep[0m  [294/427], [94mLoss[0m : 1.53742
[1mStep[0m  [336/427], [94mLoss[0m : 1.39270
[1mStep[0m  [378/427], [94mLoss[0m : 1.29476
[1mStep[0m  [420/427], [94mLoss[0m : 1.54677

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.475, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.30037
[1mStep[0m  [42/427], [94mLoss[0m : 1.30602
[1mStep[0m  [84/427], [94mLoss[0m : 1.18986
[1mStep[0m  [126/427], [94mLoss[0m : 1.46498
[1mStep[0m  [168/427], [94mLoss[0m : 1.37500
[1mStep[0m  [210/427], [94mLoss[0m : 1.46149
[1mStep[0m  [252/427], [94mLoss[0m : 1.45844
[1mStep[0m  [294/427], [94mLoss[0m : 1.97923
[1mStep[0m  [336/427], [94mLoss[0m : 2.01727
[1mStep[0m  [378/427], [94mLoss[0m : 1.40912
[1mStep[0m  [420/427], [94mLoss[0m : 1.52453

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.573, [92mTest[0m: 2.530, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.01726
[1mStep[0m  [42/427], [94mLoss[0m : 1.63308
[1mStep[0m  [84/427], [94mLoss[0m : 1.60885
[1mStep[0m  [126/427], [94mLoss[0m : 1.72947
[1mStep[0m  [168/427], [94mLoss[0m : 1.71781
[1mStep[0m  [210/427], [94mLoss[0m : 1.12603
[1mStep[0m  [252/427], [94mLoss[0m : 1.38381
[1mStep[0m  [294/427], [94mLoss[0m : 2.11094
[1mStep[0m  [336/427], [94mLoss[0m : 1.26464
[1mStep[0m  [378/427], [94mLoss[0m : 1.53344
[1mStep[0m  [420/427], [94mLoss[0m : 1.82208

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.477, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.90377
[1mStep[0m  [42/427], [94mLoss[0m : 1.66018
[1mStep[0m  [84/427], [94mLoss[0m : 1.63182
[1mStep[0m  [126/427], [94mLoss[0m : 1.43288
[1mStep[0m  [168/427], [94mLoss[0m : 1.43237
[1mStep[0m  [210/427], [94mLoss[0m : 1.45359
[1mStep[0m  [252/427], [94mLoss[0m : 1.28250
[1mStep[0m  [294/427], [94mLoss[0m : 1.98291
[1mStep[0m  [336/427], [94mLoss[0m : 1.41613
[1mStep[0m  [378/427], [94mLoss[0m : 2.01947
[1mStep[0m  [420/427], [94mLoss[0m : 1.41886

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.534, [92mTest[0m: 2.511, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.49100
[1mStep[0m  [42/427], [94mLoss[0m : 1.46210
[1mStep[0m  [84/427], [94mLoss[0m : 1.43514
[1mStep[0m  [126/427], [94mLoss[0m : 1.25817
[1mStep[0m  [168/427], [94mLoss[0m : 1.53472
[1mStep[0m  [210/427], [94mLoss[0m : 1.67887
[1mStep[0m  [252/427], [94mLoss[0m : 1.72756
[1mStep[0m  [294/427], [94mLoss[0m : 1.45926
[1mStep[0m  [336/427], [94mLoss[0m : 1.39738
[1mStep[0m  [378/427], [94mLoss[0m : 1.44460
[1mStep[0m  [420/427], [94mLoss[0m : 1.40733

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.475, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.569
====================================

Phase 2 - Evaluation MAE:  2.5690605914648708
MAE score P1      2.404504
MAE score P2      2.569061
loss              1.515348
learning_rate         0.01
batch_size              32
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.81466
[1mStep[0m  [2/26], [94mLoss[0m : 9.51142
[1mStep[0m  [4/26], [94mLoss[0m : 6.38416
[1mStep[0m  [6/26], [94mLoss[0m : 3.51712
[1mStep[0m  [8/26], [94mLoss[0m : 3.36443
[1mStep[0m  [10/26], [94mLoss[0m : 4.73257
[1mStep[0m  [12/26], [94mLoss[0m : 5.27285
[1mStep[0m  [14/26], [94mLoss[0m : 4.58342
[1mStep[0m  [16/26], [94mLoss[0m : 3.22300
[1mStep[0m  [18/26], [94mLoss[0m : 2.58292
[1mStep[0m  [20/26], [94mLoss[0m : 2.89422
[1mStep[0m  [22/26], [94mLoss[0m : 3.44159
[1mStep[0m  [24/26], [94mLoss[0m : 3.09957

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.722, [92mTest[0m: 10.847, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49900
[1mStep[0m  [2/26], [94mLoss[0m : 2.71349
[1mStep[0m  [4/26], [94mLoss[0m : 2.81064
[1mStep[0m  [6/26], [94mLoss[0m : 2.68787
[1mStep[0m  [8/26], [94mLoss[0m : 2.54838
[1mStep[0m  [10/26], [94mLoss[0m : 2.56671
[1mStep[0m  [12/26], [94mLoss[0m : 2.64104
[1mStep[0m  [14/26], [94mLoss[0m : 2.51898
[1mStep[0m  [16/26], [94mLoss[0m : 2.51017
[1mStep[0m  [18/26], [94mLoss[0m : 2.40258
[1mStep[0m  [20/26], [94mLoss[0m : 2.52606
[1mStep[0m  [22/26], [94mLoss[0m : 2.48436
[1mStep[0m  [24/26], [94mLoss[0m : 2.56603

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45451
[1mStep[0m  [2/26], [94mLoss[0m : 2.48642
[1mStep[0m  [4/26], [94mLoss[0m : 2.55978
[1mStep[0m  [6/26], [94mLoss[0m : 2.34809
[1mStep[0m  [8/26], [94mLoss[0m : 2.42465
[1mStep[0m  [10/26], [94mLoss[0m : 2.51586
[1mStep[0m  [12/26], [94mLoss[0m : 2.55599
[1mStep[0m  [14/26], [94mLoss[0m : 2.50005
[1mStep[0m  [16/26], [94mLoss[0m : 2.38185
[1mStep[0m  [18/26], [94mLoss[0m : 2.40275
[1mStep[0m  [20/26], [94mLoss[0m : 2.41250
[1mStep[0m  [22/26], [94mLoss[0m : 2.32145
[1mStep[0m  [24/26], [94mLoss[0m : 2.40352

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43888
[1mStep[0m  [2/26], [94mLoss[0m : 2.50989
[1mStep[0m  [4/26], [94mLoss[0m : 2.37879
[1mStep[0m  [6/26], [94mLoss[0m : 2.50054
[1mStep[0m  [8/26], [94mLoss[0m : 2.53779
[1mStep[0m  [10/26], [94mLoss[0m : 2.37492
[1mStep[0m  [12/26], [94mLoss[0m : 2.44015
[1mStep[0m  [14/26], [94mLoss[0m : 2.34907
[1mStep[0m  [16/26], [94mLoss[0m : 2.49563
[1mStep[0m  [18/26], [94mLoss[0m : 2.34881
[1mStep[0m  [20/26], [94mLoss[0m : 2.42833
[1mStep[0m  [22/26], [94mLoss[0m : 2.36479
[1mStep[0m  [24/26], [94mLoss[0m : 2.66759

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50299
[1mStep[0m  [2/26], [94mLoss[0m : 2.30024
[1mStep[0m  [4/26], [94mLoss[0m : 2.43064
[1mStep[0m  [6/26], [94mLoss[0m : 2.36016
[1mStep[0m  [8/26], [94mLoss[0m : 2.34199
[1mStep[0m  [10/26], [94mLoss[0m : 2.48773
[1mStep[0m  [12/26], [94mLoss[0m : 2.48964
[1mStep[0m  [14/26], [94mLoss[0m : 2.39953
[1mStep[0m  [16/26], [94mLoss[0m : 2.37323
[1mStep[0m  [18/26], [94mLoss[0m : 2.44792
[1mStep[0m  [20/26], [94mLoss[0m : 2.42564
[1mStep[0m  [22/26], [94mLoss[0m : 2.43756
[1mStep[0m  [24/26], [94mLoss[0m : 2.46598

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50438
[1mStep[0m  [2/26], [94mLoss[0m : 2.57327
[1mStep[0m  [4/26], [94mLoss[0m : 2.43187
[1mStep[0m  [6/26], [94mLoss[0m : 2.29227
[1mStep[0m  [8/26], [94mLoss[0m : 2.45604
[1mStep[0m  [10/26], [94mLoss[0m : 2.44264
[1mStep[0m  [12/26], [94mLoss[0m : 2.22993
[1mStep[0m  [14/26], [94mLoss[0m : 2.36645
[1mStep[0m  [16/26], [94mLoss[0m : 2.53474
[1mStep[0m  [18/26], [94mLoss[0m : 2.45315
[1mStep[0m  [20/26], [94mLoss[0m : 2.36859
[1mStep[0m  [22/26], [94mLoss[0m : 2.49324
[1mStep[0m  [24/26], [94mLoss[0m : 2.37700

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55993
[1mStep[0m  [2/26], [94mLoss[0m : 2.37712
[1mStep[0m  [4/26], [94mLoss[0m : 2.40428
[1mStep[0m  [6/26], [94mLoss[0m : 2.39658
[1mStep[0m  [8/26], [94mLoss[0m : 2.38190
[1mStep[0m  [10/26], [94mLoss[0m : 2.39561
[1mStep[0m  [12/26], [94mLoss[0m : 2.53016
[1mStep[0m  [14/26], [94mLoss[0m : 2.53625
[1mStep[0m  [16/26], [94mLoss[0m : 2.43596
[1mStep[0m  [18/26], [94mLoss[0m : 2.50347
[1mStep[0m  [20/26], [94mLoss[0m : 2.47028
[1mStep[0m  [22/26], [94mLoss[0m : 2.43908
[1mStep[0m  [24/26], [94mLoss[0m : 2.46913

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38535
[1mStep[0m  [2/26], [94mLoss[0m : 2.43764
[1mStep[0m  [4/26], [94mLoss[0m : 2.33666
[1mStep[0m  [6/26], [94mLoss[0m : 2.22780
[1mStep[0m  [8/26], [94mLoss[0m : 2.69886
[1mStep[0m  [10/26], [94mLoss[0m : 2.56442
[1mStep[0m  [12/26], [94mLoss[0m : 2.41608
[1mStep[0m  [14/26], [94mLoss[0m : 2.43265
[1mStep[0m  [16/26], [94mLoss[0m : 2.48002
[1mStep[0m  [18/26], [94mLoss[0m : 2.43724
[1mStep[0m  [20/26], [94mLoss[0m : 2.59371
[1mStep[0m  [22/26], [94mLoss[0m : 2.33198
[1mStep[0m  [24/26], [94mLoss[0m : 2.58707

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59930
[1mStep[0m  [2/26], [94mLoss[0m : 2.45339
[1mStep[0m  [4/26], [94mLoss[0m : 2.43515
[1mStep[0m  [6/26], [94mLoss[0m : 2.55493
[1mStep[0m  [8/26], [94mLoss[0m : 2.44207
[1mStep[0m  [10/26], [94mLoss[0m : 2.36418
[1mStep[0m  [12/26], [94mLoss[0m : 2.38179
[1mStep[0m  [14/26], [94mLoss[0m : 2.41288
[1mStep[0m  [16/26], [94mLoss[0m : 2.43507
[1mStep[0m  [18/26], [94mLoss[0m : 2.30617
[1mStep[0m  [20/26], [94mLoss[0m : 2.37447
[1mStep[0m  [22/26], [94mLoss[0m : 2.47169
[1mStep[0m  [24/26], [94mLoss[0m : 2.36525

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34141
[1mStep[0m  [2/26], [94mLoss[0m : 2.30883
[1mStep[0m  [4/26], [94mLoss[0m : 2.40225
[1mStep[0m  [6/26], [94mLoss[0m : 2.27341
[1mStep[0m  [8/26], [94mLoss[0m : 2.53206
[1mStep[0m  [10/26], [94mLoss[0m : 2.43565
[1mStep[0m  [12/26], [94mLoss[0m : 2.43722
[1mStep[0m  [14/26], [94mLoss[0m : 2.45155
[1mStep[0m  [16/26], [94mLoss[0m : 2.50052
[1mStep[0m  [18/26], [94mLoss[0m : 2.53469
[1mStep[0m  [20/26], [94mLoss[0m : 2.32448
[1mStep[0m  [22/26], [94mLoss[0m : 2.38409
[1mStep[0m  [24/26], [94mLoss[0m : 2.46684

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38240
[1mStep[0m  [2/26], [94mLoss[0m : 2.38791
[1mStep[0m  [4/26], [94mLoss[0m : 2.51411
[1mStep[0m  [6/26], [94mLoss[0m : 2.40096
[1mStep[0m  [8/26], [94mLoss[0m : 2.53855
[1mStep[0m  [10/26], [94mLoss[0m : 2.41126
[1mStep[0m  [12/26], [94mLoss[0m : 2.55387
[1mStep[0m  [14/26], [94mLoss[0m : 2.51006
[1mStep[0m  [16/26], [94mLoss[0m : 2.36741
[1mStep[0m  [18/26], [94mLoss[0m : 2.27751
[1mStep[0m  [20/26], [94mLoss[0m : 2.29643
[1mStep[0m  [22/26], [94mLoss[0m : 2.39170
[1mStep[0m  [24/26], [94mLoss[0m : 2.43802

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46999
[1mStep[0m  [2/26], [94mLoss[0m : 2.45715
[1mStep[0m  [4/26], [94mLoss[0m : 2.27202
[1mStep[0m  [6/26], [94mLoss[0m : 2.49092
[1mStep[0m  [8/26], [94mLoss[0m : 2.57905
[1mStep[0m  [10/26], [94mLoss[0m : 2.32903
[1mStep[0m  [12/26], [94mLoss[0m : 2.42072
[1mStep[0m  [14/26], [94mLoss[0m : 2.41249
[1mStep[0m  [16/26], [94mLoss[0m : 2.39425
[1mStep[0m  [18/26], [94mLoss[0m : 2.48673
[1mStep[0m  [20/26], [94mLoss[0m : 2.36736
[1mStep[0m  [22/26], [94mLoss[0m : 2.37490
[1mStep[0m  [24/26], [94mLoss[0m : 2.34363

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31960
[1mStep[0m  [2/26], [94mLoss[0m : 2.34184
[1mStep[0m  [4/26], [94mLoss[0m : 2.27318
[1mStep[0m  [6/26], [94mLoss[0m : 2.50593
[1mStep[0m  [8/26], [94mLoss[0m : 2.46461
[1mStep[0m  [10/26], [94mLoss[0m : 2.33686
[1mStep[0m  [12/26], [94mLoss[0m : 2.38688
[1mStep[0m  [14/26], [94mLoss[0m : 2.48086
[1mStep[0m  [16/26], [94mLoss[0m : 2.55042
[1mStep[0m  [18/26], [94mLoss[0m : 2.45628
[1mStep[0m  [20/26], [94mLoss[0m : 2.51227
[1mStep[0m  [22/26], [94mLoss[0m : 2.46733
[1mStep[0m  [24/26], [94mLoss[0m : 2.38183

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29455
[1mStep[0m  [2/26], [94mLoss[0m : 2.45973
[1mStep[0m  [4/26], [94mLoss[0m : 2.52355
[1mStep[0m  [6/26], [94mLoss[0m : 2.25257
[1mStep[0m  [8/26], [94mLoss[0m : 2.40140
[1mStep[0m  [10/26], [94mLoss[0m : 2.39930
[1mStep[0m  [12/26], [94mLoss[0m : 2.49712
[1mStep[0m  [14/26], [94mLoss[0m : 2.45469
[1mStep[0m  [16/26], [94mLoss[0m : 2.45840
[1mStep[0m  [18/26], [94mLoss[0m : 2.47643
[1mStep[0m  [20/26], [94mLoss[0m : 2.37358
[1mStep[0m  [22/26], [94mLoss[0m : 2.51150
[1mStep[0m  [24/26], [94mLoss[0m : 2.29423

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28191
[1mStep[0m  [2/26], [94mLoss[0m : 2.41386
[1mStep[0m  [4/26], [94mLoss[0m : 2.65074
[1mStep[0m  [6/26], [94mLoss[0m : 2.40414
[1mStep[0m  [8/26], [94mLoss[0m : 2.51719
[1mStep[0m  [10/26], [94mLoss[0m : 2.37416
[1mStep[0m  [12/26], [94mLoss[0m : 2.46038
[1mStep[0m  [14/26], [94mLoss[0m : 2.32032
[1mStep[0m  [16/26], [94mLoss[0m : 2.27651
[1mStep[0m  [18/26], [94mLoss[0m : 2.43072
[1mStep[0m  [20/26], [94mLoss[0m : 2.46312
[1mStep[0m  [22/26], [94mLoss[0m : 2.41240
[1mStep[0m  [24/26], [94mLoss[0m : 2.35264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40989
[1mStep[0m  [2/26], [94mLoss[0m : 2.38217
[1mStep[0m  [4/26], [94mLoss[0m : 2.45439
[1mStep[0m  [6/26], [94mLoss[0m : 2.35983
[1mStep[0m  [8/26], [94mLoss[0m : 2.43199
[1mStep[0m  [10/26], [94mLoss[0m : 2.31724
[1mStep[0m  [12/26], [94mLoss[0m : 2.41256
[1mStep[0m  [14/26], [94mLoss[0m : 2.37111
[1mStep[0m  [16/26], [94mLoss[0m : 2.42252
[1mStep[0m  [18/26], [94mLoss[0m : 2.32797
[1mStep[0m  [20/26], [94mLoss[0m : 2.33521
[1mStep[0m  [22/26], [94mLoss[0m : 2.42204
[1mStep[0m  [24/26], [94mLoss[0m : 2.45793

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29285
[1mStep[0m  [2/26], [94mLoss[0m : 2.32784
[1mStep[0m  [4/26], [94mLoss[0m : 2.36349
[1mStep[0m  [6/26], [94mLoss[0m : 2.39090
[1mStep[0m  [8/26], [94mLoss[0m : 2.38781
[1mStep[0m  [10/26], [94mLoss[0m : 2.31816
[1mStep[0m  [12/26], [94mLoss[0m : 2.45871
[1mStep[0m  [14/26], [94mLoss[0m : 2.46244
[1mStep[0m  [16/26], [94mLoss[0m : 2.43667
[1mStep[0m  [18/26], [94mLoss[0m : 2.30216
[1mStep[0m  [20/26], [94mLoss[0m : 2.47674
[1mStep[0m  [22/26], [94mLoss[0m : 2.53068
[1mStep[0m  [24/26], [94mLoss[0m : 2.42528

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34096
[1mStep[0m  [2/26], [94mLoss[0m : 2.22336
[1mStep[0m  [4/26], [94mLoss[0m : 2.38362
[1mStep[0m  [6/26], [94mLoss[0m : 2.44326
[1mStep[0m  [8/26], [94mLoss[0m : 2.43522
[1mStep[0m  [10/26], [94mLoss[0m : 2.55295
[1mStep[0m  [12/26], [94mLoss[0m : 2.40551
[1mStep[0m  [14/26], [94mLoss[0m : 2.63469
[1mStep[0m  [16/26], [94mLoss[0m : 2.30855
[1mStep[0m  [18/26], [94mLoss[0m : 2.49451
[1mStep[0m  [20/26], [94mLoss[0m : 2.48776
[1mStep[0m  [22/26], [94mLoss[0m : 2.32843
[1mStep[0m  [24/26], [94mLoss[0m : 2.49475

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43269
[1mStep[0m  [2/26], [94mLoss[0m : 2.31560
[1mStep[0m  [4/26], [94mLoss[0m : 2.45543
[1mStep[0m  [6/26], [94mLoss[0m : 2.43532
[1mStep[0m  [8/26], [94mLoss[0m : 2.38147
[1mStep[0m  [10/26], [94mLoss[0m : 2.56017
[1mStep[0m  [12/26], [94mLoss[0m : 2.33962
[1mStep[0m  [14/26], [94mLoss[0m : 2.31629
[1mStep[0m  [16/26], [94mLoss[0m : 2.40390
[1mStep[0m  [18/26], [94mLoss[0m : 2.40194
[1mStep[0m  [20/26], [94mLoss[0m : 2.31282
[1mStep[0m  [22/26], [94mLoss[0m : 2.28014
[1mStep[0m  [24/26], [94mLoss[0m : 2.48589

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43274
[1mStep[0m  [2/26], [94mLoss[0m : 2.49338
[1mStep[0m  [4/26], [94mLoss[0m : 2.41981
[1mStep[0m  [6/26], [94mLoss[0m : 2.50723
[1mStep[0m  [8/26], [94mLoss[0m : 2.34430
[1mStep[0m  [10/26], [94mLoss[0m : 2.22507
[1mStep[0m  [12/26], [94mLoss[0m : 2.25594
[1mStep[0m  [14/26], [94mLoss[0m : 2.43423
[1mStep[0m  [16/26], [94mLoss[0m : 2.60109
[1mStep[0m  [18/26], [94mLoss[0m : 2.49507
[1mStep[0m  [20/26], [94mLoss[0m : 2.38439
[1mStep[0m  [22/26], [94mLoss[0m : 2.37692
[1mStep[0m  [24/26], [94mLoss[0m : 2.40466

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.403, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30025
[1mStep[0m  [2/26], [94mLoss[0m : 2.25657
[1mStep[0m  [4/26], [94mLoss[0m : 2.35329
[1mStep[0m  [6/26], [94mLoss[0m : 2.37480
[1mStep[0m  [8/26], [94mLoss[0m : 2.48898
[1mStep[0m  [10/26], [94mLoss[0m : 2.40334
[1mStep[0m  [12/26], [94mLoss[0m : 2.42287
[1mStep[0m  [14/26], [94mLoss[0m : 2.32817
[1mStep[0m  [16/26], [94mLoss[0m : 2.32769
[1mStep[0m  [18/26], [94mLoss[0m : 2.30971
[1mStep[0m  [20/26], [94mLoss[0m : 2.47304
[1mStep[0m  [22/26], [94mLoss[0m : 2.32843
[1mStep[0m  [24/26], [94mLoss[0m : 2.30216

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.403, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44837
[1mStep[0m  [2/26], [94mLoss[0m : 2.41496
[1mStep[0m  [4/26], [94mLoss[0m : 2.31216
[1mStep[0m  [6/26], [94mLoss[0m : 2.38219
[1mStep[0m  [8/26], [94mLoss[0m : 2.34664
[1mStep[0m  [10/26], [94mLoss[0m : 2.51057
[1mStep[0m  [12/26], [94mLoss[0m : 2.38846
[1mStep[0m  [14/26], [94mLoss[0m : 2.49271
[1mStep[0m  [16/26], [94mLoss[0m : 2.27291
[1mStep[0m  [18/26], [94mLoss[0m : 2.32847
[1mStep[0m  [20/26], [94mLoss[0m : 2.31274
[1mStep[0m  [22/26], [94mLoss[0m : 2.41342
[1mStep[0m  [24/26], [94mLoss[0m : 2.35593

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.418, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40229
[1mStep[0m  [2/26], [94mLoss[0m : 2.25265
[1mStep[0m  [4/26], [94mLoss[0m : 2.36274
[1mStep[0m  [6/26], [94mLoss[0m : 2.55189
[1mStep[0m  [8/26], [94mLoss[0m : 2.46312
[1mStep[0m  [10/26], [94mLoss[0m : 2.42366
[1mStep[0m  [12/26], [94mLoss[0m : 2.37022
[1mStep[0m  [14/26], [94mLoss[0m : 2.31929
[1mStep[0m  [16/26], [94mLoss[0m : 2.33759
[1mStep[0m  [18/26], [94mLoss[0m : 2.32599
[1mStep[0m  [20/26], [94mLoss[0m : 2.38576
[1mStep[0m  [22/26], [94mLoss[0m : 2.41458
[1mStep[0m  [24/26], [94mLoss[0m : 2.44491

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.395, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40125
[1mStep[0m  [2/26], [94mLoss[0m : 2.35997
[1mStep[0m  [4/26], [94mLoss[0m : 2.46656
[1mStep[0m  [6/26], [94mLoss[0m : 2.47355
[1mStep[0m  [8/26], [94mLoss[0m : 2.34689
[1mStep[0m  [10/26], [94mLoss[0m : 2.33760
[1mStep[0m  [12/26], [94mLoss[0m : 2.32632
[1mStep[0m  [14/26], [94mLoss[0m : 2.31376
[1mStep[0m  [16/26], [94mLoss[0m : 2.31721
[1mStep[0m  [18/26], [94mLoss[0m : 2.32019
[1mStep[0m  [20/26], [94mLoss[0m : 2.38022
[1mStep[0m  [22/26], [94mLoss[0m : 2.34799
[1mStep[0m  [24/26], [94mLoss[0m : 2.57085

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.417, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48073
[1mStep[0m  [2/26], [94mLoss[0m : 2.36901
[1mStep[0m  [4/26], [94mLoss[0m : 2.33377
[1mStep[0m  [6/26], [94mLoss[0m : 2.49212
[1mStep[0m  [8/26], [94mLoss[0m : 2.30545
[1mStep[0m  [10/26], [94mLoss[0m : 2.37239
[1mStep[0m  [12/26], [94mLoss[0m : 2.46836
[1mStep[0m  [14/26], [94mLoss[0m : 2.36567
[1mStep[0m  [16/26], [94mLoss[0m : 2.43840
[1mStep[0m  [18/26], [94mLoss[0m : 2.31411
[1mStep[0m  [20/26], [94mLoss[0m : 2.40750
[1mStep[0m  [22/26], [94mLoss[0m : 2.46940
[1mStep[0m  [24/26], [94mLoss[0m : 2.34387

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.395, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49317
[1mStep[0m  [2/26], [94mLoss[0m : 2.53200
[1mStep[0m  [4/26], [94mLoss[0m : 2.42827
[1mStep[0m  [6/26], [94mLoss[0m : 2.54350
[1mStep[0m  [8/26], [94mLoss[0m : 2.29678
[1mStep[0m  [10/26], [94mLoss[0m : 2.34094
[1mStep[0m  [12/26], [94mLoss[0m : 2.34602
[1mStep[0m  [14/26], [94mLoss[0m : 2.38385
[1mStep[0m  [16/26], [94mLoss[0m : 2.41694
[1mStep[0m  [18/26], [94mLoss[0m : 2.40997
[1mStep[0m  [20/26], [94mLoss[0m : 2.45741
[1mStep[0m  [22/26], [94mLoss[0m : 2.30443
[1mStep[0m  [24/26], [94mLoss[0m : 2.31236

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.408, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31721
[1mStep[0m  [2/26], [94mLoss[0m : 2.29504
[1mStep[0m  [4/26], [94mLoss[0m : 2.32571
[1mStep[0m  [6/26], [94mLoss[0m : 2.33387
[1mStep[0m  [8/26], [94mLoss[0m : 2.49204
[1mStep[0m  [10/26], [94mLoss[0m : 2.30724
[1mStep[0m  [12/26], [94mLoss[0m : 2.39503
[1mStep[0m  [14/26], [94mLoss[0m : 2.45307
[1mStep[0m  [16/26], [94mLoss[0m : 2.32269
[1mStep[0m  [18/26], [94mLoss[0m : 2.46400
[1mStep[0m  [20/26], [94mLoss[0m : 2.33844
[1mStep[0m  [22/26], [94mLoss[0m : 2.43841
[1mStep[0m  [24/26], [94mLoss[0m : 2.35847

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39717
[1mStep[0m  [2/26], [94mLoss[0m : 2.40102
[1mStep[0m  [4/26], [94mLoss[0m : 2.42082
[1mStep[0m  [6/26], [94mLoss[0m : 2.45483
[1mStep[0m  [8/26], [94mLoss[0m : 2.55542
[1mStep[0m  [10/26], [94mLoss[0m : 2.37542
[1mStep[0m  [12/26], [94mLoss[0m : 2.35162
[1mStep[0m  [14/26], [94mLoss[0m : 2.29965
[1mStep[0m  [16/26], [94mLoss[0m : 2.37332
[1mStep[0m  [18/26], [94mLoss[0m : 2.50039
[1mStep[0m  [20/26], [94mLoss[0m : 2.55302
[1mStep[0m  [22/26], [94mLoss[0m : 2.19029
[1mStep[0m  [24/26], [94mLoss[0m : 2.38802

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.424, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38352
[1mStep[0m  [2/26], [94mLoss[0m : 2.33398
[1mStep[0m  [4/26], [94mLoss[0m : 2.27857
[1mStep[0m  [6/26], [94mLoss[0m : 2.35093
[1mStep[0m  [8/26], [94mLoss[0m : 2.30546
[1mStep[0m  [10/26], [94mLoss[0m : 2.45115
[1mStep[0m  [12/26], [94mLoss[0m : 2.36223
[1mStep[0m  [14/26], [94mLoss[0m : 2.42609
[1mStep[0m  [16/26], [94mLoss[0m : 2.58950
[1mStep[0m  [18/26], [94mLoss[0m : 2.32048
[1mStep[0m  [20/26], [94mLoss[0m : 2.25411
[1mStep[0m  [22/26], [94mLoss[0m : 2.34644
[1mStep[0m  [24/26], [94mLoss[0m : 2.39711

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.394, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26089
[1mStep[0m  [2/26], [94mLoss[0m : 2.41149
[1mStep[0m  [4/26], [94mLoss[0m : 2.40132
[1mStep[0m  [6/26], [94mLoss[0m : 2.50057
[1mStep[0m  [8/26], [94mLoss[0m : 2.35886
[1mStep[0m  [10/26], [94mLoss[0m : 2.27695
[1mStep[0m  [12/26], [94mLoss[0m : 2.50270
[1mStep[0m  [14/26], [94mLoss[0m : 2.35888
[1mStep[0m  [16/26], [94mLoss[0m : 2.36754
[1mStep[0m  [18/26], [94mLoss[0m : 2.29469
[1mStep[0m  [20/26], [94mLoss[0m : 2.40639
[1mStep[0m  [22/26], [94mLoss[0m : 2.47803
[1mStep[0m  [24/26], [94mLoss[0m : 2.40695

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.398, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.401
====================================

Phase 1 - Evaluation MAE:  2.4010718052203837
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.31287
[1mStep[0m  [2/26], [94mLoss[0m : 2.52871
[1mStep[0m  [4/26], [94mLoss[0m : 2.39922
[1mStep[0m  [6/26], [94mLoss[0m : 2.40424
[1mStep[0m  [8/26], [94mLoss[0m : 2.35227
[1mStep[0m  [10/26], [94mLoss[0m : 2.46970
[1mStep[0m  [12/26], [94mLoss[0m : 2.54823
[1mStep[0m  [14/26], [94mLoss[0m : 2.35521
[1mStep[0m  [16/26], [94mLoss[0m : 2.42471
[1mStep[0m  [18/26], [94mLoss[0m : 2.42735
[1mStep[0m  [20/26], [94mLoss[0m : 2.45425
[1mStep[0m  [22/26], [94mLoss[0m : 2.39895
[1mStep[0m  [24/26], [94mLoss[0m : 2.47287

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29776
[1mStep[0m  [2/26], [94mLoss[0m : 2.42092
[1mStep[0m  [4/26], [94mLoss[0m : 2.49192
[1mStep[0m  [6/26], [94mLoss[0m : 2.18170
[1mStep[0m  [8/26], [94mLoss[0m : 2.32177
[1mStep[0m  [10/26], [94mLoss[0m : 2.21254
[1mStep[0m  [12/26], [94mLoss[0m : 2.59660
[1mStep[0m  [14/26], [94mLoss[0m : 2.38461
[1mStep[0m  [16/26], [94mLoss[0m : 2.36517
[1mStep[0m  [18/26], [94mLoss[0m : 2.12216
[1mStep[0m  [20/26], [94mLoss[0m : 2.43061
[1mStep[0m  [22/26], [94mLoss[0m : 2.37491
[1mStep[0m  [24/26], [94mLoss[0m : 2.33617

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25087
[1mStep[0m  [2/26], [94mLoss[0m : 2.13411
[1mStep[0m  [4/26], [94mLoss[0m : 2.27889
[1mStep[0m  [6/26], [94mLoss[0m : 2.22198
[1mStep[0m  [8/26], [94mLoss[0m : 2.29606
[1mStep[0m  [10/26], [94mLoss[0m : 2.35257
[1mStep[0m  [12/26], [94mLoss[0m : 2.11612
[1mStep[0m  [14/26], [94mLoss[0m : 2.15176
[1mStep[0m  [16/26], [94mLoss[0m : 2.26899
[1mStep[0m  [18/26], [94mLoss[0m : 2.21578
[1mStep[0m  [20/26], [94mLoss[0m : 2.21770
[1mStep[0m  [22/26], [94mLoss[0m : 2.29272
[1mStep[0m  [24/26], [94mLoss[0m : 2.23582

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17355
[1mStep[0m  [2/26], [94mLoss[0m : 2.06374
[1mStep[0m  [4/26], [94mLoss[0m : 2.22415
[1mStep[0m  [6/26], [94mLoss[0m : 2.19113
[1mStep[0m  [8/26], [94mLoss[0m : 2.18581
[1mStep[0m  [10/26], [94mLoss[0m : 2.20015
[1mStep[0m  [12/26], [94mLoss[0m : 2.27008
[1mStep[0m  [14/26], [94mLoss[0m : 2.18010
[1mStep[0m  [16/26], [94mLoss[0m : 2.19622
[1mStep[0m  [18/26], [94mLoss[0m : 2.10400
[1mStep[0m  [20/26], [94mLoss[0m : 2.22242
[1mStep[0m  [22/26], [94mLoss[0m : 2.29814
[1mStep[0m  [24/26], [94mLoss[0m : 2.12595

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.758, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.99952
[1mStep[0m  [2/26], [94mLoss[0m : 2.07446
[1mStep[0m  [4/26], [94mLoss[0m : 2.16320
[1mStep[0m  [6/26], [94mLoss[0m : 2.13330
[1mStep[0m  [8/26], [94mLoss[0m : 2.07090
[1mStep[0m  [10/26], [94mLoss[0m : 2.13582
[1mStep[0m  [12/26], [94mLoss[0m : 2.12205
[1mStep[0m  [14/26], [94mLoss[0m : 1.94721
[1mStep[0m  [16/26], [94mLoss[0m : 2.01701
[1mStep[0m  [18/26], [94mLoss[0m : 2.04034
[1mStep[0m  [20/26], [94mLoss[0m : 2.18055
[1mStep[0m  [22/26], [94mLoss[0m : 2.12807
[1mStep[0m  [24/26], [94mLoss[0m : 2.11297

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88844
[1mStep[0m  [2/26], [94mLoss[0m : 1.90253
[1mStep[0m  [4/26], [94mLoss[0m : 2.05118
[1mStep[0m  [6/26], [94mLoss[0m : 2.13598
[1mStep[0m  [8/26], [94mLoss[0m : 1.98563
[1mStep[0m  [10/26], [94mLoss[0m : 2.06031
[1mStep[0m  [12/26], [94mLoss[0m : 2.02535
[1mStep[0m  [14/26], [94mLoss[0m : 1.81051
[1mStep[0m  [16/26], [94mLoss[0m : 1.97698
[1mStep[0m  [18/26], [94mLoss[0m : 2.02919
[1mStep[0m  [20/26], [94mLoss[0m : 2.03465
[1mStep[0m  [22/26], [94mLoss[0m : 2.16321
[1mStep[0m  [24/26], [94mLoss[0m : 1.99419

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07802
[1mStep[0m  [2/26], [94mLoss[0m : 1.98685
[1mStep[0m  [4/26], [94mLoss[0m : 1.97706
[1mStep[0m  [6/26], [94mLoss[0m : 1.83742
[1mStep[0m  [8/26], [94mLoss[0m : 2.00069
[1mStep[0m  [10/26], [94mLoss[0m : 2.00382
[1mStep[0m  [12/26], [94mLoss[0m : 1.93631
[1mStep[0m  [14/26], [94mLoss[0m : 1.87440
[1mStep[0m  [16/26], [94mLoss[0m : 2.15373
[1mStep[0m  [18/26], [94mLoss[0m : 1.88873
[1mStep[0m  [20/26], [94mLoss[0m : 1.94960
[1mStep[0m  [22/26], [94mLoss[0m : 1.94901
[1mStep[0m  [24/26], [94mLoss[0m : 1.84553

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.74951
[1mStep[0m  [2/26], [94mLoss[0m : 1.79469
[1mStep[0m  [4/26], [94mLoss[0m : 1.91955
[1mStep[0m  [6/26], [94mLoss[0m : 1.93397
[1mStep[0m  [8/26], [94mLoss[0m : 1.95200
[1mStep[0m  [10/26], [94mLoss[0m : 1.94266
[1mStep[0m  [12/26], [94mLoss[0m : 2.01875
[1mStep[0m  [14/26], [94mLoss[0m : 1.74265
[1mStep[0m  [16/26], [94mLoss[0m : 1.80770
[1mStep[0m  [18/26], [94mLoss[0m : 1.94974
[1mStep[0m  [20/26], [94mLoss[0m : 1.85626
[1mStep[0m  [22/26], [94mLoss[0m : 1.80595
[1mStep[0m  [24/26], [94mLoss[0m : 1.92623

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88222
[1mStep[0m  [2/26], [94mLoss[0m : 1.85337
[1mStep[0m  [4/26], [94mLoss[0m : 1.92842
[1mStep[0m  [6/26], [94mLoss[0m : 1.84606
[1mStep[0m  [8/26], [94mLoss[0m : 1.79538
[1mStep[0m  [10/26], [94mLoss[0m : 1.78758
[1mStep[0m  [12/26], [94mLoss[0m : 1.89548
[1mStep[0m  [14/26], [94mLoss[0m : 1.72542
[1mStep[0m  [16/26], [94mLoss[0m : 1.81141
[1mStep[0m  [18/26], [94mLoss[0m : 1.94247
[1mStep[0m  [20/26], [94mLoss[0m : 1.65395
[1mStep[0m  [22/26], [94mLoss[0m : 1.88320
[1mStep[0m  [24/26], [94mLoss[0m : 2.01299

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.79911
[1mStep[0m  [2/26], [94mLoss[0m : 1.77663
[1mStep[0m  [4/26], [94mLoss[0m : 1.67099
[1mStep[0m  [6/26], [94mLoss[0m : 1.73915
[1mStep[0m  [8/26], [94mLoss[0m : 1.80331
[1mStep[0m  [10/26], [94mLoss[0m : 1.61283
[1mStep[0m  [12/26], [94mLoss[0m : 1.78336
[1mStep[0m  [14/26], [94mLoss[0m : 1.73099
[1mStep[0m  [16/26], [94mLoss[0m : 1.77457
[1mStep[0m  [18/26], [94mLoss[0m : 1.83807
[1mStep[0m  [20/26], [94mLoss[0m : 1.86719
[1mStep[0m  [22/26], [94mLoss[0m : 1.82373
[1mStep[0m  [24/26], [94mLoss[0m : 1.67645

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.77958
[1mStep[0m  [2/26], [94mLoss[0m : 1.63235
[1mStep[0m  [4/26], [94mLoss[0m : 1.66566
[1mStep[0m  [6/26], [94mLoss[0m : 1.76151
[1mStep[0m  [8/26], [94mLoss[0m : 1.68523
[1mStep[0m  [10/26], [94mLoss[0m : 1.71765
[1mStep[0m  [12/26], [94mLoss[0m : 1.85434
[1mStep[0m  [14/26], [94mLoss[0m : 1.70492
[1mStep[0m  [16/26], [94mLoss[0m : 1.63037
[1mStep[0m  [18/26], [94mLoss[0m : 1.73974
[1mStep[0m  [20/26], [94mLoss[0m : 1.77883
[1mStep[0m  [22/26], [94mLoss[0m : 1.70579
[1mStep[0m  [24/26], [94mLoss[0m : 1.80739

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.58035
[1mStep[0m  [2/26], [94mLoss[0m : 1.62621
[1mStep[0m  [4/26], [94mLoss[0m : 1.71158
[1mStep[0m  [6/26], [94mLoss[0m : 1.69461
[1mStep[0m  [8/26], [94mLoss[0m : 1.55667
[1mStep[0m  [10/26], [94mLoss[0m : 1.70323
[1mStep[0m  [12/26], [94mLoss[0m : 1.74229
[1mStep[0m  [14/26], [94mLoss[0m : 1.66410
[1mStep[0m  [16/26], [94mLoss[0m : 1.61702
[1mStep[0m  [18/26], [94mLoss[0m : 1.73159
[1mStep[0m  [20/26], [94mLoss[0m : 1.65626
[1mStep[0m  [22/26], [94mLoss[0m : 1.68055
[1mStep[0m  [24/26], [94mLoss[0m : 1.80908

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.690, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.62713
[1mStep[0m  [2/26], [94mLoss[0m : 1.60844
[1mStep[0m  [4/26], [94mLoss[0m : 1.51852
[1mStep[0m  [6/26], [94mLoss[0m : 1.52119
[1mStep[0m  [8/26], [94mLoss[0m : 1.75084
[1mStep[0m  [10/26], [94mLoss[0m : 1.57036
[1mStep[0m  [12/26], [94mLoss[0m : 1.79572
[1mStep[0m  [14/26], [94mLoss[0m : 1.72751
[1mStep[0m  [16/26], [94mLoss[0m : 1.62502
[1mStep[0m  [18/26], [94mLoss[0m : 1.70849
[1mStep[0m  [20/26], [94mLoss[0m : 1.64336
[1mStep[0m  [22/26], [94mLoss[0m : 1.67264
[1mStep[0m  [24/26], [94mLoss[0m : 1.73918

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.651, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.60291
[1mStep[0m  [2/26], [94mLoss[0m : 1.68605
[1mStep[0m  [4/26], [94mLoss[0m : 1.61929
[1mStep[0m  [6/26], [94mLoss[0m : 1.53540
[1mStep[0m  [8/26], [94mLoss[0m : 1.50635
[1mStep[0m  [10/26], [94mLoss[0m : 1.69234
[1mStep[0m  [12/26], [94mLoss[0m : 1.62745
[1mStep[0m  [14/26], [94mLoss[0m : 1.51321
[1mStep[0m  [16/26], [94mLoss[0m : 1.49035
[1mStep[0m  [18/26], [94mLoss[0m : 1.60258
[1mStep[0m  [20/26], [94mLoss[0m : 1.70217
[1mStep[0m  [22/26], [94mLoss[0m : 1.66492
[1mStep[0m  [24/26], [94mLoss[0m : 1.73873

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59741
[1mStep[0m  [2/26], [94mLoss[0m : 1.58520
[1mStep[0m  [4/26], [94mLoss[0m : 1.49075
[1mStep[0m  [6/26], [94mLoss[0m : 1.54653
[1mStep[0m  [8/26], [94mLoss[0m : 1.62884
[1mStep[0m  [10/26], [94mLoss[0m : 1.46120
[1mStep[0m  [12/26], [94mLoss[0m : 1.46106
[1mStep[0m  [14/26], [94mLoss[0m : 1.66202
[1mStep[0m  [16/26], [94mLoss[0m : 1.47748
[1mStep[0m  [18/26], [94mLoss[0m : 1.60552
[1mStep[0m  [20/26], [94mLoss[0m : 1.64205
[1mStep[0m  [22/26], [94mLoss[0m : 1.59149
[1mStep[0m  [24/26], [94mLoss[0m : 1.65698

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.50473
[1mStep[0m  [2/26], [94mLoss[0m : 1.49161
[1mStep[0m  [4/26], [94mLoss[0m : 1.44334
[1mStep[0m  [6/26], [94mLoss[0m : 1.57048
[1mStep[0m  [8/26], [94mLoss[0m : 1.50711
[1mStep[0m  [10/26], [94mLoss[0m : 1.54996
[1mStep[0m  [12/26], [94mLoss[0m : 1.46797
[1mStep[0m  [14/26], [94mLoss[0m : 1.54507
[1mStep[0m  [16/26], [94mLoss[0m : 1.55166
[1mStep[0m  [18/26], [94mLoss[0m : 1.50681
[1mStep[0m  [20/26], [94mLoss[0m : 1.46018
[1mStep[0m  [22/26], [94mLoss[0m : 1.50148
[1mStep[0m  [24/26], [94mLoss[0m : 1.54101

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.520, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.47637
[1mStep[0m  [2/26], [94mLoss[0m : 1.48037
[1mStep[0m  [4/26], [94mLoss[0m : 1.38875
[1mStep[0m  [6/26], [94mLoss[0m : 1.35886
[1mStep[0m  [8/26], [94mLoss[0m : 1.45001
[1mStep[0m  [10/26], [94mLoss[0m : 1.50745
[1mStep[0m  [12/26], [94mLoss[0m : 1.44745
[1mStep[0m  [14/26], [94mLoss[0m : 1.48733
[1mStep[0m  [16/26], [94mLoss[0m : 1.53581
[1mStep[0m  [18/26], [94mLoss[0m : 1.57061
[1mStep[0m  [20/26], [94mLoss[0m : 1.55887
[1mStep[0m  [22/26], [94mLoss[0m : 1.46302
[1mStep[0m  [24/26], [94mLoss[0m : 1.52346

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.554, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49072
[1mStep[0m  [2/26], [94mLoss[0m : 1.46038
[1mStep[0m  [4/26], [94mLoss[0m : 1.39385
[1mStep[0m  [6/26], [94mLoss[0m : 1.33257
[1mStep[0m  [8/26], [94mLoss[0m : 1.51071
[1mStep[0m  [10/26], [94mLoss[0m : 1.52804
[1mStep[0m  [12/26], [94mLoss[0m : 1.45992
[1mStep[0m  [14/26], [94mLoss[0m : 1.51817
[1mStep[0m  [16/26], [94mLoss[0m : 1.43398
[1mStep[0m  [18/26], [94mLoss[0m : 1.53876
[1mStep[0m  [20/26], [94mLoss[0m : 1.54568
[1mStep[0m  [22/26], [94mLoss[0m : 1.50083
[1mStep[0m  [24/26], [94mLoss[0m : 1.45456

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.523, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.28943
[1mStep[0m  [2/26], [94mLoss[0m : 1.47907
[1mStep[0m  [4/26], [94mLoss[0m : 1.28144
[1mStep[0m  [6/26], [94mLoss[0m : 1.30813
[1mStep[0m  [8/26], [94mLoss[0m : 1.34747
[1mStep[0m  [10/26], [94mLoss[0m : 1.47307
[1mStep[0m  [12/26], [94mLoss[0m : 1.31367
[1mStep[0m  [14/26], [94mLoss[0m : 1.40342
[1mStep[0m  [16/26], [94mLoss[0m : 1.40752
[1mStep[0m  [18/26], [94mLoss[0m : 1.44940
[1mStep[0m  [20/26], [94mLoss[0m : 1.44046
[1mStep[0m  [22/26], [94mLoss[0m : 1.48691
[1mStep[0m  [24/26], [94mLoss[0m : 1.47843

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.415, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.29635
[1mStep[0m  [2/26], [94mLoss[0m : 1.40855
[1mStep[0m  [4/26], [94mLoss[0m : 1.34832
[1mStep[0m  [6/26], [94mLoss[0m : 1.46933
[1mStep[0m  [8/26], [94mLoss[0m : 1.42577
[1mStep[0m  [10/26], [94mLoss[0m : 1.32041
[1mStep[0m  [12/26], [94mLoss[0m : 1.27975
[1mStep[0m  [14/26], [94mLoss[0m : 1.41067
[1mStep[0m  [16/26], [94mLoss[0m : 1.48946
[1mStep[0m  [18/26], [94mLoss[0m : 1.30007
[1mStep[0m  [20/26], [94mLoss[0m : 1.46525
[1mStep[0m  [22/26], [94mLoss[0m : 1.42463
[1mStep[0m  [24/26], [94mLoss[0m : 1.30953

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.377, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.31483
[1mStep[0m  [2/26], [94mLoss[0m : 1.38041
[1mStep[0m  [4/26], [94mLoss[0m : 1.39011
[1mStep[0m  [6/26], [94mLoss[0m : 1.42512
[1mStep[0m  [8/26], [94mLoss[0m : 1.41052
[1mStep[0m  [10/26], [94mLoss[0m : 1.40615
[1mStep[0m  [12/26], [94mLoss[0m : 1.34736
[1mStep[0m  [14/26], [94mLoss[0m : 1.41686
[1mStep[0m  [16/26], [94mLoss[0m : 1.28177
[1mStep[0m  [18/26], [94mLoss[0m : 1.36444
[1mStep[0m  [20/26], [94mLoss[0m : 1.27332
[1mStep[0m  [22/26], [94mLoss[0m : 1.30052
[1mStep[0m  [24/26], [94mLoss[0m : 1.37959

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.361, [92mTest[0m: 2.490, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 20 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.533
====================================

Phase 2 - Evaluation MAE:  2.53259853216318
MAE score P1      2.401072
MAE score P2      2.532599
loss              1.360709
learning_rate     0.007525
batch_size             512
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 8, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 11.13953
[1mStep[0m  [2/26], [94mLoss[0m : 11.27166
[1mStep[0m  [4/26], [94mLoss[0m : 10.46017
[1mStep[0m  [6/26], [94mLoss[0m : 9.94391
[1mStep[0m  [8/26], [94mLoss[0m : 9.31625
[1mStep[0m  [10/26], [94mLoss[0m : 8.61965
[1mStep[0m  [12/26], [94mLoss[0m : 7.55317
[1mStep[0m  [14/26], [94mLoss[0m : 6.64603
[1mStep[0m  [16/26], [94mLoss[0m : 5.92219
[1mStep[0m  [18/26], [94mLoss[0m : 4.56900
[1mStep[0m  [20/26], [94mLoss[0m : 3.86849
[1mStep[0m  [22/26], [94mLoss[0m : 3.19847
[1mStep[0m  [24/26], [94mLoss[0m : 2.97984

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.204, [92mTest[0m: 10.989, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.99374
[1mStep[0m  [2/26], [94mLoss[0m : 3.10037
[1mStep[0m  [4/26], [94mLoss[0m : 3.21252
[1mStep[0m  [6/26], [94mLoss[0m : 3.20318
[1mStep[0m  [8/26], [94mLoss[0m : 3.25541
[1mStep[0m  [10/26], [94mLoss[0m : 3.09229
[1mStep[0m  [12/26], [94mLoss[0m : 3.07400
[1mStep[0m  [14/26], [94mLoss[0m : 2.87316
[1mStep[0m  [16/26], [94mLoss[0m : 2.94174
[1mStep[0m  [18/26], [94mLoss[0m : 2.84533
[1mStep[0m  [20/26], [94mLoss[0m : 2.95520
[1mStep[0m  [22/26], [94mLoss[0m : 2.81345
[1mStep[0m  [24/26], [94mLoss[0m : 3.08399

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.016, [92mTest[0m: 4.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.88307
[1mStep[0m  [2/26], [94mLoss[0m : 2.81542
[1mStep[0m  [4/26], [94mLoss[0m : 2.71466
[1mStep[0m  [6/26], [94mLoss[0m : 2.63680
[1mStep[0m  [8/26], [94mLoss[0m : 2.84547
[1mStep[0m  [10/26], [94mLoss[0m : 2.57445
[1mStep[0m  [12/26], [94mLoss[0m : 2.61571
[1mStep[0m  [14/26], [94mLoss[0m : 2.62284
[1mStep[0m  [16/26], [94mLoss[0m : 2.80684
[1mStep[0m  [18/26], [94mLoss[0m : 2.71035
[1mStep[0m  [20/26], [94mLoss[0m : 2.63588
[1mStep[0m  [22/26], [94mLoss[0m : 2.71581
[1mStep[0m  [24/26], [94mLoss[0m : 2.86656

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.823, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78819
[1mStep[0m  [2/26], [94mLoss[0m : 2.64667
[1mStep[0m  [4/26], [94mLoss[0m : 2.64830
[1mStep[0m  [6/26], [94mLoss[0m : 2.76452
[1mStep[0m  [8/26], [94mLoss[0m : 2.77324
[1mStep[0m  [10/26], [94mLoss[0m : 2.49096
[1mStep[0m  [12/26], [94mLoss[0m : 2.73108
[1mStep[0m  [14/26], [94mLoss[0m : 2.71888
[1mStep[0m  [16/26], [94mLoss[0m : 2.67525
[1mStep[0m  [18/26], [94mLoss[0m : 2.72072
[1mStep[0m  [20/26], [94mLoss[0m : 2.67391
[1mStep[0m  [22/26], [94mLoss[0m : 2.76724
[1mStep[0m  [24/26], [94mLoss[0m : 2.70985

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.579, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.76121
[1mStep[0m  [2/26], [94mLoss[0m : 2.63460
[1mStep[0m  [4/26], [94mLoss[0m : 2.64340
[1mStep[0m  [6/26], [94mLoss[0m : 2.60893
[1mStep[0m  [8/26], [94mLoss[0m : 2.74934
[1mStep[0m  [10/26], [94mLoss[0m : 2.66883
[1mStep[0m  [12/26], [94mLoss[0m : 2.54360
[1mStep[0m  [14/26], [94mLoss[0m : 2.67323
[1mStep[0m  [16/26], [94mLoss[0m : 2.58927
[1mStep[0m  [18/26], [94mLoss[0m : 2.67220
[1mStep[0m  [20/26], [94mLoss[0m : 2.63480
[1mStep[0m  [22/26], [94mLoss[0m : 2.67140
[1mStep[0m  [24/26], [94mLoss[0m : 2.71940

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69701
[1mStep[0m  [2/26], [94mLoss[0m : 2.49087
[1mStep[0m  [4/26], [94mLoss[0m : 2.64125
[1mStep[0m  [6/26], [94mLoss[0m : 2.59997
[1mStep[0m  [8/26], [94mLoss[0m : 2.56500
[1mStep[0m  [10/26], [94mLoss[0m : 2.51896
[1mStep[0m  [12/26], [94mLoss[0m : 2.70254
[1mStep[0m  [14/26], [94mLoss[0m : 2.54772
[1mStep[0m  [16/26], [94mLoss[0m : 2.71911
[1mStep[0m  [18/26], [94mLoss[0m : 2.60541
[1mStep[0m  [20/26], [94mLoss[0m : 2.72232
[1mStep[0m  [22/26], [94mLoss[0m : 2.48780
[1mStep[0m  [24/26], [94mLoss[0m : 2.67898

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56905
[1mStep[0m  [2/26], [94mLoss[0m : 2.61659
[1mStep[0m  [4/26], [94mLoss[0m : 2.64295
[1mStep[0m  [6/26], [94mLoss[0m : 2.59242
[1mStep[0m  [8/26], [94mLoss[0m : 2.47477
[1mStep[0m  [10/26], [94mLoss[0m : 2.45840
[1mStep[0m  [12/26], [94mLoss[0m : 2.55427
[1mStep[0m  [14/26], [94mLoss[0m : 2.81655
[1mStep[0m  [16/26], [94mLoss[0m : 2.56288
[1mStep[0m  [18/26], [94mLoss[0m : 2.67945
[1mStep[0m  [20/26], [94mLoss[0m : 2.61498
[1mStep[0m  [22/26], [94mLoss[0m : 2.60338
[1mStep[0m  [24/26], [94mLoss[0m : 2.39032

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55279
[1mStep[0m  [2/26], [94mLoss[0m : 2.60563
[1mStep[0m  [4/26], [94mLoss[0m : 2.77517
[1mStep[0m  [6/26], [94mLoss[0m : 2.45996
[1mStep[0m  [8/26], [94mLoss[0m : 2.48762
[1mStep[0m  [10/26], [94mLoss[0m : 2.50398
[1mStep[0m  [12/26], [94mLoss[0m : 2.68286
[1mStep[0m  [14/26], [94mLoss[0m : 2.79362
[1mStep[0m  [16/26], [94mLoss[0m : 2.61974
[1mStep[0m  [18/26], [94mLoss[0m : 2.49547
[1mStep[0m  [20/26], [94mLoss[0m : 2.67257
[1mStep[0m  [22/26], [94mLoss[0m : 2.64380
[1mStep[0m  [24/26], [94mLoss[0m : 2.60228

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61542
[1mStep[0m  [2/26], [94mLoss[0m : 2.58595
[1mStep[0m  [4/26], [94mLoss[0m : 2.64978
[1mStep[0m  [6/26], [94mLoss[0m : 2.50346
[1mStep[0m  [8/26], [94mLoss[0m : 2.66926
[1mStep[0m  [10/26], [94mLoss[0m : 2.46586
[1mStep[0m  [12/26], [94mLoss[0m : 2.61020
[1mStep[0m  [14/26], [94mLoss[0m : 2.60933
[1mStep[0m  [16/26], [94mLoss[0m : 2.57630
[1mStep[0m  [18/26], [94mLoss[0m : 2.70560
[1mStep[0m  [20/26], [94mLoss[0m : 2.65375
[1mStep[0m  [22/26], [94mLoss[0m : 2.65774
[1mStep[0m  [24/26], [94mLoss[0m : 2.54037

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45981
[1mStep[0m  [2/26], [94mLoss[0m : 2.58676
[1mStep[0m  [4/26], [94mLoss[0m : 2.61847
[1mStep[0m  [6/26], [94mLoss[0m : 2.63555
[1mStep[0m  [8/26], [94mLoss[0m : 2.39852
[1mStep[0m  [10/26], [94mLoss[0m : 2.49254
[1mStep[0m  [12/26], [94mLoss[0m : 2.53460
[1mStep[0m  [14/26], [94mLoss[0m : 2.50027
[1mStep[0m  [16/26], [94mLoss[0m : 2.55161
[1mStep[0m  [18/26], [94mLoss[0m : 2.51048
[1mStep[0m  [20/26], [94mLoss[0m : 2.37675
[1mStep[0m  [22/26], [94mLoss[0m : 2.65574
[1mStep[0m  [24/26], [94mLoss[0m : 2.59624

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49753
[1mStep[0m  [2/26], [94mLoss[0m : 2.51095
[1mStep[0m  [4/26], [94mLoss[0m : 2.61346
[1mStep[0m  [6/26], [94mLoss[0m : 2.44177
[1mStep[0m  [8/26], [94mLoss[0m : 2.47883
[1mStep[0m  [10/26], [94mLoss[0m : 2.62541
[1mStep[0m  [12/26], [94mLoss[0m : 2.51256
[1mStep[0m  [14/26], [94mLoss[0m : 2.44112
[1mStep[0m  [16/26], [94mLoss[0m : 2.41043
[1mStep[0m  [18/26], [94mLoss[0m : 2.44722
[1mStep[0m  [20/26], [94mLoss[0m : 2.52655
[1mStep[0m  [22/26], [94mLoss[0m : 2.43457
[1mStep[0m  [24/26], [94mLoss[0m : 2.56284

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43427
[1mStep[0m  [2/26], [94mLoss[0m : 2.57332
[1mStep[0m  [4/26], [94mLoss[0m : 2.37281
[1mStep[0m  [6/26], [94mLoss[0m : 2.46735
[1mStep[0m  [8/26], [94mLoss[0m : 2.53183
[1mStep[0m  [10/26], [94mLoss[0m : 2.37066
[1mStep[0m  [12/26], [94mLoss[0m : 2.53942
[1mStep[0m  [14/26], [94mLoss[0m : 2.43943
[1mStep[0m  [16/26], [94mLoss[0m : 2.59303
[1mStep[0m  [18/26], [94mLoss[0m : 2.48464
[1mStep[0m  [20/26], [94mLoss[0m : 2.70865
[1mStep[0m  [22/26], [94mLoss[0m : 2.49785
[1mStep[0m  [24/26], [94mLoss[0m : 2.44397

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54188
[1mStep[0m  [2/26], [94mLoss[0m : 2.60071
[1mStep[0m  [4/26], [94mLoss[0m : 2.53787
[1mStep[0m  [6/26], [94mLoss[0m : 2.53814
[1mStep[0m  [8/26], [94mLoss[0m : 2.55187
[1mStep[0m  [10/26], [94mLoss[0m : 2.55590
[1mStep[0m  [12/26], [94mLoss[0m : 2.48280
[1mStep[0m  [14/26], [94mLoss[0m : 2.58990
[1mStep[0m  [16/26], [94mLoss[0m : 2.35706
[1mStep[0m  [18/26], [94mLoss[0m : 2.49055
[1mStep[0m  [20/26], [94mLoss[0m : 2.28133
[1mStep[0m  [22/26], [94mLoss[0m : 2.56241
[1mStep[0m  [24/26], [94mLoss[0m : 2.44025

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44870
[1mStep[0m  [2/26], [94mLoss[0m : 2.46998
[1mStep[0m  [4/26], [94mLoss[0m : 2.29769
[1mStep[0m  [6/26], [94mLoss[0m : 2.61617
[1mStep[0m  [8/26], [94mLoss[0m : 2.47697
[1mStep[0m  [10/26], [94mLoss[0m : 2.72386
[1mStep[0m  [12/26], [94mLoss[0m : 2.56126
[1mStep[0m  [14/26], [94mLoss[0m : 2.62432
[1mStep[0m  [16/26], [94mLoss[0m : 2.58176
[1mStep[0m  [18/26], [94mLoss[0m : 2.48195
[1mStep[0m  [20/26], [94mLoss[0m : 2.51791
[1mStep[0m  [22/26], [94mLoss[0m : 2.31901
[1mStep[0m  [24/26], [94mLoss[0m : 2.47677

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47972
[1mStep[0m  [2/26], [94mLoss[0m : 2.49086
[1mStep[0m  [4/26], [94mLoss[0m : 2.46616
[1mStep[0m  [6/26], [94mLoss[0m : 2.45693
[1mStep[0m  [8/26], [94mLoss[0m : 2.42650
[1mStep[0m  [10/26], [94mLoss[0m : 2.35816
[1mStep[0m  [12/26], [94mLoss[0m : 2.44937
[1mStep[0m  [14/26], [94mLoss[0m : 2.42640
[1mStep[0m  [16/26], [94mLoss[0m : 2.40059
[1mStep[0m  [18/26], [94mLoss[0m : 2.55154
[1mStep[0m  [20/26], [94mLoss[0m : 2.52966
[1mStep[0m  [22/26], [94mLoss[0m : 2.54112
[1mStep[0m  [24/26], [94mLoss[0m : 2.63557

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57591
[1mStep[0m  [2/26], [94mLoss[0m : 2.46674
[1mStep[0m  [4/26], [94mLoss[0m : 2.56335
[1mStep[0m  [6/26], [94mLoss[0m : 2.58797
[1mStep[0m  [8/26], [94mLoss[0m : 2.39417
[1mStep[0m  [10/26], [94mLoss[0m : 2.52045
[1mStep[0m  [12/26], [94mLoss[0m : 2.55312
[1mStep[0m  [14/26], [94mLoss[0m : 2.38709
[1mStep[0m  [16/26], [94mLoss[0m : 2.40484
[1mStep[0m  [18/26], [94mLoss[0m : 2.40253
[1mStep[0m  [20/26], [94mLoss[0m : 2.62836
[1mStep[0m  [22/26], [94mLoss[0m : 2.64536
[1mStep[0m  [24/26], [94mLoss[0m : 2.48585

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48435
[1mStep[0m  [2/26], [94mLoss[0m : 2.27905
[1mStep[0m  [4/26], [94mLoss[0m : 2.52937
[1mStep[0m  [6/26], [94mLoss[0m : 2.59950
[1mStep[0m  [8/26], [94mLoss[0m : 2.47242
[1mStep[0m  [10/26], [94mLoss[0m : 2.36933
[1mStep[0m  [12/26], [94mLoss[0m : 2.38109
[1mStep[0m  [14/26], [94mLoss[0m : 2.53631
[1mStep[0m  [16/26], [94mLoss[0m : 2.49439
[1mStep[0m  [18/26], [94mLoss[0m : 2.29442
[1mStep[0m  [20/26], [94mLoss[0m : 2.46310
[1mStep[0m  [22/26], [94mLoss[0m : 2.43203
[1mStep[0m  [24/26], [94mLoss[0m : 2.53319

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39501
[1mStep[0m  [2/26], [94mLoss[0m : 2.23367
[1mStep[0m  [4/26], [94mLoss[0m : 2.61950
[1mStep[0m  [6/26], [94mLoss[0m : 2.34108
[1mStep[0m  [8/26], [94mLoss[0m : 2.49109
[1mStep[0m  [10/26], [94mLoss[0m : 2.46778
[1mStep[0m  [12/26], [94mLoss[0m : 2.51208
[1mStep[0m  [14/26], [94mLoss[0m : 2.47397
[1mStep[0m  [16/26], [94mLoss[0m : 2.46310
[1mStep[0m  [18/26], [94mLoss[0m : 2.28875
[1mStep[0m  [20/26], [94mLoss[0m : 2.54476
[1mStep[0m  [22/26], [94mLoss[0m : 2.53499
[1mStep[0m  [24/26], [94mLoss[0m : 2.55847

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53157
[1mStep[0m  [2/26], [94mLoss[0m : 2.44320
[1mStep[0m  [4/26], [94mLoss[0m : 2.34808
[1mStep[0m  [6/26], [94mLoss[0m : 2.46627
[1mStep[0m  [8/26], [94mLoss[0m : 2.48989
[1mStep[0m  [10/26], [94mLoss[0m : 2.52716
[1mStep[0m  [12/26], [94mLoss[0m : 2.28655
[1mStep[0m  [14/26], [94mLoss[0m : 2.56873
[1mStep[0m  [16/26], [94mLoss[0m : 2.52815
[1mStep[0m  [18/26], [94mLoss[0m : 2.50536
[1mStep[0m  [20/26], [94mLoss[0m : 2.30895
[1mStep[0m  [22/26], [94mLoss[0m : 2.41864
[1mStep[0m  [24/26], [94mLoss[0m : 2.44683

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41647
[1mStep[0m  [2/26], [94mLoss[0m : 2.34041
[1mStep[0m  [4/26], [94mLoss[0m : 2.51913
[1mStep[0m  [6/26], [94mLoss[0m : 2.43169
[1mStep[0m  [8/26], [94mLoss[0m : 2.43657
[1mStep[0m  [10/26], [94mLoss[0m : 2.44801
[1mStep[0m  [12/26], [94mLoss[0m : 2.55104
[1mStep[0m  [14/26], [94mLoss[0m : 2.40500
[1mStep[0m  [16/26], [94mLoss[0m : 2.32854
[1mStep[0m  [18/26], [94mLoss[0m : 2.42637
[1mStep[0m  [20/26], [94mLoss[0m : 2.49348
[1mStep[0m  [22/26], [94mLoss[0m : 2.44539
[1mStep[0m  [24/26], [94mLoss[0m : 2.52121

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30907
[1mStep[0m  [2/26], [94mLoss[0m : 2.48479
[1mStep[0m  [4/26], [94mLoss[0m : 2.59350
[1mStep[0m  [6/26], [94mLoss[0m : 2.31514
[1mStep[0m  [8/26], [94mLoss[0m : 2.43569
[1mStep[0m  [10/26], [94mLoss[0m : 2.43599
[1mStep[0m  [12/26], [94mLoss[0m : 2.36906
[1mStep[0m  [14/26], [94mLoss[0m : 2.45217
[1mStep[0m  [16/26], [94mLoss[0m : 2.26060
[1mStep[0m  [18/26], [94mLoss[0m : 2.42889
[1mStep[0m  [20/26], [94mLoss[0m : 2.51894
[1mStep[0m  [22/26], [94mLoss[0m : 2.48184
[1mStep[0m  [24/26], [94mLoss[0m : 2.56556

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43153
[1mStep[0m  [2/26], [94mLoss[0m : 2.52853
[1mStep[0m  [4/26], [94mLoss[0m : 2.47566
[1mStep[0m  [6/26], [94mLoss[0m : 2.50073
[1mStep[0m  [8/26], [94mLoss[0m : 2.51405
[1mStep[0m  [10/26], [94mLoss[0m : 2.45546
[1mStep[0m  [12/26], [94mLoss[0m : 2.49601
[1mStep[0m  [14/26], [94mLoss[0m : 2.33151
[1mStep[0m  [16/26], [94mLoss[0m : 2.53861
[1mStep[0m  [18/26], [94mLoss[0m : 2.34757
[1mStep[0m  [20/26], [94mLoss[0m : 2.49082
[1mStep[0m  [22/26], [94mLoss[0m : 2.53953
[1mStep[0m  [24/26], [94mLoss[0m : 2.44873

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50435
[1mStep[0m  [2/26], [94mLoss[0m : 2.34656
[1mStep[0m  [4/26], [94mLoss[0m : 2.53134
[1mStep[0m  [6/26], [94mLoss[0m : 2.53454
[1mStep[0m  [8/26], [94mLoss[0m : 2.41671
[1mStep[0m  [10/26], [94mLoss[0m : 2.60425
[1mStep[0m  [12/26], [94mLoss[0m : 2.55541
[1mStep[0m  [14/26], [94mLoss[0m : 2.60785
[1mStep[0m  [16/26], [94mLoss[0m : 2.30896
[1mStep[0m  [18/26], [94mLoss[0m : 2.42752
[1mStep[0m  [20/26], [94mLoss[0m : 2.31941
[1mStep[0m  [22/26], [94mLoss[0m : 2.49634
[1mStep[0m  [24/26], [94mLoss[0m : 2.42990

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.349, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44418
[1mStep[0m  [2/26], [94mLoss[0m : 2.34650
[1mStep[0m  [4/26], [94mLoss[0m : 2.33916
[1mStep[0m  [6/26], [94mLoss[0m : 2.54203
[1mStep[0m  [8/26], [94mLoss[0m : 2.47834
[1mStep[0m  [10/26], [94mLoss[0m : 2.32650
[1mStep[0m  [12/26], [94mLoss[0m : 2.59309
[1mStep[0m  [14/26], [94mLoss[0m : 2.43361
[1mStep[0m  [16/26], [94mLoss[0m : 2.42029
[1mStep[0m  [18/26], [94mLoss[0m : 2.46175
[1mStep[0m  [20/26], [94mLoss[0m : 2.50835
[1mStep[0m  [22/26], [94mLoss[0m : 2.51065
[1mStep[0m  [24/26], [94mLoss[0m : 2.47436

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38409
[1mStep[0m  [2/26], [94mLoss[0m : 2.39277
[1mStep[0m  [4/26], [94mLoss[0m : 2.45910
[1mStep[0m  [6/26], [94mLoss[0m : 2.57238
[1mStep[0m  [8/26], [94mLoss[0m : 2.32522
[1mStep[0m  [10/26], [94mLoss[0m : 2.35437
[1mStep[0m  [12/26], [94mLoss[0m : 2.48959
[1mStep[0m  [14/26], [94mLoss[0m : 2.34088
[1mStep[0m  [16/26], [94mLoss[0m : 2.35971
[1mStep[0m  [18/26], [94mLoss[0m : 2.40619
[1mStep[0m  [20/26], [94mLoss[0m : 2.54615
[1mStep[0m  [22/26], [94mLoss[0m : 2.46273
[1mStep[0m  [24/26], [94mLoss[0m : 2.38085

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32639
[1mStep[0m  [2/26], [94mLoss[0m : 2.46661
[1mStep[0m  [4/26], [94mLoss[0m : 2.41228
[1mStep[0m  [6/26], [94mLoss[0m : 2.43810
[1mStep[0m  [8/26], [94mLoss[0m : 2.27895
[1mStep[0m  [10/26], [94mLoss[0m : 2.33203
[1mStep[0m  [12/26], [94mLoss[0m : 2.69363
[1mStep[0m  [14/26], [94mLoss[0m : 2.22863
[1mStep[0m  [16/26], [94mLoss[0m : 2.46591
[1mStep[0m  [18/26], [94mLoss[0m : 2.41540
[1mStep[0m  [20/26], [94mLoss[0m : 2.45362
[1mStep[0m  [22/26], [94mLoss[0m : 2.37587
[1mStep[0m  [24/26], [94mLoss[0m : 2.32651

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34396
[1mStep[0m  [2/26], [94mLoss[0m : 2.45094
[1mStep[0m  [4/26], [94mLoss[0m : 2.53864
[1mStep[0m  [6/26], [94mLoss[0m : 2.34915
[1mStep[0m  [8/26], [94mLoss[0m : 2.30945
[1mStep[0m  [10/26], [94mLoss[0m : 2.28378
[1mStep[0m  [12/26], [94mLoss[0m : 2.48949
[1mStep[0m  [14/26], [94mLoss[0m : 2.49061
[1mStep[0m  [16/26], [94mLoss[0m : 2.50177
[1mStep[0m  [18/26], [94mLoss[0m : 2.34646
[1mStep[0m  [20/26], [94mLoss[0m : 2.47902
[1mStep[0m  [22/26], [94mLoss[0m : 2.37972
[1mStep[0m  [24/26], [94mLoss[0m : 2.41605

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35182
[1mStep[0m  [2/26], [94mLoss[0m : 2.53161
[1mStep[0m  [4/26], [94mLoss[0m : 2.39354
[1mStep[0m  [6/26], [94mLoss[0m : 2.32344
[1mStep[0m  [8/26], [94mLoss[0m : 2.32754
[1mStep[0m  [10/26], [94mLoss[0m : 2.47832
[1mStep[0m  [12/26], [94mLoss[0m : 2.28248
[1mStep[0m  [14/26], [94mLoss[0m : 2.36890
[1mStep[0m  [16/26], [94mLoss[0m : 2.38943
[1mStep[0m  [18/26], [94mLoss[0m : 2.35908
[1mStep[0m  [20/26], [94mLoss[0m : 2.45505
[1mStep[0m  [22/26], [94mLoss[0m : 2.32042
[1mStep[0m  [24/26], [94mLoss[0m : 2.37249

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.378, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.24713
[1mStep[0m  [2/26], [94mLoss[0m : 2.38478
[1mStep[0m  [4/26], [94mLoss[0m : 2.24878
[1mStep[0m  [6/26], [94mLoss[0m : 2.53307
[1mStep[0m  [8/26], [94mLoss[0m : 2.37137
[1mStep[0m  [10/26], [94mLoss[0m : 2.46257
[1mStep[0m  [12/26], [94mLoss[0m : 2.26445
[1mStep[0m  [14/26], [94mLoss[0m : 2.35114
[1mStep[0m  [16/26], [94mLoss[0m : 2.41776
[1mStep[0m  [18/26], [94mLoss[0m : 2.40216
[1mStep[0m  [20/26], [94mLoss[0m : 2.33499
[1mStep[0m  [22/26], [94mLoss[0m : 2.53427
[1mStep[0m  [24/26], [94mLoss[0m : 2.32375

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22486
[1mStep[0m  [2/26], [94mLoss[0m : 2.37174
[1mStep[0m  [4/26], [94mLoss[0m : 2.36701
[1mStep[0m  [6/26], [94mLoss[0m : 2.48081
[1mStep[0m  [8/26], [94mLoss[0m : 2.49788
[1mStep[0m  [10/26], [94mLoss[0m : 2.40381
[1mStep[0m  [12/26], [94mLoss[0m : 2.44352
[1mStep[0m  [14/26], [94mLoss[0m : 2.56432
[1mStep[0m  [16/26], [94mLoss[0m : 2.48036
[1mStep[0m  [18/26], [94mLoss[0m : 2.24982
[1mStep[0m  [20/26], [94mLoss[0m : 2.35965
[1mStep[0m  [22/26], [94mLoss[0m : 2.26710
[1mStep[0m  [24/26], [94mLoss[0m : 2.53626

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.351
====================================

Phase 1 - Evaluation MAE:  2.350760533259465
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.45691
[1mStep[0m  [2/26], [94mLoss[0m : 2.40649
[1mStep[0m  [4/26], [94mLoss[0m : 2.51712
[1mStep[0m  [6/26], [94mLoss[0m : 2.57685
[1mStep[0m  [8/26], [94mLoss[0m : 2.44394
[1mStep[0m  [10/26], [94mLoss[0m : 2.61794
[1mStep[0m  [12/26], [94mLoss[0m : 2.52344
[1mStep[0m  [14/26], [94mLoss[0m : 2.52388
[1mStep[0m  [16/26], [94mLoss[0m : 2.38480
[1mStep[0m  [18/26], [94mLoss[0m : 2.38102
[1mStep[0m  [20/26], [94mLoss[0m : 2.55994
[1mStep[0m  [22/26], [94mLoss[0m : 2.60805
[1mStep[0m  [24/26], [94mLoss[0m : 2.49500

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47764
[1mStep[0m  [2/26], [94mLoss[0m : 2.39357
[1mStep[0m  [4/26], [94mLoss[0m : 2.52493
[1mStep[0m  [6/26], [94mLoss[0m : 2.47410
[1mStep[0m  [8/26], [94mLoss[0m : 2.48825
[1mStep[0m  [10/26], [94mLoss[0m : 2.44393
[1mStep[0m  [12/26], [94mLoss[0m : 2.47042
[1mStep[0m  [14/26], [94mLoss[0m : 2.43009
[1mStep[0m  [16/26], [94mLoss[0m : 2.49471
[1mStep[0m  [18/26], [94mLoss[0m : 2.32571
[1mStep[0m  [20/26], [94mLoss[0m : 2.36802
[1mStep[0m  [22/26], [94mLoss[0m : 2.42584
[1mStep[0m  [24/26], [94mLoss[0m : 2.46995

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.593, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40020
[1mStep[0m  [2/26], [94mLoss[0m : 2.41417
[1mStep[0m  [4/26], [94mLoss[0m : 2.31564
[1mStep[0m  [6/26], [94mLoss[0m : 2.34800
[1mStep[0m  [8/26], [94mLoss[0m : 2.17428
[1mStep[0m  [10/26], [94mLoss[0m : 2.30221
[1mStep[0m  [12/26], [94mLoss[0m : 2.40050
[1mStep[0m  [14/26], [94mLoss[0m : 2.59236
[1mStep[0m  [16/26], [94mLoss[0m : 2.41738
[1mStep[0m  [18/26], [94mLoss[0m : 2.42586
[1mStep[0m  [20/26], [94mLoss[0m : 2.40040
[1mStep[0m  [22/26], [94mLoss[0m : 2.20147
[1mStep[0m  [24/26], [94mLoss[0m : 2.45803

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26355
[1mStep[0m  [2/26], [94mLoss[0m : 2.32149
[1mStep[0m  [4/26], [94mLoss[0m : 2.16103
[1mStep[0m  [6/26], [94mLoss[0m : 2.23238
[1mStep[0m  [8/26], [94mLoss[0m : 2.27413
[1mStep[0m  [10/26], [94mLoss[0m : 2.22914
[1mStep[0m  [12/26], [94mLoss[0m : 2.34296
[1mStep[0m  [14/26], [94mLoss[0m : 2.28619
[1mStep[0m  [16/26], [94mLoss[0m : 2.25862
[1mStep[0m  [18/26], [94mLoss[0m : 2.22783
[1mStep[0m  [20/26], [94mLoss[0m : 2.34014
[1mStep[0m  [22/26], [94mLoss[0m : 2.31836
[1mStep[0m  [24/26], [94mLoss[0m : 2.25609

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01971
[1mStep[0m  [2/26], [94mLoss[0m : 2.13918
[1mStep[0m  [4/26], [94mLoss[0m : 2.22948
[1mStep[0m  [6/26], [94mLoss[0m : 2.16467
[1mStep[0m  [8/26], [94mLoss[0m : 2.09201
[1mStep[0m  [10/26], [94mLoss[0m : 2.39350
[1mStep[0m  [12/26], [94mLoss[0m : 2.23371
[1mStep[0m  [14/26], [94mLoss[0m : 2.13288
[1mStep[0m  [16/26], [94mLoss[0m : 2.41759
[1mStep[0m  [18/26], [94mLoss[0m : 2.24758
[1mStep[0m  [20/26], [94mLoss[0m : 2.29289
[1mStep[0m  [22/26], [94mLoss[0m : 2.16321
[1mStep[0m  [24/26], [94mLoss[0m : 2.30710

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10741
[1mStep[0m  [2/26], [94mLoss[0m : 2.18166
[1mStep[0m  [4/26], [94mLoss[0m : 2.08604
[1mStep[0m  [6/26], [94mLoss[0m : 2.27521
[1mStep[0m  [8/26], [94mLoss[0m : 2.06331
[1mStep[0m  [10/26], [94mLoss[0m : 2.04378
[1mStep[0m  [12/26], [94mLoss[0m : 2.22340
[1mStep[0m  [14/26], [94mLoss[0m : 2.20647
[1mStep[0m  [16/26], [94mLoss[0m : 2.24704
[1mStep[0m  [18/26], [94mLoss[0m : 2.23031
[1mStep[0m  [20/26], [94mLoss[0m : 2.26073
[1mStep[0m  [22/26], [94mLoss[0m : 2.08246
[1mStep[0m  [24/26], [94mLoss[0m : 2.10777

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09513
[1mStep[0m  [2/26], [94mLoss[0m : 2.16719
[1mStep[0m  [4/26], [94mLoss[0m : 1.97246
[1mStep[0m  [6/26], [94mLoss[0m : 2.11171
[1mStep[0m  [8/26], [94mLoss[0m : 2.01135
[1mStep[0m  [10/26], [94mLoss[0m : 2.15093
[1mStep[0m  [12/26], [94mLoss[0m : 2.09581
[1mStep[0m  [14/26], [94mLoss[0m : 2.09497
[1mStep[0m  [16/26], [94mLoss[0m : 2.14601
[1mStep[0m  [18/26], [94mLoss[0m : 2.19818
[1mStep[0m  [20/26], [94mLoss[0m : 2.04652
[1mStep[0m  [22/26], [94mLoss[0m : 2.07960
[1mStep[0m  [24/26], [94mLoss[0m : 2.21568

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.03882
[1mStep[0m  [2/26], [94mLoss[0m : 2.07480
[1mStep[0m  [4/26], [94mLoss[0m : 1.81805
[1mStep[0m  [6/26], [94mLoss[0m : 2.15613
[1mStep[0m  [8/26], [94mLoss[0m : 2.05090
[1mStep[0m  [10/26], [94mLoss[0m : 1.94248
[1mStep[0m  [12/26], [94mLoss[0m : 2.10959
[1mStep[0m  [14/26], [94mLoss[0m : 2.13606
[1mStep[0m  [16/26], [94mLoss[0m : 1.95692
[1mStep[0m  [18/26], [94mLoss[0m : 2.13000
[1mStep[0m  [20/26], [94mLoss[0m : 2.08920
[1mStep[0m  [22/26], [94mLoss[0m : 2.03892
[1mStep[0m  [24/26], [94mLoss[0m : 2.07874

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.93397
[1mStep[0m  [2/26], [94mLoss[0m : 1.99164
[1mStep[0m  [4/26], [94mLoss[0m : 1.96778
[1mStep[0m  [6/26], [94mLoss[0m : 1.85705
[1mStep[0m  [8/26], [94mLoss[0m : 1.94613
[1mStep[0m  [10/26], [94mLoss[0m : 2.04971
[1mStep[0m  [12/26], [94mLoss[0m : 1.98336
[1mStep[0m  [14/26], [94mLoss[0m : 2.07697
[1mStep[0m  [16/26], [94mLoss[0m : 2.08765
[1mStep[0m  [18/26], [94mLoss[0m : 2.03978
[1mStep[0m  [20/26], [94mLoss[0m : 2.08791
[1mStep[0m  [22/26], [94mLoss[0m : 2.20973
[1mStep[0m  [24/26], [94mLoss[0m : 2.09388

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.88796
[1mStep[0m  [2/26], [94mLoss[0m : 2.01479
[1mStep[0m  [4/26], [94mLoss[0m : 1.92341
[1mStep[0m  [6/26], [94mLoss[0m : 1.85654
[1mStep[0m  [8/26], [94mLoss[0m : 1.94846
[1mStep[0m  [10/26], [94mLoss[0m : 1.98355
[1mStep[0m  [12/26], [94mLoss[0m : 2.01798
[1mStep[0m  [14/26], [94mLoss[0m : 1.86045
[1mStep[0m  [16/26], [94mLoss[0m : 2.00080
[1mStep[0m  [18/26], [94mLoss[0m : 1.96407
[1mStep[0m  [20/26], [94mLoss[0m : 2.02744
[1mStep[0m  [22/26], [94mLoss[0m : 1.96935
[1mStep[0m  [24/26], [94mLoss[0m : 2.00514

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96330
[1mStep[0m  [2/26], [94mLoss[0m : 1.91655
[1mStep[0m  [4/26], [94mLoss[0m : 1.96737
[1mStep[0m  [6/26], [94mLoss[0m : 1.77553
[1mStep[0m  [8/26], [94mLoss[0m : 1.89370
[1mStep[0m  [10/26], [94mLoss[0m : 2.00341
[1mStep[0m  [12/26], [94mLoss[0m : 1.97746
[1mStep[0m  [14/26], [94mLoss[0m : 1.88387
[1mStep[0m  [16/26], [94mLoss[0m : 1.87287
[1mStep[0m  [18/26], [94mLoss[0m : 1.87616
[1mStep[0m  [20/26], [94mLoss[0m : 2.03840
[1mStep[0m  [22/26], [94mLoss[0m : 2.03645
[1mStep[0m  [24/26], [94mLoss[0m : 1.97373

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97913
[1mStep[0m  [2/26], [94mLoss[0m : 1.79140
[1mStep[0m  [4/26], [94mLoss[0m : 1.79746
[1mStep[0m  [6/26], [94mLoss[0m : 1.93090
[1mStep[0m  [8/26], [94mLoss[0m : 1.99366
[1mStep[0m  [10/26], [94mLoss[0m : 1.80143
[1mStep[0m  [12/26], [94mLoss[0m : 1.80499
[1mStep[0m  [14/26], [94mLoss[0m : 1.90024
[1mStep[0m  [16/26], [94mLoss[0m : 1.89207
[1mStep[0m  [18/26], [94mLoss[0m : 1.80733
[1mStep[0m  [20/26], [94mLoss[0m : 1.83654
[1mStep[0m  [22/26], [94mLoss[0m : 1.91026
[1mStep[0m  [24/26], [94mLoss[0m : 1.80573

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.80166
[1mStep[0m  [2/26], [94mLoss[0m : 1.78747
[1mStep[0m  [4/26], [94mLoss[0m : 1.89250
[1mStep[0m  [6/26], [94mLoss[0m : 1.85562
[1mStep[0m  [8/26], [94mLoss[0m : 1.83434
[1mStep[0m  [10/26], [94mLoss[0m : 1.89963
[1mStep[0m  [12/26], [94mLoss[0m : 1.80503
[1mStep[0m  [14/26], [94mLoss[0m : 1.84004
[1mStep[0m  [16/26], [94mLoss[0m : 1.83255
[1mStep[0m  [18/26], [94mLoss[0m : 1.86971
[1mStep[0m  [20/26], [94mLoss[0m : 1.86515
[1mStep[0m  [22/26], [94mLoss[0m : 1.91475
[1mStep[0m  [24/26], [94mLoss[0m : 1.87768

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.80189
[1mStep[0m  [2/26], [94mLoss[0m : 1.76391
[1mStep[0m  [4/26], [94mLoss[0m : 1.71535
[1mStep[0m  [6/26], [94mLoss[0m : 1.83994
[1mStep[0m  [8/26], [94mLoss[0m : 1.87053
[1mStep[0m  [10/26], [94mLoss[0m : 1.72315
[1mStep[0m  [12/26], [94mLoss[0m : 1.86749
[1mStep[0m  [14/26], [94mLoss[0m : 1.92939
[1mStep[0m  [16/26], [94mLoss[0m : 1.88068
[1mStep[0m  [18/26], [94mLoss[0m : 1.79185
[1mStep[0m  [20/26], [94mLoss[0m : 1.79891
[1mStep[0m  [22/26], [94mLoss[0m : 1.88602
[1mStep[0m  [24/26], [94mLoss[0m : 1.92042

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.72558
[1mStep[0m  [2/26], [94mLoss[0m : 1.65038
[1mStep[0m  [4/26], [94mLoss[0m : 1.69588
[1mStep[0m  [6/26], [94mLoss[0m : 1.77647
[1mStep[0m  [8/26], [94mLoss[0m : 1.79106
[1mStep[0m  [10/26], [94mLoss[0m : 1.74873
[1mStep[0m  [12/26], [94mLoss[0m : 1.75651
[1mStep[0m  [14/26], [94mLoss[0m : 1.79726
[1mStep[0m  [16/26], [94mLoss[0m : 1.93420
[1mStep[0m  [18/26], [94mLoss[0m : 1.86914
[1mStep[0m  [20/26], [94mLoss[0m : 1.62285
[1mStep[0m  [22/26], [94mLoss[0m : 1.64563
[1mStep[0m  [24/26], [94mLoss[0m : 1.80565

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.62083
[1mStep[0m  [2/26], [94mLoss[0m : 1.64555
[1mStep[0m  [4/26], [94mLoss[0m : 1.63718
[1mStep[0m  [6/26], [94mLoss[0m : 1.72853
[1mStep[0m  [8/26], [94mLoss[0m : 1.81902
[1mStep[0m  [10/26], [94mLoss[0m : 1.80713
[1mStep[0m  [12/26], [94mLoss[0m : 1.80631
[1mStep[0m  [14/26], [94mLoss[0m : 1.70782
[1mStep[0m  [16/26], [94mLoss[0m : 1.71889
[1mStep[0m  [18/26], [94mLoss[0m : 1.89158
[1mStep[0m  [20/26], [94mLoss[0m : 1.74347
[1mStep[0m  [22/26], [94mLoss[0m : 1.82337
[1mStep[0m  [24/26], [94mLoss[0m : 1.79395

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.752, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.74113
[1mStep[0m  [2/26], [94mLoss[0m : 1.80457
[1mStep[0m  [4/26], [94mLoss[0m : 1.58078
[1mStep[0m  [6/26], [94mLoss[0m : 1.75111
[1mStep[0m  [8/26], [94mLoss[0m : 1.74740
[1mStep[0m  [10/26], [94mLoss[0m : 1.72110
[1mStep[0m  [12/26], [94mLoss[0m : 1.75298
[1mStep[0m  [14/26], [94mLoss[0m : 1.70612
[1mStep[0m  [16/26], [94mLoss[0m : 1.69965
[1mStep[0m  [18/26], [94mLoss[0m : 1.78937
[1mStep[0m  [20/26], [94mLoss[0m : 1.73706
[1mStep[0m  [22/26], [94mLoss[0m : 1.90633
[1mStep[0m  [24/26], [94mLoss[0m : 1.89819

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.68047
[1mStep[0m  [2/26], [94mLoss[0m : 1.72541
[1mStep[0m  [4/26], [94mLoss[0m : 1.57849
[1mStep[0m  [6/26], [94mLoss[0m : 1.62683
[1mStep[0m  [8/26], [94mLoss[0m : 1.66949
[1mStep[0m  [10/26], [94mLoss[0m : 1.64748
[1mStep[0m  [12/26], [94mLoss[0m : 1.61742
[1mStep[0m  [14/26], [94mLoss[0m : 1.73600
[1mStep[0m  [16/26], [94mLoss[0m : 1.71080
[1mStep[0m  [18/26], [94mLoss[0m : 1.76828
[1mStep[0m  [20/26], [94mLoss[0m : 1.82193
[1mStep[0m  [22/26], [94mLoss[0m : 1.79302
[1mStep[0m  [24/26], [94mLoss[0m : 1.67546

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.432, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.67296
[1mStep[0m  [2/26], [94mLoss[0m : 1.69470
[1mStep[0m  [4/26], [94mLoss[0m : 1.68824
[1mStep[0m  [6/26], [94mLoss[0m : 1.86178
[1mStep[0m  [8/26], [94mLoss[0m : 1.64780
[1mStep[0m  [10/26], [94mLoss[0m : 1.61406
[1mStep[0m  [12/26], [94mLoss[0m : 1.76642
[1mStep[0m  [14/26], [94mLoss[0m : 1.74868
[1mStep[0m  [16/26], [94mLoss[0m : 1.69602
[1mStep[0m  [18/26], [94mLoss[0m : 1.65195
[1mStep[0m  [20/26], [94mLoss[0m : 1.77719
[1mStep[0m  [22/26], [94mLoss[0m : 1.74388
[1mStep[0m  [24/26], [94mLoss[0m : 1.72902

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61535
[1mStep[0m  [2/26], [94mLoss[0m : 1.66146
[1mStep[0m  [4/26], [94mLoss[0m : 1.62202
[1mStep[0m  [6/26], [94mLoss[0m : 1.72883
[1mStep[0m  [8/26], [94mLoss[0m : 1.66512
[1mStep[0m  [10/26], [94mLoss[0m : 1.59986
[1mStep[0m  [12/26], [94mLoss[0m : 1.64009
[1mStep[0m  [14/26], [94mLoss[0m : 1.56768
[1mStep[0m  [16/26], [94mLoss[0m : 1.68393
[1mStep[0m  [18/26], [94mLoss[0m : 1.77270
[1mStep[0m  [20/26], [94mLoss[0m : 1.70490
[1mStep[0m  [22/26], [94mLoss[0m : 1.73606
[1mStep[0m  [24/26], [94mLoss[0m : 1.69461

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.57555
[1mStep[0m  [2/26], [94mLoss[0m : 1.62395
[1mStep[0m  [4/26], [94mLoss[0m : 1.62070
[1mStep[0m  [6/26], [94mLoss[0m : 1.73506
[1mStep[0m  [8/26], [94mLoss[0m : 1.55543
[1mStep[0m  [10/26], [94mLoss[0m : 1.66646
[1mStep[0m  [12/26], [94mLoss[0m : 1.59576
[1mStep[0m  [14/26], [94mLoss[0m : 1.73131
[1mStep[0m  [16/26], [94mLoss[0m : 1.59756
[1mStep[0m  [18/26], [94mLoss[0m : 1.65843
[1mStep[0m  [20/26], [94mLoss[0m : 1.66745
[1mStep[0m  [22/26], [94mLoss[0m : 1.67851
[1mStep[0m  [24/26], [94mLoss[0m : 1.63049

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.52865
[1mStep[0m  [2/26], [94mLoss[0m : 1.53685
[1mStep[0m  [4/26], [94mLoss[0m : 1.58485
[1mStep[0m  [6/26], [94mLoss[0m : 1.61222
[1mStep[0m  [8/26], [94mLoss[0m : 1.67936
[1mStep[0m  [10/26], [94mLoss[0m : 1.50000
[1mStep[0m  [12/26], [94mLoss[0m : 1.69704
[1mStep[0m  [14/26], [94mLoss[0m : 1.64490
[1mStep[0m  [16/26], [94mLoss[0m : 1.48363
[1mStep[0m  [18/26], [94mLoss[0m : 1.59001
[1mStep[0m  [20/26], [94mLoss[0m : 1.61551
[1mStep[0m  [22/26], [94mLoss[0m : 1.65382
[1mStep[0m  [24/26], [94mLoss[0m : 1.63712

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.58763
[1mStep[0m  [2/26], [94mLoss[0m : 1.58401
[1mStep[0m  [4/26], [94mLoss[0m : 1.63753
[1mStep[0m  [6/26], [94mLoss[0m : 1.50800
[1mStep[0m  [8/26], [94mLoss[0m : 1.64089
[1mStep[0m  [10/26], [94mLoss[0m : 1.51375
[1mStep[0m  [12/26], [94mLoss[0m : 1.52868
[1mStep[0m  [14/26], [94mLoss[0m : 1.56908
[1mStep[0m  [16/26], [94mLoss[0m : 1.51517
[1mStep[0m  [18/26], [94mLoss[0m : 1.65510
[1mStep[0m  [20/26], [94mLoss[0m : 1.52011
[1mStep[0m  [22/26], [94mLoss[0m : 1.75503
[1mStep[0m  [24/26], [94mLoss[0m : 1.56309

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.58510
[1mStep[0m  [2/26], [94mLoss[0m : 1.48693
[1mStep[0m  [4/26], [94mLoss[0m : 1.62366
[1mStep[0m  [6/26], [94mLoss[0m : 1.58790
[1mStep[0m  [8/26], [94mLoss[0m : 1.47071
[1mStep[0m  [10/26], [94mLoss[0m : 1.58788
[1mStep[0m  [12/26], [94mLoss[0m : 1.61651
[1mStep[0m  [14/26], [94mLoss[0m : 1.47338
[1mStep[0m  [16/26], [94mLoss[0m : 1.50462
[1mStep[0m  [18/26], [94mLoss[0m : 1.47323
[1mStep[0m  [20/26], [94mLoss[0m : 1.48481
[1mStep[0m  [22/26], [94mLoss[0m : 1.57039
[1mStep[0m  [24/26], [94mLoss[0m : 1.62613

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.46130
[1mStep[0m  [2/26], [94mLoss[0m : 1.57031
[1mStep[0m  [4/26], [94mLoss[0m : 1.64441
[1mStep[0m  [6/26], [94mLoss[0m : 1.56579
[1mStep[0m  [8/26], [94mLoss[0m : 1.62631
[1mStep[0m  [10/26], [94mLoss[0m : 1.38494
[1mStep[0m  [12/26], [94mLoss[0m : 1.54613
[1mStep[0m  [14/26], [94mLoss[0m : 1.46285
[1mStep[0m  [16/26], [94mLoss[0m : 1.59589
[1mStep[0m  [18/26], [94mLoss[0m : 1.57822
[1mStep[0m  [20/26], [94mLoss[0m : 1.55327
[1mStep[0m  [22/26], [94mLoss[0m : 1.49324
[1mStep[0m  [24/26], [94mLoss[0m : 1.52158

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.499, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.48565
[1mStep[0m  [2/26], [94mLoss[0m : 1.42569
[1mStep[0m  [4/26], [94mLoss[0m : 1.50052
[1mStep[0m  [6/26], [94mLoss[0m : 1.64772
[1mStep[0m  [8/26], [94mLoss[0m : 1.44396
[1mStep[0m  [10/26], [94mLoss[0m : 1.54571
[1mStep[0m  [12/26], [94mLoss[0m : 1.54884
[1mStep[0m  [14/26], [94mLoss[0m : 1.54557
[1mStep[0m  [16/26], [94mLoss[0m : 1.50671
[1mStep[0m  [18/26], [94mLoss[0m : 1.63956
[1mStep[0m  [20/26], [94mLoss[0m : 1.44968
[1mStep[0m  [22/26], [94mLoss[0m : 1.49418
[1mStep[0m  [24/26], [94mLoss[0m : 1.66867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.60028
[1mStep[0m  [2/26], [94mLoss[0m : 1.43982
[1mStep[0m  [4/26], [94mLoss[0m : 1.69420
[1mStep[0m  [6/26], [94mLoss[0m : 1.46975
[1mStep[0m  [8/26], [94mLoss[0m : 1.47541
[1mStep[0m  [10/26], [94mLoss[0m : 1.51578
[1mStep[0m  [12/26], [94mLoss[0m : 1.45482
[1mStep[0m  [14/26], [94mLoss[0m : 1.50531
[1mStep[0m  [16/26], [94mLoss[0m : 1.37649
[1mStep[0m  [18/26], [94mLoss[0m : 1.62943
[1mStep[0m  [20/26], [94mLoss[0m : 1.62101
[1mStep[0m  [22/26], [94mLoss[0m : 1.58446
[1mStep[0m  [24/26], [94mLoss[0m : 1.49754

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.499, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.44924
[1mStep[0m  [2/26], [94mLoss[0m : 1.51884
[1mStep[0m  [4/26], [94mLoss[0m : 1.51805
[1mStep[0m  [6/26], [94mLoss[0m : 1.39605
[1mStep[0m  [8/26], [94mLoss[0m : 1.55018
[1mStep[0m  [10/26], [94mLoss[0m : 1.60493
[1mStep[0m  [12/26], [94mLoss[0m : 1.44264
[1mStep[0m  [14/26], [94mLoss[0m : 1.43550
[1mStep[0m  [16/26], [94mLoss[0m : 1.50048
[1mStep[0m  [18/26], [94mLoss[0m : 1.46135
[1mStep[0m  [20/26], [94mLoss[0m : 1.50043
[1mStep[0m  [22/26], [94mLoss[0m : 1.52513
[1mStep[0m  [24/26], [94mLoss[0m : 1.45967

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.43788
[1mStep[0m  [2/26], [94mLoss[0m : 1.40191
[1mStep[0m  [4/26], [94mLoss[0m : 1.57326
[1mStep[0m  [6/26], [94mLoss[0m : 1.51296
[1mStep[0m  [8/26], [94mLoss[0m : 1.44400
[1mStep[0m  [10/26], [94mLoss[0m : 1.44287
[1mStep[0m  [12/26], [94mLoss[0m : 1.42291
[1mStep[0m  [14/26], [94mLoss[0m : 1.53012
[1mStep[0m  [16/26], [94mLoss[0m : 1.58191
[1mStep[0m  [18/26], [94mLoss[0m : 1.40793
[1mStep[0m  [20/26], [94mLoss[0m : 1.50145
[1mStep[0m  [22/26], [94mLoss[0m : 1.56872
[1mStep[0m  [24/26], [94mLoss[0m : 1.59039

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.44056
[1mStep[0m  [2/26], [94mLoss[0m : 1.44311
[1mStep[0m  [4/26], [94mLoss[0m : 1.37972
[1mStep[0m  [6/26], [94mLoss[0m : 1.48045
[1mStep[0m  [8/26], [94mLoss[0m : 1.55227
[1mStep[0m  [10/26], [94mLoss[0m : 1.54340
[1mStep[0m  [12/26], [94mLoss[0m : 1.41395
[1mStep[0m  [14/26], [94mLoss[0m : 1.46506
[1mStep[0m  [16/26], [94mLoss[0m : 1.52752
[1mStep[0m  [18/26], [94mLoss[0m : 1.49302
[1mStep[0m  [20/26], [94mLoss[0m : 1.55839
[1mStep[0m  [22/26], [94mLoss[0m : 1.54064
[1mStep[0m  [24/26], [94mLoss[0m : 1.56596

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.494, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.473
====================================

Phase 2 - Evaluation MAE:  2.472627199613131
MAE score P1       2.350761
MAE score P2       2.472627
loss               1.491178
learning_rate      0.007525
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.83650
[1mStep[0m  [2/26], [94mLoss[0m : 10.44193
[1mStep[0m  [4/26], [94mLoss[0m : 9.87934
[1mStep[0m  [6/26], [94mLoss[0m : 9.31927
[1mStep[0m  [8/26], [94mLoss[0m : 8.84033
[1mStep[0m  [10/26], [94mLoss[0m : 8.58856
[1mStep[0m  [12/26], [94mLoss[0m : 8.20501
[1mStep[0m  [14/26], [94mLoss[0m : 7.77243
[1mStep[0m  [16/26], [94mLoss[0m : 7.34240
[1mStep[0m  [18/26], [94mLoss[0m : 6.97503
[1mStep[0m  [20/26], [94mLoss[0m : 6.64419
[1mStep[0m  [22/26], [94mLoss[0m : 5.73676
[1mStep[0m  [24/26], [94mLoss[0m : 5.56955

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.090, [92mTest[0m: 10.882, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.07101
[1mStep[0m  [2/26], [94mLoss[0m : 4.99444
[1mStep[0m  [4/26], [94mLoss[0m : 4.38472
[1mStep[0m  [6/26], [94mLoss[0m : 4.30299
[1mStep[0m  [8/26], [94mLoss[0m : 4.06492
[1mStep[0m  [10/26], [94mLoss[0m : 3.71488
[1mStep[0m  [12/26], [94mLoss[0m : 3.44529
[1mStep[0m  [14/26], [94mLoss[0m : 3.69333
[1mStep[0m  [16/26], [94mLoss[0m : 3.42168
[1mStep[0m  [18/26], [94mLoss[0m : 3.24595
[1mStep[0m  [20/26], [94mLoss[0m : 3.30949
[1mStep[0m  [22/26], [94mLoss[0m : 3.21576
[1mStep[0m  [24/26], [94mLoss[0m : 2.92237

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.803, [92mTest[0m: 5.182, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.07751
[1mStep[0m  [2/26], [94mLoss[0m : 2.94758
[1mStep[0m  [4/26], [94mLoss[0m : 2.88571
[1mStep[0m  [6/26], [94mLoss[0m : 3.01194
[1mStep[0m  [8/26], [94mLoss[0m : 2.97894
[1mStep[0m  [10/26], [94mLoss[0m : 3.07921
[1mStep[0m  [12/26], [94mLoss[0m : 2.97937
[1mStep[0m  [14/26], [94mLoss[0m : 2.86440
[1mStep[0m  [16/26], [94mLoss[0m : 2.81374
[1mStep[0m  [18/26], [94mLoss[0m : 2.85068
[1mStep[0m  [20/26], [94mLoss[0m : 2.79828
[1mStep[0m  [22/26], [94mLoss[0m : 2.73003
[1mStep[0m  [24/26], [94mLoss[0m : 2.69000

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.890, [92mTest[0m: 2.950, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78981
[1mStep[0m  [2/26], [94mLoss[0m : 2.73226
[1mStep[0m  [4/26], [94mLoss[0m : 2.62015
[1mStep[0m  [6/26], [94mLoss[0m : 2.77418
[1mStep[0m  [8/26], [94mLoss[0m : 2.75832
[1mStep[0m  [10/26], [94mLoss[0m : 2.81449
[1mStep[0m  [12/26], [94mLoss[0m : 2.65501
[1mStep[0m  [14/26], [94mLoss[0m : 2.59440
[1mStep[0m  [16/26], [94mLoss[0m : 2.72988
[1mStep[0m  [18/26], [94mLoss[0m : 2.78534
[1mStep[0m  [20/26], [94mLoss[0m : 2.77901
[1mStep[0m  [22/26], [94mLoss[0m : 2.73285
[1mStep[0m  [24/26], [94mLoss[0m : 2.78104

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.639, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66129
[1mStep[0m  [2/26], [94mLoss[0m : 2.64837
[1mStep[0m  [4/26], [94mLoss[0m : 2.78818
[1mStep[0m  [6/26], [94mLoss[0m : 2.78052
[1mStep[0m  [8/26], [94mLoss[0m : 2.71954
[1mStep[0m  [10/26], [94mLoss[0m : 2.69628
[1mStep[0m  [12/26], [94mLoss[0m : 2.87410
[1mStep[0m  [14/26], [94mLoss[0m : 2.68186
[1mStep[0m  [16/26], [94mLoss[0m : 2.49359
[1mStep[0m  [18/26], [94mLoss[0m : 2.75748
[1mStep[0m  [20/26], [94mLoss[0m : 2.60885
[1mStep[0m  [22/26], [94mLoss[0m : 2.58044
[1mStep[0m  [24/26], [94mLoss[0m : 2.58048

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.551, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56876
[1mStep[0m  [2/26], [94mLoss[0m : 2.73262
[1mStep[0m  [4/26], [94mLoss[0m : 2.48619
[1mStep[0m  [6/26], [94mLoss[0m : 2.64214
[1mStep[0m  [8/26], [94mLoss[0m : 2.58634
[1mStep[0m  [10/26], [94mLoss[0m : 2.42622
[1mStep[0m  [12/26], [94mLoss[0m : 2.61793
[1mStep[0m  [14/26], [94mLoss[0m : 2.55821
[1mStep[0m  [16/26], [94mLoss[0m : 2.61591
[1mStep[0m  [18/26], [94mLoss[0m : 2.82109
[1mStep[0m  [20/26], [94mLoss[0m : 2.54327
[1mStep[0m  [22/26], [94mLoss[0m : 2.44823
[1mStep[0m  [24/26], [94mLoss[0m : 2.82985

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.522, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61745
[1mStep[0m  [2/26], [94mLoss[0m : 2.54868
[1mStep[0m  [4/26], [94mLoss[0m : 2.64719
[1mStep[0m  [6/26], [94mLoss[0m : 2.59404
[1mStep[0m  [8/26], [94mLoss[0m : 2.57920
[1mStep[0m  [10/26], [94mLoss[0m : 2.71979
[1mStep[0m  [12/26], [94mLoss[0m : 2.64716
[1mStep[0m  [14/26], [94mLoss[0m : 2.68548
[1mStep[0m  [16/26], [94mLoss[0m : 2.59157
[1mStep[0m  [18/26], [94mLoss[0m : 2.62806
[1mStep[0m  [20/26], [94mLoss[0m : 2.65809
[1mStep[0m  [22/26], [94mLoss[0m : 2.76368
[1mStep[0m  [24/26], [94mLoss[0m : 2.60714

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.515, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58761
[1mStep[0m  [2/26], [94mLoss[0m : 2.61732
[1mStep[0m  [4/26], [94mLoss[0m : 2.59075
[1mStep[0m  [6/26], [94mLoss[0m : 2.66413
[1mStep[0m  [8/26], [94mLoss[0m : 2.54847
[1mStep[0m  [10/26], [94mLoss[0m : 2.63471
[1mStep[0m  [12/26], [94mLoss[0m : 2.66185
[1mStep[0m  [14/26], [94mLoss[0m : 2.59716
[1mStep[0m  [16/26], [94mLoss[0m : 2.59202
[1mStep[0m  [18/26], [94mLoss[0m : 2.71921
[1mStep[0m  [20/26], [94mLoss[0m : 2.67794
[1mStep[0m  [22/26], [94mLoss[0m : 2.57005
[1mStep[0m  [24/26], [94mLoss[0m : 2.46708

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.78604
[1mStep[0m  [2/26], [94mLoss[0m : 2.74607
[1mStep[0m  [4/26], [94mLoss[0m : 2.56169
[1mStep[0m  [6/26], [94mLoss[0m : 2.58751
[1mStep[0m  [8/26], [94mLoss[0m : 2.56050
[1mStep[0m  [10/26], [94mLoss[0m : 2.62929
[1mStep[0m  [12/26], [94mLoss[0m : 2.81024
[1mStep[0m  [14/26], [94mLoss[0m : 2.51761
[1mStep[0m  [16/26], [94mLoss[0m : 2.58206
[1mStep[0m  [18/26], [94mLoss[0m : 2.52122
[1mStep[0m  [20/26], [94mLoss[0m : 2.63059
[1mStep[0m  [22/26], [94mLoss[0m : 2.47872
[1mStep[0m  [24/26], [94mLoss[0m : 2.66334

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44894
[1mStep[0m  [2/26], [94mLoss[0m : 2.70851
[1mStep[0m  [4/26], [94mLoss[0m : 2.54958
[1mStep[0m  [6/26], [94mLoss[0m : 2.61352
[1mStep[0m  [8/26], [94mLoss[0m : 2.51565
[1mStep[0m  [10/26], [94mLoss[0m : 2.55259
[1mStep[0m  [12/26], [94mLoss[0m : 2.55822
[1mStep[0m  [14/26], [94mLoss[0m : 2.48810
[1mStep[0m  [16/26], [94mLoss[0m : 2.57197
[1mStep[0m  [18/26], [94mLoss[0m : 2.50644
[1mStep[0m  [20/26], [94mLoss[0m : 2.45121
[1mStep[0m  [22/26], [94mLoss[0m : 2.68070
[1mStep[0m  [24/26], [94mLoss[0m : 2.64700

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72715
[1mStep[0m  [2/26], [94mLoss[0m : 2.62952
[1mStep[0m  [4/26], [94mLoss[0m : 2.59909
[1mStep[0m  [6/26], [94mLoss[0m : 2.62316
[1mStep[0m  [8/26], [94mLoss[0m : 2.64063
[1mStep[0m  [10/26], [94mLoss[0m : 2.54058
[1mStep[0m  [12/26], [94mLoss[0m : 2.65761
[1mStep[0m  [14/26], [94mLoss[0m : 2.71672
[1mStep[0m  [16/26], [94mLoss[0m : 2.54916
[1mStep[0m  [18/26], [94mLoss[0m : 2.77802
[1mStep[0m  [20/26], [94mLoss[0m : 2.59081
[1mStep[0m  [22/26], [94mLoss[0m : 2.55300
[1mStep[0m  [24/26], [94mLoss[0m : 2.68196

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62302
[1mStep[0m  [2/26], [94mLoss[0m : 2.50001
[1mStep[0m  [4/26], [94mLoss[0m : 2.54762
[1mStep[0m  [6/26], [94mLoss[0m : 2.50438
[1mStep[0m  [8/26], [94mLoss[0m : 2.80257
[1mStep[0m  [10/26], [94mLoss[0m : 2.74830
[1mStep[0m  [12/26], [94mLoss[0m : 2.74724
[1mStep[0m  [14/26], [94mLoss[0m : 2.59899
[1mStep[0m  [16/26], [94mLoss[0m : 2.48586
[1mStep[0m  [18/26], [94mLoss[0m : 2.60573
[1mStep[0m  [20/26], [94mLoss[0m : 2.53946
[1mStep[0m  [22/26], [94mLoss[0m : 2.58268
[1mStep[0m  [24/26], [94mLoss[0m : 2.52310

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.486, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59025
[1mStep[0m  [2/26], [94mLoss[0m : 2.44211
[1mStep[0m  [4/26], [94mLoss[0m : 2.54234
[1mStep[0m  [6/26], [94mLoss[0m : 2.57730
[1mStep[0m  [8/26], [94mLoss[0m : 2.59534
[1mStep[0m  [10/26], [94mLoss[0m : 2.50829
[1mStep[0m  [12/26], [94mLoss[0m : 2.51761
[1mStep[0m  [14/26], [94mLoss[0m : 2.77022
[1mStep[0m  [16/26], [94mLoss[0m : 2.67089
[1mStep[0m  [18/26], [94mLoss[0m : 2.53967
[1mStep[0m  [20/26], [94mLoss[0m : 2.42755
[1mStep[0m  [22/26], [94mLoss[0m : 2.57737
[1mStep[0m  [24/26], [94mLoss[0m : 2.62972

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43375
[1mStep[0m  [2/26], [94mLoss[0m : 2.64137
[1mStep[0m  [4/26], [94mLoss[0m : 2.62841
[1mStep[0m  [6/26], [94mLoss[0m : 2.63333
[1mStep[0m  [8/26], [94mLoss[0m : 2.63319
[1mStep[0m  [10/26], [94mLoss[0m : 2.66695
[1mStep[0m  [12/26], [94mLoss[0m : 2.73034
[1mStep[0m  [14/26], [94mLoss[0m : 2.64292
[1mStep[0m  [16/26], [94mLoss[0m : 2.53698
[1mStep[0m  [18/26], [94mLoss[0m : 2.68837
[1mStep[0m  [20/26], [94mLoss[0m : 2.60412
[1mStep[0m  [22/26], [94mLoss[0m : 2.42939
[1mStep[0m  [24/26], [94mLoss[0m : 2.63083

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54566
[1mStep[0m  [2/26], [94mLoss[0m : 2.61692
[1mStep[0m  [4/26], [94mLoss[0m : 2.50896
[1mStep[0m  [6/26], [94mLoss[0m : 2.59364
[1mStep[0m  [8/26], [94mLoss[0m : 2.66788
[1mStep[0m  [10/26], [94mLoss[0m : 2.52059
[1mStep[0m  [12/26], [94mLoss[0m : 2.60547
[1mStep[0m  [14/26], [94mLoss[0m : 2.54947
[1mStep[0m  [16/26], [94mLoss[0m : 2.55039
[1mStep[0m  [18/26], [94mLoss[0m : 2.60122
[1mStep[0m  [20/26], [94mLoss[0m : 2.66941
[1mStep[0m  [22/26], [94mLoss[0m : 2.56329
[1mStep[0m  [24/26], [94mLoss[0m : 2.59620

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69553
[1mStep[0m  [2/26], [94mLoss[0m : 2.44284
[1mStep[0m  [4/26], [94mLoss[0m : 2.61709
[1mStep[0m  [6/26], [94mLoss[0m : 2.74282
[1mStep[0m  [8/26], [94mLoss[0m : 2.70785
[1mStep[0m  [10/26], [94mLoss[0m : 2.60910
[1mStep[0m  [12/26], [94mLoss[0m : 2.59225
[1mStep[0m  [14/26], [94mLoss[0m : 2.62944
[1mStep[0m  [16/26], [94mLoss[0m : 2.55835
[1mStep[0m  [18/26], [94mLoss[0m : 2.58775
[1mStep[0m  [20/26], [94mLoss[0m : 2.53245
[1mStep[0m  [22/26], [94mLoss[0m : 2.67186
[1mStep[0m  [24/26], [94mLoss[0m : 2.55266

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63971
[1mStep[0m  [2/26], [94mLoss[0m : 2.59407
[1mStep[0m  [4/26], [94mLoss[0m : 2.55459
[1mStep[0m  [6/26], [94mLoss[0m : 2.61316
[1mStep[0m  [8/26], [94mLoss[0m : 2.51330
[1mStep[0m  [10/26], [94mLoss[0m : 2.61247
[1mStep[0m  [12/26], [94mLoss[0m : 2.50400
[1mStep[0m  [14/26], [94mLoss[0m : 2.52208
[1mStep[0m  [16/26], [94mLoss[0m : 2.72244
[1mStep[0m  [18/26], [94mLoss[0m : 2.52428
[1mStep[0m  [20/26], [94mLoss[0m : 2.57778
[1mStep[0m  [22/26], [94mLoss[0m : 2.68147
[1mStep[0m  [24/26], [94mLoss[0m : 2.63450

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45952
[1mStep[0m  [2/26], [94mLoss[0m : 2.73183
[1mStep[0m  [4/26], [94mLoss[0m : 2.68680
[1mStep[0m  [6/26], [94mLoss[0m : 2.56354
[1mStep[0m  [8/26], [94mLoss[0m : 2.64660
[1mStep[0m  [10/26], [94mLoss[0m : 2.59255
[1mStep[0m  [12/26], [94mLoss[0m : 2.69030
[1mStep[0m  [14/26], [94mLoss[0m : 2.56256
[1mStep[0m  [16/26], [94mLoss[0m : 2.60800
[1mStep[0m  [18/26], [94mLoss[0m : 2.65056
[1mStep[0m  [20/26], [94mLoss[0m : 2.52962
[1mStep[0m  [22/26], [94mLoss[0m : 2.71158
[1mStep[0m  [24/26], [94mLoss[0m : 2.55516

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49495
[1mStep[0m  [2/26], [94mLoss[0m : 2.62408
[1mStep[0m  [4/26], [94mLoss[0m : 2.48609
[1mStep[0m  [6/26], [94mLoss[0m : 2.60187
[1mStep[0m  [8/26], [94mLoss[0m : 2.43530
[1mStep[0m  [10/26], [94mLoss[0m : 2.48984
[1mStep[0m  [12/26], [94mLoss[0m : 2.48978
[1mStep[0m  [14/26], [94mLoss[0m : 2.55475
[1mStep[0m  [16/26], [94mLoss[0m : 2.57116
[1mStep[0m  [18/26], [94mLoss[0m : 2.49301
[1mStep[0m  [20/26], [94mLoss[0m : 2.53578
[1mStep[0m  [22/26], [94mLoss[0m : 2.60450
[1mStep[0m  [24/26], [94mLoss[0m : 2.51599

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44368
[1mStep[0m  [2/26], [94mLoss[0m : 2.67368
[1mStep[0m  [4/26], [94mLoss[0m : 2.48202
[1mStep[0m  [6/26], [94mLoss[0m : 2.67559
[1mStep[0m  [8/26], [94mLoss[0m : 2.42626
[1mStep[0m  [10/26], [94mLoss[0m : 2.40522
[1mStep[0m  [12/26], [94mLoss[0m : 2.55626
[1mStep[0m  [14/26], [94mLoss[0m : 2.57186
[1mStep[0m  [16/26], [94mLoss[0m : 2.60154
[1mStep[0m  [18/26], [94mLoss[0m : 2.44973
[1mStep[0m  [20/26], [94mLoss[0m : 2.65707
[1mStep[0m  [22/26], [94mLoss[0m : 2.71228
[1mStep[0m  [24/26], [94mLoss[0m : 2.47399

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.458, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56722
[1mStep[0m  [2/26], [94mLoss[0m : 2.45982
[1mStep[0m  [4/26], [94mLoss[0m : 2.63917
[1mStep[0m  [6/26], [94mLoss[0m : 2.57516
[1mStep[0m  [8/26], [94mLoss[0m : 2.52417
[1mStep[0m  [10/26], [94mLoss[0m : 2.67478
[1mStep[0m  [12/26], [94mLoss[0m : 2.63015
[1mStep[0m  [14/26], [94mLoss[0m : 2.60005
[1mStep[0m  [16/26], [94mLoss[0m : 2.69294
[1mStep[0m  [18/26], [94mLoss[0m : 2.59881
[1mStep[0m  [20/26], [94mLoss[0m : 2.57568
[1mStep[0m  [22/26], [94mLoss[0m : 2.35088
[1mStep[0m  [24/26], [94mLoss[0m : 2.42900

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.460, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62099
[1mStep[0m  [2/26], [94mLoss[0m : 2.56091
[1mStep[0m  [4/26], [94mLoss[0m : 2.61292
[1mStep[0m  [6/26], [94mLoss[0m : 2.44603
[1mStep[0m  [8/26], [94mLoss[0m : 2.62047
[1mStep[0m  [10/26], [94mLoss[0m : 2.46556
[1mStep[0m  [12/26], [94mLoss[0m : 2.63156
[1mStep[0m  [14/26], [94mLoss[0m : 2.59444
[1mStep[0m  [16/26], [94mLoss[0m : 2.56821
[1mStep[0m  [18/26], [94mLoss[0m : 2.57427
[1mStep[0m  [20/26], [94mLoss[0m : 2.46001
[1mStep[0m  [22/26], [94mLoss[0m : 2.60509
[1mStep[0m  [24/26], [94mLoss[0m : 2.58697

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70224
[1mStep[0m  [2/26], [94mLoss[0m : 2.61939
[1mStep[0m  [4/26], [94mLoss[0m : 2.41455
[1mStep[0m  [6/26], [94mLoss[0m : 2.60107
[1mStep[0m  [8/26], [94mLoss[0m : 2.55818
[1mStep[0m  [10/26], [94mLoss[0m : 2.48949
[1mStep[0m  [12/26], [94mLoss[0m : 2.59503
[1mStep[0m  [14/26], [94mLoss[0m : 2.40529
[1mStep[0m  [16/26], [94mLoss[0m : 2.56899
[1mStep[0m  [18/26], [94mLoss[0m : 2.46574
[1mStep[0m  [20/26], [94mLoss[0m : 2.44808
[1mStep[0m  [22/26], [94mLoss[0m : 2.60164
[1mStep[0m  [24/26], [94mLoss[0m : 2.68317

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53547
[1mStep[0m  [2/26], [94mLoss[0m : 2.56862
[1mStep[0m  [4/26], [94mLoss[0m : 2.52009
[1mStep[0m  [6/26], [94mLoss[0m : 2.69149
[1mStep[0m  [8/26], [94mLoss[0m : 2.49805
[1mStep[0m  [10/26], [94mLoss[0m : 2.59081
[1mStep[0m  [12/26], [94mLoss[0m : 2.50212
[1mStep[0m  [14/26], [94mLoss[0m : 2.65252
[1mStep[0m  [16/26], [94mLoss[0m : 2.45532
[1mStep[0m  [18/26], [94mLoss[0m : 2.66042
[1mStep[0m  [20/26], [94mLoss[0m : 2.46886
[1mStep[0m  [22/26], [94mLoss[0m : 2.70807
[1mStep[0m  [24/26], [94mLoss[0m : 2.57435

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71429
[1mStep[0m  [2/26], [94mLoss[0m : 2.54002
[1mStep[0m  [4/26], [94mLoss[0m : 2.57213
[1mStep[0m  [6/26], [94mLoss[0m : 2.49338
[1mStep[0m  [8/26], [94mLoss[0m : 2.40712
[1mStep[0m  [10/26], [94mLoss[0m : 2.58995
[1mStep[0m  [12/26], [94mLoss[0m : 2.51834
[1mStep[0m  [14/26], [94mLoss[0m : 2.60843
[1mStep[0m  [16/26], [94mLoss[0m : 2.34259
[1mStep[0m  [18/26], [94mLoss[0m : 2.57547
[1mStep[0m  [20/26], [94mLoss[0m : 2.55545
[1mStep[0m  [22/26], [94mLoss[0m : 2.57162
[1mStep[0m  [24/26], [94mLoss[0m : 2.66042

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.446, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56130
[1mStep[0m  [2/26], [94mLoss[0m : 2.45651
[1mStep[0m  [4/26], [94mLoss[0m : 2.44788
[1mStep[0m  [6/26], [94mLoss[0m : 2.69777
[1mStep[0m  [8/26], [94mLoss[0m : 2.51012
[1mStep[0m  [10/26], [94mLoss[0m : 2.57163
[1mStep[0m  [12/26], [94mLoss[0m : 2.55431
[1mStep[0m  [14/26], [94mLoss[0m : 2.55195
[1mStep[0m  [16/26], [94mLoss[0m : 2.67709
[1mStep[0m  [18/26], [94mLoss[0m : 2.65143
[1mStep[0m  [20/26], [94mLoss[0m : 2.49985
[1mStep[0m  [22/26], [94mLoss[0m : 2.66502
[1mStep[0m  [24/26], [94mLoss[0m : 2.63277

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55829
[1mStep[0m  [2/26], [94mLoss[0m : 2.64593
[1mStep[0m  [4/26], [94mLoss[0m : 2.58233
[1mStep[0m  [6/26], [94mLoss[0m : 2.45052
[1mStep[0m  [8/26], [94mLoss[0m : 2.46530
[1mStep[0m  [10/26], [94mLoss[0m : 2.51571
[1mStep[0m  [12/26], [94mLoss[0m : 2.50012
[1mStep[0m  [14/26], [94mLoss[0m : 2.44671
[1mStep[0m  [16/26], [94mLoss[0m : 2.48809
[1mStep[0m  [18/26], [94mLoss[0m : 2.57343
[1mStep[0m  [20/26], [94mLoss[0m : 2.47965
[1mStep[0m  [22/26], [94mLoss[0m : 2.73030
[1mStep[0m  [24/26], [94mLoss[0m : 2.66314

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.450, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63174
[1mStep[0m  [2/26], [94mLoss[0m : 2.72527
[1mStep[0m  [4/26], [94mLoss[0m : 2.58225
[1mStep[0m  [6/26], [94mLoss[0m : 2.48072
[1mStep[0m  [8/26], [94mLoss[0m : 2.59550
[1mStep[0m  [10/26], [94mLoss[0m : 2.57671
[1mStep[0m  [12/26], [94mLoss[0m : 2.57460
[1mStep[0m  [14/26], [94mLoss[0m : 2.40190
[1mStep[0m  [16/26], [94mLoss[0m : 2.55544
[1mStep[0m  [18/26], [94mLoss[0m : 2.46250
[1mStep[0m  [20/26], [94mLoss[0m : 2.46239
[1mStep[0m  [22/26], [94mLoss[0m : 2.45636
[1mStep[0m  [24/26], [94mLoss[0m : 2.57272

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.443, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57844
[1mStep[0m  [2/26], [94mLoss[0m : 2.53104
[1mStep[0m  [4/26], [94mLoss[0m : 2.61485
[1mStep[0m  [6/26], [94mLoss[0m : 2.52777
[1mStep[0m  [8/26], [94mLoss[0m : 2.51260
[1mStep[0m  [10/26], [94mLoss[0m : 2.56643
[1mStep[0m  [12/26], [94mLoss[0m : 2.59373
[1mStep[0m  [14/26], [94mLoss[0m : 2.67344
[1mStep[0m  [16/26], [94mLoss[0m : 2.61428
[1mStep[0m  [18/26], [94mLoss[0m : 2.55889
[1mStep[0m  [20/26], [94mLoss[0m : 2.51473
[1mStep[0m  [22/26], [94mLoss[0m : 2.64685
[1mStep[0m  [24/26], [94mLoss[0m : 2.47128

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65081
[1mStep[0m  [2/26], [94mLoss[0m : 2.53807
[1mStep[0m  [4/26], [94mLoss[0m : 2.49560
[1mStep[0m  [6/26], [94mLoss[0m : 2.60446
[1mStep[0m  [8/26], [94mLoss[0m : 2.53312
[1mStep[0m  [10/26], [94mLoss[0m : 2.58652
[1mStep[0m  [12/26], [94mLoss[0m : 2.54902
[1mStep[0m  [14/26], [94mLoss[0m : 2.47798
[1mStep[0m  [16/26], [94mLoss[0m : 2.50348
[1mStep[0m  [18/26], [94mLoss[0m : 2.57276
[1mStep[0m  [20/26], [94mLoss[0m : 2.60919
[1mStep[0m  [22/26], [94mLoss[0m : 2.74268
[1mStep[0m  [24/26], [94mLoss[0m : 2.56411

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.437, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.447
====================================

Phase 1 - Evaluation MAE:  2.446640234727126
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.52387
[1mStep[0m  [2/26], [94mLoss[0m : 2.44562
[1mStep[0m  [4/26], [94mLoss[0m : 2.48535
[1mStep[0m  [6/26], [94mLoss[0m : 2.63998
[1mStep[0m  [8/26], [94mLoss[0m : 2.57835
[1mStep[0m  [10/26], [94mLoss[0m : 2.52042
[1mStep[0m  [12/26], [94mLoss[0m : 2.68078
[1mStep[0m  [14/26], [94mLoss[0m : 2.60687
[1mStep[0m  [16/26], [94mLoss[0m : 2.56961
[1mStep[0m  [18/26], [94mLoss[0m : 2.53262
[1mStep[0m  [20/26], [94mLoss[0m : 2.59465
[1mStep[0m  [22/26], [94mLoss[0m : 2.62228
[1mStep[0m  [24/26], [94mLoss[0m : 2.51907

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55033
[1mStep[0m  [2/26], [94mLoss[0m : 2.48723
[1mStep[0m  [4/26], [94mLoss[0m : 2.55351
[1mStep[0m  [6/26], [94mLoss[0m : 2.51318
[1mStep[0m  [8/26], [94mLoss[0m : 2.69150
[1mStep[0m  [10/26], [94mLoss[0m : 2.60725
[1mStep[0m  [12/26], [94mLoss[0m : 2.50843
[1mStep[0m  [14/26], [94mLoss[0m : 2.60724
[1mStep[0m  [16/26], [94mLoss[0m : 2.56768
[1mStep[0m  [18/26], [94mLoss[0m : 2.45592
[1mStep[0m  [20/26], [94mLoss[0m : 2.52045
[1mStep[0m  [22/26], [94mLoss[0m : 2.63366
[1mStep[0m  [24/26], [94mLoss[0m : 2.53229

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72565
[1mStep[0m  [2/26], [94mLoss[0m : 2.51548
[1mStep[0m  [4/26], [94mLoss[0m : 2.51953
[1mStep[0m  [6/26], [94mLoss[0m : 2.47282
[1mStep[0m  [8/26], [94mLoss[0m : 2.41162
[1mStep[0m  [10/26], [94mLoss[0m : 2.54027
[1mStep[0m  [12/26], [94mLoss[0m : 2.54968
[1mStep[0m  [14/26], [94mLoss[0m : 2.66212
[1mStep[0m  [16/26], [94mLoss[0m : 2.46888
[1mStep[0m  [18/26], [94mLoss[0m : 2.50318
[1mStep[0m  [20/26], [94mLoss[0m : 2.57078
[1mStep[0m  [22/26], [94mLoss[0m : 2.58027
[1mStep[0m  [24/26], [94mLoss[0m : 2.55211

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58110
[1mStep[0m  [2/26], [94mLoss[0m : 2.56951
[1mStep[0m  [4/26], [94mLoss[0m : 2.68135
[1mStep[0m  [6/26], [94mLoss[0m : 2.53952
[1mStep[0m  [8/26], [94mLoss[0m : 2.40847
[1mStep[0m  [10/26], [94mLoss[0m : 2.60344
[1mStep[0m  [12/26], [94mLoss[0m : 2.55917
[1mStep[0m  [14/26], [94mLoss[0m : 2.54042
[1mStep[0m  [16/26], [94mLoss[0m : 2.70121
[1mStep[0m  [18/26], [94mLoss[0m : 2.51157
[1mStep[0m  [20/26], [94mLoss[0m : 2.60103
[1mStep[0m  [22/26], [94mLoss[0m : 2.49426
[1mStep[0m  [24/26], [94mLoss[0m : 2.58474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45160
[1mStep[0m  [2/26], [94mLoss[0m : 2.50932
[1mStep[0m  [4/26], [94mLoss[0m : 2.38479
[1mStep[0m  [6/26], [94mLoss[0m : 2.51424
[1mStep[0m  [8/26], [94mLoss[0m : 2.43235
[1mStep[0m  [10/26], [94mLoss[0m : 2.66351
[1mStep[0m  [12/26], [94mLoss[0m : 2.50727
[1mStep[0m  [14/26], [94mLoss[0m : 2.72050
[1mStep[0m  [16/26], [94mLoss[0m : 2.60648
[1mStep[0m  [18/26], [94mLoss[0m : 2.51806
[1mStep[0m  [20/26], [94mLoss[0m : 2.49342
[1mStep[0m  [22/26], [94mLoss[0m : 2.43660
[1mStep[0m  [24/26], [94mLoss[0m : 2.48466

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63112
[1mStep[0m  [2/26], [94mLoss[0m : 2.42811
[1mStep[0m  [4/26], [94mLoss[0m : 2.62525
[1mStep[0m  [6/26], [94mLoss[0m : 2.46009
[1mStep[0m  [8/26], [94mLoss[0m : 2.53451
[1mStep[0m  [10/26], [94mLoss[0m : 2.55922
[1mStep[0m  [12/26], [94mLoss[0m : 2.25437
[1mStep[0m  [14/26], [94mLoss[0m : 2.56786
[1mStep[0m  [16/26], [94mLoss[0m : 2.50986
[1mStep[0m  [18/26], [94mLoss[0m : 2.49599
[1mStep[0m  [20/26], [94mLoss[0m : 2.58020
[1mStep[0m  [22/26], [94mLoss[0m : 2.68522
[1mStep[0m  [24/26], [94mLoss[0m : 2.53619

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.499, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47488
[1mStep[0m  [2/26], [94mLoss[0m : 2.41055
[1mStep[0m  [4/26], [94mLoss[0m : 2.44116
[1mStep[0m  [6/26], [94mLoss[0m : 2.40715
[1mStep[0m  [8/26], [94mLoss[0m : 2.46786
[1mStep[0m  [10/26], [94mLoss[0m : 2.47417
[1mStep[0m  [12/26], [94mLoss[0m : 2.46574
[1mStep[0m  [14/26], [94mLoss[0m : 2.65028
[1mStep[0m  [16/26], [94mLoss[0m : 2.49846
[1mStep[0m  [18/26], [94mLoss[0m : 2.54000
[1mStep[0m  [20/26], [94mLoss[0m : 2.56884
[1mStep[0m  [22/26], [94mLoss[0m : 2.66625
[1mStep[0m  [24/26], [94mLoss[0m : 2.52933

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.572, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43668
[1mStep[0m  [2/26], [94mLoss[0m : 2.34255
[1mStep[0m  [4/26], [94mLoss[0m : 2.32268
[1mStep[0m  [6/26], [94mLoss[0m : 2.55825
[1mStep[0m  [8/26], [94mLoss[0m : 2.52272
[1mStep[0m  [10/26], [94mLoss[0m : 2.48327
[1mStep[0m  [12/26], [94mLoss[0m : 2.43675
[1mStep[0m  [14/26], [94mLoss[0m : 2.62558
[1mStep[0m  [16/26], [94mLoss[0m : 2.44584
[1mStep[0m  [18/26], [94mLoss[0m : 2.45493
[1mStep[0m  [20/26], [94mLoss[0m : 2.60015
[1mStep[0m  [22/26], [94mLoss[0m : 2.65756
[1mStep[0m  [24/26], [94mLoss[0m : 2.43444

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.511, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59124
[1mStep[0m  [2/26], [94mLoss[0m : 2.49428
[1mStep[0m  [4/26], [94mLoss[0m : 2.37032
[1mStep[0m  [6/26], [94mLoss[0m : 2.27397
[1mStep[0m  [8/26], [94mLoss[0m : 2.58446
[1mStep[0m  [10/26], [94mLoss[0m : 2.43086
[1mStep[0m  [12/26], [94mLoss[0m : 2.52845
[1mStep[0m  [14/26], [94mLoss[0m : 2.46758
[1mStep[0m  [16/26], [94mLoss[0m : 2.63273
[1mStep[0m  [18/26], [94mLoss[0m : 2.55542
[1mStep[0m  [20/26], [94mLoss[0m : 2.52251
[1mStep[0m  [22/26], [94mLoss[0m : 2.33250
[1mStep[0m  [24/26], [94mLoss[0m : 2.48244

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45489
[1mStep[0m  [2/26], [94mLoss[0m : 2.54790
[1mStep[0m  [4/26], [94mLoss[0m : 2.42951
[1mStep[0m  [6/26], [94mLoss[0m : 2.37914
[1mStep[0m  [8/26], [94mLoss[0m : 2.49371
[1mStep[0m  [10/26], [94mLoss[0m : 2.68352
[1mStep[0m  [12/26], [94mLoss[0m : 2.54748
[1mStep[0m  [14/26], [94mLoss[0m : 2.38882
[1mStep[0m  [16/26], [94mLoss[0m : 2.50596
[1mStep[0m  [18/26], [94mLoss[0m : 2.42191
[1mStep[0m  [20/26], [94mLoss[0m : 2.54190
[1mStep[0m  [22/26], [94mLoss[0m : 2.53250
[1mStep[0m  [24/26], [94mLoss[0m : 2.56812

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.634, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50649
[1mStep[0m  [2/26], [94mLoss[0m : 2.42987
[1mStep[0m  [4/26], [94mLoss[0m : 2.40554
[1mStep[0m  [6/26], [94mLoss[0m : 2.26354
[1mStep[0m  [8/26], [94mLoss[0m : 2.41351
[1mStep[0m  [10/26], [94mLoss[0m : 2.46785
[1mStep[0m  [12/26], [94mLoss[0m : 2.63578
[1mStep[0m  [14/26], [94mLoss[0m : 2.54696
[1mStep[0m  [16/26], [94mLoss[0m : 2.54225
[1mStep[0m  [18/26], [94mLoss[0m : 2.43645
[1mStep[0m  [20/26], [94mLoss[0m : 2.42747
[1mStep[0m  [22/26], [94mLoss[0m : 2.51170
[1mStep[0m  [24/26], [94mLoss[0m : 2.45360

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.576, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35491
[1mStep[0m  [2/26], [94mLoss[0m : 2.55053
[1mStep[0m  [4/26], [94mLoss[0m : 2.35642
[1mStep[0m  [6/26], [94mLoss[0m : 2.50012
[1mStep[0m  [8/26], [94mLoss[0m : 2.57647
[1mStep[0m  [10/26], [94mLoss[0m : 2.52287
[1mStep[0m  [12/26], [94mLoss[0m : 2.45509
[1mStep[0m  [14/26], [94mLoss[0m : 2.56039
[1mStep[0m  [16/26], [94mLoss[0m : 2.57615
[1mStep[0m  [18/26], [94mLoss[0m : 2.51584
[1mStep[0m  [20/26], [94mLoss[0m : 2.34691
[1mStep[0m  [22/26], [94mLoss[0m : 2.47430
[1mStep[0m  [24/26], [94mLoss[0m : 2.41241

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.570, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56105
[1mStep[0m  [2/26], [94mLoss[0m : 2.42870
[1mStep[0m  [4/26], [94mLoss[0m : 2.41228
[1mStep[0m  [6/26], [94mLoss[0m : 2.57523
[1mStep[0m  [8/26], [94mLoss[0m : 2.47439
[1mStep[0m  [10/26], [94mLoss[0m : 2.54835
[1mStep[0m  [12/26], [94mLoss[0m : 2.35053
[1mStep[0m  [14/26], [94mLoss[0m : 2.53103
[1mStep[0m  [16/26], [94mLoss[0m : 2.55123
[1mStep[0m  [18/26], [94mLoss[0m : 2.43608
[1mStep[0m  [20/26], [94mLoss[0m : 2.56270
[1mStep[0m  [22/26], [94mLoss[0m : 2.41801
[1mStep[0m  [24/26], [94mLoss[0m : 2.48920

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.560, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49225
[1mStep[0m  [2/26], [94mLoss[0m : 2.43512
[1mStep[0m  [4/26], [94mLoss[0m : 2.34585
[1mStep[0m  [6/26], [94mLoss[0m : 2.53333
[1mStep[0m  [8/26], [94mLoss[0m : 2.41559
[1mStep[0m  [10/26], [94mLoss[0m : 2.49372
[1mStep[0m  [12/26], [94mLoss[0m : 2.42475
[1mStep[0m  [14/26], [94mLoss[0m : 2.43932
[1mStep[0m  [16/26], [94mLoss[0m : 2.53286
[1mStep[0m  [18/26], [94mLoss[0m : 2.47433
[1mStep[0m  [20/26], [94mLoss[0m : 2.45240
[1mStep[0m  [22/26], [94mLoss[0m : 2.42850
[1mStep[0m  [24/26], [94mLoss[0m : 2.48558

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.550, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51440
[1mStep[0m  [2/26], [94mLoss[0m : 2.40164
[1mStep[0m  [4/26], [94mLoss[0m : 2.35519
[1mStep[0m  [6/26], [94mLoss[0m : 2.28862
[1mStep[0m  [8/26], [94mLoss[0m : 2.34674
[1mStep[0m  [10/26], [94mLoss[0m : 2.50388
[1mStep[0m  [12/26], [94mLoss[0m : 2.52123
[1mStep[0m  [14/26], [94mLoss[0m : 2.56979
[1mStep[0m  [16/26], [94mLoss[0m : 2.46237
[1mStep[0m  [18/26], [94mLoss[0m : 2.44908
[1mStep[0m  [20/26], [94mLoss[0m : 2.36703
[1mStep[0m  [22/26], [94mLoss[0m : 2.39608
[1mStep[0m  [24/26], [94mLoss[0m : 2.41264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35588
[1mStep[0m  [2/26], [94mLoss[0m : 2.38194
[1mStep[0m  [4/26], [94mLoss[0m : 2.15914
[1mStep[0m  [6/26], [94mLoss[0m : 2.45312
[1mStep[0m  [8/26], [94mLoss[0m : 2.60279
[1mStep[0m  [10/26], [94mLoss[0m : 2.47097
[1mStep[0m  [12/26], [94mLoss[0m : 2.52059
[1mStep[0m  [14/26], [94mLoss[0m : 2.41358
[1mStep[0m  [16/26], [94mLoss[0m : 2.39312
[1mStep[0m  [18/26], [94mLoss[0m : 2.53450
[1mStep[0m  [20/26], [94mLoss[0m : 2.65991
[1mStep[0m  [22/26], [94mLoss[0m : 2.64911
[1mStep[0m  [24/26], [94mLoss[0m : 2.31771

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.578, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35885
[1mStep[0m  [2/26], [94mLoss[0m : 2.42151
[1mStep[0m  [4/26], [94mLoss[0m : 2.28229
[1mStep[0m  [6/26], [94mLoss[0m : 2.49912
[1mStep[0m  [8/26], [94mLoss[0m : 2.57201
[1mStep[0m  [10/26], [94mLoss[0m : 2.37123
[1mStep[0m  [12/26], [94mLoss[0m : 2.41530
[1mStep[0m  [14/26], [94mLoss[0m : 2.46011
[1mStep[0m  [16/26], [94mLoss[0m : 2.41400
[1mStep[0m  [18/26], [94mLoss[0m : 2.25716
[1mStep[0m  [20/26], [94mLoss[0m : 2.46306
[1mStep[0m  [22/26], [94mLoss[0m : 2.38595
[1mStep[0m  [24/26], [94mLoss[0m : 2.37190

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.683, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33769
[1mStep[0m  [2/26], [94mLoss[0m : 2.39575
[1mStep[0m  [4/26], [94mLoss[0m : 2.21263
[1mStep[0m  [6/26], [94mLoss[0m : 2.52482
[1mStep[0m  [8/26], [94mLoss[0m : 2.42933
[1mStep[0m  [10/26], [94mLoss[0m : 2.25877
[1mStep[0m  [12/26], [94mLoss[0m : 2.36132
[1mStep[0m  [14/26], [94mLoss[0m : 2.46934
[1mStep[0m  [16/26], [94mLoss[0m : 2.48535
[1mStep[0m  [18/26], [94mLoss[0m : 2.41332
[1mStep[0m  [20/26], [94mLoss[0m : 2.28912
[1mStep[0m  [22/26], [94mLoss[0m : 2.39368
[1mStep[0m  [24/26], [94mLoss[0m : 2.47515

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.597, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38048
[1mStep[0m  [2/26], [94mLoss[0m : 2.34498
[1mStep[0m  [4/26], [94mLoss[0m : 2.33936
[1mStep[0m  [6/26], [94mLoss[0m : 2.27892
[1mStep[0m  [8/26], [94mLoss[0m : 2.58665
[1mStep[0m  [10/26], [94mLoss[0m : 2.40436
[1mStep[0m  [12/26], [94mLoss[0m : 2.42756
[1mStep[0m  [14/26], [94mLoss[0m : 2.48610
[1mStep[0m  [16/26], [94mLoss[0m : 2.61485
[1mStep[0m  [18/26], [94mLoss[0m : 2.40626
[1mStep[0m  [20/26], [94mLoss[0m : 2.36706
[1mStep[0m  [22/26], [94mLoss[0m : 2.34038
[1mStep[0m  [24/26], [94mLoss[0m : 2.60648

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.644, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51882
[1mStep[0m  [2/26], [94mLoss[0m : 2.50684
[1mStep[0m  [4/26], [94mLoss[0m : 2.30973
[1mStep[0m  [6/26], [94mLoss[0m : 2.49392
[1mStep[0m  [8/26], [94mLoss[0m : 2.39390
[1mStep[0m  [10/26], [94mLoss[0m : 2.12550
[1mStep[0m  [12/26], [94mLoss[0m : 2.48608
[1mStep[0m  [14/26], [94mLoss[0m : 2.35471
[1mStep[0m  [16/26], [94mLoss[0m : 2.55285
[1mStep[0m  [18/26], [94mLoss[0m : 2.25209
[1mStep[0m  [20/26], [94mLoss[0m : 2.47925
[1mStep[0m  [22/26], [94mLoss[0m : 2.21054
[1mStep[0m  [24/26], [94mLoss[0m : 2.60249

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.641, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44684
[1mStep[0m  [2/26], [94mLoss[0m : 2.23129
[1mStep[0m  [4/26], [94mLoss[0m : 2.39314
[1mStep[0m  [6/26], [94mLoss[0m : 2.39715
[1mStep[0m  [8/26], [94mLoss[0m : 2.52638
[1mStep[0m  [10/26], [94mLoss[0m : 2.43672
[1mStep[0m  [12/26], [94mLoss[0m : 2.42797
[1mStep[0m  [14/26], [94mLoss[0m : 2.30216
[1mStep[0m  [16/26], [94mLoss[0m : 2.42525
[1mStep[0m  [18/26], [94mLoss[0m : 2.48016
[1mStep[0m  [20/26], [94mLoss[0m : 2.16138
[1mStep[0m  [22/26], [94mLoss[0m : 2.31636
[1mStep[0m  [24/26], [94mLoss[0m : 2.40608

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.528, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43541
[1mStep[0m  [2/26], [94mLoss[0m : 2.29262
[1mStep[0m  [4/26], [94mLoss[0m : 2.35014
[1mStep[0m  [6/26], [94mLoss[0m : 2.44916
[1mStep[0m  [8/26], [94mLoss[0m : 2.31024
[1mStep[0m  [10/26], [94mLoss[0m : 2.32431
[1mStep[0m  [12/26], [94mLoss[0m : 2.33175
[1mStep[0m  [14/26], [94mLoss[0m : 2.30167
[1mStep[0m  [16/26], [94mLoss[0m : 2.37010
[1mStep[0m  [18/26], [94mLoss[0m : 2.47149
[1mStep[0m  [20/26], [94mLoss[0m : 2.24560
[1mStep[0m  [22/26], [94mLoss[0m : 2.46023
[1mStep[0m  [24/26], [94mLoss[0m : 2.28192

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.640, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26105
[1mStep[0m  [2/26], [94mLoss[0m : 2.43099
[1mStep[0m  [4/26], [94mLoss[0m : 2.37054
[1mStep[0m  [6/26], [94mLoss[0m : 2.38408
[1mStep[0m  [8/26], [94mLoss[0m : 2.44335
[1mStep[0m  [10/26], [94mLoss[0m : 2.41069
[1mStep[0m  [12/26], [94mLoss[0m : 2.32111
[1mStep[0m  [14/26], [94mLoss[0m : 2.24950
[1mStep[0m  [16/26], [94mLoss[0m : 2.33514
[1mStep[0m  [18/26], [94mLoss[0m : 2.40152
[1mStep[0m  [20/26], [94mLoss[0m : 2.40410
[1mStep[0m  [22/26], [94mLoss[0m : 2.32740
[1mStep[0m  [24/26], [94mLoss[0m : 2.41088

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.651, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37789
[1mStep[0m  [2/26], [94mLoss[0m : 2.26013
[1mStep[0m  [4/26], [94mLoss[0m : 2.38910
[1mStep[0m  [6/26], [94mLoss[0m : 2.28954
[1mStep[0m  [8/26], [94mLoss[0m : 2.37218
[1mStep[0m  [10/26], [94mLoss[0m : 2.36347
[1mStep[0m  [12/26], [94mLoss[0m : 2.38317
[1mStep[0m  [14/26], [94mLoss[0m : 2.35238
[1mStep[0m  [16/26], [94mLoss[0m : 2.32377
[1mStep[0m  [18/26], [94mLoss[0m : 2.32976
[1mStep[0m  [20/26], [94mLoss[0m : 2.46050
[1mStep[0m  [22/26], [94mLoss[0m : 2.27371
[1mStep[0m  [24/26], [94mLoss[0m : 2.44531

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.675, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.28423
[1mStep[0m  [2/26], [94mLoss[0m : 2.37725
[1mStep[0m  [4/26], [94mLoss[0m : 2.30804
[1mStep[0m  [6/26], [94mLoss[0m : 2.43526
[1mStep[0m  [8/26], [94mLoss[0m : 2.31036
[1mStep[0m  [10/26], [94mLoss[0m : 2.37482
[1mStep[0m  [12/26], [94mLoss[0m : 2.24171
[1mStep[0m  [14/26], [94mLoss[0m : 2.34943
[1mStep[0m  [16/26], [94mLoss[0m : 2.31581
[1mStep[0m  [18/26], [94mLoss[0m : 2.44077
[1mStep[0m  [20/26], [94mLoss[0m : 2.30466
[1mStep[0m  [22/26], [94mLoss[0m : 2.12271
[1mStep[0m  [24/26], [94mLoss[0m : 2.41340

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35269
[1mStep[0m  [2/26], [94mLoss[0m : 2.24325
[1mStep[0m  [4/26], [94mLoss[0m : 2.25424
[1mStep[0m  [6/26], [94mLoss[0m : 2.33769
[1mStep[0m  [8/26], [94mLoss[0m : 2.39869
[1mStep[0m  [10/26], [94mLoss[0m : 2.35675
[1mStep[0m  [12/26], [94mLoss[0m : 2.25990
[1mStep[0m  [14/26], [94mLoss[0m : 2.20310
[1mStep[0m  [16/26], [94mLoss[0m : 2.31333
[1mStep[0m  [18/26], [94mLoss[0m : 2.23126
[1mStep[0m  [20/26], [94mLoss[0m : 2.31297
[1mStep[0m  [22/26], [94mLoss[0m : 2.33404
[1mStep[0m  [24/26], [94mLoss[0m : 2.28980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.658, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41862
[1mStep[0m  [2/26], [94mLoss[0m : 2.34022
[1mStep[0m  [4/26], [94mLoss[0m : 2.45055
[1mStep[0m  [6/26], [94mLoss[0m : 2.14966
[1mStep[0m  [8/26], [94mLoss[0m : 2.35502
[1mStep[0m  [10/26], [94mLoss[0m : 2.35250
[1mStep[0m  [12/26], [94mLoss[0m : 2.40975
[1mStep[0m  [14/26], [94mLoss[0m : 2.19125
[1mStep[0m  [16/26], [94mLoss[0m : 2.32237
[1mStep[0m  [18/26], [94mLoss[0m : 2.24760
[1mStep[0m  [20/26], [94mLoss[0m : 2.42536
[1mStep[0m  [22/26], [94mLoss[0m : 2.24274
[1mStep[0m  [24/26], [94mLoss[0m : 2.30453

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.629, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41394
[1mStep[0m  [2/26], [94mLoss[0m : 2.19990
[1mStep[0m  [4/26], [94mLoss[0m : 2.26387
[1mStep[0m  [6/26], [94mLoss[0m : 2.27562
[1mStep[0m  [8/26], [94mLoss[0m : 2.22333
[1mStep[0m  [10/26], [94mLoss[0m : 2.31387
[1mStep[0m  [12/26], [94mLoss[0m : 2.24681
[1mStep[0m  [14/26], [94mLoss[0m : 2.26079
[1mStep[0m  [16/26], [94mLoss[0m : 2.27249
[1mStep[0m  [18/26], [94mLoss[0m : 2.27278
[1mStep[0m  [20/26], [94mLoss[0m : 2.33908
[1mStep[0m  [22/26], [94mLoss[0m : 2.23810
[1mStep[0m  [24/26], [94mLoss[0m : 2.22214

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.573, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31219
[1mStep[0m  [2/26], [94mLoss[0m : 2.24481
[1mStep[0m  [4/26], [94mLoss[0m : 2.31511
[1mStep[0m  [6/26], [94mLoss[0m : 2.38238
[1mStep[0m  [8/26], [94mLoss[0m : 2.18236
[1mStep[0m  [10/26], [94mLoss[0m : 2.33093
[1mStep[0m  [12/26], [94mLoss[0m : 2.17954
[1mStep[0m  [14/26], [94mLoss[0m : 2.16495
[1mStep[0m  [16/26], [94mLoss[0m : 2.23561
[1mStep[0m  [18/26], [94mLoss[0m : 2.17226
[1mStep[0m  [20/26], [94mLoss[0m : 2.32028
[1mStep[0m  [22/26], [94mLoss[0m : 2.29213
[1mStep[0m  [24/26], [94mLoss[0m : 2.23669

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.562, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26128
[1mStep[0m  [2/26], [94mLoss[0m : 2.37898
[1mStep[0m  [4/26], [94mLoss[0m : 2.34882
[1mStep[0m  [6/26], [94mLoss[0m : 2.19954
[1mStep[0m  [8/26], [94mLoss[0m : 2.33709
[1mStep[0m  [10/26], [94mLoss[0m : 2.36699
[1mStep[0m  [12/26], [94mLoss[0m : 2.30717
[1mStep[0m  [14/26], [94mLoss[0m : 2.16535
[1mStep[0m  [16/26], [94mLoss[0m : 2.28540
[1mStep[0m  [18/26], [94mLoss[0m : 2.30956
[1mStep[0m  [20/26], [94mLoss[0m : 2.21061
[1mStep[0m  [22/26], [94mLoss[0m : 2.20991
[1mStep[0m  [24/26], [94mLoss[0m : 2.34602

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.587, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.603
====================================

Phase 2 - Evaluation MAE:  2.6032171616187463
MAE score P1         2.44664
MAE score P2        2.603217
loss                2.261702
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.1
weight_decay           0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.95039
[1mStep[0m  [2/26], [94mLoss[0m : 10.93179
[1mStep[0m  [4/26], [94mLoss[0m : 10.54977
[1mStep[0m  [6/26], [94mLoss[0m : 10.28251
[1mStep[0m  [8/26], [94mLoss[0m : 9.94099
[1mStep[0m  [10/26], [94mLoss[0m : 9.93621
[1mStep[0m  [12/26], [94mLoss[0m : 9.37023
[1mStep[0m  [14/26], [94mLoss[0m : 9.38462
[1mStep[0m  [16/26], [94mLoss[0m : 9.01236
[1mStep[0m  [18/26], [94mLoss[0m : 8.74256
[1mStep[0m  [20/26], [94mLoss[0m : 8.73837
[1mStep[0m  [22/26], [94mLoss[0m : 8.37937
[1mStep[0m  [24/26], [94mLoss[0m : 8.35534

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.546, [92mTest[0m: 11.035, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.04752
[1mStep[0m  [2/26], [94mLoss[0m : 7.86171
[1mStep[0m  [4/26], [94mLoss[0m : 7.55890
[1mStep[0m  [6/26], [94mLoss[0m : 7.70270
[1mStep[0m  [8/26], [94mLoss[0m : 6.86062
[1mStep[0m  [10/26], [94mLoss[0m : 6.91408
[1mStep[0m  [12/26], [94mLoss[0m : 7.06939
[1mStep[0m  [14/26], [94mLoss[0m : 6.18009
[1mStep[0m  [16/26], [94mLoss[0m : 5.96820
[1mStep[0m  [18/26], [94mLoss[0m : 5.99711
[1mStep[0m  [20/26], [94mLoss[0m : 5.74662
[1mStep[0m  [22/26], [94mLoss[0m : 5.51746
[1mStep[0m  [24/26], [94mLoss[0m : 5.45573

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.622, [92mTest[0m: 8.031, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.86715
[1mStep[0m  [2/26], [94mLoss[0m : 4.98538
[1mStep[0m  [4/26], [94mLoss[0m : 4.81656
[1mStep[0m  [6/26], [94mLoss[0m : 4.73287
[1mStep[0m  [8/26], [94mLoss[0m : 4.51843
[1mStep[0m  [10/26], [94mLoss[0m : 4.52096
[1mStep[0m  [12/26], [94mLoss[0m : 4.20543
[1mStep[0m  [14/26], [94mLoss[0m : 4.14651
[1mStep[0m  [16/26], [94mLoss[0m : 4.36000
[1mStep[0m  [18/26], [94mLoss[0m : 3.87870
[1mStep[0m  [20/26], [94mLoss[0m : 3.58793
[1mStep[0m  [22/26], [94mLoss[0m : 3.47946
[1mStep[0m  [24/26], [94mLoss[0m : 3.59934

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.265, [92mTest[0m: 5.124, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.47888
[1mStep[0m  [2/26], [94mLoss[0m : 3.49135
[1mStep[0m  [4/26], [94mLoss[0m : 3.47097
[1mStep[0m  [6/26], [94mLoss[0m : 3.48169
[1mStep[0m  [8/26], [94mLoss[0m : 3.39046
[1mStep[0m  [10/26], [94mLoss[0m : 3.25276
[1mStep[0m  [12/26], [94mLoss[0m : 3.09594
[1mStep[0m  [14/26], [94mLoss[0m : 3.09906
[1mStep[0m  [16/26], [94mLoss[0m : 3.04258
[1mStep[0m  [18/26], [94mLoss[0m : 2.91094
[1mStep[0m  [20/26], [94mLoss[0m : 3.06017
[1mStep[0m  [22/26], [94mLoss[0m : 3.15819
[1mStep[0m  [24/26], [94mLoss[0m : 2.79074

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.201, [92mTest[0m: 3.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.86120
[1mStep[0m  [2/26], [94mLoss[0m : 2.98376
[1mStep[0m  [4/26], [94mLoss[0m : 2.76118
[1mStep[0m  [6/26], [94mLoss[0m : 2.80770
[1mStep[0m  [8/26], [94mLoss[0m : 2.73861
[1mStep[0m  [10/26], [94mLoss[0m : 2.87373
[1mStep[0m  [12/26], [94mLoss[0m : 2.88105
[1mStep[0m  [14/26], [94mLoss[0m : 2.90672
[1mStep[0m  [16/26], [94mLoss[0m : 2.92902
[1mStep[0m  [18/26], [94mLoss[0m : 2.64903
[1mStep[0m  [20/26], [94mLoss[0m : 2.68709
[1mStep[0m  [22/26], [94mLoss[0m : 2.74012
[1mStep[0m  [24/26], [94mLoss[0m : 2.76864

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.752, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71133
[1mStep[0m  [2/26], [94mLoss[0m : 2.72991
[1mStep[0m  [4/26], [94mLoss[0m : 2.50394
[1mStep[0m  [6/26], [94mLoss[0m : 2.72479
[1mStep[0m  [8/26], [94mLoss[0m : 2.75606
[1mStep[0m  [10/26], [94mLoss[0m : 2.77524
[1mStep[0m  [12/26], [94mLoss[0m : 2.67512
[1mStep[0m  [14/26], [94mLoss[0m : 2.71128
[1mStep[0m  [16/26], [94mLoss[0m : 2.65519
[1mStep[0m  [18/26], [94mLoss[0m : 2.74531
[1mStep[0m  [20/26], [94mLoss[0m : 2.62299
[1mStep[0m  [22/26], [94mLoss[0m : 2.67325
[1mStep[0m  [24/26], [94mLoss[0m : 2.69621

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72357
[1mStep[0m  [2/26], [94mLoss[0m : 2.69784
[1mStep[0m  [4/26], [94mLoss[0m : 2.75116
[1mStep[0m  [6/26], [94mLoss[0m : 2.64172
[1mStep[0m  [8/26], [94mLoss[0m : 2.77235
[1mStep[0m  [10/26], [94mLoss[0m : 2.74601
[1mStep[0m  [12/26], [94mLoss[0m : 2.55583
[1mStep[0m  [14/26], [94mLoss[0m : 2.60772
[1mStep[0m  [16/26], [94mLoss[0m : 2.59846
[1mStep[0m  [18/26], [94mLoss[0m : 2.74183
[1mStep[0m  [20/26], [94mLoss[0m : 2.52214
[1mStep[0m  [22/26], [94mLoss[0m : 2.66982
[1mStep[0m  [24/26], [94mLoss[0m : 2.55280

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60790
[1mStep[0m  [2/26], [94mLoss[0m : 2.54511
[1mStep[0m  [4/26], [94mLoss[0m : 2.61512
[1mStep[0m  [6/26], [94mLoss[0m : 2.61546
[1mStep[0m  [8/26], [94mLoss[0m : 2.65758
[1mStep[0m  [10/26], [94mLoss[0m : 2.75248
[1mStep[0m  [12/26], [94mLoss[0m : 2.52968
[1mStep[0m  [14/26], [94mLoss[0m : 2.62517
[1mStep[0m  [16/26], [94mLoss[0m : 2.63123
[1mStep[0m  [18/26], [94mLoss[0m : 2.62844
[1mStep[0m  [20/26], [94mLoss[0m : 2.51100
[1mStep[0m  [22/26], [94mLoss[0m : 2.64449
[1mStep[0m  [24/26], [94mLoss[0m : 2.52217

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60452
[1mStep[0m  [2/26], [94mLoss[0m : 2.68192
[1mStep[0m  [4/26], [94mLoss[0m : 2.64723
[1mStep[0m  [6/26], [94mLoss[0m : 2.60368
[1mStep[0m  [8/26], [94mLoss[0m : 2.57476
[1mStep[0m  [10/26], [94mLoss[0m : 2.74124
[1mStep[0m  [12/26], [94mLoss[0m : 2.65560
[1mStep[0m  [14/26], [94mLoss[0m : 2.71498
[1mStep[0m  [16/26], [94mLoss[0m : 2.61130
[1mStep[0m  [18/26], [94mLoss[0m : 2.51243
[1mStep[0m  [20/26], [94mLoss[0m : 2.63331
[1mStep[0m  [22/26], [94mLoss[0m : 2.76794
[1mStep[0m  [24/26], [94mLoss[0m : 2.62540

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57980
[1mStep[0m  [2/26], [94mLoss[0m : 2.58104
[1mStep[0m  [4/26], [94mLoss[0m : 2.46499
[1mStep[0m  [6/26], [94mLoss[0m : 2.54086
[1mStep[0m  [8/26], [94mLoss[0m : 2.46437
[1mStep[0m  [10/26], [94mLoss[0m : 2.72445
[1mStep[0m  [12/26], [94mLoss[0m : 2.61079
[1mStep[0m  [14/26], [94mLoss[0m : 2.52047
[1mStep[0m  [16/26], [94mLoss[0m : 2.68085
[1mStep[0m  [18/26], [94mLoss[0m : 2.68409
[1mStep[0m  [20/26], [94mLoss[0m : 2.59580
[1mStep[0m  [22/26], [94mLoss[0m : 2.73871
[1mStep[0m  [24/26], [94mLoss[0m : 2.56074

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61866
[1mStep[0m  [2/26], [94mLoss[0m : 2.74001
[1mStep[0m  [4/26], [94mLoss[0m : 2.65444
[1mStep[0m  [6/26], [94mLoss[0m : 2.66169
[1mStep[0m  [8/26], [94mLoss[0m : 2.61038
[1mStep[0m  [10/26], [94mLoss[0m : 2.48413
[1mStep[0m  [12/26], [94mLoss[0m : 2.77821
[1mStep[0m  [14/26], [94mLoss[0m : 2.65214
[1mStep[0m  [16/26], [94mLoss[0m : 2.45356
[1mStep[0m  [18/26], [94mLoss[0m : 2.60191
[1mStep[0m  [20/26], [94mLoss[0m : 2.54231
[1mStep[0m  [22/26], [94mLoss[0m : 2.63637
[1mStep[0m  [24/26], [94mLoss[0m : 2.61132

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51144
[1mStep[0m  [2/26], [94mLoss[0m : 2.52109
[1mStep[0m  [4/26], [94mLoss[0m : 2.71725
[1mStep[0m  [6/26], [94mLoss[0m : 2.57563
[1mStep[0m  [8/26], [94mLoss[0m : 2.64693
[1mStep[0m  [10/26], [94mLoss[0m : 2.51317
[1mStep[0m  [12/26], [94mLoss[0m : 2.63811
[1mStep[0m  [14/26], [94mLoss[0m : 2.60888
[1mStep[0m  [16/26], [94mLoss[0m : 2.48389
[1mStep[0m  [18/26], [94mLoss[0m : 2.59884
[1mStep[0m  [20/26], [94mLoss[0m : 2.53195
[1mStep[0m  [22/26], [94mLoss[0m : 2.56076
[1mStep[0m  [24/26], [94mLoss[0m : 2.48338

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55529
[1mStep[0m  [2/26], [94mLoss[0m : 2.38020
[1mStep[0m  [4/26], [94mLoss[0m : 2.58163
[1mStep[0m  [6/26], [94mLoss[0m : 2.61973
[1mStep[0m  [8/26], [94mLoss[0m : 2.55147
[1mStep[0m  [10/26], [94mLoss[0m : 2.56686
[1mStep[0m  [12/26], [94mLoss[0m : 2.67923
[1mStep[0m  [14/26], [94mLoss[0m : 2.67535
[1mStep[0m  [16/26], [94mLoss[0m : 2.58378
[1mStep[0m  [18/26], [94mLoss[0m : 2.77081
[1mStep[0m  [20/26], [94mLoss[0m : 2.69231
[1mStep[0m  [22/26], [94mLoss[0m : 2.72384
[1mStep[0m  [24/26], [94mLoss[0m : 2.58275

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53405
[1mStep[0m  [2/26], [94mLoss[0m : 2.53659
[1mStep[0m  [4/26], [94mLoss[0m : 2.47784
[1mStep[0m  [6/26], [94mLoss[0m : 2.61066
[1mStep[0m  [8/26], [94mLoss[0m : 2.70404
[1mStep[0m  [10/26], [94mLoss[0m : 2.69467
[1mStep[0m  [12/26], [94mLoss[0m : 2.53192
[1mStep[0m  [14/26], [94mLoss[0m : 2.56181
[1mStep[0m  [16/26], [94mLoss[0m : 2.58907
[1mStep[0m  [18/26], [94mLoss[0m : 2.58586
[1mStep[0m  [20/26], [94mLoss[0m : 2.60681
[1mStep[0m  [22/26], [94mLoss[0m : 2.66298
[1mStep[0m  [24/26], [94mLoss[0m : 2.61031

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65776
[1mStep[0m  [2/26], [94mLoss[0m : 2.58450
[1mStep[0m  [4/26], [94mLoss[0m : 2.65611
[1mStep[0m  [6/26], [94mLoss[0m : 2.61170
[1mStep[0m  [8/26], [94mLoss[0m : 2.56691
[1mStep[0m  [10/26], [94mLoss[0m : 2.62510
[1mStep[0m  [12/26], [94mLoss[0m : 2.66413
[1mStep[0m  [14/26], [94mLoss[0m : 2.67285
[1mStep[0m  [16/26], [94mLoss[0m : 2.50399
[1mStep[0m  [18/26], [94mLoss[0m : 2.54421
[1mStep[0m  [20/26], [94mLoss[0m : 2.43544
[1mStep[0m  [22/26], [94mLoss[0m : 2.53199
[1mStep[0m  [24/26], [94mLoss[0m : 2.43738

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56069
[1mStep[0m  [2/26], [94mLoss[0m : 2.55122
[1mStep[0m  [4/26], [94mLoss[0m : 2.57276
[1mStep[0m  [6/26], [94mLoss[0m : 2.61942
[1mStep[0m  [8/26], [94mLoss[0m : 2.55934
[1mStep[0m  [10/26], [94mLoss[0m : 2.62491
[1mStep[0m  [12/26], [94mLoss[0m : 2.45786
[1mStep[0m  [14/26], [94mLoss[0m : 2.67751
[1mStep[0m  [16/26], [94mLoss[0m : 2.70579
[1mStep[0m  [18/26], [94mLoss[0m : 2.58508
[1mStep[0m  [20/26], [94mLoss[0m : 2.47907
[1mStep[0m  [22/26], [94mLoss[0m : 2.64350
[1mStep[0m  [24/26], [94mLoss[0m : 2.61310

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53040
[1mStep[0m  [2/26], [94mLoss[0m : 2.71826
[1mStep[0m  [4/26], [94mLoss[0m : 2.46085
[1mStep[0m  [6/26], [94mLoss[0m : 2.61464
[1mStep[0m  [8/26], [94mLoss[0m : 2.52384
[1mStep[0m  [10/26], [94mLoss[0m : 2.56206
[1mStep[0m  [12/26], [94mLoss[0m : 2.42063
[1mStep[0m  [14/26], [94mLoss[0m : 2.50961
[1mStep[0m  [16/26], [94mLoss[0m : 2.53243
[1mStep[0m  [18/26], [94mLoss[0m : 2.52675
[1mStep[0m  [20/26], [94mLoss[0m : 2.63667
[1mStep[0m  [22/26], [94mLoss[0m : 2.50255
[1mStep[0m  [24/26], [94mLoss[0m : 2.66049

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49727
[1mStep[0m  [2/26], [94mLoss[0m : 2.56775
[1mStep[0m  [4/26], [94mLoss[0m : 2.65358
[1mStep[0m  [6/26], [94mLoss[0m : 2.57606
[1mStep[0m  [8/26], [94mLoss[0m : 2.50018
[1mStep[0m  [10/26], [94mLoss[0m : 2.61607
[1mStep[0m  [12/26], [94mLoss[0m : 2.68329
[1mStep[0m  [14/26], [94mLoss[0m : 2.71442
[1mStep[0m  [16/26], [94mLoss[0m : 2.48526
[1mStep[0m  [18/26], [94mLoss[0m : 2.63515
[1mStep[0m  [20/26], [94mLoss[0m : 2.67476
[1mStep[0m  [22/26], [94mLoss[0m : 2.64674
[1mStep[0m  [24/26], [94mLoss[0m : 2.61573

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51759
[1mStep[0m  [2/26], [94mLoss[0m : 2.65158
[1mStep[0m  [4/26], [94mLoss[0m : 2.56049
[1mStep[0m  [6/26], [94mLoss[0m : 2.54874
[1mStep[0m  [8/26], [94mLoss[0m : 2.57278
[1mStep[0m  [10/26], [94mLoss[0m : 2.57854
[1mStep[0m  [12/26], [94mLoss[0m : 2.62345
[1mStep[0m  [14/26], [94mLoss[0m : 2.58850
[1mStep[0m  [16/26], [94mLoss[0m : 2.52494
[1mStep[0m  [18/26], [94mLoss[0m : 2.57203
[1mStep[0m  [20/26], [94mLoss[0m : 2.71216
[1mStep[0m  [22/26], [94mLoss[0m : 2.58951
[1mStep[0m  [24/26], [94mLoss[0m : 2.52431

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52404
[1mStep[0m  [2/26], [94mLoss[0m : 2.45300
[1mStep[0m  [4/26], [94mLoss[0m : 2.63033
[1mStep[0m  [6/26], [94mLoss[0m : 2.66413
[1mStep[0m  [8/26], [94mLoss[0m : 2.51786
[1mStep[0m  [10/26], [94mLoss[0m : 2.63402
[1mStep[0m  [12/26], [94mLoss[0m : 2.45554
[1mStep[0m  [14/26], [94mLoss[0m : 2.63109
[1mStep[0m  [16/26], [94mLoss[0m : 2.58421
[1mStep[0m  [18/26], [94mLoss[0m : 2.68594
[1mStep[0m  [20/26], [94mLoss[0m : 2.66241
[1mStep[0m  [22/26], [94mLoss[0m : 2.47624
[1mStep[0m  [24/26], [94mLoss[0m : 2.63788

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.439, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59674
[1mStep[0m  [2/26], [94mLoss[0m : 2.52910
[1mStep[0m  [4/26], [94mLoss[0m : 2.40555
[1mStep[0m  [6/26], [94mLoss[0m : 2.52178
[1mStep[0m  [8/26], [94mLoss[0m : 2.58915
[1mStep[0m  [10/26], [94mLoss[0m : 2.41044
[1mStep[0m  [12/26], [94mLoss[0m : 2.56367
[1mStep[0m  [14/26], [94mLoss[0m : 2.62231
[1mStep[0m  [16/26], [94mLoss[0m : 2.65676
[1mStep[0m  [18/26], [94mLoss[0m : 2.50616
[1mStep[0m  [20/26], [94mLoss[0m : 2.75882
[1mStep[0m  [22/26], [94mLoss[0m : 2.67340
[1mStep[0m  [24/26], [94mLoss[0m : 2.54111

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.440, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56935
[1mStep[0m  [2/26], [94mLoss[0m : 2.56404
[1mStep[0m  [4/26], [94mLoss[0m : 2.65998
[1mStep[0m  [6/26], [94mLoss[0m : 2.48426
[1mStep[0m  [8/26], [94mLoss[0m : 2.55417
[1mStep[0m  [10/26], [94mLoss[0m : 2.58428
[1mStep[0m  [12/26], [94mLoss[0m : 2.61822
[1mStep[0m  [14/26], [94mLoss[0m : 2.44486
[1mStep[0m  [16/26], [94mLoss[0m : 2.51329
[1mStep[0m  [18/26], [94mLoss[0m : 2.40169
[1mStep[0m  [20/26], [94mLoss[0m : 2.59084
[1mStep[0m  [22/26], [94mLoss[0m : 2.65674
[1mStep[0m  [24/26], [94mLoss[0m : 2.37899

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.442, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58024
[1mStep[0m  [2/26], [94mLoss[0m : 2.70651
[1mStep[0m  [4/26], [94mLoss[0m : 2.45117
[1mStep[0m  [6/26], [94mLoss[0m : 2.48626
[1mStep[0m  [8/26], [94mLoss[0m : 2.53833
[1mStep[0m  [10/26], [94mLoss[0m : 2.61847
[1mStep[0m  [12/26], [94mLoss[0m : 2.59014
[1mStep[0m  [14/26], [94mLoss[0m : 2.59444
[1mStep[0m  [16/26], [94mLoss[0m : 2.73366
[1mStep[0m  [18/26], [94mLoss[0m : 2.48727
[1mStep[0m  [20/26], [94mLoss[0m : 2.47589
[1mStep[0m  [22/26], [94mLoss[0m : 2.60842
[1mStep[0m  [24/26], [94mLoss[0m : 2.60989

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.434, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54066
[1mStep[0m  [2/26], [94mLoss[0m : 2.74362
[1mStep[0m  [4/26], [94mLoss[0m : 2.40429
[1mStep[0m  [6/26], [94mLoss[0m : 2.58244
[1mStep[0m  [8/26], [94mLoss[0m : 2.58355
[1mStep[0m  [10/26], [94mLoss[0m : 2.57878
[1mStep[0m  [12/26], [94mLoss[0m : 2.52136
[1mStep[0m  [14/26], [94mLoss[0m : 2.52454
[1mStep[0m  [16/26], [94mLoss[0m : 2.64039
[1mStep[0m  [18/26], [94mLoss[0m : 2.60044
[1mStep[0m  [20/26], [94mLoss[0m : 2.64523
[1mStep[0m  [22/26], [94mLoss[0m : 2.43091
[1mStep[0m  [24/26], [94mLoss[0m : 2.45293

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63261
[1mStep[0m  [2/26], [94mLoss[0m : 2.42757
[1mStep[0m  [4/26], [94mLoss[0m : 2.64604
[1mStep[0m  [6/26], [94mLoss[0m : 2.47270
[1mStep[0m  [8/26], [94mLoss[0m : 2.45217
[1mStep[0m  [10/26], [94mLoss[0m : 2.64160
[1mStep[0m  [12/26], [94mLoss[0m : 2.53190
[1mStep[0m  [14/26], [94mLoss[0m : 2.71866
[1mStep[0m  [16/26], [94mLoss[0m : 2.66710
[1mStep[0m  [18/26], [94mLoss[0m : 2.55134
[1mStep[0m  [20/26], [94mLoss[0m : 2.48176
[1mStep[0m  [22/26], [94mLoss[0m : 2.63469
[1mStep[0m  [24/26], [94mLoss[0m : 2.52265

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.425, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56845
[1mStep[0m  [2/26], [94mLoss[0m : 2.68363
[1mStep[0m  [4/26], [94mLoss[0m : 2.51568
[1mStep[0m  [6/26], [94mLoss[0m : 2.68302
[1mStep[0m  [8/26], [94mLoss[0m : 2.43219
[1mStep[0m  [10/26], [94mLoss[0m : 2.62929
[1mStep[0m  [12/26], [94mLoss[0m : 2.47756
[1mStep[0m  [14/26], [94mLoss[0m : 2.42265
[1mStep[0m  [16/26], [94mLoss[0m : 2.55883
[1mStep[0m  [18/26], [94mLoss[0m : 2.45918
[1mStep[0m  [20/26], [94mLoss[0m : 2.53889
[1mStep[0m  [22/26], [94mLoss[0m : 2.57037
[1mStep[0m  [24/26], [94mLoss[0m : 2.58206

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.426, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58258
[1mStep[0m  [2/26], [94mLoss[0m : 2.55916
[1mStep[0m  [4/26], [94mLoss[0m : 2.49292
[1mStep[0m  [6/26], [94mLoss[0m : 2.64426
[1mStep[0m  [8/26], [94mLoss[0m : 2.49733
[1mStep[0m  [10/26], [94mLoss[0m : 2.65928
[1mStep[0m  [12/26], [94mLoss[0m : 2.60536
[1mStep[0m  [14/26], [94mLoss[0m : 2.57470
[1mStep[0m  [16/26], [94mLoss[0m : 2.70543
[1mStep[0m  [18/26], [94mLoss[0m : 2.63557
[1mStep[0m  [20/26], [94mLoss[0m : 2.78463
[1mStep[0m  [22/26], [94mLoss[0m : 2.55144
[1mStep[0m  [24/26], [94mLoss[0m : 2.59585

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.425, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65812
[1mStep[0m  [2/26], [94mLoss[0m : 2.62057
[1mStep[0m  [4/26], [94mLoss[0m : 2.47355
[1mStep[0m  [6/26], [94mLoss[0m : 2.44681
[1mStep[0m  [8/26], [94mLoss[0m : 2.43474
[1mStep[0m  [10/26], [94mLoss[0m : 2.45752
[1mStep[0m  [12/26], [94mLoss[0m : 2.55677
[1mStep[0m  [14/26], [94mLoss[0m : 2.58699
[1mStep[0m  [16/26], [94mLoss[0m : 2.49834
[1mStep[0m  [18/26], [94mLoss[0m : 2.48928
[1mStep[0m  [20/26], [94mLoss[0m : 2.55242
[1mStep[0m  [22/26], [94mLoss[0m : 2.47942
[1mStep[0m  [24/26], [94mLoss[0m : 2.69204

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.426, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.75535
[1mStep[0m  [2/26], [94mLoss[0m : 2.54175
[1mStep[0m  [4/26], [94mLoss[0m : 2.47604
[1mStep[0m  [6/26], [94mLoss[0m : 2.40079
[1mStep[0m  [8/26], [94mLoss[0m : 2.63367
[1mStep[0m  [10/26], [94mLoss[0m : 2.47112
[1mStep[0m  [12/26], [94mLoss[0m : 2.38762
[1mStep[0m  [14/26], [94mLoss[0m : 2.46929
[1mStep[0m  [16/26], [94mLoss[0m : 2.52557
[1mStep[0m  [18/26], [94mLoss[0m : 2.51459
[1mStep[0m  [20/26], [94mLoss[0m : 2.47972
[1mStep[0m  [22/26], [94mLoss[0m : 2.60952
[1mStep[0m  [24/26], [94mLoss[0m : 2.72870

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64130
[1mStep[0m  [2/26], [94mLoss[0m : 2.53184
[1mStep[0m  [4/26], [94mLoss[0m : 2.51454
[1mStep[0m  [6/26], [94mLoss[0m : 2.43425
[1mStep[0m  [8/26], [94mLoss[0m : 2.64702
[1mStep[0m  [10/26], [94mLoss[0m : 2.67067
[1mStep[0m  [12/26], [94mLoss[0m : 2.59656
[1mStep[0m  [14/26], [94mLoss[0m : 2.53410
[1mStep[0m  [16/26], [94mLoss[0m : 2.49486
[1mStep[0m  [18/26], [94mLoss[0m : 2.46037
[1mStep[0m  [20/26], [94mLoss[0m : 2.58661
[1mStep[0m  [22/26], [94mLoss[0m : 2.51690
[1mStep[0m  [24/26], [94mLoss[0m : 2.59994

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.428, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.424
====================================

Phase 1 - Evaluation MAE:  2.4236756288088284
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.59795
[1mStep[0m  [2/26], [94mLoss[0m : 2.60434
[1mStep[0m  [4/26], [94mLoss[0m : 2.54574
[1mStep[0m  [6/26], [94mLoss[0m : 2.58714
[1mStep[0m  [8/26], [94mLoss[0m : 2.58449
[1mStep[0m  [10/26], [94mLoss[0m : 2.51773
[1mStep[0m  [12/26], [94mLoss[0m : 2.57125
[1mStep[0m  [14/26], [94mLoss[0m : 2.41053
[1mStep[0m  [16/26], [94mLoss[0m : 2.66059
[1mStep[0m  [18/26], [94mLoss[0m : 2.48157
[1mStep[0m  [20/26], [94mLoss[0m : 2.48273
[1mStep[0m  [22/26], [94mLoss[0m : 2.60331
[1mStep[0m  [24/26], [94mLoss[0m : 2.62573

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64830
[1mStep[0m  [2/26], [94mLoss[0m : 2.57122
[1mStep[0m  [4/26], [94mLoss[0m : 2.49061
[1mStep[0m  [6/26], [94mLoss[0m : 2.46072
[1mStep[0m  [8/26], [94mLoss[0m : 2.53337
[1mStep[0m  [10/26], [94mLoss[0m : 2.52523
[1mStep[0m  [12/26], [94mLoss[0m : 2.44117
[1mStep[0m  [14/26], [94mLoss[0m : 2.58347
[1mStep[0m  [16/26], [94mLoss[0m : 2.53011
[1mStep[0m  [18/26], [94mLoss[0m : 2.69401
[1mStep[0m  [20/26], [94mLoss[0m : 2.66329
[1mStep[0m  [22/26], [94mLoss[0m : 2.60816
[1mStep[0m  [24/26], [94mLoss[0m : 2.47540

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53226
[1mStep[0m  [2/26], [94mLoss[0m : 2.51583
[1mStep[0m  [4/26], [94mLoss[0m : 2.45940
[1mStep[0m  [6/26], [94mLoss[0m : 2.59935
[1mStep[0m  [8/26], [94mLoss[0m : 2.48218
[1mStep[0m  [10/26], [94mLoss[0m : 2.56060
[1mStep[0m  [12/26], [94mLoss[0m : 2.46923
[1mStep[0m  [14/26], [94mLoss[0m : 2.59974
[1mStep[0m  [16/26], [94mLoss[0m : 2.49415
[1mStep[0m  [18/26], [94mLoss[0m : 2.59402
[1mStep[0m  [20/26], [94mLoss[0m : 2.67026
[1mStep[0m  [22/26], [94mLoss[0m : 2.62503
[1mStep[0m  [24/26], [94mLoss[0m : 2.48160

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61915
[1mStep[0m  [2/26], [94mLoss[0m : 2.47306
[1mStep[0m  [4/26], [94mLoss[0m : 2.59035
[1mStep[0m  [6/26], [94mLoss[0m : 2.52044
[1mStep[0m  [8/26], [94mLoss[0m : 2.49416
[1mStep[0m  [10/26], [94mLoss[0m : 2.61859
[1mStep[0m  [12/26], [94mLoss[0m : 2.54992
[1mStep[0m  [14/26], [94mLoss[0m : 2.66194
[1mStep[0m  [16/26], [94mLoss[0m : 2.47614
[1mStep[0m  [18/26], [94mLoss[0m : 2.46674
[1mStep[0m  [20/26], [94mLoss[0m : 2.49829
[1mStep[0m  [22/26], [94mLoss[0m : 2.79979
[1mStep[0m  [24/26], [94mLoss[0m : 2.67830

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40063
[1mStep[0m  [2/26], [94mLoss[0m : 2.61294
[1mStep[0m  [4/26], [94mLoss[0m : 2.44580
[1mStep[0m  [6/26], [94mLoss[0m : 2.56972
[1mStep[0m  [8/26], [94mLoss[0m : 2.43680
[1mStep[0m  [10/26], [94mLoss[0m : 2.58832
[1mStep[0m  [12/26], [94mLoss[0m : 2.61841
[1mStep[0m  [14/26], [94mLoss[0m : 2.58433
[1mStep[0m  [16/26], [94mLoss[0m : 2.56525
[1mStep[0m  [18/26], [94mLoss[0m : 2.57934
[1mStep[0m  [20/26], [94mLoss[0m : 2.57421
[1mStep[0m  [22/26], [94mLoss[0m : 2.50617
[1mStep[0m  [24/26], [94mLoss[0m : 2.50055

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45649
[1mStep[0m  [2/26], [94mLoss[0m : 2.59959
[1mStep[0m  [4/26], [94mLoss[0m : 2.54051
[1mStep[0m  [6/26], [94mLoss[0m : 2.65620
[1mStep[0m  [8/26], [94mLoss[0m : 2.72650
[1mStep[0m  [10/26], [94mLoss[0m : 2.57125
[1mStep[0m  [12/26], [94mLoss[0m : 2.48291
[1mStep[0m  [14/26], [94mLoss[0m : 2.46548
[1mStep[0m  [16/26], [94mLoss[0m : 2.58374
[1mStep[0m  [18/26], [94mLoss[0m : 2.42521
[1mStep[0m  [20/26], [94mLoss[0m : 2.53284
[1mStep[0m  [22/26], [94mLoss[0m : 2.43164
[1mStep[0m  [24/26], [94mLoss[0m : 2.38779

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46028
[1mStep[0m  [2/26], [94mLoss[0m : 2.43918
[1mStep[0m  [4/26], [94mLoss[0m : 2.54985
[1mStep[0m  [6/26], [94mLoss[0m : 2.53787
[1mStep[0m  [8/26], [94mLoss[0m : 2.65514
[1mStep[0m  [10/26], [94mLoss[0m : 2.62770
[1mStep[0m  [12/26], [94mLoss[0m : 2.40688
[1mStep[0m  [14/26], [94mLoss[0m : 2.57427
[1mStep[0m  [16/26], [94mLoss[0m : 2.53585
[1mStep[0m  [18/26], [94mLoss[0m : 2.51457
[1mStep[0m  [20/26], [94mLoss[0m : 2.54811
[1mStep[0m  [22/26], [94mLoss[0m : 2.45623
[1mStep[0m  [24/26], [94mLoss[0m : 2.39590

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55203
[1mStep[0m  [2/26], [94mLoss[0m : 2.60150
[1mStep[0m  [4/26], [94mLoss[0m : 2.59876
[1mStep[0m  [6/26], [94mLoss[0m : 2.50568
[1mStep[0m  [8/26], [94mLoss[0m : 2.34908
[1mStep[0m  [10/26], [94mLoss[0m : 2.51070
[1mStep[0m  [12/26], [94mLoss[0m : 2.49979
[1mStep[0m  [14/26], [94mLoss[0m : 2.55106
[1mStep[0m  [16/26], [94mLoss[0m : 2.66120
[1mStep[0m  [18/26], [94mLoss[0m : 2.61977
[1mStep[0m  [20/26], [94mLoss[0m : 2.45473
[1mStep[0m  [22/26], [94mLoss[0m : 2.49723
[1mStep[0m  [24/26], [94mLoss[0m : 2.46172

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55567
[1mStep[0m  [2/26], [94mLoss[0m : 2.52505
[1mStep[0m  [4/26], [94mLoss[0m : 2.75989
[1mStep[0m  [6/26], [94mLoss[0m : 2.50332
[1mStep[0m  [8/26], [94mLoss[0m : 2.49250
[1mStep[0m  [10/26], [94mLoss[0m : 2.53012
[1mStep[0m  [12/26], [94mLoss[0m : 2.39669
[1mStep[0m  [14/26], [94mLoss[0m : 2.38851
[1mStep[0m  [16/26], [94mLoss[0m : 2.46489
[1mStep[0m  [18/26], [94mLoss[0m : 2.46749
[1mStep[0m  [20/26], [94mLoss[0m : 2.38341
[1mStep[0m  [22/26], [94mLoss[0m : 2.53582
[1mStep[0m  [24/26], [94mLoss[0m : 2.53837

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63421
[1mStep[0m  [2/26], [94mLoss[0m : 2.47524
[1mStep[0m  [4/26], [94mLoss[0m : 2.53158
[1mStep[0m  [6/26], [94mLoss[0m : 2.58138
[1mStep[0m  [8/26], [94mLoss[0m : 2.38887
[1mStep[0m  [10/26], [94mLoss[0m : 2.43868
[1mStep[0m  [12/26], [94mLoss[0m : 2.42042
[1mStep[0m  [14/26], [94mLoss[0m : 2.40940
[1mStep[0m  [16/26], [94mLoss[0m : 2.54518
[1mStep[0m  [18/26], [94mLoss[0m : 2.49296
[1mStep[0m  [20/26], [94mLoss[0m : 2.48812
[1mStep[0m  [22/26], [94mLoss[0m : 2.37788
[1mStep[0m  [24/26], [94mLoss[0m : 2.58860

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50098
[1mStep[0m  [2/26], [94mLoss[0m : 2.56222
[1mStep[0m  [4/26], [94mLoss[0m : 2.44303
[1mStep[0m  [6/26], [94mLoss[0m : 2.45561
[1mStep[0m  [8/26], [94mLoss[0m : 2.35939
[1mStep[0m  [10/26], [94mLoss[0m : 2.35395
[1mStep[0m  [12/26], [94mLoss[0m : 2.35227
[1mStep[0m  [14/26], [94mLoss[0m : 2.59770
[1mStep[0m  [16/26], [94mLoss[0m : 2.32205
[1mStep[0m  [18/26], [94mLoss[0m : 2.49284
[1mStep[0m  [20/26], [94mLoss[0m : 2.39853
[1mStep[0m  [22/26], [94mLoss[0m : 2.43031
[1mStep[0m  [24/26], [94mLoss[0m : 2.32176

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40386
[1mStep[0m  [2/26], [94mLoss[0m : 2.36738
[1mStep[0m  [4/26], [94mLoss[0m : 2.51057
[1mStep[0m  [6/26], [94mLoss[0m : 2.47529
[1mStep[0m  [8/26], [94mLoss[0m : 2.47838
[1mStep[0m  [10/26], [94mLoss[0m : 2.39177
[1mStep[0m  [12/26], [94mLoss[0m : 2.50200
[1mStep[0m  [14/26], [94mLoss[0m : 2.44938
[1mStep[0m  [16/26], [94mLoss[0m : 2.51010
[1mStep[0m  [18/26], [94mLoss[0m : 2.59167
[1mStep[0m  [20/26], [94mLoss[0m : 2.42302
[1mStep[0m  [22/26], [94mLoss[0m : 2.45668
[1mStep[0m  [24/26], [94mLoss[0m : 2.49398

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.31173
[1mStep[0m  [2/26], [94mLoss[0m : 2.53835
[1mStep[0m  [4/26], [94mLoss[0m : 2.33122
[1mStep[0m  [6/26], [94mLoss[0m : 2.48336
[1mStep[0m  [8/26], [94mLoss[0m : 2.57213
[1mStep[0m  [10/26], [94mLoss[0m : 2.41534
[1mStep[0m  [12/26], [94mLoss[0m : 2.44819
[1mStep[0m  [14/26], [94mLoss[0m : 2.43299
[1mStep[0m  [16/26], [94mLoss[0m : 2.43005
[1mStep[0m  [18/26], [94mLoss[0m : 2.53288
[1mStep[0m  [20/26], [94mLoss[0m : 2.39965
[1mStep[0m  [22/26], [94mLoss[0m : 2.43208
[1mStep[0m  [24/26], [94mLoss[0m : 2.41930

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58639
[1mStep[0m  [2/26], [94mLoss[0m : 2.50037
[1mStep[0m  [4/26], [94mLoss[0m : 2.33064
[1mStep[0m  [6/26], [94mLoss[0m : 2.57367
[1mStep[0m  [8/26], [94mLoss[0m : 2.58219
[1mStep[0m  [10/26], [94mLoss[0m : 2.37832
[1mStep[0m  [12/26], [94mLoss[0m : 2.40811
[1mStep[0m  [14/26], [94mLoss[0m : 2.37902
[1mStep[0m  [16/26], [94mLoss[0m : 2.32711
[1mStep[0m  [18/26], [94mLoss[0m : 2.46835
[1mStep[0m  [20/26], [94mLoss[0m : 2.54047
[1mStep[0m  [22/26], [94mLoss[0m : 2.49171
[1mStep[0m  [24/26], [94mLoss[0m : 2.60522

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49163
[1mStep[0m  [2/26], [94mLoss[0m : 2.28161
[1mStep[0m  [4/26], [94mLoss[0m : 2.41798
[1mStep[0m  [6/26], [94mLoss[0m : 2.40330
[1mStep[0m  [8/26], [94mLoss[0m : 2.46542
[1mStep[0m  [10/26], [94mLoss[0m : 2.29093
[1mStep[0m  [12/26], [94mLoss[0m : 2.53178
[1mStep[0m  [14/26], [94mLoss[0m : 2.33231
[1mStep[0m  [16/26], [94mLoss[0m : 2.52037
[1mStep[0m  [18/26], [94mLoss[0m : 2.26170
[1mStep[0m  [20/26], [94mLoss[0m : 2.54518
[1mStep[0m  [22/26], [94mLoss[0m : 2.56537
[1mStep[0m  [24/26], [94mLoss[0m : 2.41683

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38357
[1mStep[0m  [2/26], [94mLoss[0m : 2.46263
[1mStep[0m  [4/26], [94mLoss[0m : 2.49829
[1mStep[0m  [6/26], [94mLoss[0m : 2.41492
[1mStep[0m  [8/26], [94mLoss[0m : 2.37515
[1mStep[0m  [10/26], [94mLoss[0m : 2.46353
[1mStep[0m  [12/26], [94mLoss[0m : 2.39863
[1mStep[0m  [14/26], [94mLoss[0m : 2.42546
[1mStep[0m  [16/26], [94mLoss[0m : 2.30774
[1mStep[0m  [18/26], [94mLoss[0m : 2.39696
[1mStep[0m  [20/26], [94mLoss[0m : 2.55659
[1mStep[0m  [22/26], [94mLoss[0m : 2.50283
[1mStep[0m  [24/26], [94mLoss[0m : 2.31107

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42344
[1mStep[0m  [2/26], [94mLoss[0m : 2.50718
[1mStep[0m  [4/26], [94mLoss[0m : 2.43862
[1mStep[0m  [6/26], [94mLoss[0m : 2.47725
[1mStep[0m  [8/26], [94mLoss[0m : 2.38170
[1mStep[0m  [10/26], [94mLoss[0m : 2.42844
[1mStep[0m  [12/26], [94mLoss[0m : 2.32033
[1mStep[0m  [14/26], [94mLoss[0m : 2.40476
[1mStep[0m  [16/26], [94mLoss[0m : 2.50661
[1mStep[0m  [18/26], [94mLoss[0m : 2.36454
[1mStep[0m  [20/26], [94mLoss[0m : 2.49504
[1mStep[0m  [22/26], [94mLoss[0m : 2.29515
[1mStep[0m  [24/26], [94mLoss[0m : 2.34914

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50323
[1mStep[0m  [2/26], [94mLoss[0m : 2.38389
[1mStep[0m  [4/26], [94mLoss[0m : 2.36832
[1mStep[0m  [6/26], [94mLoss[0m : 2.45971
[1mStep[0m  [8/26], [94mLoss[0m : 2.25384
[1mStep[0m  [10/26], [94mLoss[0m : 2.40309
[1mStep[0m  [12/26], [94mLoss[0m : 2.50343
[1mStep[0m  [14/26], [94mLoss[0m : 2.29358
[1mStep[0m  [16/26], [94mLoss[0m : 2.54396
[1mStep[0m  [18/26], [94mLoss[0m : 2.32127
[1mStep[0m  [20/26], [94mLoss[0m : 2.39803
[1mStep[0m  [22/26], [94mLoss[0m : 2.35906
[1mStep[0m  [24/26], [94mLoss[0m : 2.41436

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29642
[1mStep[0m  [2/26], [94mLoss[0m : 2.40260
[1mStep[0m  [4/26], [94mLoss[0m : 2.46245
[1mStep[0m  [6/26], [94mLoss[0m : 2.59366
[1mStep[0m  [8/26], [94mLoss[0m : 2.50068
[1mStep[0m  [10/26], [94mLoss[0m : 2.18575
[1mStep[0m  [12/26], [94mLoss[0m : 2.37832
[1mStep[0m  [14/26], [94mLoss[0m : 2.31803
[1mStep[0m  [16/26], [94mLoss[0m : 2.28608
[1mStep[0m  [18/26], [94mLoss[0m : 2.52447
[1mStep[0m  [20/26], [94mLoss[0m : 2.26308
[1mStep[0m  [22/26], [94mLoss[0m : 2.43687
[1mStep[0m  [24/26], [94mLoss[0m : 2.33482

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.15084
[1mStep[0m  [2/26], [94mLoss[0m : 2.38536
[1mStep[0m  [4/26], [94mLoss[0m : 2.35497
[1mStep[0m  [6/26], [94mLoss[0m : 2.54839
[1mStep[0m  [8/26], [94mLoss[0m : 2.40542
[1mStep[0m  [10/26], [94mLoss[0m : 2.25567
[1mStep[0m  [12/26], [94mLoss[0m : 2.50370
[1mStep[0m  [14/26], [94mLoss[0m : 2.28805
[1mStep[0m  [16/26], [94mLoss[0m : 2.36054
[1mStep[0m  [18/26], [94mLoss[0m : 2.35177
[1mStep[0m  [20/26], [94mLoss[0m : 2.34337
[1mStep[0m  [22/26], [94mLoss[0m : 2.52181
[1mStep[0m  [24/26], [94mLoss[0m : 2.30842

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33910
[1mStep[0m  [2/26], [94mLoss[0m : 2.36651
[1mStep[0m  [4/26], [94mLoss[0m : 2.25615
[1mStep[0m  [6/26], [94mLoss[0m : 2.40771
[1mStep[0m  [8/26], [94mLoss[0m : 2.27317
[1mStep[0m  [10/26], [94mLoss[0m : 2.32380
[1mStep[0m  [12/26], [94mLoss[0m : 2.38504
[1mStep[0m  [14/26], [94mLoss[0m : 2.14353
[1mStep[0m  [16/26], [94mLoss[0m : 2.26282
[1mStep[0m  [18/26], [94mLoss[0m : 2.41164
[1mStep[0m  [20/26], [94mLoss[0m : 2.36941
[1mStep[0m  [22/26], [94mLoss[0m : 2.30931
[1mStep[0m  [24/26], [94mLoss[0m : 2.30357

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42872
[1mStep[0m  [2/26], [94mLoss[0m : 2.37678
[1mStep[0m  [4/26], [94mLoss[0m : 2.34440
[1mStep[0m  [6/26], [94mLoss[0m : 2.29371
[1mStep[0m  [8/26], [94mLoss[0m : 2.33078
[1mStep[0m  [10/26], [94mLoss[0m : 2.29486
[1mStep[0m  [12/26], [94mLoss[0m : 2.39353
[1mStep[0m  [14/26], [94mLoss[0m : 2.29285
[1mStep[0m  [16/26], [94mLoss[0m : 2.38349
[1mStep[0m  [18/26], [94mLoss[0m : 2.16909
[1mStep[0m  [20/26], [94mLoss[0m : 2.29822
[1mStep[0m  [22/26], [94mLoss[0m : 2.38878
[1mStep[0m  [24/26], [94mLoss[0m : 2.33771

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.441, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23792
[1mStep[0m  [2/26], [94mLoss[0m : 2.28836
[1mStep[0m  [4/26], [94mLoss[0m : 2.28988
[1mStep[0m  [6/26], [94mLoss[0m : 2.37917
[1mStep[0m  [8/26], [94mLoss[0m : 2.29427
[1mStep[0m  [10/26], [94mLoss[0m : 2.47121
[1mStep[0m  [12/26], [94mLoss[0m : 2.29231
[1mStep[0m  [14/26], [94mLoss[0m : 2.19128
[1mStep[0m  [16/26], [94mLoss[0m : 2.22667
[1mStep[0m  [18/26], [94mLoss[0m : 2.22209
[1mStep[0m  [20/26], [94mLoss[0m : 2.35045
[1mStep[0m  [22/26], [94mLoss[0m : 2.21807
[1mStep[0m  [24/26], [94mLoss[0m : 2.21696

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.411, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45237
[1mStep[0m  [2/26], [94mLoss[0m : 2.42979
[1mStep[0m  [4/26], [94mLoss[0m : 2.26390
[1mStep[0m  [6/26], [94mLoss[0m : 2.40827
[1mStep[0m  [8/26], [94mLoss[0m : 2.38283
[1mStep[0m  [10/26], [94mLoss[0m : 2.24779
[1mStep[0m  [12/26], [94mLoss[0m : 2.27682
[1mStep[0m  [14/26], [94mLoss[0m : 2.28981
[1mStep[0m  [16/26], [94mLoss[0m : 2.31756
[1mStep[0m  [18/26], [94mLoss[0m : 2.21595
[1mStep[0m  [20/26], [94mLoss[0m : 2.24196
[1mStep[0m  [22/26], [94mLoss[0m : 2.41051
[1mStep[0m  [24/26], [94mLoss[0m : 2.35313

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.424, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23947
[1mStep[0m  [2/26], [94mLoss[0m : 2.18541
[1mStep[0m  [4/26], [94mLoss[0m : 2.31822
[1mStep[0m  [6/26], [94mLoss[0m : 2.16775
[1mStep[0m  [8/26], [94mLoss[0m : 2.32472
[1mStep[0m  [10/26], [94mLoss[0m : 2.30773
[1mStep[0m  [12/26], [94mLoss[0m : 2.33185
[1mStep[0m  [14/26], [94mLoss[0m : 2.35026
[1mStep[0m  [16/26], [94mLoss[0m : 2.25081
[1mStep[0m  [18/26], [94mLoss[0m : 2.30624
[1mStep[0m  [20/26], [94mLoss[0m : 2.34960
[1mStep[0m  [22/26], [94mLoss[0m : 2.18937
[1mStep[0m  [24/26], [94mLoss[0m : 2.34099

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23334
[1mStep[0m  [2/26], [94mLoss[0m : 2.14160
[1mStep[0m  [4/26], [94mLoss[0m : 2.25036
[1mStep[0m  [6/26], [94mLoss[0m : 2.24611
[1mStep[0m  [8/26], [94mLoss[0m : 2.19251
[1mStep[0m  [10/26], [94mLoss[0m : 2.31697
[1mStep[0m  [12/26], [94mLoss[0m : 2.39446
[1mStep[0m  [14/26], [94mLoss[0m : 2.25269
[1mStep[0m  [16/26], [94mLoss[0m : 2.30291
[1mStep[0m  [18/26], [94mLoss[0m : 2.16164
[1mStep[0m  [20/26], [94mLoss[0m : 2.31228
[1mStep[0m  [22/26], [94mLoss[0m : 2.29247
[1mStep[0m  [24/26], [94mLoss[0m : 2.39336

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.26725
[1mStep[0m  [2/26], [94mLoss[0m : 2.30202
[1mStep[0m  [4/26], [94mLoss[0m : 2.30717
[1mStep[0m  [6/26], [94mLoss[0m : 2.31311
[1mStep[0m  [8/26], [94mLoss[0m : 2.24557
[1mStep[0m  [10/26], [94mLoss[0m : 2.31305
[1mStep[0m  [12/26], [94mLoss[0m : 2.31900
[1mStep[0m  [14/26], [94mLoss[0m : 2.24612
[1mStep[0m  [16/26], [94mLoss[0m : 2.21839
[1mStep[0m  [18/26], [94mLoss[0m : 2.29134
[1mStep[0m  [20/26], [94mLoss[0m : 2.33715
[1mStep[0m  [22/26], [94mLoss[0m : 2.30057
[1mStep[0m  [24/26], [94mLoss[0m : 2.04659

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.14926
[1mStep[0m  [2/26], [94mLoss[0m : 2.09469
[1mStep[0m  [4/26], [94mLoss[0m : 2.37927
[1mStep[0m  [6/26], [94mLoss[0m : 2.15454
[1mStep[0m  [8/26], [94mLoss[0m : 2.30160
[1mStep[0m  [10/26], [94mLoss[0m : 2.42020
[1mStep[0m  [12/26], [94mLoss[0m : 2.34213
[1mStep[0m  [14/26], [94mLoss[0m : 2.20383
[1mStep[0m  [16/26], [94mLoss[0m : 2.25223
[1mStep[0m  [18/26], [94mLoss[0m : 2.20242
[1mStep[0m  [20/26], [94mLoss[0m : 2.34252
[1mStep[0m  [22/26], [94mLoss[0m : 2.20381
[1mStep[0m  [24/26], [94mLoss[0m : 2.06340

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.237, [92mTest[0m: 2.432, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23163
[1mStep[0m  [2/26], [94mLoss[0m : 2.23903
[1mStep[0m  [4/26], [94mLoss[0m : 2.27220
[1mStep[0m  [6/26], [94mLoss[0m : 2.09553
[1mStep[0m  [8/26], [94mLoss[0m : 2.21622
[1mStep[0m  [10/26], [94mLoss[0m : 2.26745
[1mStep[0m  [12/26], [94mLoss[0m : 2.17966
[1mStep[0m  [14/26], [94mLoss[0m : 2.24200
[1mStep[0m  [16/26], [94mLoss[0m : 2.28096
[1mStep[0m  [18/26], [94mLoss[0m : 2.24657
[1mStep[0m  [20/26], [94mLoss[0m : 2.14789
[1mStep[0m  [22/26], [94mLoss[0m : 2.12748
[1mStep[0m  [24/26], [94mLoss[0m : 2.19643

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.441, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.06988
[1mStep[0m  [2/26], [94mLoss[0m : 2.25475
[1mStep[0m  [4/26], [94mLoss[0m : 2.24702
[1mStep[0m  [6/26], [94mLoss[0m : 2.24040
[1mStep[0m  [8/26], [94mLoss[0m : 2.07368
[1mStep[0m  [10/26], [94mLoss[0m : 2.32140
[1mStep[0m  [12/26], [94mLoss[0m : 2.21337
[1mStep[0m  [14/26], [94mLoss[0m : 2.27320
[1mStep[0m  [16/26], [94mLoss[0m : 2.11238
[1mStep[0m  [18/26], [94mLoss[0m : 2.29722
[1mStep[0m  [20/26], [94mLoss[0m : 2.18075
[1mStep[0m  [22/26], [94mLoss[0m : 2.23910
[1mStep[0m  [24/26], [94mLoss[0m : 2.08278

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.451, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.473
====================================

Phase 2 - Evaluation MAE:  2.473080323292659
MAE score P1       2.423676
MAE score P2        2.47308
loss                2.19379
learning_rate      0.007525
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay           0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/427], [94mLoss[0m : 11.04887
[1mStep[0m  [42/427], [94mLoss[0m : 3.14913
[1mStep[0m  [84/427], [94mLoss[0m : 2.23880
[1mStep[0m  [126/427], [94mLoss[0m : 2.49243
[1mStep[0m  [168/427], [94mLoss[0m : 2.30220
[1mStep[0m  [210/427], [94mLoss[0m : 1.88371
[1mStep[0m  [252/427], [94mLoss[0m : 2.27659
[1mStep[0m  [294/427], [94mLoss[0m : 3.17070
[1mStep[0m  [336/427], [94mLoss[0m : 2.64086
[1mStep[0m  [378/427], [94mLoss[0m : 2.61674
[1mStep[0m  [420/427], [94mLoss[0m : 2.52798

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.738, [92mTest[0m: 11.020, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.52831
[1mStep[0m  [42/427], [94mLoss[0m : 2.57210
[1mStep[0m  [84/427], [94mLoss[0m : 2.40243
[1mStep[0m  [126/427], [94mLoss[0m : 2.42606
[1mStep[0m  [168/427], [94mLoss[0m : 2.61900
[1mStep[0m  [210/427], [94mLoss[0m : 2.80876
[1mStep[0m  [252/427], [94mLoss[0m : 1.90711
[1mStep[0m  [294/427], [94mLoss[0m : 2.59767
[1mStep[0m  [336/427], [94mLoss[0m : 3.00037
[1mStep[0m  [378/427], [94mLoss[0m : 2.03194
[1mStep[0m  [420/427], [94mLoss[0m : 2.29779

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.19956
[1mStep[0m  [42/427], [94mLoss[0m : 2.25972
[1mStep[0m  [84/427], [94mLoss[0m : 2.32333
[1mStep[0m  [126/427], [94mLoss[0m : 2.85705
[1mStep[0m  [168/427], [94mLoss[0m : 2.07846
[1mStep[0m  [210/427], [94mLoss[0m : 2.59189
[1mStep[0m  [252/427], [94mLoss[0m : 2.14449
[1mStep[0m  [294/427], [94mLoss[0m : 2.81898
[1mStep[0m  [336/427], [94mLoss[0m : 2.20512
[1mStep[0m  [378/427], [94mLoss[0m : 2.38777
[1mStep[0m  [420/427], [94mLoss[0m : 2.22867

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.43483
[1mStep[0m  [42/427], [94mLoss[0m : 2.62602
[1mStep[0m  [84/427], [94mLoss[0m : 2.61058
[1mStep[0m  [126/427], [94mLoss[0m : 2.55600
[1mStep[0m  [168/427], [94mLoss[0m : 3.83817
[1mStep[0m  [210/427], [94mLoss[0m : 2.19354
[1mStep[0m  [252/427], [94mLoss[0m : 3.18089
[1mStep[0m  [294/427], [94mLoss[0m : 2.57073
[1mStep[0m  [336/427], [94mLoss[0m : 3.26374
[1mStep[0m  [378/427], [94mLoss[0m : 2.37262
[1mStep[0m  [420/427], [94mLoss[0m : 2.88977

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.45777
[1mStep[0m  [42/427], [94mLoss[0m : 2.33495
[1mStep[0m  [84/427], [94mLoss[0m : 2.11743
[1mStep[0m  [126/427], [94mLoss[0m : 2.15966
[1mStep[0m  [168/427], [94mLoss[0m : 2.32949
[1mStep[0m  [210/427], [94mLoss[0m : 2.91341
[1mStep[0m  [252/427], [94mLoss[0m : 2.88879
[1mStep[0m  [294/427], [94mLoss[0m : 2.67559
[1mStep[0m  [336/427], [94mLoss[0m : 1.86722
[1mStep[0m  [378/427], [94mLoss[0m : 3.00877
[1mStep[0m  [420/427], [94mLoss[0m : 2.26965

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.67852
[1mStep[0m  [42/427], [94mLoss[0m : 2.15375
[1mStep[0m  [84/427], [94mLoss[0m : 2.57723
[1mStep[0m  [126/427], [94mLoss[0m : 2.79848
[1mStep[0m  [168/427], [94mLoss[0m : 2.08383
[1mStep[0m  [210/427], [94mLoss[0m : 2.80747
[1mStep[0m  [252/427], [94mLoss[0m : 2.23106
[1mStep[0m  [294/427], [94mLoss[0m : 3.21651
[1mStep[0m  [336/427], [94mLoss[0m : 3.17348
[1mStep[0m  [378/427], [94mLoss[0m : 2.37735
[1mStep[0m  [420/427], [94mLoss[0m : 2.19866

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.54681
[1mStep[0m  [42/427], [94mLoss[0m : 2.17099
[1mStep[0m  [84/427], [94mLoss[0m : 2.43867
[1mStep[0m  [126/427], [94mLoss[0m : 2.39708
[1mStep[0m  [168/427], [94mLoss[0m : 2.51880
[1mStep[0m  [210/427], [94mLoss[0m : 2.83431
[1mStep[0m  [252/427], [94mLoss[0m : 2.19296
[1mStep[0m  [294/427], [94mLoss[0m : 2.38028
[1mStep[0m  [336/427], [94mLoss[0m : 1.92033
[1mStep[0m  [378/427], [94mLoss[0m : 2.58567
[1mStep[0m  [420/427], [94mLoss[0m : 2.87826

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.27596
[1mStep[0m  [42/427], [94mLoss[0m : 2.69335
[1mStep[0m  [84/427], [94mLoss[0m : 2.49169
[1mStep[0m  [126/427], [94mLoss[0m : 2.13750
[1mStep[0m  [168/427], [94mLoss[0m : 2.08880
[1mStep[0m  [210/427], [94mLoss[0m : 2.64013
[1mStep[0m  [252/427], [94mLoss[0m : 2.76917
[1mStep[0m  [294/427], [94mLoss[0m : 2.97774
[1mStep[0m  [336/427], [94mLoss[0m : 1.70962
[1mStep[0m  [378/427], [94mLoss[0m : 2.50211
[1mStep[0m  [420/427], [94mLoss[0m : 2.28031

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.472, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.60860
[1mStep[0m  [42/427], [94mLoss[0m : 2.53473
[1mStep[0m  [84/427], [94mLoss[0m : 2.56486
[1mStep[0m  [126/427], [94mLoss[0m : 2.96990
[1mStep[0m  [168/427], [94mLoss[0m : 2.42801
[1mStep[0m  [210/427], [94mLoss[0m : 1.97734
[1mStep[0m  [252/427], [94mLoss[0m : 2.29810
[1mStep[0m  [294/427], [94mLoss[0m : 2.00643
[1mStep[0m  [336/427], [94mLoss[0m : 2.38590
[1mStep[0m  [378/427], [94mLoss[0m : 2.48079
[1mStep[0m  [420/427], [94mLoss[0m : 2.66904

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.43009
[1mStep[0m  [42/427], [94mLoss[0m : 2.60508
[1mStep[0m  [84/427], [94mLoss[0m : 2.73380
[1mStep[0m  [126/427], [94mLoss[0m : 2.66346
[1mStep[0m  [168/427], [94mLoss[0m : 2.75701
[1mStep[0m  [210/427], [94mLoss[0m : 2.26037
[1mStep[0m  [252/427], [94mLoss[0m : 2.23327
[1mStep[0m  [294/427], [94mLoss[0m : 2.35541
[1mStep[0m  [336/427], [94mLoss[0m : 2.07371
[1mStep[0m  [378/427], [94mLoss[0m : 2.47818
[1mStep[0m  [420/427], [94mLoss[0m : 2.42884

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.510, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.46335
[1mStep[0m  [42/427], [94mLoss[0m : 2.34783
[1mStep[0m  [84/427], [94mLoss[0m : 2.51293
[1mStep[0m  [126/427], [94mLoss[0m : 2.25599
[1mStep[0m  [168/427], [94mLoss[0m : 2.30067
[1mStep[0m  [210/427], [94mLoss[0m : 2.53617
[1mStep[0m  [252/427], [94mLoss[0m : 2.47517
[1mStep[0m  [294/427], [94mLoss[0m : 2.38141
[1mStep[0m  [336/427], [94mLoss[0m : 2.34165
[1mStep[0m  [378/427], [94mLoss[0m : 2.79751
[1mStep[0m  [420/427], [94mLoss[0m : 2.63864

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.87887
[1mStep[0m  [42/427], [94mLoss[0m : 2.34548
[1mStep[0m  [84/427], [94mLoss[0m : 2.69495
[1mStep[0m  [126/427], [94mLoss[0m : 2.38242
[1mStep[0m  [168/427], [94mLoss[0m : 2.99976
[1mStep[0m  [210/427], [94mLoss[0m : 2.72595
[1mStep[0m  [252/427], [94mLoss[0m : 3.01119
[1mStep[0m  [294/427], [94mLoss[0m : 2.78922
[1mStep[0m  [336/427], [94mLoss[0m : 2.82786
[1mStep[0m  [378/427], [94mLoss[0m : 2.79572
[1mStep[0m  [420/427], [94mLoss[0m : 2.55494

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.13231
[1mStep[0m  [42/427], [94mLoss[0m : 2.28382
[1mStep[0m  [84/427], [94mLoss[0m : 2.43050
[1mStep[0m  [126/427], [94mLoss[0m : 2.46759
[1mStep[0m  [168/427], [94mLoss[0m : 1.84600
[1mStep[0m  [210/427], [94mLoss[0m : 2.51989
[1mStep[0m  [252/427], [94mLoss[0m : 2.64097
[1mStep[0m  [294/427], [94mLoss[0m : 2.09986
[1mStep[0m  [336/427], [94mLoss[0m : 2.11007
[1mStep[0m  [378/427], [94mLoss[0m : 3.16404
[1mStep[0m  [420/427], [94mLoss[0m : 2.72377

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.60099
[1mStep[0m  [42/427], [94mLoss[0m : 2.91737
[1mStep[0m  [84/427], [94mLoss[0m : 2.14041
[1mStep[0m  [126/427], [94mLoss[0m : 2.63136
[1mStep[0m  [168/427], [94mLoss[0m : 2.80085
[1mStep[0m  [210/427], [94mLoss[0m : 2.56202
[1mStep[0m  [252/427], [94mLoss[0m : 1.87400
[1mStep[0m  [294/427], [94mLoss[0m : 2.67080
[1mStep[0m  [336/427], [94mLoss[0m : 2.42756
[1mStep[0m  [378/427], [94mLoss[0m : 2.74152
[1mStep[0m  [420/427], [94mLoss[0m : 2.85642

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.06333
[1mStep[0m  [42/427], [94mLoss[0m : 2.99644
[1mStep[0m  [84/427], [94mLoss[0m : 2.71190
[1mStep[0m  [126/427], [94mLoss[0m : 2.50773
[1mStep[0m  [168/427], [94mLoss[0m : 3.04164
[1mStep[0m  [210/427], [94mLoss[0m : 2.10375
[1mStep[0m  [252/427], [94mLoss[0m : 2.25905
[1mStep[0m  [294/427], [94mLoss[0m : 2.13519
[1mStep[0m  [336/427], [94mLoss[0m : 2.28328
[1mStep[0m  [378/427], [94mLoss[0m : 3.55221
[1mStep[0m  [420/427], [94mLoss[0m : 2.86302

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.618, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.24775
[1mStep[0m  [42/427], [94mLoss[0m : 2.61378
[1mStep[0m  [84/427], [94mLoss[0m : 2.87567
[1mStep[0m  [126/427], [94mLoss[0m : 1.97278
[1mStep[0m  [168/427], [94mLoss[0m : 2.19223
[1mStep[0m  [210/427], [94mLoss[0m : 3.10739
[1mStep[0m  [252/427], [94mLoss[0m : 2.33049
[1mStep[0m  [294/427], [94mLoss[0m : 2.42788
[1mStep[0m  [336/427], [94mLoss[0m : 2.52288
[1mStep[0m  [378/427], [94mLoss[0m : 2.96647
[1mStep[0m  [420/427], [94mLoss[0m : 2.80172

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.10712
[1mStep[0m  [42/427], [94mLoss[0m : 2.00031
[1mStep[0m  [84/427], [94mLoss[0m : 2.18467
[1mStep[0m  [126/427], [94mLoss[0m : 2.75865
[1mStep[0m  [168/427], [94mLoss[0m : 3.50541
[1mStep[0m  [210/427], [94mLoss[0m : 2.06499
[1mStep[0m  [252/427], [94mLoss[0m : 2.34799
[1mStep[0m  [294/427], [94mLoss[0m : 2.40676
[1mStep[0m  [336/427], [94mLoss[0m : 2.05206
[1mStep[0m  [378/427], [94mLoss[0m : 2.60370
[1mStep[0m  [420/427], [94mLoss[0m : 2.37454

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.59993
[1mStep[0m  [42/427], [94mLoss[0m : 2.42915
[1mStep[0m  [84/427], [94mLoss[0m : 2.44327
[1mStep[0m  [126/427], [94mLoss[0m : 2.51422
[1mStep[0m  [168/427], [94mLoss[0m : 2.88930
[1mStep[0m  [210/427], [94mLoss[0m : 2.17969
[1mStep[0m  [252/427], [94mLoss[0m : 2.09120
[1mStep[0m  [294/427], [94mLoss[0m : 2.49265
[1mStep[0m  [336/427], [94mLoss[0m : 2.64224
[1mStep[0m  [378/427], [94mLoss[0m : 2.62803
[1mStep[0m  [420/427], [94mLoss[0m : 2.48791

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.731, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.70325
[1mStep[0m  [42/427], [94mLoss[0m : 3.15995
[1mStep[0m  [84/427], [94mLoss[0m : 2.49354
[1mStep[0m  [126/427], [94mLoss[0m : 1.67258
[1mStep[0m  [168/427], [94mLoss[0m : 2.51590
[1mStep[0m  [210/427], [94mLoss[0m : 3.13543
[1mStep[0m  [252/427], [94mLoss[0m : 2.82375
[1mStep[0m  [294/427], [94mLoss[0m : 2.77985
[1mStep[0m  [336/427], [94mLoss[0m : 3.11159
[1mStep[0m  [378/427], [94mLoss[0m : 3.02331
[1mStep[0m  [420/427], [94mLoss[0m : 2.59788

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.49948
[1mStep[0m  [42/427], [94mLoss[0m : 2.60578
[1mStep[0m  [84/427], [94mLoss[0m : 2.48984
[1mStep[0m  [126/427], [94mLoss[0m : 3.07212
[1mStep[0m  [168/427], [94mLoss[0m : 2.16533
[1mStep[0m  [210/427], [94mLoss[0m : 2.46980
[1mStep[0m  [252/427], [94mLoss[0m : 2.26965
[1mStep[0m  [294/427], [94mLoss[0m : 2.79417
[1mStep[0m  [336/427], [94mLoss[0m : 3.45101
[1mStep[0m  [378/427], [94mLoss[0m : 2.32497
[1mStep[0m  [420/427], [94mLoss[0m : 2.26001

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.74001
[1mStep[0m  [42/427], [94mLoss[0m : 2.59664
[1mStep[0m  [84/427], [94mLoss[0m : 2.31001
[1mStep[0m  [126/427], [94mLoss[0m : 3.08616
[1mStep[0m  [168/427], [94mLoss[0m : 2.25391
[1mStep[0m  [210/427], [94mLoss[0m : 2.58624
[1mStep[0m  [252/427], [94mLoss[0m : 2.65406
[1mStep[0m  [294/427], [94mLoss[0m : 2.76802
[1mStep[0m  [336/427], [94mLoss[0m : 2.36473
[1mStep[0m  [378/427], [94mLoss[0m : 2.53030
[1mStep[0m  [420/427], [94mLoss[0m : 2.37960

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.42576
[1mStep[0m  [42/427], [94mLoss[0m : 1.93304
[1mStep[0m  [84/427], [94mLoss[0m : 3.96970
[1mStep[0m  [126/427], [94mLoss[0m : 2.88724
[1mStep[0m  [168/427], [94mLoss[0m : 1.78526
[1mStep[0m  [210/427], [94mLoss[0m : 2.37769
[1mStep[0m  [252/427], [94mLoss[0m : 2.56403
[1mStep[0m  [294/427], [94mLoss[0m : 2.39652
[1mStep[0m  [336/427], [94mLoss[0m : 2.54020
[1mStep[0m  [378/427], [94mLoss[0m : 2.40186
[1mStep[0m  [420/427], [94mLoss[0m : 2.43399

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.426, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.68659
[1mStep[0m  [42/427], [94mLoss[0m : 1.91711
[1mStep[0m  [84/427], [94mLoss[0m : 2.11892
[1mStep[0m  [126/427], [94mLoss[0m : 3.58787
[1mStep[0m  [168/427], [94mLoss[0m : 2.32794
[1mStep[0m  [210/427], [94mLoss[0m : 2.77734
[1mStep[0m  [252/427], [94mLoss[0m : 2.69020
[1mStep[0m  [294/427], [94mLoss[0m : 2.41558
[1mStep[0m  [336/427], [94mLoss[0m : 2.38079
[1mStep[0m  [378/427], [94mLoss[0m : 2.21092
[1mStep[0m  [420/427], [94mLoss[0m : 3.04274

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.72356
[1mStep[0m  [42/427], [94mLoss[0m : 1.96017
[1mStep[0m  [84/427], [94mLoss[0m : 2.71984
[1mStep[0m  [126/427], [94mLoss[0m : 2.69314
[1mStep[0m  [168/427], [94mLoss[0m : 2.01334
[1mStep[0m  [210/427], [94mLoss[0m : 2.07071
[1mStep[0m  [252/427], [94mLoss[0m : 2.18777
[1mStep[0m  [294/427], [94mLoss[0m : 2.47258
[1mStep[0m  [336/427], [94mLoss[0m : 2.45611
[1mStep[0m  [378/427], [94mLoss[0m : 2.40054
[1mStep[0m  [420/427], [94mLoss[0m : 3.24862

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.451, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.57485
[1mStep[0m  [42/427], [94mLoss[0m : 2.63520
[1mStep[0m  [84/427], [94mLoss[0m : 2.56954
[1mStep[0m  [126/427], [94mLoss[0m : 2.49935
[1mStep[0m  [168/427], [94mLoss[0m : 2.24445
[1mStep[0m  [210/427], [94mLoss[0m : 2.07909
[1mStep[0m  [252/427], [94mLoss[0m : 2.11766
[1mStep[0m  [294/427], [94mLoss[0m : 3.49673
[1mStep[0m  [336/427], [94mLoss[0m : 2.93991
[1mStep[0m  [378/427], [94mLoss[0m : 2.27262
[1mStep[0m  [420/427], [94mLoss[0m : 2.69558

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.69234
[1mStep[0m  [42/427], [94mLoss[0m : 3.50693
[1mStep[0m  [84/427], [94mLoss[0m : 2.08330
[1mStep[0m  [126/427], [94mLoss[0m : 2.43023
[1mStep[0m  [168/427], [94mLoss[0m : 2.99321
[1mStep[0m  [210/427], [94mLoss[0m : 2.47376
[1mStep[0m  [252/427], [94mLoss[0m : 2.37141
[1mStep[0m  [294/427], [94mLoss[0m : 2.60558
[1mStep[0m  [336/427], [94mLoss[0m : 2.77508
[1mStep[0m  [378/427], [94mLoss[0m : 2.29099
[1mStep[0m  [420/427], [94mLoss[0m : 2.66132

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.592, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.49917
[1mStep[0m  [42/427], [94mLoss[0m : 2.17689
[1mStep[0m  [84/427], [94mLoss[0m : 2.95592
[1mStep[0m  [126/427], [94mLoss[0m : 1.81118
[1mStep[0m  [168/427], [94mLoss[0m : 2.28147
[1mStep[0m  [210/427], [94mLoss[0m : 2.57505
[1mStep[0m  [252/427], [94mLoss[0m : 2.52352
[1mStep[0m  [294/427], [94mLoss[0m : 2.58802
[1mStep[0m  [336/427], [94mLoss[0m : 2.36712
[1mStep[0m  [378/427], [94mLoss[0m : 2.81434
[1mStep[0m  [420/427], [94mLoss[0m : 2.65498

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.443, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.59274
[1mStep[0m  [42/427], [94mLoss[0m : 1.91915
[1mStep[0m  [84/427], [94mLoss[0m : 2.33974
[1mStep[0m  [126/427], [94mLoss[0m : 2.36091
[1mStep[0m  [168/427], [94mLoss[0m : 2.35721
[1mStep[0m  [210/427], [94mLoss[0m : 2.82002
[1mStep[0m  [252/427], [94mLoss[0m : 2.33476
[1mStep[0m  [294/427], [94mLoss[0m : 2.78965
[1mStep[0m  [336/427], [94mLoss[0m : 2.77215
[1mStep[0m  [378/427], [94mLoss[0m : 2.38645
[1mStep[0m  [420/427], [94mLoss[0m : 3.08358

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.427, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.26893
[1mStep[0m  [42/427], [94mLoss[0m : 2.26040
[1mStep[0m  [84/427], [94mLoss[0m : 2.49964
[1mStep[0m  [126/427], [94mLoss[0m : 2.59077
[1mStep[0m  [168/427], [94mLoss[0m : 3.05721
[1mStep[0m  [210/427], [94mLoss[0m : 2.38768
[1mStep[0m  [252/427], [94mLoss[0m : 2.74882
[1mStep[0m  [294/427], [94mLoss[0m : 2.50772
[1mStep[0m  [336/427], [94mLoss[0m : 2.59037
[1mStep[0m  [378/427], [94mLoss[0m : 2.51357
[1mStep[0m  [420/427], [94mLoss[0m : 2.47278

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.618, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.61728
[1mStep[0m  [42/427], [94mLoss[0m : 2.42293
[1mStep[0m  [84/427], [94mLoss[0m : 2.02943
[1mStep[0m  [126/427], [94mLoss[0m : 1.97867
[1mStep[0m  [168/427], [94mLoss[0m : 2.55271
[1mStep[0m  [210/427], [94mLoss[0m : 2.63123
[1mStep[0m  [252/427], [94mLoss[0m : 2.77423
[1mStep[0m  [294/427], [94mLoss[0m : 2.74028
[1mStep[0m  [336/427], [94mLoss[0m : 2.86146
[1mStep[0m  [378/427], [94mLoss[0m : 2.67522
[1mStep[0m  [420/427], [94mLoss[0m : 2.26015

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.554, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.435
====================================

Phase 1 - Evaluation MAE:  2.4351766462057407
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/427], [94mLoss[0m : 2.67728
[1mStep[0m  [42/427], [94mLoss[0m : 2.60956
[1mStep[0m  [84/427], [94mLoss[0m : 2.59035
[1mStep[0m  [126/427], [94mLoss[0m : 1.99739
[1mStep[0m  [168/427], [94mLoss[0m : 2.79615
[1mStep[0m  [210/427], [94mLoss[0m : 2.46543
[1mStep[0m  [252/427], [94mLoss[0m : 2.17080
[1mStep[0m  [294/427], [94mLoss[0m : 2.26914
[1mStep[0m  [336/427], [94mLoss[0m : 2.63500
[1mStep[0m  [378/427], [94mLoss[0m : 2.86690
[1mStep[0m  [420/427], [94mLoss[0m : 2.34300

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.18189
[1mStep[0m  [42/427], [94mLoss[0m : 2.30538
[1mStep[0m  [84/427], [94mLoss[0m : 2.34093
[1mStep[0m  [126/427], [94mLoss[0m : 2.50143
[1mStep[0m  [168/427], [94mLoss[0m : 2.95480
[1mStep[0m  [210/427], [94mLoss[0m : 2.66518
[1mStep[0m  [252/427], [94mLoss[0m : 3.48185
[1mStep[0m  [294/427], [94mLoss[0m : 2.62960
[1mStep[0m  [336/427], [94mLoss[0m : 2.44050
[1mStep[0m  [378/427], [94mLoss[0m : 2.70170
[1mStep[0m  [420/427], [94mLoss[0m : 2.96841

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.86339
[1mStep[0m  [42/427], [94mLoss[0m : 2.59087
[1mStep[0m  [84/427], [94mLoss[0m : 2.46592
[1mStep[0m  [126/427], [94mLoss[0m : 2.20963
[1mStep[0m  [168/427], [94mLoss[0m : 2.28949
[1mStep[0m  [210/427], [94mLoss[0m : 2.99217
[1mStep[0m  [252/427], [94mLoss[0m : 2.16740
[1mStep[0m  [294/427], [94mLoss[0m : 2.26933
[1mStep[0m  [336/427], [94mLoss[0m : 2.54322
[1mStep[0m  [378/427], [94mLoss[0m : 2.93607
[1mStep[0m  [420/427], [94mLoss[0m : 2.35649

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.570, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.68131
[1mStep[0m  [42/427], [94mLoss[0m : 2.21969
[1mStep[0m  [84/427], [94mLoss[0m : 3.01950
[1mStep[0m  [126/427], [94mLoss[0m : 2.80950
[1mStep[0m  [168/427], [94mLoss[0m : 2.66824
[1mStep[0m  [210/427], [94mLoss[0m : 2.37032
[1mStep[0m  [252/427], [94mLoss[0m : 1.98886
[1mStep[0m  [294/427], [94mLoss[0m : 2.76846
[1mStep[0m  [336/427], [94mLoss[0m : 2.65642
[1mStep[0m  [378/427], [94mLoss[0m : 2.74689
[1mStep[0m  [420/427], [94mLoss[0m : 1.95634

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.96311
[1mStep[0m  [42/427], [94mLoss[0m : 2.57437
[1mStep[0m  [84/427], [94mLoss[0m : 2.02070
[1mStep[0m  [126/427], [94mLoss[0m : 2.65885
[1mStep[0m  [168/427], [94mLoss[0m : 2.22735
[1mStep[0m  [210/427], [94mLoss[0m : 2.52797
[1mStep[0m  [252/427], [94mLoss[0m : 2.16164
[1mStep[0m  [294/427], [94mLoss[0m : 2.98090
[1mStep[0m  [336/427], [94mLoss[0m : 2.74344
[1mStep[0m  [378/427], [94mLoss[0m : 2.67720
[1mStep[0m  [420/427], [94mLoss[0m : 3.58673

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.505, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.93171
[1mStep[0m  [42/427], [94mLoss[0m : 2.79848
[1mStep[0m  [84/427], [94mLoss[0m : 2.22820
[1mStep[0m  [126/427], [94mLoss[0m : 2.42869
[1mStep[0m  [168/427], [94mLoss[0m : 2.27088
[1mStep[0m  [210/427], [94mLoss[0m : 2.23916
[1mStep[0m  [252/427], [94mLoss[0m : 2.25854
[1mStep[0m  [294/427], [94mLoss[0m : 3.06946
[1mStep[0m  [336/427], [94mLoss[0m : 2.70209
[1mStep[0m  [378/427], [94mLoss[0m : 2.58662
[1mStep[0m  [420/427], [94mLoss[0m : 2.64101

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.75332
[1mStep[0m  [42/427], [94mLoss[0m : 2.29679
[1mStep[0m  [84/427], [94mLoss[0m : 2.19340
[1mStep[0m  [126/427], [94mLoss[0m : 2.90891
[1mStep[0m  [168/427], [94mLoss[0m : 1.85313
[1mStep[0m  [210/427], [94mLoss[0m : 3.39759
[1mStep[0m  [252/427], [94mLoss[0m : 2.77831
[1mStep[0m  [294/427], [94mLoss[0m : 2.08873
[1mStep[0m  [336/427], [94mLoss[0m : 3.20672
[1mStep[0m  [378/427], [94mLoss[0m : 2.31031
[1mStep[0m  [420/427], [94mLoss[0m : 2.29819

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.44343
[1mStep[0m  [42/427], [94mLoss[0m : 3.26370
[1mStep[0m  [84/427], [94mLoss[0m : 2.33627
[1mStep[0m  [126/427], [94mLoss[0m : 2.58793
[1mStep[0m  [168/427], [94mLoss[0m : 2.27936
[1mStep[0m  [210/427], [94mLoss[0m : 2.47725
[1mStep[0m  [252/427], [94mLoss[0m : 2.67449
[1mStep[0m  [294/427], [94mLoss[0m : 2.24070
[1mStep[0m  [336/427], [94mLoss[0m : 2.44568
[1mStep[0m  [378/427], [94mLoss[0m : 2.37217
[1mStep[0m  [420/427], [94mLoss[0m : 2.70536

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.77548
[1mStep[0m  [42/427], [94mLoss[0m : 2.83313
[1mStep[0m  [84/427], [94mLoss[0m : 2.07479
[1mStep[0m  [126/427], [94mLoss[0m : 2.49089
[1mStep[0m  [168/427], [94mLoss[0m : 2.13564
[1mStep[0m  [210/427], [94mLoss[0m : 2.78166
[1mStep[0m  [252/427], [94mLoss[0m : 2.48488
[1mStep[0m  [294/427], [94mLoss[0m : 2.97407
[1mStep[0m  [336/427], [94mLoss[0m : 2.77906
[1mStep[0m  [378/427], [94mLoss[0m : 2.78339
[1mStep[0m  [420/427], [94mLoss[0m : 2.64205

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.08038
[1mStep[0m  [42/427], [94mLoss[0m : 2.62848
[1mStep[0m  [84/427], [94mLoss[0m : 3.82598
[1mStep[0m  [126/427], [94mLoss[0m : 3.00162
[1mStep[0m  [168/427], [94mLoss[0m : 2.28214
[1mStep[0m  [210/427], [94mLoss[0m : 2.66411
[1mStep[0m  [252/427], [94mLoss[0m : 2.88561
[1mStep[0m  [294/427], [94mLoss[0m : 2.25975
[1mStep[0m  [336/427], [94mLoss[0m : 2.85579
[1mStep[0m  [378/427], [94mLoss[0m : 2.57141
[1mStep[0m  [420/427], [94mLoss[0m : 2.72049

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.521, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.71711
[1mStep[0m  [42/427], [94mLoss[0m : 2.78404
[1mStep[0m  [84/427], [94mLoss[0m : 2.50966
[1mStep[0m  [126/427], [94mLoss[0m : 2.43940
[1mStep[0m  [168/427], [94mLoss[0m : 2.98872
[1mStep[0m  [210/427], [94mLoss[0m : 2.17323
[1mStep[0m  [252/427], [94mLoss[0m : 2.14609
[1mStep[0m  [294/427], [94mLoss[0m : 2.46425
[1mStep[0m  [336/427], [94mLoss[0m : 2.92673
[1mStep[0m  [378/427], [94mLoss[0m : 2.28067
[1mStep[0m  [420/427], [94mLoss[0m : 2.64474

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.79731
[1mStep[0m  [42/427], [94mLoss[0m : 3.09015
[1mStep[0m  [84/427], [94mLoss[0m : 2.75171
[1mStep[0m  [126/427], [94mLoss[0m : 2.98970
[1mStep[0m  [168/427], [94mLoss[0m : 2.59003
[1mStep[0m  [210/427], [94mLoss[0m : 2.72134
[1mStep[0m  [252/427], [94mLoss[0m : 2.37334
[1mStep[0m  [294/427], [94mLoss[0m : 2.73996
[1mStep[0m  [336/427], [94mLoss[0m : 2.65446
[1mStep[0m  [378/427], [94mLoss[0m : 2.97021
[1mStep[0m  [420/427], [94mLoss[0m : 2.17171

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.66280
[1mStep[0m  [42/427], [94mLoss[0m : 2.17040
[1mStep[0m  [84/427], [94mLoss[0m : 2.76641
[1mStep[0m  [126/427], [94mLoss[0m : 2.61691
[1mStep[0m  [168/427], [94mLoss[0m : 2.76544
[1mStep[0m  [210/427], [94mLoss[0m : 2.79361
[1mStep[0m  [252/427], [94mLoss[0m : 2.81997
[1mStep[0m  [294/427], [94mLoss[0m : 2.46250
[1mStep[0m  [336/427], [94mLoss[0m : 2.49711
[1mStep[0m  [378/427], [94mLoss[0m : 3.19682
[1mStep[0m  [420/427], [94mLoss[0m : 2.74178

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.698, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.67371
[1mStep[0m  [42/427], [94mLoss[0m : 2.88406
[1mStep[0m  [84/427], [94mLoss[0m : 2.30036
[1mStep[0m  [126/427], [94mLoss[0m : 1.79562
[1mStep[0m  [168/427], [94mLoss[0m : 2.42481
[1mStep[0m  [210/427], [94mLoss[0m : 2.38439
[1mStep[0m  [252/427], [94mLoss[0m : 2.37795
[1mStep[0m  [294/427], [94mLoss[0m : 2.67309
[1mStep[0m  [336/427], [94mLoss[0m : 2.50869
[1mStep[0m  [378/427], [94mLoss[0m : 3.37274
[1mStep[0m  [420/427], [94mLoss[0m : 2.17689

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.540, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.10582
[1mStep[0m  [42/427], [94mLoss[0m : 2.86482
[1mStep[0m  [84/427], [94mLoss[0m : 3.04795
[1mStep[0m  [126/427], [94mLoss[0m : 2.96946
[1mStep[0m  [168/427], [94mLoss[0m : 2.84075
[1mStep[0m  [210/427], [94mLoss[0m : 2.12198
[1mStep[0m  [252/427], [94mLoss[0m : 2.45233
[1mStep[0m  [294/427], [94mLoss[0m : 3.08310
[1mStep[0m  [336/427], [94mLoss[0m : 2.58912
[1mStep[0m  [378/427], [94mLoss[0m : 2.68947
[1mStep[0m  [420/427], [94mLoss[0m : 2.26479

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.78230
[1mStep[0m  [42/427], [94mLoss[0m : 2.79034
[1mStep[0m  [84/427], [94mLoss[0m : 2.18015
[1mStep[0m  [126/427], [94mLoss[0m : 2.99812
[1mStep[0m  [168/427], [94mLoss[0m : 2.46267
[1mStep[0m  [210/427], [94mLoss[0m : 3.12477
[1mStep[0m  [252/427], [94mLoss[0m : 2.78195
[1mStep[0m  [294/427], [94mLoss[0m : 2.68674
[1mStep[0m  [336/427], [94mLoss[0m : 2.56952
[1mStep[0m  [378/427], [94mLoss[0m : 3.15721
[1mStep[0m  [420/427], [94mLoss[0m : 2.13558

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.506, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.43546
[1mStep[0m  [42/427], [94mLoss[0m : 1.93890
[1mStep[0m  [84/427], [94mLoss[0m : 3.01297
[1mStep[0m  [126/427], [94mLoss[0m : 2.97845
[1mStep[0m  [168/427], [94mLoss[0m : 2.58481
[1mStep[0m  [210/427], [94mLoss[0m : 1.86449
[1mStep[0m  [252/427], [94mLoss[0m : 2.96148
[1mStep[0m  [294/427], [94mLoss[0m : 3.21828
[1mStep[0m  [336/427], [94mLoss[0m : 3.01333
[1mStep[0m  [378/427], [94mLoss[0m : 1.91281
[1mStep[0m  [420/427], [94mLoss[0m : 2.18087

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.69334
[1mStep[0m  [42/427], [94mLoss[0m : 2.24362
[1mStep[0m  [84/427], [94mLoss[0m : 2.51993
[1mStep[0m  [126/427], [94mLoss[0m : 2.23049
[1mStep[0m  [168/427], [94mLoss[0m : 2.24328
[1mStep[0m  [210/427], [94mLoss[0m : 2.94964
[1mStep[0m  [252/427], [94mLoss[0m : 3.27156
[1mStep[0m  [294/427], [94mLoss[0m : 2.28541
[1mStep[0m  [336/427], [94mLoss[0m : 2.36367
[1mStep[0m  [378/427], [94mLoss[0m : 2.44218
[1mStep[0m  [420/427], [94mLoss[0m : 2.89117

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.531, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.16856
[1mStep[0m  [42/427], [94mLoss[0m : 2.98300
[1mStep[0m  [84/427], [94mLoss[0m : 2.29804
[1mStep[0m  [126/427], [94mLoss[0m : 2.22295
[1mStep[0m  [168/427], [94mLoss[0m : 2.25420
[1mStep[0m  [210/427], [94mLoss[0m : 2.24996
[1mStep[0m  [252/427], [94mLoss[0m : 2.83277
[1mStep[0m  [294/427], [94mLoss[0m : 2.88436
[1mStep[0m  [336/427], [94mLoss[0m : 2.52350
[1mStep[0m  [378/427], [94mLoss[0m : 2.26019
[1mStep[0m  [420/427], [94mLoss[0m : 3.08345

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.723, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.63946
[1mStep[0m  [42/427], [94mLoss[0m : 2.25684
[1mStep[0m  [84/427], [94mLoss[0m : 2.44268
[1mStep[0m  [126/427], [94mLoss[0m : 2.26672
[1mStep[0m  [168/427], [94mLoss[0m : 2.62459
[1mStep[0m  [210/427], [94mLoss[0m : 2.32773
[1mStep[0m  [252/427], [94mLoss[0m : 2.10740
[1mStep[0m  [294/427], [94mLoss[0m : 3.00378
[1mStep[0m  [336/427], [94mLoss[0m : 2.54940
[1mStep[0m  [378/427], [94mLoss[0m : 2.51406
[1mStep[0m  [420/427], [94mLoss[0m : 2.09867

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.554, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.71358
[1mStep[0m  [42/427], [94mLoss[0m : 3.25300
[1mStep[0m  [84/427], [94mLoss[0m : 3.23310
[1mStep[0m  [126/427], [94mLoss[0m : 2.66007
[1mStep[0m  [168/427], [94mLoss[0m : 2.54457
[1mStep[0m  [210/427], [94mLoss[0m : 2.37949
[1mStep[0m  [252/427], [94mLoss[0m : 3.12327
[1mStep[0m  [294/427], [94mLoss[0m : 2.46250
[1mStep[0m  [336/427], [94mLoss[0m : 2.15382
[1mStep[0m  [378/427], [94mLoss[0m : 3.25385
[1mStep[0m  [420/427], [94mLoss[0m : 2.27953

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.588, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.97002
[1mStep[0m  [42/427], [94mLoss[0m : 2.08890
[1mStep[0m  [84/427], [94mLoss[0m : 2.80567
[1mStep[0m  [126/427], [94mLoss[0m : 2.78489
[1mStep[0m  [168/427], [94mLoss[0m : 2.77769
[1mStep[0m  [210/427], [94mLoss[0m : 2.31694
[1mStep[0m  [252/427], [94mLoss[0m : 3.60666
[1mStep[0m  [294/427], [94mLoss[0m : 2.72140
[1mStep[0m  [336/427], [94mLoss[0m : 3.06462
[1mStep[0m  [378/427], [94mLoss[0m : 2.78308
[1mStep[0m  [420/427], [94mLoss[0m : 2.66689

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.45969
[1mStep[0m  [42/427], [94mLoss[0m : 2.91438
[1mStep[0m  [84/427], [94mLoss[0m : 1.97548
[1mStep[0m  [126/427], [94mLoss[0m : 3.32560
[1mStep[0m  [168/427], [94mLoss[0m : 2.20834
[1mStep[0m  [210/427], [94mLoss[0m : 2.63524
[1mStep[0m  [252/427], [94mLoss[0m : 2.79513
[1mStep[0m  [294/427], [94mLoss[0m : 2.47429
[1mStep[0m  [336/427], [94mLoss[0m : 2.40000
[1mStep[0m  [378/427], [94mLoss[0m : 2.27573
[1mStep[0m  [420/427], [94mLoss[0m : 2.94125

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.636, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 3.84859
[1mStep[0m  [42/427], [94mLoss[0m : 2.85965
[1mStep[0m  [84/427], [94mLoss[0m : 2.45491
[1mStep[0m  [126/427], [94mLoss[0m : 2.67448
[1mStep[0m  [168/427], [94mLoss[0m : 2.65221
[1mStep[0m  [210/427], [94mLoss[0m : 2.40465
[1mStep[0m  [252/427], [94mLoss[0m : 1.72780
[1mStep[0m  [294/427], [94mLoss[0m : 1.81308
[1mStep[0m  [336/427], [94mLoss[0m : 2.32067
[1mStep[0m  [378/427], [94mLoss[0m : 1.94930
[1mStep[0m  [420/427], [94mLoss[0m : 3.30993

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.10386
[1mStep[0m  [42/427], [94mLoss[0m : 2.38863
[1mStep[0m  [84/427], [94mLoss[0m : 2.63444
[1mStep[0m  [126/427], [94mLoss[0m : 2.59540
[1mStep[0m  [168/427], [94mLoss[0m : 2.86613
[1mStep[0m  [210/427], [94mLoss[0m : 2.57166
[1mStep[0m  [252/427], [94mLoss[0m : 2.73157
[1mStep[0m  [294/427], [94mLoss[0m : 3.07424
[1mStep[0m  [336/427], [94mLoss[0m : 2.93334
[1mStep[0m  [378/427], [94mLoss[0m : 2.54810
[1mStep[0m  [420/427], [94mLoss[0m : 2.22680

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.545, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.55004
[1mStep[0m  [42/427], [94mLoss[0m : 2.55087
[1mStep[0m  [84/427], [94mLoss[0m : 2.21029
[1mStep[0m  [126/427], [94mLoss[0m : 3.08095
[1mStep[0m  [168/427], [94mLoss[0m : 2.60929
[1mStep[0m  [210/427], [94mLoss[0m : 3.20343
[1mStep[0m  [252/427], [94mLoss[0m : 2.85243
[1mStep[0m  [294/427], [94mLoss[0m : 2.33375
[1mStep[0m  [336/427], [94mLoss[0m : 2.27165
[1mStep[0m  [378/427], [94mLoss[0m : 2.12346
[1mStep[0m  [420/427], [94mLoss[0m : 2.53829

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.574, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.95788
[1mStep[0m  [42/427], [94mLoss[0m : 2.70946
[1mStep[0m  [84/427], [94mLoss[0m : 2.37811
[1mStep[0m  [126/427], [94mLoss[0m : 2.46030
[1mStep[0m  [168/427], [94mLoss[0m : 2.95119
[1mStep[0m  [210/427], [94mLoss[0m : 2.16792
[1mStep[0m  [252/427], [94mLoss[0m : 2.40814
[1mStep[0m  [294/427], [94mLoss[0m : 2.51528
[1mStep[0m  [336/427], [94mLoss[0m : 2.32276
[1mStep[0m  [378/427], [94mLoss[0m : 2.90941
[1mStep[0m  [420/427], [94mLoss[0m : 2.67330

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.05158
[1mStep[0m  [42/427], [94mLoss[0m : 1.71398
[1mStep[0m  [84/427], [94mLoss[0m : 2.08429
[1mStep[0m  [126/427], [94mLoss[0m : 2.12049
[1mStep[0m  [168/427], [94mLoss[0m : 3.08656
[1mStep[0m  [210/427], [94mLoss[0m : 3.07438
[1mStep[0m  [252/427], [94mLoss[0m : 3.09294
[1mStep[0m  [294/427], [94mLoss[0m : 2.49748
[1mStep[0m  [336/427], [94mLoss[0m : 2.79047
[1mStep[0m  [378/427], [94mLoss[0m : 2.41765
[1mStep[0m  [420/427], [94mLoss[0m : 2.66941

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.482, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 1.99928
[1mStep[0m  [42/427], [94mLoss[0m : 2.39100
[1mStep[0m  [84/427], [94mLoss[0m : 2.53121
[1mStep[0m  [126/427], [94mLoss[0m : 1.78669
[1mStep[0m  [168/427], [94mLoss[0m : 2.27262
[1mStep[0m  [210/427], [94mLoss[0m : 2.63396
[1mStep[0m  [252/427], [94mLoss[0m : 2.58328
[1mStep[0m  [294/427], [94mLoss[0m : 3.02124
[1mStep[0m  [336/427], [94mLoss[0m : 3.04699
[1mStep[0m  [378/427], [94mLoss[0m : 2.77648
[1mStep[0m  [420/427], [94mLoss[0m : 1.99784

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.624, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/427], [94mLoss[0m : 2.32600
[1mStep[0m  [42/427], [94mLoss[0m : 3.37207
[1mStep[0m  [84/427], [94mLoss[0m : 2.15611
[1mStep[0m  [126/427], [94mLoss[0m : 1.96901
[1mStep[0m  [168/427], [94mLoss[0m : 3.03034
[1mStep[0m  [210/427], [94mLoss[0m : 2.46720
[1mStep[0m  [252/427], [94mLoss[0m : 2.30752
[1mStep[0m  [294/427], [94mLoss[0m : 2.54908
[1mStep[0m  [336/427], [94mLoss[0m : 3.26707
[1mStep[0m  [378/427], [94mLoss[0m : 2.41980
[1mStep[0m  [420/427], [94mLoss[0m : 2.55621

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.478684981664022
MAE score P1      2.435177
MAE score P2      2.478685
loss              2.527039
learning_rate         0.01
batch_size              32
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.81644
[1mStep[0m  [2/26], [94mLoss[0m : 10.56356
[1mStep[0m  [4/26], [94mLoss[0m : 10.91424
[1mStep[0m  [6/26], [94mLoss[0m : 10.80292
[1mStep[0m  [8/26], [94mLoss[0m : 10.87410
[1mStep[0m  [10/26], [94mLoss[0m : 10.93766
[1mStep[0m  [12/26], [94mLoss[0m : 10.63076
[1mStep[0m  [14/26], [94mLoss[0m : 10.66662
[1mStep[0m  [16/26], [94mLoss[0m : 10.51585
[1mStep[0m  [18/26], [94mLoss[0m : 10.29972
[1mStep[0m  [20/26], [94mLoss[0m : 10.49084
[1mStep[0m  [22/26], [94mLoss[0m : 10.26741
[1mStep[0m  [24/26], [94mLoss[0m : 10.38010

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.630, [92mTest[0m: 10.899, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.62870
[1mStep[0m  [2/26], [94mLoss[0m : 10.42926
[1mStep[0m  [4/26], [94mLoss[0m : 10.24102
[1mStep[0m  [6/26], [94mLoss[0m : 10.21571
[1mStep[0m  [8/26], [94mLoss[0m : 10.19830
[1mStep[0m  [10/26], [94mLoss[0m : 10.17560
[1mStep[0m  [12/26], [94mLoss[0m : 9.90428
[1mStep[0m  [14/26], [94mLoss[0m : 10.00890
[1mStep[0m  [16/26], [94mLoss[0m : 10.17165
[1mStep[0m  [18/26], [94mLoss[0m : 9.88964
[1mStep[0m  [20/26], [94mLoss[0m : 9.97163
[1mStep[0m  [22/26], [94mLoss[0m : 9.98744
[1mStep[0m  [24/26], [94mLoss[0m : 10.09488

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.144, [92mTest[0m: 10.275, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.99645
[1mStep[0m  [2/26], [94mLoss[0m : 9.86251
[1mStep[0m  [4/26], [94mLoss[0m : 9.62373
[1mStep[0m  [6/26], [94mLoss[0m : 9.69862
[1mStep[0m  [8/26], [94mLoss[0m : 10.21253
[1mStep[0m  [10/26], [94mLoss[0m : 9.71455
[1mStep[0m  [12/26], [94mLoss[0m : 9.57271
[1mStep[0m  [14/26], [94mLoss[0m : 9.59504
[1mStep[0m  [16/26], [94mLoss[0m : 9.17568
[1mStep[0m  [18/26], [94mLoss[0m : 9.55322
[1mStep[0m  [20/26], [94mLoss[0m : 9.58302
[1mStep[0m  [22/26], [94mLoss[0m : 9.44710
[1mStep[0m  [24/26], [94mLoss[0m : 9.25723

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.609, [92mTest[0m: 9.683, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.11357
[1mStep[0m  [2/26], [94mLoss[0m : 9.25097
[1mStep[0m  [4/26], [94mLoss[0m : 9.33358
[1mStep[0m  [6/26], [94mLoss[0m : 9.07449
[1mStep[0m  [8/26], [94mLoss[0m : 9.14454
[1mStep[0m  [10/26], [94mLoss[0m : 9.03838
[1mStep[0m  [12/26], [94mLoss[0m : 8.91840
[1mStep[0m  [14/26], [94mLoss[0m : 8.95683
[1mStep[0m  [16/26], [94mLoss[0m : 8.87987
[1mStep[0m  [18/26], [94mLoss[0m : 9.05503
[1mStep[0m  [20/26], [94mLoss[0m : 8.75844
[1mStep[0m  [22/26], [94mLoss[0m : 8.83261
[1mStep[0m  [24/26], [94mLoss[0m : 8.78988

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.994, [92mTest[0m: 9.049, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.36392
[1mStep[0m  [2/26], [94mLoss[0m : 8.85393
[1mStep[0m  [4/26], [94mLoss[0m : 8.33876
[1mStep[0m  [6/26], [94mLoss[0m : 8.42245
[1mStep[0m  [8/26], [94mLoss[0m : 8.19936
[1mStep[0m  [10/26], [94mLoss[0m : 8.62848
[1mStep[0m  [12/26], [94mLoss[0m : 8.21289
[1mStep[0m  [14/26], [94mLoss[0m : 8.20073
[1mStep[0m  [16/26], [94mLoss[0m : 8.22427
[1mStep[0m  [18/26], [94mLoss[0m : 8.01995
[1mStep[0m  [20/26], [94mLoss[0m : 8.18479
[1mStep[0m  [22/26], [94mLoss[0m : 7.83317
[1mStep[0m  [24/26], [94mLoss[0m : 7.74364

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.222, [92mTest[0m: 8.310, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.74959
[1mStep[0m  [2/26], [94mLoss[0m : 7.91098
[1mStep[0m  [4/26], [94mLoss[0m : 7.47292
[1mStep[0m  [6/26], [94mLoss[0m : 7.29895
[1mStep[0m  [8/26], [94mLoss[0m : 7.65933
[1mStep[0m  [10/26], [94mLoss[0m : 7.18773
[1mStep[0m  [12/26], [94mLoss[0m : 7.19787
[1mStep[0m  [14/26], [94mLoss[0m : 6.98627
[1mStep[0m  [16/26], [94mLoss[0m : 7.10286
[1mStep[0m  [18/26], [94mLoss[0m : 7.06146
[1mStep[0m  [20/26], [94mLoss[0m : 7.00810
[1mStep[0m  [22/26], [94mLoss[0m : 6.59036
[1mStep[0m  [24/26], [94mLoss[0m : 6.61845

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.246, [92mTest[0m: 7.314, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.73498
[1mStep[0m  [2/26], [94mLoss[0m : 6.59283
[1mStep[0m  [4/26], [94mLoss[0m : 6.69632
[1mStep[0m  [6/26], [94mLoss[0m : 6.58298
[1mStep[0m  [8/26], [94mLoss[0m : 6.68991
[1mStep[0m  [10/26], [94mLoss[0m : 6.45180
[1mStep[0m  [12/26], [94mLoss[0m : 5.94926
[1mStep[0m  [14/26], [94mLoss[0m : 6.38458
[1mStep[0m  [16/26], [94mLoss[0m : 5.97400
[1mStep[0m  [18/26], [94mLoss[0m : 6.21950
[1mStep[0m  [20/26], [94mLoss[0m : 5.98581
[1mStep[0m  [22/26], [94mLoss[0m : 6.04592
[1mStep[0m  [24/26], [94mLoss[0m : 5.77772

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.261, [92mTest[0m: 6.199, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.86902
[1mStep[0m  [2/26], [94mLoss[0m : 5.71264
[1mStep[0m  [4/26], [94mLoss[0m : 5.59725
[1mStep[0m  [6/26], [94mLoss[0m : 5.45288
[1mStep[0m  [8/26], [94mLoss[0m : 5.51074
[1mStep[0m  [10/26], [94mLoss[0m : 5.58606
[1mStep[0m  [12/26], [94mLoss[0m : 5.19735
[1mStep[0m  [14/26], [94mLoss[0m : 5.39841
[1mStep[0m  [16/26], [94mLoss[0m : 5.23662
[1mStep[0m  [18/26], [94mLoss[0m : 5.04336
[1mStep[0m  [20/26], [94mLoss[0m : 4.98903
[1mStep[0m  [22/26], [94mLoss[0m : 5.06271
[1mStep[0m  [24/26], [94mLoss[0m : 4.94524

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.369, [92mTest[0m: 5.052, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.09114
[1mStep[0m  [2/26], [94mLoss[0m : 4.80523
[1mStep[0m  [4/26], [94mLoss[0m : 4.93479
[1mStep[0m  [6/26], [94mLoss[0m : 4.81018
[1mStep[0m  [8/26], [94mLoss[0m : 4.74069
[1mStep[0m  [10/26], [94mLoss[0m : 4.62512
[1mStep[0m  [12/26], [94mLoss[0m : 4.61003
[1mStep[0m  [14/26], [94mLoss[0m : 4.59724
[1mStep[0m  [16/26], [94mLoss[0m : 4.39794
[1mStep[0m  [18/26], [94mLoss[0m : 4.30802
[1mStep[0m  [20/26], [94mLoss[0m : 4.46439
[1mStep[0m  [22/26], [94mLoss[0m : 4.25614
[1mStep[0m  [24/26], [94mLoss[0m : 4.17763

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.582, [92mTest[0m: 4.255, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.13366
[1mStep[0m  [2/26], [94mLoss[0m : 4.06419
[1mStep[0m  [4/26], [94mLoss[0m : 4.21625
[1mStep[0m  [6/26], [94mLoss[0m : 4.05905
[1mStep[0m  [8/26], [94mLoss[0m : 4.13977
[1mStep[0m  [10/26], [94mLoss[0m : 4.18620
[1mStep[0m  [12/26], [94mLoss[0m : 3.86585
[1mStep[0m  [14/26], [94mLoss[0m : 4.01013
[1mStep[0m  [16/26], [94mLoss[0m : 3.65201
[1mStep[0m  [18/26], [94mLoss[0m : 3.67008
[1mStep[0m  [20/26], [94mLoss[0m : 3.71416
[1mStep[0m  [22/26], [94mLoss[0m : 3.51208
[1mStep[0m  [24/26], [94mLoss[0m : 3.35170

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.869, [92mTest[0m: 3.700, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.57313
[1mStep[0m  [2/26], [94mLoss[0m : 3.47903
[1mStep[0m  [4/26], [94mLoss[0m : 3.60187
[1mStep[0m  [6/26], [94mLoss[0m : 3.36835
[1mStep[0m  [8/26], [94mLoss[0m : 3.22478
[1mStep[0m  [10/26], [94mLoss[0m : 3.22889
[1mStep[0m  [12/26], [94mLoss[0m : 3.13349
[1mStep[0m  [14/26], [94mLoss[0m : 3.49612
[1mStep[0m  [16/26], [94mLoss[0m : 3.27662
[1mStep[0m  [18/26], [94mLoss[0m : 3.16241
[1mStep[0m  [20/26], [94mLoss[0m : 3.21840
[1mStep[0m  [22/26], [94mLoss[0m : 3.29899
[1mStep[0m  [24/26], [94mLoss[0m : 2.92367

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.281, [92mTest[0m: 3.129, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.04051
[1mStep[0m  [2/26], [94mLoss[0m : 3.08058
[1mStep[0m  [4/26], [94mLoss[0m : 2.79673
[1mStep[0m  [6/26], [94mLoss[0m : 2.89829
[1mStep[0m  [8/26], [94mLoss[0m : 2.83407
[1mStep[0m  [10/26], [94mLoss[0m : 2.89893
[1mStep[0m  [12/26], [94mLoss[0m : 2.93473
[1mStep[0m  [14/26], [94mLoss[0m : 2.85049
[1mStep[0m  [16/26], [94mLoss[0m : 2.80442
[1mStep[0m  [18/26], [94mLoss[0m : 2.82991
[1mStep[0m  [20/26], [94mLoss[0m : 2.68337
[1mStep[0m  [22/26], [94mLoss[0m : 2.70130
[1mStep[0m  [24/26], [94mLoss[0m : 2.74554

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.855, [92mTest[0m: 2.756, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59228
[1mStep[0m  [2/26], [94mLoss[0m : 2.90255
[1mStep[0m  [4/26], [94mLoss[0m : 2.80204
[1mStep[0m  [6/26], [94mLoss[0m : 2.50955
[1mStep[0m  [8/26], [94mLoss[0m : 2.70347
[1mStep[0m  [10/26], [94mLoss[0m : 2.62752
[1mStep[0m  [12/26], [94mLoss[0m : 2.65654
[1mStep[0m  [14/26], [94mLoss[0m : 2.70585
[1mStep[0m  [16/26], [94mLoss[0m : 2.69117
[1mStep[0m  [18/26], [94mLoss[0m : 2.74491
[1mStep[0m  [20/26], [94mLoss[0m : 2.50481
[1mStep[0m  [22/26], [94mLoss[0m : 2.58517
[1mStep[0m  [24/26], [94mLoss[0m : 2.83537

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57690
[1mStep[0m  [2/26], [94mLoss[0m : 2.64847
[1mStep[0m  [4/26], [94mLoss[0m : 2.65870
[1mStep[0m  [6/26], [94mLoss[0m : 2.56245
[1mStep[0m  [8/26], [94mLoss[0m : 2.58587
[1mStep[0m  [10/26], [94mLoss[0m : 2.43517
[1mStep[0m  [12/26], [94mLoss[0m : 2.73803
[1mStep[0m  [14/26], [94mLoss[0m : 2.61996
[1mStep[0m  [16/26], [94mLoss[0m : 2.78508
[1mStep[0m  [18/26], [94mLoss[0m : 2.60368
[1mStep[0m  [20/26], [94mLoss[0m : 2.63428
[1mStep[0m  [22/26], [94mLoss[0m : 2.63480
[1mStep[0m  [24/26], [94mLoss[0m : 2.62014

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42258
[1mStep[0m  [2/26], [94mLoss[0m : 2.48554
[1mStep[0m  [4/26], [94mLoss[0m : 2.94040
[1mStep[0m  [6/26], [94mLoss[0m : 2.55504
[1mStep[0m  [8/26], [94mLoss[0m : 2.75604
[1mStep[0m  [10/26], [94mLoss[0m : 2.42195
[1mStep[0m  [12/26], [94mLoss[0m : 2.38391
[1mStep[0m  [14/26], [94mLoss[0m : 2.55621
[1mStep[0m  [16/26], [94mLoss[0m : 2.51116
[1mStep[0m  [18/26], [94mLoss[0m : 2.64970
[1mStep[0m  [20/26], [94mLoss[0m : 2.32310
[1mStep[0m  [22/26], [94mLoss[0m : 2.55029
[1mStep[0m  [24/26], [94mLoss[0m : 2.47915

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.74904
[1mStep[0m  [2/26], [94mLoss[0m : 2.60933
[1mStep[0m  [4/26], [94mLoss[0m : 2.46116
[1mStep[0m  [6/26], [94mLoss[0m : 2.53754
[1mStep[0m  [8/26], [94mLoss[0m : 2.57667
[1mStep[0m  [10/26], [94mLoss[0m : 2.83573
[1mStep[0m  [12/26], [94mLoss[0m : 2.65624
[1mStep[0m  [14/26], [94mLoss[0m : 2.57096
[1mStep[0m  [16/26], [94mLoss[0m : 2.58647
[1mStep[0m  [18/26], [94mLoss[0m : 2.63227
[1mStep[0m  [20/26], [94mLoss[0m : 2.76594
[1mStep[0m  [22/26], [94mLoss[0m : 2.56501
[1mStep[0m  [24/26], [94mLoss[0m : 2.41459

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.54660
[1mStep[0m  [2/26], [94mLoss[0m : 2.55177
[1mStep[0m  [4/26], [94mLoss[0m : 2.60851
[1mStep[0m  [6/26], [94mLoss[0m : 2.49593
[1mStep[0m  [8/26], [94mLoss[0m : 2.51202
[1mStep[0m  [10/26], [94mLoss[0m : 2.58488
[1mStep[0m  [12/26], [94mLoss[0m : 2.63772
[1mStep[0m  [14/26], [94mLoss[0m : 2.88849
[1mStep[0m  [16/26], [94mLoss[0m : 2.62659
[1mStep[0m  [18/26], [94mLoss[0m : 2.45229
[1mStep[0m  [20/26], [94mLoss[0m : 2.39061
[1mStep[0m  [22/26], [94mLoss[0m : 2.57980
[1mStep[0m  [24/26], [94mLoss[0m : 2.48798

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52375
[1mStep[0m  [2/26], [94mLoss[0m : 2.71703
[1mStep[0m  [4/26], [94mLoss[0m : 2.85360
[1mStep[0m  [6/26], [94mLoss[0m : 2.42698
[1mStep[0m  [8/26], [94mLoss[0m : 2.60234
[1mStep[0m  [10/26], [94mLoss[0m : 2.43596
[1mStep[0m  [12/26], [94mLoss[0m : 2.52465
[1mStep[0m  [14/26], [94mLoss[0m : 2.63638
[1mStep[0m  [16/26], [94mLoss[0m : 2.38043
[1mStep[0m  [18/26], [94mLoss[0m : 2.72571
[1mStep[0m  [20/26], [94mLoss[0m : 2.62783
[1mStep[0m  [22/26], [94mLoss[0m : 2.47020
[1mStep[0m  [24/26], [94mLoss[0m : 2.56251

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63742
[1mStep[0m  [2/26], [94mLoss[0m : 2.64516
[1mStep[0m  [4/26], [94mLoss[0m : 2.65073
[1mStep[0m  [6/26], [94mLoss[0m : 2.52850
[1mStep[0m  [8/26], [94mLoss[0m : 2.45331
[1mStep[0m  [10/26], [94mLoss[0m : 2.78679
[1mStep[0m  [12/26], [94mLoss[0m : 2.46180
[1mStep[0m  [14/26], [94mLoss[0m : 2.59216
[1mStep[0m  [16/26], [94mLoss[0m : 2.59796
[1mStep[0m  [18/26], [94mLoss[0m : 2.47650
[1mStep[0m  [20/26], [94mLoss[0m : 2.68911
[1mStep[0m  [22/26], [94mLoss[0m : 2.62865
[1mStep[0m  [24/26], [94mLoss[0m : 2.41038

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.430, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45791
[1mStep[0m  [2/26], [94mLoss[0m : 2.45081
[1mStep[0m  [4/26], [94mLoss[0m : 2.61920
[1mStep[0m  [6/26], [94mLoss[0m : 2.60485
[1mStep[0m  [8/26], [94mLoss[0m : 2.57484
[1mStep[0m  [10/26], [94mLoss[0m : 2.61155
[1mStep[0m  [12/26], [94mLoss[0m : 2.58965
[1mStep[0m  [14/26], [94mLoss[0m : 2.46697
[1mStep[0m  [16/26], [94mLoss[0m : 2.52566
[1mStep[0m  [18/26], [94mLoss[0m : 2.58335
[1mStep[0m  [20/26], [94mLoss[0m : 2.57358
[1mStep[0m  [22/26], [94mLoss[0m : 2.61955
[1mStep[0m  [24/26], [94mLoss[0m : 2.57699

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.435, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71819
[1mStep[0m  [2/26], [94mLoss[0m : 2.71839
[1mStep[0m  [4/26], [94mLoss[0m : 2.65798
[1mStep[0m  [6/26], [94mLoss[0m : 2.44907
[1mStep[0m  [8/26], [94mLoss[0m : 2.67500
[1mStep[0m  [10/26], [94mLoss[0m : 2.64712
[1mStep[0m  [12/26], [94mLoss[0m : 2.57723
[1mStep[0m  [14/26], [94mLoss[0m : 2.56064
[1mStep[0m  [16/26], [94mLoss[0m : 2.59906
[1mStep[0m  [18/26], [94mLoss[0m : 2.58188
[1mStep[0m  [20/26], [94mLoss[0m : 2.50223
[1mStep[0m  [22/26], [94mLoss[0m : 2.48063
[1mStep[0m  [24/26], [94mLoss[0m : 2.65046

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.422, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64020
[1mStep[0m  [2/26], [94mLoss[0m : 2.45585
[1mStep[0m  [4/26], [94mLoss[0m : 2.34737
[1mStep[0m  [6/26], [94mLoss[0m : 2.37039
[1mStep[0m  [8/26], [94mLoss[0m : 2.57050
[1mStep[0m  [10/26], [94mLoss[0m : 2.57736
[1mStep[0m  [12/26], [94mLoss[0m : 2.48706
[1mStep[0m  [14/26], [94mLoss[0m : 2.38013
[1mStep[0m  [16/26], [94mLoss[0m : 2.71392
[1mStep[0m  [18/26], [94mLoss[0m : 2.48607
[1mStep[0m  [20/26], [94mLoss[0m : 2.54768
[1mStep[0m  [22/26], [94mLoss[0m : 2.45588
[1mStep[0m  [24/26], [94mLoss[0m : 2.66049

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57916
[1mStep[0m  [2/26], [94mLoss[0m : 2.40837
[1mStep[0m  [4/26], [94mLoss[0m : 2.67652
[1mStep[0m  [6/26], [94mLoss[0m : 2.80113
[1mStep[0m  [8/26], [94mLoss[0m : 2.54384
[1mStep[0m  [10/26], [94mLoss[0m : 2.51731
[1mStep[0m  [12/26], [94mLoss[0m : 2.54123
[1mStep[0m  [14/26], [94mLoss[0m : 2.66075
[1mStep[0m  [16/26], [94mLoss[0m : 2.46642
[1mStep[0m  [18/26], [94mLoss[0m : 2.51817
[1mStep[0m  [20/26], [94mLoss[0m : 2.62737
[1mStep[0m  [22/26], [94mLoss[0m : 2.35794
[1mStep[0m  [24/26], [94mLoss[0m : 2.47359

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.423, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49322
[1mStep[0m  [2/26], [94mLoss[0m : 2.55160
[1mStep[0m  [4/26], [94mLoss[0m : 2.59134
[1mStep[0m  [6/26], [94mLoss[0m : 2.57308
[1mStep[0m  [8/26], [94mLoss[0m : 2.45256
[1mStep[0m  [10/26], [94mLoss[0m : 2.53340
[1mStep[0m  [12/26], [94mLoss[0m : 2.58980
[1mStep[0m  [14/26], [94mLoss[0m : 2.37669
[1mStep[0m  [16/26], [94mLoss[0m : 2.67901
[1mStep[0m  [18/26], [94mLoss[0m : 2.45968
[1mStep[0m  [20/26], [94mLoss[0m : 2.36563
[1mStep[0m  [22/26], [94mLoss[0m : 2.47355
[1mStep[0m  [24/26], [94mLoss[0m : 2.39577

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.400, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39443
[1mStep[0m  [2/26], [94mLoss[0m : 2.65656
[1mStep[0m  [4/26], [94mLoss[0m : 2.52968
[1mStep[0m  [6/26], [94mLoss[0m : 2.36611
[1mStep[0m  [8/26], [94mLoss[0m : 2.56920
[1mStep[0m  [10/26], [94mLoss[0m : 2.45980
[1mStep[0m  [12/26], [94mLoss[0m : 2.55724
[1mStep[0m  [14/26], [94mLoss[0m : 2.57276
[1mStep[0m  [16/26], [94mLoss[0m : 2.52824
[1mStep[0m  [18/26], [94mLoss[0m : 2.63365
[1mStep[0m  [20/26], [94mLoss[0m : 2.50624
[1mStep[0m  [22/26], [94mLoss[0m : 2.39661
[1mStep[0m  [24/26], [94mLoss[0m : 2.45352

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.411, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58670
[1mStep[0m  [2/26], [94mLoss[0m : 2.59089
[1mStep[0m  [4/26], [94mLoss[0m : 2.53206
[1mStep[0m  [6/26], [94mLoss[0m : 2.40959
[1mStep[0m  [8/26], [94mLoss[0m : 2.55788
[1mStep[0m  [10/26], [94mLoss[0m : 2.44528
[1mStep[0m  [12/26], [94mLoss[0m : 2.36711
[1mStep[0m  [14/26], [94mLoss[0m : 2.49360
[1mStep[0m  [16/26], [94mLoss[0m : 2.59495
[1mStep[0m  [18/26], [94mLoss[0m : 2.70198
[1mStep[0m  [20/26], [94mLoss[0m : 2.63483
[1mStep[0m  [22/26], [94mLoss[0m : 2.75367
[1mStep[0m  [24/26], [94mLoss[0m : 2.46148

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.417, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43847
[1mStep[0m  [2/26], [94mLoss[0m : 2.54363
[1mStep[0m  [4/26], [94mLoss[0m : 2.57399
[1mStep[0m  [6/26], [94mLoss[0m : 2.62277
[1mStep[0m  [8/26], [94mLoss[0m : 2.42829
[1mStep[0m  [10/26], [94mLoss[0m : 2.44177
[1mStep[0m  [12/26], [94mLoss[0m : 2.41750
[1mStep[0m  [14/26], [94mLoss[0m : 2.65943
[1mStep[0m  [16/26], [94mLoss[0m : 2.59264
[1mStep[0m  [18/26], [94mLoss[0m : 2.51485
[1mStep[0m  [20/26], [94mLoss[0m : 2.55745
[1mStep[0m  [22/26], [94mLoss[0m : 2.63597
[1mStep[0m  [24/26], [94mLoss[0m : 2.51862

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.409, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45608
[1mStep[0m  [2/26], [94mLoss[0m : 2.57940
[1mStep[0m  [4/26], [94mLoss[0m : 2.45118
[1mStep[0m  [6/26], [94mLoss[0m : 2.47852
[1mStep[0m  [8/26], [94mLoss[0m : 2.37577
[1mStep[0m  [10/26], [94mLoss[0m : 2.60506
[1mStep[0m  [12/26], [94mLoss[0m : 2.47523
[1mStep[0m  [14/26], [94mLoss[0m : 2.55380
[1mStep[0m  [16/26], [94mLoss[0m : 2.24104
[1mStep[0m  [18/26], [94mLoss[0m : 2.49713
[1mStep[0m  [20/26], [94mLoss[0m : 2.55481
[1mStep[0m  [22/26], [94mLoss[0m : 2.54831
[1mStep[0m  [24/26], [94mLoss[0m : 2.63589

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.392, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58514
[1mStep[0m  [2/26], [94mLoss[0m : 2.52456
[1mStep[0m  [4/26], [94mLoss[0m : 2.40428
[1mStep[0m  [6/26], [94mLoss[0m : 2.47564
[1mStep[0m  [8/26], [94mLoss[0m : 2.47332
[1mStep[0m  [10/26], [94mLoss[0m : 2.39043
[1mStep[0m  [12/26], [94mLoss[0m : 2.43423
[1mStep[0m  [14/26], [94mLoss[0m : 2.44128
[1mStep[0m  [16/26], [94mLoss[0m : 2.46583
[1mStep[0m  [18/26], [94mLoss[0m : 2.65818
[1mStep[0m  [20/26], [94mLoss[0m : 2.41170
[1mStep[0m  [22/26], [94mLoss[0m : 2.61168
[1mStep[0m  [24/26], [94mLoss[0m : 2.61270

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.409, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57375
[1mStep[0m  [2/26], [94mLoss[0m : 2.41544
[1mStep[0m  [4/26], [94mLoss[0m : 2.54539
[1mStep[0m  [6/26], [94mLoss[0m : 2.54687
[1mStep[0m  [8/26], [94mLoss[0m : 2.44687
[1mStep[0m  [10/26], [94mLoss[0m : 2.49328
[1mStep[0m  [12/26], [94mLoss[0m : 2.44561
[1mStep[0m  [14/26], [94mLoss[0m : 2.47438
[1mStep[0m  [16/26], [94mLoss[0m : 2.48090
[1mStep[0m  [18/26], [94mLoss[0m : 2.58617
[1mStep[0m  [20/26], [94mLoss[0m : 2.59987
[1mStep[0m  [22/26], [94mLoss[0m : 2.50912
[1mStep[0m  [24/26], [94mLoss[0m : 2.51934

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.393, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.398
====================================

Phase 1 - Evaluation MAE:  2.3980186719160814
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.44654
[1mStep[0m  [2/26], [94mLoss[0m : 2.57472
[1mStep[0m  [4/26], [94mLoss[0m : 2.54600
[1mStep[0m  [6/26], [94mLoss[0m : 2.58191
[1mStep[0m  [8/26], [94mLoss[0m : 2.75548
[1mStep[0m  [10/26], [94mLoss[0m : 2.66013
[1mStep[0m  [12/26], [94mLoss[0m : 2.51710
[1mStep[0m  [14/26], [94mLoss[0m : 2.43001
[1mStep[0m  [16/26], [94mLoss[0m : 2.49136
[1mStep[0m  [18/26], [94mLoss[0m : 2.72050
[1mStep[0m  [20/26], [94mLoss[0m : 2.63090
[1mStep[0m  [22/26], [94mLoss[0m : 2.52834
[1mStep[0m  [24/26], [94mLoss[0m : 2.63982

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62369
[1mStep[0m  [2/26], [94mLoss[0m : 2.62853
[1mStep[0m  [4/26], [94mLoss[0m : 2.50244
[1mStep[0m  [6/26], [94mLoss[0m : 2.60669
[1mStep[0m  [8/26], [94mLoss[0m : 2.58803
[1mStep[0m  [10/26], [94mLoss[0m : 2.65618
[1mStep[0m  [12/26], [94mLoss[0m : 2.49257
[1mStep[0m  [14/26], [94mLoss[0m : 2.35992
[1mStep[0m  [16/26], [94mLoss[0m : 2.47196
[1mStep[0m  [18/26], [94mLoss[0m : 2.50459
[1mStep[0m  [20/26], [94mLoss[0m : 2.62109
[1mStep[0m  [22/26], [94mLoss[0m : 2.53750
[1mStep[0m  [24/26], [94mLoss[0m : 2.58037

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57934
[1mStep[0m  [2/26], [94mLoss[0m : 2.51617
[1mStep[0m  [4/26], [94mLoss[0m : 2.44303
[1mStep[0m  [6/26], [94mLoss[0m : 2.59698
[1mStep[0m  [8/26], [94mLoss[0m : 2.52907
[1mStep[0m  [10/26], [94mLoss[0m : 2.45091
[1mStep[0m  [12/26], [94mLoss[0m : 2.61304
[1mStep[0m  [14/26], [94mLoss[0m : 2.43343
[1mStep[0m  [16/26], [94mLoss[0m : 2.56238
[1mStep[0m  [18/26], [94mLoss[0m : 2.59527
[1mStep[0m  [20/26], [94mLoss[0m : 2.46713
[1mStep[0m  [22/26], [94mLoss[0m : 2.42564
[1mStep[0m  [24/26], [94mLoss[0m : 2.53110

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36431
[1mStep[0m  [2/26], [94mLoss[0m : 2.61648
[1mStep[0m  [4/26], [94mLoss[0m : 2.66574
[1mStep[0m  [6/26], [94mLoss[0m : 2.45799
[1mStep[0m  [8/26], [94mLoss[0m : 2.49479
[1mStep[0m  [10/26], [94mLoss[0m : 2.45885
[1mStep[0m  [12/26], [94mLoss[0m : 2.40917
[1mStep[0m  [14/26], [94mLoss[0m : 2.48754
[1mStep[0m  [16/26], [94mLoss[0m : 2.48372
[1mStep[0m  [18/26], [94mLoss[0m : 2.47040
[1mStep[0m  [20/26], [94mLoss[0m : 2.43262
[1mStep[0m  [22/26], [94mLoss[0m : 2.42902
[1mStep[0m  [24/26], [94mLoss[0m : 2.47923

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52617
[1mStep[0m  [2/26], [94mLoss[0m : 2.48034
[1mStep[0m  [4/26], [94mLoss[0m : 2.63229
[1mStep[0m  [6/26], [94mLoss[0m : 2.30195
[1mStep[0m  [8/26], [94mLoss[0m : 2.50337
[1mStep[0m  [10/26], [94mLoss[0m : 2.26646
[1mStep[0m  [12/26], [94mLoss[0m : 2.41027
[1mStep[0m  [14/26], [94mLoss[0m : 2.29942
[1mStep[0m  [16/26], [94mLoss[0m : 2.34474
[1mStep[0m  [18/26], [94mLoss[0m : 2.51827
[1mStep[0m  [20/26], [94mLoss[0m : 2.48473
[1mStep[0m  [22/26], [94mLoss[0m : 2.65842
[1mStep[0m  [24/26], [94mLoss[0m : 2.46745

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38956
[1mStep[0m  [2/26], [94mLoss[0m : 2.59133
[1mStep[0m  [4/26], [94mLoss[0m : 2.37996
[1mStep[0m  [6/26], [94mLoss[0m : 2.37656
[1mStep[0m  [8/26], [94mLoss[0m : 2.31344
[1mStep[0m  [10/26], [94mLoss[0m : 2.32647
[1mStep[0m  [12/26], [94mLoss[0m : 2.50022
[1mStep[0m  [14/26], [94mLoss[0m : 2.35879
[1mStep[0m  [16/26], [94mLoss[0m : 2.41515
[1mStep[0m  [18/26], [94mLoss[0m : 2.23029
[1mStep[0m  [20/26], [94mLoss[0m : 2.43444
[1mStep[0m  [22/26], [94mLoss[0m : 2.52211
[1mStep[0m  [24/26], [94mLoss[0m : 2.34448

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44110
[1mStep[0m  [2/26], [94mLoss[0m : 2.31477
[1mStep[0m  [4/26], [94mLoss[0m : 2.44362
[1mStep[0m  [6/26], [94mLoss[0m : 2.28064
[1mStep[0m  [8/26], [94mLoss[0m : 2.30702
[1mStep[0m  [10/26], [94mLoss[0m : 2.41074
[1mStep[0m  [12/26], [94mLoss[0m : 2.42234
[1mStep[0m  [14/26], [94mLoss[0m : 2.47589
[1mStep[0m  [16/26], [94mLoss[0m : 2.38056
[1mStep[0m  [18/26], [94mLoss[0m : 2.39685
[1mStep[0m  [20/26], [94mLoss[0m : 2.28421
[1mStep[0m  [22/26], [94mLoss[0m : 2.52486
[1mStep[0m  [24/26], [94mLoss[0m : 2.36629

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33403
[1mStep[0m  [2/26], [94mLoss[0m : 2.22548
[1mStep[0m  [4/26], [94mLoss[0m : 2.39975
[1mStep[0m  [6/26], [94mLoss[0m : 2.42059
[1mStep[0m  [8/26], [94mLoss[0m : 2.36619
[1mStep[0m  [10/26], [94mLoss[0m : 2.39395
[1mStep[0m  [12/26], [94mLoss[0m : 2.30233
[1mStep[0m  [14/26], [94mLoss[0m : 2.26709
[1mStep[0m  [16/26], [94mLoss[0m : 2.33381
[1mStep[0m  [18/26], [94mLoss[0m : 2.24235
[1mStep[0m  [20/26], [94mLoss[0m : 2.39709
[1mStep[0m  [22/26], [94mLoss[0m : 2.43814
[1mStep[0m  [24/26], [94mLoss[0m : 2.34636

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32103
[1mStep[0m  [2/26], [94mLoss[0m : 2.27819
[1mStep[0m  [4/26], [94mLoss[0m : 2.39162
[1mStep[0m  [6/26], [94mLoss[0m : 2.52628
[1mStep[0m  [8/26], [94mLoss[0m : 2.31245
[1mStep[0m  [10/26], [94mLoss[0m : 2.32077
[1mStep[0m  [12/26], [94mLoss[0m : 2.27031
[1mStep[0m  [14/26], [94mLoss[0m : 2.29149
[1mStep[0m  [16/26], [94mLoss[0m : 2.33171
[1mStep[0m  [18/26], [94mLoss[0m : 2.24833
[1mStep[0m  [20/26], [94mLoss[0m : 2.18397
[1mStep[0m  [22/26], [94mLoss[0m : 2.38096
[1mStep[0m  [24/26], [94mLoss[0m : 2.26696

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.34001
[1mStep[0m  [2/26], [94mLoss[0m : 2.38733
[1mStep[0m  [4/26], [94mLoss[0m : 2.38915
[1mStep[0m  [6/26], [94mLoss[0m : 2.17898
[1mStep[0m  [8/26], [94mLoss[0m : 2.32986
[1mStep[0m  [10/26], [94mLoss[0m : 2.33799
[1mStep[0m  [12/26], [94mLoss[0m : 2.43464
[1mStep[0m  [14/26], [94mLoss[0m : 2.32638
[1mStep[0m  [16/26], [94mLoss[0m : 2.33223
[1mStep[0m  [18/26], [94mLoss[0m : 2.33150
[1mStep[0m  [20/26], [94mLoss[0m : 2.30486
[1mStep[0m  [22/26], [94mLoss[0m : 2.14333
[1mStep[0m  [24/26], [94mLoss[0m : 2.20702

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.30256
[1mStep[0m  [2/26], [94mLoss[0m : 2.27785
[1mStep[0m  [4/26], [94mLoss[0m : 2.21405
[1mStep[0m  [6/26], [94mLoss[0m : 2.24026
[1mStep[0m  [8/26], [94mLoss[0m : 2.20536
[1mStep[0m  [10/26], [94mLoss[0m : 2.22325
[1mStep[0m  [12/26], [94mLoss[0m : 2.12937
[1mStep[0m  [14/26], [94mLoss[0m : 2.32035
[1mStep[0m  [16/26], [94mLoss[0m : 2.21569
[1mStep[0m  [18/26], [94mLoss[0m : 2.26201
[1mStep[0m  [20/26], [94mLoss[0m : 2.18110
[1mStep[0m  [22/26], [94mLoss[0m : 2.35154
[1mStep[0m  [24/26], [94mLoss[0m : 2.20056

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.253, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.18650
[1mStep[0m  [2/26], [94mLoss[0m : 2.30652
[1mStep[0m  [4/26], [94mLoss[0m : 2.19830
[1mStep[0m  [6/26], [94mLoss[0m : 2.18982
[1mStep[0m  [8/26], [94mLoss[0m : 2.30828
[1mStep[0m  [10/26], [94mLoss[0m : 2.28796
[1mStep[0m  [12/26], [94mLoss[0m : 2.22078
[1mStep[0m  [14/26], [94mLoss[0m : 2.29832
[1mStep[0m  [16/26], [94mLoss[0m : 2.18819
[1mStep[0m  [18/26], [94mLoss[0m : 2.28460
[1mStep[0m  [20/26], [94mLoss[0m : 2.28801
[1mStep[0m  [22/26], [94mLoss[0m : 2.18029
[1mStep[0m  [24/26], [94mLoss[0m : 2.37324

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.09968
[1mStep[0m  [2/26], [94mLoss[0m : 2.24350
[1mStep[0m  [4/26], [94mLoss[0m : 2.16568
[1mStep[0m  [6/26], [94mLoss[0m : 2.17823
[1mStep[0m  [8/26], [94mLoss[0m : 2.23242
[1mStep[0m  [10/26], [94mLoss[0m : 2.07023
[1mStep[0m  [12/26], [94mLoss[0m : 2.11780
[1mStep[0m  [14/26], [94mLoss[0m : 2.21037
[1mStep[0m  [16/26], [94mLoss[0m : 2.21903
[1mStep[0m  [18/26], [94mLoss[0m : 2.08005
[1mStep[0m  [20/26], [94mLoss[0m : 2.13875
[1mStep[0m  [22/26], [94mLoss[0m : 2.30586
[1mStep[0m  [24/26], [94mLoss[0m : 2.22596

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.184, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96564
[1mStep[0m  [2/26], [94mLoss[0m : 2.11295
[1mStep[0m  [4/26], [94mLoss[0m : 2.21786
[1mStep[0m  [6/26], [94mLoss[0m : 2.17554
[1mStep[0m  [8/26], [94mLoss[0m : 2.17201
[1mStep[0m  [10/26], [94mLoss[0m : 2.21642
[1mStep[0m  [12/26], [94mLoss[0m : 2.20706
[1mStep[0m  [14/26], [94mLoss[0m : 2.30810
[1mStep[0m  [16/26], [94mLoss[0m : 2.04139
[1mStep[0m  [18/26], [94mLoss[0m : 2.02605
[1mStep[0m  [20/26], [94mLoss[0m : 2.17813
[1mStep[0m  [22/26], [94mLoss[0m : 2.33765
[1mStep[0m  [24/26], [94mLoss[0m : 2.26522

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97577
[1mStep[0m  [2/26], [94mLoss[0m : 2.14735
[1mStep[0m  [4/26], [94mLoss[0m : 2.10934
[1mStep[0m  [6/26], [94mLoss[0m : 2.05136
[1mStep[0m  [8/26], [94mLoss[0m : 2.15768
[1mStep[0m  [10/26], [94mLoss[0m : 2.21546
[1mStep[0m  [12/26], [94mLoss[0m : 2.05858
[1mStep[0m  [14/26], [94mLoss[0m : 2.10793
[1mStep[0m  [16/26], [94mLoss[0m : 2.21725
[1mStep[0m  [18/26], [94mLoss[0m : 2.16892
[1mStep[0m  [20/26], [94mLoss[0m : 2.05173
[1mStep[0m  [22/26], [94mLoss[0m : 2.13371
[1mStep[0m  [24/26], [94mLoss[0m : 2.03131

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10291
[1mStep[0m  [2/26], [94mLoss[0m : 2.18365
[1mStep[0m  [4/26], [94mLoss[0m : 2.16078
[1mStep[0m  [6/26], [94mLoss[0m : 2.08359
[1mStep[0m  [8/26], [94mLoss[0m : 2.05157
[1mStep[0m  [10/26], [94mLoss[0m : 2.06675
[1mStep[0m  [12/26], [94mLoss[0m : 2.10833
[1mStep[0m  [14/26], [94mLoss[0m : 2.06994
[1mStep[0m  [16/26], [94mLoss[0m : 2.05582
[1mStep[0m  [18/26], [94mLoss[0m : 2.08515
[1mStep[0m  [20/26], [94mLoss[0m : 2.16846
[1mStep[0m  [22/26], [94mLoss[0m : 2.10985
[1mStep[0m  [24/26], [94mLoss[0m : 2.06964

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97786
[1mStep[0m  [2/26], [94mLoss[0m : 1.94995
[1mStep[0m  [4/26], [94mLoss[0m : 1.95973
[1mStep[0m  [6/26], [94mLoss[0m : 2.06960
[1mStep[0m  [8/26], [94mLoss[0m : 2.02915
[1mStep[0m  [10/26], [94mLoss[0m : 2.21199
[1mStep[0m  [12/26], [94mLoss[0m : 2.08773
[1mStep[0m  [14/26], [94mLoss[0m : 2.20543
[1mStep[0m  [16/26], [94mLoss[0m : 2.02670
[1mStep[0m  [18/26], [94mLoss[0m : 2.22888
[1mStep[0m  [20/26], [94mLoss[0m : 2.03166
[1mStep[0m  [22/26], [94mLoss[0m : 2.04884
[1mStep[0m  [24/26], [94mLoss[0m : 2.02061

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01372
[1mStep[0m  [2/26], [94mLoss[0m : 1.98314
[1mStep[0m  [4/26], [94mLoss[0m : 2.18558
[1mStep[0m  [6/26], [94mLoss[0m : 1.95928
[1mStep[0m  [8/26], [94mLoss[0m : 2.03761
[1mStep[0m  [10/26], [94mLoss[0m : 1.98996
[1mStep[0m  [12/26], [94mLoss[0m : 2.06704
[1mStep[0m  [14/26], [94mLoss[0m : 2.01857
[1mStep[0m  [16/26], [94mLoss[0m : 1.84924
[1mStep[0m  [18/26], [94mLoss[0m : 2.06064
[1mStep[0m  [20/26], [94mLoss[0m : 2.13768
[1mStep[0m  [22/26], [94mLoss[0m : 1.99570
[1mStep[0m  [24/26], [94mLoss[0m : 2.00911

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.99250
[1mStep[0m  [2/26], [94mLoss[0m : 2.06202
[1mStep[0m  [4/26], [94mLoss[0m : 2.15006
[1mStep[0m  [6/26], [94mLoss[0m : 2.15513
[1mStep[0m  [8/26], [94mLoss[0m : 1.94399
[1mStep[0m  [10/26], [94mLoss[0m : 1.95264
[1mStep[0m  [12/26], [94mLoss[0m : 2.03400
[1mStep[0m  [14/26], [94mLoss[0m : 1.86741
[1mStep[0m  [16/26], [94mLoss[0m : 1.98486
[1mStep[0m  [18/26], [94mLoss[0m : 2.10061
[1mStep[0m  [20/26], [94mLoss[0m : 2.05707
[1mStep[0m  [22/26], [94mLoss[0m : 2.02006
[1mStep[0m  [24/26], [94mLoss[0m : 1.90406

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.84940
[1mStep[0m  [2/26], [94mLoss[0m : 2.02228
[1mStep[0m  [4/26], [94mLoss[0m : 1.88777
[1mStep[0m  [6/26], [94mLoss[0m : 1.98492
[1mStep[0m  [8/26], [94mLoss[0m : 2.03583
[1mStep[0m  [10/26], [94mLoss[0m : 2.05700
[1mStep[0m  [12/26], [94mLoss[0m : 2.03591
[1mStep[0m  [14/26], [94mLoss[0m : 1.97481
[1mStep[0m  [16/26], [94mLoss[0m : 1.97267
[1mStep[0m  [18/26], [94mLoss[0m : 2.04492
[1mStep[0m  [20/26], [94mLoss[0m : 2.06889
[1mStep[0m  [22/26], [94mLoss[0m : 1.95593
[1mStep[0m  [24/26], [94mLoss[0m : 1.89970

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.413, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.95670
[1mStep[0m  [2/26], [94mLoss[0m : 1.90559
[1mStep[0m  [4/26], [94mLoss[0m : 1.92684
[1mStep[0m  [6/26], [94mLoss[0m : 1.82252
[1mStep[0m  [8/26], [94mLoss[0m : 1.92237
[1mStep[0m  [10/26], [94mLoss[0m : 1.93256
[1mStep[0m  [12/26], [94mLoss[0m : 2.02282
[1mStep[0m  [14/26], [94mLoss[0m : 1.87532
[1mStep[0m  [16/26], [94mLoss[0m : 1.99971
[1mStep[0m  [18/26], [94mLoss[0m : 1.98703
[1mStep[0m  [20/26], [94mLoss[0m : 1.99212
[1mStep[0m  [22/26], [94mLoss[0m : 1.92434
[1mStep[0m  [24/26], [94mLoss[0m : 1.98955

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.87084
[1mStep[0m  [2/26], [94mLoss[0m : 1.78454
[1mStep[0m  [4/26], [94mLoss[0m : 1.97551
[1mStep[0m  [6/26], [94mLoss[0m : 1.87487
[1mStep[0m  [8/26], [94mLoss[0m : 1.88569
[1mStep[0m  [10/26], [94mLoss[0m : 1.93369
[1mStep[0m  [12/26], [94mLoss[0m : 1.90125
[1mStep[0m  [14/26], [94mLoss[0m : 1.88376
[1mStep[0m  [16/26], [94mLoss[0m : 1.88322
[1mStep[0m  [18/26], [94mLoss[0m : 1.75914
[1mStep[0m  [20/26], [94mLoss[0m : 2.08919
[1mStep[0m  [22/26], [94mLoss[0m : 1.91328
[1mStep[0m  [24/26], [94mLoss[0m : 1.96361

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.676, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82415
[1mStep[0m  [2/26], [94mLoss[0m : 1.91049
[1mStep[0m  [4/26], [94mLoss[0m : 1.88063
[1mStep[0m  [6/26], [94mLoss[0m : 1.83111
[1mStep[0m  [8/26], [94mLoss[0m : 1.84527
[1mStep[0m  [10/26], [94mLoss[0m : 1.93206
[1mStep[0m  [12/26], [94mLoss[0m : 1.82303
[1mStep[0m  [14/26], [94mLoss[0m : 1.81116
[1mStep[0m  [16/26], [94mLoss[0m : 1.75018
[1mStep[0m  [18/26], [94mLoss[0m : 1.86390
[1mStep[0m  [20/26], [94mLoss[0m : 1.87936
[1mStep[0m  [22/26], [94mLoss[0m : 1.93762
[1mStep[0m  [24/26], [94mLoss[0m : 2.05283

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.407, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.90619
[1mStep[0m  [2/26], [94mLoss[0m : 1.90693
[1mStep[0m  [4/26], [94mLoss[0m : 1.82473
[1mStep[0m  [6/26], [94mLoss[0m : 1.83945
[1mStep[0m  [8/26], [94mLoss[0m : 1.78438
[1mStep[0m  [10/26], [94mLoss[0m : 1.98134
[1mStep[0m  [12/26], [94mLoss[0m : 1.89516
[1mStep[0m  [14/26], [94mLoss[0m : 1.96033
[1mStep[0m  [16/26], [94mLoss[0m : 1.99895
[1mStep[0m  [18/26], [94mLoss[0m : 1.88363
[1mStep[0m  [20/26], [94mLoss[0m : 1.85415
[1mStep[0m  [22/26], [94mLoss[0m : 1.93344
[1mStep[0m  [24/26], [94mLoss[0m : 1.80260

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.440, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.74963
[1mStep[0m  [2/26], [94mLoss[0m : 1.94003
[1mStep[0m  [4/26], [94mLoss[0m : 1.82946
[1mStep[0m  [6/26], [94mLoss[0m : 1.82980
[1mStep[0m  [8/26], [94mLoss[0m : 1.70146
[1mStep[0m  [10/26], [94mLoss[0m : 1.83042
[1mStep[0m  [12/26], [94mLoss[0m : 1.84882
[1mStep[0m  [14/26], [94mLoss[0m : 1.89470
[1mStep[0m  [16/26], [94mLoss[0m : 1.83697
[1mStep[0m  [18/26], [94mLoss[0m : 1.81704
[1mStep[0m  [20/26], [94mLoss[0m : 1.89736
[1mStep[0m  [22/26], [94mLoss[0m : 1.71432
[1mStep[0m  [24/26], [94mLoss[0m : 1.87814

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.440, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81801
[1mStep[0m  [2/26], [94mLoss[0m : 1.72211
[1mStep[0m  [4/26], [94mLoss[0m : 1.80066
[1mStep[0m  [6/26], [94mLoss[0m : 1.73057
[1mStep[0m  [8/26], [94mLoss[0m : 1.75242
[1mStep[0m  [10/26], [94mLoss[0m : 1.87297
[1mStep[0m  [12/26], [94mLoss[0m : 1.75335
[1mStep[0m  [14/26], [94mLoss[0m : 1.78781
[1mStep[0m  [16/26], [94mLoss[0m : 1.87692
[1mStep[0m  [18/26], [94mLoss[0m : 1.85743
[1mStep[0m  [20/26], [94mLoss[0m : 1.72537
[1mStep[0m  [22/26], [94mLoss[0m : 1.78588
[1mStep[0m  [24/26], [94mLoss[0m : 1.79645

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82494
[1mStep[0m  [2/26], [94mLoss[0m : 1.82648
[1mStep[0m  [4/26], [94mLoss[0m : 1.76568
[1mStep[0m  [6/26], [94mLoss[0m : 1.80918
[1mStep[0m  [8/26], [94mLoss[0m : 1.79790
[1mStep[0m  [10/26], [94mLoss[0m : 1.83886
[1mStep[0m  [12/26], [94mLoss[0m : 1.70207
[1mStep[0m  [14/26], [94mLoss[0m : 1.76066
[1mStep[0m  [16/26], [94mLoss[0m : 1.86631
[1mStep[0m  [18/26], [94mLoss[0m : 1.81009
[1mStep[0m  [20/26], [94mLoss[0m : 1.78274
[1mStep[0m  [22/26], [94mLoss[0m : 1.83598
[1mStep[0m  [24/26], [94mLoss[0m : 1.87411

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.81153
[1mStep[0m  [2/26], [94mLoss[0m : 1.78624
[1mStep[0m  [4/26], [94mLoss[0m : 1.71144
[1mStep[0m  [6/26], [94mLoss[0m : 1.73656
[1mStep[0m  [8/26], [94mLoss[0m : 1.66010
[1mStep[0m  [10/26], [94mLoss[0m : 1.71415
[1mStep[0m  [12/26], [94mLoss[0m : 1.70437
[1mStep[0m  [14/26], [94mLoss[0m : 1.81071
[1mStep[0m  [16/26], [94mLoss[0m : 1.94016
[1mStep[0m  [18/26], [94mLoss[0m : 1.85826
[1mStep[0m  [20/26], [94mLoss[0m : 1.72926
[1mStep[0m  [22/26], [94mLoss[0m : 1.87320
[1mStep[0m  [24/26], [94mLoss[0m : 1.72175

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73853
[1mStep[0m  [2/26], [94mLoss[0m : 1.68847
[1mStep[0m  [4/26], [94mLoss[0m : 1.67051
[1mStep[0m  [6/26], [94mLoss[0m : 1.85553
[1mStep[0m  [8/26], [94mLoss[0m : 1.78521
[1mStep[0m  [10/26], [94mLoss[0m : 1.91604
[1mStep[0m  [12/26], [94mLoss[0m : 1.79560
[1mStep[0m  [14/26], [94mLoss[0m : 1.73592
[1mStep[0m  [16/26], [94mLoss[0m : 1.60328
[1mStep[0m  [18/26], [94mLoss[0m : 1.77458
[1mStep[0m  [20/26], [94mLoss[0m : 1.84332
[1mStep[0m  [22/26], [94mLoss[0m : 1.70169
[1mStep[0m  [24/26], [94mLoss[0m : 1.90937

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.756, [92mTest[0m: 2.452, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.68919
[1mStep[0m  [2/26], [94mLoss[0m : 1.64430
[1mStep[0m  [4/26], [94mLoss[0m : 1.74019
[1mStep[0m  [6/26], [94mLoss[0m : 1.72661
[1mStep[0m  [8/26], [94mLoss[0m : 1.69109
[1mStep[0m  [10/26], [94mLoss[0m : 1.69171
[1mStep[0m  [12/26], [94mLoss[0m : 1.70718
[1mStep[0m  [14/26], [94mLoss[0m : 1.72724
[1mStep[0m  [16/26], [94mLoss[0m : 1.79373
[1mStep[0m  [18/26], [94mLoss[0m : 1.68941
[1mStep[0m  [20/26], [94mLoss[0m : 1.67773
[1mStep[0m  [22/26], [94mLoss[0m : 1.82303
[1mStep[0m  [24/26], [94mLoss[0m : 1.77920

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.442, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.463
====================================

Phase 2 - Evaluation MAE:  2.4633358808664174
MAE score P1       2.398019
MAE score P2       2.463336
loss               1.734085
learning_rate      0.007525
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.89244
[1mStep[0m  [2/26], [94mLoss[0m : 10.74448
[1mStep[0m  [4/26], [94mLoss[0m : 10.76622
[1mStep[0m  [6/26], [94mLoss[0m : 10.62381
[1mStep[0m  [8/26], [94mLoss[0m : 10.71686
[1mStep[0m  [10/26], [94mLoss[0m : 10.54781
[1mStep[0m  [12/26], [94mLoss[0m : 10.10806
[1mStep[0m  [14/26], [94mLoss[0m : 9.92570
[1mStep[0m  [16/26], [94mLoss[0m : 9.92272
[1mStep[0m  [18/26], [94mLoss[0m : 9.59961
[1mStep[0m  [20/26], [94mLoss[0m : 9.09695
[1mStep[0m  [22/26], [94mLoss[0m : 9.22329
[1mStep[0m  [24/26], [94mLoss[0m : 8.85373

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.054, [92mTest[0m: 10.810, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.42490
[1mStep[0m  [2/26], [94mLoss[0m : 8.15381
[1mStep[0m  [4/26], [94mLoss[0m : 8.07998
[1mStep[0m  [6/26], [94mLoss[0m : 7.74039
[1mStep[0m  [8/26], [94mLoss[0m : 7.35127
[1mStep[0m  [10/26], [94mLoss[0m : 7.04905
[1mStep[0m  [12/26], [94mLoss[0m : 7.04153
[1mStep[0m  [14/26], [94mLoss[0m : 6.81400
[1mStep[0m  [16/26], [94mLoss[0m : 6.56289
[1mStep[0m  [18/26], [94mLoss[0m : 6.18440
[1mStep[0m  [20/26], [94mLoss[0m : 6.45765
[1mStep[0m  [22/26], [94mLoss[0m : 6.15965
[1mStep[0m  [24/26], [94mLoss[0m : 6.07760

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.047, [92mTest[0m: 8.514, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.35064
[1mStep[0m  [2/26], [94mLoss[0m : 5.22829
[1mStep[0m  [4/26], [94mLoss[0m : 5.46014
[1mStep[0m  [6/26], [94mLoss[0m : 4.93103
[1mStep[0m  [8/26], [94mLoss[0m : 4.66938
[1mStep[0m  [10/26], [94mLoss[0m : 4.10492
[1mStep[0m  [12/26], [94mLoss[0m : 4.32853
[1mStep[0m  [14/26], [94mLoss[0m : 3.91559
[1mStep[0m  [16/26], [94mLoss[0m : 3.77379
[1mStep[0m  [18/26], [94mLoss[0m : 3.78699
[1mStep[0m  [20/26], [94mLoss[0m : 3.42212
[1mStep[0m  [22/26], [94mLoss[0m : 3.24566
[1mStep[0m  [24/26], [94mLoss[0m : 3.16536

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.187, [92mTest[0m: 5.001, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.05516
[1mStep[0m  [2/26], [94mLoss[0m : 2.87591
[1mStep[0m  [4/26], [94mLoss[0m : 2.79082
[1mStep[0m  [6/26], [94mLoss[0m : 2.76725
[1mStep[0m  [8/26], [94mLoss[0m : 2.47960
[1mStep[0m  [10/26], [94mLoss[0m : 2.86579
[1mStep[0m  [12/26], [94mLoss[0m : 2.53335
[1mStep[0m  [14/26], [94mLoss[0m : 2.64476
[1mStep[0m  [16/26], [94mLoss[0m : 2.80283
[1mStep[0m  [18/26], [94mLoss[0m : 2.87726
[1mStep[0m  [20/26], [94mLoss[0m : 2.62586
[1mStep[0m  [22/26], [94mLoss[0m : 2.61434
[1mStep[0m  [24/26], [94mLoss[0m : 2.67283

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.689, [92mTest[0m: 3.589, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53834
[1mStep[0m  [2/26], [94mLoss[0m : 2.61729
[1mStep[0m  [4/26], [94mLoss[0m : 2.52380
[1mStep[0m  [6/26], [94mLoss[0m : 2.44116
[1mStep[0m  [8/26], [94mLoss[0m : 2.74585
[1mStep[0m  [10/26], [94mLoss[0m : 2.61447
[1mStep[0m  [12/26], [94mLoss[0m : 2.78676
[1mStep[0m  [14/26], [94mLoss[0m : 2.55170
[1mStep[0m  [16/26], [94mLoss[0m : 2.56950
[1mStep[0m  [18/26], [94mLoss[0m : 2.56110
[1mStep[0m  [20/26], [94mLoss[0m : 2.63840
[1mStep[0m  [22/26], [94mLoss[0m : 2.58792
[1mStep[0m  [24/26], [94mLoss[0m : 2.67591

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.801, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53715
[1mStep[0m  [2/26], [94mLoss[0m : 2.46698
[1mStep[0m  [4/26], [94mLoss[0m : 2.41428
[1mStep[0m  [6/26], [94mLoss[0m : 2.65197
[1mStep[0m  [8/26], [94mLoss[0m : 2.55343
[1mStep[0m  [10/26], [94mLoss[0m : 2.63198
[1mStep[0m  [12/26], [94mLoss[0m : 2.63052
[1mStep[0m  [14/26], [94mLoss[0m : 2.62015
[1mStep[0m  [16/26], [94mLoss[0m : 2.64653
[1mStep[0m  [18/26], [94mLoss[0m : 2.46818
[1mStep[0m  [20/26], [94mLoss[0m : 2.60247
[1mStep[0m  [22/26], [94mLoss[0m : 2.43457
[1mStep[0m  [24/26], [94mLoss[0m : 2.54236

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.537, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45242
[1mStep[0m  [2/26], [94mLoss[0m : 2.55527
[1mStep[0m  [4/26], [94mLoss[0m : 2.57433
[1mStep[0m  [6/26], [94mLoss[0m : 2.49345
[1mStep[0m  [8/26], [94mLoss[0m : 2.49636
[1mStep[0m  [10/26], [94mLoss[0m : 2.56106
[1mStep[0m  [12/26], [94mLoss[0m : 2.53771
[1mStep[0m  [14/26], [94mLoss[0m : 2.43609
[1mStep[0m  [16/26], [94mLoss[0m : 2.50510
[1mStep[0m  [18/26], [94mLoss[0m : 2.52911
[1mStep[0m  [20/26], [94mLoss[0m : 2.42726
[1mStep[0m  [22/26], [94mLoss[0m : 2.47080
[1mStep[0m  [24/26], [94mLoss[0m : 2.31230

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46577
[1mStep[0m  [2/26], [94mLoss[0m : 2.44123
[1mStep[0m  [4/26], [94mLoss[0m : 2.46181
[1mStep[0m  [6/26], [94mLoss[0m : 2.44572
[1mStep[0m  [8/26], [94mLoss[0m : 2.52843
[1mStep[0m  [10/26], [94mLoss[0m : 2.47001
[1mStep[0m  [12/26], [94mLoss[0m : 2.42695
[1mStep[0m  [14/26], [94mLoss[0m : 2.36228
[1mStep[0m  [16/26], [94mLoss[0m : 2.48169
[1mStep[0m  [18/26], [94mLoss[0m : 2.53086
[1mStep[0m  [20/26], [94mLoss[0m : 2.45848
[1mStep[0m  [22/26], [94mLoss[0m : 2.56971
[1mStep[0m  [24/26], [94mLoss[0m : 2.52628

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53615
[1mStep[0m  [2/26], [94mLoss[0m : 2.48744
[1mStep[0m  [4/26], [94mLoss[0m : 2.58116
[1mStep[0m  [6/26], [94mLoss[0m : 2.46237
[1mStep[0m  [8/26], [94mLoss[0m : 2.51329
[1mStep[0m  [10/26], [94mLoss[0m : 2.70256
[1mStep[0m  [12/26], [94mLoss[0m : 2.45543
[1mStep[0m  [14/26], [94mLoss[0m : 2.30096
[1mStep[0m  [16/26], [94mLoss[0m : 2.35284
[1mStep[0m  [18/26], [94mLoss[0m : 2.53085
[1mStep[0m  [20/26], [94mLoss[0m : 2.61359
[1mStep[0m  [22/26], [94mLoss[0m : 2.49985
[1mStep[0m  [24/26], [94mLoss[0m : 2.47707

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52675
[1mStep[0m  [2/26], [94mLoss[0m : 2.32611
[1mStep[0m  [4/26], [94mLoss[0m : 2.55006
[1mStep[0m  [6/26], [94mLoss[0m : 2.43102
[1mStep[0m  [8/26], [94mLoss[0m : 2.42640
[1mStep[0m  [10/26], [94mLoss[0m : 2.48008
[1mStep[0m  [12/26], [94mLoss[0m : 2.56373
[1mStep[0m  [14/26], [94mLoss[0m : 2.34407
[1mStep[0m  [16/26], [94mLoss[0m : 2.43163
[1mStep[0m  [18/26], [94mLoss[0m : 2.45966
[1mStep[0m  [20/26], [94mLoss[0m : 2.59102
[1mStep[0m  [22/26], [94mLoss[0m : 2.62285
[1mStep[0m  [24/26], [94mLoss[0m : 2.43146

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.469, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43633
[1mStep[0m  [2/26], [94mLoss[0m : 2.47123
[1mStep[0m  [4/26], [94mLoss[0m : 2.46265
[1mStep[0m  [6/26], [94mLoss[0m : 2.38381
[1mStep[0m  [8/26], [94mLoss[0m : 2.34595
[1mStep[0m  [10/26], [94mLoss[0m : 2.50151
[1mStep[0m  [12/26], [94mLoss[0m : 2.45243
[1mStep[0m  [14/26], [94mLoss[0m : 2.55885
[1mStep[0m  [16/26], [94mLoss[0m : 2.55987
[1mStep[0m  [18/26], [94mLoss[0m : 2.35374
[1mStep[0m  [20/26], [94mLoss[0m : 2.44203
[1mStep[0m  [22/26], [94mLoss[0m : 2.50648
[1mStep[0m  [24/26], [94mLoss[0m : 2.33514

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47144
[1mStep[0m  [2/26], [94mLoss[0m : 2.56464
[1mStep[0m  [4/26], [94mLoss[0m : 2.37750
[1mStep[0m  [6/26], [94mLoss[0m : 2.63526
[1mStep[0m  [8/26], [94mLoss[0m : 2.26843
[1mStep[0m  [10/26], [94mLoss[0m : 2.42652
[1mStep[0m  [12/26], [94mLoss[0m : 2.44264
[1mStep[0m  [14/26], [94mLoss[0m : 2.36656
[1mStep[0m  [16/26], [94mLoss[0m : 2.44623
[1mStep[0m  [18/26], [94mLoss[0m : 2.67706
[1mStep[0m  [20/26], [94mLoss[0m : 2.41832
[1mStep[0m  [22/26], [94mLoss[0m : 2.57792
[1mStep[0m  [24/26], [94mLoss[0m : 2.59078

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49591
[1mStep[0m  [2/26], [94mLoss[0m : 2.47143
[1mStep[0m  [4/26], [94mLoss[0m : 2.54900
[1mStep[0m  [6/26], [94mLoss[0m : 2.40974
[1mStep[0m  [8/26], [94mLoss[0m : 2.47930
[1mStep[0m  [10/26], [94mLoss[0m : 2.44679
[1mStep[0m  [12/26], [94mLoss[0m : 2.28726
[1mStep[0m  [14/26], [94mLoss[0m : 2.43103
[1mStep[0m  [16/26], [94mLoss[0m : 2.44239
[1mStep[0m  [18/26], [94mLoss[0m : 2.40013
[1mStep[0m  [20/26], [94mLoss[0m : 2.36749
[1mStep[0m  [22/26], [94mLoss[0m : 2.35580
[1mStep[0m  [24/26], [94mLoss[0m : 2.65478

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37063
[1mStep[0m  [2/26], [94mLoss[0m : 2.52575
[1mStep[0m  [4/26], [94mLoss[0m : 2.45510
[1mStep[0m  [6/26], [94mLoss[0m : 2.41317
[1mStep[0m  [8/26], [94mLoss[0m : 2.42421
[1mStep[0m  [10/26], [94mLoss[0m : 2.47966
[1mStep[0m  [12/26], [94mLoss[0m : 2.50140
[1mStep[0m  [14/26], [94mLoss[0m : 2.52595
[1mStep[0m  [16/26], [94mLoss[0m : 2.58500
[1mStep[0m  [18/26], [94mLoss[0m : 2.50044
[1mStep[0m  [20/26], [94mLoss[0m : 2.52079
[1mStep[0m  [22/26], [94mLoss[0m : 2.32174
[1mStep[0m  [24/26], [94mLoss[0m : 2.59036

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40713
[1mStep[0m  [2/26], [94mLoss[0m : 2.42055
[1mStep[0m  [4/26], [94mLoss[0m : 2.53240
[1mStep[0m  [6/26], [94mLoss[0m : 2.43970
[1mStep[0m  [8/26], [94mLoss[0m : 2.54602
[1mStep[0m  [10/26], [94mLoss[0m : 2.55322
[1mStep[0m  [12/26], [94mLoss[0m : 2.47318
[1mStep[0m  [14/26], [94mLoss[0m : 2.51902
[1mStep[0m  [16/26], [94mLoss[0m : 2.45756
[1mStep[0m  [18/26], [94mLoss[0m : 2.34322
[1mStep[0m  [20/26], [94mLoss[0m : 2.43424
[1mStep[0m  [22/26], [94mLoss[0m : 2.31769
[1mStep[0m  [24/26], [94mLoss[0m : 2.45911

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44255
[1mStep[0m  [2/26], [94mLoss[0m : 2.24283
[1mStep[0m  [4/26], [94mLoss[0m : 2.49081
[1mStep[0m  [6/26], [94mLoss[0m : 2.58903
[1mStep[0m  [8/26], [94mLoss[0m : 2.33105
[1mStep[0m  [10/26], [94mLoss[0m : 2.29314
[1mStep[0m  [12/26], [94mLoss[0m : 2.42548
[1mStep[0m  [14/26], [94mLoss[0m : 2.34723
[1mStep[0m  [16/26], [94mLoss[0m : 2.28509
[1mStep[0m  [18/26], [94mLoss[0m : 2.36370
[1mStep[0m  [20/26], [94mLoss[0m : 2.50491
[1mStep[0m  [22/26], [94mLoss[0m : 2.46121
[1mStep[0m  [24/26], [94mLoss[0m : 2.56630

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37823
[1mStep[0m  [2/26], [94mLoss[0m : 2.36584
[1mStep[0m  [4/26], [94mLoss[0m : 2.42476
[1mStep[0m  [6/26], [94mLoss[0m : 2.47036
[1mStep[0m  [8/26], [94mLoss[0m : 2.39240
[1mStep[0m  [10/26], [94mLoss[0m : 2.42412
[1mStep[0m  [12/26], [94mLoss[0m : 2.22929
[1mStep[0m  [14/26], [94mLoss[0m : 2.43848
[1mStep[0m  [16/26], [94mLoss[0m : 2.51104
[1mStep[0m  [18/26], [94mLoss[0m : 2.30515
[1mStep[0m  [20/26], [94mLoss[0m : 2.43795
[1mStep[0m  [22/26], [94mLoss[0m : 2.40244
[1mStep[0m  [24/26], [94mLoss[0m : 2.39416

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35427
[1mStep[0m  [2/26], [94mLoss[0m : 2.49053
[1mStep[0m  [4/26], [94mLoss[0m : 2.16839
[1mStep[0m  [6/26], [94mLoss[0m : 2.48824
[1mStep[0m  [8/26], [94mLoss[0m : 2.23417
[1mStep[0m  [10/26], [94mLoss[0m : 2.51461
[1mStep[0m  [12/26], [94mLoss[0m : 2.33480
[1mStep[0m  [14/26], [94mLoss[0m : 2.31304
[1mStep[0m  [16/26], [94mLoss[0m : 2.44602
[1mStep[0m  [18/26], [94mLoss[0m : 2.47470
[1mStep[0m  [20/26], [94mLoss[0m : 2.54481
[1mStep[0m  [22/26], [94mLoss[0m : 2.43826
[1mStep[0m  [24/26], [94mLoss[0m : 2.48897

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29087
[1mStep[0m  [2/26], [94mLoss[0m : 2.37658
[1mStep[0m  [4/26], [94mLoss[0m : 2.56078
[1mStep[0m  [6/26], [94mLoss[0m : 2.32058
[1mStep[0m  [8/26], [94mLoss[0m : 2.34886
[1mStep[0m  [10/26], [94mLoss[0m : 2.31498
[1mStep[0m  [12/26], [94mLoss[0m : 2.40518
[1mStep[0m  [14/26], [94mLoss[0m : 2.42037
[1mStep[0m  [16/26], [94mLoss[0m : 2.55064
[1mStep[0m  [18/26], [94mLoss[0m : 2.45562
[1mStep[0m  [20/26], [94mLoss[0m : 2.52043
[1mStep[0m  [22/26], [94mLoss[0m : 2.46651
[1mStep[0m  [24/26], [94mLoss[0m : 2.42435

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42447
[1mStep[0m  [2/26], [94mLoss[0m : 2.44534
[1mStep[0m  [4/26], [94mLoss[0m : 2.38924
[1mStep[0m  [6/26], [94mLoss[0m : 2.44774
[1mStep[0m  [8/26], [94mLoss[0m : 2.38646
[1mStep[0m  [10/26], [94mLoss[0m : 2.32498
[1mStep[0m  [12/26], [94mLoss[0m : 2.40394
[1mStep[0m  [14/26], [94mLoss[0m : 2.37042
[1mStep[0m  [16/26], [94mLoss[0m : 2.44632
[1mStep[0m  [18/26], [94mLoss[0m : 2.38289
[1mStep[0m  [20/26], [94mLoss[0m : 2.41627
[1mStep[0m  [22/26], [94mLoss[0m : 2.61486
[1mStep[0m  [24/26], [94mLoss[0m : 2.45685

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.394, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.29138
[1mStep[0m  [2/26], [94mLoss[0m : 2.43038
[1mStep[0m  [4/26], [94mLoss[0m : 2.43582
[1mStep[0m  [6/26], [94mLoss[0m : 2.45490
[1mStep[0m  [8/26], [94mLoss[0m : 2.46689
[1mStep[0m  [10/26], [94mLoss[0m : 2.38722
[1mStep[0m  [12/26], [94mLoss[0m : 2.44272
[1mStep[0m  [14/26], [94mLoss[0m : 2.40809
[1mStep[0m  [16/26], [94mLoss[0m : 2.33337
[1mStep[0m  [18/26], [94mLoss[0m : 2.39573
[1mStep[0m  [20/26], [94mLoss[0m : 2.43291
[1mStep[0m  [22/26], [94mLoss[0m : 2.39306
[1mStep[0m  [24/26], [94mLoss[0m : 2.38697

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44545
[1mStep[0m  [2/26], [94mLoss[0m : 2.38389
[1mStep[0m  [4/26], [94mLoss[0m : 2.43821
[1mStep[0m  [6/26], [94mLoss[0m : 2.49512
[1mStep[0m  [8/26], [94mLoss[0m : 2.40549
[1mStep[0m  [10/26], [94mLoss[0m : 2.54623
[1mStep[0m  [12/26], [94mLoss[0m : 2.31470
[1mStep[0m  [14/26], [94mLoss[0m : 2.49588
[1mStep[0m  [16/26], [94mLoss[0m : 2.25798
[1mStep[0m  [18/26], [94mLoss[0m : 2.35630
[1mStep[0m  [20/26], [94mLoss[0m : 2.38357
[1mStep[0m  [22/26], [94mLoss[0m : 2.40111
[1mStep[0m  [24/26], [94mLoss[0m : 2.41227

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.387, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.48478
[1mStep[0m  [2/26], [94mLoss[0m : 2.27700
[1mStep[0m  [4/26], [94mLoss[0m : 2.42672
[1mStep[0m  [6/26], [94mLoss[0m : 2.34608
[1mStep[0m  [8/26], [94mLoss[0m : 2.34113
[1mStep[0m  [10/26], [94mLoss[0m : 2.53691
[1mStep[0m  [12/26], [94mLoss[0m : 2.42671
[1mStep[0m  [14/26], [94mLoss[0m : 2.36366
[1mStep[0m  [16/26], [94mLoss[0m : 2.34274
[1mStep[0m  [18/26], [94mLoss[0m : 2.44045
[1mStep[0m  [20/26], [94mLoss[0m : 2.31233
[1mStep[0m  [22/26], [94mLoss[0m : 2.32637
[1mStep[0m  [24/26], [94mLoss[0m : 2.48351

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38414
[1mStep[0m  [2/26], [94mLoss[0m : 2.43158
[1mStep[0m  [4/26], [94mLoss[0m : 2.39293
[1mStep[0m  [6/26], [94mLoss[0m : 2.15998
[1mStep[0m  [8/26], [94mLoss[0m : 2.40797
[1mStep[0m  [10/26], [94mLoss[0m : 2.29582
[1mStep[0m  [12/26], [94mLoss[0m : 2.49162
[1mStep[0m  [14/26], [94mLoss[0m : 2.33062
[1mStep[0m  [16/26], [94mLoss[0m : 2.44392
[1mStep[0m  [18/26], [94mLoss[0m : 2.49117
[1mStep[0m  [20/26], [94mLoss[0m : 2.29150
[1mStep[0m  [22/26], [94mLoss[0m : 2.34984
[1mStep[0m  [24/26], [94mLoss[0m : 2.42884

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39378
[1mStep[0m  [2/26], [94mLoss[0m : 2.29441
[1mStep[0m  [4/26], [94mLoss[0m : 2.42902
[1mStep[0m  [6/26], [94mLoss[0m : 2.54305
[1mStep[0m  [8/26], [94mLoss[0m : 2.41674
[1mStep[0m  [10/26], [94mLoss[0m : 2.40734
[1mStep[0m  [12/26], [94mLoss[0m : 2.41744
[1mStep[0m  [14/26], [94mLoss[0m : 2.44836
[1mStep[0m  [16/26], [94mLoss[0m : 2.45602
[1mStep[0m  [18/26], [94mLoss[0m : 2.37625
[1mStep[0m  [20/26], [94mLoss[0m : 2.37328
[1mStep[0m  [22/26], [94mLoss[0m : 2.34482
[1mStep[0m  [24/26], [94mLoss[0m : 2.31040

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33839
[1mStep[0m  [2/26], [94mLoss[0m : 2.40438
[1mStep[0m  [4/26], [94mLoss[0m : 2.36687
[1mStep[0m  [6/26], [94mLoss[0m : 2.47367
[1mStep[0m  [8/26], [94mLoss[0m : 2.40927
[1mStep[0m  [10/26], [94mLoss[0m : 2.46616
[1mStep[0m  [12/26], [94mLoss[0m : 2.38954
[1mStep[0m  [14/26], [94mLoss[0m : 2.33386
[1mStep[0m  [16/26], [94mLoss[0m : 2.48943
[1mStep[0m  [18/26], [94mLoss[0m : 2.35482
[1mStep[0m  [20/26], [94mLoss[0m : 2.31735
[1mStep[0m  [22/26], [94mLoss[0m : 2.48655
[1mStep[0m  [24/26], [94mLoss[0m : 2.50330

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.395, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33358
[1mStep[0m  [2/26], [94mLoss[0m : 2.30663
[1mStep[0m  [4/26], [94mLoss[0m : 2.45735
[1mStep[0m  [6/26], [94mLoss[0m : 2.42616
[1mStep[0m  [8/26], [94mLoss[0m : 2.43932
[1mStep[0m  [10/26], [94mLoss[0m : 2.42558
[1mStep[0m  [12/26], [94mLoss[0m : 2.44066
[1mStep[0m  [14/26], [94mLoss[0m : 2.45023
[1mStep[0m  [16/26], [94mLoss[0m : 2.31613
[1mStep[0m  [18/26], [94mLoss[0m : 2.29406
[1mStep[0m  [20/26], [94mLoss[0m : 2.43393
[1mStep[0m  [22/26], [94mLoss[0m : 2.40522
[1mStep[0m  [24/26], [94mLoss[0m : 2.30363

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.377, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.38928
[1mStep[0m  [2/26], [94mLoss[0m : 2.40179
[1mStep[0m  [4/26], [94mLoss[0m : 2.38185
[1mStep[0m  [6/26], [94mLoss[0m : 2.43443
[1mStep[0m  [8/26], [94mLoss[0m : 2.31649
[1mStep[0m  [10/26], [94mLoss[0m : 2.38853
[1mStep[0m  [12/26], [94mLoss[0m : 2.38169
[1mStep[0m  [14/26], [94mLoss[0m : 2.39865
[1mStep[0m  [16/26], [94mLoss[0m : 2.39725
[1mStep[0m  [18/26], [94mLoss[0m : 2.54549
[1mStep[0m  [20/26], [94mLoss[0m : 2.41536
[1mStep[0m  [22/26], [94mLoss[0m : 2.13894
[1mStep[0m  [24/26], [94mLoss[0m : 2.40639

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40900
[1mStep[0m  [2/26], [94mLoss[0m : 2.56052
[1mStep[0m  [4/26], [94mLoss[0m : 2.30586
[1mStep[0m  [6/26], [94mLoss[0m : 2.33812
[1mStep[0m  [8/26], [94mLoss[0m : 2.45081
[1mStep[0m  [10/26], [94mLoss[0m : 2.35089
[1mStep[0m  [12/26], [94mLoss[0m : 2.31085
[1mStep[0m  [14/26], [94mLoss[0m : 2.49161
[1mStep[0m  [16/26], [94mLoss[0m : 2.36152
[1mStep[0m  [18/26], [94mLoss[0m : 2.23186
[1mStep[0m  [20/26], [94mLoss[0m : 2.33707
[1mStep[0m  [22/26], [94mLoss[0m : 2.41208
[1mStep[0m  [24/26], [94mLoss[0m : 2.37452

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35167
[1mStep[0m  [2/26], [94mLoss[0m : 2.32258
[1mStep[0m  [4/26], [94mLoss[0m : 2.31416
[1mStep[0m  [6/26], [94mLoss[0m : 2.31007
[1mStep[0m  [8/26], [94mLoss[0m : 2.45485
[1mStep[0m  [10/26], [94mLoss[0m : 2.35975
[1mStep[0m  [12/26], [94mLoss[0m : 2.35162
[1mStep[0m  [14/26], [94mLoss[0m : 2.51818
[1mStep[0m  [16/26], [94mLoss[0m : 2.31801
[1mStep[0m  [18/26], [94mLoss[0m : 2.34250
[1mStep[0m  [20/26], [94mLoss[0m : 2.36588
[1mStep[0m  [22/26], [94mLoss[0m : 2.32158
[1mStep[0m  [24/26], [94mLoss[0m : 2.45604

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.370, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.369
====================================

Phase 1 - Evaluation MAE:  2.369424764926617
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 2.39283
[1mStep[0m  [2/26], [94mLoss[0m : 2.27777
[1mStep[0m  [4/26], [94mLoss[0m : 2.32237
[1mStep[0m  [6/26], [94mLoss[0m : 2.40559
[1mStep[0m  [8/26], [94mLoss[0m : 2.55392
[1mStep[0m  [10/26], [94mLoss[0m : 2.43404
[1mStep[0m  [12/26], [94mLoss[0m : 2.42320
[1mStep[0m  [14/26], [94mLoss[0m : 2.43903
[1mStep[0m  [16/26], [94mLoss[0m : 2.28199
[1mStep[0m  [18/26], [94mLoss[0m : 2.56356
[1mStep[0m  [20/26], [94mLoss[0m : 2.56340
[1mStep[0m  [22/26], [94mLoss[0m : 2.45304
[1mStep[0m  [24/26], [94mLoss[0m : 2.55228

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.23937
[1mStep[0m  [2/26], [94mLoss[0m : 2.39366
[1mStep[0m  [4/26], [94mLoss[0m : 2.38625
[1mStep[0m  [6/26], [94mLoss[0m : 2.29705
[1mStep[0m  [8/26], [94mLoss[0m : 2.43550
[1mStep[0m  [10/26], [94mLoss[0m : 2.44043
[1mStep[0m  [12/26], [94mLoss[0m : 2.35656
[1mStep[0m  [14/26], [94mLoss[0m : 2.36116
[1mStep[0m  [16/26], [94mLoss[0m : 2.36864
[1mStep[0m  [18/26], [94mLoss[0m : 2.37003
[1mStep[0m  [20/26], [94mLoss[0m : 2.27284
[1mStep[0m  [22/26], [94mLoss[0m : 2.42855
[1mStep[0m  [24/26], [94mLoss[0m : 2.38939

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.17839
[1mStep[0m  [2/26], [94mLoss[0m : 2.20046
[1mStep[0m  [4/26], [94mLoss[0m : 2.23360
[1mStep[0m  [6/26], [94mLoss[0m : 2.36874
[1mStep[0m  [8/26], [94mLoss[0m : 2.33852
[1mStep[0m  [10/26], [94mLoss[0m : 2.38526
[1mStep[0m  [12/26], [94mLoss[0m : 2.16406
[1mStep[0m  [14/26], [94mLoss[0m : 2.29992
[1mStep[0m  [16/26], [94mLoss[0m : 2.32950
[1mStep[0m  [18/26], [94mLoss[0m : 2.38864
[1mStep[0m  [20/26], [94mLoss[0m : 2.40976
[1mStep[0m  [22/26], [94mLoss[0m : 2.34720
[1mStep[0m  [24/26], [94mLoss[0m : 2.10637

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07756
[1mStep[0m  [2/26], [94mLoss[0m : 2.14439
[1mStep[0m  [4/26], [94mLoss[0m : 2.25893
[1mStep[0m  [6/26], [94mLoss[0m : 2.13399
[1mStep[0m  [8/26], [94mLoss[0m : 2.21919
[1mStep[0m  [10/26], [94mLoss[0m : 2.09629
[1mStep[0m  [12/26], [94mLoss[0m : 2.15093
[1mStep[0m  [14/26], [94mLoss[0m : 2.23750
[1mStep[0m  [16/26], [94mLoss[0m : 2.26089
[1mStep[0m  [18/26], [94mLoss[0m : 2.37579
[1mStep[0m  [20/26], [94mLoss[0m : 2.17412
[1mStep[0m  [22/26], [94mLoss[0m : 2.20905
[1mStep[0m  [24/26], [94mLoss[0m : 2.24332

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.197, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.01227
[1mStep[0m  [2/26], [94mLoss[0m : 2.03326
[1mStep[0m  [4/26], [94mLoss[0m : 2.31032
[1mStep[0m  [6/26], [94mLoss[0m : 2.08383
[1mStep[0m  [8/26], [94mLoss[0m : 2.06021
[1mStep[0m  [10/26], [94mLoss[0m : 2.10287
[1mStep[0m  [12/26], [94mLoss[0m : 2.12734
[1mStep[0m  [14/26], [94mLoss[0m : 2.01330
[1mStep[0m  [16/26], [94mLoss[0m : 2.02577
[1mStep[0m  [18/26], [94mLoss[0m : 2.07889
[1mStep[0m  [20/26], [94mLoss[0m : 2.12090
[1mStep[0m  [22/26], [94mLoss[0m : 2.12869
[1mStep[0m  [24/26], [94mLoss[0m : 2.22199

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.11787
[1mStep[0m  [2/26], [94mLoss[0m : 2.04471
[1mStep[0m  [4/26], [94mLoss[0m : 2.13141
[1mStep[0m  [6/26], [94mLoss[0m : 2.11802
[1mStep[0m  [8/26], [94mLoss[0m : 2.04454
[1mStep[0m  [10/26], [94mLoss[0m : 2.01611
[1mStep[0m  [12/26], [94mLoss[0m : 1.90445
[1mStep[0m  [14/26], [94mLoss[0m : 2.11867
[1mStep[0m  [16/26], [94mLoss[0m : 2.03360
[1mStep[0m  [18/26], [94mLoss[0m : 2.12377
[1mStep[0m  [20/26], [94mLoss[0m : 2.19968
[1mStep[0m  [22/26], [94mLoss[0m : 2.01885
[1mStep[0m  [24/26], [94mLoss[0m : 2.14110

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.536, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.98734
[1mStep[0m  [2/26], [94mLoss[0m : 1.85234
[1mStep[0m  [4/26], [94mLoss[0m : 1.88252
[1mStep[0m  [6/26], [94mLoss[0m : 1.99566
[1mStep[0m  [8/26], [94mLoss[0m : 1.88029
[1mStep[0m  [10/26], [94mLoss[0m : 1.88649
[1mStep[0m  [12/26], [94mLoss[0m : 2.03128
[1mStep[0m  [14/26], [94mLoss[0m : 1.91009
[1mStep[0m  [16/26], [94mLoss[0m : 1.97903
[1mStep[0m  [18/26], [94mLoss[0m : 1.93764
[1mStep[0m  [20/26], [94mLoss[0m : 2.02028
[1mStep[0m  [22/26], [94mLoss[0m : 1.97425
[1mStep[0m  [24/26], [94mLoss[0m : 2.08156

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.76610
[1mStep[0m  [2/26], [94mLoss[0m : 2.05765
[1mStep[0m  [4/26], [94mLoss[0m : 1.85813
[1mStep[0m  [6/26], [94mLoss[0m : 1.92821
[1mStep[0m  [8/26], [94mLoss[0m : 1.94097
[1mStep[0m  [10/26], [94mLoss[0m : 1.95154
[1mStep[0m  [12/26], [94mLoss[0m : 1.91971
[1mStep[0m  [14/26], [94mLoss[0m : 1.87357
[1mStep[0m  [16/26], [94mLoss[0m : 1.99324
[1mStep[0m  [18/26], [94mLoss[0m : 2.01916
[1mStep[0m  [20/26], [94mLoss[0m : 1.89542
[1mStep[0m  [22/26], [94mLoss[0m : 1.85509
[1mStep[0m  [24/26], [94mLoss[0m : 1.97892

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.571, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96612
[1mStep[0m  [2/26], [94mLoss[0m : 1.77995
[1mStep[0m  [4/26], [94mLoss[0m : 1.80939
[1mStep[0m  [6/26], [94mLoss[0m : 1.83154
[1mStep[0m  [8/26], [94mLoss[0m : 1.83921
[1mStep[0m  [10/26], [94mLoss[0m : 2.02092
[1mStep[0m  [12/26], [94mLoss[0m : 1.93163
[1mStep[0m  [14/26], [94mLoss[0m : 1.83512
[1mStep[0m  [16/26], [94mLoss[0m : 1.78472
[1mStep[0m  [18/26], [94mLoss[0m : 1.89279
[1mStep[0m  [20/26], [94mLoss[0m : 1.88931
[1mStep[0m  [22/26], [94mLoss[0m : 1.92438
[1mStep[0m  [24/26], [94mLoss[0m : 1.89120

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.80792
[1mStep[0m  [2/26], [94mLoss[0m : 1.79844
[1mStep[0m  [4/26], [94mLoss[0m : 1.72044
[1mStep[0m  [6/26], [94mLoss[0m : 1.80147
[1mStep[0m  [8/26], [94mLoss[0m : 1.95836
[1mStep[0m  [10/26], [94mLoss[0m : 1.82561
[1mStep[0m  [12/26], [94mLoss[0m : 1.76537
[1mStep[0m  [14/26], [94mLoss[0m : 1.71476
[1mStep[0m  [16/26], [94mLoss[0m : 1.84620
[1mStep[0m  [18/26], [94mLoss[0m : 1.83384
[1mStep[0m  [20/26], [94mLoss[0m : 1.90193
[1mStep[0m  [22/26], [94mLoss[0m : 1.92053
[1mStep[0m  [24/26], [94mLoss[0m : 1.87777

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.78984
[1mStep[0m  [2/26], [94mLoss[0m : 1.63610
[1mStep[0m  [4/26], [94mLoss[0m : 1.73330
[1mStep[0m  [6/26], [94mLoss[0m : 1.78436
[1mStep[0m  [8/26], [94mLoss[0m : 1.92761
[1mStep[0m  [10/26], [94mLoss[0m : 1.75117
[1mStep[0m  [12/26], [94mLoss[0m : 1.78323
[1mStep[0m  [14/26], [94mLoss[0m : 1.84118
[1mStep[0m  [16/26], [94mLoss[0m : 1.84957
[1mStep[0m  [18/26], [94mLoss[0m : 1.88963
[1mStep[0m  [20/26], [94mLoss[0m : 1.78412
[1mStep[0m  [22/26], [94mLoss[0m : 1.64901
[1mStep[0m  [24/26], [94mLoss[0m : 1.87942

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.77089
[1mStep[0m  [2/26], [94mLoss[0m : 1.76598
[1mStep[0m  [4/26], [94mLoss[0m : 1.72196
[1mStep[0m  [6/26], [94mLoss[0m : 1.72998
[1mStep[0m  [8/26], [94mLoss[0m : 1.67512
[1mStep[0m  [10/26], [94mLoss[0m : 1.73541
[1mStep[0m  [12/26], [94mLoss[0m : 1.59410
[1mStep[0m  [14/26], [94mLoss[0m : 1.67495
[1mStep[0m  [16/26], [94mLoss[0m : 1.81070
[1mStep[0m  [18/26], [94mLoss[0m : 1.65376
[1mStep[0m  [20/26], [94mLoss[0m : 1.71490
[1mStep[0m  [22/26], [94mLoss[0m : 1.68488
[1mStep[0m  [24/26], [94mLoss[0m : 1.70668

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.543, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.59931
[1mStep[0m  [2/26], [94mLoss[0m : 1.69558
[1mStep[0m  [4/26], [94mLoss[0m : 1.63680
[1mStep[0m  [6/26], [94mLoss[0m : 1.66598
[1mStep[0m  [8/26], [94mLoss[0m : 1.54867
[1mStep[0m  [10/26], [94mLoss[0m : 1.59633
[1mStep[0m  [12/26], [94mLoss[0m : 1.62066
[1mStep[0m  [14/26], [94mLoss[0m : 1.65922
[1mStep[0m  [16/26], [94mLoss[0m : 1.63420
[1mStep[0m  [18/26], [94mLoss[0m : 1.67259
[1mStep[0m  [20/26], [94mLoss[0m : 1.73323
[1mStep[0m  [22/26], [94mLoss[0m : 1.78587
[1mStep[0m  [24/26], [94mLoss[0m : 1.79933

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.577, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.62521
[1mStep[0m  [2/26], [94mLoss[0m : 1.59784
[1mStep[0m  [4/26], [94mLoss[0m : 1.57599
[1mStep[0m  [6/26], [94mLoss[0m : 1.70639
[1mStep[0m  [8/26], [94mLoss[0m : 1.59290
[1mStep[0m  [10/26], [94mLoss[0m : 1.54184
[1mStep[0m  [12/26], [94mLoss[0m : 1.59752
[1mStep[0m  [14/26], [94mLoss[0m : 1.66775
[1mStep[0m  [16/26], [94mLoss[0m : 1.58040
[1mStep[0m  [18/26], [94mLoss[0m : 1.68316
[1mStep[0m  [20/26], [94mLoss[0m : 1.64538
[1mStep[0m  [22/26], [94mLoss[0m : 1.69874
[1mStep[0m  [24/26], [94mLoss[0m : 1.58635

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.540, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.37385
[1mStep[0m  [2/26], [94mLoss[0m : 1.57750
[1mStep[0m  [4/26], [94mLoss[0m : 1.54735
[1mStep[0m  [6/26], [94mLoss[0m : 1.55947
[1mStep[0m  [8/26], [94mLoss[0m : 1.55545
[1mStep[0m  [10/26], [94mLoss[0m : 1.55939
[1mStep[0m  [12/26], [94mLoss[0m : 1.53146
[1mStep[0m  [14/26], [94mLoss[0m : 1.57043
[1mStep[0m  [16/26], [94mLoss[0m : 1.53468
[1mStep[0m  [18/26], [94mLoss[0m : 1.62920
[1mStep[0m  [20/26], [94mLoss[0m : 1.57601
[1mStep[0m  [22/26], [94mLoss[0m : 1.59933
[1mStep[0m  [24/26], [94mLoss[0m : 1.64940

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.590, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.53133
[1mStep[0m  [2/26], [94mLoss[0m : 1.54478
[1mStep[0m  [4/26], [94mLoss[0m : 1.60264
[1mStep[0m  [6/26], [94mLoss[0m : 1.54189
[1mStep[0m  [8/26], [94mLoss[0m : 1.50441
[1mStep[0m  [10/26], [94mLoss[0m : 1.51272
[1mStep[0m  [12/26], [94mLoss[0m : 1.54390
[1mStep[0m  [14/26], [94mLoss[0m : 1.51967
[1mStep[0m  [16/26], [94mLoss[0m : 1.58664
[1mStep[0m  [18/26], [94mLoss[0m : 1.50058
[1mStep[0m  [20/26], [94mLoss[0m : 1.64632
[1mStep[0m  [22/26], [94mLoss[0m : 1.69133
[1mStep[0m  [24/26], [94mLoss[0m : 1.62180

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.586, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.54680
[1mStep[0m  [2/26], [94mLoss[0m : 1.58062
[1mStep[0m  [4/26], [94mLoss[0m : 1.50618
[1mStep[0m  [6/26], [94mLoss[0m : 1.61298
[1mStep[0m  [8/26], [94mLoss[0m : 1.38105
[1mStep[0m  [10/26], [94mLoss[0m : 1.52411
[1mStep[0m  [12/26], [94mLoss[0m : 1.52484
[1mStep[0m  [14/26], [94mLoss[0m : 1.54699
[1mStep[0m  [16/26], [94mLoss[0m : 1.54535
[1mStep[0m  [18/26], [94mLoss[0m : 1.65955
[1mStep[0m  [20/26], [94mLoss[0m : 1.55314
[1mStep[0m  [22/26], [94mLoss[0m : 1.53053
[1mStep[0m  [24/26], [94mLoss[0m : 1.48303

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.562, [92mTest[0m: 2.546, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.47045
[1mStep[0m  [2/26], [94mLoss[0m : 1.42759
[1mStep[0m  [4/26], [94mLoss[0m : 1.55351
[1mStep[0m  [6/26], [94mLoss[0m : 1.45796
[1mStep[0m  [8/26], [94mLoss[0m : 1.53652
[1mStep[0m  [10/26], [94mLoss[0m : 1.53206
[1mStep[0m  [12/26], [94mLoss[0m : 1.51477
[1mStep[0m  [14/26], [94mLoss[0m : 1.48867
[1mStep[0m  [16/26], [94mLoss[0m : 1.46019
[1mStep[0m  [18/26], [94mLoss[0m : 1.59967
[1mStep[0m  [20/26], [94mLoss[0m : 1.48780
[1mStep[0m  [22/26], [94mLoss[0m : 1.60041
[1mStep[0m  [24/26], [94mLoss[0m : 1.49669

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.50087
[1mStep[0m  [2/26], [94mLoss[0m : 1.44500
[1mStep[0m  [4/26], [94mLoss[0m : 1.47372
[1mStep[0m  [6/26], [94mLoss[0m : 1.37418
[1mStep[0m  [8/26], [94mLoss[0m : 1.39941
[1mStep[0m  [10/26], [94mLoss[0m : 1.44235
[1mStep[0m  [12/26], [94mLoss[0m : 1.45111
[1mStep[0m  [14/26], [94mLoss[0m : 1.46803
[1mStep[0m  [16/26], [94mLoss[0m : 1.42654
[1mStep[0m  [18/26], [94mLoss[0m : 1.42848
[1mStep[0m  [20/26], [94mLoss[0m : 1.32766
[1mStep[0m  [22/26], [94mLoss[0m : 1.51538
[1mStep[0m  [24/26], [94mLoss[0m : 1.42644

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.35485
[1mStep[0m  [2/26], [94mLoss[0m : 1.35049
[1mStep[0m  [4/26], [94mLoss[0m : 1.33527
[1mStep[0m  [6/26], [94mLoss[0m : 1.49887
[1mStep[0m  [8/26], [94mLoss[0m : 1.29916
[1mStep[0m  [10/26], [94mLoss[0m : 1.52446
[1mStep[0m  [12/26], [94mLoss[0m : 1.40739
[1mStep[0m  [14/26], [94mLoss[0m : 1.46016
[1mStep[0m  [16/26], [94mLoss[0m : 1.42471
[1mStep[0m  [18/26], [94mLoss[0m : 1.45795
[1mStep[0m  [20/26], [94mLoss[0m : 1.54515
[1mStep[0m  [22/26], [94mLoss[0m : 1.45603
[1mStep[0m  [24/26], [94mLoss[0m : 1.41985

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.589, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.48035
[1mStep[0m  [2/26], [94mLoss[0m : 1.40045
[1mStep[0m  [4/26], [94mLoss[0m : 1.46757
[1mStep[0m  [6/26], [94mLoss[0m : 1.39582
[1mStep[0m  [8/26], [94mLoss[0m : 1.37984
[1mStep[0m  [10/26], [94mLoss[0m : 1.27789
[1mStep[0m  [12/26], [94mLoss[0m : 1.45079
[1mStep[0m  [14/26], [94mLoss[0m : 1.40526
[1mStep[0m  [16/26], [94mLoss[0m : 1.47537
[1mStep[0m  [18/26], [94mLoss[0m : 1.43432
[1mStep[0m  [20/26], [94mLoss[0m : 1.35521
[1mStep[0m  [22/26], [94mLoss[0m : 1.36333
[1mStep[0m  [24/26], [94mLoss[0m : 1.41747

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.395, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.40806
[1mStep[0m  [2/26], [94mLoss[0m : 1.32728
[1mStep[0m  [4/26], [94mLoss[0m : 1.36325
[1mStep[0m  [6/26], [94mLoss[0m : 1.30575
[1mStep[0m  [8/26], [94mLoss[0m : 1.35989
[1mStep[0m  [10/26], [94mLoss[0m : 1.37447
[1mStep[0m  [12/26], [94mLoss[0m : 1.34004
[1mStep[0m  [14/26], [94mLoss[0m : 1.37993
[1mStep[0m  [16/26], [94mLoss[0m : 1.36721
[1mStep[0m  [18/26], [94mLoss[0m : 1.32780
[1mStep[0m  [20/26], [94mLoss[0m : 1.40051
[1mStep[0m  [22/26], [94mLoss[0m : 1.37136
[1mStep[0m  [24/26], [94mLoss[0m : 1.44788

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.361, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.530
====================================

Phase 2 - Evaluation MAE:  2.5302025354825535
MAE score P1      2.369425
MAE score P2      2.530203
loss              1.361386
learning_rate     0.007525
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
