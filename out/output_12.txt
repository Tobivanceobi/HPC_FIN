no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  12
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.82603
[1mStep[0m  [2/21], [94mLoss[0m : 9.80131
[1mStep[0m  [4/21], [94mLoss[0m : 7.95858
[1mStep[0m  [6/21], [94mLoss[0m : 5.94069
[1mStep[0m  [8/21], [94mLoss[0m : 4.45961
[1mStep[0m  [10/21], [94mLoss[0m : 3.21106
[1mStep[0m  [12/21], [94mLoss[0m : 3.01430
[1mStep[0m  [14/21], [94mLoss[0m : 2.94600
[1mStep[0m  [16/21], [94mLoss[0m : 2.78566
[1mStep[0m  [18/21], [94mLoss[0m : 2.64360
[1mStep[0m  [20/21], [94mLoss[0m : 2.84870

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.049, [92mTest[0m: 10.890, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72139
[1mStep[0m  [2/21], [94mLoss[0m : 2.59849
[1mStep[0m  [4/21], [94mLoss[0m : 2.65845
[1mStep[0m  [6/21], [94mLoss[0m : 2.58936
[1mStep[0m  [8/21], [94mLoss[0m : 2.37842
[1mStep[0m  [10/21], [94mLoss[0m : 2.50826
[1mStep[0m  [12/21], [94mLoss[0m : 2.63748
[1mStep[0m  [14/21], [94mLoss[0m : 2.45157
[1mStep[0m  [16/21], [94mLoss[0m : 2.65481
[1mStep[0m  [18/21], [94mLoss[0m : 2.56329
[1mStep[0m  [20/21], [94mLoss[0m : 2.75648

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.701, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57387
[1mStep[0m  [2/21], [94mLoss[0m : 2.47305
[1mStep[0m  [4/21], [94mLoss[0m : 2.65261
[1mStep[0m  [6/21], [94mLoss[0m : 2.49260
[1mStep[0m  [8/21], [94mLoss[0m : 2.43273
[1mStep[0m  [10/21], [94mLoss[0m : 2.41111
[1mStep[0m  [12/21], [94mLoss[0m : 2.47597
[1mStep[0m  [14/21], [94mLoss[0m : 2.51562
[1mStep[0m  [16/21], [94mLoss[0m : 2.27746
[1mStep[0m  [18/21], [94mLoss[0m : 2.36085
[1mStep[0m  [20/21], [94mLoss[0m : 2.60621

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57017
[1mStep[0m  [2/21], [94mLoss[0m : 2.39772
[1mStep[0m  [4/21], [94mLoss[0m : 2.47238
[1mStep[0m  [6/21], [94mLoss[0m : 2.52575
[1mStep[0m  [8/21], [94mLoss[0m : 2.45917
[1mStep[0m  [10/21], [94mLoss[0m : 2.60380
[1mStep[0m  [12/21], [94mLoss[0m : 2.34424
[1mStep[0m  [14/21], [94mLoss[0m : 2.48289
[1mStep[0m  [16/21], [94mLoss[0m : 2.43534
[1mStep[0m  [18/21], [94mLoss[0m : 2.40469
[1mStep[0m  [20/21], [94mLoss[0m : 2.48106

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46905
[1mStep[0m  [2/21], [94mLoss[0m : 2.60304
[1mStep[0m  [4/21], [94mLoss[0m : 2.42151
[1mStep[0m  [6/21], [94mLoss[0m : 2.40641
[1mStep[0m  [8/21], [94mLoss[0m : 2.46895
[1mStep[0m  [10/21], [94mLoss[0m : 2.42270
[1mStep[0m  [12/21], [94mLoss[0m : 2.47448
[1mStep[0m  [14/21], [94mLoss[0m : 2.57059
[1mStep[0m  [16/21], [94mLoss[0m : 2.49846
[1mStep[0m  [18/21], [94mLoss[0m : 2.41139
[1mStep[0m  [20/21], [94mLoss[0m : 2.42077

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48398
[1mStep[0m  [2/21], [94mLoss[0m : 2.45516
[1mStep[0m  [4/21], [94mLoss[0m : 2.54592
[1mStep[0m  [6/21], [94mLoss[0m : 2.53760
[1mStep[0m  [8/21], [94mLoss[0m : 2.55997
[1mStep[0m  [10/21], [94mLoss[0m : 2.54135
[1mStep[0m  [12/21], [94mLoss[0m : 2.50097
[1mStep[0m  [14/21], [94mLoss[0m : 2.47392
[1mStep[0m  [16/21], [94mLoss[0m : 2.31828
[1mStep[0m  [18/21], [94mLoss[0m : 2.40792
[1mStep[0m  [20/21], [94mLoss[0m : 2.38319

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41974
[1mStep[0m  [2/21], [94mLoss[0m : 2.61334
[1mStep[0m  [4/21], [94mLoss[0m : 2.58036
[1mStep[0m  [6/21], [94mLoss[0m : 2.53495
[1mStep[0m  [8/21], [94mLoss[0m : 2.39834
[1mStep[0m  [10/21], [94mLoss[0m : 2.50199
[1mStep[0m  [12/21], [94mLoss[0m : 2.30681
[1mStep[0m  [14/21], [94mLoss[0m : 2.40333
[1mStep[0m  [16/21], [94mLoss[0m : 2.57552
[1mStep[0m  [18/21], [94mLoss[0m : 2.49338
[1mStep[0m  [20/21], [94mLoss[0m : 2.49507

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51276
[1mStep[0m  [2/21], [94mLoss[0m : 2.45348
[1mStep[0m  [4/21], [94mLoss[0m : 2.44645
[1mStep[0m  [6/21], [94mLoss[0m : 2.39018
[1mStep[0m  [8/21], [94mLoss[0m : 2.43810
[1mStep[0m  [10/21], [94mLoss[0m : 2.32172
[1mStep[0m  [12/21], [94mLoss[0m : 2.72945
[1mStep[0m  [14/21], [94mLoss[0m : 2.32064
[1mStep[0m  [16/21], [94mLoss[0m : 2.40516
[1mStep[0m  [18/21], [94mLoss[0m : 2.54159
[1mStep[0m  [20/21], [94mLoss[0m : 2.52851

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46974
[1mStep[0m  [2/21], [94mLoss[0m : 2.40557
[1mStep[0m  [4/21], [94mLoss[0m : 2.50049
[1mStep[0m  [6/21], [94mLoss[0m : 2.56805
[1mStep[0m  [8/21], [94mLoss[0m : 2.45379
[1mStep[0m  [10/21], [94mLoss[0m : 2.45800
[1mStep[0m  [12/21], [94mLoss[0m : 2.43875
[1mStep[0m  [14/21], [94mLoss[0m : 2.45912
[1mStep[0m  [16/21], [94mLoss[0m : 2.39688
[1mStep[0m  [18/21], [94mLoss[0m : 2.29761
[1mStep[0m  [20/21], [94mLoss[0m : 2.49583

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46706
[1mStep[0m  [2/21], [94mLoss[0m : 2.32047
[1mStep[0m  [4/21], [94mLoss[0m : 2.56007
[1mStep[0m  [6/21], [94mLoss[0m : 2.47216
[1mStep[0m  [8/21], [94mLoss[0m : 2.60594
[1mStep[0m  [10/21], [94mLoss[0m : 2.35622
[1mStep[0m  [12/21], [94mLoss[0m : 2.32298
[1mStep[0m  [14/21], [94mLoss[0m : 2.46605
[1mStep[0m  [16/21], [94mLoss[0m : 2.51575
[1mStep[0m  [18/21], [94mLoss[0m : 2.47837
[1mStep[0m  [20/21], [94mLoss[0m : 2.41996

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55801
[1mStep[0m  [2/21], [94mLoss[0m : 2.43324
[1mStep[0m  [4/21], [94mLoss[0m : 2.21566
[1mStep[0m  [6/21], [94mLoss[0m : 2.43172
[1mStep[0m  [8/21], [94mLoss[0m : 2.41354
[1mStep[0m  [10/21], [94mLoss[0m : 2.53141
[1mStep[0m  [12/21], [94mLoss[0m : 2.49647
[1mStep[0m  [14/21], [94mLoss[0m : 2.29547
[1mStep[0m  [16/21], [94mLoss[0m : 2.67465
[1mStep[0m  [18/21], [94mLoss[0m : 2.57728
[1mStep[0m  [20/21], [94mLoss[0m : 2.32316

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42510
[1mStep[0m  [2/21], [94mLoss[0m : 2.46447
[1mStep[0m  [4/21], [94mLoss[0m : 2.36644
[1mStep[0m  [6/21], [94mLoss[0m : 2.40783
[1mStep[0m  [8/21], [94mLoss[0m : 2.47526
[1mStep[0m  [10/21], [94mLoss[0m : 2.47064
[1mStep[0m  [12/21], [94mLoss[0m : 2.38966
[1mStep[0m  [14/21], [94mLoss[0m : 2.43950
[1mStep[0m  [16/21], [94mLoss[0m : 2.43273
[1mStep[0m  [18/21], [94mLoss[0m : 2.49618
[1mStep[0m  [20/21], [94mLoss[0m : 2.49240

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33736
[1mStep[0m  [2/21], [94mLoss[0m : 2.29376
[1mStep[0m  [4/21], [94mLoss[0m : 2.26620
[1mStep[0m  [6/21], [94mLoss[0m : 2.54942
[1mStep[0m  [8/21], [94mLoss[0m : 2.41874
[1mStep[0m  [10/21], [94mLoss[0m : 2.50724
[1mStep[0m  [12/21], [94mLoss[0m : 2.44842
[1mStep[0m  [14/21], [94mLoss[0m : 2.40577
[1mStep[0m  [16/21], [94mLoss[0m : 2.38480
[1mStep[0m  [18/21], [94mLoss[0m : 2.42901
[1mStep[0m  [20/21], [94mLoss[0m : 2.44791

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44110
[1mStep[0m  [2/21], [94mLoss[0m : 2.44536
[1mStep[0m  [4/21], [94mLoss[0m : 2.33678
[1mStep[0m  [6/21], [94mLoss[0m : 2.46328
[1mStep[0m  [8/21], [94mLoss[0m : 2.40676
[1mStep[0m  [10/21], [94mLoss[0m : 2.48033
[1mStep[0m  [12/21], [94mLoss[0m : 2.32087
[1mStep[0m  [14/21], [94mLoss[0m : 2.53082
[1mStep[0m  [16/21], [94mLoss[0m : 2.37737
[1mStep[0m  [18/21], [94mLoss[0m : 2.27832
[1mStep[0m  [20/21], [94mLoss[0m : 2.51250

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35758
[1mStep[0m  [2/21], [94mLoss[0m : 2.45204
[1mStep[0m  [4/21], [94mLoss[0m : 2.59493
[1mStep[0m  [6/21], [94mLoss[0m : 2.32359
[1mStep[0m  [8/21], [94mLoss[0m : 2.31594
[1mStep[0m  [10/21], [94mLoss[0m : 2.53387
[1mStep[0m  [12/21], [94mLoss[0m : 2.45132
[1mStep[0m  [14/21], [94mLoss[0m : 2.48327
[1mStep[0m  [16/21], [94mLoss[0m : 2.43346
[1mStep[0m  [18/21], [94mLoss[0m : 2.42997
[1mStep[0m  [20/21], [94mLoss[0m : 2.43221

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56246
[1mStep[0m  [2/21], [94mLoss[0m : 2.36836
[1mStep[0m  [4/21], [94mLoss[0m : 2.26554
[1mStep[0m  [6/21], [94mLoss[0m : 2.42351
[1mStep[0m  [8/21], [94mLoss[0m : 2.36079
[1mStep[0m  [10/21], [94mLoss[0m : 2.44563
[1mStep[0m  [12/21], [94mLoss[0m : 2.52522
[1mStep[0m  [14/21], [94mLoss[0m : 2.52289
[1mStep[0m  [16/21], [94mLoss[0m : 2.38336
[1mStep[0m  [18/21], [94mLoss[0m : 2.45501
[1mStep[0m  [20/21], [94mLoss[0m : 2.37750

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55839
[1mStep[0m  [2/21], [94mLoss[0m : 2.60500
[1mStep[0m  [4/21], [94mLoss[0m : 2.39117
[1mStep[0m  [6/21], [94mLoss[0m : 2.40944
[1mStep[0m  [8/21], [94mLoss[0m : 2.31123
[1mStep[0m  [10/21], [94mLoss[0m : 2.48393
[1mStep[0m  [12/21], [94mLoss[0m : 2.46845
[1mStep[0m  [14/21], [94mLoss[0m : 2.37417
[1mStep[0m  [16/21], [94mLoss[0m : 2.35577
[1mStep[0m  [18/21], [94mLoss[0m : 2.40561
[1mStep[0m  [20/21], [94mLoss[0m : 2.46546

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36866
[1mStep[0m  [2/21], [94mLoss[0m : 2.35292
[1mStep[0m  [4/21], [94mLoss[0m : 2.55739
[1mStep[0m  [6/21], [94mLoss[0m : 2.43609
[1mStep[0m  [8/21], [94mLoss[0m : 2.38187
[1mStep[0m  [10/21], [94mLoss[0m : 2.40890
[1mStep[0m  [12/21], [94mLoss[0m : 2.56557
[1mStep[0m  [14/21], [94mLoss[0m : 2.31461
[1mStep[0m  [16/21], [94mLoss[0m : 2.47077
[1mStep[0m  [18/21], [94mLoss[0m : 2.45477
[1mStep[0m  [20/21], [94mLoss[0m : 2.47759

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30304
[1mStep[0m  [2/21], [94mLoss[0m : 2.48320
[1mStep[0m  [4/21], [94mLoss[0m : 2.45241
[1mStep[0m  [6/21], [94mLoss[0m : 2.60830
[1mStep[0m  [8/21], [94mLoss[0m : 2.34542
[1mStep[0m  [10/21], [94mLoss[0m : 2.47094
[1mStep[0m  [12/21], [94mLoss[0m : 2.34560
[1mStep[0m  [14/21], [94mLoss[0m : 2.47864
[1mStep[0m  [16/21], [94mLoss[0m : 2.33227
[1mStep[0m  [18/21], [94mLoss[0m : 2.50385
[1mStep[0m  [20/21], [94mLoss[0m : 2.33041

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42937
[1mStep[0m  [2/21], [94mLoss[0m : 2.62855
[1mStep[0m  [4/21], [94mLoss[0m : 2.36385
[1mStep[0m  [6/21], [94mLoss[0m : 2.47370
[1mStep[0m  [8/21], [94mLoss[0m : 2.39149
[1mStep[0m  [10/21], [94mLoss[0m : 2.38060
[1mStep[0m  [12/21], [94mLoss[0m : 2.27028
[1mStep[0m  [14/21], [94mLoss[0m : 2.58176
[1mStep[0m  [16/21], [94mLoss[0m : 2.50927
[1mStep[0m  [18/21], [94mLoss[0m : 2.30527
[1mStep[0m  [20/21], [94mLoss[0m : 2.54073

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42989
[1mStep[0m  [2/21], [94mLoss[0m : 2.31209
[1mStep[0m  [4/21], [94mLoss[0m : 2.31155
[1mStep[0m  [6/21], [94mLoss[0m : 2.28062
[1mStep[0m  [8/21], [94mLoss[0m : 2.50492
[1mStep[0m  [10/21], [94mLoss[0m : 2.39441
[1mStep[0m  [12/21], [94mLoss[0m : 2.37164
[1mStep[0m  [14/21], [94mLoss[0m : 2.42251
[1mStep[0m  [16/21], [94mLoss[0m : 2.56009
[1mStep[0m  [18/21], [94mLoss[0m : 2.47180
[1mStep[0m  [20/21], [94mLoss[0m : 2.61832

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26132
[1mStep[0m  [2/21], [94mLoss[0m : 2.31461
[1mStep[0m  [4/21], [94mLoss[0m : 2.61461
[1mStep[0m  [6/21], [94mLoss[0m : 2.39399
[1mStep[0m  [8/21], [94mLoss[0m : 2.27592
[1mStep[0m  [10/21], [94mLoss[0m : 2.33924
[1mStep[0m  [12/21], [94mLoss[0m : 2.38732
[1mStep[0m  [14/21], [94mLoss[0m : 2.51295
[1mStep[0m  [16/21], [94mLoss[0m : 2.56749
[1mStep[0m  [18/21], [94mLoss[0m : 2.50301
[1mStep[0m  [20/21], [94mLoss[0m : 2.36310

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30553
[1mStep[0m  [2/21], [94mLoss[0m : 2.29173
[1mStep[0m  [4/21], [94mLoss[0m : 2.49419
[1mStep[0m  [6/21], [94mLoss[0m : 2.51352
[1mStep[0m  [8/21], [94mLoss[0m : 2.53177
[1mStep[0m  [10/21], [94mLoss[0m : 2.37104
[1mStep[0m  [12/21], [94mLoss[0m : 2.28029
[1mStep[0m  [14/21], [94mLoss[0m : 2.44699
[1mStep[0m  [16/21], [94mLoss[0m : 2.33508
[1mStep[0m  [18/21], [94mLoss[0m : 2.28634
[1mStep[0m  [20/21], [94mLoss[0m : 2.44494

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53718
[1mStep[0m  [2/21], [94mLoss[0m : 2.48078
[1mStep[0m  [4/21], [94mLoss[0m : 2.38641
[1mStep[0m  [6/21], [94mLoss[0m : 2.58102
[1mStep[0m  [8/21], [94mLoss[0m : 2.29308
[1mStep[0m  [10/21], [94mLoss[0m : 2.29826
[1mStep[0m  [12/21], [94mLoss[0m : 2.23383
[1mStep[0m  [14/21], [94mLoss[0m : 2.42251
[1mStep[0m  [16/21], [94mLoss[0m : 2.36244
[1mStep[0m  [18/21], [94mLoss[0m : 2.30365
[1mStep[0m  [20/21], [94mLoss[0m : 2.34867

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35589
[1mStep[0m  [2/21], [94mLoss[0m : 2.49920
[1mStep[0m  [4/21], [94mLoss[0m : 2.37853
[1mStep[0m  [6/21], [94mLoss[0m : 2.49114
[1mStep[0m  [8/21], [94mLoss[0m : 2.39965
[1mStep[0m  [10/21], [94mLoss[0m : 2.57756
[1mStep[0m  [12/21], [94mLoss[0m : 2.34813
[1mStep[0m  [14/21], [94mLoss[0m : 2.44972
[1mStep[0m  [16/21], [94mLoss[0m : 2.48734
[1mStep[0m  [18/21], [94mLoss[0m : 2.42754
[1mStep[0m  [20/21], [94mLoss[0m : 2.46812

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40183
[1mStep[0m  [2/21], [94mLoss[0m : 2.47638
[1mStep[0m  [4/21], [94mLoss[0m : 2.43975
[1mStep[0m  [6/21], [94mLoss[0m : 2.30906
[1mStep[0m  [8/21], [94mLoss[0m : 2.40776
[1mStep[0m  [10/21], [94mLoss[0m : 2.39791
[1mStep[0m  [12/21], [94mLoss[0m : 2.30551
[1mStep[0m  [14/21], [94mLoss[0m : 2.37411
[1mStep[0m  [16/21], [94mLoss[0m : 2.46212
[1mStep[0m  [18/21], [94mLoss[0m : 2.41723
[1mStep[0m  [20/21], [94mLoss[0m : 2.44338

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27930
[1mStep[0m  [2/21], [94mLoss[0m : 2.42656
[1mStep[0m  [4/21], [94mLoss[0m : 2.55599
[1mStep[0m  [6/21], [94mLoss[0m : 2.41883
[1mStep[0m  [8/21], [94mLoss[0m : 2.46432
[1mStep[0m  [10/21], [94mLoss[0m : 2.39518
[1mStep[0m  [12/21], [94mLoss[0m : 2.43656
[1mStep[0m  [14/21], [94mLoss[0m : 2.41725
[1mStep[0m  [16/21], [94mLoss[0m : 2.42655
[1mStep[0m  [18/21], [94mLoss[0m : 2.52681
[1mStep[0m  [20/21], [94mLoss[0m : 2.58692

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31939
[1mStep[0m  [2/21], [94mLoss[0m : 2.35674
[1mStep[0m  [4/21], [94mLoss[0m : 2.52461
[1mStep[0m  [6/21], [94mLoss[0m : 2.54297
[1mStep[0m  [8/21], [94mLoss[0m : 2.31856
[1mStep[0m  [10/21], [94mLoss[0m : 2.37704
[1mStep[0m  [12/21], [94mLoss[0m : 2.35141
[1mStep[0m  [14/21], [94mLoss[0m : 2.32474
[1mStep[0m  [16/21], [94mLoss[0m : 2.39939
[1mStep[0m  [18/21], [94mLoss[0m : 2.43514
[1mStep[0m  [20/21], [94mLoss[0m : 2.37049

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43476
[1mStep[0m  [2/21], [94mLoss[0m : 2.24745
[1mStep[0m  [4/21], [94mLoss[0m : 2.46377
[1mStep[0m  [6/21], [94mLoss[0m : 2.34028
[1mStep[0m  [8/21], [94mLoss[0m : 2.50598
[1mStep[0m  [10/21], [94mLoss[0m : 2.54831
[1mStep[0m  [12/21], [94mLoss[0m : 2.31539
[1mStep[0m  [14/21], [94mLoss[0m : 2.44581
[1mStep[0m  [16/21], [94mLoss[0m : 2.32793
[1mStep[0m  [18/21], [94mLoss[0m : 2.46799
[1mStep[0m  [20/21], [94mLoss[0m : 2.38612

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40280
[1mStep[0m  [2/21], [94mLoss[0m : 2.42692
[1mStep[0m  [4/21], [94mLoss[0m : 2.29113
[1mStep[0m  [6/21], [94mLoss[0m : 2.39957
[1mStep[0m  [8/21], [94mLoss[0m : 2.36846
[1mStep[0m  [10/21], [94mLoss[0m : 2.41862
[1mStep[0m  [12/21], [94mLoss[0m : 2.30097
[1mStep[0m  [14/21], [94mLoss[0m : 2.36149
[1mStep[0m  [16/21], [94mLoss[0m : 2.49635
[1mStep[0m  [18/21], [94mLoss[0m : 2.46263
[1mStep[0m  [20/21], [94mLoss[0m : 2.33200

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.334
====================================

Phase 1 - Evaluation MAE:  2.333538770675659
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.39506
[1mStep[0m  [2/21], [94mLoss[0m : 2.31562
[1mStep[0m  [4/21], [94mLoss[0m : 2.27445
[1mStep[0m  [6/21], [94mLoss[0m : 2.49285
[1mStep[0m  [8/21], [94mLoss[0m : 2.41098
[1mStep[0m  [10/21], [94mLoss[0m : 2.29064
[1mStep[0m  [12/21], [94mLoss[0m : 2.43981
[1mStep[0m  [14/21], [94mLoss[0m : 2.42416
[1mStep[0m  [16/21], [94mLoss[0m : 2.47808
[1mStep[0m  [18/21], [94mLoss[0m : 2.35244
[1mStep[0m  [20/21], [94mLoss[0m : 2.42256

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49508
[1mStep[0m  [2/21], [94mLoss[0m : 2.21930
[1mStep[0m  [4/21], [94mLoss[0m : 2.41656
[1mStep[0m  [6/21], [94mLoss[0m : 2.56682
[1mStep[0m  [8/21], [94mLoss[0m : 2.39302
[1mStep[0m  [10/21], [94mLoss[0m : 2.33892
[1mStep[0m  [12/21], [94mLoss[0m : 2.45525
[1mStep[0m  [14/21], [94mLoss[0m : 2.48853
[1mStep[0m  [16/21], [94mLoss[0m : 2.55010
[1mStep[0m  [18/21], [94mLoss[0m : 2.33071
[1mStep[0m  [20/21], [94mLoss[0m : 2.39219

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41273
[1mStep[0m  [2/21], [94mLoss[0m : 2.30733
[1mStep[0m  [4/21], [94mLoss[0m : 2.34659
[1mStep[0m  [6/21], [94mLoss[0m : 2.29498
[1mStep[0m  [8/21], [94mLoss[0m : 2.34017
[1mStep[0m  [10/21], [94mLoss[0m : 2.45928
[1mStep[0m  [12/21], [94mLoss[0m : 2.30015
[1mStep[0m  [14/21], [94mLoss[0m : 2.42134
[1mStep[0m  [16/21], [94mLoss[0m : 2.24542
[1mStep[0m  [18/21], [94mLoss[0m : 2.35486
[1mStep[0m  [20/21], [94mLoss[0m : 2.38462

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36162
[1mStep[0m  [2/21], [94mLoss[0m : 2.38620
[1mStep[0m  [4/21], [94mLoss[0m : 2.43905
[1mStep[0m  [6/21], [94mLoss[0m : 2.37149
[1mStep[0m  [8/21], [94mLoss[0m : 2.42671
[1mStep[0m  [10/21], [94mLoss[0m : 2.25904
[1mStep[0m  [12/21], [94mLoss[0m : 2.45486
[1mStep[0m  [14/21], [94mLoss[0m : 2.44306
[1mStep[0m  [16/21], [94mLoss[0m : 2.46584
[1mStep[0m  [18/21], [94mLoss[0m : 2.21928
[1mStep[0m  [20/21], [94mLoss[0m : 2.37368

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41755
[1mStep[0m  [2/21], [94mLoss[0m : 2.38167
[1mStep[0m  [4/21], [94mLoss[0m : 2.32163
[1mStep[0m  [6/21], [94mLoss[0m : 2.41002
[1mStep[0m  [8/21], [94mLoss[0m : 2.43118
[1mStep[0m  [10/21], [94mLoss[0m : 2.17470
[1mStep[0m  [12/21], [94mLoss[0m : 2.47586
[1mStep[0m  [14/21], [94mLoss[0m : 2.29638
[1mStep[0m  [16/21], [94mLoss[0m : 2.33124
[1mStep[0m  [18/21], [94mLoss[0m : 2.33407
[1mStep[0m  [20/21], [94mLoss[0m : 2.30987

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33351
[1mStep[0m  [2/21], [94mLoss[0m : 2.18995
[1mStep[0m  [4/21], [94mLoss[0m : 2.41286
[1mStep[0m  [6/21], [94mLoss[0m : 2.19996
[1mStep[0m  [8/21], [94mLoss[0m : 2.29776
[1mStep[0m  [10/21], [94mLoss[0m : 2.35919
[1mStep[0m  [12/21], [94mLoss[0m : 2.36032
[1mStep[0m  [14/21], [94mLoss[0m : 2.52547
[1mStep[0m  [16/21], [94mLoss[0m : 2.25883
[1mStep[0m  [18/21], [94mLoss[0m : 2.35222
[1mStep[0m  [20/21], [94mLoss[0m : 2.31948

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19346
[1mStep[0m  [2/21], [94mLoss[0m : 2.14007
[1mStep[0m  [4/21], [94mLoss[0m : 2.28883
[1mStep[0m  [6/21], [94mLoss[0m : 2.23664
[1mStep[0m  [8/21], [94mLoss[0m : 2.35171
[1mStep[0m  [10/21], [94mLoss[0m : 2.35478
[1mStep[0m  [12/21], [94mLoss[0m : 2.33274
[1mStep[0m  [14/21], [94mLoss[0m : 2.31622
[1mStep[0m  [16/21], [94mLoss[0m : 2.46603
[1mStep[0m  [18/21], [94mLoss[0m : 2.32492
[1mStep[0m  [20/21], [94mLoss[0m : 2.12994

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22903
[1mStep[0m  [2/21], [94mLoss[0m : 2.09919
[1mStep[0m  [4/21], [94mLoss[0m : 2.18322
[1mStep[0m  [6/21], [94mLoss[0m : 2.14557
[1mStep[0m  [8/21], [94mLoss[0m : 2.40525
[1mStep[0m  [10/21], [94mLoss[0m : 2.29489
[1mStep[0m  [12/21], [94mLoss[0m : 2.24707
[1mStep[0m  [14/21], [94mLoss[0m : 2.26187
[1mStep[0m  [16/21], [94mLoss[0m : 2.30929
[1mStep[0m  [18/21], [94mLoss[0m : 2.14344
[1mStep[0m  [20/21], [94mLoss[0m : 2.18949

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25490
[1mStep[0m  [2/21], [94mLoss[0m : 2.26032
[1mStep[0m  [4/21], [94mLoss[0m : 2.26095
[1mStep[0m  [6/21], [94mLoss[0m : 2.29554
[1mStep[0m  [8/21], [94mLoss[0m : 2.24459
[1mStep[0m  [10/21], [94mLoss[0m : 2.16536
[1mStep[0m  [12/21], [94mLoss[0m : 2.15672
[1mStep[0m  [14/21], [94mLoss[0m : 2.23773
[1mStep[0m  [16/21], [94mLoss[0m : 2.36816
[1mStep[0m  [18/21], [94mLoss[0m : 2.48111
[1mStep[0m  [20/21], [94mLoss[0m : 2.22212

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28826
[1mStep[0m  [2/21], [94mLoss[0m : 2.17757
[1mStep[0m  [4/21], [94mLoss[0m : 2.30371
[1mStep[0m  [6/21], [94mLoss[0m : 2.22007
[1mStep[0m  [8/21], [94mLoss[0m : 2.34829
[1mStep[0m  [10/21], [94mLoss[0m : 2.16104
[1mStep[0m  [12/21], [94mLoss[0m : 2.22832
[1mStep[0m  [14/21], [94mLoss[0m : 2.32231
[1mStep[0m  [16/21], [94mLoss[0m : 2.19870
[1mStep[0m  [18/21], [94mLoss[0m : 2.29018
[1mStep[0m  [20/21], [94mLoss[0m : 2.22866

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16327
[1mStep[0m  [2/21], [94mLoss[0m : 2.30671
[1mStep[0m  [4/21], [94mLoss[0m : 2.17181
[1mStep[0m  [6/21], [94mLoss[0m : 2.26811
[1mStep[0m  [8/21], [94mLoss[0m : 2.13932
[1mStep[0m  [10/21], [94mLoss[0m : 2.20136
[1mStep[0m  [12/21], [94mLoss[0m : 2.16469
[1mStep[0m  [14/21], [94mLoss[0m : 2.20392
[1mStep[0m  [16/21], [94mLoss[0m : 2.28611
[1mStep[0m  [18/21], [94mLoss[0m : 2.25579
[1mStep[0m  [20/21], [94mLoss[0m : 2.21108

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.190, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13451
[1mStep[0m  [2/21], [94mLoss[0m : 2.34596
[1mStep[0m  [4/21], [94mLoss[0m : 2.30925
[1mStep[0m  [6/21], [94mLoss[0m : 2.00482
[1mStep[0m  [8/21], [94mLoss[0m : 2.19588
[1mStep[0m  [10/21], [94mLoss[0m : 2.07402
[1mStep[0m  [12/21], [94mLoss[0m : 2.22963
[1mStep[0m  [14/21], [94mLoss[0m : 2.18084
[1mStep[0m  [16/21], [94mLoss[0m : 2.07968
[1mStep[0m  [18/21], [94mLoss[0m : 2.11848
[1mStep[0m  [20/21], [94mLoss[0m : 2.26918

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10470
[1mStep[0m  [2/21], [94mLoss[0m : 2.15912
[1mStep[0m  [4/21], [94mLoss[0m : 2.18462
[1mStep[0m  [6/21], [94mLoss[0m : 2.03021
[1mStep[0m  [8/21], [94mLoss[0m : 2.16744
[1mStep[0m  [10/21], [94mLoss[0m : 2.05750
[1mStep[0m  [12/21], [94mLoss[0m : 2.03703
[1mStep[0m  [14/21], [94mLoss[0m : 2.27127
[1mStep[0m  [16/21], [94mLoss[0m : 2.14303
[1mStep[0m  [18/21], [94mLoss[0m : 2.05855
[1mStep[0m  [20/21], [94mLoss[0m : 2.17503

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.00340
[1mStep[0m  [2/21], [94mLoss[0m : 2.16724
[1mStep[0m  [4/21], [94mLoss[0m : 2.03096
[1mStep[0m  [6/21], [94mLoss[0m : 2.12960
[1mStep[0m  [8/21], [94mLoss[0m : 2.23011
[1mStep[0m  [10/21], [94mLoss[0m : 2.00229
[1mStep[0m  [12/21], [94mLoss[0m : 2.14290
[1mStep[0m  [14/21], [94mLoss[0m : 2.23663
[1mStep[0m  [16/21], [94mLoss[0m : 1.96584
[1mStep[0m  [18/21], [94mLoss[0m : 2.13937
[1mStep[0m  [20/21], [94mLoss[0m : 2.14842

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06819
[1mStep[0m  [2/21], [94mLoss[0m : 2.35374
[1mStep[0m  [4/21], [94mLoss[0m : 2.00042
[1mStep[0m  [6/21], [94mLoss[0m : 2.14072
[1mStep[0m  [8/21], [94mLoss[0m : 2.08450
[1mStep[0m  [10/21], [94mLoss[0m : 2.07615
[1mStep[0m  [12/21], [94mLoss[0m : 1.94654
[1mStep[0m  [14/21], [94mLoss[0m : 2.13086
[1mStep[0m  [16/21], [94mLoss[0m : 2.23356
[1mStep[0m  [18/21], [94mLoss[0m : 2.16362
[1mStep[0m  [20/21], [94mLoss[0m : 2.04385

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.95329
[1mStep[0m  [2/21], [94mLoss[0m : 2.02600
[1mStep[0m  [4/21], [94mLoss[0m : 2.08561
[1mStep[0m  [6/21], [94mLoss[0m : 2.06077
[1mStep[0m  [8/21], [94mLoss[0m : 2.01782
[1mStep[0m  [10/21], [94mLoss[0m : 2.10456
[1mStep[0m  [12/21], [94mLoss[0m : 2.02564
[1mStep[0m  [14/21], [94mLoss[0m : 1.92057
[1mStep[0m  [16/21], [94mLoss[0m : 2.13800
[1mStep[0m  [18/21], [94mLoss[0m : 2.07540
[1mStep[0m  [20/21], [94mLoss[0m : 2.01765

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09133
[1mStep[0m  [2/21], [94mLoss[0m : 1.93571
[1mStep[0m  [4/21], [94mLoss[0m : 2.08879
[1mStep[0m  [6/21], [94mLoss[0m : 2.08774
[1mStep[0m  [8/21], [94mLoss[0m : 2.07552
[1mStep[0m  [10/21], [94mLoss[0m : 1.90613
[1mStep[0m  [12/21], [94mLoss[0m : 1.88091
[1mStep[0m  [14/21], [94mLoss[0m : 1.90560
[1mStep[0m  [16/21], [94mLoss[0m : 2.00440
[1mStep[0m  [18/21], [94mLoss[0m : 1.94957
[1mStep[0m  [20/21], [94mLoss[0m : 2.01264

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.82361
[1mStep[0m  [2/21], [94mLoss[0m : 1.94689
[1mStep[0m  [4/21], [94mLoss[0m : 1.86410
[1mStep[0m  [6/21], [94mLoss[0m : 2.01451
[1mStep[0m  [8/21], [94mLoss[0m : 2.01821
[1mStep[0m  [10/21], [94mLoss[0m : 2.09564
[1mStep[0m  [12/21], [94mLoss[0m : 1.89738
[1mStep[0m  [14/21], [94mLoss[0m : 2.02880
[1mStep[0m  [16/21], [94mLoss[0m : 2.16811
[1mStep[0m  [18/21], [94mLoss[0m : 1.90711
[1mStep[0m  [20/21], [94mLoss[0m : 1.99603

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96122
[1mStep[0m  [2/21], [94mLoss[0m : 1.74111
[1mStep[0m  [4/21], [94mLoss[0m : 2.06518
[1mStep[0m  [6/21], [94mLoss[0m : 1.83795
[1mStep[0m  [8/21], [94mLoss[0m : 2.00779
[1mStep[0m  [10/21], [94mLoss[0m : 1.85362
[1mStep[0m  [12/21], [94mLoss[0m : 1.99793
[1mStep[0m  [14/21], [94mLoss[0m : 2.00396
[1mStep[0m  [16/21], [94mLoss[0m : 1.93910
[1mStep[0m  [18/21], [94mLoss[0m : 2.09730
[1mStep[0m  [20/21], [94mLoss[0m : 1.87030

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91691
[1mStep[0m  [2/21], [94mLoss[0m : 1.81706
[1mStep[0m  [4/21], [94mLoss[0m : 1.93679
[1mStep[0m  [6/21], [94mLoss[0m : 2.02751
[1mStep[0m  [8/21], [94mLoss[0m : 1.84282
[1mStep[0m  [10/21], [94mLoss[0m : 1.99908
[1mStep[0m  [12/21], [94mLoss[0m : 1.95956
[1mStep[0m  [14/21], [94mLoss[0m : 1.86024
[1mStep[0m  [16/21], [94mLoss[0m : 1.93968
[1mStep[0m  [18/21], [94mLoss[0m : 1.73279
[1mStep[0m  [20/21], [94mLoss[0m : 1.92171

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.457, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86352
[1mStep[0m  [2/21], [94mLoss[0m : 1.91954
[1mStep[0m  [4/21], [94mLoss[0m : 2.00888
[1mStep[0m  [6/21], [94mLoss[0m : 1.92885
[1mStep[0m  [8/21], [94mLoss[0m : 1.78473
[1mStep[0m  [10/21], [94mLoss[0m : 1.77292
[1mStep[0m  [12/21], [94mLoss[0m : 1.89818
[1mStep[0m  [14/21], [94mLoss[0m : 1.92062
[1mStep[0m  [16/21], [94mLoss[0m : 1.82973
[1mStep[0m  [18/21], [94mLoss[0m : 1.89666
[1mStep[0m  [20/21], [94mLoss[0m : 1.99988

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.458, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78584
[1mStep[0m  [2/21], [94mLoss[0m : 1.86703
[1mStep[0m  [4/21], [94mLoss[0m : 1.72369
[1mStep[0m  [6/21], [94mLoss[0m : 1.78194
[1mStep[0m  [8/21], [94mLoss[0m : 1.86719
[1mStep[0m  [10/21], [94mLoss[0m : 1.75185
[1mStep[0m  [12/21], [94mLoss[0m : 1.94583
[1mStep[0m  [14/21], [94mLoss[0m : 1.86144
[1mStep[0m  [16/21], [94mLoss[0m : 1.85478
[1mStep[0m  [18/21], [94mLoss[0m : 1.80703
[1mStep[0m  [20/21], [94mLoss[0m : 1.84585

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79734
[1mStep[0m  [2/21], [94mLoss[0m : 1.80028
[1mStep[0m  [4/21], [94mLoss[0m : 1.76985
[1mStep[0m  [6/21], [94mLoss[0m : 1.85039
[1mStep[0m  [8/21], [94mLoss[0m : 1.79857
[1mStep[0m  [10/21], [94mLoss[0m : 1.77452
[1mStep[0m  [12/21], [94mLoss[0m : 1.82878
[1mStep[0m  [14/21], [94mLoss[0m : 1.93366
[1mStep[0m  [16/21], [94mLoss[0m : 1.70851
[1mStep[0m  [18/21], [94mLoss[0m : 1.79651
[1mStep[0m  [20/21], [94mLoss[0m : 1.74928

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.76981
[1mStep[0m  [2/21], [94mLoss[0m : 1.81041
[1mStep[0m  [4/21], [94mLoss[0m : 1.79044
[1mStep[0m  [6/21], [94mLoss[0m : 1.87970
[1mStep[0m  [8/21], [94mLoss[0m : 1.69137
[1mStep[0m  [10/21], [94mLoss[0m : 1.85237
[1mStep[0m  [12/21], [94mLoss[0m : 1.73071
[1mStep[0m  [14/21], [94mLoss[0m : 1.84474
[1mStep[0m  [16/21], [94mLoss[0m : 1.85262
[1mStep[0m  [18/21], [94mLoss[0m : 1.78220
[1mStep[0m  [20/21], [94mLoss[0m : 1.66258

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.453, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91791
[1mStep[0m  [2/21], [94mLoss[0m : 1.63230
[1mStep[0m  [4/21], [94mLoss[0m : 1.71635
[1mStep[0m  [6/21], [94mLoss[0m : 1.78984
[1mStep[0m  [8/21], [94mLoss[0m : 1.63711
[1mStep[0m  [10/21], [94mLoss[0m : 1.53205
[1mStep[0m  [12/21], [94mLoss[0m : 1.86175
[1mStep[0m  [14/21], [94mLoss[0m : 1.74583
[1mStep[0m  [16/21], [94mLoss[0m : 1.74429
[1mStep[0m  [18/21], [94mLoss[0m : 1.88030
[1mStep[0m  [20/21], [94mLoss[0m : 1.76437

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.586, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.64643
[1mStep[0m  [2/21], [94mLoss[0m : 1.81923
[1mStep[0m  [4/21], [94mLoss[0m : 1.75033
[1mStep[0m  [6/21], [94mLoss[0m : 1.66421
[1mStep[0m  [8/21], [94mLoss[0m : 1.66084
[1mStep[0m  [10/21], [94mLoss[0m : 1.78643
[1mStep[0m  [12/21], [94mLoss[0m : 1.60676
[1mStep[0m  [14/21], [94mLoss[0m : 1.73879
[1mStep[0m  [16/21], [94mLoss[0m : 1.78091
[1mStep[0m  [18/21], [94mLoss[0m : 1.81936
[1mStep[0m  [20/21], [94mLoss[0m : 1.79910

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.564, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.62268
[1mStep[0m  [2/21], [94mLoss[0m : 1.65424
[1mStep[0m  [4/21], [94mLoss[0m : 1.69355
[1mStep[0m  [6/21], [94mLoss[0m : 1.73853
[1mStep[0m  [8/21], [94mLoss[0m : 1.70006
[1mStep[0m  [10/21], [94mLoss[0m : 1.71873
[1mStep[0m  [12/21], [94mLoss[0m : 1.90113
[1mStep[0m  [14/21], [94mLoss[0m : 1.65711
[1mStep[0m  [16/21], [94mLoss[0m : 1.57286
[1mStep[0m  [18/21], [94mLoss[0m : 1.72014
[1mStep[0m  [20/21], [94mLoss[0m : 1.74372

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.65199
[1mStep[0m  [2/21], [94mLoss[0m : 1.52886
[1mStep[0m  [4/21], [94mLoss[0m : 1.61654
[1mStep[0m  [6/21], [94mLoss[0m : 1.65829
[1mStep[0m  [8/21], [94mLoss[0m : 1.71084
[1mStep[0m  [10/21], [94mLoss[0m : 1.69162
[1mStep[0m  [12/21], [94mLoss[0m : 1.69666
[1mStep[0m  [14/21], [94mLoss[0m : 1.69430
[1mStep[0m  [16/21], [94mLoss[0m : 1.66514
[1mStep[0m  [18/21], [94mLoss[0m : 1.73448
[1mStep[0m  [20/21], [94mLoss[0m : 1.67409

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.60486
[1mStep[0m  [2/21], [94mLoss[0m : 1.61653
[1mStep[0m  [4/21], [94mLoss[0m : 1.73330
[1mStep[0m  [6/21], [94mLoss[0m : 1.57826
[1mStep[0m  [8/21], [94mLoss[0m : 1.64062
[1mStep[0m  [10/21], [94mLoss[0m : 1.69625
[1mStep[0m  [12/21], [94mLoss[0m : 1.72567
[1mStep[0m  [14/21], [94mLoss[0m : 1.67866
[1mStep[0m  [16/21], [94mLoss[0m : 1.50130
[1mStep[0m  [18/21], [94mLoss[0m : 1.64509
[1mStep[0m  [20/21], [94mLoss[0m : 1.63357

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.50897
[1mStep[0m  [2/21], [94mLoss[0m : 1.52590
[1mStep[0m  [4/21], [94mLoss[0m : 1.65642
[1mStep[0m  [6/21], [94mLoss[0m : 1.52781
[1mStep[0m  [8/21], [94mLoss[0m : 1.77530
[1mStep[0m  [10/21], [94mLoss[0m : 1.60191
[1mStep[0m  [12/21], [94mLoss[0m : 1.66324
[1mStep[0m  [14/21], [94mLoss[0m : 1.60634
[1mStep[0m  [16/21], [94mLoss[0m : 1.59887
[1mStep[0m  [18/21], [94mLoss[0m : 1.64668
[1mStep[0m  [20/21], [94mLoss[0m : 1.59762

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.583, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.4794249534606934
MAE score P1      2.333539
MAE score P2      2.479425
loss              1.623934
learning_rate     0.007525
batch_size             512
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.23772
[1mStep[0m  [4/42], [94mLoss[0m : 9.11863
[1mStep[0m  [8/42], [94mLoss[0m : 7.37326
[1mStep[0m  [12/42], [94mLoss[0m : 4.98837
[1mStep[0m  [16/42], [94mLoss[0m : 3.05832
[1mStep[0m  [20/42], [94mLoss[0m : 2.92944
[1mStep[0m  [24/42], [94mLoss[0m : 3.13069
[1mStep[0m  [28/42], [94mLoss[0m : 3.21412
[1mStep[0m  [32/42], [94mLoss[0m : 3.11660
[1mStep[0m  [36/42], [94mLoss[0m : 2.72867
[1mStep[0m  [40/42], [94mLoss[0m : 2.71394

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.642, [92mTest[0m: 10.216, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71647
[1mStep[0m  [4/42], [94mLoss[0m : 3.00798
[1mStep[0m  [8/42], [94mLoss[0m : 2.71648
[1mStep[0m  [12/42], [94mLoss[0m : 2.79251
[1mStep[0m  [16/42], [94mLoss[0m : 2.68537
[1mStep[0m  [20/42], [94mLoss[0m : 2.60995
[1mStep[0m  [24/42], [94mLoss[0m : 2.60166
[1mStep[0m  [28/42], [94mLoss[0m : 2.58626
[1mStep[0m  [32/42], [94mLoss[0m : 2.66354
[1mStep[0m  [36/42], [94mLoss[0m : 2.56561
[1mStep[0m  [40/42], [94mLoss[0m : 2.64794

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.711, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87195
[1mStep[0m  [4/42], [94mLoss[0m : 2.56846
[1mStep[0m  [8/42], [94mLoss[0m : 2.81519
[1mStep[0m  [12/42], [94mLoss[0m : 2.63661
[1mStep[0m  [16/42], [94mLoss[0m : 2.61803
[1mStep[0m  [20/42], [94mLoss[0m : 2.53938
[1mStep[0m  [24/42], [94mLoss[0m : 2.64914
[1mStep[0m  [28/42], [94mLoss[0m : 2.40388
[1mStep[0m  [32/42], [94mLoss[0m : 2.61497
[1mStep[0m  [36/42], [94mLoss[0m : 2.62639
[1mStep[0m  [40/42], [94mLoss[0m : 2.54527

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75640
[1mStep[0m  [4/42], [94mLoss[0m : 2.65021
[1mStep[0m  [8/42], [94mLoss[0m : 2.71933
[1mStep[0m  [12/42], [94mLoss[0m : 2.72693
[1mStep[0m  [16/42], [94mLoss[0m : 2.66427
[1mStep[0m  [20/42], [94mLoss[0m : 2.47328
[1mStep[0m  [24/42], [94mLoss[0m : 2.64723
[1mStep[0m  [28/42], [94mLoss[0m : 2.48697
[1mStep[0m  [32/42], [94mLoss[0m : 2.67223
[1mStep[0m  [36/42], [94mLoss[0m : 2.34305
[1mStep[0m  [40/42], [94mLoss[0m : 2.56173

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70877
[1mStep[0m  [4/42], [94mLoss[0m : 2.45868
[1mStep[0m  [8/42], [94mLoss[0m : 2.66746
[1mStep[0m  [12/42], [94mLoss[0m : 2.77506
[1mStep[0m  [16/42], [94mLoss[0m : 2.63148
[1mStep[0m  [20/42], [94mLoss[0m : 2.69937
[1mStep[0m  [24/42], [94mLoss[0m : 2.51350
[1mStep[0m  [28/42], [94mLoss[0m : 2.60515
[1mStep[0m  [32/42], [94mLoss[0m : 2.39012
[1mStep[0m  [36/42], [94mLoss[0m : 2.60944
[1mStep[0m  [40/42], [94mLoss[0m : 2.76661

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52129
[1mStep[0m  [4/42], [94mLoss[0m : 2.54082
[1mStep[0m  [8/42], [94mLoss[0m : 2.43068
[1mStep[0m  [12/42], [94mLoss[0m : 2.67676
[1mStep[0m  [16/42], [94mLoss[0m : 2.32301
[1mStep[0m  [20/42], [94mLoss[0m : 2.99561
[1mStep[0m  [24/42], [94mLoss[0m : 2.72394
[1mStep[0m  [28/42], [94mLoss[0m : 2.52371
[1mStep[0m  [32/42], [94mLoss[0m : 2.55491
[1mStep[0m  [36/42], [94mLoss[0m : 2.44065
[1mStep[0m  [40/42], [94mLoss[0m : 2.67359

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36171
[1mStep[0m  [4/42], [94mLoss[0m : 2.51260
[1mStep[0m  [8/42], [94mLoss[0m : 2.52089
[1mStep[0m  [12/42], [94mLoss[0m : 2.81139
[1mStep[0m  [16/42], [94mLoss[0m : 2.55275
[1mStep[0m  [20/42], [94mLoss[0m : 2.63501
[1mStep[0m  [24/42], [94mLoss[0m : 2.77998
[1mStep[0m  [28/42], [94mLoss[0m : 2.57106
[1mStep[0m  [32/42], [94mLoss[0m : 2.70854
[1mStep[0m  [36/42], [94mLoss[0m : 2.58701
[1mStep[0m  [40/42], [94mLoss[0m : 2.48173

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44372
[1mStep[0m  [4/42], [94mLoss[0m : 2.53056
[1mStep[0m  [8/42], [94mLoss[0m : 2.38358
[1mStep[0m  [12/42], [94mLoss[0m : 2.54654
[1mStep[0m  [16/42], [94mLoss[0m : 2.65375
[1mStep[0m  [20/42], [94mLoss[0m : 2.75652
[1mStep[0m  [24/42], [94mLoss[0m : 2.39039
[1mStep[0m  [28/42], [94mLoss[0m : 2.57520
[1mStep[0m  [32/42], [94mLoss[0m : 2.66174
[1mStep[0m  [36/42], [94mLoss[0m : 2.75681
[1mStep[0m  [40/42], [94mLoss[0m : 2.63298

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61558
[1mStep[0m  [4/42], [94mLoss[0m : 2.50829
[1mStep[0m  [8/42], [94mLoss[0m : 2.74534
[1mStep[0m  [12/42], [94mLoss[0m : 2.51853
[1mStep[0m  [16/42], [94mLoss[0m : 2.64720
[1mStep[0m  [20/42], [94mLoss[0m : 2.52886
[1mStep[0m  [24/42], [94mLoss[0m : 2.71494
[1mStep[0m  [28/42], [94mLoss[0m : 2.28775
[1mStep[0m  [32/42], [94mLoss[0m : 2.64905
[1mStep[0m  [36/42], [94mLoss[0m : 2.46657
[1mStep[0m  [40/42], [94mLoss[0m : 2.45903

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44956
[1mStep[0m  [4/42], [94mLoss[0m : 2.55477
[1mStep[0m  [8/42], [94mLoss[0m : 2.59595
[1mStep[0m  [12/42], [94mLoss[0m : 2.53293
[1mStep[0m  [16/42], [94mLoss[0m : 2.78180
[1mStep[0m  [20/42], [94mLoss[0m : 2.38080
[1mStep[0m  [24/42], [94mLoss[0m : 2.54146
[1mStep[0m  [28/42], [94mLoss[0m : 2.43427
[1mStep[0m  [32/42], [94mLoss[0m : 2.62053
[1mStep[0m  [36/42], [94mLoss[0m : 2.56430
[1mStep[0m  [40/42], [94mLoss[0m : 2.39717

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61539
[1mStep[0m  [4/42], [94mLoss[0m : 2.61216
[1mStep[0m  [8/42], [94mLoss[0m : 2.57437
[1mStep[0m  [12/42], [94mLoss[0m : 2.60636
[1mStep[0m  [16/42], [94mLoss[0m : 2.53328
[1mStep[0m  [20/42], [94mLoss[0m : 2.56281
[1mStep[0m  [24/42], [94mLoss[0m : 2.50861
[1mStep[0m  [28/42], [94mLoss[0m : 2.61924
[1mStep[0m  [32/42], [94mLoss[0m : 2.50943
[1mStep[0m  [36/42], [94mLoss[0m : 2.54341
[1mStep[0m  [40/42], [94mLoss[0m : 2.51530

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41137
[1mStep[0m  [4/42], [94mLoss[0m : 2.45742
[1mStep[0m  [8/42], [94mLoss[0m : 2.29552
[1mStep[0m  [12/42], [94mLoss[0m : 2.56498
[1mStep[0m  [16/42], [94mLoss[0m : 2.62424
[1mStep[0m  [20/42], [94mLoss[0m : 2.59268
[1mStep[0m  [24/42], [94mLoss[0m : 2.55692
[1mStep[0m  [28/42], [94mLoss[0m : 2.59440
[1mStep[0m  [32/42], [94mLoss[0m : 2.53787
[1mStep[0m  [36/42], [94mLoss[0m : 2.77788
[1mStep[0m  [40/42], [94mLoss[0m : 2.51270

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52633
[1mStep[0m  [4/42], [94mLoss[0m : 2.41187
[1mStep[0m  [8/42], [94mLoss[0m : 2.63382
[1mStep[0m  [12/42], [94mLoss[0m : 2.62335
[1mStep[0m  [16/42], [94mLoss[0m : 2.69643
[1mStep[0m  [20/42], [94mLoss[0m : 2.67434
[1mStep[0m  [24/42], [94mLoss[0m : 2.69655
[1mStep[0m  [28/42], [94mLoss[0m : 2.64667
[1mStep[0m  [32/42], [94mLoss[0m : 2.68027
[1mStep[0m  [36/42], [94mLoss[0m : 2.42627
[1mStep[0m  [40/42], [94mLoss[0m : 2.56224

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61909
[1mStep[0m  [4/42], [94mLoss[0m : 2.63824
[1mStep[0m  [8/42], [94mLoss[0m : 2.88053
[1mStep[0m  [12/42], [94mLoss[0m : 2.61296
[1mStep[0m  [16/42], [94mLoss[0m : 2.35253
[1mStep[0m  [20/42], [94mLoss[0m : 2.58682
[1mStep[0m  [24/42], [94mLoss[0m : 2.48613
[1mStep[0m  [28/42], [94mLoss[0m : 2.58744
[1mStep[0m  [32/42], [94mLoss[0m : 2.40256
[1mStep[0m  [36/42], [94mLoss[0m : 2.34994
[1mStep[0m  [40/42], [94mLoss[0m : 2.54087

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60643
[1mStep[0m  [4/42], [94mLoss[0m : 2.48788
[1mStep[0m  [8/42], [94mLoss[0m : 2.69950
[1mStep[0m  [12/42], [94mLoss[0m : 2.63759
[1mStep[0m  [16/42], [94mLoss[0m : 2.71849
[1mStep[0m  [20/42], [94mLoss[0m : 2.52952
[1mStep[0m  [24/42], [94mLoss[0m : 2.48663
[1mStep[0m  [28/42], [94mLoss[0m : 2.49409
[1mStep[0m  [32/42], [94mLoss[0m : 2.36777
[1mStep[0m  [36/42], [94mLoss[0m : 2.60014
[1mStep[0m  [40/42], [94mLoss[0m : 2.65757

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47064
[1mStep[0m  [4/42], [94mLoss[0m : 2.47348
[1mStep[0m  [8/42], [94mLoss[0m : 2.68735
[1mStep[0m  [12/42], [94mLoss[0m : 2.49868
[1mStep[0m  [16/42], [94mLoss[0m : 2.46454
[1mStep[0m  [20/42], [94mLoss[0m : 2.69984
[1mStep[0m  [24/42], [94mLoss[0m : 2.54137
[1mStep[0m  [28/42], [94mLoss[0m : 2.30888
[1mStep[0m  [32/42], [94mLoss[0m : 2.69851
[1mStep[0m  [36/42], [94mLoss[0m : 2.49628
[1mStep[0m  [40/42], [94mLoss[0m : 2.64513

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58374
[1mStep[0m  [4/42], [94mLoss[0m : 2.46087
[1mStep[0m  [8/42], [94mLoss[0m : 2.63047
[1mStep[0m  [12/42], [94mLoss[0m : 2.72157
[1mStep[0m  [16/42], [94mLoss[0m : 2.50287
[1mStep[0m  [20/42], [94mLoss[0m : 2.61130
[1mStep[0m  [24/42], [94mLoss[0m : 2.46486
[1mStep[0m  [28/42], [94mLoss[0m : 2.48248
[1mStep[0m  [32/42], [94mLoss[0m : 2.49652
[1mStep[0m  [36/42], [94mLoss[0m : 2.62953
[1mStep[0m  [40/42], [94mLoss[0m : 2.39375

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31530
[1mStep[0m  [4/42], [94mLoss[0m : 2.44517
[1mStep[0m  [8/42], [94mLoss[0m : 2.59016
[1mStep[0m  [12/42], [94mLoss[0m : 2.40996
[1mStep[0m  [16/42], [94mLoss[0m : 2.64301
[1mStep[0m  [20/42], [94mLoss[0m : 2.46112
[1mStep[0m  [24/42], [94mLoss[0m : 2.64064
[1mStep[0m  [28/42], [94mLoss[0m : 2.56381
[1mStep[0m  [32/42], [94mLoss[0m : 2.36260
[1mStep[0m  [36/42], [94mLoss[0m : 2.59958
[1mStep[0m  [40/42], [94mLoss[0m : 2.61987

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65057
[1mStep[0m  [4/42], [94mLoss[0m : 2.46538
[1mStep[0m  [8/42], [94mLoss[0m : 2.23861
[1mStep[0m  [12/42], [94mLoss[0m : 2.66738
[1mStep[0m  [16/42], [94mLoss[0m : 2.44166
[1mStep[0m  [20/42], [94mLoss[0m : 2.64094
[1mStep[0m  [24/42], [94mLoss[0m : 2.44418
[1mStep[0m  [28/42], [94mLoss[0m : 2.50084
[1mStep[0m  [32/42], [94mLoss[0m : 2.46932
[1mStep[0m  [36/42], [94mLoss[0m : 2.42917
[1mStep[0m  [40/42], [94mLoss[0m : 2.51442

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69533
[1mStep[0m  [4/42], [94mLoss[0m : 2.66896
[1mStep[0m  [8/42], [94mLoss[0m : 2.52169
[1mStep[0m  [12/42], [94mLoss[0m : 2.64347
[1mStep[0m  [16/42], [94mLoss[0m : 2.63371
[1mStep[0m  [20/42], [94mLoss[0m : 2.51092
[1mStep[0m  [24/42], [94mLoss[0m : 2.49397
[1mStep[0m  [28/42], [94mLoss[0m : 2.70874
[1mStep[0m  [32/42], [94mLoss[0m : 2.50814
[1mStep[0m  [36/42], [94mLoss[0m : 2.63603
[1mStep[0m  [40/42], [94mLoss[0m : 2.34587

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44356
[1mStep[0m  [4/42], [94mLoss[0m : 2.44060
[1mStep[0m  [8/42], [94mLoss[0m : 2.50160
[1mStep[0m  [12/42], [94mLoss[0m : 2.47332
[1mStep[0m  [16/42], [94mLoss[0m : 2.60977
[1mStep[0m  [20/42], [94mLoss[0m : 2.51412
[1mStep[0m  [24/42], [94mLoss[0m : 2.60817
[1mStep[0m  [28/42], [94mLoss[0m : 2.81127
[1mStep[0m  [32/42], [94mLoss[0m : 2.54851
[1mStep[0m  [36/42], [94mLoss[0m : 2.45924
[1mStep[0m  [40/42], [94mLoss[0m : 2.38284

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60772
[1mStep[0m  [4/42], [94mLoss[0m : 2.80800
[1mStep[0m  [8/42], [94mLoss[0m : 2.70736
[1mStep[0m  [12/42], [94mLoss[0m : 2.60373
[1mStep[0m  [16/42], [94mLoss[0m : 2.50318
[1mStep[0m  [20/42], [94mLoss[0m : 2.44820
[1mStep[0m  [24/42], [94mLoss[0m : 2.62905
[1mStep[0m  [28/42], [94mLoss[0m : 2.44070
[1mStep[0m  [32/42], [94mLoss[0m : 2.46676
[1mStep[0m  [36/42], [94mLoss[0m : 2.25207
[1mStep[0m  [40/42], [94mLoss[0m : 2.67374

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60177
[1mStep[0m  [4/42], [94mLoss[0m : 2.70669
[1mStep[0m  [8/42], [94mLoss[0m : 2.47375
[1mStep[0m  [12/42], [94mLoss[0m : 2.42992
[1mStep[0m  [16/42], [94mLoss[0m : 2.91250
[1mStep[0m  [20/42], [94mLoss[0m : 2.63997
[1mStep[0m  [24/42], [94mLoss[0m : 2.48762
[1mStep[0m  [28/42], [94mLoss[0m : 2.43007
[1mStep[0m  [32/42], [94mLoss[0m : 2.49704
[1mStep[0m  [36/42], [94mLoss[0m : 2.40484
[1mStep[0m  [40/42], [94mLoss[0m : 2.43754

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65084
[1mStep[0m  [4/42], [94mLoss[0m : 2.50135
[1mStep[0m  [8/42], [94mLoss[0m : 2.66007
[1mStep[0m  [12/42], [94mLoss[0m : 2.43034
[1mStep[0m  [16/42], [94mLoss[0m : 2.49999
[1mStep[0m  [20/42], [94mLoss[0m : 2.59619
[1mStep[0m  [24/42], [94mLoss[0m : 2.56549
[1mStep[0m  [28/42], [94mLoss[0m : 2.45118
[1mStep[0m  [32/42], [94mLoss[0m : 2.44923
[1mStep[0m  [36/42], [94mLoss[0m : 2.81976
[1mStep[0m  [40/42], [94mLoss[0m : 2.35230

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42455
[1mStep[0m  [4/42], [94mLoss[0m : 2.52930
[1mStep[0m  [8/42], [94mLoss[0m : 2.46338
[1mStep[0m  [12/42], [94mLoss[0m : 2.41347
[1mStep[0m  [16/42], [94mLoss[0m : 2.32711
[1mStep[0m  [20/42], [94mLoss[0m : 2.40157
[1mStep[0m  [24/42], [94mLoss[0m : 2.51415
[1mStep[0m  [28/42], [94mLoss[0m : 2.56400
[1mStep[0m  [32/42], [94mLoss[0m : 2.47143
[1mStep[0m  [36/42], [94mLoss[0m : 2.63459
[1mStep[0m  [40/42], [94mLoss[0m : 2.45984

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51649
[1mStep[0m  [4/42], [94mLoss[0m : 2.39255
[1mStep[0m  [8/42], [94mLoss[0m : 2.29474
[1mStep[0m  [12/42], [94mLoss[0m : 2.56550
[1mStep[0m  [16/42], [94mLoss[0m : 2.55735
[1mStep[0m  [20/42], [94mLoss[0m : 2.27868
[1mStep[0m  [24/42], [94mLoss[0m : 2.33095
[1mStep[0m  [28/42], [94mLoss[0m : 2.40387
[1mStep[0m  [32/42], [94mLoss[0m : 2.38230
[1mStep[0m  [36/42], [94mLoss[0m : 2.67957
[1mStep[0m  [40/42], [94mLoss[0m : 2.60704

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58889
[1mStep[0m  [4/42], [94mLoss[0m : 2.38506
[1mStep[0m  [8/42], [94mLoss[0m : 2.44465
[1mStep[0m  [12/42], [94mLoss[0m : 2.52691
[1mStep[0m  [16/42], [94mLoss[0m : 2.60128
[1mStep[0m  [20/42], [94mLoss[0m : 2.43590
[1mStep[0m  [24/42], [94mLoss[0m : 2.38284
[1mStep[0m  [28/42], [94mLoss[0m : 2.70106
[1mStep[0m  [32/42], [94mLoss[0m : 2.37364
[1mStep[0m  [36/42], [94mLoss[0m : 2.51066
[1mStep[0m  [40/42], [94mLoss[0m : 2.49731

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71724
[1mStep[0m  [4/42], [94mLoss[0m : 2.56806
[1mStep[0m  [8/42], [94mLoss[0m : 2.40013
[1mStep[0m  [12/42], [94mLoss[0m : 2.46942
[1mStep[0m  [16/42], [94mLoss[0m : 2.57071
[1mStep[0m  [20/42], [94mLoss[0m : 2.51422
[1mStep[0m  [24/42], [94mLoss[0m : 2.40492
[1mStep[0m  [28/42], [94mLoss[0m : 2.46485
[1mStep[0m  [32/42], [94mLoss[0m : 2.44447
[1mStep[0m  [36/42], [94mLoss[0m : 2.42777
[1mStep[0m  [40/42], [94mLoss[0m : 2.42743

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49615
[1mStep[0m  [4/42], [94mLoss[0m : 2.57204
[1mStep[0m  [8/42], [94mLoss[0m : 2.36516
[1mStep[0m  [12/42], [94mLoss[0m : 2.52190
[1mStep[0m  [16/42], [94mLoss[0m : 2.62862
[1mStep[0m  [20/42], [94mLoss[0m : 2.36507
[1mStep[0m  [24/42], [94mLoss[0m : 2.60815
[1mStep[0m  [28/42], [94mLoss[0m : 2.65722
[1mStep[0m  [32/42], [94mLoss[0m : 2.59878
[1mStep[0m  [36/42], [94mLoss[0m : 2.58781
[1mStep[0m  [40/42], [94mLoss[0m : 2.84728

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52050
[1mStep[0m  [4/42], [94mLoss[0m : 2.49497
[1mStep[0m  [8/42], [94mLoss[0m : 2.45681
[1mStep[0m  [12/42], [94mLoss[0m : 2.40284
[1mStep[0m  [16/42], [94mLoss[0m : 2.43155
[1mStep[0m  [20/42], [94mLoss[0m : 2.36842
[1mStep[0m  [24/42], [94mLoss[0m : 2.36371
[1mStep[0m  [28/42], [94mLoss[0m : 2.59694
[1mStep[0m  [32/42], [94mLoss[0m : 2.48065
[1mStep[0m  [36/42], [94mLoss[0m : 2.72302
[1mStep[0m  [40/42], [94mLoss[0m : 2.36636

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3324121066502164
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.35022
[1mStep[0m  [4/42], [94mLoss[0m : 2.62765
[1mStep[0m  [8/42], [94mLoss[0m : 2.53406
[1mStep[0m  [12/42], [94mLoss[0m : 2.51651
[1mStep[0m  [16/42], [94mLoss[0m : 2.68136
[1mStep[0m  [20/42], [94mLoss[0m : 2.68056
[1mStep[0m  [24/42], [94mLoss[0m : 2.72420
[1mStep[0m  [28/42], [94mLoss[0m : 2.50201
[1mStep[0m  [32/42], [94mLoss[0m : 2.68972
[1mStep[0m  [36/42], [94mLoss[0m : 2.59241
[1mStep[0m  [40/42], [94mLoss[0m : 2.44629

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48500
[1mStep[0m  [4/42], [94mLoss[0m : 2.40454
[1mStep[0m  [8/42], [94mLoss[0m : 2.53205
[1mStep[0m  [12/42], [94mLoss[0m : 2.23999
[1mStep[0m  [16/42], [94mLoss[0m : 2.55574
[1mStep[0m  [20/42], [94mLoss[0m : 2.48927
[1mStep[0m  [24/42], [94mLoss[0m : 2.42983
[1mStep[0m  [28/42], [94mLoss[0m : 2.36955
[1mStep[0m  [32/42], [94mLoss[0m : 2.50902
[1mStep[0m  [36/42], [94mLoss[0m : 2.37219
[1mStep[0m  [40/42], [94mLoss[0m : 2.42397

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20344
[1mStep[0m  [4/42], [94mLoss[0m : 2.32663
[1mStep[0m  [8/42], [94mLoss[0m : 2.33440
[1mStep[0m  [12/42], [94mLoss[0m : 2.16883
[1mStep[0m  [16/42], [94mLoss[0m : 2.53342
[1mStep[0m  [20/42], [94mLoss[0m : 2.17134
[1mStep[0m  [24/42], [94mLoss[0m : 2.41214
[1mStep[0m  [28/42], [94mLoss[0m : 2.49845
[1mStep[0m  [32/42], [94mLoss[0m : 2.38322
[1mStep[0m  [36/42], [94mLoss[0m : 2.32118
[1mStep[0m  [40/42], [94mLoss[0m : 2.37604

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.541, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25376
[1mStep[0m  [4/42], [94mLoss[0m : 2.39619
[1mStep[0m  [8/42], [94mLoss[0m : 2.12290
[1mStep[0m  [12/42], [94mLoss[0m : 2.37306
[1mStep[0m  [16/42], [94mLoss[0m : 2.08033
[1mStep[0m  [20/42], [94mLoss[0m : 2.08385
[1mStep[0m  [24/42], [94mLoss[0m : 2.38455
[1mStep[0m  [28/42], [94mLoss[0m : 2.13613
[1mStep[0m  [32/42], [94mLoss[0m : 2.26016
[1mStep[0m  [36/42], [94mLoss[0m : 2.36927
[1mStep[0m  [40/42], [94mLoss[0m : 2.29449

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18403
[1mStep[0m  [4/42], [94mLoss[0m : 1.92965
[1mStep[0m  [8/42], [94mLoss[0m : 1.92617
[1mStep[0m  [12/42], [94mLoss[0m : 2.30143
[1mStep[0m  [16/42], [94mLoss[0m : 2.34985
[1mStep[0m  [20/42], [94mLoss[0m : 2.13419
[1mStep[0m  [24/42], [94mLoss[0m : 2.28631
[1mStep[0m  [28/42], [94mLoss[0m : 2.24305
[1mStep[0m  [32/42], [94mLoss[0m : 2.33220
[1mStep[0m  [36/42], [94mLoss[0m : 2.37658
[1mStep[0m  [40/42], [94mLoss[0m : 2.24242

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99735
[1mStep[0m  [4/42], [94mLoss[0m : 2.01145
[1mStep[0m  [8/42], [94mLoss[0m : 2.21907
[1mStep[0m  [12/42], [94mLoss[0m : 2.16523
[1mStep[0m  [16/42], [94mLoss[0m : 2.23356
[1mStep[0m  [20/42], [94mLoss[0m : 2.20581
[1mStep[0m  [24/42], [94mLoss[0m : 1.94681
[1mStep[0m  [28/42], [94mLoss[0m : 2.27610
[1mStep[0m  [32/42], [94mLoss[0m : 2.13001
[1mStep[0m  [36/42], [94mLoss[0m : 2.25304
[1mStep[0m  [40/42], [94mLoss[0m : 2.02338

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95712
[1mStep[0m  [4/42], [94mLoss[0m : 2.20743
[1mStep[0m  [8/42], [94mLoss[0m : 2.07351
[1mStep[0m  [12/42], [94mLoss[0m : 1.97400
[1mStep[0m  [16/42], [94mLoss[0m : 2.35445
[1mStep[0m  [20/42], [94mLoss[0m : 1.94139
[1mStep[0m  [24/42], [94mLoss[0m : 1.96353
[1mStep[0m  [28/42], [94mLoss[0m : 2.28257
[1mStep[0m  [32/42], [94mLoss[0m : 2.05559
[1mStep[0m  [36/42], [94mLoss[0m : 2.15581
[1mStep[0m  [40/42], [94mLoss[0m : 2.13632

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11581
[1mStep[0m  [4/42], [94mLoss[0m : 1.93544
[1mStep[0m  [8/42], [94mLoss[0m : 1.86218
[1mStep[0m  [12/42], [94mLoss[0m : 2.13406
[1mStep[0m  [16/42], [94mLoss[0m : 2.03996
[1mStep[0m  [20/42], [94mLoss[0m : 2.17984
[1mStep[0m  [24/42], [94mLoss[0m : 2.08360
[1mStep[0m  [28/42], [94mLoss[0m : 1.98933
[1mStep[0m  [32/42], [94mLoss[0m : 2.06450
[1mStep[0m  [36/42], [94mLoss[0m : 2.14415
[1mStep[0m  [40/42], [94mLoss[0m : 2.09643

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99717
[1mStep[0m  [4/42], [94mLoss[0m : 1.88092
[1mStep[0m  [8/42], [94mLoss[0m : 1.96417
[1mStep[0m  [12/42], [94mLoss[0m : 1.78484
[1mStep[0m  [16/42], [94mLoss[0m : 1.85889
[1mStep[0m  [20/42], [94mLoss[0m : 2.03231
[1mStep[0m  [24/42], [94mLoss[0m : 1.95144
[1mStep[0m  [28/42], [94mLoss[0m : 2.11430
[1mStep[0m  [32/42], [94mLoss[0m : 2.07337
[1mStep[0m  [36/42], [94mLoss[0m : 1.89631
[1mStep[0m  [40/42], [94mLoss[0m : 2.00025

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72696
[1mStep[0m  [4/42], [94mLoss[0m : 1.71467
[1mStep[0m  [8/42], [94mLoss[0m : 1.95844
[1mStep[0m  [12/42], [94mLoss[0m : 1.82366
[1mStep[0m  [16/42], [94mLoss[0m : 1.91351
[1mStep[0m  [20/42], [94mLoss[0m : 1.83647
[1mStep[0m  [24/42], [94mLoss[0m : 1.88489
[1mStep[0m  [28/42], [94mLoss[0m : 1.89302
[1mStep[0m  [32/42], [94mLoss[0m : 1.69340
[1mStep[0m  [36/42], [94mLoss[0m : 1.82795
[1mStep[0m  [40/42], [94mLoss[0m : 1.99737

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83795
[1mStep[0m  [4/42], [94mLoss[0m : 1.80328
[1mStep[0m  [8/42], [94mLoss[0m : 2.05140
[1mStep[0m  [12/42], [94mLoss[0m : 1.79444
[1mStep[0m  [16/42], [94mLoss[0m : 1.81782
[1mStep[0m  [20/42], [94mLoss[0m : 1.83453
[1mStep[0m  [24/42], [94mLoss[0m : 1.99143
[1mStep[0m  [28/42], [94mLoss[0m : 1.88801
[1mStep[0m  [32/42], [94mLoss[0m : 1.98873
[1mStep[0m  [36/42], [94mLoss[0m : 1.99661
[1mStep[0m  [40/42], [94mLoss[0m : 1.83831

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83598
[1mStep[0m  [4/42], [94mLoss[0m : 1.64679
[1mStep[0m  [8/42], [94mLoss[0m : 1.80153
[1mStep[0m  [12/42], [94mLoss[0m : 1.79192
[1mStep[0m  [16/42], [94mLoss[0m : 1.76489
[1mStep[0m  [20/42], [94mLoss[0m : 1.67517
[1mStep[0m  [24/42], [94mLoss[0m : 1.88432
[1mStep[0m  [28/42], [94mLoss[0m : 1.82422
[1mStep[0m  [32/42], [94mLoss[0m : 1.86975
[1mStep[0m  [36/42], [94mLoss[0m : 1.98461
[1mStep[0m  [40/42], [94mLoss[0m : 1.79348

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.464, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81257
[1mStep[0m  [4/42], [94mLoss[0m : 1.60373
[1mStep[0m  [8/42], [94mLoss[0m : 1.76298
[1mStep[0m  [12/42], [94mLoss[0m : 1.76736
[1mStep[0m  [16/42], [94mLoss[0m : 1.86579
[1mStep[0m  [20/42], [94mLoss[0m : 1.83720
[1mStep[0m  [24/42], [94mLoss[0m : 1.88218
[1mStep[0m  [28/42], [94mLoss[0m : 1.99080
[1mStep[0m  [32/42], [94mLoss[0m : 1.84045
[1mStep[0m  [36/42], [94mLoss[0m : 1.93939
[1mStep[0m  [40/42], [94mLoss[0m : 1.89276

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71790
[1mStep[0m  [4/42], [94mLoss[0m : 1.68124
[1mStep[0m  [8/42], [94mLoss[0m : 1.73667
[1mStep[0m  [12/42], [94mLoss[0m : 1.78568
[1mStep[0m  [16/42], [94mLoss[0m : 1.73561
[1mStep[0m  [20/42], [94mLoss[0m : 1.83035
[1mStep[0m  [24/42], [94mLoss[0m : 1.70152
[1mStep[0m  [28/42], [94mLoss[0m : 1.84826
[1mStep[0m  [32/42], [94mLoss[0m : 1.82479
[1mStep[0m  [36/42], [94mLoss[0m : 1.71400
[1mStep[0m  [40/42], [94mLoss[0m : 1.94709

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74752
[1mStep[0m  [4/42], [94mLoss[0m : 1.81740
[1mStep[0m  [8/42], [94mLoss[0m : 1.53317
[1mStep[0m  [12/42], [94mLoss[0m : 1.77619
[1mStep[0m  [16/42], [94mLoss[0m : 1.74252
[1mStep[0m  [20/42], [94mLoss[0m : 1.79243
[1mStep[0m  [24/42], [94mLoss[0m : 1.60799
[1mStep[0m  [28/42], [94mLoss[0m : 1.80921
[1mStep[0m  [32/42], [94mLoss[0m : 1.70227
[1mStep[0m  [36/42], [94mLoss[0m : 1.61987
[1mStep[0m  [40/42], [94mLoss[0m : 1.85231

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.721, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73209
[1mStep[0m  [4/42], [94mLoss[0m : 1.82066
[1mStep[0m  [8/42], [94mLoss[0m : 1.87287
[1mStep[0m  [12/42], [94mLoss[0m : 1.61884
[1mStep[0m  [16/42], [94mLoss[0m : 1.74491
[1mStep[0m  [20/42], [94mLoss[0m : 1.65882
[1mStep[0m  [24/42], [94mLoss[0m : 1.68764
[1mStep[0m  [28/42], [94mLoss[0m : 1.65594
[1mStep[0m  [32/42], [94mLoss[0m : 1.73523
[1mStep[0m  [36/42], [94mLoss[0m : 1.45930
[1mStep[0m  [40/42], [94mLoss[0m : 1.59957

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.563, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51710
[1mStep[0m  [4/42], [94mLoss[0m : 1.60405
[1mStep[0m  [8/42], [94mLoss[0m : 1.57713
[1mStep[0m  [12/42], [94mLoss[0m : 1.78401
[1mStep[0m  [16/42], [94mLoss[0m : 1.67779
[1mStep[0m  [20/42], [94mLoss[0m : 1.70055
[1mStep[0m  [24/42], [94mLoss[0m : 1.63113
[1mStep[0m  [28/42], [94mLoss[0m : 1.65018
[1mStep[0m  [32/42], [94mLoss[0m : 1.69350
[1mStep[0m  [36/42], [94mLoss[0m : 1.69233
[1mStep[0m  [40/42], [94mLoss[0m : 1.62513

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56654
[1mStep[0m  [4/42], [94mLoss[0m : 1.71660
[1mStep[0m  [8/42], [94mLoss[0m : 1.61519
[1mStep[0m  [12/42], [94mLoss[0m : 1.55601
[1mStep[0m  [16/42], [94mLoss[0m : 1.52831
[1mStep[0m  [20/42], [94mLoss[0m : 1.60470
[1mStep[0m  [24/42], [94mLoss[0m : 1.56066
[1mStep[0m  [28/42], [94mLoss[0m : 1.38587
[1mStep[0m  [32/42], [94mLoss[0m : 1.62298
[1mStep[0m  [36/42], [94mLoss[0m : 1.66608
[1mStep[0m  [40/42], [94mLoss[0m : 1.46884

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59676
[1mStep[0m  [4/42], [94mLoss[0m : 1.47618
[1mStep[0m  [8/42], [94mLoss[0m : 1.48749
[1mStep[0m  [12/42], [94mLoss[0m : 1.57338
[1mStep[0m  [16/42], [94mLoss[0m : 1.62811
[1mStep[0m  [20/42], [94mLoss[0m : 1.75497
[1mStep[0m  [24/42], [94mLoss[0m : 1.46593
[1mStep[0m  [28/42], [94mLoss[0m : 1.76064
[1mStep[0m  [32/42], [94mLoss[0m : 1.66983
[1mStep[0m  [36/42], [94mLoss[0m : 1.63043
[1mStep[0m  [40/42], [94mLoss[0m : 1.77523

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66587
[1mStep[0m  [4/42], [94mLoss[0m : 1.44144
[1mStep[0m  [8/42], [94mLoss[0m : 1.66570
[1mStep[0m  [12/42], [94mLoss[0m : 1.40231
[1mStep[0m  [16/42], [94mLoss[0m : 1.69939
[1mStep[0m  [20/42], [94mLoss[0m : 1.56082
[1mStep[0m  [24/42], [94mLoss[0m : 1.55123
[1mStep[0m  [28/42], [94mLoss[0m : 1.63175
[1mStep[0m  [32/42], [94mLoss[0m : 1.47610
[1mStep[0m  [36/42], [94mLoss[0m : 1.58566
[1mStep[0m  [40/42], [94mLoss[0m : 1.75071

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.525, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58001
[1mStep[0m  [4/42], [94mLoss[0m : 1.39611
[1mStep[0m  [8/42], [94mLoss[0m : 1.54440
[1mStep[0m  [12/42], [94mLoss[0m : 1.48712
[1mStep[0m  [16/42], [94mLoss[0m : 1.41760
[1mStep[0m  [20/42], [94mLoss[0m : 1.46436
[1mStep[0m  [24/42], [94mLoss[0m : 1.49657
[1mStep[0m  [28/42], [94mLoss[0m : 1.71077
[1mStep[0m  [32/42], [94mLoss[0m : 1.60352
[1mStep[0m  [36/42], [94mLoss[0m : 1.55876
[1mStep[0m  [40/42], [94mLoss[0m : 1.54673

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52157
[1mStep[0m  [4/42], [94mLoss[0m : 1.40410
[1mStep[0m  [8/42], [94mLoss[0m : 1.63051
[1mStep[0m  [12/42], [94mLoss[0m : 1.53970
[1mStep[0m  [16/42], [94mLoss[0m : 1.45017
[1mStep[0m  [20/42], [94mLoss[0m : 1.43973
[1mStep[0m  [24/42], [94mLoss[0m : 1.50818
[1mStep[0m  [28/42], [94mLoss[0m : 1.46419
[1mStep[0m  [32/42], [94mLoss[0m : 1.55037
[1mStep[0m  [36/42], [94mLoss[0m : 1.54606
[1mStep[0m  [40/42], [94mLoss[0m : 1.57340

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.490, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49464
[1mStep[0m  [4/42], [94mLoss[0m : 1.43409
[1mStep[0m  [8/42], [94mLoss[0m : 1.37154
[1mStep[0m  [12/42], [94mLoss[0m : 1.66587
[1mStep[0m  [16/42], [94mLoss[0m : 1.51324
[1mStep[0m  [20/42], [94mLoss[0m : 1.45562
[1mStep[0m  [24/42], [94mLoss[0m : 1.40973
[1mStep[0m  [28/42], [94mLoss[0m : 1.59408
[1mStep[0m  [32/42], [94mLoss[0m : 1.45451
[1mStep[0m  [36/42], [94mLoss[0m : 1.40259
[1mStep[0m  [40/42], [94mLoss[0m : 1.47492

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.521, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49351
[1mStep[0m  [4/42], [94mLoss[0m : 1.31796
[1mStep[0m  [8/42], [94mLoss[0m : 1.53112
[1mStep[0m  [12/42], [94mLoss[0m : 1.41072
[1mStep[0m  [16/42], [94mLoss[0m : 1.54451
[1mStep[0m  [20/42], [94mLoss[0m : 1.51586
[1mStep[0m  [24/42], [94mLoss[0m : 1.44495
[1mStep[0m  [28/42], [94mLoss[0m : 1.47907
[1mStep[0m  [32/42], [94mLoss[0m : 1.41261
[1mStep[0m  [36/42], [94mLoss[0m : 1.57002
[1mStep[0m  [40/42], [94mLoss[0m : 1.38011

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43756
[1mStep[0m  [4/42], [94mLoss[0m : 1.51922
[1mStep[0m  [8/42], [94mLoss[0m : 1.40901
[1mStep[0m  [12/42], [94mLoss[0m : 1.59604
[1mStep[0m  [16/42], [94mLoss[0m : 1.46912
[1mStep[0m  [20/42], [94mLoss[0m : 1.40713
[1mStep[0m  [24/42], [94mLoss[0m : 1.45423
[1mStep[0m  [28/42], [94mLoss[0m : 1.45139
[1mStep[0m  [32/42], [94mLoss[0m : 1.41476
[1mStep[0m  [36/42], [94mLoss[0m : 1.43301
[1mStep[0m  [40/42], [94mLoss[0m : 1.64389

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41320
[1mStep[0m  [4/42], [94mLoss[0m : 1.52083
[1mStep[0m  [8/42], [94mLoss[0m : 1.34078
[1mStep[0m  [12/42], [94mLoss[0m : 1.40046
[1mStep[0m  [16/42], [94mLoss[0m : 1.43371
[1mStep[0m  [20/42], [94mLoss[0m : 1.49875
[1mStep[0m  [24/42], [94mLoss[0m : 1.40594
[1mStep[0m  [28/42], [94mLoss[0m : 1.46953
[1mStep[0m  [32/42], [94mLoss[0m : 1.49739
[1mStep[0m  [36/42], [94mLoss[0m : 1.33674
[1mStep[0m  [40/42], [94mLoss[0m : 1.43500

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.443, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.29948
[1mStep[0m  [4/42], [94mLoss[0m : 1.47576
[1mStep[0m  [8/42], [94mLoss[0m : 1.36173
[1mStep[0m  [12/42], [94mLoss[0m : 1.26377
[1mStep[0m  [16/42], [94mLoss[0m : 1.61974
[1mStep[0m  [20/42], [94mLoss[0m : 1.56229
[1mStep[0m  [24/42], [94mLoss[0m : 1.51408
[1mStep[0m  [28/42], [94mLoss[0m : 1.43511
[1mStep[0m  [32/42], [94mLoss[0m : 1.28868
[1mStep[0m  [36/42], [94mLoss[0m : 1.40539
[1mStep[0m  [40/42], [94mLoss[0m : 1.53790

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.428, [92mTest[0m: 2.553, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.491740209715707
MAE score P1       2.332412
MAE score P2        2.49174
loss               1.427708
learning_rate      0.007525
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay         0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.06585
[1mStep[0m  [2/21], [94mLoss[0m : 10.86430
[1mStep[0m  [4/21], [94mLoss[0m : 10.52591
[1mStep[0m  [6/21], [94mLoss[0m : 10.71679
[1mStep[0m  [8/21], [94mLoss[0m : 10.50367
[1mStep[0m  [10/21], [94mLoss[0m : 10.79928
[1mStep[0m  [12/21], [94mLoss[0m : 10.62667
[1mStep[0m  [14/21], [94mLoss[0m : 10.63522
[1mStep[0m  [16/21], [94mLoss[0m : 10.52375
[1mStep[0m  [18/21], [94mLoss[0m : 10.69030
[1mStep[0m  [20/21], [94mLoss[0m : 10.81017

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.691, [92mTest[0m: 10.766, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59506
[1mStep[0m  [2/21], [94mLoss[0m : 10.52242
[1mStep[0m  [4/21], [94mLoss[0m : 10.51881
[1mStep[0m  [6/21], [94mLoss[0m : 10.12176
[1mStep[0m  [8/21], [94mLoss[0m : 10.35826
[1mStep[0m  [10/21], [94mLoss[0m : 10.64792
[1mStep[0m  [12/21], [94mLoss[0m : 10.45257
[1mStep[0m  [14/21], [94mLoss[0m : 10.56388
[1mStep[0m  [16/21], [94mLoss[0m : 10.19871
[1mStep[0m  [18/21], [94mLoss[0m : 10.43685
[1mStep[0m  [20/21], [94mLoss[0m : 10.28918

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.476, [92mTest[0m: 10.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.17428
[1mStep[0m  [2/21], [94mLoss[0m : 9.96687
[1mStep[0m  [4/21], [94mLoss[0m : 10.46181
[1mStep[0m  [6/21], [94mLoss[0m : 10.50588
[1mStep[0m  [8/21], [94mLoss[0m : 10.26223
[1mStep[0m  [10/21], [94mLoss[0m : 10.30223
[1mStep[0m  [12/21], [94mLoss[0m : 10.38367
[1mStep[0m  [14/21], [94mLoss[0m : 9.99687
[1mStep[0m  [16/21], [94mLoss[0m : 10.27155
[1mStep[0m  [18/21], [94mLoss[0m : 10.21590
[1mStep[0m  [20/21], [94mLoss[0m : 10.11255

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.268, [92mTest[0m: 10.276, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.01836
[1mStep[0m  [2/21], [94mLoss[0m : 10.26087
[1mStep[0m  [4/21], [94mLoss[0m : 9.87336
[1mStep[0m  [6/21], [94mLoss[0m : 10.28528
[1mStep[0m  [8/21], [94mLoss[0m : 10.10543
[1mStep[0m  [10/21], [94mLoss[0m : 9.77790
[1mStep[0m  [12/21], [94mLoss[0m : 10.01886
[1mStep[0m  [14/21], [94mLoss[0m : 10.01793
[1mStep[0m  [16/21], [94mLoss[0m : 10.05646
[1mStep[0m  [18/21], [94mLoss[0m : 9.76986
[1mStep[0m  [20/21], [94mLoss[0m : 9.97320

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.045, [92mTest[0m: 10.029, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.97915
[1mStep[0m  [2/21], [94mLoss[0m : 9.90197
[1mStep[0m  [4/21], [94mLoss[0m : 9.64080
[1mStep[0m  [6/21], [94mLoss[0m : 9.89751
[1mStep[0m  [8/21], [94mLoss[0m : 10.24512
[1mStep[0m  [10/21], [94mLoss[0m : 10.15492
[1mStep[0m  [12/21], [94mLoss[0m : 9.81636
[1mStep[0m  [14/21], [94mLoss[0m : 9.70952
[1mStep[0m  [16/21], [94mLoss[0m : 9.68276
[1mStep[0m  [18/21], [94mLoss[0m : 9.66365
[1mStep[0m  [20/21], [94mLoss[0m : 9.63907

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.833, [92mTest[0m: 9.776, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.65490
[1mStep[0m  [2/21], [94mLoss[0m : 9.70606
[1mStep[0m  [4/21], [94mLoss[0m : 9.72583
[1mStep[0m  [6/21], [94mLoss[0m : 9.73730
[1mStep[0m  [8/21], [94mLoss[0m : 9.66060
[1mStep[0m  [10/21], [94mLoss[0m : 9.20839
[1mStep[0m  [12/21], [94mLoss[0m : 9.70479
[1mStep[0m  [14/21], [94mLoss[0m : 9.80905
[1mStep[0m  [16/21], [94mLoss[0m : 9.57112
[1mStep[0m  [18/21], [94mLoss[0m : 9.44397
[1mStep[0m  [20/21], [94mLoss[0m : 9.61864

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.600, [92mTest[0m: 9.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.41482
[1mStep[0m  [2/21], [94mLoss[0m : 9.60166
[1mStep[0m  [4/21], [94mLoss[0m : 9.57100
[1mStep[0m  [6/21], [94mLoss[0m : 9.35317
[1mStep[0m  [8/21], [94mLoss[0m : 9.21719
[1mStep[0m  [10/21], [94mLoss[0m : 9.24928
[1mStep[0m  [12/21], [94mLoss[0m : 9.49469
[1mStep[0m  [14/21], [94mLoss[0m : 9.41702
[1mStep[0m  [16/21], [94mLoss[0m : 9.36502
[1mStep[0m  [18/21], [94mLoss[0m : 9.13909
[1mStep[0m  [20/21], [94mLoss[0m : 9.11703

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.351, [92mTest[0m: 9.218, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.28236
[1mStep[0m  [2/21], [94mLoss[0m : 9.03163
[1mStep[0m  [4/21], [94mLoss[0m : 8.87817
[1mStep[0m  [6/21], [94mLoss[0m : 9.20423
[1mStep[0m  [8/21], [94mLoss[0m : 8.91249
[1mStep[0m  [10/21], [94mLoss[0m : 9.12204
[1mStep[0m  [12/21], [94mLoss[0m : 9.12715
[1mStep[0m  [14/21], [94mLoss[0m : 8.94918
[1mStep[0m  [16/21], [94mLoss[0m : 8.99600
[1mStep[0m  [18/21], [94mLoss[0m : 8.85364
[1mStep[0m  [20/21], [94mLoss[0m : 8.98313

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.083, [92mTest[0m: 8.927, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.00259
[1mStep[0m  [2/21], [94mLoss[0m : 8.97082
[1mStep[0m  [4/21], [94mLoss[0m : 8.61822
[1mStep[0m  [6/21], [94mLoss[0m : 8.83357
[1mStep[0m  [8/21], [94mLoss[0m : 8.93493
[1mStep[0m  [10/21], [94mLoss[0m : 8.67166
[1mStep[0m  [12/21], [94mLoss[0m : 8.76614
[1mStep[0m  [14/21], [94mLoss[0m : 8.84563
[1mStep[0m  [16/21], [94mLoss[0m : 8.65358
[1mStep[0m  [18/21], [94mLoss[0m : 8.39498
[1mStep[0m  [20/21], [94mLoss[0m : 8.41848

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.778, [92mTest[0m: 8.606, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.23818
[1mStep[0m  [2/21], [94mLoss[0m : 8.58915
[1mStep[0m  [4/21], [94mLoss[0m : 8.49437
[1mStep[0m  [6/21], [94mLoss[0m : 8.71739
[1mStep[0m  [8/21], [94mLoss[0m : 8.51629
[1mStep[0m  [10/21], [94mLoss[0m : 8.08644
[1mStep[0m  [12/21], [94mLoss[0m : 8.74101
[1mStep[0m  [14/21], [94mLoss[0m : 8.59995
[1mStep[0m  [16/21], [94mLoss[0m : 8.34944
[1mStep[0m  [18/21], [94mLoss[0m : 8.51516
[1mStep[0m  [20/21], [94mLoss[0m : 8.41932

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.463, [92mTest[0m: 8.240, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.36204
[1mStep[0m  [2/21], [94mLoss[0m : 8.17444
[1mStep[0m  [4/21], [94mLoss[0m : 8.10258
[1mStep[0m  [6/21], [94mLoss[0m : 8.25120
[1mStep[0m  [8/21], [94mLoss[0m : 8.28097
[1mStep[0m  [10/21], [94mLoss[0m : 8.17098
[1mStep[0m  [12/21], [94mLoss[0m : 7.94908
[1mStep[0m  [14/21], [94mLoss[0m : 8.15915
[1mStep[0m  [16/21], [94mLoss[0m : 7.85205
[1mStep[0m  [18/21], [94mLoss[0m : 7.68191
[1mStep[0m  [20/21], [94mLoss[0m : 7.85418

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.088, [92mTest[0m: 7.884, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.74967
[1mStep[0m  [2/21], [94mLoss[0m : 7.69874
[1mStep[0m  [4/21], [94mLoss[0m : 7.76729
[1mStep[0m  [6/21], [94mLoss[0m : 7.75803
[1mStep[0m  [8/21], [94mLoss[0m : 7.73825
[1mStep[0m  [10/21], [94mLoss[0m : 8.12055
[1mStep[0m  [12/21], [94mLoss[0m : 7.58488
[1mStep[0m  [14/21], [94mLoss[0m : 7.69706
[1mStep[0m  [16/21], [94mLoss[0m : 7.45008
[1mStep[0m  [18/21], [94mLoss[0m : 7.66924
[1mStep[0m  [20/21], [94mLoss[0m : 7.31552

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.689, [92mTest[0m: 7.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.31884
[1mStep[0m  [2/21], [94mLoss[0m : 7.62439
[1mStep[0m  [4/21], [94mLoss[0m : 7.28366
[1mStep[0m  [6/21], [94mLoss[0m : 7.48049
[1mStep[0m  [8/21], [94mLoss[0m : 7.22373
[1mStep[0m  [10/21], [94mLoss[0m : 7.23978
[1mStep[0m  [12/21], [94mLoss[0m : 7.33052
[1mStep[0m  [14/21], [94mLoss[0m : 7.09844
[1mStep[0m  [16/21], [94mLoss[0m : 7.15280
[1mStep[0m  [18/21], [94mLoss[0m : 7.55042
[1mStep[0m  [20/21], [94mLoss[0m : 7.09895

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.275, [92mTest[0m: 6.927, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.05595
[1mStep[0m  [2/21], [94mLoss[0m : 6.86880
[1mStep[0m  [4/21], [94mLoss[0m : 6.76869
[1mStep[0m  [6/21], [94mLoss[0m : 7.03492
[1mStep[0m  [8/21], [94mLoss[0m : 6.98757
[1mStep[0m  [10/21], [94mLoss[0m : 6.91039
[1mStep[0m  [12/21], [94mLoss[0m : 6.73544
[1mStep[0m  [14/21], [94mLoss[0m : 6.71695
[1mStep[0m  [16/21], [94mLoss[0m : 6.66155
[1mStep[0m  [18/21], [94mLoss[0m : 6.54595
[1mStep[0m  [20/21], [94mLoss[0m : 6.43608

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.814, [92mTest[0m: 6.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.67631
[1mStep[0m  [2/21], [94mLoss[0m : 6.36720
[1mStep[0m  [4/21], [94mLoss[0m : 6.48637
[1mStep[0m  [6/21], [94mLoss[0m : 6.48188
[1mStep[0m  [8/21], [94mLoss[0m : 6.24205
[1mStep[0m  [10/21], [94mLoss[0m : 6.29177
[1mStep[0m  [12/21], [94mLoss[0m : 5.99919
[1mStep[0m  [14/21], [94mLoss[0m : 6.39741
[1mStep[0m  [16/21], [94mLoss[0m : 6.44374
[1mStep[0m  [18/21], [94mLoss[0m : 5.97402
[1mStep[0m  [20/21], [94mLoss[0m : 6.25957

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.374, [92mTest[0m: 5.945, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.22936
[1mStep[0m  [2/21], [94mLoss[0m : 6.03276
[1mStep[0m  [4/21], [94mLoss[0m : 6.22796
[1mStep[0m  [6/21], [94mLoss[0m : 6.01257
[1mStep[0m  [8/21], [94mLoss[0m : 6.03563
[1mStep[0m  [10/21], [94mLoss[0m : 5.83080
[1mStep[0m  [12/21], [94mLoss[0m : 5.78026
[1mStep[0m  [14/21], [94mLoss[0m : 5.73105
[1mStep[0m  [16/21], [94mLoss[0m : 6.19145
[1mStep[0m  [18/21], [94mLoss[0m : 5.81821
[1mStep[0m  [20/21], [94mLoss[0m : 5.60804

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.982, [92mTest[0m: 5.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.61824
[1mStep[0m  [2/21], [94mLoss[0m : 5.67681
[1mStep[0m  [4/21], [94mLoss[0m : 5.81089
[1mStep[0m  [6/21], [94mLoss[0m : 5.43680
[1mStep[0m  [8/21], [94mLoss[0m : 5.77112
[1mStep[0m  [10/21], [94mLoss[0m : 5.64477
[1mStep[0m  [12/21], [94mLoss[0m : 5.68803
[1mStep[0m  [14/21], [94mLoss[0m : 5.14595
[1mStep[0m  [16/21], [94mLoss[0m : 5.36772
[1mStep[0m  [18/21], [94mLoss[0m : 5.61566
[1mStep[0m  [20/21], [94mLoss[0m : 5.53051

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.589, [92mTest[0m: 4.982, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.47185
[1mStep[0m  [2/21], [94mLoss[0m : 5.31193
[1mStep[0m  [4/21], [94mLoss[0m : 5.37420
[1mStep[0m  [6/21], [94mLoss[0m : 5.26807
[1mStep[0m  [8/21], [94mLoss[0m : 5.07564
[1mStep[0m  [10/21], [94mLoss[0m : 5.25292
[1mStep[0m  [12/21], [94mLoss[0m : 5.03418
[1mStep[0m  [14/21], [94mLoss[0m : 5.22621
[1mStep[0m  [16/21], [94mLoss[0m : 5.20706
[1mStep[0m  [18/21], [94mLoss[0m : 4.97248
[1mStep[0m  [20/21], [94mLoss[0m : 5.17127

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.208, [92mTest[0m: 4.588, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.84870
[1mStep[0m  [2/21], [94mLoss[0m : 4.93589
[1mStep[0m  [4/21], [94mLoss[0m : 4.89142
[1mStep[0m  [6/21], [94mLoss[0m : 4.80934
[1mStep[0m  [8/21], [94mLoss[0m : 4.95972
[1mStep[0m  [10/21], [94mLoss[0m : 4.98300
[1mStep[0m  [12/21], [94mLoss[0m : 4.83402
[1mStep[0m  [14/21], [94mLoss[0m : 4.66890
[1mStep[0m  [16/21], [94mLoss[0m : 4.63239
[1mStep[0m  [18/21], [94mLoss[0m : 4.58823
[1mStep[0m  [20/21], [94mLoss[0m : 4.59462

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.820, [92mTest[0m: 4.205, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.58108
[1mStep[0m  [2/21], [94mLoss[0m : 4.70222
[1mStep[0m  [4/21], [94mLoss[0m : 4.34751
[1mStep[0m  [6/21], [94mLoss[0m : 4.67277
[1mStep[0m  [8/21], [94mLoss[0m : 4.48621
[1mStep[0m  [10/21], [94mLoss[0m : 4.33032
[1mStep[0m  [12/21], [94mLoss[0m : 4.60510
[1mStep[0m  [14/21], [94mLoss[0m : 4.45744
[1mStep[0m  [16/21], [94mLoss[0m : 4.14606
[1mStep[0m  [18/21], [94mLoss[0m : 4.37936
[1mStep[0m  [20/21], [94mLoss[0m : 4.35208

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.478, [92mTest[0m: 3.897, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.29205
[1mStep[0m  [2/21], [94mLoss[0m : 4.41670
[1mStep[0m  [4/21], [94mLoss[0m : 4.27485
[1mStep[0m  [6/21], [94mLoss[0m : 4.19625
[1mStep[0m  [8/21], [94mLoss[0m : 3.82661
[1mStep[0m  [10/21], [94mLoss[0m : 4.09192
[1mStep[0m  [12/21], [94mLoss[0m : 4.19698
[1mStep[0m  [14/21], [94mLoss[0m : 3.95817
[1mStep[0m  [16/21], [94mLoss[0m : 3.88557
[1mStep[0m  [18/21], [94mLoss[0m : 3.95458
[1mStep[0m  [20/21], [94mLoss[0m : 4.22855

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.136, [92mTest[0m: 3.556, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.04410
[1mStep[0m  [2/21], [94mLoss[0m : 3.91982
[1mStep[0m  [4/21], [94mLoss[0m : 3.85647
[1mStep[0m  [6/21], [94mLoss[0m : 3.86204
[1mStep[0m  [8/21], [94mLoss[0m : 3.87646
[1mStep[0m  [10/21], [94mLoss[0m : 3.95983
[1mStep[0m  [12/21], [94mLoss[0m : 3.78150
[1mStep[0m  [14/21], [94mLoss[0m : 3.81971
[1mStep[0m  [16/21], [94mLoss[0m : 3.77540
[1mStep[0m  [18/21], [94mLoss[0m : 3.73331
[1mStep[0m  [20/21], [94mLoss[0m : 3.96496

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.844, [92mTest[0m: 3.266, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.81479
[1mStep[0m  [2/21], [94mLoss[0m : 3.64678
[1mStep[0m  [4/21], [94mLoss[0m : 3.49267
[1mStep[0m  [6/21], [94mLoss[0m : 3.57095
[1mStep[0m  [8/21], [94mLoss[0m : 3.42399
[1mStep[0m  [10/21], [94mLoss[0m : 3.59886
[1mStep[0m  [12/21], [94mLoss[0m : 3.53492
[1mStep[0m  [14/21], [94mLoss[0m : 3.77207
[1mStep[0m  [16/21], [94mLoss[0m : 3.71459
[1mStep[0m  [18/21], [94mLoss[0m : 3.48190
[1mStep[0m  [20/21], [94mLoss[0m : 3.48319

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.588, [92mTest[0m: 3.039, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.35352
[1mStep[0m  [2/21], [94mLoss[0m : 3.54143
[1mStep[0m  [4/21], [94mLoss[0m : 3.42686
[1mStep[0m  [6/21], [94mLoss[0m : 3.48660
[1mStep[0m  [8/21], [94mLoss[0m : 3.64271
[1mStep[0m  [10/21], [94mLoss[0m : 3.36087
[1mStep[0m  [12/21], [94mLoss[0m : 3.40681
[1mStep[0m  [14/21], [94mLoss[0m : 3.27509
[1mStep[0m  [16/21], [94mLoss[0m : 3.32658
[1mStep[0m  [18/21], [94mLoss[0m : 3.41144
[1mStep[0m  [20/21], [94mLoss[0m : 3.33069

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.394, [92mTest[0m: 2.859, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.20802
[1mStep[0m  [2/21], [94mLoss[0m : 3.24320
[1mStep[0m  [4/21], [94mLoss[0m : 3.17668
[1mStep[0m  [6/21], [94mLoss[0m : 3.25835
[1mStep[0m  [8/21], [94mLoss[0m : 3.39426
[1mStep[0m  [10/21], [94mLoss[0m : 3.10875
[1mStep[0m  [12/21], [94mLoss[0m : 3.14789
[1mStep[0m  [14/21], [94mLoss[0m : 3.19489
[1mStep[0m  [16/21], [94mLoss[0m : 3.06572
[1mStep[0m  [18/21], [94mLoss[0m : 3.20531
[1mStep[0m  [20/21], [94mLoss[0m : 3.13342

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.198, [92mTest[0m: 2.698, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.09120
[1mStep[0m  [2/21], [94mLoss[0m : 3.13385
[1mStep[0m  [4/21], [94mLoss[0m : 3.09229
[1mStep[0m  [6/21], [94mLoss[0m : 3.41376
[1mStep[0m  [8/21], [94mLoss[0m : 3.29925
[1mStep[0m  [10/21], [94mLoss[0m : 2.94538
[1mStep[0m  [12/21], [94mLoss[0m : 3.18265
[1mStep[0m  [14/21], [94mLoss[0m : 3.16772
[1mStep[0m  [16/21], [94mLoss[0m : 2.96770
[1mStep[0m  [18/21], [94mLoss[0m : 3.00334
[1mStep[0m  [20/21], [94mLoss[0m : 3.01106

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.102, [92mTest[0m: 2.603, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.11575
[1mStep[0m  [2/21], [94mLoss[0m : 2.92869
[1mStep[0m  [4/21], [94mLoss[0m : 3.26332
[1mStep[0m  [6/21], [94mLoss[0m : 3.12037
[1mStep[0m  [8/21], [94mLoss[0m : 3.18734
[1mStep[0m  [10/21], [94mLoss[0m : 3.11942
[1mStep[0m  [12/21], [94mLoss[0m : 3.01221
[1mStep[0m  [14/21], [94mLoss[0m : 2.91069
[1mStep[0m  [16/21], [94mLoss[0m : 3.02932
[1mStep[0m  [18/21], [94mLoss[0m : 2.93095
[1mStep[0m  [20/21], [94mLoss[0m : 2.88704

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.022, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.13699
[1mStep[0m  [2/21], [94mLoss[0m : 2.84905
[1mStep[0m  [4/21], [94mLoss[0m : 2.98565
[1mStep[0m  [6/21], [94mLoss[0m : 2.88903
[1mStep[0m  [8/21], [94mLoss[0m : 2.95510
[1mStep[0m  [10/21], [94mLoss[0m : 2.82540
[1mStep[0m  [12/21], [94mLoss[0m : 3.06222
[1mStep[0m  [14/21], [94mLoss[0m : 2.98097
[1mStep[0m  [16/21], [94mLoss[0m : 3.04692
[1mStep[0m  [18/21], [94mLoss[0m : 2.81836
[1mStep[0m  [20/21], [94mLoss[0m : 2.84478

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.958, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82056
[1mStep[0m  [2/21], [94mLoss[0m : 2.93343
[1mStep[0m  [4/21], [94mLoss[0m : 2.96692
[1mStep[0m  [6/21], [94mLoss[0m : 2.91477
[1mStep[0m  [8/21], [94mLoss[0m : 2.72963
[1mStep[0m  [10/21], [94mLoss[0m : 3.14248
[1mStep[0m  [12/21], [94mLoss[0m : 2.96356
[1mStep[0m  [14/21], [94mLoss[0m : 2.89909
[1mStep[0m  [16/21], [94mLoss[0m : 2.87064
[1mStep[0m  [18/21], [94mLoss[0m : 3.06215
[1mStep[0m  [20/21], [94mLoss[0m : 2.80083

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.906, [92mTest[0m: 2.446, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76891
[1mStep[0m  [2/21], [94mLoss[0m : 2.94891
[1mStep[0m  [4/21], [94mLoss[0m : 2.83981
[1mStep[0m  [6/21], [94mLoss[0m : 2.90705
[1mStep[0m  [8/21], [94mLoss[0m : 2.91452
[1mStep[0m  [10/21], [94mLoss[0m : 2.89321
[1mStep[0m  [12/21], [94mLoss[0m : 2.70138
[1mStep[0m  [14/21], [94mLoss[0m : 2.94132
[1mStep[0m  [16/21], [94mLoss[0m : 2.99513
[1mStep[0m  [18/21], [94mLoss[0m : 3.00162
[1mStep[0m  [20/21], [94mLoss[0m : 2.87782

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.856, [92mTest[0m: 2.425, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.413
====================================

Phase 1 - Evaluation MAE:  2.4130281720842635
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.75850
[1mStep[0m  [2/21], [94mLoss[0m : 3.02139
[1mStep[0m  [4/21], [94mLoss[0m : 2.86035
[1mStep[0m  [6/21], [94mLoss[0m : 2.99059
[1mStep[0m  [8/21], [94mLoss[0m : 2.89284
[1mStep[0m  [10/21], [94mLoss[0m : 2.82118
[1mStep[0m  [12/21], [94mLoss[0m : 2.79096
[1mStep[0m  [14/21], [94mLoss[0m : 2.93342
[1mStep[0m  [16/21], [94mLoss[0m : 2.68989
[1mStep[0m  [18/21], [94mLoss[0m : 3.00157
[1mStep[0m  [20/21], [94mLoss[0m : 2.87091

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.885, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.97068
[1mStep[0m  [2/21], [94mLoss[0m : 2.96726
[1mStep[0m  [4/21], [94mLoss[0m : 2.79424
[1mStep[0m  [6/21], [94mLoss[0m : 2.84898
[1mStep[0m  [8/21], [94mLoss[0m : 3.14541
[1mStep[0m  [10/21], [94mLoss[0m : 2.87269
[1mStep[0m  [12/21], [94mLoss[0m : 2.87485
[1mStep[0m  [14/21], [94mLoss[0m : 2.80808
[1mStep[0m  [16/21], [94mLoss[0m : 2.68654
[1mStep[0m  [18/21], [94mLoss[0m : 2.80674
[1mStep[0m  [20/21], [94mLoss[0m : 2.94925

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.872, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80022
[1mStep[0m  [2/21], [94mLoss[0m : 2.76910
[1mStep[0m  [4/21], [94mLoss[0m : 2.84800
[1mStep[0m  [6/21], [94mLoss[0m : 2.97314
[1mStep[0m  [8/21], [94mLoss[0m : 2.93623
[1mStep[0m  [10/21], [94mLoss[0m : 2.79065
[1mStep[0m  [12/21], [94mLoss[0m : 2.96964
[1mStep[0m  [14/21], [94mLoss[0m : 2.67917
[1mStep[0m  [16/21], [94mLoss[0m : 2.94015
[1mStep[0m  [18/21], [94mLoss[0m : 2.71386
[1mStep[0m  [20/21], [94mLoss[0m : 2.66958

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.831, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.85418
[1mStep[0m  [2/21], [94mLoss[0m : 2.89594
[1mStep[0m  [4/21], [94mLoss[0m : 2.61304
[1mStep[0m  [6/21], [94mLoss[0m : 2.81114
[1mStep[0m  [8/21], [94mLoss[0m : 2.74238
[1mStep[0m  [10/21], [94mLoss[0m : 2.77805
[1mStep[0m  [12/21], [94mLoss[0m : 2.91081
[1mStep[0m  [14/21], [94mLoss[0m : 2.76520
[1mStep[0m  [16/21], [94mLoss[0m : 2.79077
[1mStep[0m  [18/21], [94mLoss[0m : 2.84124
[1mStep[0m  [20/21], [94mLoss[0m : 2.85751

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.820, [92mTest[0m: 2.545, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64818
[1mStep[0m  [2/21], [94mLoss[0m : 2.67398
[1mStep[0m  [4/21], [94mLoss[0m : 2.68917
[1mStep[0m  [6/21], [94mLoss[0m : 2.86584
[1mStep[0m  [8/21], [94mLoss[0m : 2.93975
[1mStep[0m  [10/21], [94mLoss[0m : 2.78399
[1mStep[0m  [12/21], [94mLoss[0m : 2.87922
[1mStep[0m  [14/21], [94mLoss[0m : 2.92468
[1mStep[0m  [16/21], [94mLoss[0m : 2.92474
[1mStep[0m  [18/21], [94mLoss[0m : 2.88755
[1mStep[0m  [20/21], [94mLoss[0m : 2.64377

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.808, [92mTest[0m: 2.646, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78315
[1mStep[0m  [2/21], [94mLoss[0m : 2.88346
[1mStep[0m  [4/21], [94mLoss[0m : 2.67748
[1mStep[0m  [6/21], [94mLoss[0m : 2.62620
[1mStep[0m  [8/21], [94mLoss[0m : 2.74212
[1mStep[0m  [10/21], [94mLoss[0m : 2.78403
[1mStep[0m  [12/21], [94mLoss[0m : 2.83188
[1mStep[0m  [14/21], [94mLoss[0m : 2.78901
[1mStep[0m  [16/21], [94mLoss[0m : 2.67428
[1mStep[0m  [18/21], [94mLoss[0m : 2.73205
[1mStep[0m  [20/21], [94mLoss[0m : 2.75396

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.773, [92mTest[0m: 2.662, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76097
[1mStep[0m  [2/21], [94mLoss[0m : 2.68355
[1mStep[0m  [4/21], [94mLoss[0m : 2.85995
[1mStep[0m  [6/21], [94mLoss[0m : 2.59978
[1mStep[0m  [8/21], [94mLoss[0m : 2.75714
[1mStep[0m  [10/21], [94mLoss[0m : 2.69602
[1mStep[0m  [12/21], [94mLoss[0m : 2.58104
[1mStep[0m  [14/21], [94mLoss[0m : 2.84107
[1mStep[0m  [16/21], [94mLoss[0m : 2.79158
[1mStep[0m  [18/21], [94mLoss[0m : 2.76035
[1mStep[0m  [20/21], [94mLoss[0m : 2.82344

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.621, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61607
[1mStep[0m  [2/21], [94mLoss[0m : 2.73945
[1mStep[0m  [4/21], [94mLoss[0m : 2.65393
[1mStep[0m  [6/21], [94mLoss[0m : 2.65662
[1mStep[0m  [8/21], [94mLoss[0m : 2.60991
[1mStep[0m  [10/21], [94mLoss[0m : 2.83819
[1mStep[0m  [12/21], [94mLoss[0m : 2.81604
[1mStep[0m  [14/21], [94mLoss[0m : 3.03407
[1mStep[0m  [16/21], [94mLoss[0m : 2.71080
[1mStep[0m  [18/21], [94mLoss[0m : 2.81894
[1mStep[0m  [20/21], [94mLoss[0m : 2.64036

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.595, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78952
[1mStep[0m  [2/21], [94mLoss[0m : 2.62933
[1mStep[0m  [4/21], [94mLoss[0m : 2.82903
[1mStep[0m  [6/21], [94mLoss[0m : 2.71481
[1mStep[0m  [8/21], [94mLoss[0m : 2.62205
[1mStep[0m  [10/21], [94mLoss[0m : 2.75833
[1mStep[0m  [12/21], [94mLoss[0m : 2.61518
[1mStep[0m  [14/21], [94mLoss[0m : 2.66431
[1mStep[0m  [16/21], [94mLoss[0m : 2.62980
[1mStep[0m  [18/21], [94mLoss[0m : 2.69042
[1mStep[0m  [20/21], [94mLoss[0m : 2.77964

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.568, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70156
[1mStep[0m  [2/21], [94mLoss[0m : 2.74384
[1mStep[0m  [4/21], [94mLoss[0m : 2.78452
[1mStep[0m  [6/21], [94mLoss[0m : 2.71921
[1mStep[0m  [8/21], [94mLoss[0m : 2.71149
[1mStep[0m  [10/21], [94mLoss[0m : 2.68941
[1mStep[0m  [12/21], [94mLoss[0m : 2.72245
[1mStep[0m  [14/21], [94mLoss[0m : 2.88788
[1mStep[0m  [16/21], [94mLoss[0m : 2.70999
[1mStep[0m  [18/21], [94mLoss[0m : 2.80625
[1mStep[0m  [20/21], [94mLoss[0m : 2.80194

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.699, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78288
[1mStep[0m  [2/21], [94mLoss[0m : 2.71053
[1mStep[0m  [4/21], [94mLoss[0m : 2.75588
[1mStep[0m  [6/21], [94mLoss[0m : 2.68197
[1mStep[0m  [8/21], [94mLoss[0m : 2.83080
[1mStep[0m  [10/21], [94mLoss[0m : 2.67224
[1mStep[0m  [12/21], [94mLoss[0m : 2.80192
[1mStep[0m  [14/21], [94mLoss[0m : 2.69911
[1mStep[0m  [16/21], [94mLoss[0m : 2.58970
[1mStep[0m  [18/21], [94mLoss[0m : 2.74871
[1mStep[0m  [20/21], [94mLoss[0m : 2.61961

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.668, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70778
[1mStep[0m  [2/21], [94mLoss[0m : 2.47727
[1mStep[0m  [4/21], [94mLoss[0m : 2.79101
[1mStep[0m  [6/21], [94mLoss[0m : 2.68569
[1mStep[0m  [8/21], [94mLoss[0m : 2.59695
[1mStep[0m  [10/21], [94mLoss[0m : 2.81465
[1mStep[0m  [12/21], [94mLoss[0m : 2.71552
[1mStep[0m  [14/21], [94mLoss[0m : 2.63169
[1mStep[0m  [16/21], [94mLoss[0m : 2.69550
[1mStep[0m  [18/21], [94mLoss[0m : 2.73547
[1mStep[0m  [20/21], [94mLoss[0m : 2.67790

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.593, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72472
[1mStep[0m  [2/21], [94mLoss[0m : 2.55255
[1mStep[0m  [4/21], [94mLoss[0m : 2.53059
[1mStep[0m  [6/21], [94mLoss[0m : 2.55466
[1mStep[0m  [8/21], [94mLoss[0m : 2.77777
[1mStep[0m  [10/21], [94mLoss[0m : 2.66478
[1mStep[0m  [12/21], [94mLoss[0m : 2.57552
[1mStep[0m  [14/21], [94mLoss[0m : 2.59186
[1mStep[0m  [16/21], [94mLoss[0m : 2.76451
[1mStep[0m  [18/21], [94mLoss[0m : 2.54046
[1mStep[0m  [20/21], [94mLoss[0m : 2.65265

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.600, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56007
[1mStep[0m  [2/21], [94mLoss[0m : 2.62258
[1mStep[0m  [4/21], [94mLoss[0m : 2.55269
[1mStep[0m  [6/21], [94mLoss[0m : 2.72066
[1mStep[0m  [8/21], [94mLoss[0m : 2.81635
[1mStep[0m  [10/21], [94mLoss[0m : 2.61692
[1mStep[0m  [12/21], [94mLoss[0m : 2.70914
[1mStep[0m  [14/21], [94mLoss[0m : 2.68632
[1mStep[0m  [16/21], [94mLoss[0m : 2.57881
[1mStep[0m  [18/21], [94mLoss[0m : 2.55107
[1mStep[0m  [20/21], [94mLoss[0m : 2.74214

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.643, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68612
[1mStep[0m  [2/21], [94mLoss[0m : 2.68847
[1mStep[0m  [4/21], [94mLoss[0m : 2.57462
[1mStep[0m  [6/21], [94mLoss[0m : 2.32146
[1mStep[0m  [8/21], [94mLoss[0m : 2.68480
[1mStep[0m  [10/21], [94mLoss[0m : 2.50598
[1mStep[0m  [12/21], [94mLoss[0m : 2.60473
[1mStep[0m  [14/21], [94mLoss[0m : 2.66405
[1mStep[0m  [16/21], [94mLoss[0m : 2.74927
[1mStep[0m  [18/21], [94mLoss[0m : 2.66907
[1mStep[0m  [20/21], [94mLoss[0m : 2.72761

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.581, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64374
[1mStep[0m  [2/21], [94mLoss[0m : 2.65172
[1mStep[0m  [4/21], [94mLoss[0m : 2.66891
[1mStep[0m  [6/21], [94mLoss[0m : 2.59430
[1mStep[0m  [8/21], [94mLoss[0m : 2.73698
[1mStep[0m  [10/21], [94mLoss[0m : 2.63029
[1mStep[0m  [12/21], [94mLoss[0m : 2.67199
[1mStep[0m  [14/21], [94mLoss[0m : 2.49527
[1mStep[0m  [16/21], [94mLoss[0m : 2.55355
[1mStep[0m  [18/21], [94mLoss[0m : 2.56935
[1mStep[0m  [20/21], [94mLoss[0m : 2.68985

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.590, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64208
[1mStep[0m  [2/21], [94mLoss[0m : 2.66341
[1mStep[0m  [4/21], [94mLoss[0m : 2.39905
[1mStep[0m  [6/21], [94mLoss[0m : 2.67206
[1mStep[0m  [8/21], [94mLoss[0m : 2.71263
[1mStep[0m  [10/21], [94mLoss[0m : 2.60085
[1mStep[0m  [12/21], [94mLoss[0m : 2.55297
[1mStep[0m  [14/21], [94mLoss[0m : 2.63399
[1mStep[0m  [16/21], [94mLoss[0m : 2.72816
[1mStep[0m  [18/21], [94mLoss[0m : 2.66483
[1mStep[0m  [20/21], [94mLoss[0m : 2.60809

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.630, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64896
[1mStep[0m  [2/21], [94mLoss[0m : 2.54113
[1mStep[0m  [4/21], [94mLoss[0m : 2.67731
[1mStep[0m  [6/21], [94mLoss[0m : 2.51196
[1mStep[0m  [8/21], [94mLoss[0m : 2.70244
[1mStep[0m  [10/21], [94mLoss[0m : 2.65271
[1mStep[0m  [12/21], [94mLoss[0m : 2.52949
[1mStep[0m  [14/21], [94mLoss[0m : 2.38274
[1mStep[0m  [16/21], [94mLoss[0m : 2.56982
[1mStep[0m  [18/21], [94mLoss[0m : 2.60432
[1mStep[0m  [20/21], [94mLoss[0m : 2.61535

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.712, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48635
[1mStep[0m  [2/21], [94mLoss[0m : 2.70898
[1mStep[0m  [4/21], [94mLoss[0m : 2.52669
[1mStep[0m  [6/21], [94mLoss[0m : 2.60812
[1mStep[0m  [8/21], [94mLoss[0m : 2.55799
[1mStep[0m  [10/21], [94mLoss[0m : 2.61475
[1mStep[0m  [12/21], [94mLoss[0m : 2.58262
[1mStep[0m  [14/21], [94mLoss[0m : 2.39373
[1mStep[0m  [16/21], [94mLoss[0m : 2.49404
[1mStep[0m  [18/21], [94mLoss[0m : 2.47088
[1mStep[0m  [20/21], [94mLoss[0m : 2.44743

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.706, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55751
[1mStep[0m  [2/21], [94mLoss[0m : 2.43161
[1mStep[0m  [4/21], [94mLoss[0m : 2.50513
[1mStep[0m  [6/21], [94mLoss[0m : 2.56417
[1mStep[0m  [8/21], [94mLoss[0m : 2.61351
[1mStep[0m  [10/21], [94mLoss[0m : 2.39562
[1mStep[0m  [12/21], [94mLoss[0m : 2.57789
[1mStep[0m  [14/21], [94mLoss[0m : 2.45298
[1mStep[0m  [16/21], [94mLoss[0m : 2.61109
[1mStep[0m  [18/21], [94mLoss[0m : 2.73188
[1mStep[0m  [20/21], [94mLoss[0m : 2.62801

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.618, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59321
[1mStep[0m  [2/21], [94mLoss[0m : 2.65400
[1mStep[0m  [4/21], [94mLoss[0m : 2.55557
[1mStep[0m  [6/21], [94mLoss[0m : 2.50100
[1mStep[0m  [8/21], [94mLoss[0m : 2.51241
[1mStep[0m  [10/21], [94mLoss[0m : 2.53037
[1mStep[0m  [12/21], [94mLoss[0m : 2.60650
[1mStep[0m  [14/21], [94mLoss[0m : 2.52487
[1mStep[0m  [16/21], [94mLoss[0m : 2.52714
[1mStep[0m  [18/21], [94mLoss[0m : 2.43303
[1mStep[0m  [20/21], [94mLoss[0m : 2.41061

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.645, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48930
[1mStep[0m  [2/21], [94mLoss[0m : 2.49061
[1mStep[0m  [4/21], [94mLoss[0m : 2.47667
[1mStep[0m  [6/21], [94mLoss[0m : 2.52953
[1mStep[0m  [8/21], [94mLoss[0m : 2.58987
[1mStep[0m  [10/21], [94mLoss[0m : 2.47267
[1mStep[0m  [12/21], [94mLoss[0m : 2.60398
[1mStep[0m  [14/21], [94mLoss[0m : 2.58487
[1mStep[0m  [16/21], [94mLoss[0m : 2.58593
[1mStep[0m  [18/21], [94mLoss[0m : 2.43259
[1mStep[0m  [20/21], [94mLoss[0m : 2.62144

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61779
[1mStep[0m  [2/21], [94mLoss[0m : 2.57187
[1mStep[0m  [4/21], [94mLoss[0m : 2.47275
[1mStep[0m  [6/21], [94mLoss[0m : 2.47350
[1mStep[0m  [8/21], [94mLoss[0m : 2.60692
[1mStep[0m  [10/21], [94mLoss[0m : 2.56034
[1mStep[0m  [12/21], [94mLoss[0m : 2.37390
[1mStep[0m  [14/21], [94mLoss[0m : 2.47558
[1mStep[0m  [16/21], [94mLoss[0m : 2.32973
[1mStep[0m  [18/21], [94mLoss[0m : 2.50751
[1mStep[0m  [20/21], [94mLoss[0m : 2.50479

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.577, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51777
[1mStep[0m  [2/21], [94mLoss[0m : 2.57876
[1mStep[0m  [4/21], [94mLoss[0m : 2.29189
[1mStep[0m  [6/21], [94mLoss[0m : 2.48619
[1mStep[0m  [8/21], [94mLoss[0m : 2.56486
[1mStep[0m  [10/21], [94mLoss[0m : 2.65491
[1mStep[0m  [12/21], [94mLoss[0m : 2.48518
[1mStep[0m  [14/21], [94mLoss[0m : 2.49508
[1mStep[0m  [16/21], [94mLoss[0m : 2.51506
[1mStep[0m  [18/21], [94mLoss[0m : 2.58284
[1mStep[0m  [20/21], [94mLoss[0m : 2.65900

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43593
[1mStep[0m  [2/21], [94mLoss[0m : 2.54883
[1mStep[0m  [4/21], [94mLoss[0m : 2.29297
[1mStep[0m  [6/21], [94mLoss[0m : 2.38165
[1mStep[0m  [8/21], [94mLoss[0m : 2.54766
[1mStep[0m  [10/21], [94mLoss[0m : 2.50273
[1mStep[0m  [12/21], [94mLoss[0m : 2.50326
[1mStep[0m  [14/21], [94mLoss[0m : 2.45817
[1mStep[0m  [16/21], [94mLoss[0m : 2.51726
[1mStep[0m  [18/21], [94mLoss[0m : 2.46107
[1mStep[0m  [20/21], [94mLoss[0m : 2.59384

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42696
[1mStep[0m  [2/21], [94mLoss[0m : 2.47837
[1mStep[0m  [4/21], [94mLoss[0m : 2.58576
[1mStep[0m  [6/21], [94mLoss[0m : 2.45317
[1mStep[0m  [8/21], [94mLoss[0m : 2.39142
[1mStep[0m  [10/21], [94mLoss[0m : 2.37131
[1mStep[0m  [12/21], [94mLoss[0m : 2.47350
[1mStep[0m  [14/21], [94mLoss[0m : 2.28807
[1mStep[0m  [16/21], [94mLoss[0m : 2.57770
[1mStep[0m  [18/21], [94mLoss[0m : 2.40490
[1mStep[0m  [20/21], [94mLoss[0m : 2.60328

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.557, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43558
[1mStep[0m  [2/21], [94mLoss[0m : 2.57478
[1mStep[0m  [4/21], [94mLoss[0m : 2.52633
[1mStep[0m  [6/21], [94mLoss[0m : 2.48056
[1mStep[0m  [8/21], [94mLoss[0m : 2.40825
[1mStep[0m  [10/21], [94mLoss[0m : 2.41358
[1mStep[0m  [12/21], [94mLoss[0m : 2.51903
[1mStep[0m  [14/21], [94mLoss[0m : 2.54625
[1mStep[0m  [16/21], [94mLoss[0m : 2.59827
[1mStep[0m  [18/21], [94mLoss[0m : 2.39080
[1mStep[0m  [20/21], [94mLoss[0m : 2.42433

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.533, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41521
[1mStep[0m  [2/21], [94mLoss[0m : 2.38416
[1mStep[0m  [4/21], [94mLoss[0m : 2.31447
[1mStep[0m  [6/21], [94mLoss[0m : 2.49344
[1mStep[0m  [8/21], [94mLoss[0m : 2.57350
[1mStep[0m  [10/21], [94mLoss[0m : 2.49918
[1mStep[0m  [12/21], [94mLoss[0m : 2.35261
[1mStep[0m  [14/21], [94mLoss[0m : 2.36779
[1mStep[0m  [16/21], [94mLoss[0m : 2.42176
[1mStep[0m  [18/21], [94mLoss[0m : 2.38960
[1mStep[0m  [20/21], [94mLoss[0m : 2.44831

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.512, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47801
[1mStep[0m  [2/21], [94mLoss[0m : 2.37866
[1mStep[0m  [4/21], [94mLoss[0m : 2.41303
[1mStep[0m  [6/21], [94mLoss[0m : 2.53657
[1mStep[0m  [8/21], [94mLoss[0m : 2.44114
[1mStep[0m  [10/21], [94mLoss[0m : 2.42071
[1mStep[0m  [12/21], [94mLoss[0m : 2.49351
[1mStep[0m  [14/21], [94mLoss[0m : 2.27145
[1mStep[0m  [16/21], [94mLoss[0m : 2.44300
[1mStep[0m  [18/21], [94mLoss[0m : 2.31445
[1mStep[0m  [20/21], [94mLoss[0m : 2.44301

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48296
[1mStep[0m  [2/21], [94mLoss[0m : 2.51629
[1mStep[0m  [4/21], [94mLoss[0m : 2.39387
[1mStep[0m  [6/21], [94mLoss[0m : 2.32996
[1mStep[0m  [8/21], [94mLoss[0m : 2.31966
[1mStep[0m  [10/21], [94mLoss[0m : 2.40682
[1mStep[0m  [12/21], [94mLoss[0m : 2.31677
[1mStep[0m  [14/21], [94mLoss[0m : 2.38877
[1mStep[0m  [16/21], [94mLoss[0m : 2.45061
[1mStep[0m  [18/21], [94mLoss[0m : 2.34107
[1mStep[0m  [20/21], [94mLoss[0m : 2.57864

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.513
====================================

Phase 2 - Evaluation MAE:  2.512584822518485
MAE score P1       2.413028
MAE score P2       2.512585
loss               2.419717
learning_rate      0.007525
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay         0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.18940
[1mStep[0m  [4/42], [94mLoss[0m : 10.07718
[1mStep[0m  [8/42], [94mLoss[0m : 7.83367
[1mStep[0m  [12/42], [94mLoss[0m : 4.89392
[1mStep[0m  [16/42], [94mLoss[0m : 3.34564
[1mStep[0m  [20/42], [94mLoss[0m : 3.22208
[1mStep[0m  [24/42], [94mLoss[0m : 3.13968
[1mStep[0m  [28/42], [94mLoss[0m : 3.20693
[1mStep[0m  [32/42], [94mLoss[0m : 3.13188
[1mStep[0m  [36/42], [94mLoss[0m : 2.66635
[1mStep[0m  [40/42], [94mLoss[0m : 2.83125

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.725, [92mTest[0m: 10.521, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.00910
[1mStep[0m  [4/42], [94mLoss[0m : 2.63951
[1mStep[0m  [8/42], [94mLoss[0m : 2.46131
[1mStep[0m  [12/42], [94mLoss[0m : 2.50963
[1mStep[0m  [16/42], [94mLoss[0m : 2.69339
[1mStep[0m  [20/42], [94mLoss[0m : 2.36051
[1mStep[0m  [24/42], [94mLoss[0m : 2.75114
[1mStep[0m  [28/42], [94mLoss[0m : 2.86648
[1mStep[0m  [32/42], [94mLoss[0m : 3.02149
[1mStep[0m  [36/42], [94mLoss[0m : 2.67790
[1mStep[0m  [40/42], [94mLoss[0m : 2.67532

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60185
[1mStep[0m  [4/42], [94mLoss[0m : 2.80828
[1mStep[0m  [8/42], [94mLoss[0m : 2.76999
[1mStep[0m  [12/42], [94mLoss[0m : 2.54548
[1mStep[0m  [16/42], [94mLoss[0m : 2.50015
[1mStep[0m  [20/42], [94mLoss[0m : 2.70884
[1mStep[0m  [24/42], [94mLoss[0m : 2.56043
[1mStep[0m  [28/42], [94mLoss[0m : 2.67253
[1mStep[0m  [32/42], [94mLoss[0m : 2.66275
[1mStep[0m  [36/42], [94mLoss[0m : 2.80580
[1mStep[0m  [40/42], [94mLoss[0m : 2.72808

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54177
[1mStep[0m  [4/42], [94mLoss[0m : 2.84527
[1mStep[0m  [8/42], [94mLoss[0m : 2.48313
[1mStep[0m  [12/42], [94mLoss[0m : 2.63189
[1mStep[0m  [16/42], [94mLoss[0m : 2.77327
[1mStep[0m  [20/42], [94mLoss[0m : 2.59547
[1mStep[0m  [24/42], [94mLoss[0m : 2.51611
[1mStep[0m  [28/42], [94mLoss[0m : 2.67796
[1mStep[0m  [32/42], [94mLoss[0m : 2.26701
[1mStep[0m  [36/42], [94mLoss[0m : 2.53681
[1mStep[0m  [40/42], [94mLoss[0m : 2.58042

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36816
[1mStep[0m  [4/42], [94mLoss[0m : 2.56468
[1mStep[0m  [8/42], [94mLoss[0m : 2.59904
[1mStep[0m  [12/42], [94mLoss[0m : 2.48030
[1mStep[0m  [16/42], [94mLoss[0m : 2.69042
[1mStep[0m  [20/42], [94mLoss[0m : 2.82510
[1mStep[0m  [24/42], [94mLoss[0m : 2.48501
[1mStep[0m  [28/42], [94mLoss[0m : 2.50836
[1mStep[0m  [32/42], [94mLoss[0m : 2.56853
[1mStep[0m  [36/42], [94mLoss[0m : 2.41351
[1mStep[0m  [40/42], [94mLoss[0m : 2.69184

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65246
[1mStep[0m  [4/42], [94mLoss[0m : 2.55135
[1mStep[0m  [8/42], [94mLoss[0m : 2.51312
[1mStep[0m  [12/42], [94mLoss[0m : 2.54609
[1mStep[0m  [16/42], [94mLoss[0m : 2.64723
[1mStep[0m  [20/42], [94mLoss[0m : 2.50205
[1mStep[0m  [24/42], [94mLoss[0m : 2.66542
[1mStep[0m  [28/42], [94mLoss[0m : 2.56395
[1mStep[0m  [32/42], [94mLoss[0m : 2.57672
[1mStep[0m  [36/42], [94mLoss[0m : 2.84499
[1mStep[0m  [40/42], [94mLoss[0m : 2.53783

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66347
[1mStep[0m  [4/42], [94mLoss[0m : 2.68752
[1mStep[0m  [8/42], [94mLoss[0m : 2.53867
[1mStep[0m  [12/42], [94mLoss[0m : 2.65063
[1mStep[0m  [16/42], [94mLoss[0m : 2.43694
[1mStep[0m  [20/42], [94mLoss[0m : 2.58377
[1mStep[0m  [24/42], [94mLoss[0m : 2.57017
[1mStep[0m  [28/42], [94mLoss[0m : 2.52911
[1mStep[0m  [32/42], [94mLoss[0m : 2.61427
[1mStep[0m  [36/42], [94mLoss[0m : 2.58019
[1mStep[0m  [40/42], [94mLoss[0m : 2.63279

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58260
[1mStep[0m  [4/42], [94mLoss[0m : 2.47534
[1mStep[0m  [8/42], [94mLoss[0m : 2.56027
[1mStep[0m  [12/42], [94mLoss[0m : 2.66199
[1mStep[0m  [16/42], [94mLoss[0m : 2.43387
[1mStep[0m  [20/42], [94mLoss[0m : 2.70081
[1mStep[0m  [24/42], [94mLoss[0m : 2.68906
[1mStep[0m  [28/42], [94mLoss[0m : 2.54445
[1mStep[0m  [32/42], [94mLoss[0m : 2.52987
[1mStep[0m  [36/42], [94mLoss[0m : 2.52931
[1mStep[0m  [40/42], [94mLoss[0m : 2.51583

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25327
[1mStep[0m  [4/42], [94mLoss[0m : 2.40483
[1mStep[0m  [8/42], [94mLoss[0m : 2.51412
[1mStep[0m  [12/42], [94mLoss[0m : 2.63357
[1mStep[0m  [16/42], [94mLoss[0m : 2.63214
[1mStep[0m  [20/42], [94mLoss[0m : 2.63758
[1mStep[0m  [24/42], [94mLoss[0m : 2.64685
[1mStep[0m  [28/42], [94mLoss[0m : 2.80328
[1mStep[0m  [32/42], [94mLoss[0m : 2.69616
[1mStep[0m  [36/42], [94mLoss[0m : 2.72465
[1mStep[0m  [40/42], [94mLoss[0m : 2.61265

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77161
[1mStep[0m  [4/42], [94mLoss[0m : 2.54410
[1mStep[0m  [8/42], [94mLoss[0m : 2.39413
[1mStep[0m  [12/42], [94mLoss[0m : 2.67936
[1mStep[0m  [16/42], [94mLoss[0m : 2.37818
[1mStep[0m  [20/42], [94mLoss[0m : 2.66261
[1mStep[0m  [24/42], [94mLoss[0m : 2.52754
[1mStep[0m  [28/42], [94mLoss[0m : 2.64411
[1mStep[0m  [32/42], [94mLoss[0m : 2.58477
[1mStep[0m  [36/42], [94mLoss[0m : 2.73736
[1mStep[0m  [40/42], [94mLoss[0m : 2.97652

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54881
[1mStep[0m  [4/42], [94mLoss[0m : 2.59317
[1mStep[0m  [8/42], [94mLoss[0m : 2.46583
[1mStep[0m  [12/42], [94mLoss[0m : 2.67447
[1mStep[0m  [16/42], [94mLoss[0m : 2.54672
[1mStep[0m  [20/42], [94mLoss[0m : 2.51678
[1mStep[0m  [24/42], [94mLoss[0m : 2.46874
[1mStep[0m  [28/42], [94mLoss[0m : 2.48449
[1mStep[0m  [32/42], [94mLoss[0m : 2.57826
[1mStep[0m  [36/42], [94mLoss[0m : 2.38822
[1mStep[0m  [40/42], [94mLoss[0m : 2.66362

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43919
[1mStep[0m  [4/42], [94mLoss[0m : 2.58110
[1mStep[0m  [8/42], [94mLoss[0m : 2.39917
[1mStep[0m  [12/42], [94mLoss[0m : 2.34917
[1mStep[0m  [16/42], [94mLoss[0m : 2.47193
[1mStep[0m  [20/42], [94mLoss[0m : 2.63029
[1mStep[0m  [24/42], [94mLoss[0m : 2.59373
[1mStep[0m  [28/42], [94mLoss[0m : 2.68658
[1mStep[0m  [32/42], [94mLoss[0m : 2.80095
[1mStep[0m  [36/42], [94mLoss[0m : 2.61496
[1mStep[0m  [40/42], [94mLoss[0m : 2.31063

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61016
[1mStep[0m  [4/42], [94mLoss[0m : 2.29327
[1mStep[0m  [8/42], [94mLoss[0m : 2.64990
[1mStep[0m  [12/42], [94mLoss[0m : 2.67004
[1mStep[0m  [16/42], [94mLoss[0m : 2.44598
[1mStep[0m  [20/42], [94mLoss[0m : 2.69482
[1mStep[0m  [24/42], [94mLoss[0m : 2.39702
[1mStep[0m  [28/42], [94mLoss[0m : 2.85085
[1mStep[0m  [32/42], [94mLoss[0m : 2.54830
[1mStep[0m  [36/42], [94mLoss[0m : 2.46369
[1mStep[0m  [40/42], [94mLoss[0m : 2.68801

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32191
[1mStep[0m  [4/42], [94mLoss[0m : 2.57860
[1mStep[0m  [8/42], [94mLoss[0m : 2.73707
[1mStep[0m  [12/42], [94mLoss[0m : 2.63084
[1mStep[0m  [16/42], [94mLoss[0m : 2.44159
[1mStep[0m  [20/42], [94mLoss[0m : 2.61719
[1mStep[0m  [24/42], [94mLoss[0m : 2.69862
[1mStep[0m  [28/42], [94mLoss[0m : 2.36151
[1mStep[0m  [32/42], [94mLoss[0m : 2.61399
[1mStep[0m  [36/42], [94mLoss[0m : 2.60930
[1mStep[0m  [40/42], [94mLoss[0m : 2.55315

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71982
[1mStep[0m  [4/42], [94mLoss[0m : 2.68871
[1mStep[0m  [8/42], [94mLoss[0m : 2.60939
[1mStep[0m  [12/42], [94mLoss[0m : 2.57873
[1mStep[0m  [16/42], [94mLoss[0m : 2.59051
[1mStep[0m  [20/42], [94mLoss[0m : 2.36208
[1mStep[0m  [24/42], [94mLoss[0m : 2.56973
[1mStep[0m  [28/42], [94mLoss[0m : 2.55422
[1mStep[0m  [32/42], [94mLoss[0m : 2.61596
[1mStep[0m  [36/42], [94mLoss[0m : 2.35800
[1mStep[0m  [40/42], [94mLoss[0m : 2.40527

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50227
[1mStep[0m  [4/42], [94mLoss[0m : 2.62626
[1mStep[0m  [8/42], [94mLoss[0m : 2.70181
[1mStep[0m  [12/42], [94mLoss[0m : 2.38729
[1mStep[0m  [16/42], [94mLoss[0m : 2.40017
[1mStep[0m  [20/42], [94mLoss[0m : 2.73567
[1mStep[0m  [24/42], [94mLoss[0m : 2.46246
[1mStep[0m  [28/42], [94mLoss[0m : 2.58136
[1mStep[0m  [32/42], [94mLoss[0m : 2.42080
[1mStep[0m  [36/42], [94mLoss[0m : 2.70038
[1mStep[0m  [40/42], [94mLoss[0m : 2.61857

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35822
[1mStep[0m  [4/42], [94mLoss[0m : 2.49032
[1mStep[0m  [8/42], [94mLoss[0m : 2.25895
[1mStep[0m  [12/42], [94mLoss[0m : 2.37927
[1mStep[0m  [16/42], [94mLoss[0m : 2.63092
[1mStep[0m  [20/42], [94mLoss[0m : 2.35719
[1mStep[0m  [24/42], [94mLoss[0m : 2.66515
[1mStep[0m  [28/42], [94mLoss[0m : 2.69091
[1mStep[0m  [32/42], [94mLoss[0m : 2.55012
[1mStep[0m  [36/42], [94mLoss[0m : 2.57394
[1mStep[0m  [40/42], [94mLoss[0m : 2.49595

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43685
[1mStep[0m  [4/42], [94mLoss[0m : 2.32515
[1mStep[0m  [8/42], [94mLoss[0m : 2.49783
[1mStep[0m  [12/42], [94mLoss[0m : 2.67864
[1mStep[0m  [16/42], [94mLoss[0m : 2.70457
[1mStep[0m  [20/42], [94mLoss[0m : 2.54376
[1mStep[0m  [24/42], [94mLoss[0m : 2.44269
[1mStep[0m  [28/42], [94mLoss[0m : 2.65130
[1mStep[0m  [32/42], [94mLoss[0m : 2.44361
[1mStep[0m  [36/42], [94mLoss[0m : 2.37970
[1mStep[0m  [40/42], [94mLoss[0m : 2.38162

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75957
[1mStep[0m  [4/42], [94mLoss[0m : 2.67069
[1mStep[0m  [8/42], [94mLoss[0m : 2.57035
[1mStep[0m  [12/42], [94mLoss[0m : 2.40437
[1mStep[0m  [16/42], [94mLoss[0m : 2.62832
[1mStep[0m  [20/42], [94mLoss[0m : 2.47595
[1mStep[0m  [24/42], [94mLoss[0m : 2.37198
[1mStep[0m  [28/42], [94mLoss[0m : 2.34027
[1mStep[0m  [32/42], [94mLoss[0m : 2.51502
[1mStep[0m  [36/42], [94mLoss[0m : 2.45164
[1mStep[0m  [40/42], [94mLoss[0m : 2.53195

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73625
[1mStep[0m  [4/42], [94mLoss[0m : 2.67626
[1mStep[0m  [8/42], [94mLoss[0m : 2.61700
[1mStep[0m  [12/42], [94mLoss[0m : 2.34525
[1mStep[0m  [16/42], [94mLoss[0m : 2.45510
[1mStep[0m  [20/42], [94mLoss[0m : 2.44115
[1mStep[0m  [24/42], [94mLoss[0m : 2.71356
[1mStep[0m  [28/42], [94mLoss[0m : 2.49843
[1mStep[0m  [32/42], [94mLoss[0m : 2.65631
[1mStep[0m  [36/42], [94mLoss[0m : 2.60800
[1mStep[0m  [40/42], [94mLoss[0m : 2.53069

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64493
[1mStep[0m  [4/42], [94mLoss[0m : 2.56825
[1mStep[0m  [8/42], [94mLoss[0m : 2.42298
[1mStep[0m  [12/42], [94mLoss[0m : 2.72557
[1mStep[0m  [16/42], [94mLoss[0m : 2.52231
[1mStep[0m  [20/42], [94mLoss[0m : 2.53114
[1mStep[0m  [24/42], [94mLoss[0m : 2.58302
[1mStep[0m  [28/42], [94mLoss[0m : 2.36223
[1mStep[0m  [32/42], [94mLoss[0m : 2.34516
[1mStep[0m  [36/42], [94mLoss[0m : 2.40122
[1mStep[0m  [40/42], [94mLoss[0m : 2.57412

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58970
[1mStep[0m  [4/42], [94mLoss[0m : 2.56492
[1mStep[0m  [8/42], [94mLoss[0m : 2.71863
[1mStep[0m  [12/42], [94mLoss[0m : 2.63804
[1mStep[0m  [16/42], [94mLoss[0m : 2.46534
[1mStep[0m  [20/42], [94mLoss[0m : 2.40235
[1mStep[0m  [24/42], [94mLoss[0m : 2.20796
[1mStep[0m  [28/42], [94mLoss[0m : 2.65534
[1mStep[0m  [32/42], [94mLoss[0m : 2.36645
[1mStep[0m  [36/42], [94mLoss[0m : 2.54496
[1mStep[0m  [40/42], [94mLoss[0m : 2.58936

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63573
[1mStep[0m  [4/42], [94mLoss[0m : 2.37421
[1mStep[0m  [8/42], [94mLoss[0m : 2.64337
[1mStep[0m  [12/42], [94mLoss[0m : 2.33785
[1mStep[0m  [16/42], [94mLoss[0m : 2.51188
[1mStep[0m  [20/42], [94mLoss[0m : 2.56096
[1mStep[0m  [24/42], [94mLoss[0m : 2.48405
[1mStep[0m  [28/42], [94mLoss[0m : 2.37656
[1mStep[0m  [32/42], [94mLoss[0m : 2.40354
[1mStep[0m  [36/42], [94mLoss[0m : 2.60207
[1mStep[0m  [40/42], [94mLoss[0m : 2.38055

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50194
[1mStep[0m  [4/42], [94mLoss[0m : 2.59747
[1mStep[0m  [8/42], [94mLoss[0m : 2.59986
[1mStep[0m  [12/42], [94mLoss[0m : 2.55985
[1mStep[0m  [16/42], [94mLoss[0m : 2.56054
[1mStep[0m  [20/42], [94mLoss[0m : 2.57962
[1mStep[0m  [24/42], [94mLoss[0m : 2.40150
[1mStep[0m  [28/42], [94mLoss[0m : 2.58803
[1mStep[0m  [32/42], [94mLoss[0m : 2.36774
[1mStep[0m  [36/42], [94mLoss[0m : 2.41632
[1mStep[0m  [40/42], [94mLoss[0m : 2.59377

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47667
[1mStep[0m  [4/42], [94mLoss[0m : 2.47699
[1mStep[0m  [8/42], [94mLoss[0m : 2.61348
[1mStep[0m  [12/42], [94mLoss[0m : 2.35213
[1mStep[0m  [16/42], [94mLoss[0m : 2.55104
[1mStep[0m  [20/42], [94mLoss[0m : 2.77009
[1mStep[0m  [24/42], [94mLoss[0m : 2.56811
[1mStep[0m  [28/42], [94mLoss[0m : 2.74968
[1mStep[0m  [32/42], [94mLoss[0m : 2.45523
[1mStep[0m  [36/42], [94mLoss[0m : 2.48356
[1mStep[0m  [40/42], [94mLoss[0m : 2.55125

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43576
[1mStep[0m  [4/42], [94mLoss[0m : 2.34777
[1mStep[0m  [8/42], [94mLoss[0m : 2.20123
[1mStep[0m  [12/42], [94mLoss[0m : 2.60581
[1mStep[0m  [16/42], [94mLoss[0m : 2.58338
[1mStep[0m  [20/42], [94mLoss[0m : 2.46503
[1mStep[0m  [24/42], [94mLoss[0m : 2.81775
[1mStep[0m  [28/42], [94mLoss[0m : 2.37052
[1mStep[0m  [32/42], [94mLoss[0m : 2.44289
[1mStep[0m  [36/42], [94mLoss[0m : 2.46941
[1mStep[0m  [40/42], [94mLoss[0m : 2.59684

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60353
[1mStep[0m  [4/42], [94mLoss[0m : 2.68365
[1mStep[0m  [8/42], [94mLoss[0m : 2.56318
[1mStep[0m  [12/42], [94mLoss[0m : 2.59990
[1mStep[0m  [16/42], [94mLoss[0m : 2.60963
[1mStep[0m  [20/42], [94mLoss[0m : 2.51805
[1mStep[0m  [24/42], [94mLoss[0m : 2.59174
[1mStep[0m  [28/42], [94mLoss[0m : 2.59866
[1mStep[0m  [32/42], [94mLoss[0m : 2.48173
[1mStep[0m  [36/42], [94mLoss[0m : 2.56065
[1mStep[0m  [40/42], [94mLoss[0m : 2.44191

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13106
[1mStep[0m  [4/42], [94mLoss[0m : 2.49394
[1mStep[0m  [8/42], [94mLoss[0m : 2.32682
[1mStep[0m  [12/42], [94mLoss[0m : 2.48454
[1mStep[0m  [16/42], [94mLoss[0m : 2.24724
[1mStep[0m  [20/42], [94mLoss[0m : 2.44940
[1mStep[0m  [24/42], [94mLoss[0m : 2.33323
[1mStep[0m  [28/42], [94mLoss[0m : 2.37571
[1mStep[0m  [32/42], [94mLoss[0m : 2.67762
[1mStep[0m  [36/42], [94mLoss[0m : 2.54150
[1mStep[0m  [40/42], [94mLoss[0m : 2.56541

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41586
[1mStep[0m  [4/42], [94mLoss[0m : 2.55643
[1mStep[0m  [8/42], [94mLoss[0m : 2.50773
[1mStep[0m  [12/42], [94mLoss[0m : 2.33942
[1mStep[0m  [16/42], [94mLoss[0m : 2.62122
[1mStep[0m  [20/42], [94mLoss[0m : 2.66839
[1mStep[0m  [24/42], [94mLoss[0m : 2.65547
[1mStep[0m  [28/42], [94mLoss[0m : 2.43088
[1mStep[0m  [32/42], [94mLoss[0m : 2.38284
[1mStep[0m  [36/42], [94mLoss[0m : 2.53346
[1mStep[0m  [40/42], [94mLoss[0m : 2.53198

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34111
[1mStep[0m  [4/42], [94mLoss[0m : 2.26814
[1mStep[0m  [8/42], [94mLoss[0m : 2.53453
[1mStep[0m  [12/42], [94mLoss[0m : 2.33604
[1mStep[0m  [16/42], [94mLoss[0m : 2.36158
[1mStep[0m  [20/42], [94mLoss[0m : 2.33406
[1mStep[0m  [24/42], [94mLoss[0m : 2.28253
[1mStep[0m  [28/42], [94mLoss[0m : 2.57881
[1mStep[0m  [32/42], [94mLoss[0m : 2.64762
[1mStep[0m  [36/42], [94mLoss[0m : 2.68149
[1mStep[0m  [40/42], [94mLoss[0m : 2.26022

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.330
====================================

Phase 1 - Evaluation MAE:  2.330123577799116
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.33975
[1mStep[0m  [4/42], [94mLoss[0m : 2.74689
[1mStep[0m  [8/42], [94mLoss[0m : 2.57950
[1mStep[0m  [12/42], [94mLoss[0m : 2.52256
[1mStep[0m  [16/42], [94mLoss[0m : 2.54119
[1mStep[0m  [20/42], [94mLoss[0m : 2.64882
[1mStep[0m  [24/42], [94mLoss[0m : 2.70680
[1mStep[0m  [28/42], [94mLoss[0m : 2.47149
[1mStep[0m  [32/42], [94mLoss[0m : 2.40584
[1mStep[0m  [36/42], [94mLoss[0m : 2.81085
[1mStep[0m  [40/42], [94mLoss[0m : 2.76611

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35103
[1mStep[0m  [4/42], [94mLoss[0m : 2.51224
[1mStep[0m  [8/42], [94mLoss[0m : 2.23128
[1mStep[0m  [12/42], [94mLoss[0m : 2.33684
[1mStep[0m  [16/42], [94mLoss[0m : 2.37049
[1mStep[0m  [20/42], [94mLoss[0m : 2.42683
[1mStep[0m  [24/42], [94mLoss[0m : 2.39463
[1mStep[0m  [28/42], [94mLoss[0m : 2.31751
[1mStep[0m  [32/42], [94mLoss[0m : 2.48294
[1mStep[0m  [36/42], [94mLoss[0m : 2.43046
[1mStep[0m  [40/42], [94mLoss[0m : 2.57917

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38378
[1mStep[0m  [4/42], [94mLoss[0m : 2.57789
[1mStep[0m  [8/42], [94mLoss[0m : 2.33816
[1mStep[0m  [12/42], [94mLoss[0m : 2.40996
[1mStep[0m  [16/42], [94mLoss[0m : 2.44481
[1mStep[0m  [20/42], [94mLoss[0m : 2.16178
[1mStep[0m  [24/42], [94mLoss[0m : 2.30575
[1mStep[0m  [28/42], [94mLoss[0m : 2.24397
[1mStep[0m  [32/42], [94mLoss[0m : 2.15830
[1mStep[0m  [36/42], [94mLoss[0m : 2.24713
[1mStep[0m  [40/42], [94mLoss[0m : 2.48204

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55406
[1mStep[0m  [4/42], [94mLoss[0m : 2.37304
[1mStep[0m  [8/42], [94mLoss[0m : 2.34411
[1mStep[0m  [12/42], [94mLoss[0m : 2.14807
[1mStep[0m  [16/42], [94mLoss[0m : 2.38002
[1mStep[0m  [20/42], [94mLoss[0m : 2.26820
[1mStep[0m  [24/42], [94mLoss[0m : 2.21756
[1mStep[0m  [28/42], [94mLoss[0m : 2.17945
[1mStep[0m  [32/42], [94mLoss[0m : 2.34935
[1mStep[0m  [36/42], [94mLoss[0m : 2.41390
[1mStep[0m  [40/42], [94mLoss[0m : 2.29294

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32510
[1mStep[0m  [4/42], [94mLoss[0m : 2.06138
[1mStep[0m  [8/42], [94mLoss[0m : 2.20482
[1mStep[0m  [12/42], [94mLoss[0m : 2.22840
[1mStep[0m  [16/42], [94mLoss[0m : 2.16483
[1mStep[0m  [20/42], [94mLoss[0m : 2.05264
[1mStep[0m  [24/42], [94mLoss[0m : 2.23987
[1mStep[0m  [28/42], [94mLoss[0m : 2.17386
[1mStep[0m  [32/42], [94mLoss[0m : 2.24399
[1mStep[0m  [36/42], [94mLoss[0m : 2.38931
[1mStep[0m  [40/42], [94mLoss[0m : 2.38244

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96350
[1mStep[0m  [4/42], [94mLoss[0m : 1.98723
[1mStep[0m  [8/42], [94mLoss[0m : 2.29843
[1mStep[0m  [12/42], [94mLoss[0m : 2.15747
[1mStep[0m  [16/42], [94mLoss[0m : 2.22787
[1mStep[0m  [20/42], [94mLoss[0m : 2.07709
[1mStep[0m  [24/42], [94mLoss[0m : 2.05332
[1mStep[0m  [28/42], [94mLoss[0m : 2.32355
[1mStep[0m  [32/42], [94mLoss[0m : 2.37055
[1mStep[0m  [36/42], [94mLoss[0m : 2.24975
[1mStep[0m  [40/42], [94mLoss[0m : 2.18292

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20335
[1mStep[0m  [4/42], [94mLoss[0m : 2.09060
[1mStep[0m  [8/42], [94mLoss[0m : 2.26110
[1mStep[0m  [12/42], [94mLoss[0m : 2.02724
[1mStep[0m  [16/42], [94mLoss[0m : 2.20254
[1mStep[0m  [20/42], [94mLoss[0m : 2.09406
[1mStep[0m  [24/42], [94mLoss[0m : 2.10881
[1mStep[0m  [28/42], [94mLoss[0m : 2.05799
[1mStep[0m  [32/42], [94mLoss[0m : 2.05652
[1mStep[0m  [36/42], [94mLoss[0m : 2.26623
[1mStep[0m  [40/42], [94mLoss[0m : 1.93778

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09719
[1mStep[0m  [4/42], [94mLoss[0m : 2.00391
[1mStep[0m  [8/42], [94mLoss[0m : 2.07430
[1mStep[0m  [12/42], [94mLoss[0m : 1.89252
[1mStep[0m  [16/42], [94mLoss[0m : 2.03977
[1mStep[0m  [20/42], [94mLoss[0m : 2.08417
[1mStep[0m  [24/42], [94mLoss[0m : 2.04705
[1mStep[0m  [28/42], [94mLoss[0m : 1.91427
[1mStep[0m  [32/42], [94mLoss[0m : 1.94283
[1mStep[0m  [36/42], [94mLoss[0m : 2.19591
[1mStep[0m  [40/42], [94mLoss[0m : 2.08391

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07145
[1mStep[0m  [4/42], [94mLoss[0m : 2.06198
[1mStep[0m  [8/42], [94mLoss[0m : 2.01969
[1mStep[0m  [12/42], [94mLoss[0m : 1.89093
[1mStep[0m  [16/42], [94mLoss[0m : 1.95342
[1mStep[0m  [20/42], [94mLoss[0m : 2.17631
[1mStep[0m  [24/42], [94mLoss[0m : 1.86455
[1mStep[0m  [28/42], [94mLoss[0m : 2.01352
[1mStep[0m  [32/42], [94mLoss[0m : 2.02154
[1mStep[0m  [36/42], [94mLoss[0m : 2.08530
[1mStep[0m  [40/42], [94mLoss[0m : 2.14762

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79355
[1mStep[0m  [4/42], [94mLoss[0m : 2.07282
[1mStep[0m  [8/42], [94mLoss[0m : 1.91342
[1mStep[0m  [12/42], [94mLoss[0m : 2.09227
[1mStep[0m  [16/42], [94mLoss[0m : 2.02710
[1mStep[0m  [20/42], [94mLoss[0m : 1.84108
[1mStep[0m  [24/42], [94mLoss[0m : 1.86900
[1mStep[0m  [28/42], [94mLoss[0m : 1.86449
[1mStep[0m  [32/42], [94mLoss[0m : 2.00865
[1mStep[0m  [36/42], [94mLoss[0m : 1.85980
[1mStep[0m  [40/42], [94mLoss[0m : 2.03430

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79528
[1mStep[0m  [4/42], [94mLoss[0m : 1.97702
[1mStep[0m  [8/42], [94mLoss[0m : 1.99072
[1mStep[0m  [12/42], [94mLoss[0m : 1.96932
[1mStep[0m  [16/42], [94mLoss[0m : 2.05438
[1mStep[0m  [20/42], [94mLoss[0m : 1.82938
[1mStep[0m  [24/42], [94mLoss[0m : 1.64613
[1mStep[0m  [28/42], [94mLoss[0m : 1.93012
[1mStep[0m  [32/42], [94mLoss[0m : 1.96565
[1mStep[0m  [36/42], [94mLoss[0m : 2.08443
[1mStep[0m  [40/42], [94mLoss[0m : 1.97814

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73836
[1mStep[0m  [4/42], [94mLoss[0m : 1.84934
[1mStep[0m  [8/42], [94mLoss[0m : 1.70549
[1mStep[0m  [12/42], [94mLoss[0m : 1.93045
[1mStep[0m  [16/42], [94mLoss[0m : 1.87374
[1mStep[0m  [20/42], [94mLoss[0m : 1.70168
[1mStep[0m  [24/42], [94mLoss[0m : 1.92135
[1mStep[0m  [28/42], [94mLoss[0m : 1.89812
[1mStep[0m  [32/42], [94mLoss[0m : 1.88139
[1mStep[0m  [36/42], [94mLoss[0m : 1.93036
[1mStep[0m  [40/42], [94mLoss[0m : 1.86105

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63492
[1mStep[0m  [4/42], [94mLoss[0m : 1.66923
[1mStep[0m  [8/42], [94mLoss[0m : 1.56054
[1mStep[0m  [12/42], [94mLoss[0m : 1.93403
[1mStep[0m  [16/42], [94mLoss[0m : 1.70231
[1mStep[0m  [20/42], [94mLoss[0m : 1.63121
[1mStep[0m  [24/42], [94mLoss[0m : 2.02881
[1mStep[0m  [28/42], [94mLoss[0m : 1.83676
[1mStep[0m  [32/42], [94mLoss[0m : 1.88838
[1mStep[0m  [36/42], [94mLoss[0m : 1.77033
[1mStep[0m  [40/42], [94mLoss[0m : 1.75163

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.464, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67718
[1mStep[0m  [4/42], [94mLoss[0m : 1.81631
[1mStep[0m  [8/42], [94mLoss[0m : 1.75425
[1mStep[0m  [12/42], [94mLoss[0m : 1.75572
[1mStep[0m  [16/42], [94mLoss[0m : 1.94734
[1mStep[0m  [20/42], [94mLoss[0m : 1.90611
[1mStep[0m  [24/42], [94mLoss[0m : 2.01453
[1mStep[0m  [28/42], [94mLoss[0m : 1.80883
[1mStep[0m  [32/42], [94mLoss[0m : 1.86160
[1mStep[0m  [36/42], [94mLoss[0m : 1.84770
[1mStep[0m  [40/42], [94mLoss[0m : 1.73232

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77667
[1mStep[0m  [4/42], [94mLoss[0m : 1.71466
[1mStep[0m  [8/42], [94mLoss[0m : 1.65243
[1mStep[0m  [12/42], [94mLoss[0m : 1.88970
[1mStep[0m  [16/42], [94mLoss[0m : 1.86452
[1mStep[0m  [20/42], [94mLoss[0m : 1.86953
[1mStep[0m  [24/42], [94mLoss[0m : 1.67100
[1mStep[0m  [28/42], [94mLoss[0m : 1.76978
[1mStep[0m  [32/42], [94mLoss[0m : 1.70932
[1mStep[0m  [36/42], [94mLoss[0m : 1.68600
[1mStep[0m  [40/42], [94mLoss[0m : 1.86751

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84047
[1mStep[0m  [4/42], [94mLoss[0m : 1.68606
[1mStep[0m  [8/42], [94mLoss[0m : 1.71244
[1mStep[0m  [12/42], [94mLoss[0m : 1.84982
[1mStep[0m  [16/42], [94mLoss[0m : 1.46504
[1mStep[0m  [20/42], [94mLoss[0m : 1.75736
[1mStep[0m  [24/42], [94mLoss[0m : 1.81997
[1mStep[0m  [28/42], [94mLoss[0m : 1.62949
[1mStep[0m  [32/42], [94mLoss[0m : 1.83291
[1mStep[0m  [36/42], [94mLoss[0m : 1.79820
[1mStep[0m  [40/42], [94mLoss[0m : 1.54411

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66628
[1mStep[0m  [4/42], [94mLoss[0m : 1.88911
[1mStep[0m  [8/42], [94mLoss[0m : 1.69018
[1mStep[0m  [12/42], [94mLoss[0m : 1.73263
[1mStep[0m  [16/42], [94mLoss[0m : 1.67142
[1mStep[0m  [20/42], [94mLoss[0m : 1.64792
[1mStep[0m  [24/42], [94mLoss[0m : 1.60636
[1mStep[0m  [28/42], [94mLoss[0m : 1.78772
[1mStep[0m  [32/42], [94mLoss[0m : 1.63832
[1mStep[0m  [36/42], [94mLoss[0m : 1.79874
[1mStep[0m  [40/42], [94mLoss[0m : 1.88106

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68698
[1mStep[0m  [4/42], [94mLoss[0m : 1.62591
[1mStep[0m  [8/42], [94mLoss[0m : 1.71539
[1mStep[0m  [12/42], [94mLoss[0m : 1.61944
[1mStep[0m  [16/42], [94mLoss[0m : 1.60351
[1mStep[0m  [20/42], [94mLoss[0m : 1.67540
[1mStep[0m  [24/42], [94mLoss[0m : 1.73322
[1mStep[0m  [28/42], [94mLoss[0m : 1.76146
[1mStep[0m  [32/42], [94mLoss[0m : 1.72197
[1mStep[0m  [36/42], [94mLoss[0m : 1.61204
[1mStep[0m  [40/42], [94mLoss[0m : 1.61037

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58188
[1mStep[0m  [4/42], [94mLoss[0m : 1.72771
[1mStep[0m  [8/42], [94mLoss[0m : 1.58717
[1mStep[0m  [12/42], [94mLoss[0m : 1.61902
[1mStep[0m  [16/42], [94mLoss[0m : 1.57185
[1mStep[0m  [20/42], [94mLoss[0m : 1.63969
[1mStep[0m  [24/42], [94mLoss[0m : 1.72967
[1mStep[0m  [28/42], [94mLoss[0m : 1.70762
[1mStep[0m  [32/42], [94mLoss[0m : 1.61416
[1mStep[0m  [36/42], [94mLoss[0m : 1.62749
[1mStep[0m  [40/42], [94mLoss[0m : 1.67064

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59113
[1mStep[0m  [4/42], [94mLoss[0m : 1.43843
[1mStep[0m  [8/42], [94mLoss[0m : 1.53799
[1mStep[0m  [12/42], [94mLoss[0m : 1.68117
[1mStep[0m  [16/42], [94mLoss[0m : 1.61672
[1mStep[0m  [20/42], [94mLoss[0m : 1.54616
[1mStep[0m  [24/42], [94mLoss[0m : 1.59056
[1mStep[0m  [28/42], [94mLoss[0m : 1.70961
[1mStep[0m  [32/42], [94mLoss[0m : 1.45533
[1mStep[0m  [36/42], [94mLoss[0m : 1.56356
[1mStep[0m  [40/42], [94mLoss[0m : 1.65673

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68795
[1mStep[0m  [4/42], [94mLoss[0m : 1.48014
[1mStep[0m  [8/42], [94mLoss[0m : 1.34693
[1mStep[0m  [12/42], [94mLoss[0m : 1.73375
[1mStep[0m  [16/42], [94mLoss[0m : 1.49568
[1mStep[0m  [20/42], [94mLoss[0m : 1.62901
[1mStep[0m  [24/42], [94mLoss[0m : 1.46937
[1mStep[0m  [28/42], [94mLoss[0m : 1.64310
[1mStep[0m  [32/42], [94mLoss[0m : 1.50803
[1mStep[0m  [36/42], [94mLoss[0m : 1.71279
[1mStep[0m  [40/42], [94mLoss[0m : 1.63391

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.481, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52771
[1mStep[0m  [4/42], [94mLoss[0m : 1.43046
[1mStep[0m  [8/42], [94mLoss[0m : 1.52417
[1mStep[0m  [12/42], [94mLoss[0m : 1.50722
[1mStep[0m  [16/42], [94mLoss[0m : 1.52081
[1mStep[0m  [20/42], [94mLoss[0m : 1.43040
[1mStep[0m  [24/42], [94mLoss[0m : 1.62183
[1mStep[0m  [28/42], [94mLoss[0m : 1.53184
[1mStep[0m  [32/42], [94mLoss[0m : 1.50853
[1mStep[0m  [36/42], [94mLoss[0m : 1.57776
[1mStep[0m  [40/42], [94mLoss[0m : 1.45607

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.555, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55257
[1mStep[0m  [4/42], [94mLoss[0m : 1.59342
[1mStep[0m  [8/42], [94mLoss[0m : 1.49543
[1mStep[0m  [12/42], [94mLoss[0m : 1.54889
[1mStep[0m  [16/42], [94mLoss[0m : 1.42913
[1mStep[0m  [20/42], [94mLoss[0m : 1.58050
[1mStep[0m  [24/42], [94mLoss[0m : 1.45544
[1mStep[0m  [28/42], [94mLoss[0m : 1.51448
[1mStep[0m  [32/42], [94mLoss[0m : 1.62666
[1mStep[0m  [36/42], [94mLoss[0m : 1.49654
[1mStep[0m  [40/42], [94mLoss[0m : 1.42965

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48718
[1mStep[0m  [4/42], [94mLoss[0m : 1.48991
[1mStep[0m  [8/42], [94mLoss[0m : 1.57332
[1mStep[0m  [12/42], [94mLoss[0m : 1.44779
[1mStep[0m  [16/42], [94mLoss[0m : 1.51999
[1mStep[0m  [20/42], [94mLoss[0m : 1.53658
[1mStep[0m  [24/42], [94mLoss[0m : 1.61454
[1mStep[0m  [28/42], [94mLoss[0m : 1.74146
[1mStep[0m  [32/42], [94mLoss[0m : 1.62131
[1mStep[0m  [36/42], [94mLoss[0m : 1.67126
[1mStep[0m  [40/42], [94mLoss[0m : 1.58340

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.533, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45675
[1mStep[0m  [4/42], [94mLoss[0m : 1.53753
[1mStep[0m  [8/42], [94mLoss[0m : 1.29340
[1mStep[0m  [12/42], [94mLoss[0m : 1.72302
[1mStep[0m  [16/42], [94mLoss[0m : 1.68654
[1mStep[0m  [20/42], [94mLoss[0m : 1.49852
[1mStep[0m  [24/42], [94mLoss[0m : 1.49776
[1mStep[0m  [28/42], [94mLoss[0m : 1.46559
[1mStep[0m  [32/42], [94mLoss[0m : 1.43415
[1mStep[0m  [36/42], [94mLoss[0m : 1.35017
[1mStep[0m  [40/42], [94mLoss[0m : 1.51871

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.574, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41745
[1mStep[0m  [4/42], [94mLoss[0m : 1.46950
[1mStep[0m  [8/42], [94mLoss[0m : 1.51706
[1mStep[0m  [12/42], [94mLoss[0m : 1.39690
[1mStep[0m  [16/42], [94mLoss[0m : 1.39081
[1mStep[0m  [20/42], [94mLoss[0m : 1.45534
[1mStep[0m  [24/42], [94mLoss[0m : 1.50827
[1mStep[0m  [28/42], [94mLoss[0m : 1.52139
[1mStep[0m  [32/42], [94mLoss[0m : 1.48351
[1mStep[0m  [36/42], [94mLoss[0m : 1.49511
[1mStep[0m  [40/42], [94mLoss[0m : 1.39824

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43830
[1mStep[0m  [4/42], [94mLoss[0m : 1.47913
[1mStep[0m  [8/42], [94mLoss[0m : 1.41389
[1mStep[0m  [12/42], [94mLoss[0m : 1.44043
[1mStep[0m  [16/42], [94mLoss[0m : 1.56680
[1mStep[0m  [20/42], [94mLoss[0m : 1.51596
[1mStep[0m  [24/42], [94mLoss[0m : 1.46033
[1mStep[0m  [28/42], [94mLoss[0m : 1.70873
[1mStep[0m  [32/42], [94mLoss[0m : 1.55700
[1mStep[0m  [36/42], [94mLoss[0m : 1.46992
[1mStep[0m  [40/42], [94mLoss[0m : 1.35134

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.27443
[1mStep[0m  [4/42], [94mLoss[0m : 1.45294
[1mStep[0m  [8/42], [94mLoss[0m : 1.48028
[1mStep[0m  [12/42], [94mLoss[0m : 1.55163
[1mStep[0m  [16/42], [94mLoss[0m : 1.44867
[1mStep[0m  [20/42], [94mLoss[0m : 1.48204
[1mStep[0m  [24/42], [94mLoss[0m : 1.56398
[1mStep[0m  [28/42], [94mLoss[0m : 1.45173
[1mStep[0m  [32/42], [94mLoss[0m : 1.46467
[1mStep[0m  [36/42], [94mLoss[0m : 1.56508
[1mStep[0m  [40/42], [94mLoss[0m : 1.55288

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.30535
[1mStep[0m  [4/42], [94mLoss[0m : 1.36738
[1mStep[0m  [8/42], [94mLoss[0m : 1.37170
[1mStep[0m  [12/42], [94mLoss[0m : 1.51358
[1mStep[0m  [16/42], [94mLoss[0m : 1.51696
[1mStep[0m  [20/42], [94mLoss[0m : 1.43896
[1mStep[0m  [24/42], [94mLoss[0m : 1.50611
[1mStep[0m  [28/42], [94mLoss[0m : 1.41552
[1mStep[0m  [32/42], [94mLoss[0m : 1.45027
[1mStep[0m  [36/42], [94mLoss[0m : 1.40276
[1mStep[0m  [40/42], [94mLoss[0m : 1.36604

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.422, [92mTest[0m: 2.475, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.39145
[1mStep[0m  [4/42], [94mLoss[0m : 1.43761
[1mStep[0m  [8/42], [94mLoss[0m : 1.55165
[1mStep[0m  [12/42], [94mLoss[0m : 1.48507
[1mStep[0m  [16/42], [94mLoss[0m : 1.53143
[1mStep[0m  [20/42], [94mLoss[0m : 1.43325
[1mStep[0m  [24/42], [94mLoss[0m : 1.40293
[1mStep[0m  [28/42], [94mLoss[0m : 1.54506
[1mStep[0m  [32/42], [94mLoss[0m : 1.50283
[1mStep[0m  [36/42], [94mLoss[0m : 1.37688
[1mStep[0m  [40/42], [94mLoss[0m : 1.49881

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.457, [92mTest[0m: 2.503, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.470
====================================

Phase 2 - Evaluation MAE:  2.47043890612466
MAE score P1       2.330124
MAE score P2       2.470439
loss               1.421791
learning_rate      0.007525
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.24061
[1mStep[0m  [2/21], [94mLoss[0m : 10.77802
[1mStep[0m  [4/21], [94mLoss[0m : 10.74354
[1mStep[0m  [6/21], [94mLoss[0m : 10.73671
[1mStep[0m  [8/21], [94mLoss[0m : 10.24989
[1mStep[0m  [10/21], [94mLoss[0m : 10.43256
[1mStep[0m  [12/21], [94mLoss[0m : 10.11617
[1mStep[0m  [14/21], [94mLoss[0m : 9.84023
[1mStep[0m  [16/21], [94mLoss[0m : 9.65523
[1mStep[0m  [18/21], [94mLoss[0m : 9.06626
[1mStep[0m  [20/21], [94mLoss[0m : 9.12368

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.162, [92mTest[0m: 11.308, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.67654
[1mStep[0m  [2/21], [94mLoss[0m : 8.95300
[1mStep[0m  [4/21], [94mLoss[0m : 8.53384
[1mStep[0m  [6/21], [94mLoss[0m : 8.02587
[1mStep[0m  [8/21], [94mLoss[0m : 7.89862
[1mStep[0m  [10/21], [94mLoss[0m : 8.16705
[1mStep[0m  [12/21], [94mLoss[0m : 7.62706
[1mStep[0m  [14/21], [94mLoss[0m : 7.01970
[1mStep[0m  [16/21], [94mLoss[0m : 7.14206
[1mStep[0m  [18/21], [94mLoss[0m : 6.69819
[1mStep[0m  [20/21], [94mLoss[0m : 7.05231

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.789, [92mTest[0m: 8.935, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.59856
[1mStep[0m  [2/21], [94mLoss[0m : 6.15509
[1mStep[0m  [4/21], [94mLoss[0m : 6.34436
[1mStep[0m  [6/21], [94mLoss[0m : 5.81514
[1mStep[0m  [8/21], [94mLoss[0m : 5.82941
[1mStep[0m  [10/21], [94mLoss[0m : 5.44080
[1mStep[0m  [12/21], [94mLoss[0m : 5.10552
[1mStep[0m  [14/21], [94mLoss[0m : 4.95128
[1mStep[0m  [16/21], [94mLoss[0m : 4.88252
[1mStep[0m  [18/21], [94mLoss[0m : 4.37927
[1mStep[0m  [20/21], [94mLoss[0m : 4.41447

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.476, [92mTest[0m: 6.545, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82879
[1mStep[0m  [2/21], [94mLoss[0m : 4.38620
[1mStep[0m  [4/21], [94mLoss[0m : 4.08799
[1mStep[0m  [6/21], [94mLoss[0m : 4.06969
[1mStep[0m  [8/21], [94mLoss[0m : 3.78443
[1mStep[0m  [10/21], [94mLoss[0m : 3.66053
[1mStep[0m  [12/21], [94mLoss[0m : 3.87974
[1mStep[0m  [14/21], [94mLoss[0m : 3.69762
[1mStep[0m  [16/21], [94mLoss[0m : 3.45238
[1mStep[0m  [18/21], [94mLoss[0m : 3.46512
[1mStep[0m  [20/21], [94mLoss[0m : 3.48585

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.865, [92mTest[0m: 4.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.67125
[1mStep[0m  [2/21], [94mLoss[0m : 3.23046
[1mStep[0m  [4/21], [94mLoss[0m : 3.21928
[1mStep[0m  [6/21], [94mLoss[0m : 3.21282
[1mStep[0m  [8/21], [94mLoss[0m : 3.14673
[1mStep[0m  [10/21], [94mLoss[0m : 3.07552
[1mStep[0m  [12/21], [94mLoss[0m : 3.07879
[1mStep[0m  [14/21], [94mLoss[0m : 3.15936
[1mStep[0m  [16/21], [94mLoss[0m : 3.06957
[1mStep[0m  [18/21], [94mLoss[0m : 3.26994
[1mStep[0m  [20/21], [94mLoss[0m : 3.00439

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.153, [92mTest[0m: 3.293, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84462
[1mStep[0m  [2/21], [94mLoss[0m : 2.99531
[1mStep[0m  [4/21], [94mLoss[0m : 2.97888
[1mStep[0m  [6/21], [94mLoss[0m : 2.60314
[1mStep[0m  [8/21], [94mLoss[0m : 2.84279
[1mStep[0m  [10/21], [94mLoss[0m : 3.01202
[1mStep[0m  [12/21], [94mLoss[0m : 2.90103
[1mStep[0m  [14/21], [94mLoss[0m : 2.71637
[1mStep[0m  [16/21], [94mLoss[0m : 2.64824
[1mStep[0m  [18/21], [94mLoss[0m : 2.78299
[1mStep[0m  [20/21], [94mLoss[0m : 2.74739

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.831, [92mTest[0m: 2.765, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63664
[1mStep[0m  [2/21], [94mLoss[0m : 2.70074
[1mStep[0m  [4/21], [94mLoss[0m : 2.76545
[1mStep[0m  [6/21], [94mLoss[0m : 2.73103
[1mStep[0m  [8/21], [94mLoss[0m : 2.67310
[1mStep[0m  [10/21], [94mLoss[0m : 2.76258
[1mStep[0m  [12/21], [94mLoss[0m : 2.74506
[1mStep[0m  [14/21], [94mLoss[0m : 2.69635
[1mStep[0m  [16/21], [94mLoss[0m : 2.78456
[1mStep[0m  [18/21], [94mLoss[0m : 2.73342
[1mStep[0m  [20/21], [94mLoss[0m : 2.57154

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58473
[1mStep[0m  [2/21], [94mLoss[0m : 2.74118
[1mStep[0m  [4/21], [94mLoss[0m : 2.65467
[1mStep[0m  [6/21], [94mLoss[0m : 2.65600
[1mStep[0m  [8/21], [94mLoss[0m : 2.71667
[1mStep[0m  [10/21], [94mLoss[0m : 2.58995
[1mStep[0m  [12/21], [94mLoss[0m : 2.56418
[1mStep[0m  [14/21], [94mLoss[0m : 2.58530
[1mStep[0m  [16/21], [94mLoss[0m : 2.64678
[1mStep[0m  [18/21], [94mLoss[0m : 2.52184
[1mStep[0m  [20/21], [94mLoss[0m : 2.77188

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79169
[1mStep[0m  [2/21], [94mLoss[0m : 2.55245
[1mStep[0m  [4/21], [94mLoss[0m : 2.66086
[1mStep[0m  [6/21], [94mLoss[0m : 2.58713
[1mStep[0m  [8/21], [94mLoss[0m : 2.54965
[1mStep[0m  [10/21], [94mLoss[0m : 2.46702
[1mStep[0m  [12/21], [94mLoss[0m : 2.69097
[1mStep[0m  [14/21], [94mLoss[0m : 2.82891
[1mStep[0m  [16/21], [94mLoss[0m : 2.57980
[1mStep[0m  [18/21], [94mLoss[0m : 2.52156
[1mStep[0m  [20/21], [94mLoss[0m : 2.58943

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53934
[1mStep[0m  [2/21], [94mLoss[0m : 2.74201
[1mStep[0m  [4/21], [94mLoss[0m : 2.60983
[1mStep[0m  [6/21], [94mLoss[0m : 2.64617
[1mStep[0m  [8/21], [94mLoss[0m : 2.64410
[1mStep[0m  [10/21], [94mLoss[0m : 2.56552
[1mStep[0m  [12/21], [94mLoss[0m : 2.55198
[1mStep[0m  [14/21], [94mLoss[0m : 2.61733
[1mStep[0m  [16/21], [94mLoss[0m : 2.43974
[1mStep[0m  [18/21], [94mLoss[0m : 2.71371
[1mStep[0m  [20/21], [94mLoss[0m : 2.69666

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41886
[1mStep[0m  [2/21], [94mLoss[0m : 2.54491
[1mStep[0m  [4/21], [94mLoss[0m : 2.68770
[1mStep[0m  [6/21], [94mLoss[0m : 2.46697
[1mStep[0m  [8/21], [94mLoss[0m : 2.53550
[1mStep[0m  [10/21], [94mLoss[0m : 2.60309
[1mStep[0m  [12/21], [94mLoss[0m : 2.45427
[1mStep[0m  [14/21], [94mLoss[0m : 2.62284
[1mStep[0m  [16/21], [94mLoss[0m : 2.51885
[1mStep[0m  [18/21], [94mLoss[0m : 2.66533
[1mStep[0m  [20/21], [94mLoss[0m : 2.65283

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68414
[1mStep[0m  [2/21], [94mLoss[0m : 2.68613
[1mStep[0m  [4/21], [94mLoss[0m : 2.61900
[1mStep[0m  [6/21], [94mLoss[0m : 2.52761
[1mStep[0m  [8/21], [94mLoss[0m : 2.61667
[1mStep[0m  [10/21], [94mLoss[0m : 2.69953
[1mStep[0m  [12/21], [94mLoss[0m : 2.54397
[1mStep[0m  [14/21], [94mLoss[0m : 2.62011
[1mStep[0m  [16/21], [94mLoss[0m : 2.66969
[1mStep[0m  [18/21], [94mLoss[0m : 2.57990
[1mStep[0m  [20/21], [94mLoss[0m : 2.72334

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59356
[1mStep[0m  [2/21], [94mLoss[0m : 2.62258
[1mStep[0m  [4/21], [94mLoss[0m : 2.65400
[1mStep[0m  [6/21], [94mLoss[0m : 2.46522
[1mStep[0m  [8/21], [94mLoss[0m : 2.50129
[1mStep[0m  [10/21], [94mLoss[0m : 2.48620
[1mStep[0m  [12/21], [94mLoss[0m : 2.50604
[1mStep[0m  [14/21], [94mLoss[0m : 2.62898
[1mStep[0m  [16/21], [94mLoss[0m : 2.53867
[1mStep[0m  [18/21], [94mLoss[0m : 2.67794
[1mStep[0m  [20/21], [94mLoss[0m : 2.63979

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39469
[1mStep[0m  [2/21], [94mLoss[0m : 2.64469
[1mStep[0m  [4/21], [94mLoss[0m : 2.42669
[1mStep[0m  [6/21], [94mLoss[0m : 2.55888
[1mStep[0m  [8/21], [94mLoss[0m : 2.68390
[1mStep[0m  [10/21], [94mLoss[0m : 2.60217
[1mStep[0m  [12/21], [94mLoss[0m : 2.75094
[1mStep[0m  [14/21], [94mLoss[0m : 2.51215
[1mStep[0m  [16/21], [94mLoss[0m : 2.53188
[1mStep[0m  [18/21], [94mLoss[0m : 2.70771
[1mStep[0m  [20/21], [94mLoss[0m : 2.47321

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60234
[1mStep[0m  [2/21], [94mLoss[0m : 2.48046
[1mStep[0m  [4/21], [94mLoss[0m : 2.59789
[1mStep[0m  [6/21], [94mLoss[0m : 2.51375
[1mStep[0m  [8/21], [94mLoss[0m : 2.49552
[1mStep[0m  [10/21], [94mLoss[0m : 2.59070
[1mStep[0m  [12/21], [94mLoss[0m : 2.60329
[1mStep[0m  [14/21], [94mLoss[0m : 2.61957
[1mStep[0m  [16/21], [94mLoss[0m : 2.66289
[1mStep[0m  [18/21], [94mLoss[0m : 2.49051
[1mStep[0m  [20/21], [94mLoss[0m : 2.61502

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68572
[1mStep[0m  [2/21], [94mLoss[0m : 2.55957
[1mStep[0m  [4/21], [94mLoss[0m : 2.57251
[1mStep[0m  [6/21], [94mLoss[0m : 2.58723
[1mStep[0m  [8/21], [94mLoss[0m : 2.62456
[1mStep[0m  [10/21], [94mLoss[0m : 2.89416
[1mStep[0m  [12/21], [94mLoss[0m : 2.65663
[1mStep[0m  [14/21], [94mLoss[0m : 2.71460
[1mStep[0m  [16/21], [94mLoss[0m : 2.74512
[1mStep[0m  [18/21], [94mLoss[0m : 2.43531
[1mStep[0m  [20/21], [94mLoss[0m : 2.50513

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66620
[1mStep[0m  [2/21], [94mLoss[0m : 2.62073
[1mStep[0m  [4/21], [94mLoss[0m : 2.85071
[1mStep[0m  [6/21], [94mLoss[0m : 2.61086
[1mStep[0m  [8/21], [94mLoss[0m : 2.46366
[1mStep[0m  [10/21], [94mLoss[0m : 2.54297
[1mStep[0m  [12/21], [94mLoss[0m : 2.44655
[1mStep[0m  [14/21], [94mLoss[0m : 2.65650
[1mStep[0m  [16/21], [94mLoss[0m : 2.65155
[1mStep[0m  [18/21], [94mLoss[0m : 2.50523
[1mStep[0m  [20/21], [94mLoss[0m : 2.42613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53554
[1mStep[0m  [2/21], [94mLoss[0m : 2.48013
[1mStep[0m  [4/21], [94mLoss[0m : 2.52705
[1mStep[0m  [6/21], [94mLoss[0m : 2.69005
[1mStep[0m  [8/21], [94mLoss[0m : 2.66670
[1mStep[0m  [10/21], [94mLoss[0m : 2.68512
[1mStep[0m  [12/21], [94mLoss[0m : 2.57608
[1mStep[0m  [14/21], [94mLoss[0m : 2.56395
[1mStep[0m  [16/21], [94mLoss[0m : 2.66286
[1mStep[0m  [18/21], [94mLoss[0m : 2.58669
[1mStep[0m  [20/21], [94mLoss[0m : 2.69543

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69081
[1mStep[0m  [2/21], [94mLoss[0m : 2.61772
[1mStep[0m  [4/21], [94mLoss[0m : 2.50744
[1mStep[0m  [6/21], [94mLoss[0m : 2.78616
[1mStep[0m  [8/21], [94mLoss[0m : 2.61271
[1mStep[0m  [10/21], [94mLoss[0m : 2.63463
[1mStep[0m  [12/21], [94mLoss[0m : 2.44768
[1mStep[0m  [14/21], [94mLoss[0m : 2.68514
[1mStep[0m  [16/21], [94mLoss[0m : 2.52954
[1mStep[0m  [18/21], [94mLoss[0m : 2.49049
[1mStep[0m  [20/21], [94mLoss[0m : 2.52265

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49937
[1mStep[0m  [2/21], [94mLoss[0m : 2.65062
[1mStep[0m  [4/21], [94mLoss[0m : 2.61389
[1mStep[0m  [6/21], [94mLoss[0m : 2.67565
[1mStep[0m  [8/21], [94mLoss[0m : 2.44521
[1mStep[0m  [10/21], [94mLoss[0m : 2.60645
[1mStep[0m  [12/21], [94mLoss[0m : 2.48609
[1mStep[0m  [14/21], [94mLoss[0m : 2.48920
[1mStep[0m  [16/21], [94mLoss[0m : 2.64552
[1mStep[0m  [18/21], [94mLoss[0m : 2.47332
[1mStep[0m  [20/21], [94mLoss[0m : 2.75205

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.374, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68305
[1mStep[0m  [2/21], [94mLoss[0m : 2.44005
[1mStep[0m  [4/21], [94mLoss[0m : 2.51543
[1mStep[0m  [6/21], [94mLoss[0m : 2.78553
[1mStep[0m  [8/21], [94mLoss[0m : 2.51366
[1mStep[0m  [10/21], [94mLoss[0m : 2.54767
[1mStep[0m  [12/21], [94mLoss[0m : 2.64649
[1mStep[0m  [14/21], [94mLoss[0m : 2.63550
[1mStep[0m  [16/21], [94mLoss[0m : 2.58774
[1mStep[0m  [18/21], [94mLoss[0m : 2.52540
[1mStep[0m  [20/21], [94mLoss[0m : 2.51009

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.369, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74933
[1mStep[0m  [2/21], [94mLoss[0m : 2.39739
[1mStep[0m  [4/21], [94mLoss[0m : 2.60288
[1mStep[0m  [6/21], [94mLoss[0m : 2.62090
[1mStep[0m  [8/21], [94mLoss[0m : 2.64835
[1mStep[0m  [10/21], [94mLoss[0m : 2.43115
[1mStep[0m  [12/21], [94mLoss[0m : 2.61718
[1mStep[0m  [14/21], [94mLoss[0m : 2.57209
[1mStep[0m  [16/21], [94mLoss[0m : 2.50117
[1mStep[0m  [18/21], [94mLoss[0m : 2.47425
[1mStep[0m  [20/21], [94mLoss[0m : 2.62395

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45448
[1mStep[0m  [2/21], [94mLoss[0m : 2.51404
[1mStep[0m  [4/21], [94mLoss[0m : 2.65237
[1mStep[0m  [6/21], [94mLoss[0m : 2.65656
[1mStep[0m  [8/21], [94mLoss[0m : 2.72775
[1mStep[0m  [10/21], [94mLoss[0m : 2.56042
[1mStep[0m  [12/21], [94mLoss[0m : 2.57351
[1mStep[0m  [14/21], [94mLoss[0m : 2.41313
[1mStep[0m  [16/21], [94mLoss[0m : 2.63363
[1mStep[0m  [18/21], [94mLoss[0m : 2.46268
[1mStep[0m  [20/21], [94mLoss[0m : 2.48323

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56198
[1mStep[0m  [2/21], [94mLoss[0m : 2.56498
[1mStep[0m  [4/21], [94mLoss[0m : 2.64526
[1mStep[0m  [6/21], [94mLoss[0m : 2.52514
[1mStep[0m  [8/21], [94mLoss[0m : 2.71805
[1mStep[0m  [10/21], [94mLoss[0m : 2.65868
[1mStep[0m  [12/21], [94mLoss[0m : 2.52372
[1mStep[0m  [14/21], [94mLoss[0m : 2.57487
[1mStep[0m  [16/21], [94mLoss[0m : 2.62434
[1mStep[0m  [18/21], [94mLoss[0m : 2.51247
[1mStep[0m  [20/21], [94mLoss[0m : 2.50298

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56002
[1mStep[0m  [2/21], [94mLoss[0m : 2.66883
[1mStep[0m  [4/21], [94mLoss[0m : 2.46957
[1mStep[0m  [6/21], [94mLoss[0m : 2.51460
[1mStep[0m  [8/21], [94mLoss[0m : 2.49540
[1mStep[0m  [10/21], [94mLoss[0m : 2.55300
[1mStep[0m  [12/21], [94mLoss[0m : 2.55739
[1mStep[0m  [14/21], [94mLoss[0m : 2.70197
[1mStep[0m  [16/21], [94mLoss[0m : 2.45659
[1mStep[0m  [18/21], [94mLoss[0m : 2.52496
[1mStep[0m  [20/21], [94mLoss[0m : 2.48535

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46487
[1mStep[0m  [2/21], [94mLoss[0m : 2.40877
[1mStep[0m  [4/21], [94mLoss[0m : 2.57973
[1mStep[0m  [6/21], [94mLoss[0m : 2.61491
[1mStep[0m  [8/21], [94mLoss[0m : 2.63189
[1mStep[0m  [10/21], [94mLoss[0m : 2.55602
[1mStep[0m  [12/21], [94mLoss[0m : 2.49216
[1mStep[0m  [14/21], [94mLoss[0m : 2.42663
[1mStep[0m  [16/21], [94mLoss[0m : 2.54094
[1mStep[0m  [18/21], [94mLoss[0m : 2.62975
[1mStep[0m  [20/21], [94mLoss[0m : 2.60329

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60341
[1mStep[0m  [2/21], [94mLoss[0m : 2.52424
[1mStep[0m  [4/21], [94mLoss[0m : 2.58024
[1mStep[0m  [6/21], [94mLoss[0m : 2.58071
[1mStep[0m  [8/21], [94mLoss[0m : 2.69987
[1mStep[0m  [10/21], [94mLoss[0m : 2.51325
[1mStep[0m  [12/21], [94mLoss[0m : 2.46745
[1mStep[0m  [14/21], [94mLoss[0m : 2.49069
[1mStep[0m  [16/21], [94mLoss[0m : 2.49479
[1mStep[0m  [18/21], [94mLoss[0m : 2.56102
[1mStep[0m  [20/21], [94mLoss[0m : 2.58411

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46379
[1mStep[0m  [2/21], [94mLoss[0m : 2.68847
[1mStep[0m  [4/21], [94mLoss[0m : 2.52596
[1mStep[0m  [6/21], [94mLoss[0m : 2.51316
[1mStep[0m  [8/21], [94mLoss[0m : 2.71143
[1mStep[0m  [10/21], [94mLoss[0m : 2.71226
[1mStep[0m  [12/21], [94mLoss[0m : 2.53933
[1mStep[0m  [14/21], [94mLoss[0m : 2.63076
[1mStep[0m  [16/21], [94mLoss[0m : 2.49347
[1mStep[0m  [18/21], [94mLoss[0m : 2.53916
[1mStep[0m  [20/21], [94mLoss[0m : 2.78849

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47520
[1mStep[0m  [2/21], [94mLoss[0m : 2.56229
[1mStep[0m  [4/21], [94mLoss[0m : 2.53361
[1mStep[0m  [6/21], [94mLoss[0m : 2.69307
[1mStep[0m  [8/21], [94mLoss[0m : 2.49487
[1mStep[0m  [10/21], [94mLoss[0m : 2.54334
[1mStep[0m  [12/21], [94mLoss[0m : 2.71762
[1mStep[0m  [14/21], [94mLoss[0m : 2.62352
[1mStep[0m  [16/21], [94mLoss[0m : 2.41498
[1mStep[0m  [18/21], [94mLoss[0m : 2.58697
[1mStep[0m  [20/21], [94mLoss[0m : 2.62296

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49759
[1mStep[0m  [2/21], [94mLoss[0m : 2.54243
[1mStep[0m  [4/21], [94mLoss[0m : 2.53868
[1mStep[0m  [6/21], [94mLoss[0m : 2.52531
[1mStep[0m  [8/21], [94mLoss[0m : 2.62468
[1mStep[0m  [10/21], [94mLoss[0m : 2.52433
[1mStep[0m  [12/21], [94mLoss[0m : 2.61353
[1mStep[0m  [14/21], [94mLoss[0m : 2.58575
[1mStep[0m  [16/21], [94mLoss[0m : 2.52140
[1mStep[0m  [18/21], [94mLoss[0m : 2.66544
[1mStep[0m  [20/21], [94mLoss[0m : 2.53673

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.349, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.355
====================================

Phase 1 - Evaluation MAE:  2.355430909565517
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.58629
[1mStep[0m  [2/21], [94mLoss[0m : 2.51850
[1mStep[0m  [4/21], [94mLoss[0m : 2.44472
[1mStep[0m  [6/21], [94mLoss[0m : 2.54421
[1mStep[0m  [8/21], [94mLoss[0m : 2.58821
[1mStep[0m  [10/21], [94mLoss[0m : 2.53970
[1mStep[0m  [12/21], [94mLoss[0m : 2.68425
[1mStep[0m  [14/21], [94mLoss[0m : 2.56358
[1mStep[0m  [16/21], [94mLoss[0m : 2.64269
[1mStep[0m  [18/21], [94mLoss[0m : 2.52566
[1mStep[0m  [20/21], [94mLoss[0m : 2.60957

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72778
[1mStep[0m  [2/21], [94mLoss[0m : 2.67651
[1mStep[0m  [4/21], [94mLoss[0m : 2.45314
[1mStep[0m  [6/21], [94mLoss[0m : 2.52725
[1mStep[0m  [8/21], [94mLoss[0m : 2.50668
[1mStep[0m  [10/21], [94mLoss[0m : 2.62513
[1mStep[0m  [12/21], [94mLoss[0m : 2.68890
[1mStep[0m  [14/21], [94mLoss[0m : 2.49021
[1mStep[0m  [16/21], [94mLoss[0m : 2.58507
[1mStep[0m  [18/21], [94mLoss[0m : 2.57048
[1mStep[0m  [20/21], [94mLoss[0m : 2.56680

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61269
[1mStep[0m  [2/21], [94mLoss[0m : 2.65692
[1mStep[0m  [4/21], [94mLoss[0m : 2.54678
[1mStep[0m  [6/21], [94mLoss[0m : 2.37647
[1mStep[0m  [8/21], [94mLoss[0m : 2.68275
[1mStep[0m  [10/21], [94mLoss[0m : 2.65031
[1mStep[0m  [12/21], [94mLoss[0m : 2.55810
[1mStep[0m  [14/21], [94mLoss[0m : 2.60396
[1mStep[0m  [16/21], [94mLoss[0m : 2.41069
[1mStep[0m  [18/21], [94mLoss[0m : 2.44660
[1mStep[0m  [20/21], [94mLoss[0m : 2.53082

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64519
[1mStep[0m  [2/21], [94mLoss[0m : 2.51871
[1mStep[0m  [4/21], [94mLoss[0m : 2.59412
[1mStep[0m  [6/21], [94mLoss[0m : 2.50897
[1mStep[0m  [8/21], [94mLoss[0m : 2.64326
[1mStep[0m  [10/21], [94mLoss[0m : 2.48243
[1mStep[0m  [12/21], [94mLoss[0m : 2.70468
[1mStep[0m  [14/21], [94mLoss[0m : 2.53942
[1mStep[0m  [16/21], [94mLoss[0m : 2.44114
[1mStep[0m  [18/21], [94mLoss[0m : 2.42137
[1mStep[0m  [20/21], [94mLoss[0m : 2.43657

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56756
[1mStep[0m  [2/21], [94mLoss[0m : 2.45697
[1mStep[0m  [4/21], [94mLoss[0m : 2.60210
[1mStep[0m  [6/21], [94mLoss[0m : 2.49809
[1mStep[0m  [8/21], [94mLoss[0m : 2.59612
[1mStep[0m  [10/21], [94mLoss[0m : 2.50565
[1mStep[0m  [12/21], [94mLoss[0m : 2.63507
[1mStep[0m  [14/21], [94mLoss[0m : 2.56553
[1mStep[0m  [16/21], [94mLoss[0m : 2.50192
[1mStep[0m  [18/21], [94mLoss[0m : 2.72644
[1mStep[0m  [20/21], [94mLoss[0m : 2.60410

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60597
[1mStep[0m  [2/21], [94mLoss[0m : 2.36815
[1mStep[0m  [4/21], [94mLoss[0m : 2.55037
[1mStep[0m  [6/21], [94mLoss[0m : 2.56556
[1mStep[0m  [8/21], [94mLoss[0m : 2.61136
[1mStep[0m  [10/21], [94mLoss[0m : 2.56262
[1mStep[0m  [12/21], [94mLoss[0m : 2.52234
[1mStep[0m  [14/21], [94mLoss[0m : 2.54192
[1mStep[0m  [16/21], [94mLoss[0m : 2.55068
[1mStep[0m  [18/21], [94mLoss[0m : 2.54437
[1mStep[0m  [20/21], [94mLoss[0m : 2.70960

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36347
[1mStep[0m  [2/21], [94mLoss[0m : 2.62373
[1mStep[0m  [4/21], [94mLoss[0m : 2.64619
[1mStep[0m  [6/21], [94mLoss[0m : 2.48423
[1mStep[0m  [8/21], [94mLoss[0m : 2.44543
[1mStep[0m  [10/21], [94mLoss[0m : 2.71577
[1mStep[0m  [12/21], [94mLoss[0m : 2.42659
[1mStep[0m  [14/21], [94mLoss[0m : 2.47178
[1mStep[0m  [16/21], [94mLoss[0m : 2.50769
[1mStep[0m  [18/21], [94mLoss[0m : 2.54573
[1mStep[0m  [20/21], [94mLoss[0m : 2.49165

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53087
[1mStep[0m  [2/21], [94mLoss[0m : 2.32516
[1mStep[0m  [4/21], [94mLoss[0m : 2.60656
[1mStep[0m  [6/21], [94mLoss[0m : 2.52489
[1mStep[0m  [8/21], [94mLoss[0m : 2.51458
[1mStep[0m  [10/21], [94mLoss[0m : 2.40416
[1mStep[0m  [12/21], [94mLoss[0m : 2.54894
[1mStep[0m  [14/21], [94mLoss[0m : 2.56468
[1mStep[0m  [16/21], [94mLoss[0m : 2.40944
[1mStep[0m  [18/21], [94mLoss[0m : 2.43851
[1mStep[0m  [20/21], [94mLoss[0m : 2.50011

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47187
[1mStep[0m  [2/21], [94mLoss[0m : 2.45388
[1mStep[0m  [4/21], [94mLoss[0m : 2.45804
[1mStep[0m  [6/21], [94mLoss[0m : 2.45850
[1mStep[0m  [8/21], [94mLoss[0m : 2.55899
[1mStep[0m  [10/21], [94mLoss[0m : 2.64261
[1mStep[0m  [12/21], [94mLoss[0m : 2.56410
[1mStep[0m  [14/21], [94mLoss[0m : 2.52417
[1mStep[0m  [16/21], [94mLoss[0m : 2.56411
[1mStep[0m  [18/21], [94mLoss[0m : 2.54532
[1mStep[0m  [20/21], [94mLoss[0m : 2.39800

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49789
[1mStep[0m  [2/21], [94mLoss[0m : 2.62135
[1mStep[0m  [4/21], [94mLoss[0m : 2.47609
[1mStep[0m  [6/21], [94mLoss[0m : 2.47608
[1mStep[0m  [8/21], [94mLoss[0m : 2.41389
[1mStep[0m  [10/21], [94mLoss[0m : 2.45497
[1mStep[0m  [12/21], [94mLoss[0m : 2.47804
[1mStep[0m  [14/21], [94mLoss[0m : 2.46321
[1mStep[0m  [16/21], [94mLoss[0m : 2.58816
[1mStep[0m  [18/21], [94mLoss[0m : 2.54561
[1mStep[0m  [20/21], [94mLoss[0m : 2.40947

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44522
[1mStep[0m  [2/21], [94mLoss[0m : 2.59941
[1mStep[0m  [4/21], [94mLoss[0m : 2.51577
[1mStep[0m  [6/21], [94mLoss[0m : 2.29828
[1mStep[0m  [8/21], [94mLoss[0m : 2.50812
[1mStep[0m  [10/21], [94mLoss[0m : 2.55886
[1mStep[0m  [12/21], [94mLoss[0m : 2.45803
[1mStep[0m  [14/21], [94mLoss[0m : 2.50504
[1mStep[0m  [16/21], [94mLoss[0m : 2.55535
[1mStep[0m  [18/21], [94mLoss[0m : 2.63916
[1mStep[0m  [20/21], [94mLoss[0m : 2.38539

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.485, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36644
[1mStep[0m  [2/21], [94mLoss[0m : 2.41460
[1mStep[0m  [4/21], [94mLoss[0m : 2.51846
[1mStep[0m  [6/21], [94mLoss[0m : 2.30701
[1mStep[0m  [8/21], [94mLoss[0m : 2.43556
[1mStep[0m  [10/21], [94mLoss[0m : 2.46686
[1mStep[0m  [12/21], [94mLoss[0m : 2.50594
[1mStep[0m  [14/21], [94mLoss[0m : 2.45249
[1mStep[0m  [16/21], [94mLoss[0m : 2.40918
[1mStep[0m  [18/21], [94mLoss[0m : 2.54597
[1mStep[0m  [20/21], [94mLoss[0m : 2.57595

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43649
[1mStep[0m  [2/21], [94mLoss[0m : 2.48254
[1mStep[0m  [4/21], [94mLoss[0m : 2.56414
[1mStep[0m  [6/21], [94mLoss[0m : 2.41070
[1mStep[0m  [8/21], [94mLoss[0m : 2.26860
[1mStep[0m  [10/21], [94mLoss[0m : 2.50893
[1mStep[0m  [12/21], [94mLoss[0m : 2.54417
[1mStep[0m  [14/21], [94mLoss[0m : 2.45853
[1mStep[0m  [16/21], [94mLoss[0m : 2.43148
[1mStep[0m  [18/21], [94mLoss[0m : 2.52353
[1mStep[0m  [20/21], [94mLoss[0m : 2.56193

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40096
[1mStep[0m  [2/21], [94mLoss[0m : 2.39184
[1mStep[0m  [4/21], [94mLoss[0m : 2.39404
[1mStep[0m  [6/21], [94mLoss[0m : 2.32258
[1mStep[0m  [8/21], [94mLoss[0m : 2.46569
[1mStep[0m  [10/21], [94mLoss[0m : 2.44317
[1mStep[0m  [12/21], [94mLoss[0m : 2.36151
[1mStep[0m  [14/21], [94mLoss[0m : 2.49032
[1mStep[0m  [16/21], [94mLoss[0m : 2.19818
[1mStep[0m  [18/21], [94mLoss[0m : 2.44925
[1mStep[0m  [20/21], [94mLoss[0m : 2.40354

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33315
[1mStep[0m  [2/21], [94mLoss[0m : 2.40647
[1mStep[0m  [4/21], [94mLoss[0m : 2.45777
[1mStep[0m  [6/21], [94mLoss[0m : 2.37148
[1mStep[0m  [8/21], [94mLoss[0m : 2.35795
[1mStep[0m  [10/21], [94mLoss[0m : 2.46464
[1mStep[0m  [12/21], [94mLoss[0m : 2.37003
[1mStep[0m  [14/21], [94mLoss[0m : 2.42532
[1mStep[0m  [16/21], [94mLoss[0m : 2.47915
[1mStep[0m  [18/21], [94mLoss[0m : 2.50725
[1mStep[0m  [20/21], [94mLoss[0m : 2.47135

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56009
[1mStep[0m  [2/21], [94mLoss[0m : 2.45616
[1mStep[0m  [4/21], [94mLoss[0m : 2.48442
[1mStep[0m  [6/21], [94mLoss[0m : 2.44599
[1mStep[0m  [8/21], [94mLoss[0m : 2.47375
[1mStep[0m  [10/21], [94mLoss[0m : 2.37813
[1mStep[0m  [12/21], [94mLoss[0m : 2.33035
[1mStep[0m  [14/21], [94mLoss[0m : 2.38674
[1mStep[0m  [16/21], [94mLoss[0m : 2.51298
[1mStep[0m  [18/21], [94mLoss[0m : 2.50464
[1mStep[0m  [20/21], [94mLoss[0m : 2.42471

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59519
[1mStep[0m  [2/21], [94mLoss[0m : 2.23083
[1mStep[0m  [4/21], [94mLoss[0m : 2.41434
[1mStep[0m  [6/21], [94mLoss[0m : 2.43185
[1mStep[0m  [8/21], [94mLoss[0m : 2.38294
[1mStep[0m  [10/21], [94mLoss[0m : 2.39468
[1mStep[0m  [12/21], [94mLoss[0m : 2.39065
[1mStep[0m  [14/21], [94mLoss[0m : 2.41880
[1mStep[0m  [16/21], [94mLoss[0m : 2.36338
[1mStep[0m  [18/21], [94mLoss[0m : 2.43225
[1mStep[0m  [20/21], [94mLoss[0m : 2.44527

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.628, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44781
[1mStep[0m  [2/21], [94mLoss[0m : 2.35911
[1mStep[0m  [4/21], [94mLoss[0m : 2.37741
[1mStep[0m  [6/21], [94mLoss[0m : 2.41794
[1mStep[0m  [8/21], [94mLoss[0m : 2.53598
[1mStep[0m  [10/21], [94mLoss[0m : 2.39787
[1mStep[0m  [12/21], [94mLoss[0m : 2.37018
[1mStep[0m  [14/21], [94mLoss[0m : 2.41557
[1mStep[0m  [16/21], [94mLoss[0m : 2.27176
[1mStep[0m  [18/21], [94mLoss[0m : 2.38383
[1mStep[0m  [20/21], [94mLoss[0m : 2.33296

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.535, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35947
[1mStep[0m  [2/21], [94mLoss[0m : 2.32151
[1mStep[0m  [4/21], [94mLoss[0m : 2.35579
[1mStep[0m  [6/21], [94mLoss[0m : 2.28635
[1mStep[0m  [8/21], [94mLoss[0m : 2.42818
[1mStep[0m  [10/21], [94mLoss[0m : 2.49900
[1mStep[0m  [12/21], [94mLoss[0m : 2.53137
[1mStep[0m  [14/21], [94mLoss[0m : 2.46353
[1mStep[0m  [16/21], [94mLoss[0m : 2.37969
[1mStep[0m  [18/21], [94mLoss[0m : 2.34152
[1mStep[0m  [20/21], [94mLoss[0m : 2.57752

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.598, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48784
[1mStep[0m  [2/21], [94mLoss[0m : 2.36037
[1mStep[0m  [4/21], [94mLoss[0m : 2.28549
[1mStep[0m  [6/21], [94mLoss[0m : 2.22864
[1mStep[0m  [8/21], [94mLoss[0m : 2.33343
[1mStep[0m  [10/21], [94mLoss[0m : 2.32400
[1mStep[0m  [12/21], [94mLoss[0m : 2.35039
[1mStep[0m  [14/21], [94mLoss[0m : 2.43534
[1mStep[0m  [16/21], [94mLoss[0m : 2.52351
[1mStep[0m  [18/21], [94mLoss[0m : 2.40135
[1mStep[0m  [20/21], [94mLoss[0m : 2.38770

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27287
[1mStep[0m  [2/21], [94mLoss[0m : 2.42186
[1mStep[0m  [4/21], [94mLoss[0m : 2.43297
[1mStep[0m  [6/21], [94mLoss[0m : 2.30943
[1mStep[0m  [8/21], [94mLoss[0m : 2.46985
[1mStep[0m  [10/21], [94mLoss[0m : 2.36076
[1mStep[0m  [12/21], [94mLoss[0m : 2.34232
[1mStep[0m  [14/21], [94mLoss[0m : 2.39697
[1mStep[0m  [16/21], [94mLoss[0m : 2.28351
[1mStep[0m  [18/21], [94mLoss[0m : 2.45634
[1mStep[0m  [20/21], [94mLoss[0m : 2.30212

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.628, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40905
[1mStep[0m  [2/21], [94mLoss[0m : 2.40070
[1mStep[0m  [4/21], [94mLoss[0m : 2.29987
[1mStep[0m  [6/21], [94mLoss[0m : 2.40978
[1mStep[0m  [8/21], [94mLoss[0m : 2.23886
[1mStep[0m  [10/21], [94mLoss[0m : 2.53035
[1mStep[0m  [12/21], [94mLoss[0m : 2.21922
[1mStep[0m  [14/21], [94mLoss[0m : 2.43464
[1mStep[0m  [16/21], [94mLoss[0m : 2.35985
[1mStep[0m  [18/21], [94mLoss[0m : 2.42041
[1mStep[0m  [20/21], [94mLoss[0m : 2.40541

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.562, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42011
[1mStep[0m  [2/21], [94mLoss[0m : 2.39034
[1mStep[0m  [4/21], [94mLoss[0m : 2.38886
[1mStep[0m  [6/21], [94mLoss[0m : 2.42719
[1mStep[0m  [8/21], [94mLoss[0m : 2.42379
[1mStep[0m  [10/21], [94mLoss[0m : 2.41544
[1mStep[0m  [12/21], [94mLoss[0m : 2.43566
[1mStep[0m  [14/21], [94mLoss[0m : 2.20413
[1mStep[0m  [16/21], [94mLoss[0m : 2.22447
[1mStep[0m  [18/21], [94mLoss[0m : 2.34107
[1mStep[0m  [20/21], [94mLoss[0m : 2.40521

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.646, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40131
[1mStep[0m  [2/21], [94mLoss[0m : 2.33328
[1mStep[0m  [4/21], [94mLoss[0m : 2.33704
[1mStep[0m  [6/21], [94mLoss[0m : 2.23101
[1mStep[0m  [8/21], [94mLoss[0m : 2.28997
[1mStep[0m  [10/21], [94mLoss[0m : 2.57767
[1mStep[0m  [12/21], [94mLoss[0m : 2.39172
[1mStep[0m  [14/21], [94mLoss[0m : 2.29425
[1mStep[0m  [16/21], [94mLoss[0m : 2.17629
[1mStep[0m  [18/21], [94mLoss[0m : 2.39995
[1mStep[0m  [20/21], [94mLoss[0m : 2.28597

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.601, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31097
[1mStep[0m  [2/21], [94mLoss[0m : 2.26311
[1mStep[0m  [4/21], [94mLoss[0m : 2.14498
[1mStep[0m  [6/21], [94mLoss[0m : 2.20193
[1mStep[0m  [8/21], [94mLoss[0m : 2.30689
[1mStep[0m  [10/21], [94mLoss[0m : 2.28385
[1mStep[0m  [12/21], [94mLoss[0m : 2.34219
[1mStep[0m  [14/21], [94mLoss[0m : 2.31153
[1mStep[0m  [16/21], [94mLoss[0m : 2.43568
[1mStep[0m  [18/21], [94mLoss[0m : 2.30488
[1mStep[0m  [20/21], [94mLoss[0m : 2.31971

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.550, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28203
[1mStep[0m  [2/21], [94mLoss[0m : 2.28400
[1mStep[0m  [4/21], [94mLoss[0m : 2.38218
[1mStep[0m  [6/21], [94mLoss[0m : 2.21217
[1mStep[0m  [8/21], [94mLoss[0m : 2.30400
[1mStep[0m  [10/21], [94mLoss[0m : 2.28780
[1mStep[0m  [12/21], [94mLoss[0m : 2.32479
[1mStep[0m  [14/21], [94mLoss[0m : 2.30345
[1mStep[0m  [16/21], [94mLoss[0m : 2.28408
[1mStep[0m  [18/21], [94mLoss[0m : 2.44904
[1mStep[0m  [20/21], [94mLoss[0m : 2.26556

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37946
[1mStep[0m  [2/21], [94mLoss[0m : 2.23339
[1mStep[0m  [4/21], [94mLoss[0m : 2.22832
[1mStep[0m  [6/21], [94mLoss[0m : 2.26149
[1mStep[0m  [8/21], [94mLoss[0m : 2.38974
[1mStep[0m  [10/21], [94mLoss[0m : 2.26551
[1mStep[0m  [12/21], [94mLoss[0m : 2.24300
[1mStep[0m  [14/21], [94mLoss[0m : 2.25571
[1mStep[0m  [16/21], [94mLoss[0m : 2.49151
[1mStep[0m  [18/21], [94mLoss[0m : 2.17667
[1mStep[0m  [20/21], [94mLoss[0m : 2.26376

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.571, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.19113
[1mStep[0m  [2/21], [94mLoss[0m : 2.37182
[1mStep[0m  [4/21], [94mLoss[0m : 2.19688
[1mStep[0m  [6/21], [94mLoss[0m : 2.34223
[1mStep[0m  [8/21], [94mLoss[0m : 2.43301
[1mStep[0m  [10/21], [94mLoss[0m : 2.24565
[1mStep[0m  [12/21], [94mLoss[0m : 2.29217
[1mStep[0m  [14/21], [94mLoss[0m : 2.20976
[1mStep[0m  [16/21], [94mLoss[0m : 2.26839
[1mStep[0m  [18/21], [94mLoss[0m : 2.18450
[1mStep[0m  [20/21], [94mLoss[0m : 2.41732

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.587, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32001
[1mStep[0m  [2/21], [94mLoss[0m : 2.18902
[1mStep[0m  [4/21], [94mLoss[0m : 2.28647
[1mStep[0m  [6/21], [94mLoss[0m : 2.30938
[1mStep[0m  [8/21], [94mLoss[0m : 2.29184
[1mStep[0m  [10/21], [94mLoss[0m : 2.15881
[1mStep[0m  [12/21], [94mLoss[0m : 2.22651
[1mStep[0m  [14/21], [94mLoss[0m : 2.24781
[1mStep[0m  [16/21], [94mLoss[0m : 2.29988
[1mStep[0m  [18/21], [94mLoss[0m : 2.16242
[1mStep[0m  [20/21], [94mLoss[0m : 2.11626

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.687, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22237
[1mStep[0m  [2/21], [94mLoss[0m : 2.09358
[1mStep[0m  [4/21], [94mLoss[0m : 2.19217
[1mStep[0m  [6/21], [94mLoss[0m : 2.22872
[1mStep[0m  [8/21], [94mLoss[0m : 2.34104
[1mStep[0m  [10/21], [94mLoss[0m : 2.29219
[1mStep[0m  [12/21], [94mLoss[0m : 2.27765
[1mStep[0m  [14/21], [94mLoss[0m : 2.29936
[1mStep[0m  [16/21], [94mLoss[0m : 2.06636
[1mStep[0m  [18/21], [94mLoss[0m : 2.43213
[1mStep[0m  [20/21], [94mLoss[0m : 2.17885

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.576, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.587
====================================

Phase 2 - Evaluation MAE:  2.5868620531899587
MAE score P1       2.355431
MAE score P2       2.586862
loss               2.242564
learning_rate      0.007525
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.36491
[1mStep[0m  [2/21], [94mLoss[0m : 11.10288
[1mStep[0m  [4/21], [94mLoss[0m : 10.39382
[1mStep[0m  [6/21], [94mLoss[0m : 9.40369
[1mStep[0m  [8/21], [94mLoss[0m : 7.74738
[1mStep[0m  [10/21], [94mLoss[0m : 6.69156
[1mStep[0m  [12/21], [94mLoss[0m : 5.38370
[1mStep[0m  [14/21], [94mLoss[0m : 3.73862
[1mStep[0m  [16/21], [94mLoss[0m : 3.01990
[1mStep[0m  [18/21], [94mLoss[0m : 2.95819
[1mStep[0m  [20/21], [94mLoss[0m : 3.00538

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.834, [92mTest[0m: 11.014, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.29083
[1mStep[0m  [2/21], [94mLoss[0m : 3.52498
[1mStep[0m  [4/21], [94mLoss[0m : 3.65151
[1mStep[0m  [6/21], [94mLoss[0m : 2.99645
[1mStep[0m  [8/21], [94mLoss[0m : 2.92060
[1mStep[0m  [10/21], [94mLoss[0m : 2.49583
[1mStep[0m  [12/21], [94mLoss[0m : 2.57086
[1mStep[0m  [14/21], [94mLoss[0m : 2.61798
[1mStep[0m  [16/21], [94mLoss[0m : 2.76537
[1mStep[0m  [18/21], [94mLoss[0m : 2.75660
[1mStep[0m  [20/21], [94mLoss[0m : 2.73735

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.964, [92mTest[0m: 6.079, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77506
[1mStep[0m  [2/21], [94mLoss[0m : 2.57625
[1mStep[0m  [4/21], [94mLoss[0m : 2.67247
[1mStep[0m  [6/21], [94mLoss[0m : 2.41654
[1mStep[0m  [8/21], [94mLoss[0m : 2.51867
[1mStep[0m  [10/21], [94mLoss[0m : 2.62139
[1mStep[0m  [12/21], [94mLoss[0m : 2.68914
[1mStep[0m  [14/21], [94mLoss[0m : 2.55798
[1mStep[0m  [16/21], [94mLoss[0m : 2.65863
[1mStep[0m  [18/21], [94mLoss[0m : 2.61377
[1mStep[0m  [20/21], [94mLoss[0m : 2.44064

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.600, [92mTest[0m: 3.021, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56786
[1mStep[0m  [2/21], [94mLoss[0m : 2.49354
[1mStep[0m  [4/21], [94mLoss[0m : 2.39548
[1mStep[0m  [6/21], [94mLoss[0m : 2.66676
[1mStep[0m  [8/21], [94mLoss[0m : 2.59488
[1mStep[0m  [10/21], [94mLoss[0m : 2.49597
[1mStep[0m  [12/21], [94mLoss[0m : 2.50259
[1mStep[0m  [14/21], [94mLoss[0m : 2.51754
[1mStep[0m  [16/21], [94mLoss[0m : 2.60565
[1mStep[0m  [18/21], [94mLoss[0m : 2.56171
[1mStep[0m  [20/21], [94mLoss[0m : 2.62557

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39580
[1mStep[0m  [2/21], [94mLoss[0m : 2.47268
[1mStep[0m  [4/21], [94mLoss[0m : 2.49870
[1mStep[0m  [6/21], [94mLoss[0m : 2.43087
[1mStep[0m  [8/21], [94mLoss[0m : 2.62803
[1mStep[0m  [10/21], [94mLoss[0m : 2.48974
[1mStep[0m  [12/21], [94mLoss[0m : 2.52947
[1mStep[0m  [14/21], [94mLoss[0m : 2.50006
[1mStep[0m  [16/21], [94mLoss[0m : 2.53375
[1mStep[0m  [18/21], [94mLoss[0m : 2.60410
[1mStep[0m  [20/21], [94mLoss[0m : 2.52852

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30053
[1mStep[0m  [2/21], [94mLoss[0m : 2.48679
[1mStep[0m  [4/21], [94mLoss[0m : 2.67380
[1mStep[0m  [6/21], [94mLoss[0m : 2.54861
[1mStep[0m  [8/21], [94mLoss[0m : 2.43176
[1mStep[0m  [10/21], [94mLoss[0m : 2.50052
[1mStep[0m  [12/21], [94mLoss[0m : 2.48436
[1mStep[0m  [14/21], [94mLoss[0m : 2.50485
[1mStep[0m  [16/21], [94mLoss[0m : 2.53293
[1mStep[0m  [18/21], [94mLoss[0m : 2.46309
[1mStep[0m  [20/21], [94mLoss[0m : 2.38718

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54816
[1mStep[0m  [2/21], [94mLoss[0m : 2.52375
[1mStep[0m  [4/21], [94mLoss[0m : 2.46061
[1mStep[0m  [6/21], [94mLoss[0m : 2.48599
[1mStep[0m  [8/21], [94mLoss[0m : 2.46473
[1mStep[0m  [10/21], [94mLoss[0m : 2.39562
[1mStep[0m  [12/21], [94mLoss[0m : 2.36121
[1mStep[0m  [14/21], [94mLoss[0m : 2.47074
[1mStep[0m  [16/21], [94mLoss[0m : 2.42302
[1mStep[0m  [18/21], [94mLoss[0m : 2.43374
[1mStep[0m  [20/21], [94mLoss[0m : 2.44088

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42229
[1mStep[0m  [2/21], [94mLoss[0m : 2.42923
[1mStep[0m  [4/21], [94mLoss[0m : 2.39840
[1mStep[0m  [6/21], [94mLoss[0m : 2.40976
[1mStep[0m  [8/21], [94mLoss[0m : 2.67056
[1mStep[0m  [10/21], [94mLoss[0m : 2.58629
[1mStep[0m  [12/21], [94mLoss[0m : 2.42010
[1mStep[0m  [14/21], [94mLoss[0m : 2.38635
[1mStep[0m  [16/21], [94mLoss[0m : 2.53050
[1mStep[0m  [18/21], [94mLoss[0m : 2.32328
[1mStep[0m  [20/21], [94mLoss[0m : 2.33619

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45827
[1mStep[0m  [2/21], [94mLoss[0m : 2.32876
[1mStep[0m  [4/21], [94mLoss[0m : 2.39383
[1mStep[0m  [6/21], [94mLoss[0m : 2.54251
[1mStep[0m  [8/21], [94mLoss[0m : 2.31109
[1mStep[0m  [10/21], [94mLoss[0m : 2.36667
[1mStep[0m  [12/21], [94mLoss[0m : 2.42468
[1mStep[0m  [14/21], [94mLoss[0m : 2.36848
[1mStep[0m  [16/21], [94mLoss[0m : 2.28571
[1mStep[0m  [18/21], [94mLoss[0m : 2.48841
[1mStep[0m  [20/21], [94mLoss[0m : 2.41358

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35484
[1mStep[0m  [2/21], [94mLoss[0m : 2.47511
[1mStep[0m  [4/21], [94mLoss[0m : 2.41708
[1mStep[0m  [6/21], [94mLoss[0m : 2.33138
[1mStep[0m  [8/21], [94mLoss[0m : 2.40043
[1mStep[0m  [10/21], [94mLoss[0m : 2.34572
[1mStep[0m  [12/21], [94mLoss[0m : 2.34585
[1mStep[0m  [14/21], [94mLoss[0m : 2.41225
[1mStep[0m  [16/21], [94mLoss[0m : 2.42312
[1mStep[0m  [18/21], [94mLoss[0m : 2.32505
[1mStep[0m  [20/21], [94mLoss[0m : 2.42668

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42001
[1mStep[0m  [2/21], [94mLoss[0m : 2.44794
[1mStep[0m  [4/21], [94mLoss[0m : 2.34579
[1mStep[0m  [6/21], [94mLoss[0m : 2.40180
[1mStep[0m  [8/21], [94mLoss[0m : 2.47631
[1mStep[0m  [10/21], [94mLoss[0m : 2.37363
[1mStep[0m  [12/21], [94mLoss[0m : 2.30796
[1mStep[0m  [14/21], [94mLoss[0m : 2.39858
[1mStep[0m  [16/21], [94mLoss[0m : 2.37169
[1mStep[0m  [18/21], [94mLoss[0m : 2.39121
[1mStep[0m  [20/21], [94mLoss[0m : 2.34718

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50995
[1mStep[0m  [2/21], [94mLoss[0m : 2.29199
[1mStep[0m  [4/21], [94mLoss[0m : 2.38945
[1mStep[0m  [6/21], [94mLoss[0m : 2.50676
[1mStep[0m  [8/21], [94mLoss[0m : 2.32138
[1mStep[0m  [10/21], [94mLoss[0m : 2.40164
[1mStep[0m  [12/21], [94mLoss[0m : 2.35976
[1mStep[0m  [14/21], [94mLoss[0m : 2.40167
[1mStep[0m  [16/21], [94mLoss[0m : 2.39650
[1mStep[0m  [18/21], [94mLoss[0m : 2.34250
[1mStep[0m  [20/21], [94mLoss[0m : 2.37947

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34750
[1mStep[0m  [2/21], [94mLoss[0m : 2.31115
[1mStep[0m  [4/21], [94mLoss[0m : 2.28629
[1mStep[0m  [6/21], [94mLoss[0m : 2.50917
[1mStep[0m  [8/21], [94mLoss[0m : 2.46722
[1mStep[0m  [10/21], [94mLoss[0m : 2.32908
[1mStep[0m  [12/21], [94mLoss[0m : 2.28701
[1mStep[0m  [14/21], [94mLoss[0m : 2.52090
[1mStep[0m  [16/21], [94mLoss[0m : 2.54642
[1mStep[0m  [18/21], [94mLoss[0m : 2.44663
[1mStep[0m  [20/21], [94mLoss[0m : 2.37937

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23308
[1mStep[0m  [2/21], [94mLoss[0m : 2.30694
[1mStep[0m  [4/21], [94mLoss[0m : 2.46244
[1mStep[0m  [6/21], [94mLoss[0m : 2.44180
[1mStep[0m  [8/21], [94mLoss[0m : 2.32081
[1mStep[0m  [10/21], [94mLoss[0m : 2.33076
[1mStep[0m  [12/21], [94mLoss[0m : 2.33723
[1mStep[0m  [14/21], [94mLoss[0m : 2.37950
[1mStep[0m  [16/21], [94mLoss[0m : 2.38128
[1mStep[0m  [18/21], [94mLoss[0m : 2.26644
[1mStep[0m  [20/21], [94mLoss[0m : 2.44302

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35835
[1mStep[0m  [2/21], [94mLoss[0m : 2.36718
[1mStep[0m  [4/21], [94mLoss[0m : 2.38709
[1mStep[0m  [6/21], [94mLoss[0m : 2.33530
[1mStep[0m  [8/21], [94mLoss[0m : 2.43090
[1mStep[0m  [10/21], [94mLoss[0m : 2.35878
[1mStep[0m  [12/21], [94mLoss[0m : 2.30445
[1mStep[0m  [14/21], [94mLoss[0m : 2.41500
[1mStep[0m  [16/21], [94mLoss[0m : 2.38470
[1mStep[0m  [18/21], [94mLoss[0m : 2.36726
[1mStep[0m  [20/21], [94mLoss[0m : 2.39505

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39332
[1mStep[0m  [2/21], [94mLoss[0m : 2.31679
[1mStep[0m  [4/21], [94mLoss[0m : 2.37232
[1mStep[0m  [6/21], [94mLoss[0m : 2.40748
[1mStep[0m  [8/21], [94mLoss[0m : 2.34294
[1mStep[0m  [10/21], [94mLoss[0m : 2.33548
[1mStep[0m  [12/21], [94mLoss[0m : 2.24168
[1mStep[0m  [14/21], [94mLoss[0m : 2.44310
[1mStep[0m  [16/21], [94mLoss[0m : 2.37452
[1mStep[0m  [18/21], [94mLoss[0m : 2.37227
[1mStep[0m  [20/21], [94mLoss[0m : 2.44611

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24207
[1mStep[0m  [2/21], [94mLoss[0m : 2.23884
[1mStep[0m  [4/21], [94mLoss[0m : 2.37881
[1mStep[0m  [6/21], [94mLoss[0m : 2.25245
[1mStep[0m  [8/21], [94mLoss[0m : 2.23803
[1mStep[0m  [10/21], [94mLoss[0m : 2.39005
[1mStep[0m  [12/21], [94mLoss[0m : 2.42238
[1mStep[0m  [14/21], [94mLoss[0m : 2.42633
[1mStep[0m  [16/21], [94mLoss[0m : 2.25785
[1mStep[0m  [18/21], [94mLoss[0m : 2.39810
[1mStep[0m  [20/21], [94mLoss[0m : 2.34009

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27578
[1mStep[0m  [2/21], [94mLoss[0m : 2.43070
[1mStep[0m  [4/21], [94mLoss[0m : 2.48496
[1mStep[0m  [6/21], [94mLoss[0m : 2.47260
[1mStep[0m  [8/21], [94mLoss[0m : 2.28264
[1mStep[0m  [10/21], [94mLoss[0m : 2.32727
[1mStep[0m  [12/21], [94mLoss[0m : 2.22656
[1mStep[0m  [14/21], [94mLoss[0m : 2.37923
[1mStep[0m  [16/21], [94mLoss[0m : 2.40025
[1mStep[0m  [18/21], [94mLoss[0m : 2.40489
[1mStep[0m  [20/21], [94mLoss[0m : 2.25777

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34610
[1mStep[0m  [2/21], [94mLoss[0m : 2.21262
[1mStep[0m  [4/21], [94mLoss[0m : 2.33878
[1mStep[0m  [6/21], [94mLoss[0m : 2.32262
[1mStep[0m  [8/21], [94mLoss[0m : 2.38379
[1mStep[0m  [10/21], [94mLoss[0m : 2.30441
[1mStep[0m  [12/21], [94mLoss[0m : 2.28307
[1mStep[0m  [14/21], [94mLoss[0m : 2.13518
[1mStep[0m  [16/21], [94mLoss[0m : 2.28794
[1mStep[0m  [18/21], [94mLoss[0m : 2.52537
[1mStep[0m  [20/21], [94mLoss[0m : 2.14995

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48107
[1mStep[0m  [2/21], [94mLoss[0m : 2.26047
[1mStep[0m  [4/21], [94mLoss[0m : 2.44969
[1mStep[0m  [6/21], [94mLoss[0m : 2.29455
[1mStep[0m  [8/21], [94mLoss[0m : 2.26133
[1mStep[0m  [10/21], [94mLoss[0m : 2.20991
[1mStep[0m  [12/21], [94mLoss[0m : 2.37301
[1mStep[0m  [14/21], [94mLoss[0m : 2.26761
[1mStep[0m  [16/21], [94mLoss[0m : 2.32262
[1mStep[0m  [18/21], [94mLoss[0m : 2.43306
[1mStep[0m  [20/21], [94mLoss[0m : 2.11744

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34061
[1mStep[0m  [2/21], [94mLoss[0m : 2.30760
[1mStep[0m  [4/21], [94mLoss[0m : 2.55988
[1mStep[0m  [6/21], [94mLoss[0m : 2.39870
[1mStep[0m  [8/21], [94mLoss[0m : 2.22209
[1mStep[0m  [10/21], [94mLoss[0m : 2.28246
[1mStep[0m  [12/21], [94mLoss[0m : 2.27815
[1mStep[0m  [14/21], [94mLoss[0m : 2.35664
[1mStep[0m  [16/21], [94mLoss[0m : 2.16140
[1mStep[0m  [18/21], [94mLoss[0m : 2.20712
[1mStep[0m  [20/21], [94mLoss[0m : 2.34181

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23526
[1mStep[0m  [2/21], [94mLoss[0m : 2.24167
[1mStep[0m  [4/21], [94mLoss[0m : 2.25882
[1mStep[0m  [6/21], [94mLoss[0m : 2.28598
[1mStep[0m  [8/21], [94mLoss[0m : 2.18702
[1mStep[0m  [10/21], [94mLoss[0m : 2.30817
[1mStep[0m  [12/21], [94mLoss[0m : 2.39687
[1mStep[0m  [14/21], [94mLoss[0m : 2.26218
[1mStep[0m  [16/21], [94mLoss[0m : 2.40251
[1mStep[0m  [18/21], [94mLoss[0m : 2.24613
[1mStep[0m  [20/21], [94mLoss[0m : 2.33675

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43474
[1mStep[0m  [2/21], [94mLoss[0m : 2.41181
[1mStep[0m  [4/21], [94mLoss[0m : 2.49598
[1mStep[0m  [6/21], [94mLoss[0m : 2.09500
[1mStep[0m  [8/21], [94mLoss[0m : 2.30735
[1mStep[0m  [10/21], [94mLoss[0m : 2.36833
[1mStep[0m  [12/21], [94mLoss[0m : 2.35272
[1mStep[0m  [14/21], [94mLoss[0m : 2.22270
[1mStep[0m  [16/21], [94mLoss[0m : 2.34308
[1mStep[0m  [18/21], [94mLoss[0m : 2.23475
[1mStep[0m  [20/21], [94mLoss[0m : 2.20783

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27785
[1mStep[0m  [2/21], [94mLoss[0m : 2.26241
[1mStep[0m  [4/21], [94mLoss[0m : 2.30187
[1mStep[0m  [6/21], [94mLoss[0m : 2.34682
[1mStep[0m  [8/21], [94mLoss[0m : 2.26630
[1mStep[0m  [10/21], [94mLoss[0m : 2.27304
[1mStep[0m  [12/21], [94mLoss[0m : 2.34007
[1mStep[0m  [14/21], [94mLoss[0m : 2.33268
[1mStep[0m  [16/21], [94mLoss[0m : 2.27788
[1mStep[0m  [18/21], [94mLoss[0m : 2.28308
[1mStep[0m  [20/21], [94mLoss[0m : 2.26940

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.313, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34423
[1mStep[0m  [2/21], [94mLoss[0m : 2.21807
[1mStep[0m  [4/21], [94mLoss[0m : 2.23768
[1mStep[0m  [6/21], [94mLoss[0m : 2.25451
[1mStep[0m  [8/21], [94mLoss[0m : 2.31603
[1mStep[0m  [10/21], [94mLoss[0m : 2.22314
[1mStep[0m  [12/21], [94mLoss[0m : 2.23113
[1mStep[0m  [14/21], [94mLoss[0m : 2.06885
[1mStep[0m  [16/21], [94mLoss[0m : 2.28730
[1mStep[0m  [18/21], [94mLoss[0m : 2.24539
[1mStep[0m  [20/21], [94mLoss[0m : 2.30157

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35656
[1mStep[0m  [2/21], [94mLoss[0m : 2.41510
[1mStep[0m  [4/21], [94mLoss[0m : 2.23966
[1mStep[0m  [6/21], [94mLoss[0m : 2.40137
[1mStep[0m  [8/21], [94mLoss[0m : 2.27307
[1mStep[0m  [10/21], [94mLoss[0m : 2.30029
[1mStep[0m  [12/21], [94mLoss[0m : 2.27076
[1mStep[0m  [14/21], [94mLoss[0m : 2.46589
[1mStep[0m  [16/21], [94mLoss[0m : 2.25732
[1mStep[0m  [18/21], [94mLoss[0m : 2.22215
[1mStep[0m  [20/21], [94mLoss[0m : 2.33892

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.312, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27381
[1mStep[0m  [2/21], [94mLoss[0m : 2.22479
[1mStep[0m  [4/21], [94mLoss[0m : 2.33640
[1mStep[0m  [6/21], [94mLoss[0m : 2.19764
[1mStep[0m  [8/21], [94mLoss[0m : 2.28181
[1mStep[0m  [10/21], [94mLoss[0m : 2.33328
[1mStep[0m  [12/21], [94mLoss[0m : 2.29390
[1mStep[0m  [14/21], [94mLoss[0m : 2.15831
[1mStep[0m  [16/21], [94mLoss[0m : 2.34893
[1mStep[0m  [18/21], [94mLoss[0m : 2.11857
[1mStep[0m  [20/21], [94mLoss[0m : 2.21806

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24351
[1mStep[0m  [2/21], [94mLoss[0m : 2.37897
[1mStep[0m  [4/21], [94mLoss[0m : 2.50133
[1mStep[0m  [6/21], [94mLoss[0m : 2.22079
[1mStep[0m  [8/21], [94mLoss[0m : 2.15067
[1mStep[0m  [10/21], [94mLoss[0m : 2.18962
[1mStep[0m  [12/21], [94mLoss[0m : 2.22222
[1mStep[0m  [14/21], [94mLoss[0m : 2.24707
[1mStep[0m  [16/21], [94mLoss[0m : 2.25692
[1mStep[0m  [18/21], [94mLoss[0m : 2.19257
[1mStep[0m  [20/21], [94mLoss[0m : 2.19352

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23600
[1mStep[0m  [2/21], [94mLoss[0m : 2.20100
[1mStep[0m  [4/21], [94mLoss[0m : 2.15125
[1mStep[0m  [6/21], [94mLoss[0m : 2.14787
[1mStep[0m  [8/21], [94mLoss[0m : 2.40838
[1mStep[0m  [10/21], [94mLoss[0m : 2.26000
[1mStep[0m  [12/21], [94mLoss[0m : 2.23904
[1mStep[0m  [14/21], [94mLoss[0m : 2.29617
[1mStep[0m  [16/21], [94mLoss[0m : 2.25184
[1mStep[0m  [18/21], [94mLoss[0m : 2.23108
[1mStep[0m  [20/21], [94mLoss[0m : 2.30602

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34875
[1mStep[0m  [2/21], [94mLoss[0m : 2.31053
[1mStep[0m  [4/21], [94mLoss[0m : 2.28230
[1mStep[0m  [6/21], [94mLoss[0m : 2.19230
[1mStep[0m  [8/21], [94mLoss[0m : 2.21509
[1mStep[0m  [10/21], [94mLoss[0m : 2.13480
[1mStep[0m  [12/21], [94mLoss[0m : 2.25282
[1mStep[0m  [14/21], [94mLoss[0m : 2.23311
[1mStep[0m  [16/21], [94mLoss[0m : 2.31591
[1mStep[0m  [18/21], [94mLoss[0m : 2.25073
[1mStep[0m  [20/21], [94mLoss[0m : 2.28477

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.327
====================================

Phase 1 - Evaluation MAE:  2.32733382497515
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.24556
[1mStep[0m  [2/21], [94mLoss[0m : 2.36251
[1mStep[0m  [4/21], [94mLoss[0m : 2.38133
[1mStep[0m  [6/21], [94mLoss[0m : 2.46649
[1mStep[0m  [8/21], [94mLoss[0m : 2.45738
[1mStep[0m  [10/21], [94mLoss[0m : 2.67806
[1mStep[0m  [12/21], [94mLoss[0m : 2.44741
[1mStep[0m  [14/21], [94mLoss[0m : 2.46997
[1mStep[0m  [16/21], [94mLoss[0m : 2.50891
[1mStep[0m  [18/21], [94mLoss[0m : 2.54934
[1mStep[0m  [20/21], [94mLoss[0m : 2.42098

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31842
[1mStep[0m  [2/21], [94mLoss[0m : 2.23430
[1mStep[0m  [4/21], [94mLoss[0m : 2.34713
[1mStep[0m  [6/21], [94mLoss[0m : 2.24638
[1mStep[0m  [8/21], [94mLoss[0m : 2.22119
[1mStep[0m  [10/21], [94mLoss[0m : 2.52787
[1mStep[0m  [12/21], [94mLoss[0m : 2.48203
[1mStep[0m  [14/21], [94mLoss[0m : 2.35524
[1mStep[0m  [16/21], [94mLoss[0m : 2.22916
[1mStep[0m  [18/21], [94mLoss[0m : 2.30243
[1mStep[0m  [20/21], [94mLoss[0m : 2.27691

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.657, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23292
[1mStep[0m  [2/21], [94mLoss[0m : 2.22013
[1mStep[0m  [4/21], [94mLoss[0m : 2.22823
[1mStep[0m  [6/21], [94mLoss[0m : 2.27906
[1mStep[0m  [8/21], [94mLoss[0m : 2.27269
[1mStep[0m  [10/21], [94mLoss[0m : 2.19431
[1mStep[0m  [12/21], [94mLoss[0m : 2.14529
[1mStep[0m  [14/21], [94mLoss[0m : 2.23257
[1mStep[0m  [16/21], [94mLoss[0m : 2.26103
[1mStep[0m  [18/21], [94mLoss[0m : 2.31643
[1mStep[0m  [20/21], [94mLoss[0m : 2.48330

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20778
[1mStep[0m  [2/21], [94mLoss[0m : 2.25012
[1mStep[0m  [4/21], [94mLoss[0m : 2.14284
[1mStep[0m  [6/21], [94mLoss[0m : 2.33044
[1mStep[0m  [8/21], [94mLoss[0m : 2.17681
[1mStep[0m  [10/21], [94mLoss[0m : 2.21271
[1mStep[0m  [12/21], [94mLoss[0m : 2.16101
[1mStep[0m  [14/21], [94mLoss[0m : 2.33527
[1mStep[0m  [16/21], [94mLoss[0m : 2.22640
[1mStep[0m  [18/21], [94mLoss[0m : 2.06348
[1mStep[0m  [20/21], [94mLoss[0m : 2.25610

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08832
[1mStep[0m  [2/21], [94mLoss[0m : 2.14194
[1mStep[0m  [4/21], [94mLoss[0m : 2.12202
[1mStep[0m  [6/21], [94mLoss[0m : 2.16905
[1mStep[0m  [8/21], [94mLoss[0m : 2.12487
[1mStep[0m  [10/21], [94mLoss[0m : 2.14448
[1mStep[0m  [12/21], [94mLoss[0m : 2.10482
[1mStep[0m  [14/21], [94mLoss[0m : 2.17814
[1mStep[0m  [16/21], [94mLoss[0m : 2.13472
[1mStep[0m  [18/21], [94mLoss[0m : 2.07568
[1mStep[0m  [20/21], [94mLoss[0m : 2.14369

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10910
[1mStep[0m  [2/21], [94mLoss[0m : 1.98788
[1mStep[0m  [4/21], [94mLoss[0m : 2.09875
[1mStep[0m  [6/21], [94mLoss[0m : 1.99597
[1mStep[0m  [8/21], [94mLoss[0m : 1.98162
[1mStep[0m  [10/21], [94mLoss[0m : 2.09291
[1mStep[0m  [12/21], [94mLoss[0m : 2.11482
[1mStep[0m  [14/21], [94mLoss[0m : 2.08721
[1mStep[0m  [16/21], [94mLoss[0m : 1.94136
[1mStep[0m  [18/21], [94mLoss[0m : 2.13139
[1mStep[0m  [20/21], [94mLoss[0m : 2.15264

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.96933
[1mStep[0m  [2/21], [94mLoss[0m : 1.83551
[1mStep[0m  [4/21], [94mLoss[0m : 2.09079
[1mStep[0m  [6/21], [94mLoss[0m : 1.96355
[1mStep[0m  [8/21], [94mLoss[0m : 1.96935
[1mStep[0m  [10/21], [94mLoss[0m : 2.01434
[1mStep[0m  [12/21], [94mLoss[0m : 2.10904
[1mStep[0m  [14/21], [94mLoss[0m : 2.11744
[1mStep[0m  [16/21], [94mLoss[0m : 1.80912
[1mStep[0m  [18/21], [94mLoss[0m : 2.13310
[1mStep[0m  [20/21], [94mLoss[0m : 2.06223

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93278
[1mStep[0m  [2/21], [94mLoss[0m : 1.84845
[1mStep[0m  [4/21], [94mLoss[0m : 1.89685
[1mStep[0m  [6/21], [94mLoss[0m : 1.83997
[1mStep[0m  [8/21], [94mLoss[0m : 1.98214
[1mStep[0m  [10/21], [94mLoss[0m : 2.02184
[1mStep[0m  [12/21], [94mLoss[0m : 1.95896
[1mStep[0m  [14/21], [94mLoss[0m : 1.81268
[1mStep[0m  [16/21], [94mLoss[0m : 1.91299
[1mStep[0m  [18/21], [94mLoss[0m : 2.04130
[1mStep[0m  [20/21], [94mLoss[0m : 1.88185

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.81898
[1mStep[0m  [2/21], [94mLoss[0m : 2.04183
[1mStep[0m  [4/21], [94mLoss[0m : 1.73869
[1mStep[0m  [6/21], [94mLoss[0m : 1.80977
[1mStep[0m  [8/21], [94mLoss[0m : 1.95157
[1mStep[0m  [10/21], [94mLoss[0m : 1.83373
[1mStep[0m  [12/21], [94mLoss[0m : 1.84908
[1mStep[0m  [14/21], [94mLoss[0m : 1.75416
[1mStep[0m  [16/21], [94mLoss[0m : 1.99615
[1mStep[0m  [18/21], [94mLoss[0m : 1.93733
[1mStep[0m  [20/21], [94mLoss[0m : 1.87000

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.79283
[1mStep[0m  [2/21], [94mLoss[0m : 1.71178
[1mStep[0m  [4/21], [94mLoss[0m : 1.84086
[1mStep[0m  [6/21], [94mLoss[0m : 1.90436
[1mStep[0m  [8/21], [94mLoss[0m : 1.84363
[1mStep[0m  [10/21], [94mLoss[0m : 1.74289
[1mStep[0m  [12/21], [94mLoss[0m : 1.98506
[1mStep[0m  [14/21], [94mLoss[0m : 1.81313
[1mStep[0m  [16/21], [94mLoss[0m : 1.76395
[1mStep[0m  [18/21], [94mLoss[0m : 1.90649
[1mStep[0m  [20/21], [94mLoss[0m : 1.85083

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.75895
[1mStep[0m  [2/21], [94mLoss[0m : 1.74582
[1mStep[0m  [4/21], [94mLoss[0m : 1.94033
[1mStep[0m  [6/21], [94mLoss[0m : 1.87539
[1mStep[0m  [8/21], [94mLoss[0m : 1.74935
[1mStep[0m  [10/21], [94mLoss[0m : 1.78534
[1mStep[0m  [12/21], [94mLoss[0m : 1.65228
[1mStep[0m  [14/21], [94mLoss[0m : 1.87242
[1mStep[0m  [16/21], [94mLoss[0m : 1.86390
[1mStep[0m  [18/21], [94mLoss[0m : 1.71782
[1mStep[0m  [20/21], [94mLoss[0m : 1.75002

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.70940
[1mStep[0m  [2/21], [94mLoss[0m : 1.75533
[1mStep[0m  [4/21], [94mLoss[0m : 1.68194
[1mStep[0m  [6/21], [94mLoss[0m : 1.81193
[1mStep[0m  [8/21], [94mLoss[0m : 1.88518
[1mStep[0m  [10/21], [94mLoss[0m : 1.65906
[1mStep[0m  [12/21], [94mLoss[0m : 1.82179
[1mStep[0m  [14/21], [94mLoss[0m : 1.71845
[1mStep[0m  [16/21], [94mLoss[0m : 1.70782
[1mStep[0m  [18/21], [94mLoss[0m : 1.69054
[1mStep[0m  [20/21], [94mLoss[0m : 1.82411

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55295
[1mStep[0m  [2/21], [94mLoss[0m : 1.68900
[1mStep[0m  [4/21], [94mLoss[0m : 1.69065
[1mStep[0m  [6/21], [94mLoss[0m : 1.66994
[1mStep[0m  [8/21], [94mLoss[0m : 1.62037
[1mStep[0m  [10/21], [94mLoss[0m : 1.51682
[1mStep[0m  [12/21], [94mLoss[0m : 1.70054
[1mStep[0m  [14/21], [94mLoss[0m : 1.60332
[1mStep[0m  [16/21], [94mLoss[0m : 1.82436
[1mStep[0m  [18/21], [94mLoss[0m : 1.72825
[1mStep[0m  [20/21], [94mLoss[0m : 1.76896

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.518, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.57088
[1mStep[0m  [2/21], [94mLoss[0m : 1.62579
[1mStep[0m  [4/21], [94mLoss[0m : 1.57801
[1mStep[0m  [6/21], [94mLoss[0m : 1.59726
[1mStep[0m  [8/21], [94mLoss[0m : 1.66173
[1mStep[0m  [10/21], [94mLoss[0m : 1.67910
[1mStep[0m  [12/21], [94mLoss[0m : 1.60495
[1mStep[0m  [14/21], [94mLoss[0m : 1.79851
[1mStep[0m  [16/21], [94mLoss[0m : 1.68769
[1mStep[0m  [18/21], [94mLoss[0m : 1.68338
[1mStep[0m  [20/21], [94mLoss[0m : 1.77758

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.45558
[1mStep[0m  [2/21], [94mLoss[0m : 1.50324
[1mStep[0m  [4/21], [94mLoss[0m : 1.74922
[1mStep[0m  [6/21], [94mLoss[0m : 1.63706
[1mStep[0m  [8/21], [94mLoss[0m : 1.64195
[1mStep[0m  [10/21], [94mLoss[0m : 1.56157
[1mStep[0m  [12/21], [94mLoss[0m : 1.54353
[1mStep[0m  [14/21], [94mLoss[0m : 1.61579
[1mStep[0m  [16/21], [94mLoss[0m : 1.60047
[1mStep[0m  [18/21], [94mLoss[0m : 1.52951
[1mStep[0m  [20/21], [94mLoss[0m : 1.66482

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61191
[1mStep[0m  [2/21], [94mLoss[0m : 1.70932
[1mStep[0m  [4/21], [94mLoss[0m : 1.51171
[1mStep[0m  [6/21], [94mLoss[0m : 1.52695
[1mStep[0m  [8/21], [94mLoss[0m : 1.58915
[1mStep[0m  [10/21], [94mLoss[0m : 1.63139
[1mStep[0m  [12/21], [94mLoss[0m : 1.58471
[1mStep[0m  [14/21], [94mLoss[0m : 1.53541
[1mStep[0m  [16/21], [94mLoss[0m : 1.48521
[1mStep[0m  [18/21], [94mLoss[0m : 1.67533
[1mStep[0m  [20/21], [94mLoss[0m : 1.71407

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.67884
[1mStep[0m  [2/21], [94mLoss[0m : 1.39504
[1mStep[0m  [4/21], [94mLoss[0m : 1.59810
[1mStep[0m  [6/21], [94mLoss[0m : 1.52131
[1mStep[0m  [8/21], [94mLoss[0m : 1.52601
[1mStep[0m  [10/21], [94mLoss[0m : 1.57546
[1mStep[0m  [12/21], [94mLoss[0m : 1.50906
[1mStep[0m  [14/21], [94mLoss[0m : 1.51430
[1mStep[0m  [16/21], [94mLoss[0m : 1.52578
[1mStep[0m  [18/21], [94mLoss[0m : 1.51947
[1mStep[0m  [20/21], [94mLoss[0m : 1.53757

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.54959
[1mStep[0m  [2/21], [94mLoss[0m : 1.45620
[1mStep[0m  [4/21], [94mLoss[0m : 1.48249
[1mStep[0m  [6/21], [94mLoss[0m : 1.50927
[1mStep[0m  [8/21], [94mLoss[0m : 1.61503
[1mStep[0m  [10/21], [94mLoss[0m : 1.47242
[1mStep[0m  [12/21], [94mLoss[0m : 1.65684
[1mStep[0m  [14/21], [94mLoss[0m : 1.36213
[1mStep[0m  [16/21], [94mLoss[0m : 1.52982
[1mStep[0m  [18/21], [94mLoss[0m : 1.48105
[1mStep[0m  [20/21], [94mLoss[0m : 1.51845

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43814
[1mStep[0m  [2/21], [94mLoss[0m : 1.46454
[1mStep[0m  [4/21], [94mLoss[0m : 1.42762
[1mStep[0m  [6/21], [94mLoss[0m : 1.55397
[1mStep[0m  [8/21], [94mLoss[0m : 1.45441
[1mStep[0m  [10/21], [94mLoss[0m : 1.52090
[1mStep[0m  [12/21], [94mLoss[0m : 1.36501
[1mStep[0m  [14/21], [94mLoss[0m : 1.65442
[1mStep[0m  [16/21], [94mLoss[0m : 1.46748
[1mStep[0m  [18/21], [94mLoss[0m : 1.50771
[1mStep[0m  [20/21], [94mLoss[0m : 1.42792

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46514
[1mStep[0m  [2/21], [94mLoss[0m : 1.43800
[1mStep[0m  [4/21], [94mLoss[0m : 1.48804
[1mStep[0m  [6/21], [94mLoss[0m : 1.44380
[1mStep[0m  [8/21], [94mLoss[0m : 1.46461
[1mStep[0m  [10/21], [94mLoss[0m : 1.44841
[1mStep[0m  [12/21], [94mLoss[0m : 1.40423
[1mStep[0m  [14/21], [94mLoss[0m : 1.51093
[1mStep[0m  [16/21], [94mLoss[0m : 1.57886
[1mStep[0m  [18/21], [94mLoss[0m : 1.36312
[1mStep[0m  [20/21], [94mLoss[0m : 1.53516

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.458, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.36784
[1mStep[0m  [2/21], [94mLoss[0m : 1.36627
[1mStep[0m  [4/21], [94mLoss[0m : 1.39719
[1mStep[0m  [6/21], [94mLoss[0m : 1.46361
[1mStep[0m  [8/21], [94mLoss[0m : 1.36848
[1mStep[0m  [10/21], [94mLoss[0m : 1.45835
[1mStep[0m  [12/21], [94mLoss[0m : 1.36095
[1mStep[0m  [14/21], [94mLoss[0m : 1.43701
[1mStep[0m  [16/21], [94mLoss[0m : 1.37141
[1mStep[0m  [18/21], [94mLoss[0m : 1.46076
[1mStep[0m  [20/21], [94mLoss[0m : 1.48598

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.43040
[1mStep[0m  [2/21], [94mLoss[0m : 1.33657
[1mStep[0m  [4/21], [94mLoss[0m : 1.39737
[1mStep[0m  [6/21], [94mLoss[0m : 1.36579
[1mStep[0m  [8/21], [94mLoss[0m : 1.30605
[1mStep[0m  [10/21], [94mLoss[0m : 1.30416
[1mStep[0m  [12/21], [94mLoss[0m : 1.38651
[1mStep[0m  [14/21], [94mLoss[0m : 1.42710
[1mStep[0m  [16/21], [94mLoss[0m : 1.34379
[1mStep[0m  [18/21], [94mLoss[0m : 1.37672
[1mStep[0m  [20/21], [94mLoss[0m : 1.45689

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.375, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.504
====================================

Phase 2 - Evaluation MAE:  2.5044244698115756
MAE score P1        2.327334
MAE score P2        2.504424
loss                1.374613
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay           0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 11.24303
[1mStep[0m  [33/339], [94mLoss[0m : 3.51449
[1mStep[0m  [66/339], [94mLoss[0m : 2.08970
[1mStep[0m  [99/339], [94mLoss[0m : 1.83886
[1mStep[0m  [132/339], [94mLoss[0m : 1.85531
[1mStep[0m  [165/339], [94mLoss[0m : 2.33132
[1mStep[0m  [198/339], [94mLoss[0m : 2.54544
[1mStep[0m  [231/339], [94mLoss[0m : 2.72163
[1mStep[0m  [264/339], [94mLoss[0m : 1.66924
[1mStep[0m  [297/339], [94mLoss[0m : 2.11149
[1mStep[0m  [330/339], [94mLoss[0m : 2.28955

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.718, [92mTest[0m: 11.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40889
[1mStep[0m  [33/339], [94mLoss[0m : 2.35460
[1mStep[0m  [66/339], [94mLoss[0m : 2.34259
[1mStep[0m  [99/339], [94mLoss[0m : 2.57803
[1mStep[0m  [132/339], [94mLoss[0m : 2.66001
[1mStep[0m  [165/339], [94mLoss[0m : 2.35257
[1mStep[0m  [198/339], [94mLoss[0m : 2.53102
[1mStep[0m  [231/339], [94mLoss[0m : 2.54356
[1mStep[0m  [264/339], [94mLoss[0m : 2.68004
[1mStep[0m  [297/339], [94mLoss[0m : 2.07630
[1mStep[0m  [330/339], [94mLoss[0m : 2.43106

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52951
[1mStep[0m  [33/339], [94mLoss[0m : 2.44529
[1mStep[0m  [66/339], [94mLoss[0m : 3.14195
[1mStep[0m  [99/339], [94mLoss[0m : 2.80624
[1mStep[0m  [132/339], [94mLoss[0m : 2.34387
[1mStep[0m  [165/339], [94mLoss[0m : 2.07870
[1mStep[0m  [198/339], [94mLoss[0m : 2.78911
[1mStep[0m  [231/339], [94mLoss[0m : 2.33233
[1mStep[0m  [264/339], [94mLoss[0m : 2.57526
[1mStep[0m  [297/339], [94mLoss[0m : 1.99536
[1mStep[0m  [330/339], [94mLoss[0m : 2.49158

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28764
[1mStep[0m  [33/339], [94mLoss[0m : 2.36769
[1mStep[0m  [66/339], [94mLoss[0m : 2.49567
[1mStep[0m  [99/339], [94mLoss[0m : 2.78667
[1mStep[0m  [132/339], [94mLoss[0m : 2.73380
[1mStep[0m  [165/339], [94mLoss[0m : 2.28327
[1mStep[0m  [198/339], [94mLoss[0m : 2.82768
[1mStep[0m  [231/339], [94mLoss[0m : 2.66639
[1mStep[0m  [264/339], [94mLoss[0m : 2.69899
[1mStep[0m  [297/339], [94mLoss[0m : 3.06811
[1mStep[0m  [330/339], [94mLoss[0m : 2.47726

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27802
[1mStep[0m  [33/339], [94mLoss[0m : 2.22379
[1mStep[0m  [66/339], [94mLoss[0m : 2.55585
[1mStep[0m  [99/339], [94mLoss[0m : 2.72818
[1mStep[0m  [132/339], [94mLoss[0m : 2.54093
[1mStep[0m  [165/339], [94mLoss[0m : 2.66262
[1mStep[0m  [198/339], [94mLoss[0m : 2.26227
[1mStep[0m  [231/339], [94mLoss[0m : 1.86084
[1mStep[0m  [264/339], [94mLoss[0m : 2.28238
[1mStep[0m  [297/339], [94mLoss[0m : 3.01790
[1mStep[0m  [330/339], [94mLoss[0m : 2.33910

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56848
[1mStep[0m  [33/339], [94mLoss[0m : 2.87210
[1mStep[0m  [66/339], [94mLoss[0m : 2.97006
[1mStep[0m  [99/339], [94mLoss[0m : 2.06803
[1mStep[0m  [132/339], [94mLoss[0m : 2.62220
[1mStep[0m  [165/339], [94mLoss[0m : 2.32548
[1mStep[0m  [198/339], [94mLoss[0m : 2.62145
[1mStep[0m  [231/339], [94mLoss[0m : 2.48427
[1mStep[0m  [264/339], [94mLoss[0m : 1.85616
[1mStep[0m  [297/339], [94mLoss[0m : 1.92171
[1mStep[0m  [330/339], [94mLoss[0m : 1.93690

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33666
[1mStep[0m  [33/339], [94mLoss[0m : 2.47769
[1mStep[0m  [66/339], [94mLoss[0m : 2.55490
[1mStep[0m  [99/339], [94mLoss[0m : 2.58411
[1mStep[0m  [132/339], [94mLoss[0m : 2.60498
[1mStep[0m  [165/339], [94mLoss[0m : 2.13304
[1mStep[0m  [198/339], [94mLoss[0m : 3.04218
[1mStep[0m  [231/339], [94mLoss[0m : 1.75263
[1mStep[0m  [264/339], [94mLoss[0m : 1.89431
[1mStep[0m  [297/339], [94mLoss[0m : 2.59718
[1mStep[0m  [330/339], [94mLoss[0m : 2.15082

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96324
[1mStep[0m  [33/339], [94mLoss[0m : 2.48760
[1mStep[0m  [66/339], [94mLoss[0m : 2.75954
[1mStep[0m  [99/339], [94mLoss[0m : 2.22705
[1mStep[0m  [132/339], [94mLoss[0m : 2.63901
[1mStep[0m  [165/339], [94mLoss[0m : 1.79807
[1mStep[0m  [198/339], [94mLoss[0m : 1.93644
[1mStep[0m  [231/339], [94mLoss[0m : 2.94901
[1mStep[0m  [264/339], [94mLoss[0m : 1.88215
[1mStep[0m  [297/339], [94mLoss[0m : 2.65302
[1mStep[0m  [330/339], [94mLoss[0m : 2.31141

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59032
[1mStep[0m  [33/339], [94mLoss[0m : 2.37797
[1mStep[0m  [66/339], [94mLoss[0m : 3.07260
[1mStep[0m  [99/339], [94mLoss[0m : 2.52874
[1mStep[0m  [132/339], [94mLoss[0m : 2.35262
[1mStep[0m  [165/339], [94mLoss[0m : 2.29708
[1mStep[0m  [198/339], [94mLoss[0m : 1.89183
[1mStep[0m  [231/339], [94mLoss[0m : 2.35258
[1mStep[0m  [264/339], [94mLoss[0m : 2.96240
[1mStep[0m  [297/339], [94mLoss[0m : 2.21337
[1mStep[0m  [330/339], [94mLoss[0m : 2.80543

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94913
[1mStep[0m  [33/339], [94mLoss[0m : 2.71057
[1mStep[0m  [66/339], [94mLoss[0m : 2.59724
[1mStep[0m  [99/339], [94mLoss[0m : 2.93145
[1mStep[0m  [132/339], [94mLoss[0m : 2.84663
[1mStep[0m  [165/339], [94mLoss[0m : 2.64185
[1mStep[0m  [198/339], [94mLoss[0m : 2.26760
[1mStep[0m  [231/339], [94mLoss[0m : 2.70741
[1mStep[0m  [264/339], [94mLoss[0m : 2.25845
[1mStep[0m  [297/339], [94mLoss[0m : 3.02988
[1mStep[0m  [330/339], [94mLoss[0m : 2.00356

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83433
[1mStep[0m  [33/339], [94mLoss[0m : 2.46743
[1mStep[0m  [66/339], [94mLoss[0m : 2.39667
[1mStep[0m  [99/339], [94mLoss[0m : 2.22407
[1mStep[0m  [132/339], [94mLoss[0m : 3.11395
[1mStep[0m  [165/339], [94mLoss[0m : 2.19949
[1mStep[0m  [198/339], [94mLoss[0m : 2.71309
[1mStep[0m  [231/339], [94mLoss[0m : 2.03797
[1mStep[0m  [264/339], [94mLoss[0m : 2.28474
[1mStep[0m  [297/339], [94mLoss[0m : 2.09194
[1mStep[0m  [330/339], [94mLoss[0m : 3.34397

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20541
[1mStep[0m  [33/339], [94mLoss[0m : 2.57917
[1mStep[0m  [66/339], [94mLoss[0m : 2.06645
[1mStep[0m  [99/339], [94mLoss[0m : 2.00622
[1mStep[0m  [132/339], [94mLoss[0m : 2.23251
[1mStep[0m  [165/339], [94mLoss[0m : 2.67805
[1mStep[0m  [198/339], [94mLoss[0m : 2.53406
[1mStep[0m  [231/339], [94mLoss[0m : 3.30096
[1mStep[0m  [264/339], [94mLoss[0m : 2.45158
[1mStep[0m  [297/339], [94mLoss[0m : 2.76273
[1mStep[0m  [330/339], [94mLoss[0m : 2.28760

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68837
[1mStep[0m  [33/339], [94mLoss[0m : 1.74602
[1mStep[0m  [66/339], [94mLoss[0m : 2.25820
[1mStep[0m  [99/339], [94mLoss[0m : 2.29568
[1mStep[0m  [132/339], [94mLoss[0m : 2.13753
[1mStep[0m  [165/339], [94mLoss[0m : 2.21179
[1mStep[0m  [198/339], [94mLoss[0m : 2.84138
[1mStep[0m  [231/339], [94mLoss[0m : 2.73005
[1mStep[0m  [264/339], [94mLoss[0m : 2.32750
[1mStep[0m  [297/339], [94mLoss[0m : 2.00817
[1mStep[0m  [330/339], [94mLoss[0m : 2.49707

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93957
[1mStep[0m  [33/339], [94mLoss[0m : 2.90185
[1mStep[0m  [66/339], [94mLoss[0m : 2.27691
[1mStep[0m  [99/339], [94mLoss[0m : 2.12055
[1mStep[0m  [132/339], [94mLoss[0m : 2.61688
[1mStep[0m  [165/339], [94mLoss[0m : 2.96180
[1mStep[0m  [198/339], [94mLoss[0m : 2.88419
[1mStep[0m  [231/339], [94mLoss[0m : 2.12945
[1mStep[0m  [264/339], [94mLoss[0m : 1.85714
[1mStep[0m  [297/339], [94mLoss[0m : 1.76501
[1mStep[0m  [330/339], [94mLoss[0m : 2.53206

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37370
[1mStep[0m  [33/339], [94mLoss[0m : 2.82023
[1mStep[0m  [66/339], [94mLoss[0m : 2.72111
[1mStep[0m  [99/339], [94mLoss[0m : 2.63839
[1mStep[0m  [132/339], [94mLoss[0m : 2.04370
[1mStep[0m  [165/339], [94mLoss[0m : 2.44607
[1mStep[0m  [198/339], [94mLoss[0m : 2.42265
[1mStep[0m  [231/339], [94mLoss[0m : 2.57996
[1mStep[0m  [264/339], [94mLoss[0m : 2.37027
[1mStep[0m  [297/339], [94mLoss[0m : 2.78141
[1mStep[0m  [330/339], [94mLoss[0m : 1.99025

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42230
[1mStep[0m  [33/339], [94mLoss[0m : 2.48017
[1mStep[0m  [66/339], [94mLoss[0m : 2.32569
[1mStep[0m  [99/339], [94mLoss[0m : 2.90932
[1mStep[0m  [132/339], [94mLoss[0m : 2.31160
[1mStep[0m  [165/339], [94mLoss[0m : 3.20424
[1mStep[0m  [198/339], [94mLoss[0m : 3.14573
[1mStep[0m  [231/339], [94mLoss[0m : 2.69443
[1mStep[0m  [264/339], [94mLoss[0m : 2.33705
[1mStep[0m  [297/339], [94mLoss[0m : 2.93558
[1mStep[0m  [330/339], [94mLoss[0m : 2.36578

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38403
[1mStep[0m  [33/339], [94mLoss[0m : 2.12829
[1mStep[0m  [66/339], [94mLoss[0m : 2.89521
[1mStep[0m  [99/339], [94mLoss[0m : 2.41689
[1mStep[0m  [132/339], [94mLoss[0m : 2.63041
[1mStep[0m  [165/339], [94mLoss[0m : 2.74094
[1mStep[0m  [198/339], [94mLoss[0m : 2.65969
[1mStep[0m  [231/339], [94mLoss[0m : 2.96876
[1mStep[0m  [264/339], [94mLoss[0m : 2.40162
[1mStep[0m  [297/339], [94mLoss[0m : 2.42457
[1mStep[0m  [330/339], [94mLoss[0m : 1.83904

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45500
[1mStep[0m  [33/339], [94mLoss[0m : 2.34501
[1mStep[0m  [66/339], [94mLoss[0m : 2.68746
[1mStep[0m  [99/339], [94mLoss[0m : 2.30472
[1mStep[0m  [132/339], [94mLoss[0m : 2.16290
[1mStep[0m  [165/339], [94mLoss[0m : 2.62232
[1mStep[0m  [198/339], [94mLoss[0m : 2.31343
[1mStep[0m  [231/339], [94mLoss[0m : 2.04857
[1mStep[0m  [264/339], [94mLoss[0m : 2.29302
[1mStep[0m  [297/339], [94mLoss[0m : 2.16063
[1mStep[0m  [330/339], [94mLoss[0m : 2.19025

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31003
[1mStep[0m  [33/339], [94mLoss[0m : 2.79699
[1mStep[0m  [66/339], [94mLoss[0m : 3.26079
[1mStep[0m  [99/339], [94mLoss[0m : 2.80517
[1mStep[0m  [132/339], [94mLoss[0m : 1.68379
[1mStep[0m  [165/339], [94mLoss[0m : 2.49445
[1mStep[0m  [198/339], [94mLoss[0m : 2.63050
[1mStep[0m  [231/339], [94mLoss[0m : 2.20323
[1mStep[0m  [264/339], [94mLoss[0m : 2.32945
[1mStep[0m  [297/339], [94mLoss[0m : 1.83723
[1mStep[0m  [330/339], [94mLoss[0m : 2.21500

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76147
[1mStep[0m  [33/339], [94mLoss[0m : 2.34280
[1mStep[0m  [66/339], [94mLoss[0m : 2.71863
[1mStep[0m  [99/339], [94mLoss[0m : 2.19780
[1mStep[0m  [132/339], [94mLoss[0m : 2.93663
[1mStep[0m  [165/339], [94mLoss[0m : 3.30042
[1mStep[0m  [198/339], [94mLoss[0m : 2.16570
[1mStep[0m  [231/339], [94mLoss[0m : 3.06040
[1mStep[0m  [264/339], [94mLoss[0m : 2.61992
[1mStep[0m  [297/339], [94mLoss[0m : 2.59233
[1mStep[0m  [330/339], [94mLoss[0m : 2.75687

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78929
[1mStep[0m  [33/339], [94mLoss[0m : 2.27429
[1mStep[0m  [66/339], [94mLoss[0m : 2.12064
[1mStep[0m  [99/339], [94mLoss[0m : 2.34143
[1mStep[0m  [132/339], [94mLoss[0m : 2.62365
[1mStep[0m  [165/339], [94mLoss[0m : 2.65947
[1mStep[0m  [198/339], [94mLoss[0m : 2.01824
[1mStep[0m  [231/339], [94mLoss[0m : 2.55951
[1mStep[0m  [264/339], [94mLoss[0m : 2.24276
[1mStep[0m  [297/339], [94mLoss[0m : 2.57860
[1mStep[0m  [330/339], [94mLoss[0m : 2.22518

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54597
[1mStep[0m  [33/339], [94mLoss[0m : 2.71438
[1mStep[0m  [66/339], [94mLoss[0m : 2.57774
[1mStep[0m  [99/339], [94mLoss[0m : 2.37188
[1mStep[0m  [132/339], [94mLoss[0m : 2.21295
[1mStep[0m  [165/339], [94mLoss[0m : 2.72617
[1mStep[0m  [198/339], [94mLoss[0m : 2.30602
[1mStep[0m  [231/339], [94mLoss[0m : 2.21551
[1mStep[0m  [264/339], [94mLoss[0m : 3.02008
[1mStep[0m  [297/339], [94mLoss[0m : 2.81421
[1mStep[0m  [330/339], [94mLoss[0m : 2.58160

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10452
[1mStep[0m  [33/339], [94mLoss[0m : 2.14249
[1mStep[0m  [66/339], [94mLoss[0m : 1.86695
[1mStep[0m  [99/339], [94mLoss[0m : 2.82315
[1mStep[0m  [132/339], [94mLoss[0m : 3.05006
[1mStep[0m  [165/339], [94mLoss[0m : 2.31922
[1mStep[0m  [198/339], [94mLoss[0m : 2.62707
[1mStep[0m  [231/339], [94mLoss[0m : 2.16919
[1mStep[0m  [264/339], [94mLoss[0m : 2.50285
[1mStep[0m  [297/339], [94mLoss[0m : 2.39076
[1mStep[0m  [330/339], [94mLoss[0m : 2.05349

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42711
[1mStep[0m  [33/339], [94mLoss[0m : 2.32403
[1mStep[0m  [66/339], [94mLoss[0m : 2.26656
[1mStep[0m  [99/339], [94mLoss[0m : 2.79212
[1mStep[0m  [132/339], [94mLoss[0m : 2.53532
[1mStep[0m  [165/339], [94mLoss[0m : 2.16655
[1mStep[0m  [198/339], [94mLoss[0m : 2.48136
[1mStep[0m  [231/339], [94mLoss[0m : 2.91219
[1mStep[0m  [264/339], [94mLoss[0m : 1.88625
[1mStep[0m  [297/339], [94mLoss[0m : 2.99406
[1mStep[0m  [330/339], [94mLoss[0m : 1.94303

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05609
[1mStep[0m  [33/339], [94mLoss[0m : 2.39198
[1mStep[0m  [66/339], [94mLoss[0m : 2.43745
[1mStep[0m  [99/339], [94mLoss[0m : 2.18967
[1mStep[0m  [132/339], [94mLoss[0m : 2.65731
[1mStep[0m  [165/339], [94mLoss[0m : 2.34450
[1mStep[0m  [198/339], [94mLoss[0m : 3.47574
[1mStep[0m  [231/339], [94mLoss[0m : 3.00297
[1mStep[0m  [264/339], [94mLoss[0m : 2.52603
[1mStep[0m  [297/339], [94mLoss[0m : 1.89535
[1mStep[0m  [330/339], [94mLoss[0m : 2.11115

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82039
[1mStep[0m  [33/339], [94mLoss[0m : 2.29766
[1mStep[0m  [66/339], [94mLoss[0m : 2.88899
[1mStep[0m  [99/339], [94mLoss[0m : 2.56452
[1mStep[0m  [132/339], [94mLoss[0m : 2.87351
[1mStep[0m  [165/339], [94mLoss[0m : 2.66421
[1mStep[0m  [198/339], [94mLoss[0m : 2.76806
[1mStep[0m  [231/339], [94mLoss[0m : 2.07335
[1mStep[0m  [264/339], [94mLoss[0m : 2.67201
[1mStep[0m  [297/339], [94mLoss[0m : 2.54700
[1mStep[0m  [330/339], [94mLoss[0m : 2.23316

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.99564
[1mStep[0m  [33/339], [94mLoss[0m : 2.56610
[1mStep[0m  [66/339], [94mLoss[0m : 2.83122
[1mStep[0m  [99/339], [94mLoss[0m : 2.31050
[1mStep[0m  [132/339], [94mLoss[0m : 2.30817
[1mStep[0m  [165/339], [94mLoss[0m : 2.08564
[1mStep[0m  [198/339], [94mLoss[0m : 2.38043
[1mStep[0m  [231/339], [94mLoss[0m : 2.06064
[1mStep[0m  [264/339], [94mLoss[0m : 2.50638
[1mStep[0m  [297/339], [94mLoss[0m : 2.25520
[1mStep[0m  [330/339], [94mLoss[0m : 2.53308

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77207
[1mStep[0m  [33/339], [94mLoss[0m : 2.70042
[1mStep[0m  [66/339], [94mLoss[0m : 2.77020
[1mStep[0m  [99/339], [94mLoss[0m : 2.45106
[1mStep[0m  [132/339], [94mLoss[0m : 2.49494
[1mStep[0m  [165/339], [94mLoss[0m : 2.48910
[1mStep[0m  [198/339], [94mLoss[0m : 2.70709
[1mStep[0m  [231/339], [94mLoss[0m : 3.07188
[1mStep[0m  [264/339], [94mLoss[0m : 2.23741
[1mStep[0m  [297/339], [94mLoss[0m : 2.15014
[1mStep[0m  [330/339], [94mLoss[0m : 2.11496

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31347
[1mStep[0m  [33/339], [94mLoss[0m : 1.95375
[1mStep[0m  [66/339], [94mLoss[0m : 2.33506
[1mStep[0m  [99/339], [94mLoss[0m : 2.30777
[1mStep[0m  [132/339], [94mLoss[0m : 2.44845
[1mStep[0m  [165/339], [94mLoss[0m : 2.28543
[1mStep[0m  [198/339], [94mLoss[0m : 1.96798
[1mStep[0m  [231/339], [94mLoss[0m : 2.21833
[1mStep[0m  [264/339], [94mLoss[0m : 2.50555
[1mStep[0m  [297/339], [94mLoss[0m : 2.80733
[1mStep[0m  [330/339], [94mLoss[0m : 2.51323

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32736
[1mStep[0m  [33/339], [94mLoss[0m : 2.54849
[1mStep[0m  [66/339], [94mLoss[0m : 2.15811
[1mStep[0m  [99/339], [94mLoss[0m : 2.20475
[1mStep[0m  [132/339], [94mLoss[0m : 2.57265
[1mStep[0m  [165/339], [94mLoss[0m : 1.82657
[1mStep[0m  [198/339], [94mLoss[0m : 2.64029
[1mStep[0m  [231/339], [94mLoss[0m : 2.97194
[1mStep[0m  [264/339], [94mLoss[0m : 2.61116
[1mStep[0m  [297/339], [94mLoss[0m : 2.10667
[1mStep[0m  [330/339], [94mLoss[0m : 2.27784

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.322
====================================

Phase 1 - Evaluation MAE:  2.3224201940857205
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.34672
[1mStep[0m  [33/339], [94mLoss[0m : 2.67099
[1mStep[0m  [66/339], [94mLoss[0m : 2.38826
[1mStep[0m  [99/339], [94mLoss[0m : 2.58420
[1mStep[0m  [132/339], [94mLoss[0m : 2.34505
[1mStep[0m  [165/339], [94mLoss[0m : 2.49114
[1mStep[0m  [198/339], [94mLoss[0m : 2.18116
[1mStep[0m  [231/339], [94mLoss[0m : 2.10042
[1mStep[0m  [264/339], [94mLoss[0m : 2.67582
[1mStep[0m  [297/339], [94mLoss[0m : 2.67040
[1mStep[0m  [330/339], [94mLoss[0m : 2.72977

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42322
[1mStep[0m  [33/339], [94mLoss[0m : 2.40116
[1mStep[0m  [66/339], [94mLoss[0m : 2.53328
[1mStep[0m  [99/339], [94mLoss[0m : 2.32503
[1mStep[0m  [132/339], [94mLoss[0m : 2.32802
[1mStep[0m  [165/339], [94mLoss[0m : 2.33253
[1mStep[0m  [198/339], [94mLoss[0m : 2.72248
[1mStep[0m  [231/339], [94mLoss[0m : 1.95408
[1mStep[0m  [264/339], [94mLoss[0m : 2.38511
[1mStep[0m  [297/339], [94mLoss[0m : 2.34086
[1mStep[0m  [330/339], [94mLoss[0m : 2.94719

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53803
[1mStep[0m  [33/339], [94mLoss[0m : 2.48786
[1mStep[0m  [66/339], [94mLoss[0m : 2.61697
[1mStep[0m  [99/339], [94mLoss[0m : 1.94097
[1mStep[0m  [132/339], [94mLoss[0m : 2.98048
[1mStep[0m  [165/339], [94mLoss[0m : 2.16674
[1mStep[0m  [198/339], [94mLoss[0m : 2.58674
[1mStep[0m  [231/339], [94mLoss[0m : 1.27240
[1mStep[0m  [264/339], [94mLoss[0m : 2.57558
[1mStep[0m  [297/339], [94mLoss[0m : 2.09418
[1mStep[0m  [330/339], [94mLoss[0m : 2.12577

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25927
[1mStep[0m  [33/339], [94mLoss[0m : 1.97260
[1mStep[0m  [66/339], [94mLoss[0m : 2.40175
[1mStep[0m  [99/339], [94mLoss[0m : 1.75270
[1mStep[0m  [132/339], [94mLoss[0m : 2.08864
[1mStep[0m  [165/339], [94mLoss[0m : 2.16585
[1mStep[0m  [198/339], [94mLoss[0m : 2.01870
[1mStep[0m  [231/339], [94mLoss[0m : 1.94626
[1mStep[0m  [264/339], [94mLoss[0m : 2.20698
[1mStep[0m  [297/339], [94mLoss[0m : 2.75392
[1mStep[0m  [330/339], [94mLoss[0m : 2.17325

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05144
[1mStep[0m  [33/339], [94mLoss[0m : 2.07405
[1mStep[0m  [66/339], [94mLoss[0m : 1.72099
[1mStep[0m  [99/339], [94mLoss[0m : 2.23881
[1mStep[0m  [132/339], [94mLoss[0m : 2.72846
[1mStep[0m  [165/339], [94mLoss[0m : 2.11755
[1mStep[0m  [198/339], [94mLoss[0m : 2.69394
[1mStep[0m  [231/339], [94mLoss[0m : 1.55412
[1mStep[0m  [264/339], [94mLoss[0m : 1.67438
[1mStep[0m  [297/339], [94mLoss[0m : 2.00318
[1mStep[0m  [330/339], [94mLoss[0m : 2.83765

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12835
[1mStep[0m  [33/339], [94mLoss[0m : 1.98455
[1mStep[0m  [66/339], [94mLoss[0m : 3.06206
[1mStep[0m  [99/339], [94mLoss[0m : 1.67309
[1mStep[0m  [132/339], [94mLoss[0m : 1.66045
[1mStep[0m  [165/339], [94mLoss[0m : 2.14249
[1mStep[0m  [198/339], [94mLoss[0m : 2.14455
[1mStep[0m  [231/339], [94mLoss[0m : 1.96612
[1mStep[0m  [264/339], [94mLoss[0m : 1.86653
[1mStep[0m  [297/339], [94mLoss[0m : 2.17666
[1mStep[0m  [330/339], [94mLoss[0m : 3.02947

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14333
[1mStep[0m  [33/339], [94mLoss[0m : 1.89295
[1mStep[0m  [66/339], [94mLoss[0m : 2.00064
[1mStep[0m  [99/339], [94mLoss[0m : 1.53218
[1mStep[0m  [132/339], [94mLoss[0m : 1.66274
[1mStep[0m  [165/339], [94mLoss[0m : 1.86424
[1mStep[0m  [198/339], [94mLoss[0m : 1.86285
[1mStep[0m  [231/339], [94mLoss[0m : 3.13865
[1mStep[0m  [264/339], [94mLoss[0m : 2.15011
[1mStep[0m  [297/339], [94mLoss[0m : 2.23376
[1mStep[0m  [330/339], [94mLoss[0m : 2.21449

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89922
[1mStep[0m  [33/339], [94mLoss[0m : 1.65354
[1mStep[0m  [66/339], [94mLoss[0m : 1.90618
[1mStep[0m  [99/339], [94mLoss[0m : 1.70321
[1mStep[0m  [132/339], [94mLoss[0m : 1.73611
[1mStep[0m  [165/339], [94mLoss[0m : 1.90552
[1mStep[0m  [198/339], [94mLoss[0m : 2.26912
[1mStep[0m  [231/339], [94mLoss[0m : 1.97919
[1mStep[0m  [264/339], [94mLoss[0m : 2.75228
[1mStep[0m  [297/339], [94mLoss[0m : 2.22989
[1mStep[0m  [330/339], [94mLoss[0m : 2.08575

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59250
[1mStep[0m  [33/339], [94mLoss[0m : 1.96737
[1mStep[0m  [66/339], [94mLoss[0m : 1.65119
[1mStep[0m  [99/339], [94mLoss[0m : 1.98415
[1mStep[0m  [132/339], [94mLoss[0m : 2.40400
[1mStep[0m  [165/339], [94mLoss[0m : 2.26515
[1mStep[0m  [198/339], [94mLoss[0m : 1.77764
[1mStep[0m  [231/339], [94mLoss[0m : 1.70777
[1mStep[0m  [264/339], [94mLoss[0m : 1.52907
[1mStep[0m  [297/339], [94mLoss[0m : 2.19513
[1mStep[0m  [330/339], [94mLoss[0m : 1.88998

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89311
[1mStep[0m  [33/339], [94mLoss[0m : 1.65800
[1mStep[0m  [66/339], [94mLoss[0m : 2.67974
[1mStep[0m  [99/339], [94mLoss[0m : 1.72558
[1mStep[0m  [132/339], [94mLoss[0m : 1.99367
[1mStep[0m  [165/339], [94mLoss[0m : 1.64264
[1mStep[0m  [198/339], [94mLoss[0m : 2.10001
[1mStep[0m  [231/339], [94mLoss[0m : 2.16051
[1mStep[0m  [264/339], [94mLoss[0m : 1.77041
[1mStep[0m  [297/339], [94mLoss[0m : 1.73342
[1mStep[0m  [330/339], [94mLoss[0m : 2.19535

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.942, [92mTest[0m: 2.417, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22520
[1mStep[0m  [33/339], [94mLoss[0m : 1.93208
[1mStep[0m  [66/339], [94mLoss[0m : 2.23012
[1mStep[0m  [99/339], [94mLoss[0m : 1.75752
[1mStep[0m  [132/339], [94mLoss[0m : 1.67569
[1mStep[0m  [165/339], [94mLoss[0m : 1.64137
[1mStep[0m  [198/339], [94mLoss[0m : 1.91078
[1mStep[0m  [231/339], [94mLoss[0m : 1.27124
[1mStep[0m  [264/339], [94mLoss[0m : 2.51206
[1mStep[0m  [297/339], [94mLoss[0m : 2.06157
[1mStep[0m  [330/339], [94mLoss[0m : 2.30734

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26784
[1mStep[0m  [33/339], [94mLoss[0m : 2.04139
[1mStep[0m  [66/339], [94mLoss[0m : 1.72593
[1mStep[0m  [99/339], [94mLoss[0m : 1.70521
[1mStep[0m  [132/339], [94mLoss[0m : 2.16520
[1mStep[0m  [165/339], [94mLoss[0m : 1.67083
[1mStep[0m  [198/339], [94mLoss[0m : 1.61180
[1mStep[0m  [231/339], [94mLoss[0m : 1.65980
[1mStep[0m  [264/339], [94mLoss[0m : 1.50298
[1mStep[0m  [297/339], [94mLoss[0m : 1.72806
[1mStep[0m  [330/339], [94mLoss[0m : 1.58158

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82635
[1mStep[0m  [33/339], [94mLoss[0m : 1.81460
[1mStep[0m  [66/339], [94mLoss[0m : 1.52769
[1mStep[0m  [99/339], [94mLoss[0m : 1.97644
[1mStep[0m  [132/339], [94mLoss[0m : 1.35396
[1mStep[0m  [165/339], [94mLoss[0m : 1.39826
[1mStep[0m  [198/339], [94mLoss[0m : 1.55428
[1mStep[0m  [231/339], [94mLoss[0m : 1.41190
[1mStep[0m  [264/339], [94mLoss[0m : 1.79941
[1mStep[0m  [297/339], [94mLoss[0m : 1.63182
[1mStep[0m  [330/339], [94mLoss[0m : 2.05525

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.54755
[1mStep[0m  [33/339], [94mLoss[0m : 2.17916
[1mStep[0m  [66/339], [94mLoss[0m : 1.69468
[1mStep[0m  [99/339], [94mLoss[0m : 1.42410
[1mStep[0m  [132/339], [94mLoss[0m : 1.74304
[1mStep[0m  [165/339], [94mLoss[0m : 1.44577
[1mStep[0m  [198/339], [94mLoss[0m : 1.82685
[1mStep[0m  [231/339], [94mLoss[0m : 1.92940
[1mStep[0m  [264/339], [94mLoss[0m : 1.74389
[1mStep[0m  [297/339], [94mLoss[0m : 1.67230
[1mStep[0m  [330/339], [94mLoss[0m : 1.98695

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59345
[1mStep[0m  [33/339], [94mLoss[0m : 1.63104
[1mStep[0m  [66/339], [94mLoss[0m : 1.86612
[1mStep[0m  [99/339], [94mLoss[0m : 1.76951
[1mStep[0m  [132/339], [94mLoss[0m : 1.89053
[1mStep[0m  [165/339], [94mLoss[0m : 1.56402
[1mStep[0m  [198/339], [94mLoss[0m : 2.01853
[1mStep[0m  [231/339], [94mLoss[0m : 2.06833
[1mStep[0m  [264/339], [94mLoss[0m : 1.27858
[1mStep[0m  [297/339], [94mLoss[0m : 1.31551
[1mStep[0m  [330/339], [94mLoss[0m : 1.84091

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37452
[1mStep[0m  [33/339], [94mLoss[0m : 1.60881
[1mStep[0m  [66/339], [94mLoss[0m : 1.34806
[1mStep[0m  [99/339], [94mLoss[0m : 1.73260
[1mStep[0m  [132/339], [94mLoss[0m : 1.48589
[1mStep[0m  [165/339], [94mLoss[0m : 2.01129
[1mStep[0m  [198/339], [94mLoss[0m : 1.64194
[1mStep[0m  [231/339], [94mLoss[0m : 2.03393
[1mStep[0m  [264/339], [94mLoss[0m : 1.63041
[1mStep[0m  [297/339], [94mLoss[0m : 1.85185
[1mStep[0m  [330/339], [94mLoss[0m : 2.13786

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.539, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35408
[1mStep[0m  [33/339], [94mLoss[0m : 1.71551
[1mStep[0m  [66/339], [94mLoss[0m : 1.82472
[1mStep[0m  [99/339], [94mLoss[0m : 2.39436
[1mStep[0m  [132/339], [94mLoss[0m : 1.23705
[1mStep[0m  [165/339], [94mLoss[0m : 1.76570
[1mStep[0m  [198/339], [94mLoss[0m : 1.45204
[1mStep[0m  [231/339], [94mLoss[0m : 2.23768
[1mStep[0m  [264/339], [94mLoss[0m : 1.48043
[1mStep[0m  [297/339], [94mLoss[0m : 1.88755
[1mStep[0m  [330/339], [94mLoss[0m : 1.97181

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.08094
[1mStep[0m  [33/339], [94mLoss[0m : 1.56719
[1mStep[0m  [66/339], [94mLoss[0m : 2.03749
[1mStep[0m  [99/339], [94mLoss[0m : 1.39528
[1mStep[0m  [132/339], [94mLoss[0m : 1.56992
[1mStep[0m  [165/339], [94mLoss[0m : 1.99185
[1mStep[0m  [198/339], [94mLoss[0m : 1.86034
[1mStep[0m  [231/339], [94mLoss[0m : 2.07154
[1mStep[0m  [264/339], [94mLoss[0m : 1.62090
[1mStep[0m  [297/339], [94mLoss[0m : 1.59079
[1mStep[0m  [330/339], [94mLoss[0m : 2.08560

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45587
[1mStep[0m  [33/339], [94mLoss[0m : 1.58074
[1mStep[0m  [66/339], [94mLoss[0m : 1.53567
[1mStep[0m  [99/339], [94mLoss[0m : 1.70232
[1mStep[0m  [132/339], [94mLoss[0m : 1.40385
[1mStep[0m  [165/339], [94mLoss[0m : 1.93197
[1mStep[0m  [198/339], [94mLoss[0m : 1.31194
[1mStep[0m  [231/339], [94mLoss[0m : 1.51237
[1mStep[0m  [264/339], [94mLoss[0m : 1.65647
[1mStep[0m  [297/339], [94mLoss[0m : 2.25386
[1mStep[0m  [330/339], [94mLoss[0m : 1.61569

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60019
[1mStep[0m  [33/339], [94mLoss[0m : 1.69033
[1mStep[0m  [66/339], [94mLoss[0m : 1.66470
[1mStep[0m  [99/339], [94mLoss[0m : 1.95431
[1mStep[0m  [132/339], [94mLoss[0m : 1.70019
[1mStep[0m  [165/339], [94mLoss[0m : 1.23711
[1mStep[0m  [198/339], [94mLoss[0m : 1.34208
[1mStep[0m  [231/339], [94mLoss[0m : 1.37492
[1mStep[0m  [264/339], [94mLoss[0m : 1.58156
[1mStep[0m  [297/339], [94mLoss[0m : 1.78881
[1mStep[0m  [330/339], [94mLoss[0m : 1.98357

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.533, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.25568
[1mStep[0m  [33/339], [94mLoss[0m : 1.32544
[1mStep[0m  [66/339], [94mLoss[0m : 1.30710
[1mStep[0m  [99/339], [94mLoss[0m : 1.44319
[1mStep[0m  [132/339], [94mLoss[0m : 1.49692
[1mStep[0m  [165/339], [94mLoss[0m : 1.74678
[1mStep[0m  [198/339], [94mLoss[0m : 1.30403
[1mStep[0m  [231/339], [94mLoss[0m : 1.24779
[1mStep[0m  [264/339], [94mLoss[0m : 1.81942
[1mStep[0m  [297/339], [94mLoss[0m : 1.42139
[1mStep[0m  [330/339], [94mLoss[0m : 1.66348

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.549, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42409
[1mStep[0m  [33/339], [94mLoss[0m : 1.36669
[1mStep[0m  [66/339], [94mLoss[0m : 1.35874
[1mStep[0m  [99/339], [94mLoss[0m : 1.49274
[1mStep[0m  [132/339], [94mLoss[0m : 1.75929
[1mStep[0m  [165/339], [94mLoss[0m : 1.76242
[1mStep[0m  [198/339], [94mLoss[0m : 1.42519
[1mStep[0m  [231/339], [94mLoss[0m : 1.48666
[1mStep[0m  [264/339], [94mLoss[0m : 1.72726
[1mStep[0m  [297/339], [94mLoss[0m : 1.62930
[1mStep[0m  [330/339], [94mLoss[0m : 2.20203

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.581, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90316
[1mStep[0m  [33/339], [94mLoss[0m : 1.31512
[1mStep[0m  [66/339], [94mLoss[0m : 1.46673
[1mStep[0m  [99/339], [94mLoss[0m : 1.35287
[1mStep[0m  [132/339], [94mLoss[0m : 1.30286
[1mStep[0m  [165/339], [94mLoss[0m : 2.30152
[1mStep[0m  [198/339], [94mLoss[0m : 1.32629
[1mStep[0m  [231/339], [94mLoss[0m : 1.27682
[1mStep[0m  [264/339], [94mLoss[0m : 1.52241
[1mStep[0m  [297/339], [94mLoss[0m : 1.30153
[1mStep[0m  [330/339], [94mLoss[0m : 1.59882

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37674
[1mStep[0m  [33/339], [94mLoss[0m : 1.64439
[1mStep[0m  [66/339], [94mLoss[0m : 1.16386
[1mStep[0m  [99/339], [94mLoss[0m : 1.67584
[1mStep[0m  [132/339], [94mLoss[0m : 1.33031
[1mStep[0m  [165/339], [94mLoss[0m : 1.67749
[1mStep[0m  [198/339], [94mLoss[0m : 1.66468
[1mStep[0m  [231/339], [94mLoss[0m : 1.39985
[1mStep[0m  [264/339], [94mLoss[0m : 1.71897
[1mStep[0m  [297/339], [94mLoss[0m : 1.39331
[1mStep[0m  [330/339], [94mLoss[0m : 1.68058

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.496, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47725
[1mStep[0m  [33/339], [94mLoss[0m : 0.80853
[1mStep[0m  [66/339], [94mLoss[0m : 1.80482
[1mStep[0m  [99/339], [94mLoss[0m : 1.41488
[1mStep[0m  [132/339], [94mLoss[0m : 1.55879
[1mStep[0m  [165/339], [94mLoss[0m : 1.32543
[1mStep[0m  [198/339], [94mLoss[0m : 1.76890
[1mStep[0m  [231/339], [94mLoss[0m : 1.24455
[1mStep[0m  [264/339], [94mLoss[0m : 1.64178
[1mStep[0m  [297/339], [94mLoss[0m : 1.43405
[1mStep[0m  [330/339], [94mLoss[0m : 1.79063

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.517, [92mTest[0m: 2.570, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66706
[1mStep[0m  [33/339], [94mLoss[0m : 1.03751
[1mStep[0m  [66/339], [94mLoss[0m : 1.30043
[1mStep[0m  [99/339], [94mLoss[0m : 1.11103
[1mStep[0m  [132/339], [94mLoss[0m : 1.66880
[1mStep[0m  [165/339], [94mLoss[0m : 1.60885
[1mStep[0m  [198/339], [94mLoss[0m : 1.34035
[1mStep[0m  [231/339], [94mLoss[0m : 1.56950
[1mStep[0m  [264/339], [94mLoss[0m : 1.54573
[1mStep[0m  [297/339], [94mLoss[0m : 1.17476
[1mStep[0m  [330/339], [94mLoss[0m : 1.44979

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.503, [92mTest[0m: 2.548, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45380
[1mStep[0m  [33/339], [94mLoss[0m : 1.36574
[1mStep[0m  [66/339], [94mLoss[0m : 1.00744
[1mStep[0m  [99/339], [94mLoss[0m : 1.36386
[1mStep[0m  [132/339], [94mLoss[0m : 1.48040
[1mStep[0m  [165/339], [94mLoss[0m : 2.69430
[1mStep[0m  [198/339], [94mLoss[0m : 1.49593
[1mStep[0m  [231/339], [94mLoss[0m : 1.72519
[1mStep[0m  [264/339], [94mLoss[0m : 1.72292
[1mStep[0m  [297/339], [94mLoss[0m : 1.11161
[1mStep[0m  [330/339], [94mLoss[0m : 1.54578

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.534, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24404
[1mStep[0m  [33/339], [94mLoss[0m : 1.83602
[1mStep[0m  [66/339], [94mLoss[0m : 1.65424
[1mStep[0m  [99/339], [94mLoss[0m : 1.28578
[1mStep[0m  [132/339], [94mLoss[0m : 1.49368
[1mStep[0m  [165/339], [94mLoss[0m : 1.35219
[1mStep[0m  [198/339], [94mLoss[0m : 1.14359
[1mStep[0m  [231/339], [94mLoss[0m : 1.27424
[1mStep[0m  [264/339], [94mLoss[0m : 1.29612
[1mStep[0m  [297/339], [94mLoss[0m : 1.69927
[1mStep[0m  [330/339], [94mLoss[0m : 1.52703

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.474, [92mTest[0m: 2.566, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.504
====================================

Phase 2 - Evaluation MAE:  2.5035742046558753
MAE score P1       2.32242
MAE score P2      2.503574
loss              1.474193
learning_rate         0.01
batch_size              32
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.74587
[1mStep[0m  [2/21], [94mLoss[0m : 10.32375
[1mStep[0m  [4/21], [94mLoss[0m : 9.76099
[1mStep[0m  [6/21], [94mLoss[0m : 8.92373
[1mStep[0m  [8/21], [94mLoss[0m : 7.74364
[1mStep[0m  [10/21], [94mLoss[0m : 6.61962
[1mStep[0m  [12/21], [94mLoss[0m : 5.18097
[1mStep[0m  [14/21], [94mLoss[0m : 3.90443
[1mStep[0m  [16/21], [94mLoss[0m : 3.24376
[1mStep[0m  [18/21], [94mLoss[0m : 2.97860
[1mStep[0m  [20/21], [94mLoss[0m : 2.80824

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.525, [92mTest[0m: 10.803, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84970
[1mStep[0m  [2/21], [94mLoss[0m : 3.02650
[1mStep[0m  [4/21], [94mLoss[0m : 3.27546
[1mStep[0m  [6/21], [94mLoss[0m : 2.80229
[1mStep[0m  [8/21], [94mLoss[0m : 3.15945
[1mStep[0m  [10/21], [94mLoss[0m : 2.96526
[1mStep[0m  [12/21], [94mLoss[0m : 2.81387
[1mStep[0m  [14/21], [94mLoss[0m : 2.80959
[1mStep[0m  [16/21], [94mLoss[0m : 2.63799
[1mStep[0m  [18/21], [94mLoss[0m : 2.43764
[1mStep[0m  [20/21], [94mLoss[0m : 2.51631

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.866, [92mTest[0m: 2.885, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67039
[1mStep[0m  [2/21], [94mLoss[0m : 2.47367
[1mStep[0m  [4/21], [94mLoss[0m : 2.51010
[1mStep[0m  [6/21], [94mLoss[0m : 2.69477
[1mStep[0m  [8/21], [94mLoss[0m : 2.50246
[1mStep[0m  [10/21], [94mLoss[0m : 2.58757
[1mStep[0m  [12/21], [94mLoss[0m : 2.57994
[1mStep[0m  [14/21], [94mLoss[0m : 2.37264
[1mStep[0m  [16/21], [94mLoss[0m : 2.65452
[1mStep[0m  [18/21], [94mLoss[0m : 2.56954
[1mStep[0m  [20/21], [94mLoss[0m : 2.50117

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47855
[1mStep[0m  [2/21], [94mLoss[0m : 2.54194
[1mStep[0m  [4/21], [94mLoss[0m : 2.77671
[1mStep[0m  [6/21], [94mLoss[0m : 2.45823
[1mStep[0m  [8/21], [94mLoss[0m : 2.35395
[1mStep[0m  [10/21], [94mLoss[0m : 2.51387
[1mStep[0m  [12/21], [94mLoss[0m : 2.47562
[1mStep[0m  [14/21], [94mLoss[0m : 2.39257
[1mStep[0m  [16/21], [94mLoss[0m : 2.38502
[1mStep[0m  [18/21], [94mLoss[0m : 2.38132
[1mStep[0m  [20/21], [94mLoss[0m : 2.48204

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51446
[1mStep[0m  [2/21], [94mLoss[0m : 2.53388
[1mStep[0m  [4/21], [94mLoss[0m : 2.65433
[1mStep[0m  [6/21], [94mLoss[0m : 2.49659
[1mStep[0m  [8/21], [94mLoss[0m : 2.62786
[1mStep[0m  [10/21], [94mLoss[0m : 2.33910
[1mStep[0m  [12/21], [94mLoss[0m : 2.46994
[1mStep[0m  [14/21], [94mLoss[0m : 2.43069
[1mStep[0m  [16/21], [94mLoss[0m : 2.54801
[1mStep[0m  [18/21], [94mLoss[0m : 2.54032
[1mStep[0m  [20/21], [94mLoss[0m : 2.41559

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48602
[1mStep[0m  [2/21], [94mLoss[0m : 2.48263
[1mStep[0m  [4/21], [94mLoss[0m : 2.56217
[1mStep[0m  [6/21], [94mLoss[0m : 2.59162
[1mStep[0m  [8/21], [94mLoss[0m : 2.41716
[1mStep[0m  [10/21], [94mLoss[0m : 2.47240
[1mStep[0m  [12/21], [94mLoss[0m : 2.52194
[1mStep[0m  [14/21], [94mLoss[0m : 2.48835
[1mStep[0m  [16/21], [94mLoss[0m : 2.53026
[1mStep[0m  [18/21], [94mLoss[0m : 2.46123
[1mStep[0m  [20/21], [94mLoss[0m : 2.42242

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61954
[1mStep[0m  [2/21], [94mLoss[0m : 2.55690
[1mStep[0m  [4/21], [94mLoss[0m : 2.42929
[1mStep[0m  [6/21], [94mLoss[0m : 2.52928
[1mStep[0m  [8/21], [94mLoss[0m : 2.42244
[1mStep[0m  [10/21], [94mLoss[0m : 2.59944
[1mStep[0m  [12/21], [94mLoss[0m : 2.52646
[1mStep[0m  [14/21], [94mLoss[0m : 2.46176
[1mStep[0m  [16/21], [94mLoss[0m : 2.59781
[1mStep[0m  [18/21], [94mLoss[0m : 2.57420
[1mStep[0m  [20/21], [94mLoss[0m : 2.43807

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53968
[1mStep[0m  [2/21], [94mLoss[0m : 2.47796
[1mStep[0m  [4/21], [94mLoss[0m : 2.39926
[1mStep[0m  [6/21], [94mLoss[0m : 2.48374
[1mStep[0m  [8/21], [94mLoss[0m : 2.41467
[1mStep[0m  [10/21], [94mLoss[0m : 2.47446
[1mStep[0m  [12/21], [94mLoss[0m : 2.53734
[1mStep[0m  [14/21], [94mLoss[0m : 2.55536
[1mStep[0m  [16/21], [94mLoss[0m : 2.48501
[1mStep[0m  [18/21], [94mLoss[0m : 2.43707
[1mStep[0m  [20/21], [94mLoss[0m : 2.40187

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45269
[1mStep[0m  [2/21], [94mLoss[0m : 2.54816
[1mStep[0m  [4/21], [94mLoss[0m : 2.50516
[1mStep[0m  [6/21], [94mLoss[0m : 2.60077
[1mStep[0m  [8/21], [94mLoss[0m : 2.64363
[1mStep[0m  [10/21], [94mLoss[0m : 2.55103
[1mStep[0m  [12/21], [94mLoss[0m : 2.26983
[1mStep[0m  [14/21], [94mLoss[0m : 2.36374
[1mStep[0m  [16/21], [94mLoss[0m : 2.55279
[1mStep[0m  [18/21], [94mLoss[0m : 2.41104
[1mStep[0m  [20/21], [94mLoss[0m : 2.46802

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34278
[1mStep[0m  [2/21], [94mLoss[0m : 2.36352
[1mStep[0m  [4/21], [94mLoss[0m : 2.48101
[1mStep[0m  [6/21], [94mLoss[0m : 2.58054
[1mStep[0m  [8/21], [94mLoss[0m : 2.38679
[1mStep[0m  [10/21], [94mLoss[0m : 2.67107
[1mStep[0m  [12/21], [94mLoss[0m : 2.54931
[1mStep[0m  [14/21], [94mLoss[0m : 2.50690
[1mStep[0m  [16/21], [94mLoss[0m : 2.45115
[1mStep[0m  [18/21], [94mLoss[0m : 2.51915
[1mStep[0m  [20/21], [94mLoss[0m : 2.41823

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53927
[1mStep[0m  [2/21], [94mLoss[0m : 2.54092
[1mStep[0m  [4/21], [94mLoss[0m : 2.45969
[1mStep[0m  [6/21], [94mLoss[0m : 2.53498
[1mStep[0m  [8/21], [94mLoss[0m : 2.59603
[1mStep[0m  [10/21], [94mLoss[0m : 2.49103
[1mStep[0m  [12/21], [94mLoss[0m : 2.38278
[1mStep[0m  [14/21], [94mLoss[0m : 2.30477
[1mStep[0m  [16/21], [94mLoss[0m : 2.49410
[1mStep[0m  [18/21], [94mLoss[0m : 2.48532
[1mStep[0m  [20/21], [94mLoss[0m : 2.45481

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47394
[1mStep[0m  [2/21], [94mLoss[0m : 2.39509
[1mStep[0m  [4/21], [94mLoss[0m : 2.55736
[1mStep[0m  [6/21], [94mLoss[0m : 2.61341
[1mStep[0m  [8/21], [94mLoss[0m : 2.44877
[1mStep[0m  [10/21], [94mLoss[0m : 2.53991
[1mStep[0m  [12/21], [94mLoss[0m : 2.53983
[1mStep[0m  [14/21], [94mLoss[0m : 2.29815
[1mStep[0m  [16/21], [94mLoss[0m : 2.54759
[1mStep[0m  [18/21], [94mLoss[0m : 2.50600
[1mStep[0m  [20/21], [94mLoss[0m : 2.35650

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66793
[1mStep[0m  [2/21], [94mLoss[0m : 2.42635
[1mStep[0m  [4/21], [94mLoss[0m : 2.52005
[1mStep[0m  [6/21], [94mLoss[0m : 2.46138
[1mStep[0m  [8/21], [94mLoss[0m : 2.56777
[1mStep[0m  [10/21], [94mLoss[0m : 2.59811
[1mStep[0m  [12/21], [94mLoss[0m : 2.28583
[1mStep[0m  [14/21], [94mLoss[0m : 2.51850
[1mStep[0m  [16/21], [94mLoss[0m : 2.58236
[1mStep[0m  [18/21], [94mLoss[0m : 2.47177
[1mStep[0m  [20/21], [94mLoss[0m : 2.50045

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42878
[1mStep[0m  [2/21], [94mLoss[0m : 2.42291
[1mStep[0m  [4/21], [94mLoss[0m : 2.54355
[1mStep[0m  [6/21], [94mLoss[0m : 2.41922
[1mStep[0m  [8/21], [94mLoss[0m : 2.61153
[1mStep[0m  [10/21], [94mLoss[0m : 2.52462
[1mStep[0m  [12/21], [94mLoss[0m : 2.51777
[1mStep[0m  [14/21], [94mLoss[0m : 2.50236
[1mStep[0m  [16/21], [94mLoss[0m : 2.35756
[1mStep[0m  [18/21], [94mLoss[0m : 2.36302
[1mStep[0m  [20/21], [94mLoss[0m : 2.31074

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53911
[1mStep[0m  [2/21], [94mLoss[0m : 2.38285
[1mStep[0m  [4/21], [94mLoss[0m : 2.33961
[1mStep[0m  [6/21], [94mLoss[0m : 2.48850
[1mStep[0m  [8/21], [94mLoss[0m : 2.63527
[1mStep[0m  [10/21], [94mLoss[0m : 2.45305
[1mStep[0m  [12/21], [94mLoss[0m : 2.40646
[1mStep[0m  [14/21], [94mLoss[0m : 2.40132
[1mStep[0m  [16/21], [94mLoss[0m : 2.50493
[1mStep[0m  [18/21], [94mLoss[0m : 2.46238
[1mStep[0m  [20/21], [94mLoss[0m : 2.34330

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47653
[1mStep[0m  [2/21], [94mLoss[0m : 2.46893
[1mStep[0m  [4/21], [94mLoss[0m : 2.33188
[1mStep[0m  [6/21], [94mLoss[0m : 2.46240
[1mStep[0m  [8/21], [94mLoss[0m : 2.49335
[1mStep[0m  [10/21], [94mLoss[0m : 2.58737
[1mStep[0m  [12/21], [94mLoss[0m : 2.46301
[1mStep[0m  [14/21], [94mLoss[0m : 2.58061
[1mStep[0m  [16/21], [94mLoss[0m : 2.38789
[1mStep[0m  [18/21], [94mLoss[0m : 2.45767
[1mStep[0m  [20/21], [94mLoss[0m : 2.42203

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55143
[1mStep[0m  [2/21], [94mLoss[0m : 2.58752
[1mStep[0m  [4/21], [94mLoss[0m : 2.36967
[1mStep[0m  [6/21], [94mLoss[0m : 2.47487
[1mStep[0m  [8/21], [94mLoss[0m : 2.44615
[1mStep[0m  [10/21], [94mLoss[0m : 2.40646
[1mStep[0m  [12/21], [94mLoss[0m : 2.43447
[1mStep[0m  [14/21], [94mLoss[0m : 2.54214
[1mStep[0m  [16/21], [94mLoss[0m : 2.54500
[1mStep[0m  [18/21], [94mLoss[0m : 2.44878
[1mStep[0m  [20/21], [94mLoss[0m : 2.37454

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54690
[1mStep[0m  [2/21], [94mLoss[0m : 2.57551
[1mStep[0m  [4/21], [94mLoss[0m : 2.57735
[1mStep[0m  [6/21], [94mLoss[0m : 2.28524
[1mStep[0m  [8/21], [94mLoss[0m : 2.32167
[1mStep[0m  [10/21], [94mLoss[0m : 2.45835
[1mStep[0m  [12/21], [94mLoss[0m : 2.53014
[1mStep[0m  [14/21], [94mLoss[0m : 2.43795
[1mStep[0m  [16/21], [94mLoss[0m : 2.53455
[1mStep[0m  [18/21], [94mLoss[0m : 2.46331
[1mStep[0m  [20/21], [94mLoss[0m : 2.41444

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49418
[1mStep[0m  [2/21], [94mLoss[0m : 2.54170
[1mStep[0m  [4/21], [94mLoss[0m : 2.49197
[1mStep[0m  [6/21], [94mLoss[0m : 2.37118
[1mStep[0m  [8/21], [94mLoss[0m : 2.49326
[1mStep[0m  [10/21], [94mLoss[0m : 2.55023
[1mStep[0m  [12/21], [94mLoss[0m : 2.39444
[1mStep[0m  [14/21], [94mLoss[0m : 2.37663
[1mStep[0m  [16/21], [94mLoss[0m : 2.48701
[1mStep[0m  [18/21], [94mLoss[0m : 2.52680
[1mStep[0m  [20/21], [94mLoss[0m : 2.36430

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47085
[1mStep[0m  [2/21], [94mLoss[0m : 2.52381
[1mStep[0m  [4/21], [94mLoss[0m : 2.37344
[1mStep[0m  [6/21], [94mLoss[0m : 2.56038
[1mStep[0m  [8/21], [94mLoss[0m : 2.46243
[1mStep[0m  [10/21], [94mLoss[0m : 2.62831
[1mStep[0m  [12/21], [94mLoss[0m : 2.45537
[1mStep[0m  [14/21], [94mLoss[0m : 2.40383
[1mStep[0m  [16/21], [94mLoss[0m : 2.41364
[1mStep[0m  [18/21], [94mLoss[0m : 2.56548
[1mStep[0m  [20/21], [94mLoss[0m : 2.37280

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37345
[1mStep[0m  [2/21], [94mLoss[0m : 2.44186
[1mStep[0m  [4/21], [94mLoss[0m : 2.41748
[1mStep[0m  [6/21], [94mLoss[0m : 2.34287
[1mStep[0m  [8/21], [94mLoss[0m : 2.46772
[1mStep[0m  [10/21], [94mLoss[0m : 2.55219
[1mStep[0m  [12/21], [94mLoss[0m : 2.48257
[1mStep[0m  [14/21], [94mLoss[0m : 2.57241
[1mStep[0m  [16/21], [94mLoss[0m : 2.42940
[1mStep[0m  [18/21], [94mLoss[0m : 2.39803
[1mStep[0m  [20/21], [94mLoss[0m : 2.51141

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44229
[1mStep[0m  [2/21], [94mLoss[0m : 2.50328
[1mStep[0m  [4/21], [94mLoss[0m : 2.43370
[1mStep[0m  [6/21], [94mLoss[0m : 2.50489
[1mStep[0m  [8/21], [94mLoss[0m : 2.45381
[1mStep[0m  [10/21], [94mLoss[0m : 2.39936
[1mStep[0m  [12/21], [94mLoss[0m : 2.30106
[1mStep[0m  [14/21], [94mLoss[0m : 2.42764
[1mStep[0m  [16/21], [94mLoss[0m : 2.54821
[1mStep[0m  [18/21], [94mLoss[0m : 2.49594
[1mStep[0m  [20/21], [94mLoss[0m : 2.41660

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46353
[1mStep[0m  [2/21], [94mLoss[0m : 2.38044
[1mStep[0m  [4/21], [94mLoss[0m : 2.46249
[1mStep[0m  [6/21], [94mLoss[0m : 2.39008
[1mStep[0m  [8/21], [94mLoss[0m : 2.40927
[1mStep[0m  [10/21], [94mLoss[0m : 2.49119
[1mStep[0m  [12/21], [94mLoss[0m : 2.48724
[1mStep[0m  [14/21], [94mLoss[0m : 2.51334
[1mStep[0m  [16/21], [94mLoss[0m : 2.35650
[1mStep[0m  [18/21], [94mLoss[0m : 2.39869
[1mStep[0m  [20/21], [94mLoss[0m : 2.47070

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42019
[1mStep[0m  [2/21], [94mLoss[0m : 2.38867
[1mStep[0m  [4/21], [94mLoss[0m : 2.40913
[1mStep[0m  [6/21], [94mLoss[0m : 2.51736
[1mStep[0m  [8/21], [94mLoss[0m : 2.59526
[1mStep[0m  [10/21], [94mLoss[0m : 2.58602
[1mStep[0m  [12/21], [94mLoss[0m : 2.28915
[1mStep[0m  [14/21], [94mLoss[0m : 2.50054
[1mStep[0m  [16/21], [94mLoss[0m : 2.57950
[1mStep[0m  [18/21], [94mLoss[0m : 2.50205
[1mStep[0m  [20/21], [94mLoss[0m : 2.30636

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59976
[1mStep[0m  [2/21], [94mLoss[0m : 2.29048
[1mStep[0m  [4/21], [94mLoss[0m : 2.36978
[1mStep[0m  [6/21], [94mLoss[0m : 2.43314
[1mStep[0m  [8/21], [94mLoss[0m : 2.44995
[1mStep[0m  [10/21], [94mLoss[0m : 2.44101
[1mStep[0m  [12/21], [94mLoss[0m : 2.50158
[1mStep[0m  [14/21], [94mLoss[0m : 2.40688
[1mStep[0m  [16/21], [94mLoss[0m : 2.39434
[1mStep[0m  [18/21], [94mLoss[0m : 2.30312
[1mStep[0m  [20/21], [94mLoss[0m : 2.42831

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60578
[1mStep[0m  [2/21], [94mLoss[0m : 2.42681
[1mStep[0m  [4/21], [94mLoss[0m : 2.55130
[1mStep[0m  [6/21], [94mLoss[0m : 2.34940
[1mStep[0m  [8/21], [94mLoss[0m : 2.57883
[1mStep[0m  [10/21], [94mLoss[0m : 2.38479
[1mStep[0m  [12/21], [94mLoss[0m : 2.46320
[1mStep[0m  [14/21], [94mLoss[0m : 2.36390
[1mStep[0m  [16/21], [94mLoss[0m : 2.45482
[1mStep[0m  [18/21], [94mLoss[0m : 2.38188
[1mStep[0m  [20/21], [94mLoss[0m : 2.39979

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47255
[1mStep[0m  [2/21], [94mLoss[0m : 2.46115
[1mStep[0m  [4/21], [94mLoss[0m : 2.56779
[1mStep[0m  [6/21], [94mLoss[0m : 2.69280
[1mStep[0m  [8/21], [94mLoss[0m : 2.62804
[1mStep[0m  [10/21], [94mLoss[0m : 2.42827
[1mStep[0m  [12/21], [94mLoss[0m : 2.37334
[1mStep[0m  [14/21], [94mLoss[0m : 2.26768
[1mStep[0m  [16/21], [94mLoss[0m : 2.51238
[1mStep[0m  [18/21], [94mLoss[0m : 2.57040
[1mStep[0m  [20/21], [94mLoss[0m : 2.42962

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51120
[1mStep[0m  [2/21], [94mLoss[0m : 2.59250
[1mStep[0m  [4/21], [94mLoss[0m : 2.50020
[1mStep[0m  [6/21], [94mLoss[0m : 2.54156
[1mStep[0m  [8/21], [94mLoss[0m : 2.51105
[1mStep[0m  [10/21], [94mLoss[0m : 2.49894
[1mStep[0m  [12/21], [94mLoss[0m : 2.43833
[1mStep[0m  [14/21], [94mLoss[0m : 2.44267
[1mStep[0m  [16/21], [94mLoss[0m : 2.54768
[1mStep[0m  [18/21], [94mLoss[0m : 2.34770
[1mStep[0m  [20/21], [94mLoss[0m : 2.40505

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47780
[1mStep[0m  [2/21], [94mLoss[0m : 2.48905
[1mStep[0m  [4/21], [94mLoss[0m : 2.35329
[1mStep[0m  [6/21], [94mLoss[0m : 2.59122
[1mStep[0m  [8/21], [94mLoss[0m : 2.39105
[1mStep[0m  [10/21], [94mLoss[0m : 2.53047
[1mStep[0m  [12/21], [94mLoss[0m : 2.45321
[1mStep[0m  [14/21], [94mLoss[0m : 2.57566
[1mStep[0m  [16/21], [94mLoss[0m : 2.32651
[1mStep[0m  [18/21], [94mLoss[0m : 2.43770
[1mStep[0m  [20/21], [94mLoss[0m : 2.51817

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41250
[1mStep[0m  [2/21], [94mLoss[0m : 2.34317
[1mStep[0m  [4/21], [94mLoss[0m : 2.28459
[1mStep[0m  [6/21], [94mLoss[0m : 2.34762
[1mStep[0m  [8/21], [94mLoss[0m : 2.57132
[1mStep[0m  [10/21], [94mLoss[0m : 2.44585
[1mStep[0m  [12/21], [94mLoss[0m : 2.26604
[1mStep[0m  [14/21], [94mLoss[0m : 2.55349
[1mStep[0m  [16/21], [94mLoss[0m : 2.43840
[1mStep[0m  [18/21], [94mLoss[0m : 2.49848
[1mStep[0m  [20/21], [94mLoss[0m : 2.52615

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.332103422709874
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.31681
[1mStep[0m  [2/21], [94mLoss[0m : 2.62415
[1mStep[0m  [4/21], [94mLoss[0m : 2.28535
[1mStep[0m  [6/21], [94mLoss[0m : 2.60396
[1mStep[0m  [8/21], [94mLoss[0m : 2.46213
[1mStep[0m  [10/21], [94mLoss[0m : 2.42652
[1mStep[0m  [12/21], [94mLoss[0m : 2.53244
[1mStep[0m  [14/21], [94mLoss[0m : 2.45176
[1mStep[0m  [16/21], [94mLoss[0m : 2.55781
[1mStep[0m  [18/21], [94mLoss[0m : 2.59356
[1mStep[0m  [20/21], [94mLoss[0m : 2.59750

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23868
[1mStep[0m  [2/21], [94mLoss[0m : 2.43522
[1mStep[0m  [4/21], [94mLoss[0m : 2.42206
[1mStep[0m  [6/21], [94mLoss[0m : 2.30993
[1mStep[0m  [8/21], [94mLoss[0m : 2.61504
[1mStep[0m  [10/21], [94mLoss[0m : 2.28918
[1mStep[0m  [12/21], [94mLoss[0m : 2.44710
[1mStep[0m  [14/21], [94mLoss[0m : 2.32280
[1mStep[0m  [16/21], [94mLoss[0m : 2.28568
[1mStep[0m  [18/21], [94mLoss[0m : 2.29446
[1mStep[0m  [20/21], [94mLoss[0m : 2.43064

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22180
[1mStep[0m  [2/21], [94mLoss[0m : 2.29497
[1mStep[0m  [4/21], [94mLoss[0m : 2.36128
[1mStep[0m  [6/21], [94mLoss[0m : 2.36554
[1mStep[0m  [8/21], [94mLoss[0m : 2.32876
[1mStep[0m  [10/21], [94mLoss[0m : 2.34995
[1mStep[0m  [12/21], [94mLoss[0m : 2.34429
[1mStep[0m  [14/21], [94mLoss[0m : 2.22525
[1mStep[0m  [16/21], [94mLoss[0m : 2.28216
[1mStep[0m  [18/21], [94mLoss[0m : 2.38669
[1mStep[0m  [20/21], [94mLoss[0m : 2.28823

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44200
[1mStep[0m  [2/21], [94mLoss[0m : 2.29738
[1mStep[0m  [4/21], [94mLoss[0m : 2.33696
[1mStep[0m  [6/21], [94mLoss[0m : 2.27761
[1mStep[0m  [8/21], [94mLoss[0m : 2.40343
[1mStep[0m  [10/21], [94mLoss[0m : 2.41116
[1mStep[0m  [12/21], [94mLoss[0m : 2.23731
[1mStep[0m  [14/21], [94mLoss[0m : 2.22175
[1mStep[0m  [16/21], [94mLoss[0m : 2.04698
[1mStep[0m  [18/21], [94mLoss[0m : 2.23418
[1mStep[0m  [20/21], [94mLoss[0m : 2.21577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20870
[1mStep[0m  [2/21], [94mLoss[0m : 2.25832
[1mStep[0m  [4/21], [94mLoss[0m : 2.19747
[1mStep[0m  [6/21], [94mLoss[0m : 2.32979
[1mStep[0m  [8/21], [94mLoss[0m : 2.10841
[1mStep[0m  [10/21], [94mLoss[0m : 2.16319
[1mStep[0m  [12/21], [94mLoss[0m : 2.20533
[1mStep[0m  [14/21], [94mLoss[0m : 2.19396
[1mStep[0m  [16/21], [94mLoss[0m : 2.18530
[1mStep[0m  [18/21], [94mLoss[0m : 2.17877
[1mStep[0m  [20/21], [94mLoss[0m : 2.25598

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97409
[1mStep[0m  [2/21], [94mLoss[0m : 2.03327
[1mStep[0m  [4/21], [94mLoss[0m : 2.15288
[1mStep[0m  [6/21], [94mLoss[0m : 2.17750
[1mStep[0m  [8/21], [94mLoss[0m : 2.09720
[1mStep[0m  [10/21], [94mLoss[0m : 2.08533
[1mStep[0m  [12/21], [94mLoss[0m : 2.07525
[1mStep[0m  [14/21], [94mLoss[0m : 2.10306
[1mStep[0m  [16/21], [94mLoss[0m : 2.21451
[1mStep[0m  [18/21], [94mLoss[0m : 2.13704
[1mStep[0m  [20/21], [94mLoss[0m : 2.21676

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11193
[1mStep[0m  [2/21], [94mLoss[0m : 2.13053
[1mStep[0m  [4/21], [94mLoss[0m : 2.14788
[1mStep[0m  [6/21], [94mLoss[0m : 2.06776
[1mStep[0m  [8/21], [94mLoss[0m : 2.07504
[1mStep[0m  [10/21], [94mLoss[0m : 1.98435
[1mStep[0m  [12/21], [94mLoss[0m : 1.99445
[1mStep[0m  [14/21], [94mLoss[0m : 2.01833
[1mStep[0m  [16/21], [94mLoss[0m : 2.08573
[1mStep[0m  [18/21], [94mLoss[0m : 2.01243
[1mStep[0m  [20/21], [94mLoss[0m : 2.04892

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02258
[1mStep[0m  [2/21], [94mLoss[0m : 2.15053
[1mStep[0m  [4/21], [94mLoss[0m : 1.92889
[1mStep[0m  [6/21], [94mLoss[0m : 1.97395
[1mStep[0m  [8/21], [94mLoss[0m : 2.00406
[1mStep[0m  [10/21], [94mLoss[0m : 1.96572
[1mStep[0m  [12/21], [94mLoss[0m : 1.83860
[1mStep[0m  [14/21], [94mLoss[0m : 2.13212
[1mStep[0m  [16/21], [94mLoss[0m : 2.03449
[1mStep[0m  [18/21], [94mLoss[0m : 2.08865
[1mStep[0m  [20/21], [94mLoss[0m : 2.11510

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.80460
[1mStep[0m  [2/21], [94mLoss[0m : 1.93121
[1mStep[0m  [4/21], [94mLoss[0m : 1.84997
[1mStep[0m  [6/21], [94mLoss[0m : 1.95646
[1mStep[0m  [8/21], [94mLoss[0m : 2.04974
[1mStep[0m  [10/21], [94mLoss[0m : 1.96285
[1mStep[0m  [12/21], [94mLoss[0m : 1.89882
[1mStep[0m  [14/21], [94mLoss[0m : 1.87671
[1mStep[0m  [16/21], [94mLoss[0m : 1.96589
[1mStep[0m  [18/21], [94mLoss[0m : 2.03763
[1mStep[0m  [20/21], [94mLoss[0m : 1.98249

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.84450
[1mStep[0m  [2/21], [94mLoss[0m : 1.91788
[1mStep[0m  [4/21], [94mLoss[0m : 1.94683
[1mStep[0m  [6/21], [94mLoss[0m : 1.89312
[1mStep[0m  [8/21], [94mLoss[0m : 1.82106
[1mStep[0m  [10/21], [94mLoss[0m : 1.86032
[1mStep[0m  [12/21], [94mLoss[0m : 1.86028
[1mStep[0m  [14/21], [94mLoss[0m : 1.97036
[1mStep[0m  [16/21], [94mLoss[0m : 1.98055
[1mStep[0m  [18/21], [94mLoss[0m : 1.95139
[1mStep[0m  [20/21], [94mLoss[0m : 1.88256

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.68744
[1mStep[0m  [2/21], [94mLoss[0m : 1.74026
[1mStep[0m  [4/21], [94mLoss[0m : 1.86397
[1mStep[0m  [6/21], [94mLoss[0m : 1.84797
[1mStep[0m  [8/21], [94mLoss[0m : 1.86028
[1mStep[0m  [10/21], [94mLoss[0m : 1.90789
[1mStep[0m  [12/21], [94mLoss[0m : 1.75512
[1mStep[0m  [14/21], [94mLoss[0m : 1.92699
[1mStep[0m  [16/21], [94mLoss[0m : 1.99155
[1mStep[0m  [18/21], [94mLoss[0m : 1.80685
[1mStep[0m  [20/21], [94mLoss[0m : 1.93900

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.85302
[1mStep[0m  [2/21], [94mLoss[0m : 1.97734
[1mStep[0m  [4/21], [94mLoss[0m : 1.67679
[1mStep[0m  [6/21], [94mLoss[0m : 1.77750
[1mStep[0m  [8/21], [94mLoss[0m : 1.72053
[1mStep[0m  [10/21], [94mLoss[0m : 1.79795
[1mStep[0m  [12/21], [94mLoss[0m : 1.86037
[1mStep[0m  [14/21], [94mLoss[0m : 1.90739
[1mStep[0m  [16/21], [94mLoss[0m : 1.87286
[1mStep[0m  [18/21], [94mLoss[0m : 1.95253
[1mStep[0m  [20/21], [94mLoss[0m : 1.72274

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.499, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.73255
[1mStep[0m  [2/21], [94mLoss[0m : 1.80420
[1mStep[0m  [4/21], [94mLoss[0m : 1.74029
[1mStep[0m  [6/21], [94mLoss[0m : 1.70444
[1mStep[0m  [8/21], [94mLoss[0m : 1.69728
[1mStep[0m  [10/21], [94mLoss[0m : 1.77958
[1mStep[0m  [12/21], [94mLoss[0m : 1.77063
[1mStep[0m  [14/21], [94mLoss[0m : 1.82229
[1mStep[0m  [16/21], [94mLoss[0m : 1.71753
[1mStep[0m  [18/21], [94mLoss[0m : 1.67579
[1mStep[0m  [20/21], [94mLoss[0m : 1.78235

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.65297
[1mStep[0m  [2/21], [94mLoss[0m : 1.63488
[1mStep[0m  [4/21], [94mLoss[0m : 1.61882
[1mStep[0m  [6/21], [94mLoss[0m : 1.74379
[1mStep[0m  [8/21], [94mLoss[0m : 1.74790
[1mStep[0m  [10/21], [94mLoss[0m : 1.63907
[1mStep[0m  [12/21], [94mLoss[0m : 1.82042
[1mStep[0m  [14/21], [94mLoss[0m : 1.61847
[1mStep[0m  [16/21], [94mLoss[0m : 1.68998
[1mStep[0m  [18/21], [94mLoss[0m : 1.72694
[1mStep[0m  [20/21], [94mLoss[0m : 1.83354

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.54660
[1mStep[0m  [2/21], [94mLoss[0m : 1.59357
[1mStep[0m  [4/21], [94mLoss[0m : 1.72176
[1mStep[0m  [6/21], [94mLoss[0m : 1.69397
[1mStep[0m  [8/21], [94mLoss[0m : 1.68707
[1mStep[0m  [10/21], [94mLoss[0m : 1.72593
[1mStep[0m  [12/21], [94mLoss[0m : 1.61064
[1mStep[0m  [14/21], [94mLoss[0m : 1.71302
[1mStep[0m  [16/21], [94mLoss[0m : 1.67089
[1mStep[0m  [18/21], [94mLoss[0m : 1.68914
[1mStep[0m  [20/21], [94mLoss[0m : 1.71630

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.688, [92mTest[0m: 2.579, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.48626
[1mStep[0m  [2/21], [94mLoss[0m : 1.60838
[1mStep[0m  [4/21], [94mLoss[0m : 1.56661
[1mStep[0m  [6/21], [94mLoss[0m : 1.65960
[1mStep[0m  [8/21], [94mLoss[0m : 1.64985
[1mStep[0m  [10/21], [94mLoss[0m : 1.60829
[1mStep[0m  [12/21], [94mLoss[0m : 1.65131
[1mStep[0m  [14/21], [94mLoss[0m : 1.62487
[1mStep[0m  [16/21], [94mLoss[0m : 1.77011
[1mStep[0m  [18/21], [94mLoss[0m : 1.67571
[1mStep[0m  [20/21], [94mLoss[0m : 1.59961

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.61348
[1mStep[0m  [2/21], [94mLoss[0m : 1.64670
[1mStep[0m  [4/21], [94mLoss[0m : 1.55482
[1mStep[0m  [6/21], [94mLoss[0m : 1.70451
[1mStep[0m  [8/21], [94mLoss[0m : 1.58004
[1mStep[0m  [10/21], [94mLoss[0m : 1.61415
[1mStep[0m  [12/21], [94mLoss[0m : 1.57556
[1mStep[0m  [14/21], [94mLoss[0m : 1.58813
[1mStep[0m  [16/21], [94mLoss[0m : 1.55114
[1mStep[0m  [18/21], [94mLoss[0m : 1.59020
[1mStep[0m  [20/21], [94mLoss[0m : 1.52601

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.550, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.55837
[1mStep[0m  [2/21], [94mLoss[0m : 1.57428
[1mStep[0m  [4/21], [94mLoss[0m : 1.56636
[1mStep[0m  [6/21], [94mLoss[0m : 1.64163
[1mStep[0m  [8/21], [94mLoss[0m : 1.48555
[1mStep[0m  [10/21], [94mLoss[0m : 1.61138
[1mStep[0m  [12/21], [94mLoss[0m : 1.45764
[1mStep[0m  [14/21], [94mLoss[0m : 1.60710
[1mStep[0m  [16/21], [94mLoss[0m : 1.63192
[1mStep[0m  [18/21], [94mLoss[0m : 1.57930
[1mStep[0m  [20/21], [94mLoss[0m : 1.68642

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.51816
[1mStep[0m  [2/21], [94mLoss[0m : 1.53265
[1mStep[0m  [4/21], [94mLoss[0m : 1.51792
[1mStep[0m  [6/21], [94mLoss[0m : 1.45705
[1mStep[0m  [8/21], [94mLoss[0m : 1.44925
[1mStep[0m  [10/21], [94mLoss[0m : 1.46664
[1mStep[0m  [12/21], [94mLoss[0m : 1.42573
[1mStep[0m  [14/21], [94mLoss[0m : 1.47711
[1mStep[0m  [16/21], [94mLoss[0m : 1.51066
[1mStep[0m  [18/21], [94mLoss[0m : 1.55135
[1mStep[0m  [20/21], [94mLoss[0m : 1.79067

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.569, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.50768
[1mStep[0m  [2/21], [94mLoss[0m : 1.50299
[1mStep[0m  [4/21], [94mLoss[0m : 1.52605
[1mStep[0m  [6/21], [94mLoss[0m : 1.57635
[1mStep[0m  [8/21], [94mLoss[0m : 1.47178
[1mStep[0m  [10/21], [94mLoss[0m : 1.52084
[1mStep[0m  [12/21], [94mLoss[0m : 1.43194
[1mStep[0m  [14/21], [94mLoss[0m : 1.60262
[1mStep[0m  [16/21], [94mLoss[0m : 1.48027
[1mStep[0m  [18/21], [94mLoss[0m : 1.50035
[1mStep[0m  [20/21], [94mLoss[0m : 1.57584

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.46385
[1mStep[0m  [2/21], [94mLoss[0m : 1.49374
[1mStep[0m  [4/21], [94mLoss[0m : 1.48356
[1mStep[0m  [6/21], [94mLoss[0m : 1.44555
[1mStep[0m  [8/21], [94mLoss[0m : 1.54111
[1mStep[0m  [10/21], [94mLoss[0m : 1.56873
[1mStep[0m  [12/21], [94mLoss[0m : 1.47372
[1mStep[0m  [14/21], [94mLoss[0m : 1.52890
[1mStep[0m  [16/21], [94mLoss[0m : 1.50106
[1mStep[0m  [18/21], [94mLoss[0m : 1.46438
[1mStep[0m  [20/21], [94mLoss[0m : 1.40188

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.480, [92mTest[0m: 2.541, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.49034
[1mStep[0m  [2/21], [94mLoss[0m : 1.34665
[1mStep[0m  [4/21], [94mLoss[0m : 1.38098
[1mStep[0m  [6/21], [94mLoss[0m : 1.39211
[1mStep[0m  [8/21], [94mLoss[0m : 1.44824
[1mStep[0m  [10/21], [94mLoss[0m : 1.42629
[1mStep[0m  [12/21], [94mLoss[0m : 1.41757
[1mStep[0m  [14/21], [94mLoss[0m : 1.50689
[1mStep[0m  [16/21], [94mLoss[0m : 1.48500
[1mStep[0m  [18/21], [94mLoss[0m : 1.40706
[1mStep[0m  [20/21], [94mLoss[0m : 1.50859

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.447, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.39871
[1mStep[0m  [2/21], [94mLoss[0m : 1.37333
[1mStep[0m  [4/21], [94mLoss[0m : 1.40365
[1mStep[0m  [6/21], [94mLoss[0m : 1.38283
[1mStep[0m  [8/21], [94mLoss[0m : 1.43820
[1mStep[0m  [10/21], [94mLoss[0m : 1.45225
[1mStep[0m  [12/21], [94mLoss[0m : 1.50163
[1mStep[0m  [14/21], [94mLoss[0m : 1.39665
[1mStep[0m  [16/21], [94mLoss[0m : 1.43682
[1mStep[0m  [18/21], [94mLoss[0m : 1.41103
[1mStep[0m  [20/21], [94mLoss[0m : 1.46385

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.414, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.538
====================================

Phase 2 - Evaluation MAE:  2.537778445652553
MAE score P1       2.332103
MAE score P2       2.537778
loss               1.413802
learning_rate      0.007525
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.50622
[1mStep[0m  [2/21], [94mLoss[0m : 10.87650
[1mStep[0m  [4/21], [94mLoss[0m : 10.24661
[1mStep[0m  [6/21], [94mLoss[0m : 10.04466
[1mStep[0m  [8/21], [94mLoss[0m : 9.72049
[1mStep[0m  [10/21], [94mLoss[0m : 9.33541
[1mStep[0m  [12/21], [94mLoss[0m : 8.67867
[1mStep[0m  [14/21], [94mLoss[0m : 8.20148
[1mStep[0m  [16/21], [94mLoss[0m : 7.94634
[1mStep[0m  [18/21], [94mLoss[0m : 7.05308
[1mStep[0m  [20/21], [94mLoss[0m : 7.12926

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.194, [92mTest[0m: 11.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.86281
[1mStep[0m  [2/21], [94mLoss[0m : 6.14963
[1mStep[0m  [4/21], [94mLoss[0m : 5.78697
[1mStep[0m  [6/21], [94mLoss[0m : 5.50162
[1mStep[0m  [8/21], [94mLoss[0m : 5.31369
[1mStep[0m  [10/21], [94mLoss[0m : 4.60268
[1mStep[0m  [12/21], [94mLoss[0m : 4.50315
[1mStep[0m  [14/21], [94mLoss[0m : 4.13583
[1mStep[0m  [16/21], [94mLoss[0m : 3.81959
[1mStep[0m  [18/21], [94mLoss[0m : 3.67315
[1mStep[0m  [20/21], [94mLoss[0m : 3.62214

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.939, [92mTest[0m: 6.804, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.54947
[1mStep[0m  [2/21], [94mLoss[0m : 3.45284
[1mStep[0m  [4/21], [94mLoss[0m : 3.16941
[1mStep[0m  [6/21], [94mLoss[0m : 3.17460
[1mStep[0m  [8/21], [94mLoss[0m : 3.10843
[1mStep[0m  [10/21], [94mLoss[0m : 2.88248
[1mStep[0m  [12/21], [94mLoss[0m : 2.97706
[1mStep[0m  [14/21], [94mLoss[0m : 3.05206
[1mStep[0m  [16/21], [94mLoss[0m : 2.87815
[1mStep[0m  [18/21], [94mLoss[0m : 2.74581
[1mStep[0m  [20/21], [94mLoss[0m : 2.80647

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.112, [92mTest[0m: 3.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66560
[1mStep[0m  [2/21], [94mLoss[0m : 2.81779
[1mStep[0m  [4/21], [94mLoss[0m : 2.78772
[1mStep[0m  [6/21], [94mLoss[0m : 2.88978
[1mStep[0m  [8/21], [94mLoss[0m : 2.84640
[1mStep[0m  [10/21], [94mLoss[0m : 2.79871
[1mStep[0m  [12/21], [94mLoss[0m : 2.99710
[1mStep[0m  [14/21], [94mLoss[0m : 2.76609
[1mStep[0m  [16/21], [94mLoss[0m : 2.81366
[1mStep[0m  [18/21], [94mLoss[0m : 2.64892
[1mStep[0m  [20/21], [94mLoss[0m : 2.59311

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.723, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67272
[1mStep[0m  [2/21], [94mLoss[0m : 2.64429
[1mStep[0m  [4/21], [94mLoss[0m : 2.54176
[1mStep[0m  [6/21], [94mLoss[0m : 2.61164
[1mStep[0m  [8/21], [94mLoss[0m : 2.54947
[1mStep[0m  [10/21], [94mLoss[0m : 2.48980
[1mStep[0m  [12/21], [94mLoss[0m : 2.67193
[1mStep[0m  [14/21], [94mLoss[0m : 2.51344
[1mStep[0m  [16/21], [94mLoss[0m : 2.84073
[1mStep[0m  [18/21], [94mLoss[0m : 2.75792
[1mStep[0m  [20/21], [94mLoss[0m : 2.86730

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.520, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41291
[1mStep[0m  [2/21], [94mLoss[0m : 2.55872
[1mStep[0m  [4/21], [94mLoss[0m : 2.69627
[1mStep[0m  [6/21], [94mLoss[0m : 2.54271
[1mStep[0m  [8/21], [94mLoss[0m : 2.46264
[1mStep[0m  [10/21], [94mLoss[0m : 2.59263
[1mStep[0m  [12/21], [94mLoss[0m : 2.55084
[1mStep[0m  [14/21], [94mLoss[0m : 2.57660
[1mStep[0m  [16/21], [94mLoss[0m : 2.44107
[1mStep[0m  [18/21], [94mLoss[0m : 2.47789
[1mStep[0m  [20/21], [94mLoss[0m : 2.58769

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68320
[1mStep[0m  [2/21], [94mLoss[0m : 2.54378
[1mStep[0m  [4/21], [94mLoss[0m : 2.50635
[1mStep[0m  [6/21], [94mLoss[0m : 2.54377
[1mStep[0m  [8/21], [94mLoss[0m : 2.50362
[1mStep[0m  [10/21], [94mLoss[0m : 2.58464
[1mStep[0m  [12/21], [94mLoss[0m : 2.49897
[1mStep[0m  [14/21], [94mLoss[0m : 2.83143
[1mStep[0m  [16/21], [94mLoss[0m : 2.49352
[1mStep[0m  [18/21], [94mLoss[0m : 2.58690
[1mStep[0m  [20/21], [94mLoss[0m : 2.71238

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54872
[1mStep[0m  [2/21], [94mLoss[0m : 2.50175
[1mStep[0m  [4/21], [94mLoss[0m : 2.48016
[1mStep[0m  [6/21], [94mLoss[0m : 2.42471
[1mStep[0m  [8/21], [94mLoss[0m : 2.54192
[1mStep[0m  [10/21], [94mLoss[0m : 2.64252
[1mStep[0m  [12/21], [94mLoss[0m : 2.74036
[1mStep[0m  [14/21], [94mLoss[0m : 2.50007
[1mStep[0m  [16/21], [94mLoss[0m : 2.47347
[1mStep[0m  [18/21], [94mLoss[0m : 2.52143
[1mStep[0m  [20/21], [94mLoss[0m : 2.52804

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.434, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63131
[1mStep[0m  [2/21], [94mLoss[0m : 2.46013
[1mStep[0m  [4/21], [94mLoss[0m : 2.51149
[1mStep[0m  [6/21], [94mLoss[0m : 2.57853
[1mStep[0m  [8/21], [94mLoss[0m : 2.50754
[1mStep[0m  [10/21], [94mLoss[0m : 2.58318
[1mStep[0m  [12/21], [94mLoss[0m : 2.46643
[1mStep[0m  [14/21], [94mLoss[0m : 2.56590
[1mStep[0m  [16/21], [94mLoss[0m : 2.58024
[1mStep[0m  [18/21], [94mLoss[0m : 2.49332
[1mStep[0m  [20/21], [94mLoss[0m : 2.53024

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53848
[1mStep[0m  [2/21], [94mLoss[0m : 2.50436
[1mStep[0m  [4/21], [94mLoss[0m : 2.61723
[1mStep[0m  [6/21], [94mLoss[0m : 2.66771
[1mStep[0m  [8/21], [94mLoss[0m : 2.48572
[1mStep[0m  [10/21], [94mLoss[0m : 2.68608
[1mStep[0m  [12/21], [94mLoss[0m : 2.59975
[1mStep[0m  [14/21], [94mLoss[0m : 2.50252
[1mStep[0m  [16/21], [94mLoss[0m : 2.52385
[1mStep[0m  [18/21], [94mLoss[0m : 2.63500
[1mStep[0m  [20/21], [94mLoss[0m : 2.51610

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50465
[1mStep[0m  [2/21], [94mLoss[0m : 2.42399
[1mStep[0m  [4/21], [94mLoss[0m : 2.50200
[1mStep[0m  [6/21], [94mLoss[0m : 2.61255
[1mStep[0m  [8/21], [94mLoss[0m : 2.53563
[1mStep[0m  [10/21], [94mLoss[0m : 2.56094
[1mStep[0m  [12/21], [94mLoss[0m : 2.63776
[1mStep[0m  [14/21], [94mLoss[0m : 2.56851
[1mStep[0m  [16/21], [94mLoss[0m : 2.38591
[1mStep[0m  [18/21], [94mLoss[0m : 2.59942
[1mStep[0m  [20/21], [94mLoss[0m : 2.49335

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54511
[1mStep[0m  [2/21], [94mLoss[0m : 2.64564
[1mStep[0m  [4/21], [94mLoss[0m : 2.50581
[1mStep[0m  [6/21], [94mLoss[0m : 2.30752
[1mStep[0m  [8/21], [94mLoss[0m : 2.55026
[1mStep[0m  [10/21], [94mLoss[0m : 2.40223
[1mStep[0m  [12/21], [94mLoss[0m : 2.46385
[1mStep[0m  [14/21], [94mLoss[0m : 2.58052
[1mStep[0m  [16/21], [94mLoss[0m : 2.59612
[1mStep[0m  [18/21], [94mLoss[0m : 2.42780
[1mStep[0m  [20/21], [94mLoss[0m : 2.51771

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42817
[1mStep[0m  [2/21], [94mLoss[0m : 2.52397
[1mStep[0m  [4/21], [94mLoss[0m : 2.57189
[1mStep[0m  [6/21], [94mLoss[0m : 2.56044
[1mStep[0m  [8/21], [94mLoss[0m : 2.65435
[1mStep[0m  [10/21], [94mLoss[0m : 2.60355
[1mStep[0m  [12/21], [94mLoss[0m : 2.51461
[1mStep[0m  [14/21], [94mLoss[0m : 2.73912
[1mStep[0m  [16/21], [94mLoss[0m : 2.50795
[1mStep[0m  [18/21], [94mLoss[0m : 2.53796
[1mStep[0m  [20/21], [94mLoss[0m : 2.48705

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66970
[1mStep[0m  [2/21], [94mLoss[0m : 2.52383
[1mStep[0m  [4/21], [94mLoss[0m : 2.67694
[1mStep[0m  [6/21], [94mLoss[0m : 2.53943
[1mStep[0m  [8/21], [94mLoss[0m : 2.51466
[1mStep[0m  [10/21], [94mLoss[0m : 2.59942
[1mStep[0m  [12/21], [94mLoss[0m : 2.63240
[1mStep[0m  [14/21], [94mLoss[0m : 2.51181
[1mStep[0m  [16/21], [94mLoss[0m : 2.36342
[1mStep[0m  [18/21], [94mLoss[0m : 2.54380
[1mStep[0m  [20/21], [94mLoss[0m : 2.48146

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50611
[1mStep[0m  [2/21], [94mLoss[0m : 2.67478
[1mStep[0m  [4/21], [94mLoss[0m : 2.61613
[1mStep[0m  [6/21], [94mLoss[0m : 2.41583
[1mStep[0m  [8/21], [94mLoss[0m : 2.50669
[1mStep[0m  [10/21], [94mLoss[0m : 2.59673
[1mStep[0m  [12/21], [94mLoss[0m : 2.32479
[1mStep[0m  [14/21], [94mLoss[0m : 2.56499
[1mStep[0m  [16/21], [94mLoss[0m : 2.53738
[1mStep[0m  [18/21], [94mLoss[0m : 2.58448
[1mStep[0m  [20/21], [94mLoss[0m : 2.57192

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45495
[1mStep[0m  [2/21], [94mLoss[0m : 2.42634
[1mStep[0m  [4/21], [94mLoss[0m : 2.50990
[1mStep[0m  [6/21], [94mLoss[0m : 2.34303
[1mStep[0m  [8/21], [94mLoss[0m : 2.54611
[1mStep[0m  [10/21], [94mLoss[0m : 2.73476
[1mStep[0m  [12/21], [94mLoss[0m : 2.56274
[1mStep[0m  [14/21], [94mLoss[0m : 2.42935
[1mStep[0m  [16/21], [94mLoss[0m : 2.53178
[1mStep[0m  [18/21], [94mLoss[0m : 2.43680
[1mStep[0m  [20/21], [94mLoss[0m : 2.37933

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39428
[1mStep[0m  [2/21], [94mLoss[0m : 2.50277
[1mStep[0m  [4/21], [94mLoss[0m : 2.42484
[1mStep[0m  [6/21], [94mLoss[0m : 2.39384
[1mStep[0m  [8/21], [94mLoss[0m : 2.49791
[1mStep[0m  [10/21], [94mLoss[0m : 2.44102
[1mStep[0m  [12/21], [94mLoss[0m : 2.59020
[1mStep[0m  [14/21], [94mLoss[0m : 2.64787
[1mStep[0m  [16/21], [94mLoss[0m : 2.65063
[1mStep[0m  [18/21], [94mLoss[0m : 2.58818
[1mStep[0m  [20/21], [94mLoss[0m : 2.60219

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60122
[1mStep[0m  [2/21], [94mLoss[0m : 2.58274
[1mStep[0m  [4/21], [94mLoss[0m : 2.63131
[1mStep[0m  [6/21], [94mLoss[0m : 2.46670
[1mStep[0m  [8/21], [94mLoss[0m : 2.53315
[1mStep[0m  [10/21], [94mLoss[0m : 2.61511
[1mStep[0m  [12/21], [94mLoss[0m : 2.51806
[1mStep[0m  [14/21], [94mLoss[0m : 2.55458
[1mStep[0m  [16/21], [94mLoss[0m : 2.48780
[1mStep[0m  [18/21], [94mLoss[0m : 2.50845
[1mStep[0m  [20/21], [94mLoss[0m : 2.50330

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57510
[1mStep[0m  [2/21], [94mLoss[0m : 2.40871
[1mStep[0m  [4/21], [94mLoss[0m : 2.44456
[1mStep[0m  [6/21], [94mLoss[0m : 2.59011
[1mStep[0m  [8/21], [94mLoss[0m : 2.45176
[1mStep[0m  [10/21], [94mLoss[0m : 2.54650
[1mStep[0m  [12/21], [94mLoss[0m : 2.66593
[1mStep[0m  [14/21], [94mLoss[0m : 2.52004
[1mStep[0m  [16/21], [94mLoss[0m : 2.56369
[1mStep[0m  [18/21], [94mLoss[0m : 2.41402
[1mStep[0m  [20/21], [94mLoss[0m : 2.57497

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45046
[1mStep[0m  [2/21], [94mLoss[0m : 2.56638
[1mStep[0m  [4/21], [94mLoss[0m : 2.61120
[1mStep[0m  [6/21], [94mLoss[0m : 2.54037
[1mStep[0m  [8/21], [94mLoss[0m : 2.62882
[1mStep[0m  [10/21], [94mLoss[0m : 2.51878
[1mStep[0m  [12/21], [94mLoss[0m : 2.31818
[1mStep[0m  [14/21], [94mLoss[0m : 2.42388
[1mStep[0m  [16/21], [94mLoss[0m : 2.57635
[1mStep[0m  [18/21], [94mLoss[0m : 2.36718
[1mStep[0m  [20/21], [94mLoss[0m : 2.66316

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.373, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58375
[1mStep[0m  [2/21], [94mLoss[0m : 2.43019
[1mStep[0m  [4/21], [94mLoss[0m : 2.56617
[1mStep[0m  [6/21], [94mLoss[0m : 2.50419
[1mStep[0m  [8/21], [94mLoss[0m : 2.71396
[1mStep[0m  [10/21], [94mLoss[0m : 2.50417
[1mStep[0m  [12/21], [94mLoss[0m : 2.48718
[1mStep[0m  [14/21], [94mLoss[0m : 2.49681
[1mStep[0m  [16/21], [94mLoss[0m : 2.41881
[1mStep[0m  [18/21], [94mLoss[0m : 2.62160
[1mStep[0m  [20/21], [94mLoss[0m : 2.49538

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.374, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41068
[1mStep[0m  [2/21], [94mLoss[0m : 2.51249
[1mStep[0m  [4/21], [94mLoss[0m : 2.64877
[1mStep[0m  [6/21], [94mLoss[0m : 2.65936
[1mStep[0m  [8/21], [94mLoss[0m : 2.65856
[1mStep[0m  [10/21], [94mLoss[0m : 2.46534
[1mStep[0m  [12/21], [94mLoss[0m : 2.49527
[1mStep[0m  [14/21], [94mLoss[0m : 2.59944
[1mStep[0m  [16/21], [94mLoss[0m : 2.55976
[1mStep[0m  [18/21], [94mLoss[0m : 2.47524
[1mStep[0m  [20/21], [94mLoss[0m : 2.67385

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.373, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65739
[1mStep[0m  [2/21], [94mLoss[0m : 2.63008
[1mStep[0m  [4/21], [94mLoss[0m : 2.49522
[1mStep[0m  [6/21], [94mLoss[0m : 2.62678
[1mStep[0m  [8/21], [94mLoss[0m : 2.39785
[1mStep[0m  [10/21], [94mLoss[0m : 2.49097
[1mStep[0m  [12/21], [94mLoss[0m : 2.47411
[1mStep[0m  [14/21], [94mLoss[0m : 2.39480
[1mStep[0m  [16/21], [94mLoss[0m : 2.52163
[1mStep[0m  [18/21], [94mLoss[0m : 2.39859
[1mStep[0m  [20/21], [94mLoss[0m : 2.55831

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57201
[1mStep[0m  [2/21], [94mLoss[0m : 2.36162
[1mStep[0m  [4/21], [94mLoss[0m : 2.54817
[1mStep[0m  [6/21], [94mLoss[0m : 2.47021
[1mStep[0m  [8/21], [94mLoss[0m : 2.62983
[1mStep[0m  [10/21], [94mLoss[0m : 2.64083
[1mStep[0m  [12/21], [94mLoss[0m : 2.43258
[1mStep[0m  [14/21], [94mLoss[0m : 2.49221
[1mStep[0m  [16/21], [94mLoss[0m : 2.43711
[1mStep[0m  [18/21], [94mLoss[0m : 2.66990
[1mStep[0m  [20/21], [94mLoss[0m : 2.38805

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.369, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43392
[1mStep[0m  [2/21], [94mLoss[0m : 2.47502
[1mStep[0m  [4/21], [94mLoss[0m : 2.54310
[1mStep[0m  [6/21], [94mLoss[0m : 2.59333
[1mStep[0m  [8/21], [94mLoss[0m : 2.32362
[1mStep[0m  [10/21], [94mLoss[0m : 2.54523
[1mStep[0m  [12/21], [94mLoss[0m : 2.69991
[1mStep[0m  [14/21], [94mLoss[0m : 2.47110
[1mStep[0m  [16/21], [94mLoss[0m : 2.50226
[1mStep[0m  [18/21], [94mLoss[0m : 2.37003
[1mStep[0m  [20/21], [94mLoss[0m : 2.41840

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.365, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34064
[1mStep[0m  [2/21], [94mLoss[0m : 2.64962
[1mStep[0m  [4/21], [94mLoss[0m : 2.53826
[1mStep[0m  [6/21], [94mLoss[0m : 2.45613
[1mStep[0m  [8/21], [94mLoss[0m : 2.55832
[1mStep[0m  [10/21], [94mLoss[0m : 2.46989
[1mStep[0m  [12/21], [94mLoss[0m : 2.58661
[1mStep[0m  [14/21], [94mLoss[0m : 2.47586
[1mStep[0m  [16/21], [94mLoss[0m : 2.54484
[1mStep[0m  [18/21], [94mLoss[0m : 2.60027
[1mStep[0m  [20/21], [94mLoss[0m : 2.51132

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42849
[1mStep[0m  [2/21], [94mLoss[0m : 2.55674
[1mStep[0m  [4/21], [94mLoss[0m : 2.45548
[1mStep[0m  [6/21], [94mLoss[0m : 2.44155
[1mStep[0m  [8/21], [94mLoss[0m : 2.41889
[1mStep[0m  [10/21], [94mLoss[0m : 2.48981
[1mStep[0m  [12/21], [94mLoss[0m : 2.50773
[1mStep[0m  [14/21], [94mLoss[0m : 2.51193
[1mStep[0m  [16/21], [94mLoss[0m : 2.59402
[1mStep[0m  [18/21], [94mLoss[0m : 2.58328
[1mStep[0m  [20/21], [94mLoss[0m : 2.48545

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50697
[1mStep[0m  [2/21], [94mLoss[0m : 2.54900
[1mStep[0m  [4/21], [94mLoss[0m : 2.50394
[1mStep[0m  [6/21], [94mLoss[0m : 2.49039
[1mStep[0m  [8/21], [94mLoss[0m : 2.42484
[1mStep[0m  [10/21], [94mLoss[0m : 2.47548
[1mStep[0m  [12/21], [94mLoss[0m : 2.42878
[1mStep[0m  [14/21], [94mLoss[0m : 2.55528
[1mStep[0m  [16/21], [94mLoss[0m : 2.62314
[1mStep[0m  [18/21], [94mLoss[0m : 2.46136
[1mStep[0m  [20/21], [94mLoss[0m : 2.49452

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55855
[1mStep[0m  [2/21], [94mLoss[0m : 2.48597
[1mStep[0m  [4/21], [94mLoss[0m : 2.48912
[1mStep[0m  [6/21], [94mLoss[0m : 2.45884
[1mStep[0m  [8/21], [94mLoss[0m : 2.43499
[1mStep[0m  [10/21], [94mLoss[0m : 2.61039
[1mStep[0m  [12/21], [94mLoss[0m : 2.51584
[1mStep[0m  [14/21], [94mLoss[0m : 2.45453
[1mStep[0m  [16/21], [94mLoss[0m : 2.40058
[1mStep[0m  [18/21], [94mLoss[0m : 2.55486
[1mStep[0m  [20/21], [94mLoss[0m : 2.67314

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.361, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47148
[1mStep[0m  [2/21], [94mLoss[0m : 2.60987
[1mStep[0m  [4/21], [94mLoss[0m : 2.42602
[1mStep[0m  [6/21], [94mLoss[0m : 2.46272
[1mStep[0m  [8/21], [94mLoss[0m : 2.49321
[1mStep[0m  [10/21], [94mLoss[0m : 2.50896
[1mStep[0m  [12/21], [94mLoss[0m : 2.45171
[1mStep[0m  [14/21], [94mLoss[0m : 2.38444
[1mStep[0m  [16/21], [94mLoss[0m : 2.53351
[1mStep[0m  [18/21], [94mLoss[0m : 2.52563
[1mStep[0m  [20/21], [94mLoss[0m : 2.67925

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.355654784611293
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.61569
[1mStep[0m  [2/21], [94mLoss[0m : 2.50906
[1mStep[0m  [4/21], [94mLoss[0m : 2.44871
[1mStep[0m  [6/21], [94mLoss[0m : 2.44434
[1mStep[0m  [8/21], [94mLoss[0m : 2.57263
[1mStep[0m  [10/21], [94mLoss[0m : 2.36806
[1mStep[0m  [12/21], [94mLoss[0m : 2.49638
[1mStep[0m  [14/21], [94mLoss[0m : 2.58471
[1mStep[0m  [16/21], [94mLoss[0m : 2.62262
[1mStep[0m  [18/21], [94mLoss[0m : 2.53261
[1mStep[0m  [20/21], [94mLoss[0m : 2.41372

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49217
[1mStep[0m  [2/21], [94mLoss[0m : 2.35493
[1mStep[0m  [4/21], [94mLoss[0m : 2.52226
[1mStep[0m  [6/21], [94mLoss[0m : 2.52850
[1mStep[0m  [8/21], [94mLoss[0m : 2.50842
[1mStep[0m  [10/21], [94mLoss[0m : 2.64706
[1mStep[0m  [12/21], [94mLoss[0m : 2.55832
[1mStep[0m  [14/21], [94mLoss[0m : 2.36368
[1mStep[0m  [16/21], [94mLoss[0m : 2.60899
[1mStep[0m  [18/21], [94mLoss[0m : 2.52078
[1mStep[0m  [20/21], [94mLoss[0m : 2.42717

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63593
[1mStep[0m  [2/21], [94mLoss[0m : 2.45245
[1mStep[0m  [4/21], [94mLoss[0m : 2.51147
[1mStep[0m  [6/21], [94mLoss[0m : 2.41025
[1mStep[0m  [8/21], [94mLoss[0m : 2.53863
[1mStep[0m  [10/21], [94mLoss[0m : 2.30950
[1mStep[0m  [12/21], [94mLoss[0m : 2.48575
[1mStep[0m  [14/21], [94mLoss[0m : 2.49958
[1mStep[0m  [16/21], [94mLoss[0m : 2.49507
[1mStep[0m  [18/21], [94mLoss[0m : 2.31379
[1mStep[0m  [20/21], [94mLoss[0m : 2.40822

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37274
[1mStep[0m  [2/21], [94mLoss[0m : 2.53245
[1mStep[0m  [4/21], [94mLoss[0m : 2.41670
[1mStep[0m  [6/21], [94mLoss[0m : 2.25970
[1mStep[0m  [8/21], [94mLoss[0m : 2.42939
[1mStep[0m  [10/21], [94mLoss[0m : 2.40397
[1mStep[0m  [12/21], [94mLoss[0m : 2.52600
[1mStep[0m  [14/21], [94mLoss[0m : 2.58858
[1mStep[0m  [16/21], [94mLoss[0m : 2.31664
[1mStep[0m  [18/21], [94mLoss[0m : 2.25862
[1mStep[0m  [20/21], [94mLoss[0m : 2.61151

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54027
[1mStep[0m  [2/21], [94mLoss[0m : 2.49115
[1mStep[0m  [4/21], [94mLoss[0m : 2.54789
[1mStep[0m  [6/21], [94mLoss[0m : 2.61213
[1mStep[0m  [8/21], [94mLoss[0m : 2.46857
[1mStep[0m  [10/21], [94mLoss[0m : 2.43422
[1mStep[0m  [12/21], [94mLoss[0m : 2.62977
[1mStep[0m  [14/21], [94mLoss[0m : 2.65302
[1mStep[0m  [16/21], [94mLoss[0m : 2.54441
[1mStep[0m  [18/21], [94mLoss[0m : 2.48544
[1mStep[0m  [20/21], [94mLoss[0m : 2.50635

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33765
[1mStep[0m  [2/21], [94mLoss[0m : 2.45397
[1mStep[0m  [4/21], [94mLoss[0m : 2.45156
[1mStep[0m  [6/21], [94mLoss[0m : 2.46924
[1mStep[0m  [8/21], [94mLoss[0m : 2.34430
[1mStep[0m  [10/21], [94mLoss[0m : 2.49488
[1mStep[0m  [12/21], [94mLoss[0m : 2.47405
[1mStep[0m  [14/21], [94mLoss[0m : 2.51003
[1mStep[0m  [16/21], [94mLoss[0m : 2.56552
[1mStep[0m  [18/21], [94mLoss[0m : 2.56569
[1mStep[0m  [20/21], [94mLoss[0m : 2.36654

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51406
[1mStep[0m  [2/21], [94mLoss[0m : 2.48028
[1mStep[0m  [4/21], [94mLoss[0m : 2.42231
[1mStep[0m  [6/21], [94mLoss[0m : 2.37918
[1mStep[0m  [8/21], [94mLoss[0m : 2.49364
[1mStep[0m  [10/21], [94mLoss[0m : 2.40831
[1mStep[0m  [12/21], [94mLoss[0m : 2.42630
[1mStep[0m  [14/21], [94mLoss[0m : 2.43404
[1mStep[0m  [16/21], [94mLoss[0m : 2.44108
[1mStep[0m  [18/21], [94mLoss[0m : 2.73774
[1mStep[0m  [20/21], [94mLoss[0m : 2.46217

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43572
[1mStep[0m  [2/21], [94mLoss[0m : 2.57183
[1mStep[0m  [4/21], [94mLoss[0m : 2.50872
[1mStep[0m  [6/21], [94mLoss[0m : 2.41555
[1mStep[0m  [8/21], [94mLoss[0m : 2.37082
[1mStep[0m  [10/21], [94mLoss[0m : 2.25284
[1mStep[0m  [12/21], [94mLoss[0m : 2.48852
[1mStep[0m  [14/21], [94mLoss[0m : 2.41013
[1mStep[0m  [16/21], [94mLoss[0m : 2.66757
[1mStep[0m  [18/21], [94mLoss[0m : 2.48954
[1mStep[0m  [20/21], [94mLoss[0m : 2.61607

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39621
[1mStep[0m  [2/21], [94mLoss[0m : 2.46247
[1mStep[0m  [4/21], [94mLoss[0m : 2.43169
[1mStep[0m  [6/21], [94mLoss[0m : 2.46469
[1mStep[0m  [8/21], [94mLoss[0m : 2.42757
[1mStep[0m  [10/21], [94mLoss[0m : 2.38879
[1mStep[0m  [12/21], [94mLoss[0m : 2.42331
[1mStep[0m  [14/21], [94mLoss[0m : 2.36550
[1mStep[0m  [16/21], [94mLoss[0m : 2.46486
[1mStep[0m  [18/21], [94mLoss[0m : 2.50762
[1mStep[0m  [20/21], [94mLoss[0m : 2.46374

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41776
[1mStep[0m  [2/21], [94mLoss[0m : 2.45262
[1mStep[0m  [4/21], [94mLoss[0m : 2.43102
[1mStep[0m  [6/21], [94mLoss[0m : 2.31068
[1mStep[0m  [8/21], [94mLoss[0m : 2.32120
[1mStep[0m  [10/21], [94mLoss[0m : 2.38936
[1mStep[0m  [12/21], [94mLoss[0m : 2.52494
[1mStep[0m  [14/21], [94mLoss[0m : 2.45491
[1mStep[0m  [16/21], [94mLoss[0m : 2.38072
[1mStep[0m  [18/21], [94mLoss[0m : 2.32112
[1mStep[0m  [20/21], [94mLoss[0m : 2.48195

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34424
[1mStep[0m  [2/21], [94mLoss[0m : 2.28247
[1mStep[0m  [4/21], [94mLoss[0m : 2.59896
[1mStep[0m  [6/21], [94mLoss[0m : 2.37463
[1mStep[0m  [8/21], [94mLoss[0m : 2.52498
[1mStep[0m  [10/21], [94mLoss[0m : 2.39071
[1mStep[0m  [12/21], [94mLoss[0m : 2.32766
[1mStep[0m  [14/21], [94mLoss[0m : 2.45378
[1mStep[0m  [16/21], [94mLoss[0m : 2.24451
[1mStep[0m  [18/21], [94mLoss[0m : 2.51179
[1mStep[0m  [20/21], [94mLoss[0m : 2.48037

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28183
[1mStep[0m  [2/21], [94mLoss[0m : 2.32261
[1mStep[0m  [4/21], [94mLoss[0m : 2.31535
[1mStep[0m  [6/21], [94mLoss[0m : 2.41343
[1mStep[0m  [8/21], [94mLoss[0m : 2.40812
[1mStep[0m  [10/21], [94mLoss[0m : 2.37918
[1mStep[0m  [12/21], [94mLoss[0m : 2.35739
[1mStep[0m  [14/21], [94mLoss[0m : 2.43478
[1mStep[0m  [16/21], [94mLoss[0m : 2.51889
[1mStep[0m  [18/21], [94mLoss[0m : 2.43122
[1mStep[0m  [20/21], [94mLoss[0m : 2.49334

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.374, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32967
[1mStep[0m  [2/21], [94mLoss[0m : 2.54350
[1mStep[0m  [4/21], [94mLoss[0m : 2.42433
[1mStep[0m  [6/21], [94mLoss[0m : 2.44095
[1mStep[0m  [8/21], [94mLoss[0m : 2.31345
[1mStep[0m  [10/21], [94mLoss[0m : 2.38219
[1mStep[0m  [12/21], [94mLoss[0m : 2.25160
[1mStep[0m  [14/21], [94mLoss[0m : 2.45500
[1mStep[0m  [16/21], [94mLoss[0m : 2.44753
[1mStep[0m  [18/21], [94mLoss[0m : 2.58305
[1mStep[0m  [20/21], [94mLoss[0m : 2.45878

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50886
[1mStep[0m  [2/21], [94mLoss[0m : 2.38336
[1mStep[0m  [4/21], [94mLoss[0m : 2.23054
[1mStep[0m  [6/21], [94mLoss[0m : 2.41812
[1mStep[0m  [8/21], [94mLoss[0m : 2.42344
[1mStep[0m  [10/21], [94mLoss[0m : 2.39433
[1mStep[0m  [12/21], [94mLoss[0m : 2.40187
[1mStep[0m  [14/21], [94mLoss[0m : 2.51853
[1mStep[0m  [16/21], [94mLoss[0m : 2.37382
[1mStep[0m  [18/21], [94mLoss[0m : 2.40591
[1mStep[0m  [20/21], [94mLoss[0m : 2.27690

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53130
[1mStep[0m  [2/21], [94mLoss[0m : 2.42371
[1mStep[0m  [4/21], [94mLoss[0m : 2.34573
[1mStep[0m  [6/21], [94mLoss[0m : 2.34990
[1mStep[0m  [8/21], [94mLoss[0m : 2.46411
[1mStep[0m  [10/21], [94mLoss[0m : 2.28775
[1mStep[0m  [12/21], [94mLoss[0m : 2.32082
[1mStep[0m  [14/21], [94mLoss[0m : 2.33949
[1mStep[0m  [16/21], [94mLoss[0m : 2.40364
[1mStep[0m  [18/21], [94mLoss[0m : 2.49322
[1mStep[0m  [20/21], [94mLoss[0m : 2.35068

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36120
[1mStep[0m  [2/21], [94mLoss[0m : 2.37634
[1mStep[0m  [4/21], [94mLoss[0m : 2.32684
[1mStep[0m  [6/21], [94mLoss[0m : 2.42166
[1mStep[0m  [8/21], [94mLoss[0m : 2.48149
[1mStep[0m  [10/21], [94mLoss[0m : 2.33887
[1mStep[0m  [12/21], [94mLoss[0m : 2.26634
[1mStep[0m  [14/21], [94mLoss[0m : 2.38066
[1mStep[0m  [16/21], [94mLoss[0m : 2.30111
[1mStep[0m  [18/21], [94mLoss[0m : 2.32882
[1mStep[0m  [20/21], [94mLoss[0m : 2.26580

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.513, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32753
[1mStep[0m  [2/21], [94mLoss[0m : 2.41277
[1mStep[0m  [4/21], [94mLoss[0m : 2.34284
[1mStep[0m  [6/21], [94mLoss[0m : 2.29403
[1mStep[0m  [8/21], [94mLoss[0m : 2.28556
[1mStep[0m  [10/21], [94mLoss[0m : 2.41508
[1mStep[0m  [12/21], [94mLoss[0m : 2.35136
[1mStep[0m  [14/21], [94mLoss[0m : 2.28922
[1mStep[0m  [16/21], [94mLoss[0m : 2.35442
[1mStep[0m  [18/21], [94mLoss[0m : 2.29877
[1mStep[0m  [20/21], [94mLoss[0m : 2.39724

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.546, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34746
[1mStep[0m  [2/21], [94mLoss[0m : 2.41332
[1mStep[0m  [4/21], [94mLoss[0m : 2.10536
[1mStep[0m  [6/21], [94mLoss[0m : 2.31608
[1mStep[0m  [8/21], [94mLoss[0m : 2.32291
[1mStep[0m  [10/21], [94mLoss[0m : 2.37570
[1mStep[0m  [12/21], [94mLoss[0m : 2.36577
[1mStep[0m  [14/21], [94mLoss[0m : 2.26541
[1mStep[0m  [16/21], [94mLoss[0m : 2.36114
[1mStep[0m  [18/21], [94mLoss[0m : 2.34833
[1mStep[0m  [20/21], [94mLoss[0m : 2.48262

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.528, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26911
[1mStep[0m  [2/21], [94mLoss[0m : 2.33947
[1mStep[0m  [4/21], [94mLoss[0m : 2.32575
[1mStep[0m  [6/21], [94mLoss[0m : 2.52071
[1mStep[0m  [8/21], [94mLoss[0m : 2.45373
[1mStep[0m  [10/21], [94mLoss[0m : 2.24348
[1mStep[0m  [12/21], [94mLoss[0m : 2.24084
[1mStep[0m  [14/21], [94mLoss[0m : 2.30102
[1mStep[0m  [16/21], [94mLoss[0m : 2.29315
[1mStep[0m  [18/21], [94mLoss[0m : 2.44407
[1mStep[0m  [20/21], [94mLoss[0m : 2.22676

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24993
[1mStep[0m  [2/21], [94mLoss[0m : 2.35538
[1mStep[0m  [4/21], [94mLoss[0m : 2.35685
[1mStep[0m  [6/21], [94mLoss[0m : 2.16058
[1mStep[0m  [8/21], [94mLoss[0m : 2.34098
[1mStep[0m  [10/21], [94mLoss[0m : 2.44248
[1mStep[0m  [12/21], [94mLoss[0m : 2.10866
[1mStep[0m  [14/21], [94mLoss[0m : 2.43922
[1mStep[0m  [16/21], [94mLoss[0m : 2.18969
[1mStep[0m  [18/21], [94mLoss[0m : 2.31260
[1mStep[0m  [20/21], [94mLoss[0m : 2.26046

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28151
[1mStep[0m  [2/21], [94mLoss[0m : 2.28856
[1mStep[0m  [4/21], [94mLoss[0m : 2.21186
[1mStep[0m  [6/21], [94mLoss[0m : 2.32620
[1mStep[0m  [8/21], [94mLoss[0m : 2.42644
[1mStep[0m  [10/21], [94mLoss[0m : 2.09585
[1mStep[0m  [12/21], [94mLoss[0m : 2.18207
[1mStep[0m  [14/21], [94mLoss[0m : 2.39553
[1mStep[0m  [16/21], [94mLoss[0m : 2.37681
[1mStep[0m  [18/21], [94mLoss[0m : 2.23892
[1mStep[0m  [20/21], [94mLoss[0m : 2.40248

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22526
[1mStep[0m  [2/21], [94mLoss[0m : 2.21633
[1mStep[0m  [4/21], [94mLoss[0m : 2.35519
[1mStep[0m  [6/21], [94mLoss[0m : 2.28496
[1mStep[0m  [8/21], [94mLoss[0m : 2.35974
[1mStep[0m  [10/21], [94mLoss[0m : 2.30168
[1mStep[0m  [12/21], [94mLoss[0m : 2.41552
[1mStep[0m  [14/21], [94mLoss[0m : 2.27210
[1mStep[0m  [16/21], [94mLoss[0m : 2.23909
[1mStep[0m  [18/21], [94mLoss[0m : 2.24529
[1mStep[0m  [20/21], [94mLoss[0m : 2.34715

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.535, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43813
[1mStep[0m  [2/21], [94mLoss[0m : 2.21444
[1mStep[0m  [4/21], [94mLoss[0m : 2.40383
[1mStep[0m  [6/21], [94mLoss[0m : 2.23857
[1mStep[0m  [8/21], [94mLoss[0m : 2.27633
[1mStep[0m  [10/21], [94mLoss[0m : 2.32334
[1mStep[0m  [12/21], [94mLoss[0m : 2.26220
[1mStep[0m  [14/21], [94mLoss[0m : 2.28331
[1mStep[0m  [16/21], [94mLoss[0m : 2.23338
[1mStep[0m  [18/21], [94mLoss[0m : 2.39952
[1mStep[0m  [20/21], [94mLoss[0m : 2.16813

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.563, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11136
[1mStep[0m  [2/21], [94mLoss[0m : 2.52840
[1mStep[0m  [4/21], [94mLoss[0m : 2.38252
[1mStep[0m  [6/21], [94mLoss[0m : 2.21284
[1mStep[0m  [8/21], [94mLoss[0m : 2.16318
[1mStep[0m  [10/21], [94mLoss[0m : 2.39979
[1mStep[0m  [12/21], [94mLoss[0m : 2.33935
[1mStep[0m  [14/21], [94mLoss[0m : 2.13455
[1mStep[0m  [16/21], [94mLoss[0m : 2.42879
[1mStep[0m  [18/21], [94mLoss[0m : 2.26437
[1mStep[0m  [20/21], [94mLoss[0m : 2.23247

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.615, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26286
[1mStep[0m  [2/21], [94mLoss[0m : 2.32747
[1mStep[0m  [4/21], [94mLoss[0m : 2.26923
[1mStep[0m  [6/21], [94mLoss[0m : 2.27156
[1mStep[0m  [8/21], [94mLoss[0m : 2.37061
[1mStep[0m  [10/21], [94mLoss[0m : 2.15281
[1mStep[0m  [12/21], [94mLoss[0m : 2.26525
[1mStep[0m  [14/21], [94mLoss[0m : 2.09472
[1mStep[0m  [16/21], [94mLoss[0m : 2.29513
[1mStep[0m  [18/21], [94mLoss[0m : 2.12701
[1mStep[0m  [20/21], [94mLoss[0m : 2.34315

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.572, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.14607
[1mStep[0m  [2/21], [94mLoss[0m : 2.30813
[1mStep[0m  [4/21], [94mLoss[0m : 2.07212
[1mStep[0m  [6/21], [94mLoss[0m : 2.26965
[1mStep[0m  [8/21], [94mLoss[0m : 2.20057
[1mStep[0m  [10/21], [94mLoss[0m : 2.19540
[1mStep[0m  [12/21], [94mLoss[0m : 2.13703
[1mStep[0m  [14/21], [94mLoss[0m : 1.99629
[1mStep[0m  [16/21], [94mLoss[0m : 2.38217
[1mStep[0m  [18/21], [94mLoss[0m : 2.29928
[1mStep[0m  [20/21], [94mLoss[0m : 2.21812

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.612, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23108
[1mStep[0m  [2/21], [94mLoss[0m : 2.21368
[1mStep[0m  [4/21], [94mLoss[0m : 2.15412
[1mStep[0m  [6/21], [94mLoss[0m : 2.22200
[1mStep[0m  [8/21], [94mLoss[0m : 2.14725
[1mStep[0m  [10/21], [94mLoss[0m : 2.17727
[1mStep[0m  [12/21], [94mLoss[0m : 2.37400
[1mStep[0m  [14/21], [94mLoss[0m : 2.08716
[1mStep[0m  [16/21], [94mLoss[0m : 2.18819
[1mStep[0m  [18/21], [94mLoss[0m : 2.21964
[1mStep[0m  [20/21], [94mLoss[0m : 2.19563

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.656, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10774
[1mStep[0m  [2/21], [94mLoss[0m : 2.26519
[1mStep[0m  [4/21], [94mLoss[0m : 2.17628
[1mStep[0m  [6/21], [94mLoss[0m : 2.26842
[1mStep[0m  [8/21], [94mLoss[0m : 2.25865
[1mStep[0m  [10/21], [94mLoss[0m : 2.28414
[1mStep[0m  [12/21], [94mLoss[0m : 2.21359
[1mStep[0m  [14/21], [94mLoss[0m : 2.31647
[1mStep[0m  [16/21], [94mLoss[0m : 2.23717
[1mStep[0m  [18/21], [94mLoss[0m : 2.20963
[1mStep[0m  [20/21], [94mLoss[0m : 2.17216

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.562, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23257
[1mStep[0m  [2/21], [94mLoss[0m : 2.08964
[1mStep[0m  [4/21], [94mLoss[0m : 2.30062
[1mStep[0m  [6/21], [94mLoss[0m : 2.13762
[1mStep[0m  [8/21], [94mLoss[0m : 2.17452
[1mStep[0m  [10/21], [94mLoss[0m : 2.27031
[1mStep[0m  [12/21], [94mLoss[0m : 2.21632
[1mStep[0m  [14/21], [94mLoss[0m : 2.34833
[1mStep[0m  [16/21], [94mLoss[0m : 2.20299
[1mStep[0m  [18/21], [94mLoss[0m : 2.11157
[1mStep[0m  [20/21], [94mLoss[0m : 2.22206

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.552, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06468
[1mStep[0m  [2/21], [94mLoss[0m : 2.17432
[1mStep[0m  [4/21], [94mLoss[0m : 2.15502
[1mStep[0m  [6/21], [94mLoss[0m : 2.22400
[1mStep[0m  [8/21], [94mLoss[0m : 2.18457
[1mStep[0m  [10/21], [94mLoss[0m : 2.17897
[1mStep[0m  [12/21], [94mLoss[0m : 2.24846
[1mStep[0m  [14/21], [94mLoss[0m : 2.32102
[1mStep[0m  [16/21], [94mLoss[0m : 2.19446
[1mStep[0m  [18/21], [94mLoss[0m : 2.28740
[1mStep[0m  [20/21], [94mLoss[0m : 2.19959

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.630
====================================

Phase 2 - Evaluation MAE:  2.6301794733319963
MAE score P1        2.355655
MAE score P2        2.630179
loss                 2.17458
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay          0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.22393
[1mStep[0m  [2/21], [94mLoss[0m : 10.77285
[1mStep[0m  [4/21], [94mLoss[0m : 11.08822
[1mStep[0m  [6/21], [94mLoss[0m : 10.51607
[1mStep[0m  [8/21], [94mLoss[0m : 10.25682
[1mStep[0m  [10/21], [94mLoss[0m : 9.75606
[1mStep[0m  [12/21], [94mLoss[0m : 9.64017
[1mStep[0m  [14/21], [94mLoss[0m : 9.65607
[1mStep[0m  [16/21], [94mLoss[0m : 8.79718
[1mStep[0m  [18/21], [94mLoss[0m : 8.62614
[1mStep[0m  [20/21], [94mLoss[0m : 8.54323

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.889, [92mTest[0m: 10.954, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.55869
[1mStep[0m  [2/21], [94mLoss[0m : 8.08081
[1mStep[0m  [4/21], [94mLoss[0m : 7.79782
[1mStep[0m  [6/21], [94mLoss[0m : 7.74252
[1mStep[0m  [8/21], [94mLoss[0m : 7.49815
[1mStep[0m  [10/21], [94mLoss[0m : 7.13158
[1mStep[0m  [12/21], [94mLoss[0m : 6.74493
[1mStep[0m  [14/21], [94mLoss[0m : 6.68673
[1mStep[0m  [16/21], [94mLoss[0m : 6.14827
[1mStep[0m  [18/21], [94mLoss[0m : 5.84617
[1mStep[0m  [20/21], [94mLoss[0m : 5.96704

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.118, [92mTest[0m: 9.841, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.86508
[1mStep[0m  [2/21], [94mLoss[0m : 5.24240
[1mStep[0m  [4/21], [94mLoss[0m : 5.24165
[1mStep[0m  [6/21], [94mLoss[0m : 4.89994
[1mStep[0m  [8/21], [94mLoss[0m : 4.72859
[1mStep[0m  [10/21], [94mLoss[0m : 4.40906
[1mStep[0m  [12/21], [94mLoss[0m : 4.01855
[1mStep[0m  [14/21], [94mLoss[0m : 4.06861
[1mStep[0m  [16/21], [94mLoss[0m : 3.52017
[1mStep[0m  [18/21], [94mLoss[0m : 3.86352
[1mStep[0m  [20/21], [94mLoss[0m : 3.05033

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.471, [92mTest[0m: 7.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.28307
[1mStep[0m  [2/21], [94mLoss[0m : 3.48817
[1mStep[0m  [4/21], [94mLoss[0m : 3.25427
[1mStep[0m  [6/21], [94mLoss[0m : 3.16399
[1mStep[0m  [8/21], [94mLoss[0m : 2.86979
[1mStep[0m  [10/21], [94mLoss[0m : 3.18903
[1mStep[0m  [12/21], [94mLoss[0m : 3.13342
[1mStep[0m  [14/21], [94mLoss[0m : 2.95934
[1mStep[0m  [16/21], [94mLoss[0m : 2.88635
[1mStep[0m  [18/21], [94mLoss[0m : 3.08181
[1mStep[0m  [20/21], [94mLoss[0m : 3.13080

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.160, [92mTest[0m: 5.107, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.86949
[1mStep[0m  [2/21], [94mLoss[0m : 2.88576
[1mStep[0m  [4/21], [94mLoss[0m : 2.93443
[1mStep[0m  [6/21], [94mLoss[0m : 2.91692
[1mStep[0m  [8/21], [94mLoss[0m : 2.83780
[1mStep[0m  [10/21], [94mLoss[0m : 2.97626
[1mStep[0m  [12/21], [94mLoss[0m : 3.00282
[1mStep[0m  [14/21], [94mLoss[0m : 2.88666
[1mStep[0m  [16/21], [94mLoss[0m : 2.78917
[1mStep[0m  [18/21], [94mLoss[0m : 2.84813
[1mStep[0m  [20/21], [94mLoss[0m : 2.93483

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.898, [92mTest[0m: 3.591, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.91285
[1mStep[0m  [2/21], [94mLoss[0m : 2.62832
[1mStep[0m  [4/21], [94mLoss[0m : 2.86134
[1mStep[0m  [6/21], [94mLoss[0m : 2.80106
[1mStep[0m  [8/21], [94mLoss[0m : 2.78348
[1mStep[0m  [10/21], [94mLoss[0m : 2.90125
[1mStep[0m  [12/21], [94mLoss[0m : 2.65501
[1mStep[0m  [14/21], [94mLoss[0m : 2.57210
[1mStep[0m  [16/21], [94mLoss[0m : 2.70958
[1mStep[0m  [18/21], [94mLoss[0m : 2.72561
[1mStep[0m  [20/21], [94mLoss[0m : 3.00323

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.807, [92mTest[0m: 3.060, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.86912
[1mStep[0m  [2/21], [94mLoss[0m : 2.80378
[1mStep[0m  [4/21], [94mLoss[0m : 2.90810
[1mStep[0m  [6/21], [94mLoss[0m : 2.74044
[1mStep[0m  [8/21], [94mLoss[0m : 2.69908
[1mStep[0m  [10/21], [94mLoss[0m : 2.64266
[1mStep[0m  [12/21], [94mLoss[0m : 2.64081
[1mStep[0m  [14/21], [94mLoss[0m : 2.88829
[1mStep[0m  [16/21], [94mLoss[0m : 2.81274
[1mStep[0m  [18/21], [94mLoss[0m : 2.69595
[1mStep[0m  [20/21], [94mLoss[0m : 2.79357

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.770, [92mTest[0m: 2.733, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78798
[1mStep[0m  [2/21], [94mLoss[0m : 2.80539
[1mStep[0m  [4/21], [94mLoss[0m : 2.90440
[1mStep[0m  [6/21], [94mLoss[0m : 2.70833
[1mStep[0m  [8/21], [94mLoss[0m : 2.68067
[1mStep[0m  [10/21], [94mLoss[0m : 2.76009
[1mStep[0m  [12/21], [94mLoss[0m : 2.83356
[1mStep[0m  [14/21], [94mLoss[0m : 2.71976
[1mStep[0m  [16/21], [94mLoss[0m : 2.93513
[1mStep[0m  [18/21], [94mLoss[0m : 2.72037
[1mStep[0m  [20/21], [94mLoss[0m : 2.78465

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.761, [92mTest[0m: 2.758, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75011
[1mStep[0m  [2/21], [94mLoss[0m : 2.97284
[1mStep[0m  [4/21], [94mLoss[0m : 2.64646
[1mStep[0m  [6/21], [94mLoss[0m : 2.71557
[1mStep[0m  [8/21], [94mLoss[0m : 2.70776
[1mStep[0m  [10/21], [94mLoss[0m : 2.82479
[1mStep[0m  [12/21], [94mLoss[0m : 2.62080
[1mStep[0m  [14/21], [94mLoss[0m : 2.67940
[1mStep[0m  [16/21], [94mLoss[0m : 2.68978
[1mStep[0m  [18/21], [94mLoss[0m : 2.69969
[1mStep[0m  [20/21], [94mLoss[0m : 2.68505

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.744, [92mTest[0m: 2.652, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66377
[1mStep[0m  [2/21], [94mLoss[0m : 2.72731
[1mStep[0m  [4/21], [94mLoss[0m : 2.75085
[1mStep[0m  [6/21], [94mLoss[0m : 2.76207
[1mStep[0m  [8/21], [94mLoss[0m : 2.64455
[1mStep[0m  [10/21], [94mLoss[0m : 2.57908
[1mStep[0m  [12/21], [94mLoss[0m : 2.82467
[1mStep[0m  [14/21], [94mLoss[0m : 2.73590
[1mStep[0m  [16/21], [94mLoss[0m : 2.79720
[1mStep[0m  [18/21], [94mLoss[0m : 2.66721
[1mStep[0m  [20/21], [94mLoss[0m : 2.75343

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.583, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64691
[1mStep[0m  [2/21], [94mLoss[0m : 2.75262
[1mStep[0m  [4/21], [94mLoss[0m : 2.60734
[1mStep[0m  [6/21], [94mLoss[0m : 2.85874
[1mStep[0m  [8/21], [94mLoss[0m : 2.77491
[1mStep[0m  [10/21], [94mLoss[0m : 2.92713
[1mStep[0m  [12/21], [94mLoss[0m : 2.93090
[1mStep[0m  [14/21], [94mLoss[0m : 2.58432
[1mStep[0m  [16/21], [94mLoss[0m : 2.79437
[1mStep[0m  [18/21], [94mLoss[0m : 2.66290
[1mStep[0m  [20/21], [94mLoss[0m : 2.84718

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.571, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53707
[1mStep[0m  [2/21], [94mLoss[0m : 2.66462
[1mStep[0m  [4/21], [94mLoss[0m : 2.67889
[1mStep[0m  [6/21], [94mLoss[0m : 2.84023
[1mStep[0m  [8/21], [94mLoss[0m : 2.53751
[1mStep[0m  [10/21], [94mLoss[0m : 2.71955
[1mStep[0m  [12/21], [94mLoss[0m : 2.73891
[1mStep[0m  [14/21], [94mLoss[0m : 2.72963
[1mStep[0m  [16/21], [94mLoss[0m : 2.72285
[1mStep[0m  [18/21], [94mLoss[0m : 2.89634
[1mStep[0m  [20/21], [94mLoss[0m : 2.72574

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.528, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77904
[1mStep[0m  [2/21], [94mLoss[0m : 2.75677
[1mStep[0m  [4/21], [94mLoss[0m : 2.73546
[1mStep[0m  [6/21], [94mLoss[0m : 2.85103
[1mStep[0m  [8/21], [94mLoss[0m : 2.77616
[1mStep[0m  [10/21], [94mLoss[0m : 2.71434
[1mStep[0m  [12/21], [94mLoss[0m : 2.62547
[1mStep[0m  [14/21], [94mLoss[0m : 2.49505
[1mStep[0m  [16/21], [94mLoss[0m : 2.74778
[1mStep[0m  [18/21], [94mLoss[0m : 2.71072
[1mStep[0m  [20/21], [94mLoss[0m : 2.66723

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78065
[1mStep[0m  [2/21], [94mLoss[0m : 2.61315
[1mStep[0m  [4/21], [94mLoss[0m : 2.50959
[1mStep[0m  [6/21], [94mLoss[0m : 2.61988
[1mStep[0m  [8/21], [94mLoss[0m : 2.74876
[1mStep[0m  [10/21], [94mLoss[0m : 2.64875
[1mStep[0m  [12/21], [94mLoss[0m : 2.62696
[1mStep[0m  [14/21], [94mLoss[0m : 2.71107
[1mStep[0m  [16/21], [94mLoss[0m : 2.89781
[1mStep[0m  [18/21], [94mLoss[0m : 2.56649
[1mStep[0m  [20/21], [94mLoss[0m : 2.77083

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.88595
[1mStep[0m  [2/21], [94mLoss[0m : 2.64497
[1mStep[0m  [4/21], [94mLoss[0m : 2.58820
[1mStep[0m  [6/21], [94mLoss[0m : 2.61989
[1mStep[0m  [8/21], [94mLoss[0m : 2.66619
[1mStep[0m  [10/21], [94mLoss[0m : 2.75816
[1mStep[0m  [12/21], [94mLoss[0m : 2.79771
[1mStep[0m  [14/21], [94mLoss[0m : 2.68619
[1mStep[0m  [16/21], [94mLoss[0m : 2.80610
[1mStep[0m  [18/21], [94mLoss[0m : 2.70667
[1mStep[0m  [20/21], [94mLoss[0m : 2.73262

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69119
[1mStep[0m  [2/21], [94mLoss[0m : 2.61870
[1mStep[0m  [4/21], [94mLoss[0m : 2.64291
[1mStep[0m  [6/21], [94mLoss[0m : 2.84191
[1mStep[0m  [8/21], [94mLoss[0m : 2.67688
[1mStep[0m  [10/21], [94mLoss[0m : 2.68617
[1mStep[0m  [12/21], [94mLoss[0m : 2.82440
[1mStep[0m  [14/21], [94mLoss[0m : 2.75666
[1mStep[0m  [16/21], [94mLoss[0m : 2.63918
[1mStep[0m  [18/21], [94mLoss[0m : 2.67646
[1mStep[0m  [20/21], [94mLoss[0m : 2.65238

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.88152
[1mStep[0m  [2/21], [94mLoss[0m : 2.64523
[1mStep[0m  [4/21], [94mLoss[0m : 2.75006
[1mStep[0m  [6/21], [94mLoss[0m : 2.55039
[1mStep[0m  [8/21], [94mLoss[0m : 2.57529
[1mStep[0m  [10/21], [94mLoss[0m : 2.52438
[1mStep[0m  [12/21], [94mLoss[0m : 2.54546
[1mStep[0m  [14/21], [94mLoss[0m : 2.74429
[1mStep[0m  [16/21], [94mLoss[0m : 2.63289
[1mStep[0m  [18/21], [94mLoss[0m : 2.67097
[1mStep[0m  [20/21], [94mLoss[0m : 2.86237

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59572
[1mStep[0m  [2/21], [94mLoss[0m : 2.64691
[1mStep[0m  [4/21], [94mLoss[0m : 2.80668
[1mStep[0m  [6/21], [94mLoss[0m : 2.51795
[1mStep[0m  [8/21], [94mLoss[0m : 2.68254
[1mStep[0m  [10/21], [94mLoss[0m : 2.58929
[1mStep[0m  [12/21], [94mLoss[0m : 2.74614
[1mStep[0m  [14/21], [94mLoss[0m : 2.64147
[1mStep[0m  [16/21], [94mLoss[0m : 2.53401
[1mStep[0m  [18/21], [94mLoss[0m : 2.63099
[1mStep[0m  [20/21], [94mLoss[0m : 2.63339

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46718
[1mStep[0m  [2/21], [94mLoss[0m : 2.52866
[1mStep[0m  [4/21], [94mLoss[0m : 2.75342
[1mStep[0m  [6/21], [94mLoss[0m : 2.80396
[1mStep[0m  [8/21], [94mLoss[0m : 2.84826
[1mStep[0m  [10/21], [94mLoss[0m : 2.64082
[1mStep[0m  [12/21], [94mLoss[0m : 2.72399
[1mStep[0m  [14/21], [94mLoss[0m : 2.58314
[1mStep[0m  [16/21], [94mLoss[0m : 2.75882
[1mStep[0m  [18/21], [94mLoss[0m : 2.56000
[1mStep[0m  [20/21], [94mLoss[0m : 2.76491

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60196
[1mStep[0m  [2/21], [94mLoss[0m : 2.69785
[1mStep[0m  [4/21], [94mLoss[0m : 2.67826
[1mStep[0m  [6/21], [94mLoss[0m : 2.55390
[1mStep[0m  [8/21], [94mLoss[0m : 2.72108
[1mStep[0m  [10/21], [94mLoss[0m : 2.68813
[1mStep[0m  [12/21], [94mLoss[0m : 2.75840
[1mStep[0m  [14/21], [94mLoss[0m : 2.73122
[1mStep[0m  [16/21], [94mLoss[0m : 2.55809
[1mStep[0m  [18/21], [94mLoss[0m : 2.47809
[1mStep[0m  [20/21], [94mLoss[0m : 2.62623

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62555
[1mStep[0m  [2/21], [94mLoss[0m : 2.61331
[1mStep[0m  [4/21], [94mLoss[0m : 2.68341
[1mStep[0m  [6/21], [94mLoss[0m : 2.60214
[1mStep[0m  [8/21], [94mLoss[0m : 2.69588
[1mStep[0m  [10/21], [94mLoss[0m : 2.69585
[1mStep[0m  [12/21], [94mLoss[0m : 2.59577
[1mStep[0m  [14/21], [94mLoss[0m : 2.64964
[1mStep[0m  [16/21], [94mLoss[0m : 2.71896
[1mStep[0m  [18/21], [94mLoss[0m : 2.71746
[1mStep[0m  [20/21], [94mLoss[0m : 2.77052

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.436, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45905
[1mStep[0m  [2/21], [94mLoss[0m : 2.58162
[1mStep[0m  [4/21], [94mLoss[0m : 2.74269
[1mStep[0m  [6/21], [94mLoss[0m : 2.59437
[1mStep[0m  [8/21], [94mLoss[0m : 2.53684
[1mStep[0m  [10/21], [94mLoss[0m : 2.41158
[1mStep[0m  [12/21], [94mLoss[0m : 2.76525
[1mStep[0m  [14/21], [94mLoss[0m : 2.75332
[1mStep[0m  [16/21], [94mLoss[0m : 2.51310
[1mStep[0m  [18/21], [94mLoss[0m : 2.68955
[1mStep[0m  [20/21], [94mLoss[0m : 2.58180

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.404, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56322
[1mStep[0m  [2/21], [94mLoss[0m : 2.37400
[1mStep[0m  [4/21], [94mLoss[0m : 2.64363
[1mStep[0m  [6/21], [94mLoss[0m : 2.52969
[1mStep[0m  [8/21], [94mLoss[0m : 2.82909
[1mStep[0m  [10/21], [94mLoss[0m : 2.56947
[1mStep[0m  [12/21], [94mLoss[0m : 2.52184
[1mStep[0m  [14/21], [94mLoss[0m : 2.55932
[1mStep[0m  [16/21], [94mLoss[0m : 2.63624
[1mStep[0m  [18/21], [94mLoss[0m : 2.60375
[1mStep[0m  [20/21], [94mLoss[0m : 2.57059

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.403, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66531
[1mStep[0m  [2/21], [94mLoss[0m : 2.71250
[1mStep[0m  [4/21], [94mLoss[0m : 2.75781
[1mStep[0m  [6/21], [94mLoss[0m : 2.66565
[1mStep[0m  [8/21], [94mLoss[0m : 2.53090
[1mStep[0m  [10/21], [94mLoss[0m : 2.54785
[1mStep[0m  [12/21], [94mLoss[0m : 2.54661
[1mStep[0m  [14/21], [94mLoss[0m : 2.52292
[1mStep[0m  [16/21], [94mLoss[0m : 2.60412
[1mStep[0m  [18/21], [94mLoss[0m : 2.55910
[1mStep[0m  [20/21], [94mLoss[0m : 2.51409

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.413, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70980
[1mStep[0m  [2/21], [94mLoss[0m : 2.68724
[1mStep[0m  [4/21], [94mLoss[0m : 2.50039
[1mStep[0m  [6/21], [94mLoss[0m : 2.44453
[1mStep[0m  [8/21], [94mLoss[0m : 2.66688
[1mStep[0m  [10/21], [94mLoss[0m : 2.63975
[1mStep[0m  [12/21], [94mLoss[0m : 2.60013
[1mStep[0m  [14/21], [94mLoss[0m : 2.62314
[1mStep[0m  [16/21], [94mLoss[0m : 2.43568
[1mStep[0m  [18/21], [94mLoss[0m : 2.60875
[1mStep[0m  [20/21], [94mLoss[0m : 2.57341

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68805
[1mStep[0m  [2/21], [94mLoss[0m : 2.63343
[1mStep[0m  [4/21], [94mLoss[0m : 2.65373
[1mStep[0m  [6/21], [94mLoss[0m : 2.62751
[1mStep[0m  [8/21], [94mLoss[0m : 2.65122
[1mStep[0m  [10/21], [94mLoss[0m : 2.59611
[1mStep[0m  [12/21], [94mLoss[0m : 2.70699
[1mStep[0m  [14/21], [94mLoss[0m : 2.63364
[1mStep[0m  [16/21], [94mLoss[0m : 2.51834
[1mStep[0m  [18/21], [94mLoss[0m : 2.62250
[1mStep[0m  [20/21], [94mLoss[0m : 2.63680

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68932
[1mStep[0m  [2/21], [94mLoss[0m : 2.60313
[1mStep[0m  [4/21], [94mLoss[0m : 2.69153
[1mStep[0m  [6/21], [94mLoss[0m : 2.49372
[1mStep[0m  [8/21], [94mLoss[0m : 2.61765
[1mStep[0m  [10/21], [94mLoss[0m : 2.49984
[1mStep[0m  [12/21], [94mLoss[0m : 2.61650
[1mStep[0m  [14/21], [94mLoss[0m : 2.63655
[1mStep[0m  [16/21], [94mLoss[0m : 2.66452
[1mStep[0m  [18/21], [94mLoss[0m : 2.48584
[1mStep[0m  [20/21], [94mLoss[0m : 2.54585

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.390, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57952
[1mStep[0m  [2/21], [94mLoss[0m : 2.51518
[1mStep[0m  [4/21], [94mLoss[0m : 2.56078
[1mStep[0m  [6/21], [94mLoss[0m : 2.61472
[1mStep[0m  [8/21], [94mLoss[0m : 2.75703
[1mStep[0m  [10/21], [94mLoss[0m : 2.59728
[1mStep[0m  [12/21], [94mLoss[0m : 2.63670
[1mStep[0m  [14/21], [94mLoss[0m : 2.59812
[1mStep[0m  [16/21], [94mLoss[0m : 2.54247
[1mStep[0m  [18/21], [94mLoss[0m : 2.70201
[1mStep[0m  [20/21], [94mLoss[0m : 2.62517

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.384, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57197
[1mStep[0m  [2/21], [94mLoss[0m : 2.57885
[1mStep[0m  [4/21], [94mLoss[0m : 2.52830
[1mStep[0m  [6/21], [94mLoss[0m : 2.78453
[1mStep[0m  [8/21], [94mLoss[0m : 2.44122
[1mStep[0m  [10/21], [94mLoss[0m : 2.53430
[1mStep[0m  [12/21], [94mLoss[0m : 2.63406
[1mStep[0m  [14/21], [94mLoss[0m : 2.65623
[1mStep[0m  [16/21], [94mLoss[0m : 2.47020
[1mStep[0m  [18/21], [94mLoss[0m : 2.59083
[1mStep[0m  [20/21], [94mLoss[0m : 2.49895

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36061
[1mStep[0m  [2/21], [94mLoss[0m : 2.45360
[1mStep[0m  [4/21], [94mLoss[0m : 2.72747
[1mStep[0m  [6/21], [94mLoss[0m : 2.50264
[1mStep[0m  [8/21], [94mLoss[0m : 2.62277
[1mStep[0m  [10/21], [94mLoss[0m : 2.60302
[1mStep[0m  [12/21], [94mLoss[0m : 2.79470
[1mStep[0m  [14/21], [94mLoss[0m : 2.53307
[1mStep[0m  [16/21], [94mLoss[0m : 2.52971
[1mStep[0m  [18/21], [94mLoss[0m : 2.54431
[1mStep[0m  [20/21], [94mLoss[0m : 2.59279

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.358, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.383
====================================

Phase 1 - Evaluation MAE:  2.3825208800179616
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.49343
[1mStep[0m  [2/21], [94mLoss[0m : 2.71817
[1mStep[0m  [4/21], [94mLoss[0m : 2.60993
[1mStep[0m  [6/21], [94mLoss[0m : 2.60145
[1mStep[0m  [8/21], [94mLoss[0m : 2.63040
[1mStep[0m  [10/21], [94mLoss[0m : 2.42316
[1mStep[0m  [12/21], [94mLoss[0m : 2.89928
[1mStep[0m  [14/21], [94mLoss[0m : 2.62658
[1mStep[0m  [16/21], [94mLoss[0m : 2.62959
[1mStep[0m  [18/21], [94mLoss[0m : 2.91767
[1mStep[0m  [20/21], [94mLoss[0m : 2.89347

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56982
[1mStep[0m  [2/21], [94mLoss[0m : 2.54310
[1mStep[0m  [4/21], [94mLoss[0m : 2.53859
[1mStep[0m  [6/21], [94mLoss[0m : 2.53271
[1mStep[0m  [8/21], [94mLoss[0m : 2.71540
[1mStep[0m  [10/21], [94mLoss[0m : 2.50599
[1mStep[0m  [12/21], [94mLoss[0m : 2.70192
[1mStep[0m  [14/21], [94mLoss[0m : 2.68642
[1mStep[0m  [16/21], [94mLoss[0m : 2.89064
[1mStep[0m  [18/21], [94mLoss[0m : 2.71386
[1mStep[0m  [20/21], [94mLoss[0m : 2.43240

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.553, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62027
[1mStep[0m  [2/21], [94mLoss[0m : 2.84147
[1mStep[0m  [4/21], [94mLoss[0m : 2.61604
[1mStep[0m  [6/21], [94mLoss[0m : 2.57472
[1mStep[0m  [8/21], [94mLoss[0m : 2.57211
[1mStep[0m  [10/21], [94mLoss[0m : 2.76158
[1mStep[0m  [12/21], [94mLoss[0m : 2.85798
[1mStep[0m  [14/21], [94mLoss[0m : 2.55007
[1mStep[0m  [16/21], [94mLoss[0m : 2.58890
[1mStep[0m  [18/21], [94mLoss[0m : 2.52077
[1mStep[0m  [20/21], [94mLoss[0m : 2.56694

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.597, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49774
[1mStep[0m  [2/21], [94mLoss[0m : 2.71136
[1mStep[0m  [4/21], [94mLoss[0m : 2.80760
[1mStep[0m  [6/21], [94mLoss[0m : 2.53568
[1mStep[0m  [8/21], [94mLoss[0m : 2.56295
[1mStep[0m  [10/21], [94mLoss[0m : 2.60102
[1mStep[0m  [12/21], [94mLoss[0m : 2.73078
[1mStep[0m  [14/21], [94mLoss[0m : 2.50843
[1mStep[0m  [16/21], [94mLoss[0m : 2.72997
[1mStep[0m  [18/21], [94mLoss[0m : 2.65783
[1mStep[0m  [20/21], [94mLoss[0m : 2.74824

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.618, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58496
[1mStep[0m  [2/21], [94mLoss[0m : 2.54415
[1mStep[0m  [4/21], [94mLoss[0m : 2.65644
[1mStep[0m  [6/21], [94mLoss[0m : 2.60238
[1mStep[0m  [8/21], [94mLoss[0m : 2.53323
[1mStep[0m  [10/21], [94mLoss[0m : 2.70945
[1mStep[0m  [12/21], [94mLoss[0m : 2.47051
[1mStep[0m  [14/21], [94mLoss[0m : 2.62970
[1mStep[0m  [16/21], [94mLoss[0m : 2.40626
[1mStep[0m  [18/21], [94mLoss[0m : 2.58069
[1mStep[0m  [20/21], [94mLoss[0m : 2.69312

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.619, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48382
[1mStep[0m  [2/21], [94mLoss[0m : 2.38416
[1mStep[0m  [4/21], [94mLoss[0m : 2.42641
[1mStep[0m  [6/21], [94mLoss[0m : 2.59288
[1mStep[0m  [8/21], [94mLoss[0m : 2.58985
[1mStep[0m  [10/21], [94mLoss[0m : 2.38413
[1mStep[0m  [12/21], [94mLoss[0m : 2.47941
[1mStep[0m  [14/21], [94mLoss[0m : 2.63353
[1mStep[0m  [16/21], [94mLoss[0m : 2.65622
[1mStep[0m  [18/21], [94mLoss[0m : 2.61379
[1mStep[0m  [20/21], [94mLoss[0m : 2.53093

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47793
[1mStep[0m  [2/21], [94mLoss[0m : 2.36418
[1mStep[0m  [4/21], [94mLoss[0m : 2.50425
[1mStep[0m  [6/21], [94mLoss[0m : 2.71984
[1mStep[0m  [8/21], [94mLoss[0m : 2.59463
[1mStep[0m  [10/21], [94mLoss[0m : 2.75655
[1mStep[0m  [12/21], [94mLoss[0m : 2.58713
[1mStep[0m  [14/21], [94mLoss[0m : 2.63484
[1mStep[0m  [16/21], [94mLoss[0m : 2.65233
[1mStep[0m  [18/21], [94mLoss[0m : 2.42816
[1mStep[0m  [20/21], [94mLoss[0m : 2.62337

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45268
[1mStep[0m  [2/21], [94mLoss[0m : 2.50539
[1mStep[0m  [4/21], [94mLoss[0m : 2.61569
[1mStep[0m  [6/21], [94mLoss[0m : 2.63835
[1mStep[0m  [8/21], [94mLoss[0m : 2.48459
[1mStep[0m  [10/21], [94mLoss[0m : 2.51639
[1mStep[0m  [12/21], [94mLoss[0m : 2.70500
[1mStep[0m  [14/21], [94mLoss[0m : 2.54168
[1mStep[0m  [16/21], [94mLoss[0m : 2.45768
[1mStep[0m  [18/21], [94mLoss[0m : 2.55022
[1mStep[0m  [20/21], [94mLoss[0m : 2.38292

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.560, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49178
[1mStep[0m  [2/21], [94mLoss[0m : 2.55681
[1mStep[0m  [4/21], [94mLoss[0m : 2.53895
[1mStep[0m  [6/21], [94mLoss[0m : 2.36481
[1mStep[0m  [8/21], [94mLoss[0m : 2.34847
[1mStep[0m  [10/21], [94mLoss[0m : 2.35744
[1mStep[0m  [12/21], [94mLoss[0m : 2.51346
[1mStep[0m  [14/21], [94mLoss[0m : 2.47289
[1mStep[0m  [16/21], [94mLoss[0m : 2.40322
[1mStep[0m  [18/21], [94mLoss[0m : 2.44011
[1mStep[0m  [20/21], [94mLoss[0m : 2.58219

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25066
[1mStep[0m  [2/21], [94mLoss[0m : 2.29082
[1mStep[0m  [4/21], [94mLoss[0m : 2.47621
[1mStep[0m  [6/21], [94mLoss[0m : 2.56199
[1mStep[0m  [8/21], [94mLoss[0m : 2.60702
[1mStep[0m  [10/21], [94mLoss[0m : 2.45338
[1mStep[0m  [12/21], [94mLoss[0m : 2.39222
[1mStep[0m  [14/21], [94mLoss[0m : 2.45869
[1mStep[0m  [16/21], [94mLoss[0m : 2.44350
[1mStep[0m  [18/21], [94mLoss[0m : 2.36618
[1mStep[0m  [20/21], [94mLoss[0m : 2.63332

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46636
[1mStep[0m  [2/21], [94mLoss[0m : 2.34893
[1mStep[0m  [4/21], [94mLoss[0m : 2.28738
[1mStep[0m  [6/21], [94mLoss[0m : 2.40741
[1mStep[0m  [8/21], [94mLoss[0m : 2.41734
[1mStep[0m  [10/21], [94mLoss[0m : 2.52722
[1mStep[0m  [12/21], [94mLoss[0m : 2.43475
[1mStep[0m  [14/21], [94mLoss[0m : 2.38085
[1mStep[0m  [16/21], [94mLoss[0m : 2.43946
[1mStep[0m  [18/21], [94mLoss[0m : 2.45985
[1mStep[0m  [20/21], [94mLoss[0m : 2.36571

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23958
[1mStep[0m  [2/21], [94mLoss[0m : 2.33704
[1mStep[0m  [4/21], [94mLoss[0m : 2.54916
[1mStep[0m  [6/21], [94mLoss[0m : 2.41924
[1mStep[0m  [8/21], [94mLoss[0m : 2.26270
[1mStep[0m  [10/21], [94mLoss[0m : 2.54305
[1mStep[0m  [12/21], [94mLoss[0m : 2.46249
[1mStep[0m  [14/21], [94mLoss[0m : 2.39622
[1mStep[0m  [16/21], [94mLoss[0m : 2.39479
[1mStep[0m  [18/21], [94mLoss[0m : 2.46064
[1mStep[0m  [20/21], [94mLoss[0m : 2.43219

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.543, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37811
[1mStep[0m  [2/21], [94mLoss[0m : 2.40211
[1mStep[0m  [4/21], [94mLoss[0m : 2.42431
[1mStep[0m  [6/21], [94mLoss[0m : 2.43967
[1mStep[0m  [8/21], [94mLoss[0m : 2.36460
[1mStep[0m  [10/21], [94mLoss[0m : 2.49875
[1mStep[0m  [12/21], [94mLoss[0m : 2.39671
[1mStep[0m  [14/21], [94mLoss[0m : 2.25707
[1mStep[0m  [16/21], [94mLoss[0m : 2.45039
[1mStep[0m  [18/21], [94mLoss[0m : 2.15861
[1mStep[0m  [20/21], [94mLoss[0m : 2.41407

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.518, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40913
[1mStep[0m  [2/21], [94mLoss[0m : 2.43406
[1mStep[0m  [4/21], [94mLoss[0m : 2.31727
[1mStep[0m  [6/21], [94mLoss[0m : 2.47374
[1mStep[0m  [8/21], [94mLoss[0m : 2.42376
[1mStep[0m  [10/21], [94mLoss[0m : 2.18691
[1mStep[0m  [12/21], [94mLoss[0m : 2.22721
[1mStep[0m  [14/21], [94mLoss[0m : 2.36325
[1mStep[0m  [16/21], [94mLoss[0m : 2.29307
[1mStep[0m  [18/21], [94mLoss[0m : 2.38687
[1mStep[0m  [20/21], [94mLoss[0m : 2.22237

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.557, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35734
[1mStep[0m  [2/21], [94mLoss[0m : 2.45427
[1mStep[0m  [4/21], [94mLoss[0m : 2.28254
[1mStep[0m  [6/21], [94mLoss[0m : 2.40126
[1mStep[0m  [8/21], [94mLoss[0m : 2.40632
[1mStep[0m  [10/21], [94mLoss[0m : 2.13818
[1mStep[0m  [12/21], [94mLoss[0m : 2.30104
[1mStep[0m  [14/21], [94mLoss[0m : 2.17619
[1mStep[0m  [16/21], [94mLoss[0m : 2.36755
[1mStep[0m  [18/21], [94mLoss[0m : 2.40043
[1mStep[0m  [20/21], [94mLoss[0m : 2.27528

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.511, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34007
[1mStep[0m  [2/21], [94mLoss[0m : 2.39562
[1mStep[0m  [4/21], [94mLoss[0m : 2.31031
[1mStep[0m  [6/21], [94mLoss[0m : 2.27683
[1mStep[0m  [8/21], [94mLoss[0m : 2.31753
[1mStep[0m  [10/21], [94mLoss[0m : 2.34356
[1mStep[0m  [12/21], [94mLoss[0m : 2.25933
[1mStep[0m  [14/21], [94mLoss[0m : 2.28973
[1mStep[0m  [16/21], [94mLoss[0m : 2.38119
[1mStep[0m  [18/21], [94mLoss[0m : 2.51916
[1mStep[0m  [20/21], [94mLoss[0m : 2.51643

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32550
[1mStep[0m  [2/21], [94mLoss[0m : 2.25434
[1mStep[0m  [4/21], [94mLoss[0m : 2.25056
[1mStep[0m  [6/21], [94mLoss[0m : 2.23397
[1mStep[0m  [8/21], [94mLoss[0m : 2.17119
[1mStep[0m  [10/21], [94mLoss[0m : 2.24906
[1mStep[0m  [12/21], [94mLoss[0m : 2.24590
[1mStep[0m  [14/21], [94mLoss[0m : 2.31455
[1mStep[0m  [16/21], [94mLoss[0m : 2.21013
[1mStep[0m  [18/21], [94mLoss[0m : 2.34101
[1mStep[0m  [20/21], [94mLoss[0m : 2.18247

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09860
[1mStep[0m  [2/21], [94mLoss[0m : 2.26708
[1mStep[0m  [4/21], [94mLoss[0m : 2.15740
[1mStep[0m  [6/21], [94mLoss[0m : 2.19534
[1mStep[0m  [8/21], [94mLoss[0m : 2.22932
[1mStep[0m  [10/21], [94mLoss[0m : 2.28166
[1mStep[0m  [12/21], [94mLoss[0m : 2.16133
[1mStep[0m  [14/21], [94mLoss[0m : 2.10599
[1mStep[0m  [16/21], [94mLoss[0m : 2.23648
[1mStep[0m  [18/21], [94mLoss[0m : 2.35765
[1mStep[0m  [20/21], [94mLoss[0m : 2.16413

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33152
[1mStep[0m  [2/21], [94mLoss[0m : 2.31312
[1mStep[0m  [4/21], [94mLoss[0m : 2.23496
[1mStep[0m  [6/21], [94mLoss[0m : 2.20711
[1mStep[0m  [8/21], [94mLoss[0m : 2.30333
[1mStep[0m  [10/21], [94mLoss[0m : 2.24460
[1mStep[0m  [12/21], [94mLoss[0m : 2.18887
[1mStep[0m  [14/21], [94mLoss[0m : 2.20679
[1mStep[0m  [16/21], [94mLoss[0m : 2.21937
[1mStep[0m  [18/21], [94mLoss[0m : 2.19316
[1mStep[0m  [20/21], [94mLoss[0m : 2.30496

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20012
[1mStep[0m  [2/21], [94mLoss[0m : 2.14415
[1mStep[0m  [4/21], [94mLoss[0m : 2.11094
[1mStep[0m  [6/21], [94mLoss[0m : 2.07284
[1mStep[0m  [8/21], [94mLoss[0m : 2.34626
[1mStep[0m  [10/21], [94mLoss[0m : 2.40447
[1mStep[0m  [12/21], [94mLoss[0m : 2.07959
[1mStep[0m  [14/21], [94mLoss[0m : 2.24481
[1mStep[0m  [16/21], [94mLoss[0m : 2.18488
[1mStep[0m  [18/21], [94mLoss[0m : 2.20517
[1mStep[0m  [20/21], [94mLoss[0m : 2.23774

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.05229
[1mStep[0m  [2/21], [94mLoss[0m : 2.12958
[1mStep[0m  [4/21], [94mLoss[0m : 2.04408
[1mStep[0m  [6/21], [94mLoss[0m : 2.23810
[1mStep[0m  [8/21], [94mLoss[0m : 2.14528
[1mStep[0m  [10/21], [94mLoss[0m : 2.29755
[1mStep[0m  [12/21], [94mLoss[0m : 2.01921
[1mStep[0m  [14/21], [94mLoss[0m : 2.14136
[1mStep[0m  [16/21], [94mLoss[0m : 2.12316
[1mStep[0m  [18/21], [94mLoss[0m : 2.15187
[1mStep[0m  [20/21], [94mLoss[0m : 2.27231

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11434
[1mStep[0m  [2/21], [94mLoss[0m : 2.15726
[1mStep[0m  [4/21], [94mLoss[0m : 2.27701
[1mStep[0m  [6/21], [94mLoss[0m : 2.33768
[1mStep[0m  [8/21], [94mLoss[0m : 2.23787
[1mStep[0m  [10/21], [94mLoss[0m : 2.03560
[1mStep[0m  [12/21], [94mLoss[0m : 2.25986
[1mStep[0m  [14/21], [94mLoss[0m : 2.05620
[1mStep[0m  [16/21], [94mLoss[0m : 2.10518
[1mStep[0m  [18/21], [94mLoss[0m : 2.31390
[1mStep[0m  [20/21], [94mLoss[0m : 2.25744

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.503, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11318
[1mStep[0m  [2/21], [94mLoss[0m : 2.22944
[1mStep[0m  [4/21], [94mLoss[0m : 2.19871
[1mStep[0m  [6/21], [94mLoss[0m : 2.17893
[1mStep[0m  [8/21], [94mLoss[0m : 2.23393
[1mStep[0m  [10/21], [94mLoss[0m : 2.19441
[1mStep[0m  [12/21], [94mLoss[0m : 2.25931
[1mStep[0m  [14/21], [94mLoss[0m : 2.29644
[1mStep[0m  [16/21], [94mLoss[0m : 2.23526
[1mStep[0m  [18/21], [94mLoss[0m : 2.11882
[1mStep[0m  [20/21], [94mLoss[0m : 2.15866

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.165, [92mTest[0m: 2.483, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10728
[1mStep[0m  [2/21], [94mLoss[0m : 2.12029
[1mStep[0m  [4/21], [94mLoss[0m : 2.08852
[1mStep[0m  [6/21], [94mLoss[0m : 2.10602
[1mStep[0m  [8/21], [94mLoss[0m : 2.01756
[1mStep[0m  [10/21], [94mLoss[0m : 2.10446
[1mStep[0m  [12/21], [94mLoss[0m : 2.10495
[1mStep[0m  [14/21], [94mLoss[0m : 2.10502
[1mStep[0m  [16/21], [94mLoss[0m : 2.11052
[1mStep[0m  [18/21], [94mLoss[0m : 2.11854
[1mStep[0m  [20/21], [94mLoss[0m : 2.08699

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.521, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18499
[1mStep[0m  [2/21], [94mLoss[0m : 2.10705
[1mStep[0m  [4/21], [94mLoss[0m : 2.03814
[1mStep[0m  [6/21], [94mLoss[0m : 1.93792
[1mStep[0m  [8/21], [94mLoss[0m : 2.14838
[1mStep[0m  [10/21], [94mLoss[0m : 2.03156
[1mStep[0m  [12/21], [94mLoss[0m : 2.24129
[1mStep[0m  [14/21], [94mLoss[0m : 2.04493
[1mStep[0m  [16/21], [94mLoss[0m : 2.04009
[1mStep[0m  [18/21], [94mLoss[0m : 2.24821
[1mStep[0m  [20/21], [94mLoss[0m : 2.03460

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.559, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98848
[1mStep[0m  [2/21], [94mLoss[0m : 1.92155
[1mStep[0m  [4/21], [94mLoss[0m : 2.04150
[1mStep[0m  [6/21], [94mLoss[0m : 1.91269
[1mStep[0m  [8/21], [94mLoss[0m : 2.03831
[1mStep[0m  [10/21], [94mLoss[0m : 2.02604
[1mStep[0m  [12/21], [94mLoss[0m : 1.96789
[1mStep[0m  [14/21], [94mLoss[0m : 2.22474
[1mStep[0m  [16/21], [94mLoss[0m : 2.08176
[1mStep[0m  [18/21], [94mLoss[0m : 2.10185
[1mStep[0m  [20/21], [94mLoss[0m : 2.05804

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.065, [92mTest[0m: 2.487, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.00320
[1mStep[0m  [2/21], [94mLoss[0m : 2.05348
[1mStep[0m  [4/21], [94mLoss[0m : 2.04027
[1mStep[0m  [6/21], [94mLoss[0m : 2.02078
[1mStep[0m  [8/21], [94mLoss[0m : 2.12850
[1mStep[0m  [10/21], [94mLoss[0m : 2.02673
[1mStep[0m  [12/21], [94mLoss[0m : 2.02899
[1mStep[0m  [14/21], [94mLoss[0m : 1.96145
[1mStep[0m  [16/21], [94mLoss[0m : 2.10173
[1mStep[0m  [18/21], [94mLoss[0m : 1.94146
[1mStep[0m  [20/21], [94mLoss[0m : 1.94045

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.561, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.15177
[1mStep[0m  [2/21], [94mLoss[0m : 2.00398
[1mStep[0m  [4/21], [94mLoss[0m : 1.98412
[1mStep[0m  [6/21], [94mLoss[0m : 1.83905
[1mStep[0m  [8/21], [94mLoss[0m : 2.20090
[1mStep[0m  [10/21], [94mLoss[0m : 1.94207
[1mStep[0m  [12/21], [94mLoss[0m : 2.06789
[1mStep[0m  [14/21], [94mLoss[0m : 2.03954
[1mStep[0m  [16/21], [94mLoss[0m : 1.95270
[1mStep[0m  [18/21], [94mLoss[0m : 1.96463
[1mStep[0m  [20/21], [94mLoss[0m : 2.08581

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99857
[1mStep[0m  [2/21], [94mLoss[0m : 1.95418
[1mStep[0m  [4/21], [94mLoss[0m : 1.73761
[1mStep[0m  [6/21], [94mLoss[0m : 1.94904
[1mStep[0m  [8/21], [94mLoss[0m : 2.05212
[1mStep[0m  [10/21], [94mLoss[0m : 1.96165
[1mStep[0m  [12/21], [94mLoss[0m : 1.97518
[1mStep[0m  [14/21], [94mLoss[0m : 1.84808
[1mStep[0m  [16/21], [94mLoss[0m : 1.97619
[1mStep[0m  [18/21], [94mLoss[0m : 2.12811
[1mStep[0m  [20/21], [94mLoss[0m : 2.03277

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.573, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07955
[1mStep[0m  [2/21], [94mLoss[0m : 1.95991
[1mStep[0m  [4/21], [94mLoss[0m : 2.01455
[1mStep[0m  [6/21], [94mLoss[0m : 1.91760
[1mStep[0m  [8/21], [94mLoss[0m : 1.99834
[1mStep[0m  [10/21], [94mLoss[0m : 1.86681
[1mStep[0m  [12/21], [94mLoss[0m : 1.93105
[1mStep[0m  [14/21], [94mLoss[0m : 2.12915
[1mStep[0m  [16/21], [94mLoss[0m : 2.01555
[1mStep[0m  [18/21], [94mLoss[0m : 1.90546
[1mStep[0m  [20/21], [94mLoss[0m : 2.01262

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.598
====================================

Phase 2 - Evaluation MAE:  2.5984529427119663
MAE score P1       2.382521
MAE score P2       2.598453
loss               1.975217
learning_rate      0.007525
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.28349
[1mStep[0m  [2/21], [94mLoss[0m : 10.85916
[1mStep[0m  [4/21], [94mLoss[0m : 10.14321
[1mStep[0m  [6/21], [94mLoss[0m : 9.23616
[1mStep[0m  [8/21], [94mLoss[0m : 8.62493
[1mStep[0m  [10/21], [94mLoss[0m : 7.68548
[1mStep[0m  [12/21], [94mLoss[0m : 6.79407
[1mStep[0m  [14/21], [94mLoss[0m : 6.17305
[1mStep[0m  [16/21], [94mLoss[0m : 5.37039
[1mStep[0m  [18/21], [94mLoss[0m : 4.82729
[1mStep[0m  [20/21], [94mLoss[0m : 3.99470

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.732, [92mTest[0m: 11.216, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.06978
[1mStep[0m  [2/21], [94mLoss[0m : 3.52382
[1mStep[0m  [4/21], [94mLoss[0m : 3.58858
[1mStep[0m  [6/21], [94mLoss[0m : 3.16534
[1mStep[0m  [8/21], [94mLoss[0m : 2.97700
[1mStep[0m  [10/21], [94mLoss[0m : 2.76516
[1mStep[0m  [12/21], [94mLoss[0m : 2.98758
[1mStep[0m  [14/21], [94mLoss[0m : 3.02715
[1mStep[0m  [16/21], [94mLoss[0m : 2.79381
[1mStep[0m  [18/21], [94mLoss[0m : 2.58917
[1mStep[0m  [20/21], [94mLoss[0m : 2.67277

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.109, [92mTest[0m: 3.983, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76162
[1mStep[0m  [2/21], [94mLoss[0m : 2.66558
[1mStep[0m  [4/21], [94mLoss[0m : 2.69457
[1mStep[0m  [6/21], [94mLoss[0m : 2.73189
[1mStep[0m  [8/21], [94mLoss[0m : 2.63360
[1mStep[0m  [10/21], [94mLoss[0m : 2.65002
[1mStep[0m  [12/21], [94mLoss[0m : 2.67667
[1mStep[0m  [14/21], [94mLoss[0m : 2.62844
[1mStep[0m  [16/21], [94mLoss[0m : 2.55702
[1mStep[0m  [18/21], [94mLoss[0m : 2.54740
[1mStep[0m  [20/21], [94mLoss[0m : 2.57796

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.614, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65058
[1mStep[0m  [2/21], [94mLoss[0m : 2.66234
[1mStep[0m  [4/21], [94mLoss[0m : 2.66553
[1mStep[0m  [6/21], [94mLoss[0m : 2.68503
[1mStep[0m  [8/21], [94mLoss[0m : 2.70091
[1mStep[0m  [10/21], [94mLoss[0m : 2.68500
[1mStep[0m  [12/21], [94mLoss[0m : 2.49487
[1mStep[0m  [14/21], [94mLoss[0m : 2.65726
[1mStep[0m  [16/21], [94mLoss[0m : 2.72566
[1mStep[0m  [18/21], [94mLoss[0m : 2.53971
[1mStep[0m  [20/21], [94mLoss[0m : 2.54812

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51854
[1mStep[0m  [2/21], [94mLoss[0m : 2.50963
[1mStep[0m  [4/21], [94mLoss[0m : 2.57903
[1mStep[0m  [6/21], [94mLoss[0m : 2.68186
[1mStep[0m  [8/21], [94mLoss[0m : 2.62155
[1mStep[0m  [10/21], [94mLoss[0m : 2.44928
[1mStep[0m  [12/21], [94mLoss[0m : 2.54068
[1mStep[0m  [14/21], [94mLoss[0m : 2.50271
[1mStep[0m  [16/21], [94mLoss[0m : 2.58861
[1mStep[0m  [18/21], [94mLoss[0m : 2.63938
[1mStep[0m  [20/21], [94mLoss[0m : 2.53627

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56302
[1mStep[0m  [2/21], [94mLoss[0m : 2.52540
[1mStep[0m  [4/21], [94mLoss[0m : 2.53389
[1mStep[0m  [6/21], [94mLoss[0m : 2.64825
[1mStep[0m  [8/21], [94mLoss[0m : 2.73562
[1mStep[0m  [10/21], [94mLoss[0m : 2.49467
[1mStep[0m  [12/21], [94mLoss[0m : 2.47434
[1mStep[0m  [14/21], [94mLoss[0m : 2.45143
[1mStep[0m  [16/21], [94mLoss[0m : 2.62011
[1mStep[0m  [18/21], [94mLoss[0m : 2.46004
[1mStep[0m  [20/21], [94mLoss[0m : 2.53827

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66051
[1mStep[0m  [2/21], [94mLoss[0m : 2.58119
[1mStep[0m  [4/21], [94mLoss[0m : 2.54055
[1mStep[0m  [6/21], [94mLoss[0m : 2.65640
[1mStep[0m  [8/21], [94mLoss[0m : 2.70655
[1mStep[0m  [10/21], [94mLoss[0m : 2.39883
[1mStep[0m  [12/21], [94mLoss[0m : 2.39349
[1mStep[0m  [14/21], [94mLoss[0m : 2.58964
[1mStep[0m  [16/21], [94mLoss[0m : 2.44445
[1mStep[0m  [18/21], [94mLoss[0m : 2.47601
[1mStep[0m  [20/21], [94mLoss[0m : 2.54852

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56921
[1mStep[0m  [2/21], [94mLoss[0m : 2.49664
[1mStep[0m  [4/21], [94mLoss[0m : 2.55040
[1mStep[0m  [6/21], [94mLoss[0m : 2.47430
[1mStep[0m  [8/21], [94mLoss[0m : 2.59797
[1mStep[0m  [10/21], [94mLoss[0m : 2.51873
[1mStep[0m  [12/21], [94mLoss[0m : 2.63492
[1mStep[0m  [14/21], [94mLoss[0m : 2.37793
[1mStep[0m  [16/21], [94mLoss[0m : 2.45018
[1mStep[0m  [18/21], [94mLoss[0m : 2.59010
[1mStep[0m  [20/21], [94mLoss[0m : 2.58224

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53518
[1mStep[0m  [2/21], [94mLoss[0m : 2.52140
[1mStep[0m  [4/21], [94mLoss[0m : 2.35406
[1mStep[0m  [6/21], [94mLoss[0m : 2.58402
[1mStep[0m  [8/21], [94mLoss[0m : 2.51500
[1mStep[0m  [10/21], [94mLoss[0m : 2.50582
[1mStep[0m  [12/21], [94mLoss[0m : 2.55121
[1mStep[0m  [14/21], [94mLoss[0m : 2.63069
[1mStep[0m  [16/21], [94mLoss[0m : 2.48127
[1mStep[0m  [18/21], [94mLoss[0m : 2.68872
[1mStep[0m  [20/21], [94mLoss[0m : 2.58805

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58253
[1mStep[0m  [2/21], [94mLoss[0m : 2.48663
[1mStep[0m  [4/21], [94mLoss[0m : 2.56417
[1mStep[0m  [6/21], [94mLoss[0m : 2.58813
[1mStep[0m  [8/21], [94mLoss[0m : 2.39557
[1mStep[0m  [10/21], [94mLoss[0m : 2.52670
[1mStep[0m  [12/21], [94mLoss[0m : 2.34836
[1mStep[0m  [14/21], [94mLoss[0m : 2.64076
[1mStep[0m  [16/21], [94mLoss[0m : 2.57831
[1mStep[0m  [18/21], [94mLoss[0m : 2.63483
[1mStep[0m  [20/21], [94mLoss[0m : 2.53440

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62145
[1mStep[0m  [2/21], [94mLoss[0m : 2.54995
[1mStep[0m  [4/21], [94mLoss[0m : 2.47982
[1mStep[0m  [6/21], [94mLoss[0m : 2.55046
[1mStep[0m  [8/21], [94mLoss[0m : 2.53995
[1mStep[0m  [10/21], [94mLoss[0m : 2.63779
[1mStep[0m  [12/21], [94mLoss[0m : 2.52663
[1mStep[0m  [14/21], [94mLoss[0m : 2.46321
[1mStep[0m  [16/21], [94mLoss[0m : 2.43151
[1mStep[0m  [18/21], [94mLoss[0m : 2.52125
[1mStep[0m  [20/21], [94mLoss[0m : 2.55190

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56200
[1mStep[0m  [2/21], [94mLoss[0m : 2.49577
[1mStep[0m  [4/21], [94mLoss[0m : 2.48264
[1mStep[0m  [6/21], [94mLoss[0m : 2.56524
[1mStep[0m  [8/21], [94mLoss[0m : 2.49144
[1mStep[0m  [10/21], [94mLoss[0m : 2.69698
[1mStep[0m  [12/21], [94mLoss[0m : 2.46789
[1mStep[0m  [14/21], [94mLoss[0m : 2.53799
[1mStep[0m  [16/21], [94mLoss[0m : 2.57017
[1mStep[0m  [18/21], [94mLoss[0m : 2.50455
[1mStep[0m  [20/21], [94mLoss[0m : 2.60324

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41575
[1mStep[0m  [2/21], [94mLoss[0m : 2.51374
[1mStep[0m  [4/21], [94mLoss[0m : 2.37971
[1mStep[0m  [6/21], [94mLoss[0m : 2.51933
[1mStep[0m  [8/21], [94mLoss[0m : 2.39971
[1mStep[0m  [10/21], [94mLoss[0m : 2.46462
[1mStep[0m  [12/21], [94mLoss[0m : 2.43217
[1mStep[0m  [14/21], [94mLoss[0m : 2.67061
[1mStep[0m  [16/21], [94mLoss[0m : 2.53189
[1mStep[0m  [18/21], [94mLoss[0m : 2.54682
[1mStep[0m  [20/21], [94mLoss[0m : 2.40388

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40931
[1mStep[0m  [2/21], [94mLoss[0m : 2.57542
[1mStep[0m  [4/21], [94mLoss[0m : 2.45543
[1mStep[0m  [6/21], [94mLoss[0m : 2.38009
[1mStep[0m  [8/21], [94mLoss[0m : 2.44494
[1mStep[0m  [10/21], [94mLoss[0m : 2.55038
[1mStep[0m  [12/21], [94mLoss[0m : 2.45556
[1mStep[0m  [14/21], [94mLoss[0m : 2.66536
[1mStep[0m  [16/21], [94mLoss[0m : 2.60267
[1mStep[0m  [18/21], [94mLoss[0m : 2.52426
[1mStep[0m  [20/21], [94mLoss[0m : 2.53876

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43473
[1mStep[0m  [2/21], [94mLoss[0m : 2.56756
[1mStep[0m  [4/21], [94mLoss[0m : 2.63904
[1mStep[0m  [6/21], [94mLoss[0m : 2.55189
[1mStep[0m  [8/21], [94mLoss[0m : 2.41592
[1mStep[0m  [10/21], [94mLoss[0m : 2.51901
[1mStep[0m  [12/21], [94mLoss[0m : 2.53786
[1mStep[0m  [14/21], [94mLoss[0m : 2.39496
[1mStep[0m  [16/21], [94mLoss[0m : 2.46415
[1mStep[0m  [18/21], [94mLoss[0m : 2.51771
[1mStep[0m  [20/21], [94mLoss[0m : 2.63076

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42009
[1mStep[0m  [2/21], [94mLoss[0m : 2.47168
[1mStep[0m  [4/21], [94mLoss[0m : 2.58255
[1mStep[0m  [6/21], [94mLoss[0m : 2.38339
[1mStep[0m  [8/21], [94mLoss[0m : 2.58868
[1mStep[0m  [10/21], [94mLoss[0m : 2.32901
[1mStep[0m  [12/21], [94mLoss[0m : 2.54079
[1mStep[0m  [14/21], [94mLoss[0m : 2.55590
[1mStep[0m  [16/21], [94mLoss[0m : 2.55428
[1mStep[0m  [18/21], [94mLoss[0m : 2.48624
[1mStep[0m  [20/21], [94mLoss[0m : 2.48455

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60967
[1mStep[0m  [2/21], [94mLoss[0m : 2.61021
[1mStep[0m  [4/21], [94mLoss[0m : 2.60734
[1mStep[0m  [6/21], [94mLoss[0m : 2.49424
[1mStep[0m  [8/21], [94mLoss[0m : 2.42169
[1mStep[0m  [10/21], [94mLoss[0m : 2.54375
[1mStep[0m  [12/21], [94mLoss[0m : 2.47783
[1mStep[0m  [14/21], [94mLoss[0m : 2.63016
[1mStep[0m  [16/21], [94mLoss[0m : 2.45902
[1mStep[0m  [18/21], [94mLoss[0m : 2.40148
[1mStep[0m  [20/21], [94mLoss[0m : 2.26454

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32185
[1mStep[0m  [2/21], [94mLoss[0m : 2.38514
[1mStep[0m  [4/21], [94mLoss[0m : 2.60545
[1mStep[0m  [6/21], [94mLoss[0m : 2.66655
[1mStep[0m  [8/21], [94mLoss[0m : 2.62060
[1mStep[0m  [10/21], [94mLoss[0m : 2.58316
[1mStep[0m  [12/21], [94mLoss[0m : 2.58081
[1mStep[0m  [14/21], [94mLoss[0m : 2.55882
[1mStep[0m  [16/21], [94mLoss[0m : 2.38795
[1mStep[0m  [18/21], [94mLoss[0m : 2.35562
[1mStep[0m  [20/21], [94mLoss[0m : 2.48014

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51420
[1mStep[0m  [2/21], [94mLoss[0m : 2.63894
[1mStep[0m  [4/21], [94mLoss[0m : 2.57044
[1mStep[0m  [6/21], [94mLoss[0m : 2.54908
[1mStep[0m  [8/21], [94mLoss[0m : 2.51799
[1mStep[0m  [10/21], [94mLoss[0m : 2.54110
[1mStep[0m  [12/21], [94mLoss[0m : 2.45033
[1mStep[0m  [14/21], [94mLoss[0m : 2.45916
[1mStep[0m  [16/21], [94mLoss[0m : 2.49992
[1mStep[0m  [18/21], [94mLoss[0m : 2.55193
[1mStep[0m  [20/21], [94mLoss[0m : 2.43389

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54685
[1mStep[0m  [2/21], [94mLoss[0m : 2.45950
[1mStep[0m  [4/21], [94mLoss[0m : 2.60268
[1mStep[0m  [6/21], [94mLoss[0m : 2.44312
[1mStep[0m  [8/21], [94mLoss[0m : 2.53566
[1mStep[0m  [10/21], [94mLoss[0m : 2.63264
[1mStep[0m  [12/21], [94mLoss[0m : 2.55202
[1mStep[0m  [14/21], [94mLoss[0m : 2.57927
[1mStep[0m  [16/21], [94mLoss[0m : 2.26835
[1mStep[0m  [18/21], [94mLoss[0m : 2.46653
[1mStep[0m  [20/21], [94mLoss[0m : 2.56126

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61905
[1mStep[0m  [2/21], [94mLoss[0m : 2.39069
[1mStep[0m  [4/21], [94mLoss[0m : 2.34929
[1mStep[0m  [6/21], [94mLoss[0m : 2.52559
[1mStep[0m  [8/21], [94mLoss[0m : 2.50674
[1mStep[0m  [10/21], [94mLoss[0m : 2.39832
[1mStep[0m  [12/21], [94mLoss[0m : 2.61951
[1mStep[0m  [14/21], [94mLoss[0m : 2.66471
[1mStep[0m  [16/21], [94mLoss[0m : 2.52096
[1mStep[0m  [18/21], [94mLoss[0m : 2.48454
[1mStep[0m  [20/21], [94mLoss[0m : 2.58721

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58532
[1mStep[0m  [2/21], [94mLoss[0m : 2.40420
[1mStep[0m  [4/21], [94mLoss[0m : 2.39614
[1mStep[0m  [6/21], [94mLoss[0m : 2.45804
[1mStep[0m  [8/21], [94mLoss[0m : 2.34518
[1mStep[0m  [10/21], [94mLoss[0m : 2.41002
[1mStep[0m  [12/21], [94mLoss[0m : 2.45475
[1mStep[0m  [14/21], [94mLoss[0m : 2.44006
[1mStep[0m  [16/21], [94mLoss[0m : 2.53882
[1mStep[0m  [18/21], [94mLoss[0m : 2.44755
[1mStep[0m  [20/21], [94mLoss[0m : 2.40272

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57683
[1mStep[0m  [2/21], [94mLoss[0m : 2.41195
[1mStep[0m  [4/21], [94mLoss[0m : 2.57786
[1mStep[0m  [6/21], [94mLoss[0m : 2.62353
[1mStep[0m  [8/21], [94mLoss[0m : 2.61595
[1mStep[0m  [10/21], [94mLoss[0m : 2.49810
[1mStep[0m  [12/21], [94mLoss[0m : 2.35895
[1mStep[0m  [14/21], [94mLoss[0m : 2.42405
[1mStep[0m  [16/21], [94mLoss[0m : 2.39928
[1mStep[0m  [18/21], [94mLoss[0m : 2.56160
[1mStep[0m  [20/21], [94mLoss[0m : 2.43490

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58271
[1mStep[0m  [2/21], [94mLoss[0m : 2.42300
[1mStep[0m  [4/21], [94mLoss[0m : 2.55615
[1mStep[0m  [6/21], [94mLoss[0m : 2.49037
[1mStep[0m  [8/21], [94mLoss[0m : 2.47168
[1mStep[0m  [10/21], [94mLoss[0m : 2.37017
[1mStep[0m  [12/21], [94mLoss[0m : 2.48772
[1mStep[0m  [14/21], [94mLoss[0m : 2.40417
[1mStep[0m  [16/21], [94mLoss[0m : 2.69957
[1mStep[0m  [18/21], [94mLoss[0m : 2.48338
[1mStep[0m  [20/21], [94mLoss[0m : 2.39195

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59457
[1mStep[0m  [2/21], [94mLoss[0m : 2.67775
[1mStep[0m  [4/21], [94mLoss[0m : 2.51612
[1mStep[0m  [6/21], [94mLoss[0m : 2.52241
[1mStep[0m  [8/21], [94mLoss[0m : 2.50817
[1mStep[0m  [10/21], [94mLoss[0m : 2.46751
[1mStep[0m  [12/21], [94mLoss[0m : 2.49033
[1mStep[0m  [14/21], [94mLoss[0m : 2.56064
[1mStep[0m  [16/21], [94mLoss[0m : 2.37263
[1mStep[0m  [18/21], [94mLoss[0m : 2.53858
[1mStep[0m  [20/21], [94mLoss[0m : 2.44497

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50502
[1mStep[0m  [2/21], [94mLoss[0m : 2.53122
[1mStep[0m  [4/21], [94mLoss[0m : 2.63570
[1mStep[0m  [6/21], [94mLoss[0m : 2.37087
[1mStep[0m  [8/21], [94mLoss[0m : 2.49519
[1mStep[0m  [10/21], [94mLoss[0m : 2.34687
[1mStep[0m  [12/21], [94mLoss[0m : 2.33164
[1mStep[0m  [14/21], [94mLoss[0m : 2.50271
[1mStep[0m  [16/21], [94mLoss[0m : 2.41352
[1mStep[0m  [18/21], [94mLoss[0m : 2.55001
[1mStep[0m  [20/21], [94mLoss[0m : 2.41947

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25873
[1mStep[0m  [2/21], [94mLoss[0m : 2.42747
[1mStep[0m  [4/21], [94mLoss[0m : 2.50013
[1mStep[0m  [6/21], [94mLoss[0m : 2.43062
[1mStep[0m  [8/21], [94mLoss[0m : 2.53140
[1mStep[0m  [10/21], [94mLoss[0m : 2.39781
[1mStep[0m  [12/21], [94mLoss[0m : 2.62021
[1mStep[0m  [14/21], [94mLoss[0m : 2.68850
[1mStep[0m  [16/21], [94mLoss[0m : 2.37519
[1mStep[0m  [18/21], [94mLoss[0m : 2.53804
[1mStep[0m  [20/21], [94mLoss[0m : 2.38207

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40401
[1mStep[0m  [2/21], [94mLoss[0m : 2.39234
[1mStep[0m  [4/21], [94mLoss[0m : 2.40515
[1mStep[0m  [6/21], [94mLoss[0m : 2.57040
[1mStep[0m  [8/21], [94mLoss[0m : 2.48335
[1mStep[0m  [10/21], [94mLoss[0m : 2.46783
[1mStep[0m  [12/21], [94mLoss[0m : 2.54350
[1mStep[0m  [14/21], [94mLoss[0m : 2.38450
[1mStep[0m  [16/21], [94mLoss[0m : 2.67699
[1mStep[0m  [18/21], [94mLoss[0m : 2.37481
[1mStep[0m  [20/21], [94mLoss[0m : 2.55660

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46946
[1mStep[0m  [2/21], [94mLoss[0m : 2.47937
[1mStep[0m  [4/21], [94mLoss[0m : 2.45517
[1mStep[0m  [6/21], [94mLoss[0m : 2.55228
[1mStep[0m  [8/21], [94mLoss[0m : 2.35274
[1mStep[0m  [10/21], [94mLoss[0m : 2.48490
[1mStep[0m  [12/21], [94mLoss[0m : 2.45252
[1mStep[0m  [14/21], [94mLoss[0m : 2.52393
[1mStep[0m  [16/21], [94mLoss[0m : 2.49302
[1mStep[0m  [18/21], [94mLoss[0m : 2.52028
[1mStep[0m  [20/21], [94mLoss[0m : 2.49306

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58824
[1mStep[0m  [2/21], [94mLoss[0m : 2.43595
[1mStep[0m  [4/21], [94mLoss[0m : 2.47968
[1mStep[0m  [6/21], [94mLoss[0m : 2.65759
[1mStep[0m  [8/21], [94mLoss[0m : 2.61057
[1mStep[0m  [10/21], [94mLoss[0m : 2.56626
[1mStep[0m  [12/21], [94mLoss[0m : 2.52201
[1mStep[0m  [14/21], [94mLoss[0m : 2.53851
[1mStep[0m  [16/21], [94mLoss[0m : 2.49487
[1mStep[0m  [18/21], [94mLoss[0m : 2.48997
[1mStep[0m  [20/21], [94mLoss[0m : 2.46692

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 1 - Evaluation MAE:  2.3376899106161937
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.46882
[1mStep[0m  [2/21], [94mLoss[0m : 2.47126
[1mStep[0m  [4/21], [94mLoss[0m : 2.36395
[1mStep[0m  [6/21], [94mLoss[0m : 2.65897
[1mStep[0m  [8/21], [94mLoss[0m : 2.41687
[1mStep[0m  [10/21], [94mLoss[0m : 2.46793
[1mStep[0m  [12/21], [94mLoss[0m : 2.35126
[1mStep[0m  [14/21], [94mLoss[0m : 2.55144
[1mStep[0m  [16/21], [94mLoss[0m : 2.49660
[1mStep[0m  [18/21], [94mLoss[0m : 2.37779
[1mStep[0m  [20/21], [94mLoss[0m : 2.56957

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55480
[1mStep[0m  [2/21], [94mLoss[0m : 2.44779
[1mStep[0m  [4/21], [94mLoss[0m : 2.54304
[1mStep[0m  [6/21], [94mLoss[0m : 2.41744
[1mStep[0m  [8/21], [94mLoss[0m : 2.49326
[1mStep[0m  [10/21], [94mLoss[0m : 2.44622
[1mStep[0m  [12/21], [94mLoss[0m : 2.24736
[1mStep[0m  [14/21], [94mLoss[0m : 2.52424
[1mStep[0m  [16/21], [94mLoss[0m : 2.56476
[1mStep[0m  [18/21], [94mLoss[0m : 2.33451
[1mStep[0m  [20/21], [94mLoss[0m : 2.55007

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38407
[1mStep[0m  [2/21], [94mLoss[0m : 2.31916
[1mStep[0m  [4/21], [94mLoss[0m : 2.54457
[1mStep[0m  [6/21], [94mLoss[0m : 2.51589
[1mStep[0m  [8/21], [94mLoss[0m : 2.47807
[1mStep[0m  [10/21], [94mLoss[0m : 2.34002
[1mStep[0m  [12/21], [94mLoss[0m : 2.57641
[1mStep[0m  [14/21], [94mLoss[0m : 2.61066
[1mStep[0m  [16/21], [94mLoss[0m : 2.50094
[1mStep[0m  [18/21], [94mLoss[0m : 2.45615
[1mStep[0m  [20/21], [94mLoss[0m : 2.30842

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64716
[1mStep[0m  [2/21], [94mLoss[0m : 2.31291
[1mStep[0m  [4/21], [94mLoss[0m : 2.27853
[1mStep[0m  [6/21], [94mLoss[0m : 2.45094
[1mStep[0m  [8/21], [94mLoss[0m : 2.32070
[1mStep[0m  [10/21], [94mLoss[0m : 2.35414
[1mStep[0m  [12/21], [94mLoss[0m : 2.53115
[1mStep[0m  [14/21], [94mLoss[0m : 2.39353
[1mStep[0m  [16/21], [94mLoss[0m : 2.42637
[1mStep[0m  [18/21], [94mLoss[0m : 2.43008
[1mStep[0m  [20/21], [94mLoss[0m : 2.64292

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45680
[1mStep[0m  [2/21], [94mLoss[0m : 2.54484
[1mStep[0m  [4/21], [94mLoss[0m : 2.49661
[1mStep[0m  [6/21], [94mLoss[0m : 2.39646
[1mStep[0m  [8/21], [94mLoss[0m : 2.36042
[1mStep[0m  [10/21], [94mLoss[0m : 2.22231
[1mStep[0m  [12/21], [94mLoss[0m : 2.40039
[1mStep[0m  [14/21], [94mLoss[0m : 2.43849
[1mStep[0m  [16/21], [94mLoss[0m : 2.58685
[1mStep[0m  [18/21], [94mLoss[0m : 2.48602
[1mStep[0m  [20/21], [94mLoss[0m : 2.49422

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50012
[1mStep[0m  [2/21], [94mLoss[0m : 2.34580
[1mStep[0m  [4/21], [94mLoss[0m : 2.37502
[1mStep[0m  [6/21], [94mLoss[0m : 2.35754
[1mStep[0m  [8/21], [94mLoss[0m : 2.31554
[1mStep[0m  [10/21], [94mLoss[0m : 2.38262
[1mStep[0m  [12/21], [94mLoss[0m : 2.56352
[1mStep[0m  [14/21], [94mLoss[0m : 2.46518
[1mStep[0m  [16/21], [94mLoss[0m : 2.39832
[1mStep[0m  [18/21], [94mLoss[0m : 2.46032
[1mStep[0m  [20/21], [94mLoss[0m : 2.46279

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.509, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44489
[1mStep[0m  [2/21], [94mLoss[0m : 2.45761
[1mStep[0m  [4/21], [94mLoss[0m : 2.38448
[1mStep[0m  [6/21], [94mLoss[0m : 2.38726
[1mStep[0m  [8/21], [94mLoss[0m : 2.34486
[1mStep[0m  [10/21], [94mLoss[0m : 2.50264
[1mStep[0m  [12/21], [94mLoss[0m : 2.36095
[1mStep[0m  [14/21], [94mLoss[0m : 2.33094
[1mStep[0m  [16/21], [94mLoss[0m : 2.49196
[1mStep[0m  [18/21], [94mLoss[0m : 2.37593
[1mStep[0m  [20/21], [94mLoss[0m : 2.42412

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43835
[1mStep[0m  [2/21], [94mLoss[0m : 2.54904
[1mStep[0m  [4/21], [94mLoss[0m : 2.42753
[1mStep[0m  [6/21], [94mLoss[0m : 2.50829
[1mStep[0m  [8/21], [94mLoss[0m : 2.33217
[1mStep[0m  [10/21], [94mLoss[0m : 2.34633
[1mStep[0m  [12/21], [94mLoss[0m : 2.55625
[1mStep[0m  [14/21], [94mLoss[0m : 2.26357
[1mStep[0m  [16/21], [94mLoss[0m : 2.32038
[1mStep[0m  [18/21], [94mLoss[0m : 2.41028
[1mStep[0m  [20/21], [94mLoss[0m : 2.34452

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.531, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51358
[1mStep[0m  [2/21], [94mLoss[0m : 2.38255
[1mStep[0m  [4/21], [94mLoss[0m : 2.34109
[1mStep[0m  [6/21], [94mLoss[0m : 2.27028
[1mStep[0m  [8/21], [94mLoss[0m : 2.27007
[1mStep[0m  [10/21], [94mLoss[0m : 2.47847
[1mStep[0m  [12/21], [94mLoss[0m : 2.33870
[1mStep[0m  [14/21], [94mLoss[0m : 2.33238
[1mStep[0m  [16/21], [94mLoss[0m : 2.23475
[1mStep[0m  [18/21], [94mLoss[0m : 2.48262
[1mStep[0m  [20/21], [94mLoss[0m : 2.33032

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.661, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24906
[1mStep[0m  [2/21], [94mLoss[0m : 2.36416
[1mStep[0m  [4/21], [94mLoss[0m : 2.31301
[1mStep[0m  [6/21], [94mLoss[0m : 2.25724
[1mStep[0m  [8/21], [94mLoss[0m : 2.25720
[1mStep[0m  [10/21], [94mLoss[0m : 2.23338
[1mStep[0m  [12/21], [94mLoss[0m : 2.21174
[1mStep[0m  [14/21], [94mLoss[0m : 2.45852
[1mStep[0m  [16/21], [94mLoss[0m : 2.34021
[1mStep[0m  [18/21], [94mLoss[0m : 2.22648
[1mStep[0m  [20/21], [94mLoss[0m : 2.41837

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.638, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33868
[1mStep[0m  [2/21], [94mLoss[0m : 2.33319
[1mStep[0m  [4/21], [94mLoss[0m : 2.35426
[1mStep[0m  [6/21], [94mLoss[0m : 2.22159
[1mStep[0m  [8/21], [94mLoss[0m : 2.22622
[1mStep[0m  [10/21], [94mLoss[0m : 2.45816
[1mStep[0m  [12/21], [94mLoss[0m : 2.26253
[1mStep[0m  [14/21], [94mLoss[0m : 2.41505
[1mStep[0m  [16/21], [94mLoss[0m : 2.48356
[1mStep[0m  [18/21], [94mLoss[0m : 2.35678
[1mStep[0m  [20/21], [94mLoss[0m : 2.41854

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.577, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33110
[1mStep[0m  [2/21], [94mLoss[0m : 2.35275
[1mStep[0m  [4/21], [94mLoss[0m : 2.37692
[1mStep[0m  [6/21], [94mLoss[0m : 2.24320
[1mStep[0m  [8/21], [94mLoss[0m : 2.24308
[1mStep[0m  [10/21], [94mLoss[0m : 2.37615
[1mStep[0m  [12/21], [94mLoss[0m : 2.34057
[1mStep[0m  [14/21], [94mLoss[0m : 2.11080
[1mStep[0m  [16/21], [94mLoss[0m : 2.32021
[1mStep[0m  [18/21], [94mLoss[0m : 2.22362
[1mStep[0m  [20/21], [94mLoss[0m : 2.34541

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.651, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32847
[1mStep[0m  [2/21], [94mLoss[0m : 2.18766
[1mStep[0m  [4/21], [94mLoss[0m : 2.18228
[1mStep[0m  [6/21], [94mLoss[0m : 2.25793
[1mStep[0m  [8/21], [94mLoss[0m : 2.26610
[1mStep[0m  [10/21], [94mLoss[0m : 2.30045
[1mStep[0m  [12/21], [94mLoss[0m : 2.24867
[1mStep[0m  [14/21], [94mLoss[0m : 2.22428
[1mStep[0m  [16/21], [94mLoss[0m : 2.25307
[1mStep[0m  [18/21], [94mLoss[0m : 2.41828
[1mStep[0m  [20/21], [94mLoss[0m : 2.34190

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.645, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34099
[1mStep[0m  [2/21], [94mLoss[0m : 2.34559
[1mStep[0m  [4/21], [94mLoss[0m : 2.15791
[1mStep[0m  [6/21], [94mLoss[0m : 2.22203
[1mStep[0m  [8/21], [94mLoss[0m : 2.31866
[1mStep[0m  [10/21], [94mLoss[0m : 2.24375
[1mStep[0m  [12/21], [94mLoss[0m : 2.23279
[1mStep[0m  [14/21], [94mLoss[0m : 2.18015
[1mStep[0m  [16/21], [94mLoss[0m : 2.20675
[1mStep[0m  [18/21], [94mLoss[0m : 2.18995
[1mStep[0m  [20/21], [94mLoss[0m : 2.22613

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.629, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17362
[1mStep[0m  [2/21], [94mLoss[0m : 2.31608
[1mStep[0m  [4/21], [94mLoss[0m : 2.16701
[1mStep[0m  [6/21], [94mLoss[0m : 2.10183
[1mStep[0m  [8/21], [94mLoss[0m : 2.28484
[1mStep[0m  [10/21], [94mLoss[0m : 2.22614
[1mStep[0m  [12/21], [94mLoss[0m : 2.12741
[1mStep[0m  [14/21], [94mLoss[0m : 2.33172
[1mStep[0m  [16/21], [94mLoss[0m : 2.22311
[1mStep[0m  [18/21], [94mLoss[0m : 2.23932
[1mStep[0m  [20/21], [94mLoss[0m : 2.37658

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.638, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.17564
[1mStep[0m  [2/21], [94mLoss[0m : 2.30401
[1mStep[0m  [4/21], [94mLoss[0m : 2.03720
[1mStep[0m  [6/21], [94mLoss[0m : 2.16233
[1mStep[0m  [8/21], [94mLoss[0m : 2.36007
[1mStep[0m  [10/21], [94mLoss[0m : 2.25088
[1mStep[0m  [12/21], [94mLoss[0m : 2.27032
[1mStep[0m  [14/21], [94mLoss[0m : 2.26036
[1mStep[0m  [16/21], [94mLoss[0m : 2.12100
[1mStep[0m  [18/21], [94mLoss[0m : 2.21118
[1mStep[0m  [20/21], [94mLoss[0m : 2.11277

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.204, [92mTest[0m: 2.650, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25637
[1mStep[0m  [2/21], [94mLoss[0m : 2.23504
[1mStep[0m  [4/21], [94mLoss[0m : 2.18902
[1mStep[0m  [6/21], [94mLoss[0m : 2.05629
[1mStep[0m  [8/21], [94mLoss[0m : 2.12587
[1mStep[0m  [10/21], [94mLoss[0m : 2.12250
[1mStep[0m  [12/21], [94mLoss[0m : 2.04748
[1mStep[0m  [14/21], [94mLoss[0m : 2.19457
[1mStep[0m  [16/21], [94mLoss[0m : 2.16866
[1mStep[0m  [18/21], [94mLoss[0m : 2.17734
[1mStep[0m  [20/21], [94mLoss[0m : 2.14589

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.632, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.98860
[1mStep[0m  [2/21], [94mLoss[0m : 2.12957
[1mStep[0m  [4/21], [94mLoss[0m : 2.09385
[1mStep[0m  [6/21], [94mLoss[0m : 1.97429
[1mStep[0m  [8/21], [94mLoss[0m : 2.17317
[1mStep[0m  [10/21], [94mLoss[0m : 2.31057
[1mStep[0m  [12/21], [94mLoss[0m : 2.10130
[1mStep[0m  [14/21], [94mLoss[0m : 2.12287
[1mStep[0m  [16/21], [94mLoss[0m : 2.22566
[1mStep[0m  [18/21], [94mLoss[0m : 2.11187
[1mStep[0m  [20/21], [94mLoss[0m : 2.22530

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.568, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09256
[1mStep[0m  [2/21], [94mLoss[0m : 2.13268
[1mStep[0m  [4/21], [94mLoss[0m : 2.05612
[1mStep[0m  [6/21], [94mLoss[0m : 2.11748
[1mStep[0m  [8/21], [94mLoss[0m : 2.08874
[1mStep[0m  [10/21], [94mLoss[0m : 2.18146
[1mStep[0m  [12/21], [94mLoss[0m : 2.11455
[1mStep[0m  [14/21], [94mLoss[0m : 2.26013
[1mStep[0m  [16/21], [94mLoss[0m : 2.12871
[1mStep[0m  [18/21], [94mLoss[0m : 2.10760
[1mStep[0m  [20/21], [94mLoss[0m : 2.28529

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.602, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11413
[1mStep[0m  [2/21], [94mLoss[0m : 2.04382
[1mStep[0m  [4/21], [94mLoss[0m : 2.08380
[1mStep[0m  [6/21], [94mLoss[0m : 1.98709
[1mStep[0m  [8/21], [94mLoss[0m : 2.20554
[1mStep[0m  [10/21], [94mLoss[0m : 2.10133
[1mStep[0m  [12/21], [94mLoss[0m : 2.11740
[1mStep[0m  [14/21], [94mLoss[0m : 2.06843
[1mStep[0m  [16/21], [94mLoss[0m : 2.21982
[1mStep[0m  [18/21], [94mLoss[0m : 2.08052
[1mStep[0m  [20/21], [94mLoss[0m : 2.16693

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.513, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92312
[1mStep[0m  [2/21], [94mLoss[0m : 2.07817
[1mStep[0m  [4/21], [94mLoss[0m : 2.18488
[1mStep[0m  [6/21], [94mLoss[0m : 2.15266
[1mStep[0m  [8/21], [94mLoss[0m : 2.23423
[1mStep[0m  [10/21], [94mLoss[0m : 2.05186
[1mStep[0m  [12/21], [94mLoss[0m : 2.06908
[1mStep[0m  [14/21], [94mLoss[0m : 1.91568
[1mStep[0m  [16/21], [94mLoss[0m : 2.02192
[1mStep[0m  [18/21], [94mLoss[0m : 2.08221
[1mStep[0m  [20/21], [94mLoss[0m : 2.10806

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.548, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09240
[1mStep[0m  [2/21], [94mLoss[0m : 1.86920
[1mStep[0m  [4/21], [94mLoss[0m : 2.00434
[1mStep[0m  [6/21], [94mLoss[0m : 2.06329
[1mStep[0m  [8/21], [94mLoss[0m : 2.03865
[1mStep[0m  [10/21], [94mLoss[0m : 2.10664
[1mStep[0m  [12/21], [94mLoss[0m : 1.99759
[1mStep[0m  [14/21], [94mLoss[0m : 2.00190
[1mStep[0m  [16/21], [94mLoss[0m : 2.02924
[1mStep[0m  [18/21], [94mLoss[0m : 2.04991
[1mStep[0m  [20/21], [94mLoss[0m : 2.05528

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.021, [92mTest[0m: 2.591, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.01843
[1mStep[0m  [2/21], [94mLoss[0m : 1.97450
[1mStep[0m  [4/21], [94mLoss[0m : 2.01036
[1mStep[0m  [6/21], [94mLoss[0m : 2.08363
[1mStep[0m  [8/21], [94mLoss[0m : 2.15035
[1mStep[0m  [10/21], [94mLoss[0m : 1.93136
[1mStep[0m  [12/21], [94mLoss[0m : 2.02984
[1mStep[0m  [14/21], [94mLoss[0m : 1.93141
[1mStep[0m  [16/21], [94mLoss[0m : 1.89559
[1mStep[0m  [18/21], [94mLoss[0m : 1.87068
[1mStep[0m  [20/21], [94mLoss[0m : 1.95042

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.631, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02716
[1mStep[0m  [2/21], [94mLoss[0m : 1.89275
[1mStep[0m  [4/21], [94mLoss[0m : 1.85557
[1mStep[0m  [6/21], [94mLoss[0m : 1.87913
[1mStep[0m  [8/21], [94mLoss[0m : 1.94817
[1mStep[0m  [10/21], [94mLoss[0m : 2.05504
[1mStep[0m  [12/21], [94mLoss[0m : 2.10739
[1mStep[0m  [14/21], [94mLoss[0m : 1.93802
[1mStep[0m  [16/21], [94mLoss[0m : 2.04603
[1mStep[0m  [18/21], [94mLoss[0m : 1.93460
[1mStep[0m  [20/21], [94mLoss[0m : 1.89628

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.967, [92mTest[0m: 2.576, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91952
[1mStep[0m  [2/21], [94mLoss[0m : 1.91036
[1mStep[0m  [4/21], [94mLoss[0m : 1.94719
[1mStep[0m  [6/21], [94mLoss[0m : 1.91543
[1mStep[0m  [8/21], [94mLoss[0m : 2.15877
[1mStep[0m  [10/21], [94mLoss[0m : 1.98953
[1mStep[0m  [12/21], [94mLoss[0m : 1.92485
[1mStep[0m  [14/21], [94mLoss[0m : 1.94031
[1mStep[0m  [16/21], [94mLoss[0m : 2.00673
[1mStep[0m  [18/21], [94mLoss[0m : 1.96413
[1mStep[0m  [20/21], [94mLoss[0m : 2.08833

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.739, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88604
[1mStep[0m  [2/21], [94mLoss[0m : 1.91592
[1mStep[0m  [4/21], [94mLoss[0m : 1.73190
[1mStep[0m  [6/21], [94mLoss[0m : 1.95571
[1mStep[0m  [8/21], [94mLoss[0m : 2.03867
[1mStep[0m  [10/21], [94mLoss[0m : 1.85921
[1mStep[0m  [12/21], [94mLoss[0m : 1.91664
[1mStep[0m  [14/21], [94mLoss[0m : 1.88467
[1mStep[0m  [16/21], [94mLoss[0m : 1.84579
[1mStep[0m  [18/21], [94mLoss[0m : 1.92692
[1mStep[0m  [20/21], [94mLoss[0m : 2.02475

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.505, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93804
[1mStep[0m  [2/21], [94mLoss[0m : 1.84650
[1mStep[0m  [4/21], [94mLoss[0m : 1.82466
[1mStep[0m  [6/21], [94mLoss[0m : 1.82513
[1mStep[0m  [8/21], [94mLoss[0m : 2.01229
[1mStep[0m  [10/21], [94mLoss[0m : 1.77950
[1mStep[0m  [12/21], [94mLoss[0m : 1.91520
[1mStep[0m  [14/21], [94mLoss[0m : 1.89320
[1mStep[0m  [16/21], [94mLoss[0m : 2.03339
[1mStep[0m  [18/21], [94mLoss[0m : 1.83765
[1mStep[0m  [20/21], [94mLoss[0m : 1.96743

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.73490
[1mStep[0m  [2/21], [94mLoss[0m : 1.83569
[1mStep[0m  [4/21], [94mLoss[0m : 1.84756
[1mStep[0m  [6/21], [94mLoss[0m : 1.92422
[1mStep[0m  [8/21], [94mLoss[0m : 1.83724
[1mStep[0m  [10/21], [94mLoss[0m : 1.77150
[1mStep[0m  [12/21], [94mLoss[0m : 1.88864
[1mStep[0m  [14/21], [94mLoss[0m : 1.86499
[1mStep[0m  [16/21], [94mLoss[0m : 1.94773
[1mStep[0m  [18/21], [94mLoss[0m : 1.92413
[1mStep[0m  [20/21], [94mLoss[0m : 2.04245

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.601, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90595
[1mStep[0m  [2/21], [94mLoss[0m : 1.84917
[1mStep[0m  [4/21], [94mLoss[0m : 1.89740
[1mStep[0m  [6/21], [94mLoss[0m : 1.69191
[1mStep[0m  [8/21], [94mLoss[0m : 1.70334
[1mStep[0m  [10/21], [94mLoss[0m : 1.96884
[1mStep[0m  [12/21], [94mLoss[0m : 1.74489
[1mStep[0m  [14/21], [94mLoss[0m : 1.90113
[1mStep[0m  [16/21], [94mLoss[0m : 1.76853
[1mStep[0m  [18/21], [94mLoss[0m : 1.71697
[1mStep[0m  [20/21], [94mLoss[0m : 1.87184

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.551, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.80636
[1mStep[0m  [2/21], [94mLoss[0m : 1.83109
[1mStep[0m  [4/21], [94mLoss[0m : 1.76107
[1mStep[0m  [6/21], [94mLoss[0m : 1.74029
[1mStep[0m  [8/21], [94mLoss[0m : 1.79717
[1mStep[0m  [10/21], [94mLoss[0m : 1.82257
[1mStep[0m  [12/21], [94mLoss[0m : 1.81524
[1mStep[0m  [14/21], [94mLoss[0m : 1.87053
[1mStep[0m  [16/21], [94mLoss[0m : 1.74783
[1mStep[0m  [18/21], [94mLoss[0m : 1.65873
[1mStep[0m  [20/21], [94mLoss[0m : 1.87680

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.585, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.556
====================================

Phase 2 - Evaluation MAE:  2.5563043526240756
MAE score P1         2.33769
MAE score P2        2.556304
loss                1.801468
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay          0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 9.83372
[1mStep[0m  [4/42], [94mLoss[0m : 10.16311
[1mStep[0m  [8/42], [94mLoss[0m : 9.27173
[1mStep[0m  [12/42], [94mLoss[0m : 8.81223
[1mStep[0m  [16/42], [94mLoss[0m : 8.69603
[1mStep[0m  [20/42], [94mLoss[0m : 8.45056
[1mStep[0m  [24/42], [94mLoss[0m : 7.22376
[1mStep[0m  [28/42], [94mLoss[0m : 7.03315
[1mStep[0m  [32/42], [94mLoss[0m : 6.22083
[1mStep[0m  [36/42], [94mLoss[0m : 5.98650
[1mStep[0m  [40/42], [94mLoss[0m : 4.93667

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.821, [92mTest[0m: 10.890, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.91006
[1mStep[0m  [4/42], [94mLoss[0m : 4.35300
[1mStep[0m  [8/42], [94mLoss[0m : 4.17339
[1mStep[0m  [12/42], [94mLoss[0m : 4.09538
[1mStep[0m  [16/42], [94mLoss[0m : 3.42656
[1mStep[0m  [20/42], [94mLoss[0m : 3.07046
[1mStep[0m  [24/42], [94mLoss[0m : 3.32829
[1mStep[0m  [28/42], [94mLoss[0m : 2.99364
[1mStep[0m  [32/42], [94mLoss[0m : 2.84159
[1mStep[0m  [36/42], [94mLoss[0m : 3.13277
[1mStep[0m  [40/42], [94mLoss[0m : 3.20326

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.541, [92mTest[0m: 6.751, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87386
[1mStep[0m  [4/42], [94mLoss[0m : 2.94211
[1mStep[0m  [8/42], [94mLoss[0m : 2.59503
[1mStep[0m  [12/42], [94mLoss[0m : 2.87740
[1mStep[0m  [16/42], [94mLoss[0m : 2.98756
[1mStep[0m  [20/42], [94mLoss[0m : 2.65183
[1mStep[0m  [24/42], [94mLoss[0m : 2.76523
[1mStep[0m  [28/42], [94mLoss[0m : 2.95956
[1mStep[0m  [32/42], [94mLoss[0m : 2.91303
[1mStep[0m  [36/42], [94mLoss[0m : 2.89898
[1mStep[0m  [40/42], [94mLoss[0m : 3.07519

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.857, [92mTest[0m: 3.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71343
[1mStep[0m  [4/42], [94mLoss[0m : 2.77552
[1mStep[0m  [8/42], [94mLoss[0m : 2.46056
[1mStep[0m  [12/42], [94mLoss[0m : 2.89123
[1mStep[0m  [16/42], [94mLoss[0m : 2.89136
[1mStep[0m  [20/42], [94mLoss[0m : 2.87193
[1mStep[0m  [24/42], [94mLoss[0m : 2.79753
[1mStep[0m  [28/42], [94mLoss[0m : 2.71934
[1mStep[0m  [32/42], [94mLoss[0m : 2.76540
[1mStep[0m  [36/42], [94mLoss[0m : 2.72675
[1mStep[0m  [40/42], [94mLoss[0m : 2.85223

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.789, [92mTest[0m: 2.888, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.95114
[1mStep[0m  [4/42], [94mLoss[0m : 2.80898
[1mStep[0m  [8/42], [94mLoss[0m : 2.66953
[1mStep[0m  [12/42], [94mLoss[0m : 2.87773
[1mStep[0m  [16/42], [94mLoss[0m : 2.94086
[1mStep[0m  [20/42], [94mLoss[0m : 2.91985
[1mStep[0m  [24/42], [94mLoss[0m : 2.73534
[1mStep[0m  [28/42], [94mLoss[0m : 2.84712
[1mStep[0m  [32/42], [94mLoss[0m : 2.62822
[1mStep[0m  [36/42], [94mLoss[0m : 2.65078
[1mStep[0m  [40/42], [94mLoss[0m : 2.54554

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.598, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07150
[1mStep[0m  [4/42], [94mLoss[0m : 2.69000
[1mStep[0m  [8/42], [94mLoss[0m : 2.71693
[1mStep[0m  [12/42], [94mLoss[0m : 2.75725
[1mStep[0m  [16/42], [94mLoss[0m : 2.75972
[1mStep[0m  [20/42], [94mLoss[0m : 2.73858
[1mStep[0m  [24/42], [94mLoss[0m : 2.87419
[1mStep[0m  [28/42], [94mLoss[0m : 2.74811
[1mStep[0m  [32/42], [94mLoss[0m : 2.85576
[1mStep[0m  [36/42], [94mLoss[0m : 2.90897
[1mStep[0m  [40/42], [94mLoss[0m : 2.66448

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55226
[1mStep[0m  [4/42], [94mLoss[0m : 2.79631
[1mStep[0m  [8/42], [94mLoss[0m : 2.55248
[1mStep[0m  [12/42], [94mLoss[0m : 2.89184
[1mStep[0m  [16/42], [94mLoss[0m : 2.84481
[1mStep[0m  [20/42], [94mLoss[0m : 2.41524
[1mStep[0m  [24/42], [94mLoss[0m : 2.71601
[1mStep[0m  [28/42], [94mLoss[0m : 2.65295
[1mStep[0m  [32/42], [94mLoss[0m : 2.70821
[1mStep[0m  [36/42], [94mLoss[0m : 2.74207
[1mStep[0m  [40/42], [94mLoss[0m : 2.73945

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81390
[1mStep[0m  [4/42], [94mLoss[0m : 2.75366
[1mStep[0m  [8/42], [94mLoss[0m : 2.67696
[1mStep[0m  [12/42], [94mLoss[0m : 2.83728
[1mStep[0m  [16/42], [94mLoss[0m : 2.84854
[1mStep[0m  [20/42], [94mLoss[0m : 2.73094
[1mStep[0m  [24/42], [94mLoss[0m : 3.04786
[1mStep[0m  [28/42], [94mLoss[0m : 2.56884
[1mStep[0m  [32/42], [94mLoss[0m : 2.63823
[1mStep[0m  [36/42], [94mLoss[0m : 2.49985
[1mStep[0m  [40/42], [94mLoss[0m : 2.52564

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.440, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71396
[1mStep[0m  [4/42], [94mLoss[0m : 2.62256
[1mStep[0m  [8/42], [94mLoss[0m : 2.83770
[1mStep[0m  [12/42], [94mLoss[0m : 2.77776
[1mStep[0m  [16/42], [94mLoss[0m : 2.70633
[1mStep[0m  [20/42], [94mLoss[0m : 2.68103
[1mStep[0m  [24/42], [94mLoss[0m : 2.72153
[1mStep[0m  [28/42], [94mLoss[0m : 2.47526
[1mStep[0m  [32/42], [94mLoss[0m : 2.67962
[1mStep[0m  [36/42], [94mLoss[0m : 2.75660
[1mStep[0m  [40/42], [94mLoss[0m : 2.80030

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.429, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57358
[1mStep[0m  [4/42], [94mLoss[0m : 2.75235
[1mStep[0m  [8/42], [94mLoss[0m : 2.71795
[1mStep[0m  [12/42], [94mLoss[0m : 2.62687
[1mStep[0m  [16/42], [94mLoss[0m : 2.62767
[1mStep[0m  [20/42], [94mLoss[0m : 2.69600
[1mStep[0m  [24/42], [94mLoss[0m : 2.43525
[1mStep[0m  [28/42], [94mLoss[0m : 2.56193
[1mStep[0m  [32/42], [94mLoss[0m : 2.57480
[1mStep[0m  [36/42], [94mLoss[0m : 2.67681
[1mStep[0m  [40/42], [94mLoss[0m : 2.50196

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78752
[1mStep[0m  [4/42], [94mLoss[0m : 2.71768
[1mStep[0m  [8/42], [94mLoss[0m : 2.59211
[1mStep[0m  [12/42], [94mLoss[0m : 2.38288
[1mStep[0m  [16/42], [94mLoss[0m : 2.79679
[1mStep[0m  [20/42], [94mLoss[0m : 2.56614
[1mStep[0m  [24/42], [94mLoss[0m : 2.46466
[1mStep[0m  [28/42], [94mLoss[0m : 2.68841
[1mStep[0m  [32/42], [94mLoss[0m : 2.66482
[1mStep[0m  [36/42], [94mLoss[0m : 2.80310
[1mStep[0m  [40/42], [94mLoss[0m : 2.64455

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70741
[1mStep[0m  [4/42], [94mLoss[0m : 2.71569
[1mStep[0m  [8/42], [94mLoss[0m : 2.48053
[1mStep[0m  [12/42], [94mLoss[0m : 2.58042
[1mStep[0m  [16/42], [94mLoss[0m : 2.85654
[1mStep[0m  [20/42], [94mLoss[0m : 2.62630
[1mStep[0m  [24/42], [94mLoss[0m : 2.57723
[1mStep[0m  [28/42], [94mLoss[0m : 2.70555
[1mStep[0m  [32/42], [94mLoss[0m : 2.58848
[1mStep[0m  [36/42], [94mLoss[0m : 2.47504
[1mStep[0m  [40/42], [94mLoss[0m : 2.41949

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49212
[1mStep[0m  [4/42], [94mLoss[0m : 2.53371
[1mStep[0m  [8/42], [94mLoss[0m : 2.58793
[1mStep[0m  [12/42], [94mLoss[0m : 2.42019
[1mStep[0m  [16/42], [94mLoss[0m : 2.40949
[1mStep[0m  [20/42], [94mLoss[0m : 2.63853
[1mStep[0m  [24/42], [94mLoss[0m : 2.67817
[1mStep[0m  [28/42], [94mLoss[0m : 2.71475
[1mStep[0m  [32/42], [94mLoss[0m : 2.59978
[1mStep[0m  [36/42], [94mLoss[0m : 2.48293
[1mStep[0m  [40/42], [94mLoss[0m : 2.68690

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58104
[1mStep[0m  [4/42], [94mLoss[0m : 2.44416
[1mStep[0m  [8/42], [94mLoss[0m : 2.52985
[1mStep[0m  [12/42], [94mLoss[0m : 2.72490
[1mStep[0m  [16/42], [94mLoss[0m : 2.40347
[1mStep[0m  [20/42], [94mLoss[0m : 2.62667
[1mStep[0m  [24/42], [94mLoss[0m : 2.81035
[1mStep[0m  [28/42], [94mLoss[0m : 2.61902
[1mStep[0m  [32/42], [94mLoss[0m : 2.49936
[1mStep[0m  [36/42], [94mLoss[0m : 2.63346
[1mStep[0m  [40/42], [94mLoss[0m : 2.85317

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80502
[1mStep[0m  [4/42], [94mLoss[0m : 2.52727
[1mStep[0m  [8/42], [94mLoss[0m : 2.76611
[1mStep[0m  [12/42], [94mLoss[0m : 2.44384
[1mStep[0m  [16/42], [94mLoss[0m : 2.64215
[1mStep[0m  [20/42], [94mLoss[0m : 2.70973
[1mStep[0m  [24/42], [94mLoss[0m : 2.35476
[1mStep[0m  [28/42], [94mLoss[0m : 2.48499
[1mStep[0m  [32/42], [94mLoss[0m : 2.67568
[1mStep[0m  [36/42], [94mLoss[0m : 2.62135
[1mStep[0m  [40/42], [94mLoss[0m : 2.53062

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42427
[1mStep[0m  [4/42], [94mLoss[0m : 2.50093
[1mStep[0m  [8/42], [94mLoss[0m : 2.47984
[1mStep[0m  [12/42], [94mLoss[0m : 2.44303
[1mStep[0m  [16/42], [94mLoss[0m : 2.67953
[1mStep[0m  [20/42], [94mLoss[0m : 2.64707
[1mStep[0m  [24/42], [94mLoss[0m : 2.49595
[1mStep[0m  [28/42], [94mLoss[0m : 2.36483
[1mStep[0m  [32/42], [94mLoss[0m : 2.56067
[1mStep[0m  [36/42], [94mLoss[0m : 2.67189
[1mStep[0m  [40/42], [94mLoss[0m : 2.54807

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46102
[1mStep[0m  [4/42], [94mLoss[0m : 2.49966
[1mStep[0m  [8/42], [94mLoss[0m : 2.41979
[1mStep[0m  [12/42], [94mLoss[0m : 2.56995
[1mStep[0m  [16/42], [94mLoss[0m : 2.64380
[1mStep[0m  [20/42], [94mLoss[0m : 2.60253
[1mStep[0m  [24/42], [94mLoss[0m : 2.71252
[1mStep[0m  [28/42], [94mLoss[0m : 2.49890
[1mStep[0m  [32/42], [94mLoss[0m : 2.62158
[1mStep[0m  [36/42], [94mLoss[0m : 2.58395
[1mStep[0m  [40/42], [94mLoss[0m : 2.70674

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81839
[1mStep[0m  [4/42], [94mLoss[0m : 2.28800
[1mStep[0m  [8/42], [94mLoss[0m : 2.84678
[1mStep[0m  [12/42], [94mLoss[0m : 2.60911
[1mStep[0m  [16/42], [94mLoss[0m : 2.54420
[1mStep[0m  [20/42], [94mLoss[0m : 2.34302
[1mStep[0m  [24/42], [94mLoss[0m : 2.61243
[1mStep[0m  [28/42], [94mLoss[0m : 2.77867
[1mStep[0m  [32/42], [94mLoss[0m : 2.51212
[1mStep[0m  [36/42], [94mLoss[0m : 2.82150
[1mStep[0m  [40/42], [94mLoss[0m : 2.49441

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76026
[1mStep[0m  [4/42], [94mLoss[0m : 2.63405
[1mStep[0m  [8/42], [94mLoss[0m : 2.40143
[1mStep[0m  [12/42], [94mLoss[0m : 2.57205
[1mStep[0m  [16/42], [94mLoss[0m : 2.80741
[1mStep[0m  [20/42], [94mLoss[0m : 2.53959
[1mStep[0m  [24/42], [94mLoss[0m : 2.50152
[1mStep[0m  [28/42], [94mLoss[0m : 2.57840
[1mStep[0m  [32/42], [94mLoss[0m : 2.51726
[1mStep[0m  [36/42], [94mLoss[0m : 2.72911
[1mStep[0m  [40/42], [94mLoss[0m : 2.26959

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56767
[1mStep[0m  [4/42], [94mLoss[0m : 2.46002
[1mStep[0m  [8/42], [94mLoss[0m : 2.51361
[1mStep[0m  [12/42], [94mLoss[0m : 2.63866
[1mStep[0m  [16/42], [94mLoss[0m : 2.68441
[1mStep[0m  [20/42], [94mLoss[0m : 2.51647
[1mStep[0m  [24/42], [94mLoss[0m : 2.63635
[1mStep[0m  [28/42], [94mLoss[0m : 2.52834
[1mStep[0m  [32/42], [94mLoss[0m : 2.49033
[1mStep[0m  [36/42], [94mLoss[0m : 2.55285
[1mStep[0m  [40/42], [94mLoss[0m : 2.86186

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35345
[1mStep[0m  [4/42], [94mLoss[0m : 2.48337
[1mStep[0m  [8/42], [94mLoss[0m : 2.33701
[1mStep[0m  [12/42], [94mLoss[0m : 2.67734
[1mStep[0m  [16/42], [94mLoss[0m : 2.50376
[1mStep[0m  [20/42], [94mLoss[0m : 2.49825
[1mStep[0m  [24/42], [94mLoss[0m : 2.43658
[1mStep[0m  [28/42], [94mLoss[0m : 2.37764
[1mStep[0m  [32/42], [94mLoss[0m : 2.58413
[1mStep[0m  [36/42], [94mLoss[0m : 2.47989
[1mStep[0m  [40/42], [94mLoss[0m : 2.60547

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60552
[1mStep[0m  [4/42], [94mLoss[0m : 2.39745
[1mStep[0m  [8/42], [94mLoss[0m : 2.59012
[1mStep[0m  [12/42], [94mLoss[0m : 2.41622
[1mStep[0m  [16/42], [94mLoss[0m : 2.46104
[1mStep[0m  [20/42], [94mLoss[0m : 2.63948
[1mStep[0m  [24/42], [94mLoss[0m : 2.75886
[1mStep[0m  [28/42], [94mLoss[0m : 2.62313
[1mStep[0m  [32/42], [94mLoss[0m : 2.57079
[1mStep[0m  [36/42], [94mLoss[0m : 2.64973
[1mStep[0m  [40/42], [94mLoss[0m : 2.34610

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35499
[1mStep[0m  [4/42], [94mLoss[0m : 2.60527
[1mStep[0m  [8/42], [94mLoss[0m : 2.40909
[1mStep[0m  [12/42], [94mLoss[0m : 2.58250
[1mStep[0m  [16/42], [94mLoss[0m : 2.71699
[1mStep[0m  [20/42], [94mLoss[0m : 2.51658
[1mStep[0m  [24/42], [94mLoss[0m : 2.80627
[1mStep[0m  [28/42], [94mLoss[0m : 2.44085
[1mStep[0m  [32/42], [94mLoss[0m : 2.68478
[1mStep[0m  [36/42], [94mLoss[0m : 2.75485
[1mStep[0m  [40/42], [94mLoss[0m : 2.73502

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44297
[1mStep[0m  [4/42], [94mLoss[0m : 2.54203
[1mStep[0m  [8/42], [94mLoss[0m : 2.63393
[1mStep[0m  [12/42], [94mLoss[0m : 2.73462
[1mStep[0m  [16/42], [94mLoss[0m : 2.70199
[1mStep[0m  [20/42], [94mLoss[0m : 2.48540
[1mStep[0m  [24/42], [94mLoss[0m : 2.52208
[1mStep[0m  [28/42], [94mLoss[0m : 2.48652
[1mStep[0m  [32/42], [94mLoss[0m : 2.67762
[1mStep[0m  [36/42], [94mLoss[0m : 2.48657
[1mStep[0m  [40/42], [94mLoss[0m : 2.39318

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64613
[1mStep[0m  [4/42], [94mLoss[0m : 2.66865
[1mStep[0m  [8/42], [94mLoss[0m : 2.54045
[1mStep[0m  [12/42], [94mLoss[0m : 2.52063
[1mStep[0m  [16/42], [94mLoss[0m : 2.52229
[1mStep[0m  [20/42], [94mLoss[0m : 2.53880
[1mStep[0m  [24/42], [94mLoss[0m : 2.57897
[1mStep[0m  [28/42], [94mLoss[0m : 2.49581
[1mStep[0m  [32/42], [94mLoss[0m : 2.41676
[1mStep[0m  [36/42], [94mLoss[0m : 2.55747
[1mStep[0m  [40/42], [94mLoss[0m : 2.60970

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69046
[1mStep[0m  [4/42], [94mLoss[0m : 2.59754
[1mStep[0m  [8/42], [94mLoss[0m : 2.35314
[1mStep[0m  [12/42], [94mLoss[0m : 2.42588
[1mStep[0m  [16/42], [94mLoss[0m : 2.61311
[1mStep[0m  [20/42], [94mLoss[0m : 2.60683
[1mStep[0m  [24/42], [94mLoss[0m : 2.50532
[1mStep[0m  [28/42], [94mLoss[0m : 2.62759
[1mStep[0m  [32/42], [94mLoss[0m : 2.48923
[1mStep[0m  [36/42], [94mLoss[0m : 2.49198
[1mStep[0m  [40/42], [94mLoss[0m : 2.47634

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49189
[1mStep[0m  [4/42], [94mLoss[0m : 2.92724
[1mStep[0m  [8/42], [94mLoss[0m : 2.74307
[1mStep[0m  [12/42], [94mLoss[0m : 2.33936
[1mStep[0m  [16/42], [94mLoss[0m : 2.54360
[1mStep[0m  [20/42], [94mLoss[0m : 2.25633
[1mStep[0m  [24/42], [94mLoss[0m : 2.60014
[1mStep[0m  [28/42], [94mLoss[0m : 2.72264
[1mStep[0m  [32/42], [94mLoss[0m : 2.42615
[1mStep[0m  [36/42], [94mLoss[0m : 2.39092
[1mStep[0m  [40/42], [94mLoss[0m : 2.51531

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.321, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37500
[1mStep[0m  [4/42], [94mLoss[0m : 2.24541
[1mStep[0m  [8/42], [94mLoss[0m : 2.49108
[1mStep[0m  [12/42], [94mLoss[0m : 2.50449
[1mStep[0m  [16/42], [94mLoss[0m : 2.27412
[1mStep[0m  [20/42], [94mLoss[0m : 2.61170
[1mStep[0m  [24/42], [94mLoss[0m : 2.69220
[1mStep[0m  [28/42], [94mLoss[0m : 2.50878
[1mStep[0m  [32/42], [94mLoss[0m : 2.69738
[1mStep[0m  [36/42], [94mLoss[0m : 2.47290
[1mStep[0m  [40/42], [94mLoss[0m : 2.40392

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70038
[1mStep[0m  [4/42], [94mLoss[0m : 2.50352
[1mStep[0m  [8/42], [94mLoss[0m : 2.39571
[1mStep[0m  [12/42], [94mLoss[0m : 2.70615
[1mStep[0m  [16/42], [94mLoss[0m : 2.48194
[1mStep[0m  [20/42], [94mLoss[0m : 2.44094
[1mStep[0m  [24/42], [94mLoss[0m : 2.49180
[1mStep[0m  [28/42], [94mLoss[0m : 2.41202
[1mStep[0m  [32/42], [94mLoss[0m : 2.49310
[1mStep[0m  [36/42], [94mLoss[0m : 2.50619
[1mStep[0m  [40/42], [94mLoss[0m : 2.52023

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33579
[1mStep[0m  [4/42], [94mLoss[0m : 2.37881
[1mStep[0m  [8/42], [94mLoss[0m : 2.71350
[1mStep[0m  [12/42], [94mLoss[0m : 2.26938
[1mStep[0m  [16/42], [94mLoss[0m : 2.53268
[1mStep[0m  [20/42], [94mLoss[0m : 2.56297
[1mStep[0m  [24/42], [94mLoss[0m : 2.52426
[1mStep[0m  [28/42], [94mLoss[0m : 2.61011
[1mStep[0m  [32/42], [94mLoss[0m : 2.66624
[1mStep[0m  [36/42], [94mLoss[0m : 2.50961
[1mStep[0m  [40/42], [94mLoss[0m : 2.75313

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3312938724245345
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.57782
[1mStep[0m  [4/42], [94mLoss[0m : 2.69321
[1mStep[0m  [8/42], [94mLoss[0m : 2.61662
[1mStep[0m  [12/42], [94mLoss[0m : 2.43460
[1mStep[0m  [16/42], [94mLoss[0m : 2.49989
[1mStep[0m  [20/42], [94mLoss[0m : 2.39029
[1mStep[0m  [24/42], [94mLoss[0m : 2.64821
[1mStep[0m  [28/42], [94mLoss[0m : 2.65105
[1mStep[0m  [32/42], [94mLoss[0m : 2.90772
[1mStep[0m  [36/42], [94mLoss[0m : 2.52778
[1mStep[0m  [40/42], [94mLoss[0m : 2.32462

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57322
[1mStep[0m  [4/42], [94mLoss[0m : 2.38495
[1mStep[0m  [8/42], [94mLoss[0m : 2.58189
[1mStep[0m  [12/42], [94mLoss[0m : 2.56031
[1mStep[0m  [16/42], [94mLoss[0m : 2.68717
[1mStep[0m  [20/42], [94mLoss[0m : 2.74992
[1mStep[0m  [24/42], [94mLoss[0m : 2.64429
[1mStep[0m  [28/42], [94mLoss[0m : 2.76741
[1mStep[0m  [32/42], [94mLoss[0m : 2.57850
[1mStep[0m  [36/42], [94mLoss[0m : 2.34408
[1mStep[0m  [40/42], [94mLoss[0m : 2.33982

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63055
[1mStep[0m  [4/42], [94mLoss[0m : 2.38792
[1mStep[0m  [8/42], [94mLoss[0m : 2.44073
[1mStep[0m  [12/42], [94mLoss[0m : 2.40819
[1mStep[0m  [16/42], [94mLoss[0m : 2.42862
[1mStep[0m  [20/42], [94mLoss[0m : 2.55613
[1mStep[0m  [24/42], [94mLoss[0m : 2.64779
[1mStep[0m  [28/42], [94mLoss[0m : 2.50636
[1mStep[0m  [32/42], [94mLoss[0m : 2.48366
[1mStep[0m  [36/42], [94mLoss[0m : 2.59570
[1mStep[0m  [40/42], [94mLoss[0m : 2.43933

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.851, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33017
[1mStep[0m  [4/42], [94mLoss[0m : 2.37766
[1mStep[0m  [8/42], [94mLoss[0m : 2.70919
[1mStep[0m  [12/42], [94mLoss[0m : 2.47118
[1mStep[0m  [16/42], [94mLoss[0m : 2.58976
[1mStep[0m  [20/42], [94mLoss[0m : 2.52716
[1mStep[0m  [24/42], [94mLoss[0m : 2.40582
[1mStep[0m  [28/42], [94mLoss[0m : 2.61435
[1mStep[0m  [32/42], [94mLoss[0m : 2.55505
[1mStep[0m  [36/42], [94mLoss[0m : 2.51705
[1mStep[0m  [40/42], [94mLoss[0m : 2.56092

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.546, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42165
[1mStep[0m  [4/42], [94mLoss[0m : 2.38395
[1mStep[0m  [8/42], [94mLoss[0m : 2.42913
[1mStep[0m  [12/42], [94mLoss[0m : 2.32059
[1mStep[0m  [16/42], [94mLoss[0m : 2.50586
[1mStep[0m  [20/42], [94mLoss[0m : 2.52295
[1mStep[0m  [24/42], [94mLoss[0m : 2.49677
[1mStep[0m  [28/42], [94mLoss[0m : 2.51565
[1mStep[0m  [32/42], [94mLoss[0m : 2.70089
[1mStep[0m  [36/42], [94mLoss[0m : 2.71069
[1mStep[0m  [40/42], [94mLoss[0m : 2.27913

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37395
[1mStep[0m  [4/42], [94mLoss[0m : 2.51921
[1mStep[0m  [8/42], [94mLoss[0m : 2.36630
[1mStep[0m  [12/42], [94mLoss[0m : 2.42202
[1mStep[0m  [16/42], [94mLoss[0m : 2.57108
[1mStep[0m  [20/42], [94mLoss[0m : 2.44004
[1mStep[0m  [24/42], [94mLoss[0m : 2.42769
[1mStep[0m  [28/42], [94mLoss[0m : 2.29080
[1mStep[0m  [32/42], [94mLoss[0m : 2.36570
[1mStep[0m  [36/42], [94mLoss[0m : 2.47918
[1mStep[0m  [40/42], [94mLoss[0m : 2.32350

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38020
[1mStep[0m  [4/42], [94mLoss[0m : 2.55690
[1mStep[0m  [8/42], [94mLoss[0m : 2.55187
[1mStep[0m  [12/42], [94mLoss[0m : 2.14614
[1mStep[0m  [16/42], [94mLoss[0m : 2.35310
[1mStep[0m  [20/42], [94mLoss[0m : 2.24756
[1mStep[0m  [24/42], [94mLoss[0m : 2.14054
[1mStep[0m  [28/42], [94mLoss[0m : 2.40270
[1mStep[0m  [32/42], [94mLoss[0m : 2.25138
[1mStep[0m  [36/42], [94mLoss[0m : 2.57516
[1mStep[0m  [40/42], [94mLoss[0m : 2.27020

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39756
[1mStep[0m  [4/42], [94mLoss[0m : 2.23917
[1mStep[0m  [8/42], [94mLoss[0m : 2.26358
[1mStep[0m  [12/42], [94mLoss[0m : 2.22816
[1mStep[0m  [16/42], [94mLoss[0m : 2.33609
[1mStep[0m  [20/42], [94mLoss[0m : 2.41941
[1mStep[0m  [24/42], [94mLoss[0m : 2.57061
[1mStep[0m  [28/42], [94mLoss[0m : 2.49206
[1mStep[0m  [32/42], [94mLoss[0m : 2.25873
[1mStep[0m  [36/42], [94mLoss[0m : 2.69459
[1mStep[0m  [40/42], [94mLoss[0m : 2.29621

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24841
[1mStep[0m  [4/42], [94mLoss[0m : 2.33006
[1mStep[0m  [8/42], [94mLoss[0m : 2.33460
[1mStep[0m  [12/42], [94mLoss[0m : 2.49215
[1mStep[0m  [16/42], [94mLoss[0m : 2.19556
[1mStep[0m  [20/42], [94mLoss[0m : 2.20684
[1mStep[0m  [24/42], [94mLoss[0m : 2.36895
[1mStep[0m  [28/42], [94mLoss[0m : 2.40160
[1mStep[0m  [32/42], [94mLoss[0m : 2.43685
[1mStep[0m  [36/42], [94mLoss[0m : 2.23500
[1mStep[0m  [40/42], [94mLoss[0m : 2.29179

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17285
[1mStep[0m  [4/42], [94mLoss[0m : 2.27200
[1mStep[0m  [8/42], [94mLoss[0m : 2.30200
[1mStep[0m  [12/42], [94mLoss[0m : 2.19896
[1mStep[0m  [16/42], [94mLoss[0m : 2.22409
[1mStep[0m  [20/42], [94mLoss[0m : 2.15805
[1mStep[0m  [24/42], [94mLoss[0m : 2.39567
[1mStep[0m  [28/42], [94mLoss[0m : 2.28320
[1mStep[0m  [32/42], [94mLoss[0m : 2.30570
[1mStep[0m  [36/42], [94mLoss[0m : 2.19522
[1mStep[0m  [40/42], [94mLoss[0m : 2.14749

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27974
[1mStep[0m  [4/42], [94mLoss[0m : 2.33808
[1mStep[0m  [8/42], [94mLoss[0m : 2.35033
[1mStep[0m  [12/42], [94mLoss[0m : 2.18929
[1mStep[0m  [16/42], [94mLoss[0m : 2.08990
[1mStep[0m  [20/42], [94mLoss[0m : 2.14513
[1mStep[0m  [24/42], [94mLoss[0m : 2.27027
[1mStep[0m  [28/42], [94mLoss[0m : 2.28429
[1mStep[0m  [32/42], [94mLoss[0m : 2.15400
[1mStep[0m  [36/42], [94mLoss[0m : 2.21892
[1mStep[0m  [40/42], [94mLoss[0m : 2.33706

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28148
[1mStep[0m  [4/42], [94mLoss[0m : 2.48873
[1mStep[0m  [8/42], [94mLoss[0m : 1.99493
[1mStep[0m  [12/42], [94mLoss[0m : 2.26905
[1mStep[0m  [16/42], [94mLoss[0m : 2.03641
[1mStep[0m  [20/42], [94mLoss[0m : 2.19495
[1mStep[0m  [24/42], [94mLoss[0m : 2.23930
[1mStep[0m  [28/42], [94mLoss[0m : 2.21813
[1mStep[0m  [32/42], [94mLoss[0m : 2.12866
[1mStep[0m  [36/42], [94mLoss[0m : 2.32110
[1mStep[0m  [40/42], [94mLoss[0m : 2.17612

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28244
[1mStep[0m  [4/42], [94mLoss[0m : 2.41154
[1mStep[0m  [8/42], [94mLoss[0m : 2.09511
[1mStep[0m  [12/42], [94mLoss[0m : 2.11432
[1mStep[0m  [16/42], [94mLoss[0m : 2.22693
[1mStep[0m  [20/42], [94mLoss[0m : 2.08816
[1mStep[0m  [24/42], [94mLoss[0m : 2.26271
[1mStep[0m  [28/42], [94mLoss[0m : 2.02419
[1mStep[0m  [32/42], [94mLoss[0m : 2.00845
[1mStep[0m  [36/42], [94mLoss[0m : 2.10678
[1mStep[0m  [40/42], [94mLoss[0m : 2.14296

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03245
[1mStep[0m  [4/42], [94mLoss[0m : 2.05047
[1mStep[0m  [8/42], [94mLoss[0m : 2.07840
[1mStep[0m  [12/42], [94mLoss[0m : 2.14742
[1mStep[0m  [16/42], [94mLoss[0m : 2.10132
[1mStep[0m  [20/42], [94mLoss[0m : 2.17239
[1mStep[0m  [24/42], [94mLoss[0m : 2.02296
[1mStep[0m  [28/42], [94mLoss[0m : 1.97651
[1mStep[0m  [32/42], [94mLoss[0m : 2.21722
[1mStep[0m  [36/42], [94mLoss[0m : 2.18421
[1mStep[0m  [40/42], [94mLoss[0m : 2.05746

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05065
[1mStep[0m  [4/42], [94mLoss[0m : 2.15236
[1mStep[0m  [8/42], [94mLoss[0m : 2.05476
[1mStep[0m  [12/42], [94mLoss[0m : 1.98505
[1mStep[0m  [16/42], [94mLoss[0m : 2.05238
[1mStep[0m  [20/42], [94mLoss[0m : 1.99828
[1mStep[0m  [24/42], [94mLoss[0m : 2.03400
[1mStep[0m  [28/42], [94mLoss[0m : 1.90422
[1mStep[0m  [32/42], [94mLoss[0m : 2.04452
[1mStep[0m  [36/42], [94mLoss[0m : 2.10115
[1mStep[0m  [40/42], [94mLoss[0m : 2.00241

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96809
[1mStep[0m  [4/42], [94mLoss[0m : 2.02962
[1mStep[0m  [8/42], [94mLoss[0m : 2.03041
[1mStep[0m  [12/42], [94mLoss[0m : 1.93116
[1mStep[0m  [16/42], [94mLoss[0m : 2.18389
[1mStep[0m  [20/42], [94mLoss[0m : 2.17761
[1mStep[0m  [24/42], [94mLoss[0m : 2.00943
[1mStep[0m  [28/42], [94mLoss[0m : 2.02388
[1mStep[0m  [32/42], [94mLoss[0m : 2.04241
[1mStep[0m  [36/42], [94mLoss[0m : 1.87931
[1mStep[0m  [40/42], [94mLoss[0m : 2.16686

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97093
[1mStep[0m  [4/42], [94mLoss[0m : 2.00303
[1mStep[0m  [8/42], [94mLoss[0m : 1.74212
[1mStep[0m  [12/42], [94mLoss[0m : 1.81670
[1mStep[0m  [16/42], [94mLoss[0m : 1.90145
[1mStep[0m  [20/42], [94mLoss[0m : 2.07226
[1mStep[0m  [24/42], [94mLoss[0m : 1.86756
[1mStep[0m  [28/42], [94mLoss[0m : 2.20238
[1mStep[0m  [32/42], [94mLoss[0m : 1.84270
[1mStep[0m  [36/42], [94mLoss[0m : 2.08719
[1mStep[0m  [40/42], [94mLoss[0m : 2.08699

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.005, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04806
[1mStep[0m  [4/42], [94mLoss[0m : 1.89529
[1mStep[0m  [8/42], [94mLoss[0m : 2.03291
[1mStep[0m  [12/42], [94mLoss[0m : 2.08697
[1mStep[0m  [16/42], [94mLoss[0m : 2.18949
[1mStep[0m  [20/42], [94mLoss[0m : 1.90140
[1mStep[0m  [24/42], [94mLoss[0m : 2.08468
[1mStep[0m  [28/42], [94mLoss[0m : 1.76420
[1mStep[0m  [32/42], [94mLoss[0m : 1.78417
[1mStep[0m  [36/42], [94mLoss[0m : 1.87706
[1mStep[0m  [40/42], [94mLoss[0m : 2.09254

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.518, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78864
[1mStep[0m  [4/42], [94mLoss[0m : 1.88480
[1mStep[0m  [8/42], [94mLoss[0m : 1.99969
[1mStep[0m  [12/42], [94mLoss[0m : 2.02475
[1mStep[0m  [16/42], [94mLoss[0m : 2.01261
[1mStep[0m  [20/42], [94mLoss[0m : 2.00389
[1mStep[0m  [24/42], [94mLoss[0m : 1.99683
[1mStep[0m  [28/42], [94mLoss[0m : 1.98174
[1mStep[0m  [32/42], [94mLoss[0m : 2.03159
[1mStep[0m  [36/42], [94mLoss[0m : 1.93815
[1mStep[0m  [40/42], [94mLoss[0m : 2.08528

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91844
[1mStep[0m  [4/42], [94mLoss[0m : 1.85481
[1mStep[0m  [8/42], [94mLoss[0m : 1.88747
[1mStep[0m  [12/42], [94mLoss[0m : 2.07070
[1mStep[0m  [16/42], [94mLoss[0m : 1.90429
[1mStep[0m  [20/42], [94mLoss[0m : 2.03044
[1mStep[0m  [24/42], [94mLoss[0m : 1.84154
[1mStep[0m  [28/42], [94mLoss[0m : 1.80059
[1mStep[0m  [32/42], [94mLoss[0m : 1.81135
[1mStep[0m  [36/42], [94mLoss[0m : 1.93989
[1mStep[0m  [40/42], [94mLoss[0m : 1.99049

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91258
[1mStep[0m  [4/42], [94mLoss[0m : 1.92128
[1mStep[0m  [8/42], [94mLoss[0m : 1.83933
[1mStep[0m  [12/42], [94mLoss[0m : 1.91908
[1mStep[0m  [16/42], [94mLoss[0m : 1.82220
[1mStep[0m  [20/42], [94mLoss[0m : 1.75829
[1mStep[0m  [24/42], [94mLoss[0m : 1.88243
[1mStep[0m  [28/42], [94mLoss[0m : 1.92935
[1mStep[0m  [32/42], [94mLoss[0m : 1.83666
[1mStep[0m  [36/42], [94mLoss[0m : 1.80576
[1mStep[0m  [40/42], [94mLoss[0m : 1.73707

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82787
[1mStep[0m  [4/42], [94mLoss[0m : 1.71600
[1mStep[0m  [8/42], [94mLoss[0m : 1.69999
[1mStep[0m  [12/42], [94mLoss[0m : 1.85653
[1mStep[0m  [16/42], [94mLoss[0m : 1.91901
[1mStep[0m  [20/42], [94mLoss[0m : 1.92154
[1mStep[0m  [24/42], [94mLoss[0m : 1.67538
[1mStep[0m  [28/42], [94mLoss[0m : 1.85372
[1mStep[0m  [32/42], [94mLoss[0m : 1.60742
[1mStep[0m  [36/42], [94mLoss[0m : 2.03203
[1mStep[0m  [40/42], [94mLoss[0m : 1.70967

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82524
[1mStep[0m  [4/42], [94mLoss[0m : 1.78072
[1mStep[0m  [8/42], [94mLoss[0m : 1.77468
[1mStep[0m  [12/42], [94mLoss[0m : 2.01530
[1mStep[0m  [16/42], [94mLoss[0m : 1.86846
[1mStep[0m  [20/42], [94mLoss[0m : 1.77660
[1mStep[0m  [24/42], [94mLoss[0m : 1.81248
[1mStep[0m  [28/42], [94mLoss[0m : 1.82280
[1mStep[0m  [32/42], [94mLoss[0m : 1.69883
[1mStep[0m  [36/42], [94mLoss[0m : 1.83797
[1mStep[0m  [40/42], [94mLoss[0m : 1.93690

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70201
[1mStep[0m  [4/42], [94mLoss[0m : 1.83098
[1mStep[0m  [8/42], [94mLoss[0m : 1.64357
[1mStep[0m  [12/42], [94mLoss[0m : 1.74926
[1mStep[0m  [16/42], [94mLoss[0m : 1.78401
[1mStep[0m  [20/42], [94mLoss[0m : 1.91862
[1mStep[0m  [24/42], [94mLoss[0m : 1.69744
[1mStep[0m  [28/42], [94mLoss[0m : 1.80471
[1mStep[0m  [32/42], [94mLoss[0m : 1.64657
[1mStep[0m  [36/42], [94mLoss[0m : 1.87041
[1mStep[0m  [40/42], [94mLoss[0m : 1.84145

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.533, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70594
[1mStep[0m  [4/42], [94mLoss[0m : 1.76980
[1mStep[0m  [8/42], [94mLoss[0m : 1.72750
[1mStep[0m  [12/42], [94mLoss[0m : 1.71612
[1mStep[0m  [16/42], [94mLoss[0m : 1.81925
[1mStep[0m  [20/42], [94mLoss[0m : 1.80514
[1mStep[0m  [24/42], [94mLoss[0m : 1.75234
[1mStep[0m  [28/42], [94mLoss[0m : 1.78959
[1mStep[0m  [32/42], [94mLoss[0m : 1.84473
[1mStep[0m  [36/42], [94mLoss[0m : 1.68215
[1mStep[0m  [40/42], [94mLoss[0m : 1.91774

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.491, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86156
[1mStep[0m  [4/42], [94mLoss[0m : 1.72822
[1mStep[0m  [8/42], [94mLoss[0m : 1.83819
[1mStep[0m  [12/42], [94mLoss[0m : 1.81450
[1mStep[0m  [16/42], [94mLoss[0m : 1.62701
[1mStep[0m  [20/42], [94mLoss[0m : 1.71461
[1mStep[0m  [24/42], [94mLoss[0m : 1.68246
[1mStep[0m  [28/42], [94mLoss[0m : 1.64624
[1mStep[0m  [32/42], [94mLoss[0m : 1.71931
[1mStep[0m  [36/42], [94mLoss[0m : 1.89509
[1mStep[0m  [40/42], [94mLoss[0m : 1.71662

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76840
[1mStep[0m  [4/42], [94mLoss[0m : 1.79994
[1mStep[0m  [8/42], [94mLoss[0m : 1.62477
[1mStep[0m  [12/42], [94mLoss[0m : 1.85874
[1mStep[0m  [16/42], [94mLoss[0m : 1.75443
[1mStep[0m  [20/42], [94mLoss[0m : 1.76670
[1mStep[0m  [24/42], [94mLoss[0m : 1.76492
[1mStep[0m  [28/42], [94mLoss[0m : 1.73560
[1mStep[0m  [32/42], [94mLoss[0m : 1.67616
[1mStep[0m  [36/42], [94mLoss[0m : 1.55292
[1mStep[0m  [40/42], [94mLoss[0m : 1.61006

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.732, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79792
[1mStep[0m  [4/42], [94mLoss[0m : 1.72893
[1mStep[0m  [8/42], [94mLoss[0m : 1.59373
[1mStep[0m  [12/42], [94mLoss[0m : 1.88264
[1mStep[0m  [16/42], [94mLoss[0m : 1.67374
[1mStep[0m  [20/42], [94mLoss[0m : 1.73261
[1mStep[0m  [24/42], [94mLoss[0m : 1.72858
[1mStep[0m  [28/42], [94mLoss[0m : 1.68495
[1mStep[0m  [32/42], [94mLoss[0m : 1.58448
[1mStep[0m  [36/42], [94mLoss[0m : 1.72407
[1mStep[0m  [40/42], [94mLoss[0m : 1.87470

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.551, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57932
[1mStep[0m  [4/42], [94mLoss[0m : 1.72572
[1mStep[0m  [8/42], [94mLoss[0m : 1.82767
[1mStep[0m  [12/42], [94mLoss[0m : 1.73524
[1mStep[0m  [16/42], [94mLoss[0m : 1.64241
[1mStep[0m  [20/42], [94mLoss[0m : 1.60770
[1mStep[0m  [24/42], [94mLoss[0m : 1.78276
[1mStep[0m  [28/42], [94mLoss[0m : 1.61631
[1mStep[0m  [32/42], [94mLoss[0m : 1.85441
[1mStep[0m  [36/42], [94mLoss[0m : 1.65461
[1mStep[0m  [40/42], [94mLoss[0m : 1.71204

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58560
[1mStep[0m  [4/42], [94mLoss[0m : 1.54552
[1mStep[0m  [8/42], [94mLoss[0m : 1.57655
[1mStep[0m  [12/42], [94mLoss[0m : 1.67041
[1mStep[0m  [16/42], [94mLoss[0m : 1.68456
[1mStep[0m  [20/42], [94mLoss[0m : 1.54072
[1mStep[0m  [24/42], [94mLoss[0m : 1.51898
[1mStep[0m  [28/42], [94mLoss[0m : 1.54890
[1mStep[0m  [32/42], [94mLoss[0m : 1.58974
[1mStep[0m  [36/42], [94mLoss[0m : 1.70041
[1mStep[0m  [40/42], [94mLoss[0m : 1.78553

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.529
====================================

Phase 2 - Evaluation MAE:  2.529443655695234
MAE score P1       2.331294
MAE score P2       2.529444
loss                1.67289
learning_rate      0.007525
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.65668
[1mStep[0m  [2/21], [94mLoss[0m : 10.50559
[1mStep[0m  [4/21], [94mLoss[0m : 10.29948
[1mStep[0m  [6/21], [94mLoss[0m : 10.00834
[1mStep[0m  [8/21], [94mLoss[0m : 9.70384
[1mStep[0m  [10/21], [94mLoss[0m : 9.31654
[1mStep[0m  [12/21], [94mLoss[0m : 8.97986
[1mStep[0m  [14/21], [94mLoss[0m : 8.89551
[1mStep[0m  [16/21], [94mLoss[0m : 8.82829
[1mStep[0m  [18/21], [94mLoss[0m : 8.42635
[1mStep[0m  [20/21], [94mLoss[0m : 8.25332

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.489, [92mTest[0m: 10.954, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.03076
[1mStep[0m  [2/21], [94mLoss[0m : 7.76158
[1mStep[0m  [4/21], [94mLoss[0m : 7.29028
[1mStep[0m  [6/21], [94mLoss[0m : 7.22809
[1mStep[0m  [8/21], [94mLoss[0m : 6.72057
[1mStep[0m  [10/21], [94mLoss[0m : 6.67136
[1mStep[0m  [12/21], [94mLoss[0m : 6.31740
[1mStep[0m  [14/21], [94mLoss[0m : 6.08878
[1mStep[0m  [16/21], [94mLoss[0m : 6.02064
[1mStep[0m  [18/21], [94mLoss[0m : 5.63524
[1mStep[0m  [20/21], [94mLoss[0m : 5.38658

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.725, [92mTest[0m: 9.593, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.27961
[1mStep[0m  [2/21], [94mLoss[0m : 5.23680
[1mStep[0m  [4/21], [94mLoss[0m : 4.96777
[1mStep[0m  [6/21], [94mLoss[0m : 4.82625
[1mStep[0m  [8/21], [94mLoss[0m : 4.47695
[1mStep[0m  [10/21], [94mLoss[0m : 4.10511
[1mStep[0m  [12/21], [94mLoss[0m : 3.79459
[1mStep[0m  [14/21], [94mLoss[0m : 4.17239
[1mStep[0m  [16/21], [94mLoss[0m : 3.62232
[1mStep[0m  [18/21], [94mLoss[0m : 3.78825
[1mStep[0m  [20/21], [94mLoss[0m : 3.59163

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.339, [92mTest[0m: 7.063, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.45233
[1mStep[0m  [2/21], [94mLoss[0m : 3.55559
[1mStep[0m  [4/21], [94mLoss[0m : 3.38016
[1mStep[0m  [6/21], [94mLoss[0m : 3.12941
[1mStep[0m  [8/21], [94mLoss[0m : 3.05777
[1mStep[0m  [10/21], [94mLoss[0m : 3.04807
[1mStep[0m  [12/21], [94mLoss[0m : 2.96052
[1mStep[0m  [14/21], [94mLoss[0m : 3.07111
[1mStep[0m  [16/21], [94mLoss[0m : 3.09365
[1mStep[0m  [18/21], [94mLoss[0m : 2.80942
[1mStep[0m  [20/21], [94mLoss[0m : 2.85996

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.097, [92mTest[0m: 4.839, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75763
[1mStep[0m  [2/21], [94mLoss[0m : 2.77985
[1mStep[0m  [4/21], [94mLoss[0m : 2.67118
[1mStep[0m  [6/21], [94mLoss[0m : 2.70812
[1mStep[0m  [8/21], [94mLoss[0m : 2.92492
[1mStep[0m  [10/21], [94mLoss[0m : 2.74141
[1mStep[0m  [12/21], [94mLoss[0m : 2.59348
[1mStep[0m  [14/21], [94mLoss[0m : 2.79159
[1mStep[0m  [16/21], [94mLoss[0m : 2.70413
[1mStep[0m  [18/21], [94mLoss[0m : 2.68591
[1mStep[0m  [20/21], [94mLoss[0m : 2.70794

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.726, [92mTest[0m: 3.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73428
[1mStep[0m  [2/21], [94mLoss[0m : 2.79309
[1mStep[0m  [4/21], [94mLoss[0m : 2.55367
[1mStep[0m  [6/21], [94mLoss[0m : 2.64128
[1mStep[0m  [8/21], [94mLoss[0m : 2.57792
[1mStep[0m  [10/21], [94mLoss[0m : 2.66220
[1mStep[0m  [12/21], [94mLoss[0m : 2.74036
[1mStep[0m  [14/21], [94mLoss[0m : 2.75088
[1mStep[0m  [16/21], [94mLoss[0m : 2.61139
[1mStep[0m  [18/21], [94mLoss[0m : 2.56958
[1mStep[0m  [20/21], [94mLoss[0m : 2.71221

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.948, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58739
[1mStep[0m  [2/21], [94mLoss[0m : 2.62179
[1mStep[0m  [4/21], [94mLoss[0m : 2.62785
[1mStep[0m  [6/21], [94mLoss[0m : 2.55020
[1mStep[0m  [8/21], [94mLoss[0m : 2.56996
[1mStep[0m  [10/21], [94mLoss[0m : 2.65181
[1mStep[0m  [12/21], [94mLoss[0m : 2.65523
[1mStep[0m  [14/21], [94mLoss[0m : 2.77512
[1mStep[0m  [16/21], [94mLoss[0m : 2.57942
[1mStep[0m  [18/21], [94mLoss[0m : 2.41380
[1mStep[0m  [20/21], [94mLoss[0m : 2.65344

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.752, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67494
[1mStep[0m  [2/21], [94mLoss[0m : 2.64837
[1mStep[0m  [4/21], [94mLoss[0m : 2.63891
[1mStep[0m  [6/21], [94mLoss[0m : 2.76820
[1mStep[0m  [8/21], [94mLoss[0m : 2.47752
[1mStep[0m  [10/21], [94mLoss[0m : 2.54230
[1mStep[0m  [12/21], [94mLoss[0m : 2.49247
[1mStep[0m  [14/21], [94mLoss[0m : 2.58145
[1mStep[0m  [16/21], [94mLoss[0m : 2.64266
[1mStep[0m  [18/21], [94mLoss[0m : 2.58971
[1mStep[0m  [20/21], [94mLoss[0m : 2.67333

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.669, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53223
[1mStep[0m  [2/21], [94mLoss[0m : 2.63524
[1mStep[0m  [4/21], [94mLoss[0m : 2.55557
[1mStep[0m  [6/21], [94mLoss[0m : 2.61915
[1mStep[0m  [8/21], [94mLoss[0m : 2.66367
[1mStep[0m  [10/21], [94mLoss[0m : 2.74547
[1mStep[0m  [12/21], [94mLoss[0m : 2.48568
[1mStep[0m  [14/21], [94mLoss[0m : 2.54831
[1mStep[0m  [16/21], [94mLoss[0m : 2.54802
[1mStep[0m  [18/21], [94mLoss[0m : 2.52794
[1mStep[0m  [20/21], [94mLoss[0m : 2.53203

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.629, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60288
[1mStep[0m  [2/21], [94mLoss[0m : 2.57367
[1mStep[0m  [4/21], [94mLoss[0m : 2.56383
[1mStep[0m  [6/21], [94mLoss[0m : 2.47741
[1mStep[0m  [8/21], [94mLoss[0m : 2.67564
[1mStep[0m  [10/21], [94mLoss[0m : 2.48455
[1mStep[0m  [12/21], [94mLoss[0m : 2.56631
[1mStep[0m  [14/21], [94mLoss[0m : 2.58445
[1mStep[0m  [16/21], [94mLoss[0m : 2.66739
[1mStep[0m  [18/21], [94mLoss[0m : 2.51453
[1mStep[0m  [20/21], [94mLoss[0m : 2.53082

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.575, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49675
[1mStep[0m  [2/21], [94mLoss[0m : 2.50111
[1mStep[0m  [4/21], [94mLoss[0m : 2.70119
[1mStep[0m  [6/21], [94mLoss[0m : 2.56878
[1mStep[0m  [8/21], [94mLoss[0m : 2.62890
[1mStep[0m  [10/21], [94mLoss[0m : 2.62238
[1mStep[0m  [12/21], [94mLoss[0m : 2.54895
[1mStep[0m  [14/21], [94mLoss[0m : 2.50179
[1mStep[0m  [16/21], [94mLoss[0m : 2.53949
[1mStep[0m  [18/21], [94mLoss[0m : 2.56630
[1mStep[0m  [20/21], [94mLoss[0m : 2.54431

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50749
[1mStep[0m  [2/21], [94mLoss[0m : 2.53392
[1mStep[0m  [4/21], [94mLoss[0m : 2.46647
[1mStep[0m  [6/21], [94mLoss[0m : 2.55496
[1mStep[0m  [8/21], [94mLoss[0m : 2.65828
[1mStep[0m  [10/21], [94mLoss[0m : 2.56307
[1mStep[0m  [12/21], [94mLoss[0m : 2.56684
[1mStep[0m  [14/21], [94mLoss[0m : 2.54767
[1mStep[0m  [16/21], [94mLoss[0m : 2.64800
[1mStep[0m  [18/21], [94mLoss[0m : 2.48512
[1mStep[0m  [20/21], [94mLoss[0m : 2.49724

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.523, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45003
[1mStep[0m  [2/21], [94mLoss[0m : 2.61575
[1mStep[0m  [4/21], [94mLoss[0m : 2.63184
[1mStep[0m  [6/21], [94mLoss[0m : 2.59312
[1mStep[0m  [8/21], [94mLoss[0m : 2.56534
[1mStep[0m  [10/21], [94mLoss[0m : 2.53822
[1mStep[0m  [12/21], [94mLoss[0m : 2.61034
[1mStep[0m  [14/21], [94mLoss[0m : 2.63685
[1mStep[0m  [16/21], [94mLoss[0m : 2.52876
[1mStep[0m  [18/21], [94mLoss[0m : 2.54605
[1mStep[0m  [20/21], [94mLoss[0m : 2.44137

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.518, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51080
[1mStep[0m  [2/21], [94mLoss[0m : 2.63873
[1mStep[0m  [4/21], [94mLoss[0m : 2.48860
[1mStep[0m  [6/21], [94mLoss[0m : 2.56640
[1mStep[0m  [8/21], [94mLoss[0m : 2.47951
[1mStep[0m  [10/21], [94mLoss[0m : 2.58992
[1mStep[0m  [12/21], [94mLoss[0m : 2.53748
[1mStep[0m  [14/21], [94mLoss[0m : 2.44343
[1mStep[0m  [16/21], [94mLoss[0m : 2.59116
[1mStep[0m  [18/21], [94mLoss[0m : 2.62706
[1mStep[0m  [20/21], [94mLoss[0m : 2.46344

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48549
[1mStep[0m  [2/21], [94mLoss[0m : 2.39807
[1mStep[0m  [4/21], [94mLoss[0m : 2.48982
[1mStep[0m  [6/21], [94mLoss[0m : 2.66299
[1mStep[0m  [8/21], [94mLoss[0m : 2.59003
[1mStep[0m  [10/21], [94mLoss[0m : 2.46142
[1mStep[0m  [12/21], [94mLoss[0m : 2.57796
[1mStep[0m  [14/21], [94mLoss[0m : 2.61062
[1mStep[0m  [16/21], [94mLoss[0m : 2.68637
[1mStep[0m  [18/21], [94mLoss[0m : 2.62434
[1mStep[0m  [20/21], [94mLoss[0m : 2.44105

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.496, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52304
[1mStep[0m  [2/21], [94mLoss[0m : 2.56401
[1mStep[0m  [4/21], [94mLoss[0m : 2.52327
[1mStep[0m  [6/21], [94mLoss[0m : 2.52943
[1mStep[0m  [8/21], [94mLoss[0m : 2.46723
[1mStep[0m  [10/21], [94mLoss[0m : 2.53217
[1mStep[0m  [12/21], [94mLoss[0m : 2.51980
[1mStep[0m  [14/21], [94mLoss[0m : 2.52095
[1mStep[0m  [16/21], [94mLoss[0m : 2.40479
[1mStep[0m  [18/21], [94mLoss[0m : 2.45824
[1mStep[0m  [20/21], [94mLoss[0m : 2.57945

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56203
[1mStep[0m  [2/21], [94mLoss[0m : 2.50301
[1mStep[0m  [4/21], [94mLoss[0m : 2.60908
[1mStep[0m  [6/21], [94mLoss[0m : 2.36217
[1mStep[0m  [8/21], [94mLoss[0m : 2.49042
[1mStep[0m  [10/21], [94mLoss[0m : 2.50076
[1mStep[0m  [12/21], [94mLoss[0m : 2.57729
[1mStep[0m  [14/21], [94mLoss[0m : 2.32579
[1mStep[0m  [16/21], [94mLoss[0m : 2.51992
[1mStep[0m  [18/21], [94mLoss[0m : 2.39294
[1mStep[0m  [20/21], [94mLoss[0m : 2.56751

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67047
[1mStep[0m  [2/21], [94mLoss[0m : 2.41449
[1mStep[0m  [4/21], [94mLoss[0m : 2.49809
[1mStep[0m  [6/21], [94mLoss[0m : 2.57186
[1mStep[0m  [8/21], [94mLoss[0m : 2.55424
[1mStep[0m  [10/21], [94mLoss[0m : 2.50374
[1mStep[0m  [12/21], [94mLoss[0m : 2.52240
[1mStep[0m  [14/21], [94mLoss[0m : 2.54751
[1mStep[0m  [16/21], [94mLoss[0m : 2.48174
[1mStep[0m  [18/21], [94mLoss[0m : 2.54933
[1mStep[0m  [20/21], [94mLoss[0m : 2.43566

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54879
[1mStep[0m  [2/21], [94mLoss[0m : 2.55899
[1mStep[0m  [4/21], [94mLoss[0m : 2.50988
[1mStep[0m  [6/21], [94mLoss[0m : 2.58127
[1mStep[0m  [8/21], [94mLoss[0m : 2.28727
[1mStep[0m  [10/21], [94mLoss[0m : 2.43477
[1mStep[0m  [12/21], [94mLoss[0m : 2.65074
[1mStep[0m  [14/21], [94mLoss[0m : 2.54491
[1mStep[0m  [16/21], [94mLoss[0m : 2.45915
[1mStep[0m  [18/21], [94mLoss[0m : 2.61385
[1mStep[0m  [20/21], [94mLoss[0m : 2.47336

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.490, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48689
[1mStep[0m  [2/21], [94mLoss[0m : 2.44330
[1mStep[0m  [4/21], [94mLoss[0m : 2.47660
[1mStep[0m  [6/21], [94mLoss[0m : 2.55242
[1mStep[0m  [8/21], [94mLoss[0m : 2.49994
[1mStep[0m  [10/21], [94mLoss[0m : 2.60560
[1mStep[0m  [12/21], [94mLoss[0m : 2.49906
[1mStep[0m  [14/21], [94mLoss[0m : 2.51295
[1mStep[0m  [16/21], [94mLoss[0m : 2.38205
[1mStep[0m  [18/21], [94mLoss[0m : 2.58871
[1mStep[0m  [20/21], [94mLoss[0m : 2.42913

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.447, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66307
[1mStep[0m  [2/21], [94mLoss[0m : 2.44235
[1mStep[0m  [4/21], [94mLoss[0m : 2.56863
[1mStep[0m  [6/21], [94mLoss[0m : 2.57879
[1mStep[0m  [8/21], [94mLoss[0m : 2.40988
[1mStep[0m  [10/21], [94mLoss[0m : 2.47997
[1mStep[0m  [12/21], [94mLoss[0m : 2.44767
[1mStep[0m  [14/21], [94mLoss[0m : 2.54153
[1mStep[0m  [16/21], [94mLoss[0m : 2.64992
[1mStep[0m  [18/21], [94mLoss[0m : 2.48420
[1mStep[0m  [20/21], [94mLoss[0m : 2.39043

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.433, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44082
[1mStep[0m  [2/21], [94mLoss[0m : 2.36907
[1mStep[0m  [4/21], [94mLoss[0m : 2.48194
[1mStep[0m  [6/21], [94mLoss[0m : 2.43727
[1mStep[0m  [8/21], [94mLoss[0m : 2.52561
[1mStep[0m  [10/21], [94mLoss[0m : 2.32087
[1mStep[0m  [12/21], [94mLoss[0m : 2.42624
[1mStep[0m  [14/21], [94mLoss[0m : 2.52957
[1mStep[0m  [16/21], [94mLoss[0m : 2.64043
[1mStep[0m  [18/21], [94mLoss[0m : 2.51945
[1mStep[0m  [20/21], [94mLoss[0m : 2.43174

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.430, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31073
[1mStep[0m  [2/21], [94mLoss[0m : 2.49736
[1mStep[0m  [4/21], [94mLoss[0m : 2.55240
[1mStep[0m  [6/21], [94mLoss[0m : 2.26820
[1mStep[0m  [8/21], [94mLoss[0m : 2.46112
[1mStep[0m  [10/21], [94mLoss[0m : 2.42064
[1mStep[0m  [12/21], [94mLoss[0m : 2.43298
[1mStep[0m  [14/21], [94mLoss[0m : 2.56241
[1mStep[0m  [16/21], [94mLoss[0m : 2.53405
[1mStep[0m  [18/21], [94mLoss[0m : 2.51524
[1mStep[0m  [20/21], [94mLoss[0m : 2.52876

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.416, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51704
[1mStep[0m  [2/21], [94mLoss[0m : 2.46482
[1mStep[0m  [4/21], [94mLoss[0m : 2.43385
[1mStep[0m  [6/21], [94mLoss[0m : 2.31760
[1mStep[0m  [8/21], [94mLoss[0m : 2.47320
[1mStep[0m  [10/21], [94mLoss[0m : 2.45362
[1mStep[0m  [12/21], [94mLoss[0m : 2.47297
[1mStep[0m  [14/21], [94mLoss[0m : 2.40317
[1mStep[0m  [16/21], [94mLoss[0m : 2.37730
[1mStep[0m  [18/21], [94mLoss[0m : 2.38728
[1mStep[0m  [20/21], [94mLoss[0m : 2.58955

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.409, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49958
[1mStep[0m  [2/21], [94mLoss[0m : 2.41257
[1mStep[0m  [4/21], [94mLoss[0m : 2.39659
[1mStep[0m  [6/21], [94mLoss[0m : 2.45031
[1mStep[0m  [8/21], [94mLoss[0m : 2.38787
[1mStep[0m  [10/21], [94mLoss[0m : 2.56847
[1mStep[0m  [12/21], [94mLoss[0m : 2.47969
[1mStep[0m  [14/21], [94mLoss[0m : 2.46897
[1mStep[0m  [16/21], [94mLoss[0m : 2.43644
[1mStep[0m  [18/21], [94mLoss[0m : 2.51921
[1mStep[0m  [20/21], [94mLoss[0m : 2.62577

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.420, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53690
[1mStep[0m  [2/21], [94mLoss[0m : 2.48511
[1mStep[0m  [4/21], [94mLoss[0m : 2.54435
[1mStep[0m  [6/21], [94mLoss[0m : 2.61759
[1mStep[0m  [8/21], [94mLoss[0m : 2.44423
[1mStep[0m  [10/21], [94mLoss[0m : 2.55585
[1mStep[0m  [12/21], [94mLoss[0m : 2.33678
[1mStep[0m  [14/21], [94mLoss[0m : 2.50646
[1mStep[0m  [16/21], [94mLoss[0m : 2.45901
[1mStep[0m  [18/21], [94mLoss[0m : 2.37921
[1mStep[0m  [20/21], [94mLoss[0m : 2.48247

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.429, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51973
[1mStep[0m  [2/21], [94mLoss[0m : 2.44607
[1mStep[0m  [4/21], [94mLoss[0m : 2.51686
[1mStep[0m  [6/21], [94mLoss[0m : 2.39025
[1mStep[0m  [8/21], [94mLoss[0m : 2.38693
[1mStep[0m  [10/21], [94mLoss[0m : 2.40274
[1mStep[0m  [12/21], [94mLoss[0m : 2.46680
[1mStep[0m  [14/21], [94mLoss[0m : 2.56528
[1mStep[0m  [16/21], [94mLoss[0m : 2.40441
[1mStep[0m  [18/21], [94mLoss[0m : 2.57737
[1mStep[0m  [20/21], [94mLoss[0m : 2.43902

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.421, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42376
[1mStep[0m  [2/21], [94mLoss[0m : 2.41817
[1mStep[0m  [4/21], [94mLoss[0m : 2.50161
[1mStep[0m  [6/21], [94mLoss[0m : 2.55405
[1mStep[0m  [8/21], [94mLoss[0m : 2.31611
[1mStep[0m  [10/21], [94mLoss[0m : 2.44068
[1mStep[0m  [12/21], [94mLoss[0m : 2.41879
[1mStep[0m  [14/21], [94mLoss[0m : 2.47099
[1mStep[0m  [16/21], [94mLoss[0m : 2.63539
[1mStep[0m  [18/21], [94mLoss[0m : 2.58582
[1mStep[0m  [20/21], [94mLoss[0m : 2.42727

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34023
[1mStep[0m  [2/21], [94mLoss[0m : 2.46148
[1mStep[0m  [4/21], [94mLoss[0m : 2.51343
[1mStep[0m  [6/21], [94mLoss[0m : 2.28576
[1mStep[0m  [8/21], [94mLoss[0m : 2.43774
[1mStep[0m  [10/21], [94mLoss[0m : 2.46362
[1mStep[0m  [12/21], [94mLoss[0m : 2.51949
[1mStep[0m  [14/21], [94mLoss[0m : 2.50811
[1mStep[0m  [16/21], [94mLoss[0m : 2.50660
[1mStep[0m  [18/21], [94mLoss[0m : 2.50327
[1mStep[0m  [20/21], [94mLoss[0m : 2.46525

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.414, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47223
[1mStep[0m  [2/21], [94mLoss[0m : 2.45885
[1mStep[0m  [4/21], [94mLoss[0m : 2.51560
[1mStep[0m  [6/21], [94mLoss[0m : 2.45406
[1mStep[0m  [8/21], [94mLoss[0m : 2.43041
[1mStep[0m  [10/21], [94mLoss[0m : 2.41325
[1mStep[0m  [12/21], [94mLoss[0m : 2.41651
[1mStep[0m  [14/21], [94mLoss[0m : 2.41891
[1mStep[0m  [16/21], [94mLoss[0m : 2.26771
[1mStep[0m  [18/21], [94mLoss[0m : 2.38495
[1mStep[0m  [20/21], [94mLoss[0m : 2.48086

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.441, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.414
====================================

Phase 1 - Evaluation MAE:  2.4140190056392123
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.41668
[1mStep[0m  [2/21], [94mLoss[0m : 2.50947
[1mStep[0m  [4/21], [94mLoss[0m : 2.46593
[1mStep[0m  [6/21], [94mLoss[0m : 2.59499
[1mStep[0m  [8/21], [94mLoss[0m : 2.69903
[1mStep[0m  [10/21], [94mLoss[0m : 2.54868
[1mStep[0m  [12/21], [94mLoss[0m : 2.67478
[1mStep[0m  [14/21], [94mLoss[0m : 2.47183
[1mStep[0m  [16/21], [94mLoss[0m : 2.57824
[1mStep[0m  [18/21], [94mLoss[0m : 2.53172
[1mStep[0m  [20/21], [94mLoss[0m : 2.43985

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47046
[1mStep[0m  [2/21], [94mLoss[0m : 2.57983
[1mStep[0m  [4/21], [94mLoss[0m : 2.53447
[1mStep[0m  [6/21], [94mLoss[0m : 2.45982
[1mStep[0m  [8/21], [94mLoss[0m : 2.58798
[1mStep[0m  [10/21], [94mLoss[0m : 2.44661
[1mStep[0m  [12/21], [94mLoss[0m : 2.42430
[1mStep[0m  [14/21], [94mLoss[0m : 2.53906
[1mStep[0m  [16/21], [94mLoss[0m : 2.57035
[1mStep[0m  [18/21], [94mLoss[0m : 2.60846
[1mStep[0m  [20/21], [94mLoss[0m : 2.58950

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41006
[1mStep[0m  [2/21], [94mLoss[0m : 2.46155
[1mStep[0m  [4/21], [94mLoss[0m : 2.56135
[1mStep[0m  [6/21], [94mLoss[0m : 2.47053
[1mStep[0m  [8/21], [94mLoss[0m : 2.51463
[1mStep[0m  [10/21], [94mLoss[0m : 2.39286
[1mStep[0m  [12/21], [94mLoss[0m : 2.35208
[1mStep[0m  [14/21], [94mLoss[0m : 2.36020
[1mStep[0m  [16/21], [94mLoss[0m : 2.61207
[1mStep[0m  [18/21], [94mLoss[0m : 2.46085
[1mStep[0m  [20/21], [94mLoss[0m : 2.66534

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54966
[1mStep[0m  [2/21], [94mLoss[0m : 2.39029
[1mStep[0m  [4/21], [94mLoss[0m : 2.44546
[1mStep[0m  [6/21], [94mLoss[0m : 2.34832
[1mStep[0m  [8/21], [94mLoss[0m : 2.46749
[1mStep[0m  [10/21], [94mLoss[0m : 2.47690
[1mStep[0m  [12/21], [94mLoss[0m : 2.44955
[1mStep[0m  [14/21], [94mLoss[0m : 2.54646
[1mStep[0m  [16/21], [94mLoss[0m : 2.47671
[1mStep[0m  [18/21], [94mLoss[0m : 2.54672
[1mStep[0m  [20/21], [94mLoss[0m : 2.47704

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.642, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51654
[1mStep[0m  [2/21], [94mLoss[0m : 2.32247
[1mStep[0m  [4/21], [94mLoss[0m : 2.54758
[1mStep[0m  [6/21], [94mLoss[0m : 2.30082
[1mStep[0m  [8/21], [94mLoss[0m : 2.44796
[1mStep[0m  [10/21], [94mLoss[0m : 2.38051
[1mStep[0m  [12/21], [94mLoss[0m : 2.53436
[1mStep[0m  [14/21], [94mLoss[0m : 2.40237
[1mStep[0m  [16/21], [94mLoss[0m : 2.47344
[1mStep[0m  [18/21], [94mLoss[0m : 2.52912
[1mStep[0m  [20/21], [94mLoss[0m : 2.39711

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.474, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29682
[1mStep[0m  [2/21], [94mLoss[0m : 2.30475
[1mStep[0m  [4/21], [94mLoss[0m : 2.33298
[1mStep[0m  [6/21], [94mLoss[0m : 2.55075
[1mStep[0m  [8/21], [94mLoss[0m : 2.54154
[1mStep[0m  [10/21], [94mLoss[0m : 2.56459
[1mStep[0m  [12/21], [94mLoss[0m : 2.46703
[1mStep[0m  [14/21], [94mLoss[0m : 2.30781
[1mStep[0m  [16/21], [94mLoss[0m : 2.41271
[1mStep[0m  [18/21], [94mLoss[0m : 2.41874
[1mStep[0m  [20/21], [94mLoss[0m : 2.54656

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.547, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45028
[1mStep[0m  [2/21], [94mLoss[0m : 2.28005
[1mStep[0m  [4/21], [94mLoss[0m : 2.42588
[1mStep[0m  [6/21], [94mLoss[0m : 2.42965
[1mStep[0m  [8/21], [94mLoss[0m : 2.48581
[1mStep[0m  [10/21], [94mLoss[0m : 2.43247
[1mStep[0m  [12/21], [94mLoss[0m : 2.33527
[1mStep[0m  [14/21], [94mLoss[0m : 2.63095
[1mStep[0m  [16/21], [94mLoss[0m : 2.54886
[1mStep[0m  [18/21], [94mLoss[0m : 2.63946
[1mStep[0m  [20/21], [94mLoss[0m : 2.33912

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.611, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39360
[1mStep[0m  [2/21], [94mLoss[0m : 2.34746
[1mStep[0m  [4/21], [94mLoss[0m : 2.34315
[1mStep[0m  [6/21], [94mLoss[0m : 2.58986
[1mStep[0m  [8/21], [94mLoss[0m : 2.31236
[1mStep[0m  [10/21], [94mLoss[0m : 2.39487
[1mStep[0m  [12/21], [94mLoss[0m : 2.26998
[1mStep[0m  [14/21], [94mLoss[0m : 2.36040
[1mStep[0m  [16/21], [94mLoss[0m : 2.62093
[1mStep[0m  [18/21], [94mLoss[0m : 2.41839
[1mStep[0m  [20/21], [94mLoss[0m : 2.37245

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.599, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35775
[1mStep[0m  [2/21], [94mLoss[0m : 2.33331
[1mStep[0m  [4/21], [94mLoss[0m : 2.37030
[1mStep[0m  [6/21], [94mLoss[0m : 2.30238
[1mStep[0m  [8/21], [94mLoss[0m : 2.33785
[1mStep[0m  [10/21], [94mLoss[0m : 2.37817
[1mStep[0m  [12/21], [94mLoss[0m : 2.32455
[1mStep[0m  [14/21], [94mLoss[0m : 2.42439
[1mStep[0m  [16/21], [94mLoss[0m : 2.32873
[1mStep[0m  [18/21], [94mLoss[0m : 2.48037
[1mStep[0m  [20/21], [94mLoss[0m : 2.53297

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.567, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27700
[1mStep[0m  [2/21], [94mLoss[0m : 2.25194
[1mStep[0m  [4/21], [94mLoss[0m : 2.33678
[1mStep[0m  [6/21], [94mLoss[0m : 2.39805
[1mStep[0m  [8/21], [94mLoss[0m : 2.46310
[1mStep[0m  [10/21], [94mLoss[0m : 2.37513
[1mStep[0m  [12/21], [94mLoss[0m : 2.26350
[1mStep[0m  [14/21], [94mLoss[0m : 2.30696
[1mStep[0m  [16/21], [94mLoss[0m : 2.23860
[1mStep[0m  [18/21], [94mLoss[0m : 2.43363
[1mStep[0m  [20/21], [94mLoss[0m : 2.47964

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32331
[1mStep[0m  [2/21], [94mLoss[0m : 2.25726
[1mStep[0m  [4/21], [94mLoss[0m : 2.41355
[1mStep[0m  [6/21], [94mLoss[0m : 2.49707
[1mStep[0m  [8/21], [94mLoss[0m : 2.33136
[1mStep[0m  [10/21], [94mLoss[0m : 2.29686
[1mStep[0m  [12/21], [94mLoss[0m : 2.30654
[1mStep[0m  [14/21], [94mLoss[0m : 2.39263
[1mStep[0m  [16/21], [94mLoss[0m : 2.30757
[1mStep[0m  [18/21], [94mLoss[0m : 2.33169
[1mStep[0m  [20/21], [94mLoss[0m : 2.44066

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41328
[1mStep[0m  [2/21], [94mLoss[0m : 2.41207
[1mStep[0m  [4/21], [94mLoss[0m : 2.35179
[1mStep[0m  [6/21], [94mLoss[0m : 2.13215
[1mStep[0m  [8/21], [94mLoss[0m : 2.42905
[1mStep[0m  [10/21], [94mLoss[0m : 2.44366
[1mStep[0m  [12/21], [94mLoss[0m : 2.36615
[1mStep[0m  [14/21], [94mLoss[0m : 2.26864
[1mStep[0m  [16/21], [94mLoss[0m : 2.37125
[1mStep[0m  [18/21], [94mLoss[0m : 2.31503
[1mStep[0m  [20/21], [94mLoss[0m : 2.23264

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.541, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.21221
[1mStep[0m  [2/21], [94mLoss[0m : 2.31806
[1mStep[0m  [4/21], [94mLoss[0m : 2.32496
[1mStep[0m  [6/21], [94mLoss[0m : 2.36589
[1mStep[0m  [8/21], [94mLoss[0m : 2.37601
[1mStep[0m  [10/21], [94mLoss[0m : 2.24294
[1mStep[0m  [12/21], [94mLoss[0m : 2.35601
[1mStep[0m  [14/21], [94mLoss[0m : 2.34208
[1mStep[0m  [16/21], [94mLoss[0m : 2.50244
[1mStep[0m  [18/21], [94mLoss[0m : 2.39589
[1mStep[0m  [20/21], [94mLoss[0m : 2.15171

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44098
[1mStep[0m  [2/21], [94mLoss[0m : 2.26165
[1mStep[0m  [4/21], [94mLoss[0m : 2.27861
[1mStep[0m  [6/21], [94mLoss[0m : 2.18648
[1mStep[0m  [8/21], [94mLoss[0m : 2.31054
[1mStep[0m  [10/21], [94mLoss[0m : 2.31090
[1mStep[0m  [12/21], [94mLoss[0m : 2.26154
[1mStep[0m  [14/21], [94mLoss[0m : 2.35008
[1mStep[0m  [16/21], [94mLoss[0m : 2.41369
[1mStep[0m  [18/21], [94mLoss[0m : 2.27232
[1mStep[0m  [20/21], [94mLoss[0m : 2.35515

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28690
[1mStep[0m  [2/21], [94mLoss[0m : 2.36525
[1mStep[0m  [4/21], [94mLoss[0m : 2.31481
[1mStep[0m  [6/21], [94mLoss[0m : 2.26237
[1mStep[0m  [8/21], [94mLoss[0m : 2.14572
[1mStep[0m  [10/21], [94mLoss[0m : 2.27987
[1mStep[0m  [12/21], [94mLoss[0m : 2.26385
[1mStep[0m  [14/21], [94mLoss[0m : 2.28924
[1mStep[0m  [16/21], [94mLoss[0m : 2.22910
[1mStep[0m  [18/21], [94mLoss[0m : 2.33518
[1mStep[0m  [20/21], [94mLoss[0m : 2.41049

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.34596
[1mStep[0m  [2/21], [94mLoss[0m : 2.24722
[1mStep[0m  [4/21], [94mLoss[0m : 2.34172
[1mStep[0m  [6/21], [94mLoss[0m : 2.21729
[1mStep[0m  [8/21], [94mLoss[0m : 2.26211
[1mStep[0m  [10/21], [94mLoss[0m : 2.26545
[1mStep[0m  [12/21], [94mLoss[0m : 2.23727
[1mStep[0m  [14/21], [94mLoss[0m : 2.20264
[1mStep[0m  [16/21], [94mLoss[0m : 2.13375
[1mStep[0m  [18/21], [94mLoss[0m : 2.24131
[1mStep[0m  [20/21], [94mLoss[0m : 2.31785

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29416
[1mStep[0m  [2/21], [94mLoss[0m : 2.35031
[1mStep[0m  [4/21], [94mLoss[0m : 2.09213
[1mStep[0m  [6/21], [94mLoss[0m : 2.20916
[1mStep[0m  [8/21], [94mLoss[0m : 2.15227
[1mStep[0m  [10/21], [94mLoss[0m : 2.03314
[1mStep[0m  [12/21], [94mLoss[0m : 2.08745
[1mStep[0m  [14/21], [94mLoss[0m : 2.24007
[1mStep[0m  [16/21], [94mLoss[0m : 2.31428
[1mStep[0m  [18/21], [94mLoss[0m : 2.32524
[1mStep[0m  [20/21], [94mLoss[0m : 2.28348

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.514, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23189
[1mStep[0m  [2/21], [94mLoss[0m : 2.19665
[1mStep[0m  [4/21], [94mLoss[0m : 2.12316
[1mStep[0m  [6/21], [94mLoss[0m : 2.25808
[1mStep[0m  [8/21], [94mLoss[0m : 2.21615
[1mStep[0m  [10/21], [94mLoss[0m : 2.06385
[1mStep[0m  [12/21], [94mLoss[0m : 2.24306
[1mStep[0m  [14/21], [94mLoss[0m : 2.18505
[1mStep[0m  [16/21], [94mLoss[0m : 2.27355
[1mStep[0m  [18/21], [94mLoss[0m : 2.21492
[1mStep[0m  [20/21], [94mLoss[0m : 2.35258

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28021
[1mStep[0m  [2/21], [94mLoss[0m : 2.24005
[1mStep[0m  [4/21], [94mLoss[0m : 2.33607
[1mStep[0m  [6/21], [94mLoss[0m : 2.35499
[1mStep[0m  [8/21], [94mLoss[0m : 2.15072
[1mStep[0m  [10/21], [94mLoss[0m : 2.15390
[1mStep[0m  [12/21], [94mLoss[0m : 2.09230
[1mStep[0m  [14/21], [94mLoss[0m : 2.17028
[1mStep[0m  [16/21], [94mLoss[0m : 2.30603
[1mStep[0m  [18/21], [94mLoss[0m : 2.27984
[1mStep[0m  [20/21], [94mLoss[0m : 2.38026

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.508, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.26713
[1mStep[0m  [2/21], [94mLoss[0m : 2.17884
[1mStep[0m  [4/21], [94mLoss[0m : 2.20438
[1mStep[0m  [6/21], [94mLoss[0m : 2.22692
[1mStep[0m  [8/21], [94mLoss[0m : 2.22447
[1mStep[0m  [10/21], [94mLoss[0m : 2.16695
[1mStep[0m  [12/21], [94mLoss[0m : 2.33736
[1mStep[0m  [14/21], [94mLoss[0m : 2.21359
[1mStep[0m  [16/21], [94mLoss[0m : 2.22889
[1mStep[0m  [18/21], [94mLoss[0m : 2.01448
[1mStep[0m  [20/21], [94mLoss[0m : 2.27456

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10812
[1mStep[0m  [2/21], [94mLoss[0m : 2.22301
[1mStep[0m  [4/21], [94mLoss[0m : 2.12043
[1mStep[0m  [6/21], [94mLoss[0m : 2.06604
[1mStep[0m  [8/21], [94mLoss[0m : 2.23617
[1mStep[0m  [10/21], [94mLoss[0m : 2.04485
[1mStep[0m  [12/21], [94mLoss[0m : 2.21826
[1mStep[0m  [14/21], [94mLoss[0m : 2.14075
[1mStep[0m  [16/21], [94mLoss[0m : 2.26464
[1mStep[0m  [18/21], [94mLoss[0m : 2.20226
[1mStep[0m  [20/21], [94mLoss[0m : 2.18008

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.470, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20342
[1mStep[0m  [2/21], [94mLoss[0m : 2.18533
[1mStep[0m  [4/21], [94mLoss[0m : 2.20311
[1mStep[0m  [6/21], [94mLoss[0m : 2.14954
[1mStep[0m  [8/21], [94mLoss[0m : 2.04382
[1mStep[0m  [10/21], [94mLoss[0m : 1.96810
[1mStep[0m  [12/21], [94mLoss[0m : 2.18534
[1mStep[0m  [14/21], [94mLoss[0m : 2.23566
[1mStep[0m  [16/21], [94mLoss[0m : 2.09998
[1mStep[0m  [18/21], [94mLoss[0m : 2.05995
[1mStep[0m  [20/21], [94mLoss[0m : 2.03939

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.482, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29240
[1mStep[0m  [2/21], [94mLoss[0m : 2.14217
[1mStep[0m  [4/21], [94mLoss[0m : 2.16216
[1mStep[0m  [6/21], [94mLoss[0m : 2.24385
[1mStep[0m  [8/21], [94mLoss[0m : 2.10151
[1mStep[0m  [10/21], [94mLoss[0m : 2.21671
[1mStep[0m  [12/21], [94mLoss[0m : 2.08546
[1mStep[0m  [14/21], [94mLoss[0m : 1.99615
[1mStep[0m  [16/21], [94mLoss[0m : 2.13791
[1mStep[0m  [18/21], [94mLoss[0m : 2.30499
[1mStep[0m  [20/21], [94mLoss[0m : 2.10606

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.527, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.18664
[1mStep[0m  [2/21], [94mLoss[0m : 2.17386
[1mStep[0m  [4/21], [94mLoss[0m : 2.02484
[1mStep[0m  [6/21], [94mLoss[0m : 2.09498
[1mStep[0m  [8/21], [94mLoss[0m : 2.13911
[1mStep[0m  [10/21], [94mLoss[0m : 2.20850
[1mStep[0m  [12/21], [94mLoss[0m : 2.10390
[1mStep[0m  [14/21], [94mLoss[0m : 2.00336
[1mStep[0m  [16/21], [94mLoss[0m : 2.16203
[1mStep[0m  [18/21], [94mLoss[0m : 2.16377
[1mStep[0m  [20/21], [94mLoss[0m : 2.19304

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07029
[1mStep[0m  [2/21], [94mLoss[0m : 2.05512
[1mStep[0m  [4/21], [94mLoss[0m : 2.04488
[1mStep[0m  [6/21], [94mLoss[0m : 2.06902
[1mStep[0m  [8/21], [94mLoss[0m : 2.13537
[1mStep[0m  [10/21], [94mLoss[0m : 2.04402
[1mStep[0m  [12/21], [94mLoss[0m : 2.09588
[1mStep[0m  [14/21], [94mLoss[0m : 2.05323
[1mStep[0m  [16/21], [94mLoss[0m : 2.09856
[1mStep[0m  [18/21], [94mLoss[0m : 2.12023
[1mStep[0m  [20/21], [94mLoss[0m : 2.15617

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13043
[1mStep[0m  [2/21], [94mLoss[0m : 2.07008
[1mStep[0m  [4/21], [94mLoss[0m : 2.07078
[1mStep[0m  [6/21], [94mLoss[0m : 2.10321
[1mStep[0m  [8/21], [94mLoss[0m : 1.94731
[1mStep[0m  [10/21], [94mLoss[0m : 2.10581
[1mStep[0m  [12/21], [94mLoss[0m : 2.02327
[1mStep[0m  [14/21], [94mLoss[0m : 1.99833
[1mStep[0m  [16/21], [94mLoss[0m : 2.02797
[1mStep[0m  [18/21], [94mLoss[0m : 2.02688
[1mStep[0m  [20/21], [94mLoss[0m : 2.10478

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.505, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.07435
[1mStep[0m  [2/21], [94mLoss[0m : 2.06607
[1mStep[0m  [4/21], [94mLoss[0m : 2.02820
[1mStep[0m  [6/21], [94mLoss[0m : 2.14450
[1mStep[0m  [8/21], [94mLoss[0m : 2.15733
[1mStep[0m  [10/21], [94mLoss[0m : 2.02874
[1mStep[0m  [12/21], [94mLoss[0m : 2.00615
[1mStep[0m  [14/21], [94mLoss[0m : 1.94391
[1mStep[0m  [16/21], [94mLoss[0m : 2.25373
[1mStep[0m  [18/21], [94mLoss[0m : 2.11257
[1mStep[0m  [20/21], [94mLoss[0m : 2.02391

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.09957
[1mStep[0m  [2/21], [94mLoss[0m : 1.97782
[1mStep[0m  [4/21], [94mLoss[0m : 1.95876
[1mStep[0m  [6/21], [94mLoss[0m : 2.14007
[1mStep[0m  [8/21], [94mLoss[0m : 1.93212
[1mStep[0m  [10/21], [94mLoss[0m : 2.02072
[1mStep[0m  [12/21], [94mLoss[0m : 2.10168
[1mStep[0m  [14/21], [94mLoss[0m : 2.04015
[1mStep[0m  [16/21], [94mLoss[0m : 2.08440
[1mStep[0m  [18/21], [94mLoss[0m : 2.02935
[1mStep[0m  [20/21], [94mLoss[0m : 2.01540

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.559, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02682
[1mStep[0m  [2/21], [94mLoss[0m : 2.05100
[1mStep[0m  [4/21], [94mLoss[0m : 1.91568
[1mStep[0m  [6/21], [94mLoss[0m : 1.95887
[1mStep[0m  [8/21], [94mLoss[0m : 2.00011
[1mStep[0m  [10/21], [94mLoss[0m : 2.10291
[1mStep[0m  [12/21], [94mLoss[0m : 2.01241
[1mStep[0m  [14/21], [94mLoss[0m : 2.12322
[1mStep[0m  [16/21], [94mLoss[0m : 1.93406
[1mStep[0m  [18/21], [94mLoss[0m : 2.02250
[1mStep[0m  [20/21], [94mLoss[0m : 2.05826

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.529, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.93489
[1mStep[0m  [2/21], [94mLoss[0m : 2.09726
[1mStep[0m  [4/21], [94mLoss[0m : 2.06437
[1mStep[0m  [6/21], [94mLoss[0m : 1.99820
[1mStep[0m  [8/21], [94mLoss[0m : 2.07649
[1mStep[0m  [10/21], [94mLoss[0m : 1.91168
[1mStep[0m  [12/21], [94mLoss[0m : 2.04905
[1mStep[0m  [14/21], [94mLoss[0m : 2.08286
[1mStep[0m  [16/21], [94mLoss[0m : 1.93743
[1mStep[0m  [18/21], [94mLoss[0m : 1.97947
[1mStep[0m  [20/21], [94mLoss[0m : 2.12345

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.542, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.539
====================================

Phase 2 - Evaluation MAE:  2.5389857632773265
MAE score P1        2.414019
MAE score P2        2.538986
loss                2.014495
learning_rate       0.007525
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.29159
[1mStep[0m  [4/42], [94mLoss[0m : 10.55582
[1mStep[0m  [8/42], [94mLoss[0m : 9.83861
[1mStep[0m  [12/42], [94mLoss[0m : 9.37049
[1mStep[0m  [16/42], [94mLoss[0m : 8.67095
[1mStep[0m  [20/42], [94mLoss[0m : 8.29133
[1mStep[0m  [24/42], [94mLoss[0m : 7.89267
[1mStep[0m  [28/42], [94mLoss[0m : 7.03226
[1mStep[0m  [32/42], [94mLoss[0m : 6.54153
[1mStep[0m  [36/42], [94mLoss[0m : 6.16964
[1mStep[0m  [40/42], [94mLoss[0m : 5.60428

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.046, [92mTest[0m: 10.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.54029
[1mStep[0m  [4/42], [94mLoss[0m : 5.49895
[1mStep[0m  [8/42], [94mLoss[0m : 4.92482
[1mStep[0m  [12/42], [94mLoss[0m : 4.27238
[1mStep[0m  [16/42], [94mLoss[0m : 4.34555
[1mStep[0m  [20/42], [94mLoss[0m : 3.94789
[1mStep[0m  [24/42], [94mLoss[0m : 3.96242
[1mStep[0m  [28/42], [94mLoss[0m : 3.67524
[1mStep[0m  [32/42], [94mLoss[0m : 3.38344
[1mStep[0m  [36/42], [94mLoss[0m : 3.37827
[1mStep[0m  [40/42], [94mLoss[0m : 3.06369

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.092, [92mTest[0m: 5.596, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.18273
[1mStep[0m  [4/42], [94mLoss[0m : 2.86990
[1mStep[0m  [8/42], [94mLoss[0m : 2.92080
[1mStep[0m  [12/42], [94mLoss[0m : 2.85440
[1mStep[0m  [16/42], [94mLoss[0m : 2.69382
[1mStep[0m  [20/42], [94mLoss[0m : 2.55714
[1mStep[0m  [24/42], [94mLoss[0m : 2.96534
[1mStep[0m  [28/42], [94mLoss[0m : 2.82407
[1mStep[0m  [32/42], [94mLoss[0m : 2.69997
[1mStep[0m  [36/42], [94mLoss[0m : 2.47696
[1mStep[0m  [40/42], [94mLoss[0m : 2.48155

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.793, [92mTest[0m: 2.934, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50537
[1mStep[0m  [4/42], [94mLoss[0m : 2.75166
[1mStep[0m  [8/42], [94mLoss[0m : 2.43687
[1mStep[0m  [12/42], [94mLoss[0m : 2.80917
[1mStep[0m  [16/42], [94mLoss[0m : 2.51603
[1mStep[0m  [20/42], [94mLoss[0m : 2.30013
[1mStep[0m  [24/42], [94mLoss[0m : 2.72387
[1mStep[0m  [28/42], [94mLoss[0m : 2.49803
[1mStep[0m  [32/42], [94mLoss[0m : 2.60629
[1mStep[0m  [36/42], [94mLoss[0m : 2.42514
[1mStep[0m  [40/42], [94mLoss[0m : 2.62609

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34058
[1mStep[0m  [4/42], [94mLoss[0m : 2.58139
[1mStep[0m  [8/42], [94mLoss[0m : 2.56924
[1mStep[0m  [12/42], [94mLoss[0m : 2.70830
[1mStep[0m  [16/42], [94mLoss[0m : 2.58989
[1mStep[0m  [20/42], [94mLoss[0m : 2.55802
[1mStep[0m  [24/42], [94mLoss[0m : 2.83438
[1mStep[0m  [28/42], [94mLoss[0m : 2.31268
[1mStep[0m  [32/42], [94mLoss[0m : 2.58966
[1mStep[0m  [36/42], [94mLoss[0m : 2.42108
[1mStep[0m  [40/42], [94mLoss[0m : 2.70855

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46878
[1mStep[0m  [4/42], [94mLoss[0m : 2.36080
[1mStep[0m  [8/42], [94mLoss[0m : 2.69512
[1mStep[0m  [12/42], [94mLoss[0m : 2.23926
[1mStep[0m  [16/42], [94mLoss[0m : 2.69310
[1mStep[0m  [20/42], [94mLoss[0m : 2.74422
[1mStep[0m  [24/42], [94mLoss[0m : 2.45227
[1mStep[0m  [28/42], [94mLoss[0m : 2.47238
[1mStep[0m  [32/42], [94mLoss[0m : 2.59496
[1mStep[0m  [36/42], [94mLoss[0m : 2.55201
[1mStep[0m  [40/42], [94mLoss[0m : 2.44770

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56361
[1mStep[0m  [4/42], [94mLoss[0m : 2.60697
[1mStep[0m  [8/42], [94mLoss[0m : 2.63205
[1mStep[0m  [12/42], [94mLoss[0m : 2.65988
[1mStep[0m  [16/42], [94mLoss[0m : 2.61134
[1mStep[0m  [20/42], [94mLoss[0m : 2.37080
[1mStep[0m  [24/42], [94mLoss[0m : 2.49458
[1mStep[0m  [28/42], [94mLoss[0m : 2.34645
[1mStep[0m  [32/42], [94mLoss[0m : 2.52400
[1mStep[0m  [36/42], [94mLoss[0m : 2.70349
[1mStep[0m  [40/42], [94mLoss[0m : 2.52060

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47053
[1mStep[0m  [4/42], [94mLoss[0m : 2.46317
[1mStep[0m  [8/42], [94mLoss[0m : 2.63527
[1mStep[0m  [12/42], [94mLoss[0m : 2.29184
[1mStep[0m  [16/42], [94mLoss[0m : 2.56725
[1mStep[0m  [20/42], [94mLoss[0m : 2.59867
[1mStep[0m  [24/42], [94mLoss[0m : 2.47908
[1mStep[0m  [28/42], [94mLoss[0m : 2.56652
[1mStep[0m  [32/42], [94mLoss[0m : 2.41022
[1mStep[0m  [36/42], [94mLoss[0m : 2.67260
[1mStep[0m  [40/42], [94mLoss[0m : 2.47775

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59364
[1mStep[0m  [4/42], [94mLoss[0m : 2.44329
[1mStep[0m  [8/42], [94mLoss[0m : 2.51564
[1mStep[0m  [12/42], [94mLoss[0m : 2.41574
[1mStep[0m  [16/42], [94mLoss[0m : 2.42892
[1mStep[0m  [20/42], [94mLoss[0m : 2.44007
[1mStep[0m  [24/42], [94mLoss[0m : 2.51357
[1mStep[0m  [28/42], [94mLoss[0m : 2.56164
[1mStep[0m  [32/42], [94mLoss[0m : 2.55793
[1mStep[0m  [36/42], [94mLoss[0m : 2.54141
[1mStep[0m  [40/42], [94mLoss[0m : 2.48951

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73768
[1mStep[0m  [4/42], [94mLoss[0m : 2.78731
[1mStep[0m  [8/42], [94mLoss[0m : 2.50828
[1mStep[0m  [12/42], [94mLoss[0m : 2.53487
[1mStep[0m  [16/42], [94mLoss[0m : 2.54818
[1mStep[0m  [20/42], [94mLoss[0m : 2.42952
[1mStep[0m  [24/42], [94mLoss[0m : 2.59612
[1mStep[0m  [28/42], [94mLoss[0m : 2.40181
[1mStep[0m  [32/42], [94mLoss[0m : 2.52159
[1mStep[0m  [36/42], [94mLoss[0m : 2.64224
[1mStep[0m  [40/42], [94mLoss[0m : 2.47242

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54459
[1mStep[0m  [4/42], [94mLoss[0m : 2.57181
[1mStep[0m  [8/42], [94mLoss[0m : 2.37969
[1mStep[0m  [12/42], [94mLoss[0m : 2.45631
[1mStep[0m  [16/42], [94mLoss[0m : 2.45916
[1mStep[0m  [20/42], [94mLoss[0m : 2.44662
[1mStep[0m  [24/42], [94mLoss[0m : 2.56182
[1mStep[0m  [28/42], [94mLoss[0m : 2.43022
[1mStep[0m  [32/42], [94mLoss[0m : 2.64900
[1mStep[0m  [36/42], [94mLoss[0m : 2.39565
[1mStep[0m  [40/42], [94mLoss[0m : 2.58247

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64818
[1mStep[0m  [4/42], [94mLoss[0m : 2.64004
[1mStep[0m  [8/42], [94mLoss[0m : 2.29646
[1mStep[0m  [12/42], [94mLoss[0m : 2.84527
[1mStep[0m  [16/42], [94mLoss[0m : 2.37749
[1mStep[0m  [20/42], [94mLoss[0m : 2.31033
[1mStep[0m  [24/42], [94mLoss[0m : 2.45806
[1mStep[0m  [28/42], [94mLoss[0m : 2.37476
[1mStep[0m  [32/42], [94mLoss[0m : 2.47383
[1mStep[0m  [36/42], [94mLoss[0m : 2.44726
[1mStep[0m  [40/42], [94mLoss[0m : 2.47847

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64618
[1mStep[0m  [4/42], [94mLoss[0m : 2.37593
[1mStep[0m  [8/42], [94mLoss[0m : 2.43100
[1mStep[0m  [12/42], [94mLoss[0m : 2.72315
[1mStep[0m  [16/42], [94mLoss[0m : 2.48492
[1mStep[0m  [20/42], [94mLoss[0m : 2.34793
[1mStep[0m  [24/42], [94mLoss[0m : 2.40691
[1mStep[0m  [28/42], [94mLoss[0m : 2.56030
[1mStep[0m  [32/42], [94mLoss[0m : 2.43921
[1mStep[0m  [36/42], [94mLoss[0m : 2.45242
[1mStep[0m  [40/42], [94mLoss[0m : 2.47732

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36351
[1mStep[0m  [4/42], [94mLoss[0m : 2.60910
[1mStep[0m  [8/42], [94mLoss[0m : 2.81225
[1mStep[0m  [12/42], [94mLoss[0m : 2.63835
[1mStep[0m  [16/42], [94mLoss[0m : 2.54416
[1mStep[0m  [20/42], [94mLoss[0m : 2.73692
[1mStep[0m  [24/42], [94mLoss[0m : 2.50346
[1mStep[0m  [28/42], [94mLoss[0m : 2.36965
[1mStep[0m  [32/42], [94mLoss[0m : 2.36731
[1mStep[0m  [36/42], [94mLoss[0m : 2.44647
[1mStep[0m  [40/42], [94mLoss[0m : 2.44140

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62129
[1mStep[0m  [4/42], [94mLoss[0m : 2.51516
[1mStep[0m  [8/42], [94mLoss[0m : 2.66446
[1mStep[0m  [12/42], [94mLoss[0m : 2.54412
[1mStep[0m  [16/42], [94mLoss[0m : 2.57465
[1mStep[0m  [20/42], [94mLoss[0m : 2.39452
[1mStep[0m  [24/42], [94mLoss[0m : 2.18973
[1mStep[0m  [28/42], [94mLoss[0m : 2.35533
[1mStep[0m  [32/42], [94mLoss[0m : 2.37099
[1mStep[0m  [36/42], [94mLoss[0m : 2.47477
[1mStep[0m  [40/42], [94mLoss[0m : 2.40819

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61207
[1mStep[0m  [4/42], [94mLoss[0m : 2.32600
[1mStep[0m  [8/42], [94mLoss[0m : 2.42749
[1mStep[0m  [12/42], [94mLoss[0m : 2.65821
[1mStep[0m  [16/42], [94mLoss[0m : 2.38081
[1mStep[0m  [20/42], [94mLoss[0m : 2.56948
[1mStep[0m  [24/42], [94mLoss[0m : 2.58415
[1mStep[0m  [28/42], [94mLoss[0m : 2.57233
[1mStep[0m  [32/42], [94mLoss[0m : 2.42441
[1mStep[0m  [36/42], [94mLoss[0m : 2.65326
[1mStep[0m  [40/42], [94mLoss[0m : 2.36100

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54240
[1mStep[0m  [4/42], [94mLoss[0m : 2.46985
[1mStep[0m  [8/42], [94mLoss[0m : 2.59762
[1mStep[0m  [12/42], [94mLoss[0m : 2.57709
[1mStep[0m  [16/42], [94mLoss[0m : 2.50228
[1mStep[0m  [20/42], [94mLoss[0m : 2.45930
[1mStep[0m  [24/42], [94mLoss[0m : 2.43781
[1mStep[0m  [28/42], [94mLoss[0m : 2.35531
[1mStep[0m  [32/42], [94mLoss[0m : 2.63235
[1mStep[0m  [36/42], [94mLoss[0m : 2.57008
[1mStep[0m  [40/42], [94mLoss[0m : 2.41005

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47904
[1mStep[0m  [4/42], [94mLoss[0m : 2.43257
[1mStep[0m  [8/42], [94mLoss[0m : 2.49245
[1mStep[0m  [12/42], [94mLoss[0m : 2.59627
[1mStep[0m  [16/42], [94mLoss[0m : 2.37654
[1mStep[0m  [20/42], [94mLoss[0m : 2.49410
[1mStep[0m  [24/42], [94mLoss[0m : 2.51790
[1mStep[0m  [28/42], [94mLoss[0m : 2.51105
[1mStep[0m  [32/42], [94mLoss[0m : 2.48593
[1mStep[0m  [36/42], [94mLoss[0m : 2.43487
[1mStep[0m  [40/42], [94mLoss[0m : 2.48762

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63542
[1mStep[0m  [4/42], [94mLoss[0m : 2.74069
[1mStep[0m  [8/42], [94mLoss[0m : 2.41869
[1mStep[0m  [12/42], [94mLoss[0m : 2.50546
[1mStep[0m  [16/42], [94mLoss[0m : 2.39436
[1mStep[0m  [20/42], [94mLoss[0m : 2.48289
[1mStep[0m  [24/42], [94mLoss[0m : 2.60466
[1mStep[0m  [28/42], [94mLoss[0m : 2.32692
[1mStep[0m  [32/42], [94mLoss[0m : 2.68549
[1mStep[0m  [36/42], [94mLoss[0m : 2.41123
[1mStep[0m  [40/42], [94mLoss[0m : 2.37376

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37643
[1mStep[0m  [4/42], [94mLoss[0m : 2.34666
[1mStep[0m  [8/42], [94mLoss[0m : 2.53848
[1mStep[0m  [12/42], [94mLoss[0m : 2.50879
[1mStep[0m  [16/42], [94mLoss[0m : 2.57487
[1mStep[0m  [20/42], [94mLoss[0m : 2.55358
[1mStep[0m  [24/42], [94mLoss[0m : 2.40266
[1mStep[0m  [28/42], [94mLoss[0m : 2.23639
[1mStep[0m  [32/42], [94mLoss[0m : 2.34907
[1mStep[0m  [36/42], [94mLoss[0m : 2.57599
[1mStep[0m  [40/42], [94mLoss[0m : 2.58572

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46079
[1mStep[0m  [4/42], [94mLoss[0m : 2.50716
[1mStep[0m  [8/42], [94mLoss[0m : 2.38549
[1mStep[0m  [12/42], [94mLoss[0m : 2.36336
[1mStep[0m  [16/42], [94mLoss[0m : 2.72970
[1mStep[0m  [20/42], [94mLoss[0m : 2.50627
[1mStep[0m  [24/42], [94mLoss[0m : 2.45088
[1mStep[0m  [28/42], [94mLoss[0m : 2.46967
[1mStep[0m  [32/42], [94mLoss[0m : 2.72383
[1mStep[0m  [36/42], [94mLoss[0m : 2.50946
[1mStep[0m  [40/42], [94mLoss[0m : 2.40034

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42861
[1mStep[0m  [4/42], [94mLoss[0m : 2.50933
[1mStep[0m  [8/42], [94mLoss[0m : 2.32765
[1mStep[0m  [12/42], [94mLoss[0m : 2.45626
[1mStep[0m  [16/42], [94mLoss[0m : 2.66736
[1mStep[0m  [20/42], [94mLoss[0m : 2.39319
[1mStep[0m  [24/42], [94mLoss[0m : 2.23374
[1mStep[0m  [28/42], [94mLoss[0m : 2.39660
[1mStep[0m  [32/42], [94mLoss[0m : 2.51763
[1mStep[0m  [36/42], [94mLoss[0m : 2.38683
[1mStep[0m  [40/42], [94mLoss[0m : 2.40112

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39134
[1mStep[0m  [4/42], [94mLoss[0m : 2.73661
[1mStep[0m  [8/42], [94mLoss[0m : 2.45448
[1mStep[0m  [12/42], [94mLoss[0m : 2.36939
[1mStep[0m  [16/42], [94mLoss[0m : 2.46807
[1mStep[0m  [20/42], [94mLoss[0m : 2.65489
[1mStep[0m  [24/42], [94mLoss[0m : 2.42161
[1mStep[0m  [28/42], [94mLoss[0m : 2.61262
[1mStep[0m  [32/42], [94mLoss[0m : 2.55904
[1mStep[0m  [36/42], [94mLoss[0m : 2.36188
[1mStep[0m  [40/42], [94mLoss[0m : 2.49516

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23168
[1mStep[0m  [4/42], [94mLoss[0m : 2.44640
[1mStep[0m  [8/42], [94mLoss[0m : 2.52302
[1mStep[0m  [12/42], [94mLoss[0m : 2.35866
[1mStep[0m  [16/42], [94mLoss[0m : 2.49537
[1mStep[0m  [20/42], [94mLoss[0m : 2.37735
[1mStep[0m  [24/42], [94mLoss[0m : 2.83033
[1mStep[0m  [28/42], [94mLoss[0m : 2.58624
[1mStep[0m  [32/42], [94mLoss[0m : 2.43811
[1mStep[0m  [36/42], [94mLoss[0m : 2.39794
[1mStep[0m  [40/42], [94mLoss[0m : 2.46986

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45036
[1mStep[0m  [4/42], [94mLoss[0m : 2.35378
[1mStep[0m  [8/42], [94mLoss[0m : 2.46919
[1mStep[0m  [12/42], [94mLoss[0m : 2.39309
[1mStep[0m  [16/42], [94mLoss[0m : 2.54030
[1mStep[0m  [20/42], [94mLoss[0m : 2.61480
[1mStep[0m  [24/42], [94mLoss[0m : 2.44740
[1mStep[0m  [28/42], [94mLoss[0m : 2.47344
[1mStep[0m  [32/42], [94mLoss[0m : 2.73497
[1mStep[0m  [36/42], [94mLoss[0m : 2.33153
[1mStep[0m  [40/42], [94mLoss[0m : 2.31546

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45439
[1mStep[0m  [4/42], [94mLoss[0m : 2.35219
[1mStep[0m  [8/42], [94mLoss[0m : 2.36263
[1mStep[0m  [12/42], [94mLoss[0m : 2.45291
[1mStep[0m  [16/42], [94mLoss[0m : 2.70711
[1mStep[0m  [20/42], [94mLoss[0m : 2.51404
[1mStep[0m  [24/42], [94mLoss[0m : 2.55967
[1mStep[0m  [28/42], [94mLoss[0m : 2.46523
[1mStep[0m  [32/42], [94mLoss[0m : 2.48709
[1mStep[0m  [36/42], [94mLoss[0m : 2.43921
[1mStep[0m  [40/42], [94mLoss[0m : 2.36805

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59840
[1mStep[0m  [4/42], [94mLoss[0m : 2.34259
[1mStep[0m  [8/42], [94mLoss[0m : 2.59980
[1mStep[0m  [12/42], [94mLoss[0m : 2.41649
[1mStep[0m  [16/42], [94mLoss[0m : 2.12585
[1mStep[0m  [20/42], [94mLoss[0m : 2.43300
[1mStep[0m  [24/42], [94mLoss[0m : 2.46113
[1mStep[0m  [28/42], [94mLoss[0m : 2.41695
[1mStep[0m  [32/42], [94mLoss[0m : 2.56441
[1mStep[0m  [36/42], [94mLoss[0m : 2.55867
[1mStep[0m  [40/42], [94mLoss[0m : 2.64565

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38864
[1mStep[0m  [4/42], [94mLoss[0m : 2.57444
[1mStep[0m  [8/42], [94mLoss[0m : 2.42463
[1mStep[0m  [12/42], [94mLoss[0m : 2.34159
[1mStep[0m  [16/42], [94mLoss[0m : 2.49200
[1mStep[0m  [20/42], [94mLoss[0m : 2.47772
[1mStep[0m  [24/42], [94mLoss[0m : 2.47590
[1mStep[0m  [28/42], [94mLoss[0m : 2.52708
[1mStep[0m  [32/42], [94mLoss[0m : 2.62467
[1mStep[0m  [36/42], [94mLoss[0m : 2.40733
[1mStep[0m  [40/42], [94mLoss[0m : 2.34065

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79609
[1mStep[0m  [4/42], [94mLoss[0m : 2.42910
[1mStep[0m  [8/42], [94mLoss[0m : 2.40071
[1mStep[0m  [12/42], [94mLoss[0m : 2.32900
[1mStep[0m  [16/42], [94mLoss[0m : 2.35633
[1mStep[0m  [20/42], [94mLoss[0m : 2.70992
[1mStep[0m  [24/42], [94mLoss[0m : 2.32319
[1mStep[0m  [28/42], [94mLoss[0m : 2.50762
[1mStep[0m  [32/42], [94mLoss[0m : 2.32067
[1mStep[0m  [36/42], [94mLoss[0m : 2.52620
[1mStep[0m  [40/42], [94mLoss[0m : 2.44866

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26051
[1mStep[0m  [4/42], [94mLoss[0m : 2.44965
[1mStep[0m  [8/42], [94mLoss[0m : 2.66528
[1mStep[0m  [12/42], [94mLoss[0m : 2.60394
[1mStep[0m  [16/42], [94mLoss[0m : 2.50966
[1mStep[0m  [20/42], [94mLoss[0m : 2.41173
[1mStep[0m  [24/42], [94mLoss[0m : 2.49446
[1mStep[0m  [28/42], [94mLoss[0m : 2.46626
[1mStep[0m  [32/42], [94mLoss[0m : 2.33071
[1mStep[0m  [36/42], [94mLoss[0m : 2.51618
[1mStep[0m  [40/42], [94mLoss[0m : 2.34087

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.338
====================================

Phase 1 - Evaluation MAE:  2.337765625544957
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.52297
[1mStep[0m  [4/42], [94mLoss[0m : 2.43107
[1mStep[0m  [8/42], [94mLoss[0m : 2.41892
[1mStep[0m  [12/42], [94mLoss[0m : 2.42681
[1mStep[0m  [16/42], [94mLoss[0m : 2.47912
[1mStep[0m  [20/42], [94mLoss[0m : 2.24603
[1mStep[0m  [24/42], [94mLoss[0m : 2.65495
[1mStep[0m  [28/42], [94mLoss[0m : 2.66335
[1mStep[0m  [32/42], [94mLoss[0m : 2.35174
[1mStep[0m  [36/42], [94mLoss[0m : 2.61794
[1mStep[0m  [40/42], [94mLoss[0m : 2.39354

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77989
[1mStep[0m  [4/42], [94mLoss[0m : 2.47848
[1mStep[0m  [8/42], [94mLoss[0m : 2.49752
[1mStep[0m  [12/42], [94mLoss[0m : 2.39853
[1mStep[0m  [16/42], [94mLoss[0m : 2.48101
[1mStep[0m  [20/42], [94mLoss[0m : 2.43550
[1mStep[0m  [24/42], [94mLoss[0m : 2.31899
[1mStep[0m  [28/42], [94mLoss[0m : 2.36807
[1mStep[0m  [32/42], [94mLoss[0m : 2.29088
[1mStep[0m  [36/42], [94mLoss[0m : 2.55798
[1mStep[0m  [40/42], [94mLoss[0m : 2.31396

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41608
[1mStep[0m  [4/42], [94mLoss[0m : 2.52628
[1mStep[0m  [8/42], [94mLoss[0m : 2.20428
[1mStep[0m  [12/42], [94mLoss[0m : 2.25662
[1mStep[0m  [16/42], [94mLoss[0m : 2.43681
[1mStep[0m  [20/42], [94mLoss[0m : 2.45192
[1mStep[0m  [24/42], [94mLoss[0m : 2.44000
[1mStep[0m  [28/42], [94mLoss[0m : 2.49850
[1mStep[0m  [32/42], [94mLoss[0m : 2.45584
[1mStep[0m  [36/42], [94mLoss[0m : 2.46971
[1mStep[0m  [40/42], [94mLoss[0m : 2.23684

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26077
[1mStep[0m  [4/42], [94mLoss[0m : 2.49568
[1mStep[0m  [8/42], [94mLoss[0m : 2.47394
[1mStep[0m  [12/42], [94mLoss[0m : 2.35374
[1mStep[0m  [16/42], [94mLoss[0m : 2.46514
[1mStep[0m  [20/42], [94mLoss[0m : 2.61372
[1mStep[0m  [24/42], [94mLoss[0m : 2.24541
[1mStep[0m  [28/42], [94mLoss[0m : 2.57778
[1mStep[0m  [32/42], [94mLoss[0m : 2.33520
[1mStep[0m  [36/42], [94mLoss[0m : 2.45103
[1mStep[0m  [40/42], [94mLoss[0m : 2.46686

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.530, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52209
[1mStep[0m  [4/42], [94mLoss[0m : 2.41749
[1mStep[0m  [8/42], [94mLoss[0m : 2.18534
[1mStep[0m  [12/42], [94mLoss[0m : 2.41333
[1mStep[0m  [16/42], [94mLoss[0m : 2.42622
[1mStep[0m  [20/42], [94mLoss[0m : 2.27979
[1mStep[0m  [24/42], [94mLoss[0m : 2.43419
[1mStep[0m  [28/42], [94mLoss[0m : 2.47826
[1mStep[0m  [32/42], [94mLoss[0m : 2.61190
[1mStep[0m  [36/42], [94mLoss[0m : 2.22289
[1mStep[0m  [40/42], [94mLoss[0m : 2.16730

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.577, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32091
[1mStep[0m  [4/42], [94mLoss[0m : 2.36552
[1mStep[0m  [8/42], [94mLoss[0m : 2.35735
[1mStep[0m  [12/42], [94mLoss[0m : 2.25509
[1mStep[0m  [16/42], [94mLoss[0m : 2.61868
[1mStep[0m  [20/42], [94mLoss[0m : 2.61185
[1mStep[0m  [24/42], [94mLoss[0m : 2.39160
[1mStep[0m  [28/42], [94mLoss[0m : 2.20071
[1mStep[0m  [32/42], [94mLoss[0m : 2.24744
[1mStep[0m  [36/42], [94mLoss[0m : 2.57996
[1mStep[0m  [40/42], [94mLoss[0m : 2.24588

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.595, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34922
[1mStep[0m  [4/42], [94mLoss[0m : 2.24387
[1mStep[0m  [8/42], [94mLoss[0m : 2.47252
[1mStep[0m  [12/42], [94mLoss[0m : 2.37094
[1mStep[0m  [16/42], [94mLoss[0m : 2.26215
[1mStep[0m  [20/42], [94mLoss[0m : 2.18053
[1mStep[0m  [24/42], [94mLoss[0m : 2.52374
[1mStep[0m  [28/42], [94mLoss[0m : 2.48376
[1mStep[0m  [32/42], [94mLoss[0m : 2.33485
[1mStep[0m  [36/42], [94mLoss[0m : 2.28739
[1mStep[0m  [40/42], [94mLoss[0m : 2.25546

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.584, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15717
[1mStep[0m  [4/42], [94mLoss[0m : 2.24111
[1mStep[0m  [8/42], [94mLoss[0m : 2.32995
[1mStep[0m  [12/42], [94mLoss[0m : 2.35923
[1mStep[0m  [16/42], [94mLoss[0m : 2.31310
[1mStep[0m  [20/42], [94mLoss[0m : 2.39024
[1mStep[0m  [24/42], [94mLoss[0m : 2.43313
[1mStep[0m  [28/42], [94mLoss[0m : 2.41580
[1mStep[0m  [32/42], [94mLoss[0m : 2.32665
[1mStep[0m  [36/42], [94mLoss[0m : 2.15761
[1mStep[0m  [40/42], [94mLoss[0m : 2.38683

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.583, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41414
[1mStep[0m  [4/42], [94mLoss[0m : 2.45644
[1mStep[0m  [8/42], [94mLoss[0m : 2.29570
[1mStep[0m  [12/42], [94mLoss[0m : 2.42507
[1mStep[0m  [16/42], [94mLoss[0m : 2.29246
[1mStep[0m  [20/42], [94mLoss[0m : 2.21245
[1mStep[0m  [24/42], [94mLoss[0m : 2.35679
[1mStep[0m  [28/42], [94mLoss[0m : 2.32529
[1mStep[0m  [32/42], [94mLoss[0m : 2.16778
[1mStep[0m  [36/42], [94mLoss[0m : 2.49296
[1mStep[0m  [40/42], [94mLoss[0m : 2.27805

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.605, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31716
[1mStep[0m  [4/42], [94mLoss[0m : 2.10757
[1mStep[0m  [8/42], [94mLoss[0m : 2.47805
[1mStep[0m  [12/42], [94mLoss[0m : 2.27786
[1mStep[0m  [16/42], [94mLoss[0m : 2.17025
[1mStep[0m  [20/42], [94mLoss[0m : 2.39818
[1mStep[0m  [24/42], [94mLoss[0m : 2.03752
[1mStep[0m  [28/42], [94mLoss[0m : 2.26220
[1mStep[0m  [32/42], [94mLoss[0m : 2.38042
[1mStep[0m  [36/42], [94mLoss[0m : 2.31179
[1mStep[0m  [40/42], [94mLoss[0m : 2.28332

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.607, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14913
[1mStep[0m  [4/42], [94mLoss[0m : 2.30030
[1mStep[0m  [8/42], [94mLoss[0m : 2.32587
[1mStep[0m  [12/42], [94mLoss[0m : 2.20767
[1mStep[0m  [16/42], [94mLoss[0m : 2.41685
[1mStep[0m  [20/42], [94mLoss[0m : 2.36786
[1mStep[0m  [24/42], [94mLoss[0m : 2.21584
[1mStep[0m  [28/42], [94mLoss[0m : 2.12644
[1mStep[0m  [32/42], [94mLoss[0m : 2.25948
[1mStep[0m  [36/42], [94mLoss[0m : 2.14054
[1mStep[0m  [40/42], [94mLoss[0m : 2.14426

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.639, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29276
[1mStep[0m  [4/42], [94mLoss[0m : 2.37525
[1mStep[0m  [8/42], [94mLoss[0m : 2.38350
[1mStep[0m  [12/42], [94mLoss[0m : 2.39249
[1mStep[0m  [16/42], [94mLoss[0m : 2.31485
[1mStep[0m  [20/42], [94mLoss[0m : 2.25491
[1mStep[0m  [24/42], [94mLoss[0m : 2.27390
[1mStep[0m  [28/42], [94mLoss[0m : 2.08328
[1mStep[0m  [32/42], [94mLoss[0m : 2.27581
[1mStep[0m  [36/42], [94mLoss[0m : 2.38886
[1mStep[0m  [40/42], [94mLoss[0m : 2.24504

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.594, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13497
[1mStep[0m  [4/42], [94mLoss[0m : 2.23897
[1mStep[0m  [8/42], [94mLoss[0m : 2.13701
[1mStep[0m  [12/42], [94mLoss[0m : 2.20159
[1mStep[0m  [16/42], [94mLoss[0m : 2.22930
[1mStep[0m  [20/42], [94mLoss[0m : 2.15990
[1mStep[0m  [24/42], [94mLoss[0m : 2.32617
[1mStep[0m  [28/42], [94mLoss[0m : 2.08819
[1mStep[0m  [32/42], [94mLoss[0m : 2.24954
[1mStep[0m  [36/42], [94mLoss[0m : 2.18139
[1mStep[0m  [40/42], [94mLoss[0m : 2.02862

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.524, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17020
[1mStep[0m  [4/42], [94mLoss[0m : 2.06561
[1mStep[0m  [8/42], [94mLoss[0m : 2.08664
[1mStep[0m  [12/42], [94mLoss[0m : 2.31641
[1mStep[0m  [16/42], [94mLoss[0m : 2.18470
[1mStep[0m  [20/42], [94mLoss[0m : 2.05350
[1mStep[0m  [24/42], [94mLoss[0m : 2.05718
[1mStep[0m  [28/42], [94mLoss[0m : 2.28742
[1mStep[0m  [32/42], [94mLoss[0m : 2.10700
[1mStep[0m  [36/42], [94mLoss[0m : 2.32420
[1mStep[0m  [40/42], [94mLoss[0m : 2.18814

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.592, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27833
[1mStep[0m  [4/42], [94mLoss[0m : 2.05440
[1mStep[0m  [8/42], [94mLoss[0m : 2.13202
[1mStep[0m  [12/42], [94mLoss[0m : 2.05755
[1mStep[0m  [16/42], [94mLoss[0m : 2.06587
[1mStep[0m  [20/42], [94mLoss[0m : 2.13263
[1mStep[0m  [24/42], [94mLoss[0m : 2.18358
[1mStep[0m  [28/42], [94mLoss[0m : 2.16129
[1mStep[0m  [32/42], [94mLoss[0m : 2.17199
[1mStep[0m  [36/42], [94mLoss[0m : 2.15318
[1mStep[0m  [40/42], [94mLoss[0m : 2.06996

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.581, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06455
[1mStep[0m  [4/42], [94mLoss[0m : 1.92883
[1mStep[0m  [8/42], [94mLoss[0m : 2.05256
[1mStep[0m  [12/42], [94mLoss[0m : 2.20671
[1mStep[0m  [16/42], [94mLoss[0m : 2.14805
[1mStep[0m  [20/42], [94mLoss[0m : 1.93231
[1mStep[0m  [24/42], [94mLoss[0m : 2.06795
[1mStep[0m  [28/42], [94mLoss[0m : 2.22670
[1mStep[0m  [32/42], [94mLoss[0m : 2.20926
[1mStep[0m  [36/42], [94mLoss[0m : 2.24291
[1mStep[0m  [40/42], [94mLoss[0m : 2.36019

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.525, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02888
[1mStep[0m  [4/42], [94mLoss[0m : 2.06078
[1mStep[0m  [8/42], [94mLoss[0m : 2.03423
[1mStep[0m  [12/42], [94mLoss[0m : 2.06440
[1mStep[0m  [16/42], [94mLoss[0m : 2.05838
[1mStep[0m  [20/42], [94mLoss[0m : 2.00917
[1mStep[0m  [24/42], [94mLoss[0m : 2.03130
[1mStep[0m  [28/42], [94mLoss[0m : 2.10028
[1mStep[0m  [32/42], [94mLoss[0m : 2.09174
[1mStep[0m  [36/42], [94mLoss[0m : 2.12494
[1mStep[0m  [40/42], [94mLoss[0m : 2.03692

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.622, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99456
[1mStep[0m  [4/42], [94mLoss[0m : 2.10761
[1mStep[0m  [8/42], [94mLoss[0m : 1.95824
[1mStep[0m  [12/42], [94mLoss[0m : 2.04091
[1mStep[0m  [16/42], [94mLoss[0m : 2.14005
[1mStep[0m  [20/42], [94mLoss[0m : 2.03275
[1mStep[0m  [24/42], [94mLoss[0m : 1.87422
[1mStep[0m  [28/42], [94mLoss[0m : 1.97617
[1mStep[0m  [32/42], [94mLoss[0m : 1.92074
[1mStep[0m  [36/42], [94mLoss[0m : 2.33986
[1mStep[0m  [40/42], [94mLoss[0m : 2.10684

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.528, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97929
[1mStep[0m  [4/42], [94mLoss[0m : 2.03594
[1mStep[0m  [8/42], [94mLoss[0m : 1.95834
[1mStep[0m  [12/42], [94mLoss[0m : 2.00842
[1mStep[0m  [16/42], [94mLoss[0m : 1.97603
[1mStep[0m  [20/42], [94mLoss[0m : 2.28862
[1mStep[0m  [24/42], [94mLoss[0m : 2.05096
[1mStep[0m  [28/42], [94mLoss[0m : 2.19911
[1mStep[0m  [32/42], [94mLoss[0m : 1.82317
[1mStep[0m  [36/42], [94mLoss[0m : 2.08363
[1mStep[0m  [40/42], [94mLoss[0m : 1.94060

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75861
[1mStep[0m  [4/42], [94mLoss[0m : 1.83890
[1mStep[0m  [8/42], [94mLoss[0m : 1.98625
[1mStep[0m  [12/42], [94mLoss[0m : 1.95667
[1mStep[0m  [16/42], [94mLoss[0m : 1.86971
[1mStep[0m  [20/42], [94mLoss[0m : 1.93786
[1mStep[0m  [24/42], [94mLoss[0m : 2.09585
[1mStep[0m  [28/42], [94mLoss[0m : 2.03888
[1mStep[0m  [32/42], [94mLoss[0m : 2.08597
[1mStep[0m  [36/42], [94mLoss[0m : 2.10643
[1mStep[0m  [40/42], [94mLoss[0m : 1.96741

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.847, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79552
[1mStep[0m  [4/42], [94mLoss[0m : 1.85702
[1mStep[0m  [8/42], [94mLoss[0m : 2.06246
[1mStep[0m  [12/42], [94mLoss[0m : 2.01929
[1mStep[0m  [16/42], [94mLoss[0m : 2.04089
[1mStep[0m  [20/42], [94mLoss[0m : 1.99784
[1mStep[0m  [24/42], [94mLoss[0m : 1.75382
[1mStep[0m  [28/42], [94mLoss[0m : 1.84276
[1mStep[0m  [32/42], [94mLoss[0m : 2.06414
[1mStep[0m  [36/42], [94mLoss[0m : 2.16457
[1mStep[0m  [40/42], [94mLoss[0m : 2.20788

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03207
[1mStep[0m  [4/42], [94mLoss[0m : 1.71396
[1mStep[0m  [8/42], [94mLoss[0m : 1.91646
[1mStep[0m  [12/42], [94mLoss[0m : 1.80838
[1mStep[0m  [16/42], [94mLoss[0m : 1.87780
[1mStep[0m  [20/42], [94mLoss[0m : 1.89595
[1mStep[0m  [24/42], [94mLoss[0m : 2.17487
[1mStep[0m  [28/42], [94mLoss[0m : 1.89579
[1mStep[0m  [32/42], [94mLoss[0m : 2.05733
[1mStep[0m  [36/42], [94mLoss[0m : 1.93616
[1mStep[0m  [40/42], [94mLoss[0m : 1.94086

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.634, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95425
[1mStep[0m  [4/42], [94mLoss[0m : 1.96267
[1mStep[0m  [8/42], [94mLoss[0m : 1.83403
[1mStep[0m  [12/42], [94mLoss[0m : 2.07217
[1mStep[0m  [16/42], [94mLoss[0m : 1.87583
[1mStep[0m  [20/42], [94mLoss[0m : 1.74410
[1mStep[0m  [24/42], [94mLoss[0m : 1.70638
[1mStep[0m  [28/42], [94mLoss[0m : 2.09419
[1mStep[0m  [32/42], [94mLoss[0m : 1.88062
[1mStep[0m  [36/42], [94mLoss[0m : 1.92641
[1mStep[0m  [40/42], [94mLoss[0m : 1.95625

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89113
[1mStep[0m  [4/42], [94mLoss[0m : 1.88213
[1mStep[0m  [8/42], [94mLoss[0m : 1.78746
[1mStep[0m  [12/42], [94mLoss[0m : 1.96908
[1mStep[0m  [16/42], [94mLoss[0m : 1.98830
[1mStep[0m  [20/42], [94mLoss[0m : 1.87824
[1mStep[0m  [24/42], [94mLoss[0m : 1.74812
[1mStep[0m  [28/42], [94mLoss[0m : 1.91640
[1mStep[0m  [32/42], [94mLoss[0m : 1.91056
[1mStep[0m  [36/42], [94mLoss[0m : 1.97439
[1mStep[0m  [40/42], [94mLoss[0m : 1.95067

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78681
[1mStep[0m  [4/42], [94mLoss[0m : 1.95631
[1mStep[0m  [8/42], [94mLoss[0m : 1.80786
[1mStep[0m  [12/42], [94mLoss[0m : 1.84198
[1mStep[0m  [16/42], [94mLoss[0m : 1.97001
[1mStep[0m  [20/42], [94mLoss[0m : 1.70548
[1mStep[0m  [24/42], [94mLoss[0m : 1.70354
[1mStep[0m  [28/42], [94mLoss[0m : 2.00627
[1mStep[0m  [32/42], [94mLoss[0m : 1.79718
[1mStep[0m  [36/42], [94mLoss[0m : 1.72975
[1mStep[0m  [40/42], [94mLoss[0m : 1.70160

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.456, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76017
[1mStep[0m  [4/42], [94mLoss[0m : 1.84895
[1mStep[0m  [8/42], [94mLoss[0m : 1.99264
[1mStep[0m  [12/42], [94mLoss[0m : 1.66531
[1mStep[0m  [16/42], [94mLoss[0m : 1.64057
[1mStep[0m  [20/42], [94mLoss[0m : 1.82357
[1mStep[0m  [24/42], [94mLoss[0m : 1.80440
[1mStep[0m  [28/42], [94mLoss[0m : 1.80578
[1mStep[0m  [32/42], [94mLoss[0m : 1.97648
[1mStep[0m  [36/42], [94mLoss[0m : 1.73161
[1mStep[0m  [40/42], [94mLoss[0m : 1.76046

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76000
[1mStep[0m  [4/42], [94mLoss[0m : 1.73041
[1mStep[0m  [8/42], [94mLoss[0m : 1.80251
[1mStep[0m  [12/42], [94mLoss[0m : 1.73272
[1mStep[0m  [16/42], [94mLoss[0m : 1.79772
[1mStep[0m  [20/42], [94mLoss[0m : 1.81520
[1mStep[0m  [24/42], [94mLoss[0m : 1.66046
[1mStep[0m  [28/42], [94mLoss[0m : 2.09846
[1mStep[0m  [32/42], [94mLoss[0m : 1.67614
[1mStep[0m  [36/42], [94mLoss[0m : 1.72320
[1mStep[0m  [40/42], [94mLoss[0m : 1.96939

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75277
[1mStep[0m  [4/42], [94mLoss[0m : 1.78572
[1mStep[0m  [8/42], [94mLoss[0m : 1.74133
[1mStep[0m  [12/42], [94mLoss[0m : 1.84581
[1mStep[0m  [16/42], [94mLoss[0m : 1.74395
[1mStep[0m  [20/42], [94mLoss[0m : 1.89491
[1mStep[0m  [24/42], [94mLoss[0m : 1.78776
[1mStep[0m  [28/42], [94mLoss[0m : 1.86651
[1mStep[0m  [32/42], [94mLoss[0m : 1.92198
[1mStep[0m  [36/42], [94mLoss[0m : 1.96859
[1mStep[0m  [40/42], [94mLoss[0m : 1.84779

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81912
[1mStep[0m  [4/42], [94mLoss[0m : 1.66754
[1mStep[0m  [8/42], [94mLoss[0m : 1.75060
[1mStep[0m  [12/42], [94mLoss[0m : 1.80603
[1mStep[0m  [16/42], [94mLoss[0m : 1.68078
[1mStep[0m  [20/42], [94mLoss[0m : 1.68993
[1mStep[0m  [24/42], [94mLoss[0m : 1.61098
[1mStep[0m  [28/42], [94mLoss[0m : 1.70555
[1mStep[0m  [32/42], [94mLoss[0m : 1.90093
[1mStep[0m  [36/42], [94mLoss[0m : 1.71490
[1mStep[0m  [40/42], [94mLoss[0m : 1.65971

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82956
[1mStep[0m  [4/42], [94mLoss[0m : 1.62852
[1mStep[0m  [8/42], [94mLoss[0m : 1.79684
[1mStep[0m  [12/42], [94mLoss[0m : 1.81966
[1mStep[0m  [16/42], [94mLoss[0m : 1.75229
[1mStep[0m  [20/42], [94mLoss[0m : 1.72876
[1mStep[0m  [24/42], [94mLoss[0m : 1.65376
[1mStep[0m  [28/42], [94mLoss[0m : 1.73100
[1mStep[0m  [32/42], [94mLoss[0m : 1.78363
[1mStep[0m  [36/42], [94mLoss[0m : 1.78642
[1mStep[0m  [40/42], [94mLoss[0m : 1.69573

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.544, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.625
====================================

Phase 2 - Evaluation MAE:  2.6250806025096347
MAE score P1       2.337766
MAE score P2       2.625081
loss               1.721767
learning_rate      0.007525
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.68297
[1mStep[0m  [2/21], [94mLoss[0m : 9.60198
[1mStep[0m  [4/21], [94mLoss[0m : 7.14240
[1mStep[0m  [6/21], [94mLoss[0m : 5.22203
[1mStep[0m  [8/21], [94mLoss[0m : 3.58287
[1mStep[0m  [10/21], [94mLoss[0m : 2.80091
[1mStep[0m  [12/21], [94mLoss[0m : 2.84041
[1mStep[0m  [14/21], [94mLoss[0m : 2.82340
[1mStep[0m  [16/21], [94mLoss[0m : 2.77947
[1mStep[0m  [18/21], [94mLoss[0m : 2.74656
[1mStep[0m  [20/21], [94mLoss[0m : 2.71713

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.730, [92mTest[0m: 10.825, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61280
[1mStep[0m  [2/21], [94mLoss[0m : 2.69487
[1mStep[0m  [4/21], [94mLoss[0m : 2.77605
[1mStep[0m  [6/21], [94mLoss[0m : 2.59050
[1mStep[0m  [8/21], [94mLoss[0m : 2.54384
[1mStep[0m  [10/21], [94mLoss[0m : 2.62357
[1mStep[0m  [12/21], [94mLoss[0m : 2.43143
[1mStep[0m  [14/21], [94mLoss[0m : 2.47042
[1mStep[0m  [16/21], [94mLoss[0m : 2.53630
[1mStep[0m  [18/21], [94mLoss[0m : 2.61506
[1mStep[0m  [20/21], [94mLoss[0m : 2.44765

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.697, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52958
[1mStep[0m  [2/21], [94mLoss[0m : 2.55504
[1mStep[0m  [4/21], [94mLoss[0m : 2.42449
[1mStep[0m  [6/21], [94mLoss[0m : 2.29619
[1mStep[0m  [8/21], [94mLoss[0m : 2.57112
[1mStep[0m  [10/21], [94mLoss[0m : 2.59690
[1mStep[0m  [12/21], [94mLoss[0m : 2.62937
[1mStep[0m  [14/21], [94mLoss[0m : 2.58023
[1mStep[0m  [16/21], [94mLoss[0m : 2.54796
[1mStep[0m  [18/21], [94mLoss[0m : 2.54676
[1mStep[0m  [20/21], [94mLoss[0m : 2.53253

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48705
[1mStep[0m  [2/21], [94mLoss[0m : 2.52355
[1mStep[0m  [4/21], [94mLoss[0m : 2.61157
[1mStep[0m  [6/21], [94mLoss[0m : 2.68860
[1mStep[0m  [8/21], [94mLoss[0m : 2.48763
[1mStep[0m  [10/21], [94mLoss[0m : 2.54155
[1mStep[0m  [12/21], [94mLoss[0m : 2.44384
[1mStep[0m  [14/21], [94mLoss[0m : 2.53642
[1mStep[0m  [16/21], [94mLoss[0m : 2.49241
[1mStep[0m  [18/21], [94mLoss[0m : 2.37852
[1mStep[0m  [20/21], [94mLoss[0m : 2.38128

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43548
[1mStep[0m  [2/21], [94mLoss[0m : 2.52336
[1mStep[0m  [4/21], [94mLoss[0m : 2.38456
[1mStep[0m  [6/21], [94mLoss[0m : 2.40532
[1mStep[0m  [8/21], [94mLoss[0m : 2.48262
[1mStep[0m  [10/21], [94mLoss[0m : 2.54728
[1mStep[0m  [12/21], [94mLoss[0m : 2.50087
[1mStep[0m  [14/21], [94mLoss[0m : 2.33986
[1mStep[0m  [16/21], [94mLoss[0m : 2.39605
[1mStep[0m  [18/21], [94mLoss[0m : 2.37453
[1mStep[0m  [20/21], [94mLoss[0m : 2.51288

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51188
[1mStep[0m  [2/21], [94mLoss[0m : 2.47204
[1mStep[0m  [4/21], [94mLoss[0m : 2.41414
[1mStep[0m  [6/21], [94mLoss[0m : 2.66735
[1mStep[0m  [8/21], [94mLoss[0m : 2.46791
[1mStep[0m  [10/21], [94mLoss[0m : 2.47655
[1mStep[0m  [12/21], [94mLoss[0m : 2.40775
[1mStep[0m  [14/21], [94mLoss[0m : 2.44733
[1mStep[0m  [16/21], [94mLoss[0m : 2.39285
[1mStep[0m  [18/21], [94mLoss[0m : 2.42527
[1mStep[0m  [20/21], [94mLoss[0m : 2.49306

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43119
[1mStep[0m  [2/21], [94mLoss[0m : 2.35947
[1mStep[0m  [4/21], [94mLoss[0m : 2.55086
[1mStep[0m  [6/21], [94mLoss[0m : 2.33281
[1mStep[0m  [8/21], [94mLoss[0m : 2.41649
[1mStep[0m  [10/21], [94mLoss[0m : 2.55366
[1mStep[0m  [12/21], [94mLoss[0m : 2.49965
[1mStep[0m  [14/21], [94mLoss[0m : 2.51678
[1mStep[0m  [16/21], [94mLoss[0m : 2.36901
[1mStep[0m  [18/21], [94mLoss[0m : 2.43359
[1mStep[0m  [20/21], [94mLoss[0m : 2.44494

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46086
[1mStep[0m  [2/21], [94mLoss[0m : 2.50794
[1mStep[0m  [4/21], [94mLoss[0m : 2.35323
[1mStep[0m  [6/21], [94mLoss[0m : 2.52988
[1mStep[0m  [8/21], [94mLoss[0m : 2.46855
[1mStep[0m  [10/21], [94mLoss[0m : 2.47567
[1mStep[0m  [12/21], [94mLoss[0m : 2.42621
[1mStep[0m  [14/21], [94mLoss[0m : 2.47440
[1mStep[0m  [16/21], [94mLoss[0m : 2.46097
[1mStep[0m  [18/21], [94mLoss[0m : 2.42087
[1mStep[0m  [20/21], [94mLoss[0m : 2.58295

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.398, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53478
[1mStep[0m  [2/21], [94mLoss[0m : 2.40315
[1mStep[0m  [4/21], [94mLoss[0m : 2.48287
[1mStep[0m  [6/21], [94mLoss[0m : 2.48168
[1mStep[0m  [8/21], [94mLoss[0m : 2.45847
[1mStep[0m  [10/21], [94mLoss[0m : 2.49247
[1mStep[0m  [12/21], [94mLoss[0m : 2.51606
[1mStep[0m  [14/21], [94mLoss[0m : 2.54282
[1mStep[0m  [16/21], [94mLoss[0m : 2.52401
[1mStep[0m  [18/21], [94mLoss[0m : 2.54853
[1mStep[0m  [20/21], [94mLoss[0m : 2.37042

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.384, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48369
[1mStep[0m  [2/21], [94mLoss[0m : 2.41391
[1mStep[0m  [4/21], [94mLoss[0m : 2.45946
[1mStep[0m  [6/21], [94mLoss[0m : 2.53255
[1mStep[0m  [8/21], [94mLoss[0m : 2.38433
[1mStep[0m  [10/21], [94mLoss[0m : 2.41266
[1mStep[0m  [12/21], [94mLoss[0m : 2.32984
[1mStep[0m  [14/21], [94mLoss[0m : 2.42796
[1mStep[0m  [16/21], [94mLoss[0m : 2.50683
[1mStep[0m  [18/21], [94mLoss[0m : 2.50134
[1mStep[0m  [20/21], [94mLoss[0m : 2.54886

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43472
[1mStep[0m  [2/21], [94mLoss[0m : 2.48457
[1mStep[0m  [4/21], [94mLoss[0m : 2.38498
[1mStep[0m  [6/21], [94mLoss[0m : 2.54578
[1mStep[0m  [8/21], [94mLoss[0m : 2.42897
[1mStep[0m  [10/21], [94mLoss[0m : 2.55930
[1mStep[0m  [12/21], [94mLoss[0m : 2.34878
[1mStep[0m  [14/21], [94mLoss[0m : 2.35624
[1mStep[0m  [16/21], [94mLoss[0m : 2.44301
[1mStep[0m  [18/21], [94mLoss[0m : 2.28940
[1mStep[0m  [20/21], [94mLoss[0m : 2.49399

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52788
[1mStep[0m  [2/21], [94mLoss[0m : 2.42282
[1mStep[0m  [4/21], [94mLoss[0m : 2.36166
[1mStep[0m  [6/21], [94mLoss[0m : 2.34178
[1mStep[0m  [8/21], [94mLoss[0m : 2.44484
[1mStep[0m  [10/21], [94mLoss[0m : 2.35632
[1mStep[0m  [12/21], [94mLoss[0m : 2.34987
[1mStep[0m  [14/21], [94mLoss[0m : 2.47107
[1mStep[0m  [16/21], [94mLoss[0m : 2.38578
[1mStep[0m  [18/21], [94mLoss[0m : 2.39117
[1mStep[0m  [20/21], [94mLoss[0m : 2.27842

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.29590
[1mStep[0m  [2/21], [94mLoss[0m : 2.56031
[1mStep[0m  [4/21], [94mLoss[0m : 2.46192
[1mStep[0m  [6/21], [94mLoss[0m : 2.45092
[1mStep[0m  [8/21], [94mLoss[0m : 2.44881
[1mStep[0m  [10/21], [94mLoss[0m : 2.30651
[1mStep[0m  [12/21], [94mLoss[0m : 2.31099
[1mStep[0m  [14/21], [94mLoss[0m : 2.45987
[1mStep[0m  [16/21], [94mLoss[0m : 2.37072
[1mStep[0m  [18/21], [94mLoss[0m : 2.44322
[1mStep[0m  [20/21], [94mLoss[0m : 2.47131

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40393
[1mStep[0m  [2/21], [94mLoss[0m : 2.57407
[1mStep[0m  [4/21], [94mLoss[0m : 2.41927
[1mStep[0m  [6/21], [94mLoss[0m : 2.50931
[1mStep[0m  [8/21], [94mLoss[0m : 2.46302
[1mStep[0m  [10/21], [94mLoss[0m : 2.41516
[1mStep[0m  [12/21], [94mLoss[0m : 2.52353
[1mStep[0m  [14/21], [94mLoss[0m : 2.33537
[1mStep[0m  [16/21], [94mLoss[0m : 2.42080
[1mStep[0m  [18/21], [94mLoss[0m : 2.45120
[1mStep[0m  [20/21], [94mLoss[0m : 2.26996

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44535
[1mStep[0m  [2/21], [94mLoss[0m : 2.35266
[1mStep[0m  [4/21], [94mLoss[0m : 2.41349
[1mStep[0m  [6/21], [94mLoss[0m : 2.51698
[1mStep[0m  [8/21], [94mLoss[0m : 2.37080
[1mStep[0m  [10/21], [94mLoss[0m : 2.57463
[1mStep[0m  [12/21], [94mLoss[0m : 2.46679
[1mStep[0m  [14/21], [94mLoss[0m : 2.53738
[1mStep[0m  [16/21], [94mLoss[0m : 2.45112
[1mStep[0m  [18/21], [94mLoss[0m : 2.40228
[1mStep[0m  [20/21], [94mLoss[0m : 2.27181

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42889
[1mStep[0m  [2/21], [94mLoss[0m : 2.37267
[1mStep[0m  [4/21], [94mLoss[0m : 2.37753
[1mStep[0m  [6/21], [94mLoss[0m : 2.47483
[1mStep[0m  [8/21], [94mLoss[0m : 2.44219
[1mStep[0m  [10/21], [94mLoss[0m : 2.58489
[1mStep[0m  [12/21], [94mLoss[0m : 2.45401
[1mStep[0m  [14/21], [94mLoss[0m : 2.25363
[1mStep[0m  [16/21], [94mLoss[0m : 2.48678
[1mStep[0m  [18/21], [94mLoss[0m : 2.54791
[1mStep[0m  [20/21], [94mLoss[0m : 2.42513

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36613
[1mStep[0m  [2/21], [94mLoss[0m : 2.54111
[1mStep[0m  [4/21], [94mLoss[0m : 2.52221
[1mStep[0m  [6/21], [94mLoss[0m : 2.21649
[1mStep[0m  [8/21], [94mLoss[0m : 2.55278
[1mStep[0m  [10/21], [94mLoss[0m : 2.39943
[1mStep[0m  [12/21], [94mLoss[0m : 2.34749
[1mStep[0m  [14/21], [94mLoss[0m : 2.56221
[1mStep[0m  [16/21], [94mLoss[0m : 2.29977
[1mStep[0m  [18/21], [94mLoss[0m : 2.44964
[1mStep[0m  [20/21], [94mLoss[0m : 2.40313

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66301
[1mStep[0m  [2/21], [94mLoss[0m : 2.46648
[1mStep[0m  [4/21], [94mLoss[0m : 2.43870
[1mStep[0m  [6/21], [94mLoss[0m : 2.30421
[1mStep[0m  [8/21], [94mLoss[0m : 2.19127
[1mStep[0m  [10/21], [94mLoss[0m : 2.28435
[1mStep[0m  [12/21], [94mLoss[0m : 2.47635
[1mStep[0m  [14/21], [94mLoss[0m : 2.39063
[1mStep[0m  [16/21], [94mLoss[0m : 2.37044
[1mStep[0m  [18/21], [94mLoss[0m : 2.46266
[1mStep[0m  [20/21], [94mLoss[0m : 2.41777

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40572
[1mStep[0m  [2/21], [94mLoss[0m : 2.52377
[1mStep[0m  [4/21], [94mLoss[0m : 2.41537
[1mStep[0m  [6/21], [94mLoss[0m : 2.31099
[1mStep[0m  [8/21], [94mLoss[0m : 2.41964
[1mStep[0m  [10/21], [94mLoss[0m : 2.50835
[1mStep[0m  [12/21], [94mLoss[0m : 2.48744
[1mStep[0m  [14/21], [94mLoss[0m : 2.26768
[1mStep[0m  [16/21], [94mLoss[0m : 2.49558
[1mStep[0m  [18/21], [94mLoss[0m : 2.44815
[1mStep[0m  [20/21], [94mLoss[0m : 2.45280

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46676
[1mStep[0m  [2/21], [94mLoss[0m : 2.35112
[1mStep[0m  [4/21], [94mLoss[0m : 2.52289
[1mStep[0m  [6/21], [94mLoss[0m : 2.35527
[1mStep[0m  [8/21], [94mLoss[0m : 2.55611
[1mStep[0m  [10/21], [94mLoss[0m : 2.39928
[1mStep[0m  [12/21], [94mLoss[0m : 2.42295
[1mStep[0m  [14/21], [94mLoss[0m : 2.53100
[1mStep[0m  [16/21], [94mLoss[0m : 2.40165
[1mStep[0m  [18/21], [94mLoss[0m : 2.34971
[1mStep[0m  [20/21], [94mLoss[0m : 2.31376

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.33181
[1mStep[0m  [2/21], [94mLoss[0m : 2.45149
[1mStep[0m  [4/21], [94mLoss[0m : 2.33879
[1mStep[0m  [6/21], [94mLoss[0m : 2.46908
[1mStep[0m  [8/21], [94mLoss[0m : 2.45939
[1mStep[0m  [10/21], [94mLoss[0m : 2.49817
[1mStep[0m  [12/21], [94mLoss[0m : 2.54270
[1mStep[0m  [14/21], [94mLoss[0m : 2.33100
[1mStep[0m  [16/21], [94mLoss[0m : 2.47865
[1mStep[0m  [18/21], [94mLoss[0m : 2.50395
[1mStep[0m  [20/21], [94mLoss[0m : 2.39530

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53251
[1mStep[0m  [2/21], [94mLoss[0m : 2.41022
[1mStep[0m  [4/21], [94mLoss[0m : 2.50131
[1mStep[0m  [6/21], [94mLoss[0m : 2.47766
[1mStep[0m  [8/21], [94mLoss[0m : 2.50138
[1mStep[0m  [10/21], [94mLoss[0m : 2.42714
[1mStep[0m  [12/21], [94mLoss[0m : 2.31410
[1mStep[0m  [14/21], [94mLoss[0m : 2.40821
[1mStep[0m  [16/21], [94mLoss[0m : 2.39528
[1mStep[0m  [18/21], [94mLoss[0m : 2.50868
[1mStep[0m  [20/21], [94mLoss[0m : 2.45929

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48144
[1mStep[0m  [2/21], [94mLoss[0m : 2.37680
[1mStep[0m  [4/21], [94mLoss[0m : 2.37558
[1mStep[0m  [6/21], [94mLoss[0m : 2.48651
[1mStep[0m  [8/21], [94mLoss[0m : 2.36430
[1mStep[0m  [10/21], [94mLoss[0m : 2.37518
[1mStep[0m  [12/21], [94mLoss[0m : 2.36517
[1mStep[0m  [14/21], [94mLoss[0m : 2.46066
[1mStep[0m  [16/21], [94mLoss[0m : 2.50925
[1mStep[0m  [18/21], [94mLoss[0m : 2.42683
[1mStep[0m  [20/21], [94mLoss[0m : 2.40542

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49028
[1mStep[0m  [2/21], [94mLoss[0m : 2.57140
[1mStep[0m  [4/21], [94mLoss[0m : 2.43412
[1mStep[0m  [6/21], [94mLoss[0m : 2.48059
[1mStep[0m  [8/21], [94mLoss[0m : 2.42336
[1mStep[0m  [10/21], [94mLoss[0m : 2.32991
[1mStep[0m  [12/21], [94mLoss[0m : 2.46771
[1mStep[0m  [14/21], [94mLoss[0m : 2.37266
[1mStep[0m  [16/21], [94mLoss[0m : 2.50891
[1mStep[0m  [18/21], [94mLoss[0m : 2.44627
[1mStep[0m  [20/21], [94mLoss[0m : 2.42337

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32197
[1mStep[0m  [2/21], [94mLoss[0m : 2.60567
[1mStep[0m  [4/21], [94mLoss[0m : 2.38863
[1mStep[0m  [6/21], [94mLoss[0m : 2.44590
[1mStep[0m  [8/21], [94mLoss[0m : 2.34124
[1mStep[0m  [10/21], [94mLoss[0m : 2.48722
[1mStep[0m  [12/21], [94mLoss[0m : 2.52443
[1mStep[0m  [14/21], [94mLoss[0m : 2.27399
[1mStep[0m  [16/21], [94mLoss[0m : 2.49759
[1mStep[0m  [18/21], [94mLoss[0m : 2.49980
[1mStep[0m  [20/21], [94mLoss[0m : 2.34756

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44485
[1mStep[0m  [2/21], [94mLoss[0m : 2.50227
[1mStep[0m  [4/21], [94mLoss[0m : 2.38409
[1mStep[0m  [6/21], [94mLoss[0m : 2.28972
[1mStep[0m  [8/21], [94mLoss[0m : 2.28330
[1mStep[0m  [10/21], [94mLoss[0m : 2.49241
[1mStep[0m  [12/21], [94mLoss[0m : 2.45422
[1mStep[0m  [14/21], [94mLoss[0m : 2.56414
[1mStep[0m  [16/21], [94mLoss[0m : 2.38361
[1mStep[0m  [18/21], [94mLoss[0m : 2.37957
[1mStep[0m  [20/21], [94mLoss[0m : 2.38538

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51095
[1mStep[0m  [2/21], [94mLoss[0m : 2.44897
[1mStep[0m  [4/21], [94mLoss[0m : 2.31549
[1mStep[0m  [6/21], [94mLoss[0m : 2.41704
[1mStep[0m  [8/21], [94mLoss[0m : 2.30419
[1mStep[0m  [10/21], [94mLoss[0m : 2.21229
[1mStep[0m  [12/21], [94mLoss[0m : 2.45181
[1mStep[0m  [14/21], [94mLoss[0m : 2.40140
[1mStep[0m  [16/21], [94mLoss[0m : 2.54744
[1mStep[0m  [18/21], [94mLoss[0m : 2.44470
[1mStep[0m  [20/21], [94mLoss[0m : 2.32754

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.343, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52430
[1mStep[0m  [2/21], [94mLoss[0m : 2.47248
[1mStep[0m  [4/21], [94mLoss[0m : 2.30955
[1mStep[0m  [6/21], [94mLoss[0m : 2.41362
[1mStep[0m  [8/21], [94mLoss[0m : 2.37753
[1mStep[0m  [10/21], [94mLoss[0m : 2.42819
[1mStep[0m  [12/21], [94mLoss[0m : 2.33985
[1mStep[0m  [14/21], [94mLoss[0m : 2.30396
[1mStep[0m  [16/21], [94mLoss[0m : 2.36675
[1mStep[0m  [18/21], [94mLoss[0m : 2.45381
[1mStep[0m  [20/21], [94mLoss[0m : 2.47398

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47104
[1mStep[0m  [2/21], [94mLoss[0m : 2.34948
[1mStep[0m  [4/21], [94mLoss[0m : 2.40061
[1mStep[0m  [6/21], [94mLoss[0m : 2.45399
[1mStep[0m  [8/21], [94mLoss[0m : 2.23803
[1mStep[0m  [10/21], [94mLoss[0m : 2.38364
[1mStep[0m  [12/21], [94mLoss[0m : 2.43048
[1mStep[0m  [14/21], [94mLoss[0m : 2.30115
[1mStep[0m  [16/21], [94mLoss[0m : 2.40830
[1mStep[0m  [18/21], [94mLoss[0m : 2.37901
[1mStep[0m  [20/21], [94mLoss[0m : 2.45745

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47006
[1mStep[0m  [2/21], [94mLoss[0m : 2.37306
[1mStep[0m  [4/21], [94mLoss[0m : 2.50318
[1mStep[0m  [6/21], [94mLoss[0m : 2.30649
[1mStep[0m  [8/21], [94mLoss[0m : 2.40626
[1mStep[0m  [10/21], [94mLoss[0m : 2.53739
[1mStep[0m  [12/21], [94mLoss[0m : 2.28031
[1mStep[0m  [14/21], [94mLoss[0m : 2.35531
[1mStep[0m  [16/21], [94mLoss[0m : 2.32468
[1mStep[0m  [18/21], [94mLoss[0m : 2.34184
[1mStep[0m  [20/21], [94mLoss[0m : 2.45239

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.336
====================================

Phase 1 - Evaluation MAE:  2.3359085491725375
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 2.42428
[1mStep[0m  [2/21], [94mLoss[0m : 2.43298
[1mStep[0m  [4/21], [94mLoss[0m : 2.40872
[1mStep[0m  [6/21], [94mLoss[0m : 2.48050
[1mStep[0m  [8/21], [94mLoss[0m : 2.29897
[1mStep[0m  [10/21], [94mLoss[0m : 2.48941
[1mStep[0m  [12/21], [94mLoss[0m : 2.51477
[1mStep[0m  [14/21], [94mLoss[0m : 2.43244
[1mStep[0m  [16/21], [94mLoss[0m : 2.46985
[1mStep[0m  [18/21], [94mLoss[0m : 2.41514
[1mStep[0m  [20/21], [94mLoss[0m : 2.30580

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22865
[1mStep[0m  [2/21], [94mLoss[0m : 2.27163
[1mStep[0m  [4/21], [94mLoss[0m : 2.32785
[1mStep[0m  [6/21], [94mLoss[0m : 2.50027
[1mStep[0m  [8/21], [94mLoss[0m : 2.45771
[1mStep[0m  [10/21], [94mLoss[0m : 2.35232
[1mStep[0m  [12/21], [94mLoss[0m : 2.52835
[1mStep[0m  [14/21], [94mLoss[0m : 2.34049
[1mStep[0m  [16/21], [94mLoss[0m : 2.31610
[1mStep[0m  [18/21], [94mLoss[0m : 2.32791
[1mStep[0m  [20/21], [94mLoss[0m : 2.32848

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.36023
[1mStep[0m  [2/21], [94mLoss[0m : 2.42942
[1mStep[0m  [4/21], [94mLoss[0m : 2.30679
[1mStep[0m  [6/21], [94mLoss[0m : 2.50222
[1mStep[0m  [8/21], [94mLoss[0m : 2.34622
[1mStep[0m  [10/21], [94mLoss[0m : 2.39202
[1mStep[0m  [12/21], [94mLoss[0m : 2.32183
[1mStep[0m  [14/21], [94mLoss[0m : 2.23583
[1mStep[0m  [16/21], [94mLoss[0m : 2.49057
[1mStep[0m  [18/21], [94mLoss[0m : 2.36000
[1mStep[0m  [20/21], [94mLoss[0m : 2.39418

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37696
[1mStep[0m  [2/21], [94mLoss[0m : 2.23618
[1mStep[0m  [4/21], [94mLoss[0m : 2.32399
[1mStep[0m  [6/21], [94mLoss[0m : 2.38424
[1mStep[0m  [8/21], [94mLoss[0m : 2.35492
[1mStep[0m  [10/21], [94mLoss[0m : 2.55253
[1mStep[0m  [12/21], [94mLoss[0m : 2.33650
[1mStep[0m  [14/21], [94mLoss[0m : 2.26928
[1mStep[0m  [16/21], [94mLoss[0m : 2.44277
[1mStep[0m  [18/21], [94mLoss[0m : 2.28811
[1mStep[0m  [20/21], [94mLoss[0m : 2.38715

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.30332
[1mStep[0m  [2/21], [94mLoss[0m : 2.40199
[1mStep[0m  [4/21], [94mLoss[0m : 2.28234
[1mStep[0m  [6/21], [94mLoss[0m : 2.35025
[1mStep[0m  [8/21], [94mLoss[0m : 2.34741
[1mStep[0m  [10/21], [94mLoss[0m : 2.29444
[1mStep[0m  [12/21], [94mLoss[0m : 2.37145
[1mStep[0m  [14/21], [94mLoss[0m : 2.44841
[1mStep[0m  [16/21], [94mLoss[0m : 2.36586
[1mStep[0m  [18/21], [94mLoss[0m : 2.44199
[1mStep[0m  [20/21], [94mLoss[0m : 2.42821

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.28349
[1mStep[0m  [2/21], [94mLoss[0m : 2.41830
[1mStep[0m  [4/21], [94mLoss[0m : 2.36767
[1mStep[0m  [6/21], [94mLoss[0m : 2.40140
[1mStep[0m  [8/21], [94mLoss[0m : 2.37334
[1mStep[0m  [10/21], [94mLoss[0m : 2.45961
[1mStep[0m  [12/21], [94mLoss[0m : 2.30322
[1mStep[0m  [14/21], [94mLoss[0m : 2.23500
[1mStep[0m  [16/21], [94mLoss[0m : 2.24733
[1mStep[0m  [18/21], [94mLoss[0m : 2.42149
[1mStep[0m  [20/21], [94mLoss[0m : 2.23644

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.22274
[1mStep[0m  [2/21], [94mLoss[0m : 2.11344
[1mStep[0m  [4/21], [94mLoss[0m : 2.30380
[1mStep[0m  [6/21], [94mLoss[0m : 2.20954
[1mStep[0m  [8/21], [94mLoss[0m : 2.40945
[1mStep[0m  [10/21], [94mLoss[0m : 2.36026
[1mStep[0m  [12/21], [94mLoss[0m : 2.41120
[1mStep[0m  [14/21], [94mLoss[0m : 2.38557
[1mStep[0m  [16/21], [94mLoss[0m : 2.29468
[1mStep[0m  [18/21], [94mLoss[0m : 2.27835
[1mStep[0m  [20/21], [94mLoss[0m : 2.21006

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24833
[1mStep[0m  [2/21], [94mLoss[0m : 2.24710
[1mStep[0m  [4/21], [94mLoss[0m : 2.21765
[1mStep[0m  [6/21], [94mLoss[0m : 2.22990
[1mStep[0m  [8/21], [94mLoss[0m : 2.24360
[1mStep[0m  [10/21], [94mLoss[0m : 2.28756
[1mStep[0m  [12/21], [94mLoss[0m : 2.30755
[1mStep[0m  [14/21], [94mLoss[0m : 2.14784
[1mStep[0m  [16/21], [94mLoss[0m : 2.21691
[1mStep[0m  [18/21], [94mLoss[0m : 2.40041
[1mStep[0m  [20/21], [94mLoss[0m : 2.30306

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39965
[1mStep[0m  [2/21], [94mLoss[0m : 2.23938
[1mStep[0m  [4/21], [94mLoss[0m : 2.19815
[1mStep[0m  [6/21], [94mLoss[0m : 2.25455
[1mStep[0m  [8/21], [94mLoss[0m : 2.33114
[1mStep[0m  [10/21], [94mLoss[0m : 2.21459
[1mStep[0m  [12/21], [94mLoss[0m : 2.27039
[1mStep[0m  [14/21], [94mLoss[0m : 2.22998
[1mStep[0m  [16/21], [94mLoss[0m : 2.17237
[1mStep[0m  [18/21], [94mLoss[0m : 2.34601
[1mStep[0m  [20/21], [94mLoss[0m : 2.31798

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.630, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24268
[1mStep[0m  [2/21], [94mLoss[0m : 2.19661
[1mStep[0m  [4/21], [94mLoss[0m : 2.17720
[1mStep[0m  [6/21], [94mLoss[0m : 2.14596
[1mStep[0m  [8/21], [94mLoss[0m : 2.32430
[1mStep[0m  [10/21], [94mLoss[0m : 2.25467
[1mStep[0m  [12/21], [94mLoss[0m : 2.30560
[1mStep[0m  [14/21], [94mLoss[0m : 2.10150
[1mStep[0m  [16/21], [94mLoss[0m : 2.14933
[1mStep[0m  [18/21], [94mLoss[0m : 2.18326
[1mStep[0m  [20/21], [94mLoss[0m : 2.24710

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.25908
[1mStep[0m  [2/21], [94mLoss[0m : 2.19120
[1mStep[0m  [4/21], [94mLoss[0m : 2.16400
[1mStep[0m  [6/21], [94mLoss[0m : 2.30895
[1mStep[0m  [8/21], [94mLoss[0m : 2.22411
[1mStep[0m  [10/21], [94mLoss[0m : 2.21401
[1mStep[0m  [12/21], [94mLoss[0m : 2.23624
[1mStep[0m  [14/21], [94mLoss[0m : 2.18075
[1mStep[0m  [16/21], [94mLoss[0m : 2.22999
[1mStep[0m  [18/21], [94mLoss[0m : 2.24859
[1mStep[0m  [20/21], [94mLoss[0m : 2.15583

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.555, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.11908
[1mStep[0m  [2/21], [94mLoss[0m : 2.10348
[1mStep[0m  [4/21], [94mLoss[0m : 2.04892
[1mStep[0m  [6/21], [94mLoss[0m : 2.33724
[1mStep[0m  [8/21], [94mLoss[0m : 2.20050
[1mStep[0m  [10/21], [94mLoss[0m : 2.17169
[1mStep[0m  [12/21], [94mLoss[0m : 2.06701
[1mStep[0m  [14/21], [94mLoss[0m : 2.32660
[1mStep[0m  [16/21], [94mLoss[0m : 2.20071
[1mStep[0m  [18/21], [94mLoss[0m : 2.07364
[1mStep[0m  [20/21], [94mLoss[0m : 2.14100

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10932
[1mStep[0m  [2/21], [94mLoss[0m : 2.21636
[1mStep[0m  [4/21], [94mLoss[0m : 2.14164
[1mStep[0m  [6/21], [94mLoss[0m : 2.10848
[1mStep[0m  [8/21], [94mLoss[0m : 2.17138
[1mStep[0m  [10/21], [94mLoss[0m : 2.13472
[1mStep[0m  [12/21], [94mLoss[0m : 2.19570
[1mStep[0m  [14/21], [94mLoss[0m : 1.95787
[1mStep[0m  [16/21], [94mLoss[0m : 2.12600
[1mStep[0m  [18/21], [94mLoss[0m : 2.20728
[1mStep[0m  [20/21], [94mLoss[0m : 2.31957

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.97327
[1mStep[0m  [2/21], [94mLoss[0m : 2.17639
[1mStep[0m  [4/21], [94mLoss[0m : 2.06683
[1mStep[0m  [6/21], [94mLoss[0m : 2.21654
[1mStep[0m  [8/21], [94mLoss[0m : 2.19251
[1mStep[0m  [10/21], [94mLoss[0m : 2.28356
[1mStep[0m  [12/21], [94mLoss[0m : 1.99338
[1mStep[0m  [14/21], [94mLoss[0m : 2.25023
[1mStep[0m  [16/21], [94mLoss[0m : 2.08064
[1mStep[0m  [18/21], [94mLoss[0m : 2.17816
[1mStep[0m  [20/21], [94mLoss[0m : 2.27216

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.05698
[1mStep[0m  [2/21], [94mLoss[0m : 2.12102
[1mStep[0m  [4/21], [94mLoss[0m : 2.07882
[1mStep[0m  [6/21], [94mLoss[0m : 2.18058
[1mStep[0m  [8/21], [94mLoss[0m : 2.01195
[1mStep[0m  [10/21], [94mLoss[0m : 2.18711
[1mStep[0m  [12/21], [94mLoss[0m : 2.05437
[1mStep[0m  [14/21], [94mLoss[0m : 2.11179
[1mStep[0m  [16/21], [94mLoss[0m : 2.26451
[1mStep[0m  [18/21], [94mLoss[0m : 1.92310
[1mStep[0m  [20/21], [94mLoss[0m : 1.97209

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.620, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.03811
[1mStep[0m  [2/21], [94mLoss[0m : 2.00857
[1mStep[0m  [4/21], [94mLoss[0m : 1.98968
[1mStep[0m  [6/21], [94mLoss[0m : 2.21625
[1mStep[0m  [8/21], [94mLoss[0m : 2.10239
[1mStep[0m  [10/21], [94mLoss[0m : 2.08458
[1mStep[0m  [12/21], [94mLoss[0m : 2.02505
[1mStep[0m  [14/21], [94mLoss[0m : 1.95409
[1mStep[0m  [16/21], [94mLoss[0m : 2.15528
[1mStep[0m  [18/21], [94mLoss[0m : 2.02321
[1mStep[0m  [20/21], [94mLoss[0m : 2.11056

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.630, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.02501
[1mStep[0m  [2/21], [94mLoss[0m : 2.09832
[1mStep[0m  [4/21], [94mLoss[0m : 2.03042
[1mStep[0m  [6/21], [94mLoss[0m : 1.96369
[1mStep[0m  [8/21], [94mLoss[0m : 2.03573
[1mStep[0m  [10/21], [94mLoss[0m : 2.02294
[1mStep[0m  [12/21], [94mLoss[0m : 2.18232
[1mStep[0m  [14/21], [94mLoss[0m : 1.99644
[1mStep[0m  [16/21], [94mLoss[0m : 1.91891
[1mStep[0m  [18/21], [94mLoss[0m : 1.95497
[1mStep[0m  [20/21], [94mLoss[0m : 1.92756

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.633, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.08673
[1mStep[0m  [2/21], [94mLoss[0m : 1.83787
[1mStep[0m  [4/21], [94mLoss[0m : 1.87490
[1mStep[0m  [6/21], [94mLoss[0m : 2.05341
[1mStep[0m  [8/21], [94mLoss[0m : 2.10480
[1mStep[0m  [10/21], [94mLoss[0m : 1.88291
[1mStep[0m  [12/21], [94mLoss[0m : 1.97947
[1mStep[0m  [14/21], [94mLoss[0m : 2.02110
[1mStep[0m  [16/21], [94mLoss[0m : 2.03972
[1mStep[0m  [18/21], [94mLoss[0m : 2.11752
[1mStep[0m  [20/21], [94mLoss[0m : 1.93142

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.99898
[1mStep[0m  [2/21], [94mLoss[0m : 2.12175
[1mStep[0m  [4/21], [94mLoss[0m : 2.00278
[1mStep[0m  [6/21], [94mLoss[0m : 1.75692
[1mStep[0m  [8/21], [94mLoss[0m : 2.06423
[1mStep[0m  [10/21], [94mLoss[0m : 1.95933
[1mStep[0m  [12/21], [94mLoss[0m : 1.83517
[1mStep[0m  [14/21], [94mLoss[0m : 1.85041
[1mStep[0m  [16/21], [94mLoss[0m : 1.98145
[1mStep[0m  [18/21], [94mLoss[0m : 1.94393
[1mStep[0m  [20/21], [94mLoss[0m : 1.90623

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.585, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.87233
[1mStep[0m  [2/21], [94mLoss[0m : 1.83880
[1mStep[0m  [4/21], [94mLoss[0m : 1.77869
[1mStep[0m  [6/21], [94mLoss[0m : 1.92943
[1mStep[0m  [8/21], [94mLoss[0m : 1.77796
[1mStep[0m  [10/21], [94mLoss[0m : 1.91225
[1mStep[0m  [12/21], [94mLoss[0m : 1.99039
[1mStep[0m  [14/21], [94mLoss[0m : 1.93769
[1mStep[0m  [16/21], [94mLoss[0m : 2.07233
[1mStep[0m  [18/21], [94mLoss[0m : 1.98088
[1mStep[0m  [20/21], [94mLoss[0m : 1.99491

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.558, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91949
[1mStep[0m  [2/21], [94mLoss[0m : 1.87205
[1mStep[0m  [4/21], [94mLoss[0m : 1.87687
[1mStep[0m  [6/21], [94mLoss[0m : 1.98062
[1mStep[0m  [8/21], [94mLoss[0m : 1.86097
[1mStep[0m  [10/21], [94mLoss[0m : 1.84269
[1mStep[0m  [12/21], [94mLoss[0m : 1.82680
[1mStep[0m  [14/21], [94mLoss[0m : 1.94087
[1mStep[0m  [16/21], [94mLoss[0m : 1.86724
[1mStep[0m  [18/21], [94mLoss[0m : 1.99252
[1mStep[0m  [20/21], [94mLoss[0m : 1.65027

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.887, [92mTest[0m: 2.567, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90437
[1mStep[0m  [2/21], [94mLoss[0m : 1.84393
[1mStep[0m  [4/21], [94mLoss[0m : 1.73480
[1mStep[0m  [6/21], [94mLoss[0m : 1.83023
[1mStep[0m  [8/21], [94mLoss[0m : 1.97006
[1mStep[0m  [10/21], [94mLoss[0m : 1.94755
[1mStep[0m  [12/21], [94mLoss[0m : 1.77305
[1mStep[0m  [14/21], [94mLoss[0m : 1.78298
[1mStep[0m  [16/21], [94mLoss[0m : 1.86394
[1mStep[0m  [18/21], [94mLoss[0m : 1.97250
[1mStep[0m  [20/21], [94mLoss[0m : 1.83090

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.456, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.88929
[1mStep[0m  [2/21], [94mLoss[0m : 1.75138
[1mStep[0m  [4/21], [94mLoss[0m : 1.70439
[1mStep[0m  [6/21], [94mLoss[0m : 1.63511
[1mStep[0m  [8/21], [94mLoss[0m : 1.87897
[1mStep[0m  [10/21], [94mLoss[0m : 1.78387
[1mStep[0m  [12/21], [94mLoss[0m : 1.87694
[1mStep[0m  [14/21], [94mLoss[0m : 1.88669
[1mStep[0m  [16/21], [94mLoss[0m : 1.69932
[1mStep[0m  [18/21], [94mLoss[0m : 1.96443
[1mStep[0m  [20/21], [94mLoss[0m : 1.80948

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.616, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.85016
[1mStep[0m  [2/21], [94mLoss[0m : 1.67334
[1mStep[0m  [4/21], [94mLoss[0m : 1.78386
[1mStep[0m  [6/21], [94mLoss[0m : 1.76899
[1mStep[0m  [8/21], [94mLoss[0m : 1.88755
[1mStep[0m  [10/21], [94mLoss[0m : 1.76768
[1mStep[0m  [12/21], [94mLoss[0m : 1.66809
[1mStep[0m  [14/21], [94mLoss[0m : 1.72762
[1mStep[0m  [16/21], [94mLoss[0m : 1.85484
[1mStep[0m  [18/21], [94mLoss[0m : 1.79175
[1mStep[0m  [20/21], [94mLoss[0m : 1.78263

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.540, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.81731
[1mStep[0m  [2/21], [94mLoss[0m : 1.74932
[1mStep[0m  [4/21], [94mLoss[0m : 1.66809
[1mStep[0m  [6/21], [94mLoss[0m : 1.69646
[1mStep[0m  [8/21], [94mLoss[0m : 1.80059
[1mStep[0m  [10/21], [94mLoss[0m : 1.90936
[1mStep[0m  [12/21], [94mLoss[0m : 1.69338
[1mStep[0m  [14/21], [94mLoss[0m : 1.75515
[1mStep[0m  [16/21], [94mLoss[0m : 1.80210
[1mStep[0m  [18/21], [94mLoss[0m : 1.79956
[1mStep[0m  [20/21], [94mLoss[0m : 1.73599

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.769, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.76080
[1mStep[0m  [2/21], [94mLoss[0m : 1.76131
[1mStep[0m  [4/21], [94mLoss[0m : 1.77115
[1mStep[0m  [6/21], [94mLoss[0m : 1.66206
[1mStep[0m  [8/21], [94mLoss[0m : 1.71507
[1mStep[0m  [10/21], [94mLoss[0m : 1.81760
[1mStep[0m  [12/21], [94mLoss[0m : 1.71755
[1mStep[0m  [14/21], [94mLoss[0m : 1.76922
[1mStep[0m  [16/21], [94mLoss[0m : 1.87409
[1mStep[0m  [18/21], [94mLoss[0m : 1.84053
[1mStep[0m  [20/21], [94mLoss[0m : 1.75787

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.746, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.73040
[1mStep[0m  [2/21], [94mLoss[0m : 1.55837
[1mStep[0m  [4/21], [94mLoss[0m : 1.58395
[1mStep[0m  [6/21], [94mLoss[0m : 1.64968
[1mStep[0m  [8/21], [94mLoss[0m : 1.71578
[1mStep[0m  [10/21], [94mLoss[0m : 1.71814
[1mStep[0m  [12/21], [94mLoss[0m : 1.69852
[1mStep[0m  [14/21], [94mLoss[0m : 1.67761
[1mStep[0m  [16/21], [94mLoss[0m : 1.63161
[1mStep[0m  [18/21], [94mLoss[0m : 1.78619
[1mStep[0m  [20/21], [94mLoss[0m : 1.82125

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63971
[1mStep[0m  [2/21], [94mLoss[0m : 1.69123
[1mStep[0m  [4/21], [94mLoss[0m : 1.74076
[1mStep[0m  [6/21], [94mLoss[0m : 1.54224
[1mStep[0m  [8/21], [94mLoss[0m : 1.66776
[1mStep[0m  [10/21], [94mLoss[0m : 1.66302
[1mStep[0m  [12/21], [94mLoss[0m : 1.67834
[1mStep[0m  [14/21], [94mLoss[0m : 1.59343
[1mStep[0m  [16/21], [94mLoss[0m : 1.76890
[1mStep[0m  [18/21], [94mLoss[0m : 1.77276
[1mStep[0m  [20/21], [94mLoss[0m : 1.60236

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.681, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.63157
[1mStep[0m  [2/21], [94mLoss[0m : 1.64342
[1mStep[0m  [4/21], [94mLoss[0m : 1.59420
[1mStep[0m  [6/21], [94mLoss[0m : 1.69771
[1mStep[0m  [8/21], [94mLoss[0m : 1.57639
[1mStep[0m  [10/21], [94mLoss[0m : 1.64310
[1mStep[0m  [12/21], [94mLoss[0m : 1.59566
[1mStep[0m  [14/21], [94mLoss[0m : 1.70612
[1mStep[0m  [16/21], [94mLoss[0m : 1.65964
[1mStep[0m  [18/21], [94mLoss[0m : 1.65487
[1mStep[0m  [20/21], [94mLoss[0m : 1.67707

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.534, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.70816
[1mStep[0m  [2/21], [94mLoss[0m : 1.56426
[1mStep[0m  [4/21], [94mLoss[0m : 1.71775
[1mStep[0m  [6/21], [94mLoss[0m : 1.63339
[1mStep[0m  [8/21], [94mLoss[0m : 1.65959
[1mStep[0m  [10/21], [94mLoss[0m : 1.71939
[1mStep[0m  [12/21], [94mLoss[0m : 1.61734
[1mStep[0m  [14/21], [94mLoss[0m : 1.56427
[1mStep[0m  [16/21], [94mLoss[0m : 1.62969
[1mStep[0m  [18/21], [94mLoss[0m : 1.57348
[1mStep[0m  [20/21], [94mLoss[0m : 1.62423

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.589, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.525
====================================

Phase 2 - Evaluation MAE:  2.5252699170793806
MAE score P1      2.335909
MAE score P2       2.52527
loss              1.637582
learning_rate     0.007525
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.25322
[1mStep[0m  [4/42], [94mLoss[0m : 10.74887
[1mStep[0m  [8/42], [94mLoss[0m : 10.16949
[1mStep[0m  [12/42], [94mLoss[0m : 9.98102
[1mStep[0m  [16/42], [94mLoss[0m : 9.69999
[1mStep[0m  [20/42], [94mLoss[0m : 9.83515
[1mStep[0m  [24/42], [94mLoss[0m : 9.06014
[1mStep[0m  [28/42], [94mLoss[0m : 8.75700
[1mStep[0m  [32/42], [94mLoss[0m : 8.33052
[1mStep[0m  [36/42], [94mLoss[0m : 8.43549
[1mStep[0m  [40/42], [94mLoss[0m : 8.10431

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.440, [92mTest[0m: 10.999, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.98301
[1mStep[0m  [4/42], [94mLoss[0m : 7.33790
[1mStep[0m  [8/42], [94mLoss[0m : 6.86961
[1mStep[0m  [12/42], [94mLoss[0m : 7.18663
[1mStep[0m  [16/42], [94mLoss[0m : 6.39733
[1mStep[0m  [20/42], [94mLoss[0m : 6.64520
[1mStep[0m  [24/42], [94mLoss[0m : 5.98979
[1mStep[0m  [28/42], [94mLoss[0m : 6.01269
[1mStep[0m  [32/42], [94mLoss[0m : 5.38868
[1mStep[0m  [36/42], [94mLoss[0m : 5.10543
[1mStep[0m  [40/42], [94mLoss[0m : 4.81439

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.290, [92mTest[0m: 9.316, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.84592
[1mStep[0m  [4/42], [94mLoss[0m : 4.84315
[1mStep[0m  [8/42], [94mLoss[0m : 4.00927
[1mStep[0m  [12/42], [94mLoss[0m : 3.90071
[1mStep[0m  [16/42], [94mLoss[0m : 4.14597
[1mStep[0m  [20/42], [94mLoss[0m : 3.85721
[1mStep[0m  [24/42], [94mLoss[0m : 3.78847
[1mStep[0m  [28/42], [94mLoss[0m : 3.39848
[1mStep[0m  [32/42], [94mLoss[0m : 3.56709
[1mStep[0m  [36/42], [94mLoss[0m : 3.29217
[1mStep[0m  [40/42], [94mLoss[0m : 3.33578

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.926, [92mTest[0m: 6.906, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.53221
[1mStep[0m  [4/42], [94mLoss[0m : 3.28274
[1mStep[0m  [8/42], [94mLoss[0m : 3.11758
[1mStep[0m  [12/42], [94mLoss[0m : 3.16786
[1mStep[0m  [16/42], [94mLoss[0m : 3.01099
[1mStep[0m  [20/42], [94mLoss[0m : 3.20606
[1mStep[0m  [24/42], [94mLoss[0m : 3.34610
[1mStep[0m  [28/42], [94mLoss[0m : 3.09339
[1mStep[0m  [32/42], [94mLoss[0m : 2.88576
[1mStep[0m  [36/42], [94mLoss[0m : 3.10894
[1mStep[0m  [40/42], [94mLoss[0m : 3.03696

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.155, [92mTest[0m: 4.966, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.17803
[1mStep[0m  [4/42], [94mLoss[0m : 2.72201
[1mStep[0m  [8/42], [94mLoss[0m : 3.10898
[1mStep[0m  [12/42], [94mLoss[0m : 3.11290
[1mStep[0m  [16/42], [94mLoss[0m : 2.93754
[1mStep[0m  [20/42], [94mLoss[0m : 3.22224
[1mStep[0m  [24/42], [94mLoss[0m : 2.94932
[1mStep[0m  [28/42], [94mLoss[0m : 2.95726
[1mStep[0m  [32/42], [94mLoss[0m : 2.87746
[1mStep[0m  [36/42], [94mLoss[0m : 3.00809
[1mStep[0m  [40/42], [94mLoss[0m : 3.24648

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.025, [92mTest[0m: 3.806, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81221
[1mStep[0m  [4/42], [94mLoss[0m : 3.14937
[1mStep[0m  [8/42], [94mLoss[0m : 2.96573
[1mStep[0m  [12/42], [94mLoss[0m : 2.88982
[1mStep[0m  [16/42], [94mLoss[0m : 3.04718
[1mStep[0m  [20/42], [94mLoss[0m : 3.09133
[1mStep[0m  [24/42], [94mLoss[0m : 3.17451
[1mStep[0m  [28/42], [94mLoss[0m : 2.96242
[1mStep[0m  [32/42], [94mLoss[0m : 2.89284
[1mStep[0m  [36/42], [94mLoss[0m : 2.67929
[1mStep[0m  [40/42], [94mLoss[0m : 3.03650

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.983, [92mTest[0m: 3.311, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74346
[1mStep[0m  [4/42], [94mLoss[0m : 3.23749
[1mStep[0m  [8/42], [94mLoss[0m : 3.07981
[1mStep[0m  [12/42], [94mLoss[0m : 2.66930
[1mStep[0m  [16/42], [94mLoss[0m : 2.88860
[1mStep[0m  [20/42], [94mLoss[0m : 2.81851
[1mStep[0m  [24/42], [94mLoss[0m : 2.96544
[1mStep[0m  [28/42], [94mLoss[0m : 2.93585
[1mStep[0m  [32/42], [94mLoss[0m : 3.20754
[1mStep[0m  [36/42], [94mLoss[0m : 3.08077
[1mStep[0m  [40/42], [94mLoss[0m : 2.95998

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.950, [92mTest[0m: 3.042, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82203
[1mStep[0m  [4/42], [94mLoss[0m : 2.93825
[1mStep[0m  [8/42], [94mLoss[0m : 2.95438
[1mStep[0m  [12/42], [94mLoss[0m : 2.80981
[1mStep[0m  [16/42], [94mLoss[0m : 2.93394
[1mStep[0m  [20/42], [94mLoss[0m : 2.90981
[1mStep[0m  [24/42], [94mLoss[0m : 3.05976
[1mStep[0m  [28/42], [94mLoss[0m : 3.23128
[1mStep[0m  [32/42], [94mLoss[0m : 3.01757
[1mStep[0m  [36/42], [94mLoss[0m : 2.77755
[1mStep[0m  [40/42], [94mLoss[0m : 2.93257

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.935, [92mTest[0m: 2.874, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80474
[1mStep[0m  [4/42], [94mLoss[0m : 2.85757
[1mStep[0m  [8/42], [94mLoss[0m : 2.78731
[1mStep[0m  [12/42], [94mLoss[0m : 2.76118
[1mStep[0m  [16/42], [94mLoss[0m : 3.08999
[1mStep[0m  [20/42], [94mLoss[0m : 2.94301
[1mStep[0m  [24/42], [94mLoss[0m : 2.96550
[1mStep[0m  [28/42], [94mLoss[0m : 2.76989
[1mStep[0m  [32/42], [94mLoss[0m : 2.89800
[1mStep[0m  [36/42], [94mLoss[0m : 2.64379
[1mStep[0m  [40/42], [94mLoss[0m : 3.17610

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.894, [92mTest[0m: 2.862, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85170
[1mStep[0m  [4/42], [94mLoss[0m : 2.70818
[1mStep[0m  [8/42], [94mLoss[0m : 2.72104
[1mStep[0m  [12/42], [94mLoss[0m : 3.01856
[1mStep[0m  [16/42], [94mLoss[0m : 3.03510
[1mStep[0m  [20/42], [94mLoss[0m : 2.93517
[1mStep[0m  [24/42], [94mLoss[0m : 2.90572
[1mStep[0m  [28/42], [94mLoss[0m : 3.10687
[1mStep[0m  [32/42], [94mLoss[0m : 2.84133
[1mStep[0m  [36/42], [94mLoss[0m : 2.89869
[1mStep[0m  [40/42], [94mLoss[0m : 2.81284

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.902, [92mTest[0m: 2.761, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.12492
[1mStep[0m  [4/42], [94mLoss[0m : 2.94211
[1mStep[0m  [8/42], [94mLoss[0m : 2.82848
[1mStep[0m  [12/42], [94mLoss[0m : 2.68059
[1mStep[0m  [16/42], [94mLoss[0m : 3.07225
[1mStep[0m  [20/42], [94mLoss[0m : 3.04373
[1mStep[0m  [24/42], [94mLoss[0m : 3.08358
[1mStep[0m  [28/42], [94mLoss[0m : 3.25326
[1mStep[0m  [32/42], [94mLoss[0m : 2.82137
[1mStep[0m  [36/42], [94mLoss[0m : 3.17769
[1mStep[0m  [40/42], [94mLoss[0m : 3.01838

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.903, [92mTest[0m: 2.739, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.06162
[1mStep[0m  [4/42], [94mLoss[0m : 2.90119
[1mStep[0m  [8/42], [94mLoss[0m : 2.75226
[1mStep[0m  [12/42], [94mLoss[0m : 2.88663
[1mStep[0m  [16/42], [94mLoss[0m : 3.10399
[1mStep[0m  [20/42], [94mLoss[0m : 2.94684
[1mStep[0m  [24/42], [94mLoss[0m : 3.08714
[1mStep[0m  [28/42], [94mLoss[0m : 2.89230
[1mStep[0m  [32/42], [94mLoss[0m : 3.28034
[1mStep[0m  [36/42], [94mLoss[0m : 2.84834
[1mStep[0m  [40/42], [94mLoss[0m : 2.75221

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.841, [92mTest[0m: 2.662, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76913
[1mStep[0m  [4/42], [94mLoss[0m : 3.02928
[1mStep[0m  [8/42], [94mLoss[0m : 2.77619
[1mStep[0m  [12/42], [94mLoss[0m : 3.02540
[1mStep[0m  [16/42], [94mLoss[0m : 3.04253
[1mStep[0m  [20/42], [94mLoss[0m : 2.90437
[1mStep[0m  [24/42], [94mLoss[0m : 2.74640
[1mStep[0m  [28/42], [94mLoss[0m : 2.76608
[1mStep[0m  [32/42], [94mLoss[0m : 2.51956
[1mStep[0m  [36/42], [94mLoss[0m : 3.00849
[1mStep[0m  [40/42], [94mLoss[0m : 2.84916

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.857, [92mTest[0m: 2.611, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75485
[1mStep[0m  [4/42], [94mLoss[0m : 2.82059
[1mStep[0m  [8/42], [94mLoss[0m : 2.88261
[1mStep[0m  [12/42], [94mLoss[0m : 3.04439
[1mStep[0m  [16/42], [94mLoss[0m : 2.90841
[1mStep[0m  [20/42], [94mLoss[0m : 2.74384
[1mStep[0m  [24/42], [94mLoss[0m : 2.67917
[1mStep[0m  [28/42], [94mLoss[0m : 2.87722
[1mStep[0m  [32/42], [94mLoss[0m : 2.90660
[1mStep[0m  [36/42], [94mLoss[0m : 2.56078
[1mStep[0m  [40/42], [94mLoss[0m : 2.99921

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.818, [92mTest[0m: 2.599, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70765
[1mStep[0m  [4/42], [94mLoss[0m : 2.75195
[1mStep[0m  [8/42], [94mLoss[0m : 2.81045
[1mStep[0m  [12/42], [94mLoss[0m : 3.05368
[1mStep[0m  [16/42], [94mLoss[0m : 2.54982
[1mStep[0m  [20/42], [94mLoss[0m : 2.81224
[1mStep[0m  [24/42], [94mLoss[0m : 2.78456
[1mStep[0m  [28/42], [94mLoss[0m : 2.83347
[1mStep[0m  [32/42], [94mLoss[0m : 2.60522
[1mStep[0m  [36/42], [94mLoss[0m : 2.66396
[1mStep[0m  [40/42], [94mLoss[0m : 2.74957

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.804, [92mTest[0m: 2.574, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80539
[1mStep[0m  [4/42], [94mLoss[0m : 2.84789
[1mStep[0m  [8/42], [94mLoss[0m : 2.66947
[1mStep[0m  [12/42], [94mLoss[0m : 2.79584
[1mStep[0m  [16/42], [94mLoss[0m : 2.64518
[1mStep[0m  [20/42], [94mLoss[0m : 2.79160
[1mStep[0m  [24/42], [94mLoss[0m : 2.61100
[1mStep[0m  [28/42], [94mLoss[0m : 2.98464
[1mStep[0m  [32/42], [94mLoss[0m : 2.81824
[1mStep[0m  [36/42], [94mLoss[0m : 2.77322
[1mStep[0m  [40/42], [94mLoss[0m : 2.95345

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.808, [92mTest[0m: 2.522, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80297
[1mStep[0m  [4/42], [94mLoss[0m : 2.77326
[1mStep[0m  [8/42], [94mLoss[0m : 3.03773
[1mStep[0m  [12/42], [94mLoss[0m : 2.63212
[1mStep[0m  [16/42], [94mLoss[0m : 2.75263
[1mStep[0m  [20/42], [94mLoss[0m : 2.73237
[1mStep[0m  [24/42], [94mLoss[0m : 3.07992
[1mStep[0m  [28/42], [94mLoss[0m : 2.86162
[1mStep[0m  [32/42], [94mLoss[0m : 2.94435
[1mStep[0m  [36/42], [94mLoss[0m : 2.80275
[1mStep[0m  [40/42], [94mLoss[0m : 2.72507

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.790, [92mTest[0m: 2.544, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77190
[1mStep[0m  [4/42], [94mLoss[0m : 2.81747
[1mStep[0m  [8/42], [94mLoss[0m : 2.83654
[1mStep[0m  [12/42], [94mLoss[0m : 2.86110
[1mStep[0m  [16/42], [94mLoss[0m : 2.84702
[1mStep[0m  [20/42], [94mLoss[0m : 2.56807
[1mStep[0m  [24/42], [94mLoss[0m : 2.75511
[1mStep[0m  [28/42], [94mLoss[0m : 2.60204
[1mStep[0m  [32/42], [94mLoss[0m : 2.79694
[1mStep[0m  [36/42], [94mLoss[0m : 2.61508
[1mStep[0m  [40/42], [94mLoss[0m : 2.65890

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.774, [92mTest[0m: 2.495, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.83079
[1mStep[0m  [4/42], [94mLoss[0m : 2.64985
[1mStep[0m  [8/42], [94mLoss[0m : 2.58124
[1mStep[0m  [12/42], [94mLoss[0m : 2.63452
[1mStep[0m  [16/42], [94mLoss[0m : 2.98858
[1mStep[0m  [20/42], [94mLoss[0m : 2.70741
[1mStep[0m  [24/42], [94mLoss[0m : 2.87269
[1mStep[0m  [28/42], [94mLoss[0m : 2.85539
[1mStep[0m  [32/42], [94mLoss[0m : 2.92502
[1mStep[0m  [36/42], [94mLoss[0m : 2.84249
[1mStep[0m  [40/42], [94mLoss[0m : 2.96487

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.803, [92mTest[0m: 2.532, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.08603
[1mStep[0m  [4/42], [94mLoss[0m : 2.86652
[1mStep[0m  [8/42], [94mLoss[0m : 2.83658
[1mStep[0m  [12/42], [94mLoss[0m : 2.73305
[1mStep[0m  [16/42], [94mLoss[0m : 2.67088
[1mStep[0m  [20/42], [94mLoss[0m : 2.62442
[1mStep[0m  [24/42], [94mLoss[0m : 2.76002
[1mStep[0m  [28/42], [94mLoss[0m : 2.86899
[1mStep[0m  [32/42], [94mLoss[0m : 2.77347
[1mStep[0m  [36/42], [94mLoss[0m : 2.94910
[1mStep[0m  [40/42], [94mLoss[0m : 2.82594

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.784, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77293
[1mStep[0m  [4/42], [94mLoss[0m : 2.77923
[1mStep[0m  [8/42], [94mLoss[0m : 2.91197
[1mStep[0m  [12/42], [94mLoss[0m : 2.63693
[1mStep[0m  [16/42], [94mLoss[0m : 2.74863
[1mStep[0m  [20/42], [94mLoss[0m : 2.62090
[1mStep[0m  [24/42], [94mLoss[0m : 2.65276
[1mStep[0m  [28/42], [94mLoss[0m : 2.87366
[1mStep[0m  [32/42], [94mLoss[0m : 2.78810
[1mStep[0m  [36/42], [94mLoss[0m : 2.78912
[1mStep[0m  [40/42], [94mLoss[0m : 2.55435

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.752, [92mTest[0m: 2.445, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77964
[1mStep[0m  [4/42], [94mLoss[0m : 2.60183
[1mStep[0m  [8/42], [94mLoss[0m : 2.57510
[1mStep[0m  [12/42], [94mLoss[0m : 2.53770
[1mStep[0m  [16/42], [94mLoss[0m : 2.78653
[1mStep[0m  [20/42], [94mLoss[0m : 2.60488
[1mStep[0m  [24/42], [94mLoss[0m : 2.64320
[1mStep[0m  [28/42], [94mLoss[0m : 2.63148
[1mStep[0m  [32/42], [94mLoss[0m : 2.81904
[1mStep[0m  [36/42], [94mLoss[0m : 3.17219
[1mStep[0m  [40/42], [94mLoss[0m : 2.89664

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.764, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66912
[1mStep[0m  [4/42], [94mLoss[0m : 2.89714
[1mStep[0m  [8/42], [94mLoss[0m : 2.72163
[1mStep[0m  [12/42], [94mLoss[0m : 2.62652
[1mStep[0m  [16/42], [94mLoss[0m : 2.66769
[1mStep[0m  [20/42], [94mLoss[0m : 2.59151
[1mStep[0m  [24/42], [94mLoss[0m : 2.77263
[1mStep[0m  [28/42], [94mLoss[0m : 2.77116
[1mStep[0m  [32/42], [94mLoss[0m : 3.02822
[1mStep[0m  [36/42], [94mLoss[0m : 2.66303
[1mStep[0m  [40/42], [94mLoss[0m : 2.84815

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.98546
[1mStep[0m  [4/42], [94mLoss[0m : 3.00355
[1mStep[0m  [8/42], [94mLoss[0m : 2.62670
[1mStep[0m  [12/42], [94mLoss[0m : 2.53713
[1mStep[0m  [16/42], [94mLoss[0m : 2.94529
[1mStep[0m  [20/42], [94mLoss[0m : 2.59390
[1mStep[0m  [24/42], [94mLoss[0m : 2.83334
[1mStep[0m  [28/42], [94mLoss[0m : 2.56494
[1mStep[0m  [32/42], [94mLoss[0m : 2.76002
[1mStep[0m  [36/42], [94mLoss[0m : 2.43791
[1mStep[0m  [40/42], [94mLoss[0m : 2.77299

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60204
[1mStep[0m  [4/42], [94mLoss[0m : 2.99460
[1mStep[0m  [8/42], [94mLoss[0m : 2.91777
[1mStep[0m  [12/42], [94mLoss[0m : 2.83050
[1mStep[0m  [16/42], [94mLoss[0m : 2.95467
[1mStep[0m  [20/42], [94mLoss[0m : 2.53949
[1mStep[0m  [24/42], [94mLoss[0m : 2.95560
[1mStep[0m  [28/42], [94mLoss[0m : 2.74019
[1mStep[0m  [32/42], [94mLoss[0m : 2.70924
[1mStep[0m  [36/42], [94mLoss[0m : 2.77584
[1mStep[0m  [40/42], [94mLoss[0m : 2.51948

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.454, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76146
[1mStep[0m  [4/42], [94mLoss[0m : 2.54956
[1mStep[0m  [8/42], [94mLoss[0m : 2.60873
[1mStep[0m  [12/42], [94mLoss[0m : 2.73431
[1mStep[0m  [16/42], [94mLoss[0m : 2.86452
[1mStep[0m  [20/42], [94mLoss[0m : 2.68130
[1mStep[0m  [24/42], [94mLoss[0m : 2.59794
[1mStep[0m  [28/42], [94mLoss[0m : 2.74740
[1mStep[0m  [32/42], [94mLoss[0m : 2.52045
[1mStep[0m  [36/42], [94mLoss[0m : 2.76210
[1mStep[0m  [40/42], [94mLoss[0m : 3.11845

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.731, [92mTest[0m: 2.408, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44454
[1mStep[0m  [4/42], [94mLoss[0m : 2.65896
[1mStep[0m  [8/42], [94mLoss[0m : 2.70386
[1mStep[0m  [12/42], [94mLoss[0m : 2.51440
[1mStep[0m  [16/42], [94mLoss[0m : 2.86950
[1mStep[0m  [20/42], [94mLoss[0m : 2.73582
[1mStep[0m  [24/42], [94mLoss[0m : 2.68156
[1mStep[0m  [28/42], [94mLoss[0m : 2.45660
[1mStep[0m  [32/42], [94mLoss[0m : 2.60791
[1mStep[0m  [36/42], [94mLoss[0m : 2.80998
[1mStep[0m  [40/42], [94mLoss[0m : 2.73956

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81679
[1mStep[0m  [4/42], [94mLoss[0m : 2.85717
[1mStep[0m  [8/42], [94mLoss[0m : 2.66923
[1mStep[0m  [12/42], [94mLoss[0m : 2.75210
[1mStep[0m  [16/42], [94mLoss[0m : 2.58990
[1mStep[0m  [20/42], [94mLoss[0m : 2.56279
[1mStep[0m  [24/42], [94mLoss[0m : 2.72222
[1mStep[0m  [28/42], [94mLoss[0m : 2.61765
[1mStep[0m  [32/42], [94mLoss[0m : 2.67234
[1mStep[0m  [36/42], [94mLoss[0m : 2.88919
[1mStep[0m  [40/42], [94mLoss[0m : 2.27163

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.436, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50716
[1mStep[0m  [4/42], [94mLoss[0m : 2.51330
[1mStep[0m  [8/42], [94mLoss[0m : 2.63656
[1mStep[0m  [12/42], [94mLoss[0m : 2.53406
[1mStep[0m  [16/42], [94mLoss[0m : 2.70618
[1mStep[0m  [20/42], [94mLoss[0m : 2.53894
[1mStep[0m  [24/42], [94mLoss[0m : 2.85568
[1mStep[0m  [28/42], [94mLoss[0m : 2.67204
[1mStep[0m  [32/42], [94mLoss[0m : 3.00526
[1mStep[0m  [36/42], [94mLoss[0m : 2.68385
[1mStep[0m  [40/42], [94mLoss[0m : 2.78797

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.720, [92mTest[0m: 2.423, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71706
[1mStep[0m  [4/42], [94mLoss[0m : 3.04968
[1mStep[0m  [8/42], [94mLoss[0m : 2.77574
[1mStep[0m  [12/42], [94mLoss[0m : 2.76309
[1mStep[0m  [16/42], [94mLoss[0m : 2.71485
[1mStep[0m  [20/42], [94mLoss[0m : 2.75119
[1mStep[0m  [24/42], [94mLoss[0m : 2.60077
[1mStep[0m  [28/42], [94mLoss[0m : 2.59218
[1mStep[0m  [32/42], [94mLoss[0m : 2.55921
[1mStep[0m  [36/42], [94mLoss[0m : 2.60266
[1mStep[0m  [40/42], [94mLoss[0m : 2.79314

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.399, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.382
====================================

Phase 1 - Evaluation MAE:  2.381893345287868
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.51581
[1mStep[0m  [4/42], [94mLoss[0m : 2.62883
[1mStep[0m  [8/42], [94mLoss[0m : 2.50066
[1mStep[0m  [12/42], [94mLoss[0m : 2.78978
[1mStep[0m  [16/42], [94mLoss[0m : 2.86148
[1mStep[0m  [20/42], [94mLoss[0m : 2.71954
[1mStep[0m  [24/42], [94mLoss[0m : 2.72436
[1mStep[0m  [28/42], [94mLoss[0m : 2.95962
[1mStep[0m  [32/42], [94mLoss[0m : 2.74176
[1mStep[0m  [36/42], [94mLoss[0m : 2.87824
[1mStep[0m  [40/42], [94mLoss[0m : 2.79783

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.795, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74900
[1mStep[0m  [4/42], [94mLoss[0m : 2.76682
[1mStep[0m  [8/42], [94mLoss[0m : 2.74533
[1mStep[0m  [12/42], [94mLoss[0m : 2.46887
[1mStep[0m  [16/42], [94mLoss[0m : 2.46062
[1mStep[0m  [20/42], [94mLoss[0m : 2.78381
[1mStep[0m  [24/42], [94mLoss[0m : 2.61794
[1mStep[0m  [28/42], [94mLoss[0m : 2.62158
[1mStep[0m  [32/42], [94mLoss[0m : 2.64636
[1mStep[0m  [36/42], [94mLoss[0m : 2.78940
[1mStep[0m  [40/42], [94mLoss[0m : 2.92016

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57870
[1mStep[0m  [4/42], [94mLoss[0m : 2.83577
[1mStep[0m  [8/42], [94mLoss[0m : 2.78252
[1mStep[0m  [12/42], [94mLoss[0m : 3.03908
[1mStep[0m  [16/42], [94mLoss[0m : 2.93555
[1mStep[0m  [20/42], [94mLoss[0m : 2.77957
[1mStep[0m  [24/42], [94mLoss[0m : 2.69275
[1mStep[0m  [28/42], [94mLoss[0m : 2.72452
[1mStep[0m  [32/42], [94mLoss[0m : 2.47577
[1mStep[0m  [36/42], [94mLoss[0m : 2.69124
[1mStep[0m  [40/42], [94mLoss[0m : 2.62936

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73508
[1mStep[0m  [4/42], [94mLoss[0m : 3.06300
[1mStep[0m  [8/42], [94mLoss[0m : 2.62589
[1mStep[0m  [12/42], [94mLoss[0m : 2.75901
[1mStep[0m  [16/42], [94mLoss[0m : 2.63018
[1mStep[0m  [20/42], [94mLoss[0m : 2.71477
[1mStep[0m  [24/42], [94mLoss[0m : 2.72039
[1mStep[0m  [28/42], [94mLoss[0m : 2.66839
[1mStep[0m  [32/42], [94mLoss[0m : 2.45445
[1mStep[0m  [36/42], [94mLoss[0m : 2.78345
[1mStep[0m  [40/42], [94mLoss[0m : 2.82921

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55881
[1mStep[0m  [4/42], [94mLoss[0m : 2.85620
[1mStep[0m  [8/42], [94mLoss[0m : 3.00777
[1mStep[0m  [12/42], [94mLoss[0m : 2.61357
[1mStep[0m  [16/42], [94mLoss[0m : 2.59204
[1mStep[0m  [20/42], [94mLoss[0m : 2.65620
[1mStep[0m  [24/42], [94mLoss[0m : 2.74908
[1mStep[0m  [28/42], [94mLoss[0m : 2.89696
[1mStep[0m  [32/42], [94mLoss[0m : 2.76899
[1mStep[0m  [36/42], [94mLoss[0m : 2.74081
[1mStep[0m  [40/42], [94mLoss[0m : 2.59993

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.512, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62470
[1mStep[0m  [4/42], [94mLoss[0m : 2.73829
[1mStep[0m  [8/42], [94mLoss[0m : 2.78073
[1mStep[0m  [12/42], [94mLoss[0m : 2.50554
[1mStep[0m  [16/42], [94mLoss[0m : 2.32321
[1mStep[0m  [20/42], [94mLoss[0m : 2.56736
[1mStep[0m  [24/42], [94mLoss[0m : 2.91863
[1mStep[0m  [28/42], [94mLoss[0m : 2.73488
[1mStep[0m  [32/42], [94mLoss[0m : 2.50363
[1mStep[0m  [36/42], [94mLoss[0m : 2.81896
[1mStep[0m  [40/42], [94mLoss[0m : 2.83412

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.514, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55400
[1mStep[0m  [4/42], [94mLoss[0m : 2.49415
[1mStep[0m  [8/42], [94mLoss[0m : 2.94836
[1mStep[0m  [12/42], [94mLoss[0m : 2.61134
[1mStep[0m  [16/42], [94mLoss[0m : 2.89695
[1mStep[0m  [20/42], [94mLoss[0m : 2.76613
[1mStep[0m  [24/42], [94mLoss[0m : 2.59379
[1mStep[0m  [28/42], [94mLoss[0m : 2.45737
[1mStep[0m  [32/42], [94mLoss[0m : 2.69337
[1mStep[0m  [36/42], [94mLoss[0m : 2.67924
[1mStep[0m  [40/42], [94mLoss[0m : 3.02053

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76635
[1mStep[0m  [4/42], [94mLoss[0m : 2.44024
[1mStep[0m  [8/42], [94mLoss[0m : 2.70064
[1mStep[0m  [12/42], [94mLoss[0m : 2.59222
[1mStep[0m  [16/42], [94mLoss[0m : 2.57443
[1mStep[0m  [20/42], [94mLoss[0m : 2.85821
[1mStep[0m  [24/42], [94mLoss[0m : 2.65579
[1mStep[0m  [28/42], [94mLoss[0m : 2.53936
[1mStep[0m  [32/42], [94mLoss[0m : 2.45173
[1mStep[0m  [36/42], [94mLoss[0m : 2.54146
[1mStep[0m  [40/42], [94mLoss[0m : 2.65355

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.560, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71613
[1mStep[0m  [4/42], [94mLoss[0m : 2.46210
[1mStep[0m  [8/42], [94mLoss[0m : 2.61857
[1mStep[0m  [12/42], [94mLoss[0m : 2.69699
[1mStep[0m  [16/42], [94mLoss[0m : 2.59342
[1mStep[0m  [20/42], [94mLoss[0m : 2.66948
[1mStep[0m  [24/42], [94mLoss[0m : 2.87637
[1mStep[0m  [28/42], [94mLoss[0m : 2.64113
[1mStep[0m  [32/42], [94mLoss[0m : 2.59428
[1mStep[0m  [36/42], [94mLoss[0m : 2.72120
[1mStep[0m  [40/42], [94mLoss[0m : 2.54438

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.514, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76970
[1mStep[0m  [4/42], [94mLoss[0m : 2.31883
[1mStep[0m  [8/42], [94mLoss[0m : 2.54959
[1mStep[0m  [12/42], [94mLoss[0m : 2.64299
[1mStep[0m  [16/42], [94mLoss[0m : 2.54385
[1mStep[0m  [20/42], [94mLoss[0m : 2.71237
[1mStep[0m  [24/42], [94mLoss[0m : 2.61292
[1mStep[0m  [28/42], [94mLoss[0m : 2.43666
[1mStep[0m  [32/42], [94mLoss[0m : 2.68415
[1mStep[0m  [36/42], [94mLoss[0m : 2.63668
[1mStep[0m  [40/42], [94mLoss[0m : 2.53518

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.511, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47519
[1mStep[0m  [4/42], [94mLoss[0m : 2.71111
[1mStep[0m  [8/42], [94mLoss[0m : 2.51917
[1mStep[0m  [12/42], [94mLoss[0m : 2.60900
[1mStep[0m  [16/42], [94mLoss[0m : 2.44525
[1mStep[0m  [20/42], [94mLoss[0m : 2.58670
[1mStep[0m  [24/42], [94mLoss[0m : 2.60360
[1mStep[0m  [28/42], [94mLoss[0m : 2.39262
[1mStep[0m  [32/42], [94mLoss[0m : 2.62727
[1mStep[0m  [36/42], [94mLoss[0m : 2.51991
[1mStep[0m  [40/42], [94mLoss[0m : 2.73937

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29271
[1mStep[0m  [4/42], [94mLoss[0m : 2.53093
[1mStep[0m  [8/42], [94mLoss[0m : 2.35387
[1mStep[0m  [12/42], [94mLoss[0m : 2.44141
[1mStep[0m  [16/42], [94mLoss[0m : 2.54219
[1mStep[0m  [20/42], [94mLoss[0m : 2.53281
[1mStep[0m  [24/42], [94mLoss[0m : 2.56312
[1mStep[0m  [28/42], [94mLoss[0m : 2.48593
[1mStep[0m  [32/42], [94mLoss[0m : 2.68308
[1mStep[0m  [36/42], [94mLoss[0m : 2.65713
[1mStep[0m  [40/42], [94mLoss[0m : 2.48586

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38372
[1mStep[0m  [4/42], [94mLoss[0m : 2.40165
[1mStep[0m  [8/42], [94mLoss[0m : 2.39635
[1mStep[0m  [12/42], [94mLoss[0m : 2.55093
[1mStep[0m  [16/42], [94mLoss[0m : 2.49472
[1mStep[0m  [20/42], [94mLoss[0m : 2.67875
[1mStep[0m  [24/42], [94mLoss[0m : 2.45449
[1mStep[0m  [28/42], [94mLoss[0m : 2.36198
[1mStep[0m  [32/42], [94mLoss[0m : 2.69873
[1mStep[0m  [36/42], [94mLoss[0m : 2.71141
[1mStep[0m  [40/42], [94mLoss[0m : 2.84787

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.505, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70527
[1mStep[0m  [4/42], [94mLoss[0m : 2.38640
[1mStep[0m  [8/42], [94mLoss[0m : 2.40027
[1mStep[0m  [12/42], [94mLoss[0m : 2.44942
[1mStep[0m  [16/42], [94mLoss[0m : 2.45221
[1mStep[0m  [20/42], [94mLoss[0m : 2.68219
[1mStep[0m  [24/42], [94mLoss[0m : 2.73708
[1mStep[0m  [28/42], [94mLoss[0m : 2.38207
[1mStep[0m  [32/42], [94mLoss[0m : 2.36942
[1mStep[0m  [36/42], [94mLoss[0m : 2.57222
[1mStep[0m  [40/42], [94mLoss[0m : 2.72960

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.502, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49482
[1mStep[0m  [4/42], [94mLoss[0m : 2.50830
[1mStep[0m  [8/42], [94mLoss[0m : 2.43754
[1mStep[0m  [12/42], [94mLoss[0m : 2.43631
[1mStep[0m  [16/42], [94mLoss[0m : 2.54777
[1mStep[0m  [20/42], [94mLoss[0m : 2.54678
[1mStep[0m  [24/42], [94mLoss[0m : 2.59529
[1mStep[0m  [28/42], [94mLoss[0m : 2.23831
[1mStep[0m  [32/42], [94mLoss[0m : 2.39218
[1mStep[0m  [36/42], [94mLoss[0m : 2.42653
[1mStep[0m  [40/42], [94mLoss[0m : 2.64616

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.476, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59306
[1mStep[0m  [4/42], [94mLoss[0m : 2.42108
[1mStep[0m  [8/42], [94mLoss[0m : 2.44055
[1mStep[0m  [12/42], [94mLoss[0m : 2.44766
[1mStep[0m  [16/42], [94mLoss[0m : 2.38321
[1mStep[0m  [20/42], [94mLoss[0m : 2.53490
[1mStep[0m  [24/42], [94mLoss[0m : 2.20884
[1mStep[0m  [28/42], [94mLoss[0m : 2.50076
[1mStep[0m  [32/42], [94mLoss[0m : 2.44800
[1mStep[0m  [36/42], [94mLoss[0m : 2.67883
[1mStep[0m  [40/42], [94mLoss[0m : 2.48986

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29985
[1mStep[0m  [4/42], [94mLoss[0m : 2.46314
[1mStep[0m  [8/42], [94mLoss[0m : 2.62879
[1mStep[0m  [12/42], [94mLoss[0m : 2.30544
[1mStep[0m  [16/42], [94mLoss[0m : 2.15905
[1mStep[0m  [20/42], [94mLoss[0m : 2.35220
[1mStep[0m  [24/42], [94mLoss[0m : 2.39714
[1mStep[0m  [28/42], [94mLoss[0m : 2.75558
[1mStep[0m  [32/42], [94mLoss[0m : 2.45626
[1mStep[0m  [36/42], [94mLoss[0m : 2.54728
[1mStep[0m  [40/42], [94mLoss[0m : 2.37070

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.482, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39875
[1mStep[0m  [4/42], [94mLoss[0m : 2.54790
[1mStep[0m  [8/42], [94mLoss[0m : 2.43132
[1mStep[0m  [12/42], [94mLoss[0m : 2.30580
[1mStep[0m  [16/42], [94mLoss[0m : 2.43454
[1mStep[0m  [20/42], [94mLoss[0m : 2.33570
[1mStep[0m  [24/42], [94mLoss[0m : 2.28114
[1mStep[0m  [28/42], [94mLoss[0m : 2.39818
[1mStep[0m  [32/42], [94mLoss[0m : 2.52150
[1mStep[0m  [36/42], [94mLoss[0m : 2.52371
[1mStep[0m  [40/42], [94mLoss[0m : 2.42581

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.498, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33070
[1mStep[0m  [4/42], [94mLoss[0m : 2.35991
[1mStep[0m  [8/42], [94mLoss[0m : 2.44032
[1mStep[0m  [12/42], [94mLoss[0m : 2.33348
[1mStep[0m  [16/42], [94mLoss[0m : 2.51497
[1mStep[0m  [20/42], [94mLoss[0m : 2.25252
[1mStep[0m  [24/42], [94mLoss[0m : 2.24323
[1mStep[0m  [28/42], [94mLoss[0m : 2.26993
[1mStep[0m  [32/42], [94mLoss[0m : 2.49886
[1mStep[0m  [36/42], [94mLoss[0m : 2.23456
[1mStep[0m  [40/42], [94mLoss[0m : 2.60754

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.516, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32788
[1mStep[0m  [4/42], [94mLoss[0m : 2.43594
[1mStep[0m  [8/42], [94mLoss[0m : 2.26787
[1mStep[0m  [12/42], [94mLoss[0m : 2.63935
[1mStep[0m  [16/42], [94mLoss[0m : 2.48592
[1mStep[0m  [20/42], [94mLoss[0m : 2.26007
[1mStep[0m  [24/42], [94mLoss[0m : 2.48062
[1mStep[0m  [28/42], [94mLoss[0m : 2.49228
[1mStep[0m  [32/42], [94mLoss[0m : 2.26614
[1mStep[0m  [36/42], [94mLoss[0m : 2.35075
[1mStep[0m  [40/42], [94mLoss[0m : 2.08084

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31365
[1mStep[0m  [4/42], [94mLoss[0m : 2.31257
[1mStep[0m  [8/42], [94mLoss[0m : 2.37775
[1mStep[0m  [12/42], [94mLoss[0m : 2.26856
[1mStep[0m  [16/42], [94mLoss[0m : 2.28370
[1mStep[0m  [20/42], [94mLoss[0m : 2.39393
[1mStep[0m  [24/42], [94mLoss[0m : 2.28136
[1mStep[0m  [28/42], [94mLoss[0m : 2.52666
[1mStep[0m  [32/42], [94mLoss[0m : 2.34618
[1mStep[0m  [36/42], [94mLoss[0m : 2.30798
[1mStep[0m  [40/42], [94mLoss[0m : 2.10140

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41056
[1mStep[0m  [4/42], [94mLoss[0m : 2.43709
[1mStep[0m  [8/42], [94mLoss[0m : 2.21521
[1mStep[0m  [12/42], [94mLoss[0m : 2.40837
[1mStep[0m  [16/42], [94mLoss[0m : 2.22730
[1mStep[0m  [20/42], [94mLoss[0m : 2.14004
[1mStep[0m  [24/42], [94mLoss[0m : 2.38884
[1mStep[0m  [28/42], [94mLoss[0m : 2.20681
[1mStep[0m  [32/42], [94mLoss[0m : 2.38390
[1mStep[0m  [36/42], [94mLoss[0m : 2.54212
[1mStep[0m  [40/42], [94mLoss[0m : 2.13451

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38999
[1mStep[0m  [4/42], [94mLoss[0m : 2.32987
[1mStep[0m  [8/42], [94mLoss[0m : 2.19617
[1mStep[0m  [12/42], [94mLoss[0m : 2.25199
[1mStep[0m  [16/42], [94mLoss[0m : 2.34263
[1mStep[0m  [20/42], [94mLoss[0m : 2.12256
[1mStep[0m  [24/42], [94mLoss[0m : 2.32991
[1mStep[0m  [28/42], [94mLoss[0m : 2.26662
[1mStep[0m  [32/42], [94mLoss[0m : 2.49884
[1mStep[0m  [36/42], [94mLoss[0m : 2.36964
[1mStep[0m  [40/42], [94mLoss[0m : 2.28939

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35971
[1mStep[0m  [4/42], [94mLoss[0m : 2.20520
[1mStep[0m  [8/42], [94mLoss[0m : 2.29608
[1mStep[0m  [12/42], [94mLoss[0m : 2.54369
[1mStep[0m  [16/42], [94mLoss[0m : 2.32821
[1mStep[0m  [20/42], [94mLoss[0m : 2.26918
[1mStep[0m  [24/42], [94mLoss[0m : 2.20832
[1mStep[0m  [28/42], [94mLoss[0m : 2.21354
[1mStep[0m  [32/42], [94mLoss[0m : 2.38181
[1mStep[0m  [36/42], [94mLoss[0m : 2.49756
[1mStep[0m  [40/42], [94mLoss[0m : 2.05506

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.465, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48155
[1mStep[0m  [4/42], [94mLoss[0m : 2.22444
[1mStep[0m  [8/42], [94mLoss[0m : 2.31815
[1mStep[0m  [12/42], [94mLoss[0m : 2.19045
[1mStep[0m  [16/42], [94mLoss[0m : 2.19554
[1mStep[0m  [20/42], [94mLoss[0m : 2.23151
[1mStep[0m  [24/42], [94mLoss[0m : 2.25346
[1mStep[0m  [28/42], [94mLoss[0m : 2.22616
[1mStep[0m  [32/42], [94mLoss[0m : 2.26550
[1mStep[0m  [36/42], [94mLoss[0m : 2.37191
[1mStep[0m  [40/42], [94mLoss[0m : 2.44921

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31547
[1mStep[0m  [4/42], [94mLoss[0m : 2.04373
[1mStep[0m  [8/42], [94mLoss[0m : 2.13355
[1mStep[0m  [12/42], [94mLoss[0m : 2.35724
[1mStep[0m  [16/42], [94mLoss[0m : 2.28495
[1mStep[0m  [20/42], [94mLoss[0m : 2.33389
[1mStep[0m  [24/42], [94mLoss[0m : 2.22893
[1mStep[0m  [28/42], [94mLoss[0m : 2.12859
[1mStep[0m  [32/42], [94mLoss[0m : 2.39798
[1mStep[0m  [36/42], [94mLoss[0m : 2.10510
[1mStep[0m  [40/42], [94mLoss[0m : 2.02949

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.491, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17033
[1mStep[0m  [4/42], [94mLoss[0m : 2.18134
[1mStep[0m  [8/42], [94mLoss[0m : 2.11165
[1mStep[0m  [12/42], [94mLoss[0m : 2.31296
[1mStep[0m  [16/42], [94mLoss[0m : 2.23878
[1mStep[0m  [20/42], [94mLoss[0m : 2.02955
[1mStep[0m  [24/42], [94mLoss[0m : 2.30477
[1mStep[0m  [28/42], [94mLoss[0m : 2.28313
[1mStep[0m  [32/42], [94mLoss[0m : 2.18137
[1mStep[0m  [36/42], [94mLoss[0m : 2.28001
[1mStep[0m  [40/42], [94mLoss[0m : 2.09937

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21798
[1mStep[0m  [4/42], [94mLoss[0m : 2.17443
[1mStep[0m  [8/42], [94mLoss[0m : 2.14143
[1mStep[0m  [12/42], [94mLoss[0m : 2.31110
[1mStep[0m  [16/42], [94mLoss[0m : 2.21291
[1mStep[0m  [20/42], [94mLoss[0m : 2.30789
[1mStep[0m  [24/42], [94mLoss[0m : 2.02920
[1mStep[0m  [28/42], [94mLoss[0m : 2.32787
[1mStep[0m  [32/42], [94mLoss[0m : 2.10537
[1mStep[0m  [36/42], [94mLoss[0m : 2.14797
[1mStep[0m  [40/42], [94mLoss[0m : 2.08538

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11659
[1mStep[0m  [4/42], [94mLoss[0m : 2.23153
[1mStep[0m  [8/42], [94mLoss[0m : 2.30984
[1mStep[0m  [12/42], [94mLoss[0m : 2.23171
[1mStep[0m  [16/42], [94mLoss[0m : 1.99897
[1mStep[0m  [20/42], [94mLoss[0m : 2.27930
[1mStep[0m  [24/42], [94mLoss[0m : 2.14964
[1mStep[0m  [28/42], [94mLoss[0m : 2.25472
[1mStep[0m  [32/42], [94mLoss[0m : 2.24081
[1mStep[0m  [36/42], [94mLoss[0m : 2.26699
[1mStep[0m  [40/42], [94mLoss[0m : 2.16218

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.520, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16023
[1mStep[0m  [4/42], [94mLoss[0m : 2.40509
[1mStep[0m  [8/42], [94mLoss[0m : 2.03716
[1mStep[0m  [12/42], [94mLoss[0m : 2.23216
[1mStep[0m  [16/42], [94mLoss[0m : 2.09716
[1mStep[0m  [20/42], [94mLoss[0m : 2.05942
[1mStep[0m  [24/42], [94mLoss[0m : 2.27968
[1mStep[0m  [28/42], [94mLoss[0m : 2.05037
[1mStep[0m  [32/42], [94mLoss[0m : 2.20585
[1mStep[0m  [36/42], [94mLoss[0m : 2.06689
[1mStep[0m  [40/42], [94mLoss[0m : 1.94391

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.112, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.509
====================================

Phase 2 - Evaluation MAE:  2.5089925868170604
MAE score P1       2.381893
MAE score P2       2.508993
loss               2.112184
learning_rate      0.007525
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
