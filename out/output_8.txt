no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  8
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.65455
[1mStep[0m  [4/42], [94mLoss[0m : 9.53774
[1mStep[0m  [8/42], [94mLoss[0m : 6.89105
[1mStep[0m  [12/42], [94mLoss[0m : 3.88212
[1mStep[0m  [16/42], [94mLoss[0m : 2.76407
[1mStep[0m  [20/42], [94mLoss[0m : 3.25151
[1mStep[0m  [24/42], [94mLoss[0m : 3.59166
[1mStep[0m  [28/42], [94mLoss[0m : 3.19884
[1mStep[0m  [32/42], [94mLoss[0m : 2.54273
[1mStep[0m  [36/42], [94mLoss[0m : 2.58063
[1mStep[0m  [40/42], [94mLoss[0m : 2.69051

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.513, [92mTest[0m: 10.719, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.98192
[1mStep[0m  [4/42], [94mLoss[0m : 2.71200
[1mStep[0m  [8/42], [94mLoss[0m : 2.52565
[1mStep[0m  [12/42], [94mLoss[0m : 2.37576
[1mStep[0m  [16/42], [94mLoss[0m : 2.81566
[1mStep[0m  [20/42], [94mLoss[0m : 2.43297
[1mStep[0m  [24/42], [94mLoss[0m : 2.38674
[1mStep[0m  [28/42], [94mLoss[0m : 2.52446
[1mStep[0m  [32/42], [94mLoss[0m : 2.55447
[1mStep[0m  [36/42], [94mLoss[0m : 2.65434
[1mStep[0m  [40/42], [94mLoss[0m : 2.52560

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66818
[1mStep[0m  [4/42], [94mLoss[0m : 2.51288
[1mStep[0m  [8/42], [94mLoss[0m : 2.48976
[1mStep[0m  [12/42], [94mLoss[0m : 2.49947
[1mStep[0m  [16/42], [94mLoss[0m : 2.62396
[1mStep[0m  [20/42], [94mLoss[0m : 2.47492
[1mStep[0m  [24/42], [94mLoss[0m : 2.43607
[1mStep[0m  [28/42], [94mLoss[0m : 2.85240
[1mStep[0m  [32/42], [94mLoss[0m : 2.67487
[1mStep[0m  [36/42], [94mLoss[0m : 2.64736
[1mStep[0m  [40/42], [94mLoss[0m : 2.67403

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40709
[1mStep[0m  [4/42], [94mLoss[0m : 2.54736
[1mStep[0m  [8/42], [94mLoss[0m : 2.40660
[1mStep[0m  [12/42], [94mLoss[0m : 2.57829
[1mStep[0m  [16/42], [94mLoss[0m : 2.61845
[1mStep[0m  [20/42], [94mLoss[0m : 2.64163
[1mStep[0m  [24/42], [94mLoss[0m : 2.48158
[1mStep[0m  [28/42], [94mLoss[0m : 2.49092
[1mStep[0m  [32/42], [94mLoss[0m : 2.49654
[1mStep[0m  [36/42], [94mLoss[0m : 2.34291
[1mStep[0m  [40/42], [94mLoss[0m : 2.38994

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57709
[1mStep[0m  [4/42], [94mLoss[0m : 2.58857
[1mStep[0m  [8/42], [94mLoss[0m : 2.29143
[1mStep[0m  [12/42], [94mLoss[0m : 2.63540
[1mStep[0m  [16/42], [94mLoss[0m : 2.75607
[1mStep[0m  [20/42], [94mLoss[0m : 2.35032
[1mStep[0m  [24/42], [94mLoss[0m : 2.67942
[1mStep[0m  [28/42], [94mLoss[0m : 2.55033
[1mStep[0m  [32/42], [94mLoss[0m : 2.37217
[1mStep[0m  [36/42], [94mLoss[0m : 2.94093
[1mStep[0m  [40/42], [94mLoss[0m : 2.39344

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45712
[1mStep[0m  [4/42], [94mLoss[0m : 2.36138
[1mStep[0m  [8/42], [94mLoss[0m : 2.36925
[1mStep[0m  [12/42], [94mLoss[0m : 2.35571
[1mStep[0m  [16/42], [94mLoss[0m : 2.56380
[1mStep[0m  [20/42], [94mLoss[0m : 2.49302
[1mStep[0m  [24/42], [94mLoss[0m : 2.63946
[1mStep[0m  [28/42], [94mLoss[0m : 2.55972
[1mStep[0m  [32/42], [94mLoss[0m : 2.48236
[1mStep[0m  [36/42], [94mLoss[0m : 2.39889
[1mStep[0m  [40/42], [94mLoss[0m : 2.54494

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39773
[1mStep[0m  [4/42], [94mLoss[0m : 2.65110
[1mStep[0m  [8/42], [94mLoss[0m : 2.61278
[1mStep[0m  [12/42], [94mLoss[0m : 2.33599
[1mStep[0m  [16/42], [94mLoss[0m : 2.35228
[1mStep[0m  [20/42], [94mLoss[0m : 2.36650
[1mStep[0m  [24/42], [94mLoss[0m : 2.43073
[1mStep[0m  [28/42], [94mLoss[0m : 2.48244
[1mStep[0m  [32/42], [94mLoss[0m : 2.71675
[1mStep[0m  [36/42], [94mLoss[0m : 2.37084
[1mStep[0m  [40/42], [94mLoss[0m : 2.41611

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42057
[1mStep[0m  [4/42], [94mLoss[0m : 2.32715
[1mStep[0m  [8/42], [94mLoss[0m : 2.56583
[1mStep[0m  [12/42], [94mLoss[0m : 2.54051
[1mStep[0m  [16/42], [94mLoss[0m : 2.53301
[1mStep[0m  [20/42], [94mLoss[0m : 2.48224
[1mStep[0m  [24/42], [94mLoss[0m : 2.64950
[1mStep[0m  [28/42], [94mLoss[0m : 2.35909
[1mStep[0m  [32/42], [94mLoss[0m : 2.48136
[1mStep[0m  [36/42], [94mLoss[0m : 2.62661
[1mStep[0m  [40/42], [94mLoss[0m : 2.49520

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38251
[1mStep[0m  [4/42], [94mLoss[0m : 2.47625
[1mStep[0m  [8/42], [94mLoss[0m : 2.35976
[1mStep[0m  [12/42], [94mLoss[0m : 2.44637
[1mStep[0m  [16/42], [94mLoss[0m : 2.53147
[1mStep[0m  [20/42], [94mLoss[0m : 2.46291
[1mStep[0m  [24/42], [94mLoss[0m : 2.43088
[1mStep[0m  [28/42], [94mLoss[0m : 2.49191
[1mStep[0m  [32/42], [94mLoss[0m : 2.35375
[1mStep[0m  [36/42], [94mLoss[0m : 2.51018
[1mStep[0m  [40/42], [94mLoss[0m : 2.51576

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46004
[1mStep[0m  [4/42], [94mLoss[0m : 2.16471
[1mStep[0m  [8/42], [94mLoss[0m : 2.36320
[1mStep[0m  [12/42], [94mLoss[0m : 2.60331
[1mStep[0m  [16/42], [94mLoss[0m : 2.37290
[1mStep[0m  [20/42], [94mLoss[0m : 2.38759
[1mStep[0m  [24/42], [94mLoss[0m : 2.52096
[1mStep[0m  [28/42], [94mLoss[0m : 2.37969
[1mStep[0m  [32/42], [94mLoss[0m : 2.48081
[1mStep[0m  [36/42], [94mLoss[0m : 2.59820
[1mStep[0m  [40/42], [94mLoss[0m : 2.56022

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14999
[1mStep[0m  [4/42], [94mLoss[0m : 2.42476
[1mStep[0m  [8/42], [94mLoss[0m : 2.48532
[1mStep[0m  [12/42], [94mLoss[0m : 2.37543
[1mStep[0m  [16/42], [94mLoss[0m : 2.60053
[1mStep[0m  [20/42], [94mLoss[0m : 2.46029
[1mStep[0m  [24/42], [94mLoss[0m : 2.39023
[1mStep[0m  [28/42], [94mLoss[0m : 2.39022
[1mStep[0m  [32/42], [94mLoss[0m : 2.30255
[1mStep[0m  [36/42], [94mLoss[0m : 2.40341
[1mStep[0m  [40/42], [94mLoss[0m : 2.44382

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64342
[1mStep[0m  [4/42], [94mLoss[0m : 2.54139
[1mStep[0m  [8/42], [94mLoss[0m : 2.32167
[1mStep[0m  [12/42], [94mLoss[0m : 2.56170
[1mStep[0m  [16/42], [94mLoss[0m : 2.30641
[1mStep[0m  [20/42], [94mLoss[0m : 2.66358
[1mStep[0m  [24/42], [94mLoss[0m : 2.50202
[1mStep[0m  [28/42], [94mLoss[0m : 2.32576
[1mStep[0m  [32/42], [94mLoss[0m : 2.46096
[1mStep[0m  [36/42], [94mLoss[0m : 2.40464
[1mStep[0m  [40/42], [94mLoss[0m : 2.52637

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31681
[1mStep[0m  [4/42], [94mLoss[0m : 2.45424
[1mStep[0m  [8/42], [94mLoss[0m : 2.47288
[1mStep[0m  [12/42], [94mLoss[0m : 2.40629
[1mStep[0m  [16/42], [94mLoss[0m : 2.63693
[1mStep[0m  [20/42], [94mLoss[0m : 2.58117
[1mStep[0m  [24/42], [94mLoss[0m : 2.46839
[1mStep[0m  [28/42], [94mLoss[0m : 2.71660
[1mStep[0m  [32/42], [94mLoss[0m : 2.41098
[1mStep[0m  [36/42], [94mLoss[0m : 2.45308
[1mStep[0m  [40/42], [94mLoss[0m : 2.36715

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63665
[1mStep[0m  [4/42], [94mLoss[0m : 2.28468
[1mStep[0m  [8/42], [94mLoss[0m : 2.25350
[1mStep[0m  [12/42], [94mLoss[0m : 2.50559
[1mStep[0m  [16/42], [94mLoss[0m : 2.51915
[1mStep[0m  [20/42], [94mLoss[0m : 2.36542
[1mStep[0m  [24/42], [94mLoss[0m : 2.48857
[1mStep[0m  [28/42], [94mLoss[0m : 2.45209
[1mStep[0m  [32/42], [94mLoss[0m : 2.57918
[1mStep[0m  [36/42], [94mLoss[0m : 2.37247
[1mStep[0m  [40/42], [94mLoss[0m : 2.39077

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71607
[1mStep[0m  [4/42], [94mLoss[0m : 2.60019
[1mStep[0m  [8/42], [94mLoss[0m : 2.43201
[1mStep[0m  [12/42], [94mLoss[0m : 2.25496
[1mStep[0m  [16/42], [94mLoss[0m : 2.73851
[1mStep[0m  [20/42], [94mLoss[0m : 2.33199
[1mStep[0m  [24/42], [94mLoss[0m : 2.60462
[1mStep[0m  [28/42], [94mLoss[0m : 2.47705
[1mStep[0m  [32/42], [94mLoss[0m : 2.62048
[1mStep[0m  [36/42], [94mLoss[0m : 2.50058
[1mStep[0m  [40/42], [94mLoss[0m : 2.50693

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46212
[1mStep[0m  [4/42], [94mLoss[0m : 2.51604
[1mStep[0m  [8/42], [94mLoss[0m : 2.39518
[1mStep[0m  [12/42], [94mLoss[0m : 2.38272
[1mStep[0m  [16/42], [94mLoss[0m : 2.34195
[1mStep[0m  [20/42], [94mLoss[0m : 2.52158
[1mStep[0m  [24/42], [94mLoss[0m : 2.53516
[1mStep[0m  [28/42], [94mLoss[0m : 2.54587
[1mStep[0m  [32/42], [94mLoss[0m : 2.30918
[1mStep[0m  [36/42], [94mLoss[0m : 2.43101
[1mStep[0m  [40/42], [94mLoss[0m : 2.65736

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35737
[1mStep[0m  [4/42], [94mLoss[0m : 2.48345
[1mStep[0m  [8/42], [94mLoss[0m : 2.39606
[1mStep[0m  [12/42], [94mLoss[0m : 2.38080
[1mStep[0m  [16/42], [94mLoss[0m : 2.40135
[1mStep[0m  [20/42], [94mLoss[0m : 2.40496
[1mStep[0m  [24/42], [94mLoss[0m : 2.58667
[1mStep[0m  [28/42], [94mLoss[0m : 2.51755
[1mStep[0m  [32/42], [94mLoss[0m : 2.40597
[1mStep[0m  [36/42], [94mLoss[0m : 2.46049
[1mStep[0m  [40/42], [94mLoss[0m : 2.43084

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44977
[1mStep[0m  [4/42], [94mLoss[0m : 2.18492
[1mStep[0m  [8/42], [94mLoss[0m : 2.62211
[1mStep[0m  [12/42], [94mLoss[0m : 2.47976
[1mStep[0m  [16/42], [94mLoss[0m : 2.44475
[1mStep[0m  [20/42], [94mLoss[0m : 2.50238
[1mStep[0m  [24/42], [94mLoss[0m : 2.31514
[1mStep[0m  [28/42], [94mLoss[0m : 2.51503
[1mStep[0m  [32/42], [94mLoss[0m : 2.38890
[1mStep[0m  [36/42], [94mLoss[0m : 2.48287
[1mStep[0m  [40/42], [94mLoss[0m : 2.49078

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58543
[1mStep[0m  [4/42], [94mLoss[0m : 2.47214
[1mStep[0m  [8/42], [94mLoss[0m : 2.68656
[1mStep[0m  [12/42], [94mLoss[0m : 2.31920
[1mStep[0m  [16/42], [94mLoss[0m : 2.51196
[1mStep[0m  [20/42], [94mLoss[0m : 2.38004
[1mStep[0m  [24/42], [94mLoss[0m : 2.57879
[1mStep[0m  [28/42], [94mLoss[0m : 2.65586
[1mStep[0m  [32/42], [94mLoss[0m : 2.25192
[1mStep[0m  [36/42], [94mLoss[0m : 2.24663
[1mStep[0m  [40/42], [94mLoss[0m : 2.40832

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51980
[1mStep[0m  [4/42], [94mLoss[0m : 2.50912
[1mStep[0m  [8/42], [94mLoss[0m : 2.69486
[1mStep[0m  [12/42], [94mLoss[0m : 2.36116
[1mStep[0m  [16/42], [94mLoss[0m : 2.39307
[1mStep[0m  [20/42], [94mLoss[0m : 2.47520
[1mStep[0m  [24/42], [94mLoss[0m : 2.54995
[1mStep[0m  [28/42], [94mLoss[0m : 2.34491
[1mStep[0m  [32/42], [94mLoss[0m : 2.78595
[1mStep[0m  [36/42], [94mLoss[0m : 2.43858
[1mStep[0m  [40/42], [94mLoss[0m : 2.28708

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52127
[1mStep[0m  [4/42], [94mLoss[0m : 2.54517
[1mStep[0m  [8/42], [94mLoss[0m : 2.44734
[1mStep[0m  [12/42], [94mLoss[0m : 2.49088
[1mStep[0m  [16/42], [94mLoss[0m : 2.47014
[1mStep[0m  [20/42], [94mLoss[0m : 2.71710
[1mStep[0m  [24/42], [94mLoss[0m : 2.43593
[1mStep[0m  [28/42], [94mLoss[0m : 2.53839
[1mStep[0m  [32/42], [94mLoss[0m : 2.37150
[1mStep[0m  [36/42], [94mLoss[0m : 2.65994
[1mStep[0m  [40/42], [94mLoss[0m : 2.45071

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47388
[1mStep[0m  [4/42], [94mLoss[0m : 2.47205
[1mStep[0m  [8/42], [94mLoss[0m : 2.35139
[1mStep[0m  [12/42], [94mLoss[0m : 2.54984
[1mStep[0m  [16/42], [94mLoss[0m : 2.50161
[1mStep[0m  [20/42], [94mLoss[0m : 2.19166
[1mStep[0m  [24/42], [94mLoss[0m : 2.59543
[1mStep[0m  [28/42], [94mLoss[0m : 2.47320
[1mStep[0m  [32/42], [94mLoss[0m : 2.55383
[1mStep[0m  [36/42], [94mLoss[0m : 2.56142
[1mStep[0m  [40/42], [94mLoss[0m : 2.52990

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44347
[1mStep[0m  [4/42], [94mLoss[0m : 2.43017
[1mStep[0m  [8/42], [94mLoss[0m : 2.71862
[1mStep[0m  [12/42], [94mLoss[0m : 2.39207
[1mStep[0m  [16/42], [94mLoss[0m : 2.61955
[1mStep[0m  [20/42], [94mLoss[0m : 2.44133
[1mStep[0m  [24/42], [94mLoss[0m : 2.49790
[1mStep[0m  [28/42], [94mLoss[0m : 2.49171
[1mStep[0m  [32/42], [94mLoss[0m : 2.37696
[1mStep[0m  [36/42], [94mLoss[0m : 2.29089
[1mStep[0m  [40/42], [94mLoss[0m : 2.44263

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41622
[1mStep[0m  [4/42], [94mLoss[0m : 2.37111
[1mStep[0m  [8/42], [94mLoss[0m : 2.16885
[1mStep[0m  [12/42], [94mLoss[0m : 2.51637
[1mStep[0m  [16/42], [94mLoss[0m : 2.21668
[1mStep[0m  [20/42], [94mLoss[0m : 2.66521
[1mStep[0m  [24/42], [94mLoss[0m : 2.29279
[1mStep[0m  [28/42], [94mLoss[0m : 2.32905
[1mStep[0m  [32/42], [94mLoss[0m : 2.59330
[1mStep[0m  [36/42], [94mLoss[0m : 2.52732
[1mStep[0m  [40/42], [94mLoss[0m : 2.35691

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67091
[1mStep[0m  [4/42], [94mLoss[0m : 2.43997
[1mStep[0m  [8/42], [94mLoss[0m : 2.44833
[1mStep[0m  [12/42], [94mLoss[0m : 2.53454
[1mStep[0m  [16/42], [94mLoss[0m : 2.36018
[1mStep[0m  [20/42], [94mLoss[0m : 2.54674
[1mStep[0m  [24/42], [94mLoss[0m : 2.55543
[1mStep[0m  [28/42], [94mLoss[0m : 2.72325
[1mStep[0m  [32/42], [94mLoss[0m : 2.28652
[1mStep[0m  [36/42], [94mLoss[0m : 2.67268
[1mStep[0m  [40/42], [94mLoss[0m : 2.33227

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52331
[1mStep[0m  [4/42], [94mLoss[0m : 2.55184
[1mStep[0m  [8/42], [94mLoss[0m : 2.29069
[1mStep[0m  [12/42], [94mLoss[0m : 2.27008
[1mStep[0m  [16/42], [94mLoss[0m : 2.38665
[1mStep[0m  [20/42], [94mLoss[0m : 2.34273
[1mStep[0m  [24/42], [94mLoss[0m : 2.41134
[1mStep[0m  [28/42], [94mLoss[0m : 2.59516
[1mStep[0m  [32/42], [94mLoss[0m : 2.59330
[1mStep[0m  [36/42], [94mLoss[0m : 2.39742
[1mStep[0m  [40/42], [94mLoss[0m : 2.24943

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51562
[1mStep[0m  [4/42], [94mLoss[0m : 2.43702
[1mStep[0m  [8/42], [94mLoss[0m : 2.44590
[1mStep[0m  [12/42], [94mLoss[0m : 2.35584
[1mStep[0m  [16/42], [94mLoss[0m : 2.49000
[1mStep[0m  [20/42], [94mLoss[0m : 2.53535
[1mStep[0m  [24/42], [94mLoss[0m : 2.30754
[1mStep[0m  [28/42], [94mLoss[0m : 2.51558
[1mStep[0m  [32/42], [94mLoss[0m : 2.46727
[1mStep[0m  [36/42], [94mLoss[0m : 2.62423
[1mStep[0m  [40/42], [94mLoss[0m : 2.47476

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67197
[1mStep[0m  [4/42], [94mLoss[0m : 2.43416
[1mStep[0m  [8/42], [94mLoss[0m : 2.38188
[1mStep[0m  [12/42], [94mLoss[0m : 2.57572
[1mStep[0m  [16/42], [94mLoss[0m : 2.41387
[1mStep[0m  [20/42], [94mLoss[0m : 2.16260
[1mStep[0m  [24/42], [94mLoss[0m : 2.42797
[1mStep[0m  [28/42], [94mLoss[0m : 2.41041
[1mStep[0m  [32/42], [94mLoss[0m : 2.30904
[1mStep[0m  [36/42], [94mLoss[0m : 2.62018
[1mStep[0m  [40/42], [94mLoss[0m : 2.36711

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42788
[1mStep[0m  [4/42], [94mLoss[0m : 2.76405
[1mStep[0m  [8/42], [94mLoss[0m : 2.51543
[1mStep[0m  [12/42], [94mLoss[0m : 2.69555
[1mStep[0m  [16/42], [94mLoss[0m : 2.34506
[1mStep[0m  [20/42], [94mLoss[0m : 2.44242
[1mStep[0m  [24/42], [94mLoss[0m : 2.45900
[1mStep[0m  [28/42], [94mLoss[0m : 2.39849
[1mStep[0m  [32/42], [94mLoss[0m : 2.49606
[1mStep[0m  [36/42], [94mLoss[0m : 2.56278
[1mStep[0m  [40/42], [94mLoss[0m : 2.23199

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43879
[1mStep[0m  [4/42], [94mLoss[0m : 2.45497
[1mStep[0m  [8/42], [94mLoss[0m : 2.39148
[1mStep[0m  [12/42], [94mLoss[0m : 2.34338
[1mStep[0m  [16/42], [94mLoss[0m : 2.40254
[1mStep[0m  [20/42], [94mLoss[0m : 2.33432
[1mStep[0m  [24/42], [94mLoss[0m : 2.52287
[1mStep[0m  [28/42], [94mLoss[0m : 2.30890
[1mStep[0m  [32/42], [94mLoss[0m : 2.51139
[1mStep[0m  [36/42], [94mLoss[0m : 2.50718
[1mStep[0m  [40/42], [94mLoss[0m : 2.41272

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3314088753291538
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.39961
[1mStep[0m  [4/42], [94mLoss[0m : 2.53244
[1mStep[0m  [8/42], [94mLoss[0m : 2.17765
[1mStep[0m  [12/42], [94mLoss[0m : 2.43485
[1mStep[0m  [16/42], [94mLoss[0m : 2.41251
[1mStep[0m  [20/42], [94mLoss[0m : 2.51221
[1mStep[0m  [24/42], [94mLoss[0m : 2.43196
[1mStep[0m  [28/42], [94mLoss[0m : 2.47134
[1mStep[0m  [32/42], [94mLoss[0m : 2.32287
[1mStep[0m  [36/42], [94mLoss[0m : 2.35748
[1mStep[0m  [40/42], [94mLoss[0m : 2.44353

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43057
[1mStep[0m  [4/42], [94mLoss[0m : 2.40180
[1mStep[0m  [8/42], [94mLoss[0m : 2.31219
[1mStep[0m  [12/42], [94mLoss[0m : 2.44292
[1mStep[0m  [16/42], [94mLoss[0m : 2.49038
[1mStep[0m  [20/42], [94mLoss[0m : 2.17887
[1mStep[0m  [24/42], [94mLoss[0m : 2.39003
[1mStep[0m  [28/42], [94mLoss[0m : 2.23499
[1mStep[0m  [32/42], [94mLoss[0m : 2.31793
[1mStep[0m  [36/42], [94mLoss[0m : 2.61990
[1mStep[0m  [40/42], [94mLoss[0m : 2.37052

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37021
[1mStep[0m  [4/42], [94mLoss[0m : 2.24022
[1mStep[0m  [8/42], [94mLoss[0m : 2.26682
[1mStep[0m  [12/42], [94mLoss[0m : 2.27600
[1mStep[0m  [16/42], [94mLoss[0m : 2.45098
[1mStep[0m  [20/42], [94mLoss[0m : 2.30637
[1mStep[0m  [24/42], [94mLoss[0m : 2.19467
[1mStep[0m  [28/42], [94mLoss[0m : 2.24987
[1mStep[0m  [32/42], [94mLoss[0m : 2.34925
[1mStep[0m  [36/42], [94mLoss[0m : 2.24928
[1mStep[0m  [40/42], [94mLoss[0m : 2.41412

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26495
[1mStep[0m  [4/42], [94mLoss[0m : 2.22477
[1mStep[0m  [8/42], [94mLoss[0m : 2.26327
[1mStep[0m  [12/42], [94mLoss[0m : 2.21790
[1mStep[0m  [16/42], [94mLoss[0m : 2.09642
[1mStep[0m  [20/42], [94mLoss[0m : 2.31869
[1mStep[0m  [24/42], [94mLoss[0m : 2.29220
[1mStep[0m  [28/42], [94mLoss[0m : 2.36201
[1mStep[0m  [32/42], [94mLoss[0m : 2.33129
[1mStep[0m  [36/42], [94mLoss[0m : 2.47604
[1mStep[0m  [40/42], [94mLoss[0m : 2.42577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03943
[1mStep[0m  [4/42], [94mLoss[0m : 2.16991
[1mStep[0m  [8/42], [94mLoss[0m : 2.33932
[1mStep[0m  [12/42], [94mLoss[0m : 2.38627
[1mStep[0m  [16/42], [94mLoss[0m : 2.20250
[1mStep[0m  [20/42], [94mLoss[0m : 2.04164
[1mStep[0m  [24/42], [94mLoss[0m : 2.11839
[1mStep[0m  [28/42], [94mLoss[0m : 2.13488
[1mStep[0m  [32/42], [94mLoss[0m : 2.28951
[1mStep[0m  [36/42], [94mLoss[0m : 2.20184
[1mStep[0m  [40/42], [94mLoss[0m : 2.07334

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31617
[1mStep[0m  [4/42], [94mLoss[0m : 2.06326
[1mStep[0m  [8/42], [94mLoss[0m : 1.98746
[1mStep[0m  [12/42], [94mLoss[0m : 2.02664
[1mStep[0m  [16/42], [94mLoss[0m : 1.97482
[1mStep[0m  [20/42], [94mLoss[0m : 2.06147
[1mStep[0m  [24/42], [94mLoss[0m : 1.88869
[1mStep[0m  [28/42], [94mLoss[0m : 2.07399
[1mStep[0m  [32/42], [94mLoss[0m : 2.35637
[1mStep[0m  [36/42], [94mLoss[0m : 2.31303
[1mStep[0m  [40/42], [94mLoss[0m : 2.23417

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99885
[1mStep[0m  [4/42], [94mLoss[0m : 2.12284
[1mStep[0m  [8/42], [94mLoss[0m : 2.24301
[1mStep[0m  [12/42], [94mLoss[0m : 1.95183
[1mStep[0m  [16/42], [94mLoss[0m : 2.09861
[1mStep[0m  [20/42], [94mLoss[0m : 2.08304
[1mStep[0m  [24/42], [94mLoss[0m : 1.95289
[1mStep[0m  [28/42], [94mLoss[0m : 2.15832
[1mStep[0m  [32/42], [94mLoss[0m : 1.98909
[1mStep[0m  [36/42], [94mLoss[0m : 1.95324
[1mStep[0m  [40/42], [94mLoss[0m : 2.27930

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.588, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91836
[1mStep[0m  [4/42], [94mLoss[0m : 1.93565
[1mStep[0m  [8/42], [94mLoss[0m : 1.99617
[1mStep[0m  [12/42], [94mLoss[0m : 2.02343
[1mStep[0m  [16/42], [94mLoss[0m : 1.80348
[1mStep[0m  [20/42], [94mLoss[0m : 2.11063
[1mStep[0m  [24/42], [94mLoss[0m : 2.17575
[1mStep[0m  [28/42], [94mLoss[0m : 2.16704
[1mStep[0m  [32/42], [94mLoss[0m : 2.03309
[1mStep[0m  [36/42], [94mLoss[0m : 2.01827
[1mStep[0m  [40/42], [94mLoss[0m : 1.98535

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83746
[1mStep[0m  [4/42], [94mLoss[0m : 1.96593
[1mStep[0m  [8/42], [94mLoss[0m : 1.80889
[1mStep[0m  [12/42], [94mLoss[0m : 1.86374
[1mStep[0m  [16/42], [94mLoss[0m : 2.04213
[1mStep[0m  [20/42], [94mLoss[0m : 2.09781
[1mStep[0m  [24/42], [94mLoss[0m : 2.00486
[1mStep[0m  [28/42], [94mLoss[0m : 1.85797
[1mStep[0m  [32/42], [94mLoss[0m : 2.22485
[1mStep[0m  [36/42], [94mLoss[0m : 2.13070
[1mStep[0m  [40/42], [94mLoss[0m : 2.11062

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76274
[1mStep[0m  [4/42], [94mLoss[0m : 1.64267
[1mStep[0m  [8/42], [94mLoss[0m : 1.90204
[1mStep[0m  [12/42], [94mLoss[0m : 2.02882
[1mStep[0m  [16/42], [94mLoss[0m : 1.81391
[1mStep[0m  [20/42], [94mLoss[0m : 1.76161
[1mStep[0m  [24/42], [94mLoss[0m : 2.03175
[1mStep[0m  [28/42], [94mLoss[0m : 1.79868
[1mStep[0m  [32/42], [94mLoss[0m : 1.99245
[1mStep[0m  [36/42], [94mLoss[0m : 2.15775
[1mStep[0m  [40/42], [94mLoss[0m : 2.20748

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93524
[1mStep[0m  [4/42], [94mLoss[0m : 1.71981
[1mStep[0m  [8/42], [94mLoss[0m : 1.84806
[1mStep[0m  [12/42], [94mLoss[0m : 1.83036
[1mStep[0m  [16/42], [94mLoss[0m : 1.92254
[1mStep[0m  [20/42], [94mLoss[0m : 1.94004
[1mStep[0m  [24/42], [94mLoss[0m : 2.05383
[1mStep[0m  [28/42], [94mLoss[0m : 1.86739
[1mStep[0m  [32/42], [94mLoss[0m : 1.88013
[1mStep[0m  [36/42], [94mLoss[0m : 1.85204
[1mStep[0m  [40/42], [94mLoss[0m : 2.12491

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.904, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77969
[1mStep[0m  [4/42], [94mLoss[0m : 1.75970
[1mStep[0m  [8/42], [94mLoss[0m : 1.87729
[1mStep[0m  [12/42], [94mLoss[0m : 1.95557
[1mStep[0m  [16/42], [94mLoss[0m : 1.89348
[1mStep[0m  [20/42], [94mLoss[0m : 1.92692
[1mStep[0m  [24/42], [94mLoss[0m : 1.88299
[1mStep[0m  [28/42], [94mLoss[0m : 1.90330
[1mStep[0m  [32/42], [94mLoss[0m : 1.78905
[1mStep[0m  [36/42], [94mLoss[0m : 1.83286
[1mStep[0m  [40/42], [94mLoss[0m : 1.83617

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68677
[1mStep[0m  [4/42], [94mLoss[0m : 1.56122
[1mStep[0m  [8/42], [94mLoss[0m : 1.95587
[1mStep[0m  [12/42], [94mLoss[0m : 1.63298
[1mStep[0m  [16/42], [94mLoss[0m : 1.82095
[1mStep[0m  [20/42], [94mLoss[0m : 1.94988
[1mStep[0m  [24/42], [94mLoss[0m : 1.86363
[1mStep[0m  [28/42], [94mLoss[0m : 1.98043
[1mStep[0m  [32/42], [94mLoss[0m : 2.10495
[1mStep[0m  [36/42], [94mLoss[0m : 1.83099
[1mStep[0m  [40/42], [94mLoss[0m : 1.72651

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85100
[1mStep[0m  [4/42], [94mLoss[0m : 1.88755
[1mStep[0m  [8/42], [94mLoss[0m : 1.77683
[1mStep[0m  [12/42], [94mLoss[0m : 1.65884
[1mStep[0m  [16/42], [94mLoss[0m : 1.85340
[1mStep[0m  [20/42], [94mLoss[0m : 1.72620
[1mStep[0m  [24/42], [94mLoss[0m : 1.76170
[1mStep[0m  [28/42], [94mLoss[0m : 1.77387
[1mStep[0m  [32/42], [94mLoss[0m : 1.93530
[1mStep[0m  [36/42], [94mLoss[0m : 2.04254
[1mStep[0m  [40/42], [94mLoss[0m : 1.96450

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84882
[1mStep[0m  [4/42], [94mLoss[0m : 1.77420
[1mStep[0m  [8/42], [94mLoss[0m : 1.63452
[1mStep[0m  [12/42], [94mLoss[0m : 1.89618
[1mStep[0m  [16/42], [94mLoss[0m : 1.69276
[1mStep[0m  [20/42], [94mLoss[0m : 1.73092
[1mStep[0m  [24/42], [94mLoss[0m : 1.76254
[1mStep[0m  [28/42], [94mLoss[0m : 1.74895
[1mStep[0m  [32/42], [94mLoss[0m : 1.67899
[1mStep[0m  [36/42], [94mLoss[0m : 1.68820
[1mStep[0m  [40/42], [94mLoss[0m : 1.90095

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79863
[1mStep[0m  [4/42], [94mLoss[0m : 1.64536
[1mStep[0m  [8/42], [94mLoss[0m : 1.86383
[1mStep[0m  [12/42], [94mLoss[0m : 1.78237
[1mStep[0m  [16/42], [94mLoss[0m : 1.85000
[1mStep[0m  [20/42], [94mLoss[0m : 1.80362
[1mStep[0m  [24/42], [94mLoss[0m : 1.60884
[1mStep[0m  [28/42], [94mLoss[0m : 1.78272
[1mStep[0m  [32/42], [94mLoss[0m : 1.68510
[1mStep[0m  [36/42], [94mLoss[0m : 1.64800
[1mStep[0m  [40/42], [94mLoss[0m : 1.75185

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78628
[1mStep[0m  [4/42], [94mLoss[0m : 1.64965
[1mStep[0m  [8/42], [94mLoss[0m : 1.70886
[1mStep[0m  [12/42], [94mLoss[0m : 1.90668
[1mStep[0m  [16/42], [94mLoss[0m : 1.58687
[1mStep[0m  [20/42], [94mLoss[0m : 1.78893
[1mStep[0m  [24/42], [94mLoss[0m : 1.66631
[1mStep[0m  [28/42], [94mLoss[0m : 1.82773
[1mStep[0m  [32/42], [94mLoss[0m : 1.86023
[1mStep[0m  [36/42], [94mLoss[0m : 1.75200
[1mStep[0m  [40/42], [94mLoss[0m : 1.90244

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60646
[1mStep[0m  [4/42], [94mLoss[0m : 1.68879
[1mStep[0m  [8/42], [94mLoss[0m : 1.60793
[1mStep[0m  [12/42], [94mLoss[0m : 1.79806
[1mStep[0m  [16/42], [94mLoss[0m : 1.57867
[1mStep[0m  [20/42], [94mLoss[0m : 1.76355
[1mStep[0m  [24/42], [94mLoss[0m : 1.76792
[1mStep[0m  [28/42], [94mLoss[0m : 1.82006
[1mStep[0m  [32/42], [94mLoss[0m : 1.70333
[1mStep[0m  [36/42], [94mLoss[0m : 1.78282
[1mStep[0m  [40/42], [94mLoss[0m : 1.88554

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57421
[1mStep[0m  [4/42], [94mLoss[0m : 1.81097
[1mStep[0m  [8/42], [94mLoss[0m : 1.61657
[1mStep[0m  [12/42], [94mLoss[0m : 1.61595
[1mStep[0m  [16/42], [94mLoss[0m : 1.70952
[1mStep[0m  [20/42], [94mLoss[0m : 2.05257
[1mStep[0m  [24/42], [94mLoss[0m : 1.81279
[1mStep[0m  [28/42], [94mLoss[0m : 1.78026
[1mStep[0m  [32/42], [94mLoss[0m : 1.61678
[1mStep[0m  [36/42], [94mLoss[0m : 1.71634
[1mStep[0m  [40/42], [94mLoss[0m : 1.60850

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64702
[1mStep[0m  [4/42], [94mLoss[0m : 1.61928
[1mStep[0m  [8/42], [94mLoss[0m : 1.60746
[1mStep[0m  [12/42], [94mLoss[0m : 1.67201
[1mStep[0m  [16/42], [94mLoss[0m : 1.83974
[1mStep[0m  [20/42], [94mLoss[0m : 1.71308
[1mStep[0m  [24/42], [94mLoss[0m : 1.52093
[1mStep[0m  [28/42], [94mLoss[0m : 1.81788
[1mStep[0m  [32/42], [94mLoss[0m : 1.86495
[1mStep[0m  [36/42], [94mLoss[0m : 1.69173
[1mStep[0m  [40/42], [94mLoss[0m : 1.72659

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61665
[1mStep[0m  [4/42], [94mLoss[0m : 1.62311
[1mStep[0m  [8/42], [94mLoss[0m : 1.55287
[1mStep[0m  [12/42], [94mLoss[0m : 1.69763
[1mStep[0m  [16/42], [94mLoss[0m : 1.78443
[1mStep[0m  [20/42], [94mLoss[0m : 1.70542
[1mStep[0m  [24/42], [94mLoss[0m : 1.74062
[1mStep[0m  [28/42], [94mLoss[0m : 1.70193
[1mStep[0m  [32/42], [94mLoss[0m : 1.64101
[1mStep[0m  [36/42], [94mLoss[0m : 1.61184
[1mStep[0m  [40/42], [94mLoss[0m : 1.64845

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54558
[1mStep[0m  [4/42], [94mLoss[0m : 1.51064
[1mStep[0m  [8/42], [94mLoss[0m : 1.48142
[1mStep[0m  [12/42], [94mLoss[0m : 1.64319
[1mStep[0m  [16/42], [94mLoss[0m : 1.64739
[1mStep[0m  [20/42], [94mLoss[0m : 1.62775
[1mStep[0m  [24/42], [94mLoss[0m : 1.59624
[1mStep[0m  [28/42], [94mLoss[0m : 1.70532
[1mStep[0m  [32/42], [94mLoss[0m : 1.61705
[1mStep[0m  [36/42], [94mLoss[0m : 1.55820
[1mStep[0m  [40/42], [94mLoss[0m : 1.70721

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64056
[1mStep[0m  [4/42], [94mLoss[0m : 1.70036
[1mStep[0m  [8/42], [94mLoss[0m : 1.67491
[1mStep[0m  [12/42], [94mLoss[0m : 1.61284
[1mStep[0m  [16/42], [94mLoss[0m : 1.60998
[1mStep[0m  [20/42], [94mLoss[0m : 1.80004
[1mStep[0m  [24/42], [94mLoss[0m : 1.43260
[1mStep[0m  [28/42], [94mLoss[0m : 1.66814
[1mStep[0m  [32/42], [94mLoss[0m : 1.67407
[1mStep[0m  [36/42], [94mLoss[0m : 1.48704
[1mStep[0m  [40/42], [94mLoss[0m : 1.63613

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69862
[1mStep[0m  [4/42], [94mLoss[0m : 1.76696
[1mStep[0m  [8/42], [94mLoss[0m : 1.66557
[1mStep[0m  [12/42], [94mLoss[0m : 1.69123
[1mStep[0m  [16/42], [94mLoss[0m : 1.65937
[1mStep[0m  [20/42], [94mLoss[0m : 1.42877
[1mStep[0m  [24/42], [94mLoss[0m : 1.65642
[1mStep[0m  [28/42], [94mLoss[0m : 1.63863
[1mStep[0m  [32/42], [94mLoss[0m : 1.51573
[1mStep[0m  [36/42], [94mLoss[0m : 1.67478
[1mStep[0m  [40/42], [94mLoss[0m : 1.80381

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51383
[1mStep[0m  [4/42], [94mLoss[0m : 1.53111
[1mStep[0m  [8/42], [94mLoss[0m : 1.66286
[1mStep[0m  [12/42], [94mLoss[0m : 1.65185
[1mStep[0m  [16/42], [94mLoss[0m : 1.66792
[1mStep[0m  [20/42], [94mLoss[0m : 1.49962
[1mStep[0m  [24/42], [94mLoss[0m : 1.60109
[1mStep[0m  [28/42], [94mLoss[0m : 1.74089
[1mStep[0m  [32/42], [94mLoss[0m : 1.62290
[1mStep[0m  [36/42], [94mLoss[0m : 1.66965
[1mStep[0m  [40/42], [94mLoss[0m : 1.65865

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51936
[1mStep[0m  [4/42], [94mLoss[0m : 1.51544
[1mStep[0m  [8/42], [94mLoss[0m : 1.49115
[1mStep[0m  [12/42], [94mLoss[0m : 1.53644
[1mStep[0m  [16/42], [94mLoss[0m : 1.77130
[1mStep[0m  [20/42], [94mLoss[0m : 1.67777
[1mStep[0m  [24/42], [94mLoss[0m : 1.44920
[1mStep[0m  [28/42], [94mLoss[0m : 1.64909
[1mStep[0m  [32/42], [94mLoss[0m : 1.45323
[1mStep[0m  [36/42], [94mLoss[0m : 1.53162
[1mStep[0m  [40/42], [94mLoss[0m : 1.60167

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.538, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78826
[1mStep[0m  [4/42], [94mLoss[0m : 1.55326
[1mStep[0m  [8/42], [94mLoss[0m : 1.76279
[1mStep[0m  [12/42], [94mLoss[0m : 1.53613
[1mStep[0m  [16/42], [94mLoss[0m : 1.45019
[1mStep[0m  [20/42], [94mLoss[0m : 1.75700
[1mStep[0m  [24/42], [94mLoss[0m : 1.69242
[1mStep[0m  [28/42], [94mLoss[0m : 1.72235
[1mStep[0m  [32/42], [94mLoss[0m : 1.48846
[1mStep[0m  [36/42], [94mLoss[0m : 1.75543
[1mStep[0m  [40/42], [94mLoss[0m : 1.57614

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59328
[1mStep[0m  [4/42], [94mLoss[0m : 1.72736
[1mStep[0m  [8/42], [94mLoss[0m : 1.56870
[1mStep[0m  [12/42], [94mLoss[0m : 1.64476
[1mStep[0m  [16/42], [94mLoss[0m : 1.66063
[1mStep[0m  [20/42], [94mLoss[0m : 1.57432
[1mStep[0m  [24/42], [94mLoss[0m : 1.65603
[1mStep[0m  [28/42], [94mLoss[0m : 1.53968
[1mStep[0m  [32/42], [94mLoss[0m : 1.64865
[1mStep[0m  [36/42], [94mLoss[0m : 1.63938
[1mStep[0m  [40/42], [94mLoss[0m : 1.64698

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49850
[1mStep[0m  [4/42], [94mLoss[0m : 1.44147
[1mStep[0m  [8/42], [94mLoss[0m : 1.44463
[1mStep[0m  [12/42], [94mLoss[0m : 1.37683
[1mStep[0m  [16/42], [94mLoss[0m : 1.54744
[1mStep[0m  [20/42], [94mLoss[0m : 1.51902
[1mStep[0m  [24/42], [94mLoss[0m : 1.61701
[1mStep[0m  [28/42], [94mLoss[0m : 1.52886
[1mStep[0m  [32/42], [94mLoss[0m : 1.70804
[1mStep[0m  [36/42], [94mLoss[0m : 1.55635
[1mStep[0m  [40/42], [94mLoss[0m : 1.63454

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47715
[1mStep[0m  [4/42], [94mLoss[0m : 1.46835
[1mStep[0m  [8/42], [94mLoss[0m : 1.47859
[1mStep[0m  [12/42], [94mLoss[0m : 1.55930
[1mStep[0m  [16/42], [94mLoss[0m : 1.59505
[1mStep[0m  [20/42], [94mLoss[0m : 1.53460
[1mStep[0m  [24/42], [94mLoss[0m : 1.59761
[1mStep[0m  [28/42], [94mLoss[0m : 1.55766
[1mStep[0m  [32/42], [94mLoss[0m : 1.54975
[1mStep[0m  [36/42], [94mLoss[0m : 1.62898
[1mStep[0m  [40/42], [94mLoss[0m : 1.58475

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.555, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.4935071808951244
MAE score P1        2.331409
MAE score P2        2.493507
loss                1.554979
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay            0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.24375
[1mStep[0m  [8/84], [94mLoss[0m : 10.23195
[1mStep[0m  [16/84], [94mLoss[0m : 9.81770
[1mStep[0m  [24/84], [94mLoss[0m : 9.57229
[1mStep[0m  [32/84], [94mLoss[0m : 9.24912
[1mStep[0m  [40/84], [94mLoss[0m : 8.60641
[1mStep[0m  [48/84], [94mLoss[0m : 8.15046
[1mStep[0m  [56/84], [94mLoss[0m : 7.45900
[1mStep[0m  [64/84], [94mLoss[0m : 7.53394
[1mStep[0m  [72/84], [94mLoss[0m : 6.93345
[1mStep[0m  [80/84], [94mLoss[0m : 6.42715

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.660, [92mTest[0m: 10.885, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.54585
[1mStep[0m  [8/84], [94mLoss[0m : 6.15358
[1mStep[0m  [16/84], [94mLoss[0m : 5.33332
[1mStep[0m  [24/84], [94mLoss[0m : 4.94056
[1mStep[0m  [32/84], [94mLoss[0m : 4.76062
[1mStep[0m  [40/84], [94mLoss[0m : 4.17719
[1mStep[0m  [48/84], [94mLoss[0m : 4.42079
[1mStep[0m  [56/84], [94mLoss[0m : 4.35160
[1mStep[0m  [64/84], [94mLoss[0m : 3.82553
[1mStep[0m  [72/84], [94mLoss[0m : 3.40316
[1mStep[0m  [80/84], [94mLoss[0m : 3.87278

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.661, [92mTest[0m: 7.811, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.45637
[1mStep[0m  [8/84], [94mLoss[0m : 3.34742
[1mStep[0m  [16/84], [94mLoss[0m : 3.51393
[1mStep[0m  [24/84], [94mLoss[0m : 3.42591
[1mStep[0m  [32/84], [94mLoss[0m : 3.38267
[1mStep[0m  [40/84], [94mLoss[0m : 2.83040
[1mStep[0m  [48/84], [94mLoss[0m : 3.08547
[1mStep[0m  [56/84], [94mLoss[0m : 3.00573
[1mStep[0m  [64/84], [94mLoss[0m : 2.95175
[1mStep[0m  [72/84], [94mLoss[0m : 2.85505
[1mStep[0m  [80/84], [94mLoss[0m : 3.00868

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.180, [92mTest[0m: 4.253, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88853
[1mStep[0m  [8/84], [94mLoss[0m : 2.58722
[1mStep[0m  [16/84], [94mLoss[0m : 2.93935
[1mStep[0m  [24/84], [94mLoss[0m : 2.91127
[1mStep[0m  [32/84], [94mLoss[0m : 2.83420
[1mStep[0m  [40/84], [94mLoss[0m : 3.18261
[1mStep[0m  [48/84], [94mLoss[0m : 2.83769
[1mStep[0m  [56/84], [94mLoss[0m : 2.92522
[1mStep[0m  [64/84], [94mLoss[0m : 2.87207
[1mStep[0m  [72/84], [94mLoss[0m : 2.98252
[1mStep[0m  [80/84], [94mLoss[0m : 2.96396

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.983, [92mTest[0m: 3.218, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93379
[1mStep[0m  [8/84], [94mLoss[0m : 3.00291
[1mStep[0m  [16/84], [94mLoss[0m : 2.79822
[1mStep[0m  [24/84], [94mLoss[0m : 2.85718
[1mStep[0m  [32/84], [94mLoss[0m : 2.99073
[1mStep[0m  [40/84], [94mLoss[0m : 3.00508
[1mStep[0m  [48/84], [94mLoss[0m : 3.03981
[1mStep[0m  [56/84], [94mLoss[0m : 2.74641
[1mStep[0m  [64/84], [94mLoss[0m : 3.12379
[1mStep[0m  [72/84], [94mLoss[0m : 3.00181
[1mStep[0m  [80/84], [94mLoss[0m : 2.70564

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.917, [92mTest[0m: 2.824, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86346
[1mStep[0m  [8/84], [94mLoss[0m : 2.99975
[1mStep[0m  [16/84], [94mLoss[0m : 2.90140
[1mStep[0m  [24/84], [94mLoss[0m : 3.33174
[1mStep[0m  [32/84], [94mLoss[0m : 2.71990
[1mStep[0m  [40/84], [94mLoss[0m : 2.86734
[1mStep[0m  [48/84], [94mLoss[0m : 3.18045
[1mStep[0m  [56/84], [94mLoss[0m : 2.79115
[1mStep[0m  [64/84], [94mLoss[0m : 3.33078
[1mStep[0m  [72/84], [94mLoss[0m : 2.86307
[1mStep[0m  [80/84], [94mLoss[0m : 2.58257

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.914, [92mTest[0m: 2.702, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82102
[1mStep[0m  [8/84], [94mLoss[0m : 3.04048
[1mStep[0m  [16/84], [94mLoss[0m : 3.07152
[1mStep[0m  [24/84], [94mLoss[0m : 2.60384
[1mStep[0m  [32/84], [94mLoss[0m : 3.09552
[1mStep[0m  [40/84], [94mLoss[0m : 2.88353
[1mStep[0m  [48/84], [94mLoss[0m : 2.94977
[1mStep[0m  [56/84], [94mLoss[0m : 3.02383
[1mStep[0m  [64/84], [94mLoss[0m : 3.23660
[1mStep[0m  [72/84], [94mLoss[0m : 2.54820
[1mStep[0m  [80/84], [94mLoss[0m : 2.90972

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.879, [92mTest[0m: 2.712, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58764
[1mStep[0m  [8/84], [94mLoss[0m : 2.55156
[1mStep[0m  [16/84], [94mLoss[0m : 2.98252
[1mStep[0m  [24/84], [94mLoss[0m : 2.93279
[1mStep[0m  [32/84], [94mLoss[0m : 3.25924
[1mStep[0m  [40/84], [94mLoss[0m : 2.53616
[1mStep[0m  [48/84], [94mLoss[0m : 2.78088
[1mStep[0m  [56/84], [94mLoss[0m : 2.94598
[1mStep[0m  [64/84], [94mLoss[0m : 2.95969
[1mStep[0m  [72/84], [94mLoss[0m : 2.97060
[1mStep[0m  [80/84], [94mLoss[0m : 3.01082

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.866, [92mTest[0m: 2.636, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.10660
[1mStep[0m  [8/84], [94mLoss[0m : 2.99999
[1mStep[0m  [16/84], [94mLoss[0m : 2.76782
[1mStep[0m  [24/84], [94mLoss[0m : 2.71511
[1mStep[0m  [32/84], [94mLoss[0m : 3.25758
[1mStep[0m  [40/84], [94mLoss[0m : 2.47106
[1mStep[0m  [48/84], [94mLoss[0m : 2.68519
[1mStep[0m  [56/84], [94mLoss[0m : 3.05414
[1mStep[0m  [64/84], [94mLoss[0m : 2.64575
[1mStep[0m  [72/84], [94mLoss[0m : 3.15122
[1mStep[0m  [80/84], [94mLoss[0m : 2.88245

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.844, [92mTest[0m: 2.562, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81606
[1mStep[0m  [8/84], [94mLoss[0m : 2.98621
[1mStep[0m  [16/84], [94mLoss[0m : 2.74585
[1mStep[0m  [24/84], [94mLoss[0m : 2.77966
[1mStep[0m  [32/84], [94mLoss[0m : 2.85454
[1mStep[0m  [40/84], [94mLoss[0m : 3.22350
[1mStep[0m  [48/84], [94mLoss[0m : 3.00999
[1mStep[0m  [56/84], [94mLoss[0m : 3.02458
[1mStep[0m  [64/84], [94mLoss[0m : 3.00579
[1mStep[0m  [72/84], [94mLoss[0m : 3.06657
[1mStep[0m  [80/84], [94mLoss[0m : 2.59877

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.835, [92mTest[0m: 2.562, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.11093
[1mStep[0m  [8/84], [94mLoss[0m : 2.71986
[1mStep[0m  [16/84], [94mLoss[0m : 2.29136
[1mStep[0m  [24/84], [94mLoss[0m : 2.59030
[1mStep[0m  [32/84], [94mLoss[0m : 2.61243
[1mStep[0m  [40/84], [94mLoss[0m : 2.74740
[1mStep[0m  [48/84], [94mLoss[0m : 3.23292
[1mStep[0m  [56/84], [94mLoss[0m : 2.56163
[1mStep[0m  [64/84], [94mLoss[0m : 2.61587
[1mStep[0m  [72/84], [94mLoss[0m : 2.51340
[1mStep[0m  [80/84], [94mLoss[0m : 2.86578

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.840, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68537
[1mStep[0m  [8/84], [94mLoss[0m : 2.87554
[1mStep[0m  [16/84], [94mLoss[0m : 2.82291
[1mStep[0m  [24/84], [94mLoss[0m : 2.69525
[1mStep[0m  [32/84], [94mLoss[0m : 3.16699
[1mStep[0m  [40/84], [94mLoss[0m : 2.92315
[1mStep[0m  [48/84], [94mLoss[0m : 2.74296
[1mStep[0m  [56/84], [94mLoss[0m : 2.65447
[1mStep[0m  [64/84], [94mLoss[0m : 2.93866
[1mStep[0m  [72/84], [94mLoss[0m : 2.80667
[1mStep[0m  [80/84], [94mLoss[0m : 2.88062

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.831, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.13842
[1mStep[0m  [8/84], [94mLoss[0m : 2.78950
[1mStep[0m  [16/84], [94mLoss[0m : 2.75799
[1mStep[0m  [24/84], [94mLoss[0m : 2.91193
[1mStep[0m  [32/84], [94mLoss[0m : 2.64604
[1mStep[0m  [40/84], [94mLoss[0m : 2.83480
[1mStep[0m  [48/84], [94mLoss[0m : 2.32913
[1mStep[0m  [56/84], [94mLoss[0m : 2.83274
[1mStep[0m  [64/84], [94mLoss[0m : 2.79932
[1mStep[0m  [72/84], [94mLoss[0m : 2.60791
[1mStep[0m  [80/84], [94mLoss[0m : 2.96358

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.785, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.33462
[1mStep[0m  [8/84], [94mLoss[0m : 2.93123
[1mStep[0m  [16/84], [94mLoss[0m : 2.83263
[1mStep[0m  [24/84], [94mLoss[0m : 2.88190
[1mStep[0m  [32/84], [94mLoss[0m : 2.75679
[1mStep[0m  [40/84], [94mLoss[0m : 2.75378
[1mStep[0m  [48/84], [94mLoss[0m : 2.93077
[1mStep[0m  [56/84], [94mLoss[0m : 2.78040
[1mStep[0m  [64/84], [94mLoss[0m : 2.37110
[1mStep[0m  [72/84], [94mLoss[0m : 2.77014
[1mStep[0m  [80/84], [94mLoss[0m : 2.84070

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.760, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57561
[1mStep[0m  [8/84], [94mLoss[0m : 2.59108
[1mStep[0m  [16/84], [94mLoss[0m : 3.03911
[1mStep[0m  [24/84], [94mLoss[0m : 2.54578
[1mStep[0m  [32/84], [94mLoss[0m : 2.65437
[1mStep[0m  [40/84], [94mLoss[0m : 2.81787
[1mStep[0m  [48/84], [94mLoss[0m : 2.65953
[1mStep[0m  [56/84], [94mLoss[0m : 2.35148
[1mStep[0m  [64/84], [94mLoss[0m : 2.86490
[1mStep[0m  [72/84], [94mLoss[0m : 3.02263
[1mStep[0m  [80/84], [94mLoss[0m : 2.76148

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.754, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47630
[1mStep[0m  [8/84], [94mLoss[0m : 2.79381
[1mStep[0m  [16/84], [94mLoss[0m : 2.80194
[1mStep[0m  [24/84], [94mLoss[0m : 2.77228
[1mStep[0m  [32/84], [94mLoss[0m : 3.06987
[1mStep[0m  [40/84], [94mLoss[0m : 2.65931
[1mStep[0m  [48/84], [94mLoss[0m : 2.82636
[1mStep[0m  [56/84], [94mLoss[0m : 2.82303
[1mStep[0m  [64/84], [94mLoss[0m : 2.41174
[1mStep[0m  [72/84], [94mLoss[0m : 2.75781
[1mStep[0m  [80/84], [94mLoss[0m : 2.63747

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.787, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61960
[1mStep[0m  [8/84], [94mLoss[0m : 2.76575
[1mStep[0m  [16/84], [94mLoss[0m : 2.66298
[1mStep[0m  [24/84], [94mLoss[0m : 2.76295
[1mStep[0m  [32/84], [94mLoss[0m : 2.58348
[1mStep[0m  [40/84], [94mLoss[0m : 2.71397
[1mStep[0m  [48/84], [94mLoss[0m : 2.36701
[1mStep[0m  [56/84], [94mLoss[0m : 2.53102
[1mStep[0m  [64/84], [94mLoss[0m : 2.66873
[1mStep[0m  [72/84], [94mLoss[0m : 2.78465
[1mStep[0m  [80/84], [94mLoss[0m : 2.36187

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.730, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62357
[1mStep[0m  [8/84], [94mLoss[0m : 2.42513
[1mStep[0m  [16/84], [94mLoss[0m : 2.82034
[1mStep[0m  [24/84], [94mLoss[0m : 2.88221
[1mStep[0m  [32/84], [94mLoss[0m : 2.79632
[1mStep[0m  [40/84], [94mLoss[0m : 2.68247
[1mStep[0m  [48/84], [94mLoss[0m : 3.12355
[1mStep[0m  [56/84], [94mLoss[0m : 2.46972
[1mStep[0m  [64/84], [94mLoss[0m : 2.86997
[1mStep[0m  [72/84], [94mLoss[0m : 2.66113
[1mStep[0m  [80/84], [94mLoss[0m : 2.96456

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42517
[1mStep[0m  [8/84], [94mLoss[0m : 3.05142
[1mStep[0m  [16/84], [94mLoss[0m : 2.65208
[1mStep[0m  [24/84], [94mLoss[0m : 2.97001
[1mStep[0m  [32/84], [94mLoss[0m : 2.54417
[1mStep[0m  [40/84], [94mLoss[0m : 2.49478
[1mStep[0m  [48/84], [94mLoss[0m : 2.73181
[1mStep[0m  [56/84], [94mLoss[0m : 2.69288
[1mStep[0m  [64/84], [94mLoss[0m : 2.65797
[1mStep[0m  [72/84], [94mLoss[0m : 3.12122
[1mStep[0m  [80/84], [94mLoss[0m : 2.45343

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70352
[1mStep[0m  [8/84], [94mLoss[0m : 2.68236
[1mStep[0m  [16/84], [94mLoss[0m : 2.88215
[1mStep[0m  [24/84], [94mLoss[0m : 2.59958
[1mStep[0m  [32/84], [94mLoss[0m : 2.77237
[1mStep[0m  [40/84], [94mLoss[0m : 2.73225
[1mStep[0m  [48/84], [94mLoss[0m : 2.53288
[1mStep[0m  [56/84], [94mLoss[0m : 2.83674
[1mStep[0m  [64/84], [94mLoss[0m : 2.54409
[1mStep[0m  [72/84], [94mLoss[0m : 2.61633
[1mStep[0m  [80/84], [94mLoss[0m : 2.61426

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61143
[1mStep[0m  [8/84], [94mLoss[0m : 2.95879
[1mStep[0m  [16/84], [94mLoss[0m : 2.60090
[1mStep[0m  [24/84], [94mLoss[0m : 2.64161
[1mStep[0m  [32/84], [94mLoss[0m : 2.88296
[1mStep[0m  [40/84], [94mLoss[0m : 2.66707
[1mStep[0m  [48/84], [94mLoss[0m : 2.96039
[1mStep[0m  [56/84], [94mLoss[0m : 2.97970
[1mStep[0m  [64/84], [94mLoss[0m : 3.21961
[1mStep[0m  [72/84], [94mLoss[0m : 2.81067
[1mStep[0m  [80/84], [94mLoss[0m : 2.89211

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.393, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90881
[1mStep[0m  [8/84], [94mLoss[0m : 2.44035
[1mStep[0m  [16/84], [94mLoss[0m : 2.58013
[1mStep[0m  [24/84], [94mLoss[0m : 2.48363
[1mStep[0m  [32/84], [94mLoss[0m : 2.46854
[1mStep[0m  [40/84], [94mLoss[0m : 2.84750
[1mStep[0m  [48/84], [94mLoss[0m : 2.85676
[1mStep[0m  [56/84], [94mLoss[0m : 2.73399
[1mStep[0m  [64/84], [94mLoss[0m : 2.72988
[1mStep[0m  [72/84], [94mLoss[0m : 2.53155
[1mStep[0m  [80/84], [94mLoss[0m : 2.91413

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99912
[1mStep[0m  [8/84], [94mLoss[0m : 2.54058
[1mStep[0m  [16/84], [94mLoss[0m : 2.48805
[1mStep[0m  [24/84], [94mLoss[0m : 2.91715
[1mStep[0m  [32/84], [94mLoss[0m : 2.41302
[1mStep[0m  [40/84], [94mLoss[0m : 2.83703
[1mStep[0m  [48/84], [94mLoss[0m : 2.50260
[1mStep[0m  [56/84], [94mLoss[0m : 2.47665
[1mStep[0m  [64/84], [94mLoss[0m : 2.47375
[1mStep[0m  [72/84], [94mLoss[0m : 2.59995
[1mStep[0m  [80/84], [94mLoss[0m : 2.37142

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44397
[1mStep[0m  [8/84], [94mLoss[0m : 2.91080
[1mStep[0m  [16/84], [94mLoss[0m : 2.59675
[1mStep[0m  [24/84], [94mLoss[0m : 2.77316
[1mStep[0m  [32/84], [94mLoss[0m : 2.60554
[1mStep[0m  [40/84], [94mLoss[0m : 2.98551
[1mStep[0m  [48/84], [94mLoss[0m : 2.65072
[1mStep[0m  [56/84], [94mLoss[0m : 2.63816
[1mStep[0m  [64/84], [94mLoss[0m : 2.74650
[1mStep[0m  [72/84], [94mLoss[0m : 2.55577
[1mStep[0m  [80/84], [94mLoss[0m : 2.79749

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.702, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67973
[1mStep[0m  [8/84], [94mLoss[0m : 2.54156
[1mStep[0m  [16/84], [94mLoss[0m : 2.79437
[1mStep[0m  [24/84], [94mLoss[0m : 2.38759
[1mStep[0m  [32/84], [94mLoss[0m : 2.97631
[1mStep[0m  [40/84], [94mLoss[0m : 2.65150
[1mStep[0m  [48/84], [94mLoss[0m : 2.64953
[1mStep[0m  [56/84], [94mLoss[0m : 2.78156
[1mStep[0m  [64/84], [94mLoss[0m : 2.68753
[1mStep[0m  [72/84], [94mLoss[0m : 2.54177
[1mStep[0m  [80/84], [94mLoss[0m : 2.52128

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63984
[1mStep[0m  [8/84], [94mLoss[0m : 2.94233
[1mStep[0m  [16/84], [94mLoss[0m : 2.67966
[1mStep[0m  [24/84], [94mLoss[0m : 2.49013
[1mStep[0m  [32/84], [94mLoss[0m : 2.71398
[1mStep[0m  [40/84], [94mLoss[0m : 2.36451
[1mStep[0m  [48/84], [94mLoss[0m : 2.97119
[1mStep[0m  [56/84], [94mLoss[0m : 2.82204
[1mStep[0m  [64/84], [94mLoss[0m : 2.70847
[1mStep[0m  [72/84], [94mLoss[0m : 2.85011
[1mStep[0m  [80/84], [94mLoss[0m : 2.73342

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68924
[1mStep[0m  [8/84], [94mLoss[0m : 2.74386
[1mStep[0m  [16/84], [94mLoss[0m : 2.78354
[1mStep[0m  [24/84], [94mLoss[0m : 2.70425
[1mStep[0m  [32/84], [94mLoss[0m : 2.62560
[1mStep[0m  [40/84], [94mLoss[0m : 2.59496
[1mStep[0m  [48/84], [94mLoss[0m : 2.62229
[1mStep[0m  [56/84], [94mLoss[0m : 2.48758
[1mStep[0m  [64/84], [94mLoss[0m : 2.64426
[1mStep[0m  [72/84], [94mLoss[0m : 2.96286
[1mStep[0m  [80/84], [94mLoss[0m : 2.68468

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57362
[1mStep[0m  [8/84], [94mLoss[0m : 2.65478
[1mStep[0m  [16/84], [94mLoss[0m : 2.57679
[1mStep[0m  [24/84], [94mLoss[0m : 2.34872
[1mStep[0m  [32/84], [94mLoss[0m : 3.00331
[1mStep[0m  [40/84], [94mLoss[0m : 2.81356
[1mStep[0m  [48/84], [94mLoss[0m : 3.04219
[1mStep[0m  [56/84], [94mLoss[0m : 2.45110
[1mStep[0m  [64/84], [94mLoss[0m : 3.08307
[1mStep[0m  [72/84], [94mLoss[0m : 2.39751
[1mStep[0m  [80/84], [94mLoss[0m : 2.59846

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98223
[1mStep[0m  [8/84], [94mLoss[0m : 2.53320
[1mStep[0m  [16/84], [94mLoss[0m : 2.72667
[1mStep[0m  [24/84], [94mLoss[0m : 2.56435
[1mStep[0m  [32/84], [94mLoss[0m : 2.73397
[1mStep[0m  [40/84], [94mLoss[0m : 2.73143
[1mStep[0m  [48/84], [94mLoss[0m : 2.55994
[1mStep[0m  [56/84], [94mLoss[0m : 3.36789
[1mStep[0m  [64/84], [94mLoss[0m : 2.69642
[1mStep[0m  [72/84], [94mLoss[0m : 2.38810
[1mStep[0m  [80/84], [94mLoss[0m : 2.72089

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96965
[1mStep[0m  [8/84], [94mLoss[0m : 2.77250
[1mStep[0m  [16/84], [94mLoss[0m : 2.84278
[1mStep[0m  [24/84], [94mLoss[0m : 2.59102
[1mStep[0m  [32/84], [94mLoss[0m : 2.70551
[1mStep[0m  [40/84], [94mLoss[0m : 2.78167
[1mStep[0m  [48/84], [94mLoss[0m : 2.61397
[1mStep[0m  [56/84], [94mLoss[0m : 2.53024
[1mStep[0m  [64/84], [94mLoss[0m : 2.39863
[1mStep[0m  [72/84], [94mLoss[0m : 2.73850
[1mStep[0m  [80/84], [94mLoss[0m : 2.69818

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.363
====================================

Phase 1 - Evaluation MAE:  2.3629795483180454
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.79781
[1mStep[0m  [8/84], [94mLoss[0m : 2.86334
[1mStep[0m  [16/84], [94mLoss[0m : 2.64358
[1mStep[0m  [24/84], [94mLoss[0m : 2.72978
[1mStep[0m  [32/84], [94mLoss[0m : 2.86365
[1mStep[0m  [40/84], [94mLoss[0m : 2.20576
[1mStep[0m  [48/84], [94mLoss[0m : 2.51493
[1mStep[0m  [56/84], [94mLoss[0m : 2.77870
[1mStep[0m  [64/84], [94mLoss[0m : 2.85706
[1mStep[0m  [72/84], [94mLoss[0m : 2.98941
[1mStep[0m  [80/84], [94mLoss[0m : 2.95547

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76002
[1mStep[0m  [8/84], [94mLoss[0m : 2.60695
[1mStep[0m  [16/84], [94mLoss[0m : 2.55533
[1mStep[0m  [24/84], [94mLoss[0m : 2.58346
[1mStep[0m  [32/84], [94mLoss[0m : 2.91832
[1mStep[0m  [40/84], [94mLoss[0m : 2.73714
[1mStep[0m  [48/84], [94mLoss[0m : 2.40580
[1mStep[0m  [56/84], [94mLoss[0m : 2.50257
[1mStep[0m  [64/84], [94mLoss[0m : 2.89467
[1mStep[0m  [72/84], [94mLoss[0m : 3.09254
[1mStep[0m  [80/84], [94mLoss[0m : 2.86550

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.762, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74099
[1mStep[0m  [8/84], [94mLoss[0m : 2.83499
[1mStep[0m  [16/84], [94mLoss[0m : 2.69301
[1mStep[0m  [24/84], [94mLoss[0m : 2.46292
[1mStep[0m  [32/84], [94mLoss[0m : 2.75297
[1mStep[0m  [40/84], [94mLoss[0m : 2.69709
[1mStep[0m  [48/84], [94mLoss[0m : 2.43653
[1mStep[0m  [56/84], [94mLoss[0m : 2.85880
[1mStep[0m  [64/84], [94mLoss[0m : 2.68810
[1mStep[0m  [72/84], [94mLoss[0m : 2.52139
[1mStep[0m  [80/84], [94mLoss[0m : 2.68013

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.708, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54600
[1mStep[0m  [8/84], [94mLoss[0m : 3.10946
[1mStep[0m  [16/84], [94mLoss[0m : 2.96652
[1mStep[0m  [24/84], [94mLoss[0m : 2.48882
[1mStep[0m  [32/84], [94mLoss[0m : 2.63226
[1mStep[0m  [40/84], [94mLoss[0m : 3.24760
[1mStep[0m  [48/84], [94mLoss[0m : 2.37272
[1mStep[0m  [56/84], [94mLoss[0m : 2.58462
[1mStep[0m  [64/84], [94mLoss[0m : 2.46313
[1mStep[0m  [72/84], [94mLoss[0m : 2.39947
[1mStep[0m  [80/84], [94mLoss[0m : 2.98588

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.696, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73754
[1mStep[0m  [8/84], [94mLoss[0m : 2.56766
[1mStep[0m  [16/84], [94mLoss[0m : 2.38584
[1mStep[0m  [24/84], [94mLoss[0m : 2.68237
[1mStep[0m  [32/84], [94mLoss[0m : 2.39395
[1mStep[0m  [40/84], [94mLoss[0m : 2.61191
[1mStep[0m  [48/84], [94mLoss[0m : 2.53869
[1mStep[0m  [56/84], [94mLoss[0m : 2.73936
[1mStep[0m  [64/84], [94mLoss[0m : 2.39142
[1mStep[0m  [72/84], [94mLoss[0m : 2.43931
[1mStep[0m  [80/84], [94mLoss[0m : 2.51324

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.557, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77051
[1mStep[0m  [8/84], [94mLoss[0m : 2.78650
[1mStep[0m  [16/84], [94mLoss[0m : 2.94594
[1mStep[0m  [24/84], [94mLoss[0m : 2.34032
[1mStep[0m  [32/84], [94mLoss[0m : 2.57720
[1mStep[0m  [40/84], [94mLoss[0m : 2.81110
[1mStep[0m  [48/84], [94mLoss[0m : 2.58624
[1mStep[0m  [56/84], [94mLoss[0m : 2.72230
[1mStep[0m  [64/84], [94mLoss[0m : 2.53087
[1mStep[0m  [72/84], [94mLoss[0m : 2.57050
[1mStep[0m  [80/84], [94mLoss[0m : 2.77381

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48555
[1mStep[0m  [8/84], [94mLoss[0m : 2.38679
[1mStep[0m  [16/84], [94mLoss[0m : 2.28452
[1mStep[0m  [24/84], [94mLoss[0m : 2.51317
[1mStep[0m  [32/84], [94mLoss[0m : 2.46170
[1mStep[0m  [40/84], [94mLoss[0m : 2.56146
[1mStep[0m  [48/84], [94mLoss[0m : 2.67915
[1mStep[0m  [56/84], [94mLoss[0m : 2.55176
[1mStep[0m  [64/84], [94mLoss[0m : 2.69036
[1mStep[0m  [72/84], [94mLoss[0m : 2.64116
[1mStep[0m  [80/84], [94mLoss[0m : 2.36963

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26703
[1mStep[0m  [8/84], [94mLoss[0m : 2.63505
[1mStep[0m  [16/84], [94mLoss[0m : 2.55729
[1mStep[0m  [24/84], [94mLoss[0m : 2.57404
[1mStep[0m  [32/84], [94mLoss[0m : 2.74617
[1mStep[0m  [40/84], [94mLoss[0m : 2.51035
[1mStep[0m  [48/84], [94mLoss[0m : 2.44816
[1mStep[0m  [56/84], [94mLoss[0m : 2.54187
[1mStep[0m  [64/84], [94mLoss[0m : 2.28662
[1mStep[0m  [72/84], [94mLoss[0m : 2.45141
[1mStep[0m  [80/84], [94mLoss[0m : 2.60269

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60789
[1mStep[0m  [8/84], [94mLoss[0m : 2.58714
[1mStep[0m  [16/84], [94mLoss[0m : 2.62458
[1mStep[0m  [24/84], [94mLoss[0m : 2.38303
[1mStep[0m  [32/84], [94mLoss[0m : 2.64311
[1mStep[0m  [40/84], [94mLoss[0m : 2.41121
[1mStep[0m  [48/84], [94mLoss[0m : 2.46243
[1mStep[0m  [56/84], [94mLoss[0m : 2.38848
[1mStep[0m  [64/84], [94mLoss[0m : 2.56575
[1mStep[0m  [72/84], [94mLoss[0m : 2.86378
[1mStep[0m  [80/84], [94mLoss[0m : 2.33558

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41327
[1mStep[0m  [8/84], [94mLoss[0m : 2.66775
[1mStep[0m  [16/84], [94mLoss[0m : 2.23854
[1mStep[0m  [24/84], [94mLoss[0m : 2.47592
[1mStep[0m  [32/84], [94mLoss[0m : 2.43264
[1mStep[0m  [40/84], [94mLoss[0m : 2.47548
[1mStep[0m  [48/84], [94mLoss[0m : 2.49272
[1mStep[0m  [56/84], [94mLoss[0m : 2.21904
[1mStep[0m  [64/84], [94mLoss[0m : 2.54631
[1mStep[0m  [72/84], [94mLoss[0m : 2.54641
[1mStep[0m  [80/84], [94mLoss[0m : 2.37608

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42367
[1mStep[0m  [8/84], [94mLoss[0m : 2.28293
[1mStep[0m  [16/84], [94mLoss[0m : 2.43067
[1mStep[0m  [24/84], [94mLoss[0m : 2.29869
[1mStep[0m  [32/84], [94mLoss[0m : 2.36499
[1mStep[0m  [40/84], [94mLoss[0m : 2.54463
[1mStep[0m  [48/84], [94mLoss[0m : 2.24465
[1mStep[0m  [56/84], [94mLoss[0m : 2.54855
[1mStep[0m  [64/84], [94mLoss[0m : 2.53866
[1mStep[0m  [72/84], [94mLoss[0m : 2.52192
[1mStep[0m  [80/84], [94mLoss[0m : 2.38344

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30011
[1mStep[0m  [8/84], [94mLoss[0m : 2.68327
[1mStep[0m  [16/84], [94mLoss[0m : 2.42203
[1mStep[0m  [24/84], [94mLoss[0m : 2.23650
[1mStep[0m  [32/84], [94mLoss[0m : 2.59138
[1mStep[0m  [40/84], [94mLoss[0m : 2.58250
[1mStep[0m  [48/84], [94mLoss[0m : 2.50405
[1mStep[0m  [56/84], [94mLoss[0m : 2.44083
[1mStep[0m  [64/84], [94mLoss[0m : 2.50847
[1mStep[0m  [72/84], [94mLoss[0m : 2.36961
[1mStep[0m  [80/84], [94mLoss[0m : 2.58481

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19157
[1mStep[0m  [8/84], [94mLoss[0m : 2.28228
[1mStep[0m  [16/84], [94mLoss[0m : 2.52264
[1mStep[0m  [24/84], [94mLoss[0m : 2.59655
[1mStep[0m  [32/84], [94mLoss[0m : 2.46346
[1mStep[0m  [40/84], [94mLoss[0m : 2.60589
[1mStep[0m  [48/84], [94mLoss[0m : 2.37618
[1mStep[0m  [56/84], [94mLoss[0m : 2.33277
[1mStep[0m  [64/84], [94mLoss[0m : 2.16030
[1mStep[0m  [72/84], [94mLoss[0m : 2.25255
[1mStep[0m  [80/84], [94mLoss[0m : 2.42536

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43017
[1mStep[0m  [8/84], [94mLoss[0m : 2.24949
[1mStep[0m  [16/84], [94mLoss[0m : 2.56645
[1mStep[0m  [24/84], [94mLoss[0m : 2.41135
[1mStep[0m  [32/84], [94mLoss[0m : 2.38985
[1mStep[0m  [40/84], [94mLoss[0m : 2.65710
[1mStep[0m  [48/84], [94mLoss[0m : 2.46162
[1mStep[0m  [56/84], [94mLoss[0m : 2.22320
[1mStep[0m  [64/84], [94mLoss[0m : 2.55830
[1mStep[0m  [72/84], [94mLoss[0m : 2.35910
[1mStep[0m  [80/84], [94mLoss[0m : 2.08352

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36040
[1mStep[0m  [8/84], [94mLoss[0m : 2.13197
[1mStep[0m  [16/84], [94mLoss[0m : 2.08264
[1mStep[0m  [24/84], [94mLoss[0m : 2.11022
[1mStep[0m  [32/84], [94mLoss[0m : 2.26947
[1mStep[0m  [40/84], [94mLoss[0m : 2.21603
[1mStep[0m  [48/84], [94mLoss[0m : 2.05976
[1mStep[0m  [56/84], [94mLoss[0m : 2.36689
[1mStep[0m  [64/84], [94mLoss[0m : 2.29107
[1mStep[0m  [72/84], [94mLoss[0m : 2.65680
[1mStep[0m  [80/84], [94mLoss[0m : 2.59411

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32870
[1mStep[0m  [8/84], [94mLoss[0m : 2.50686
[1mStep[0m  [16/84], [94mLoss[0m : 2.41947
[1mStep[0m  [24/84], [94mLoss[0m : 2.15408
[1mStep[0m  [32/84], [94mLoss[0m : 2.38891
[1mStep[0m  [40/84], [94mLoss[0m : 2.12594
[1mStep[0m  [48/84], [94mLoss[0m : 2.33092
[1mStep[0m  [56/84], [94mLoss[0m : 2.30182
[1mStep[0m  [64/84], [94mLoss[0m : 2.30119
[1mStep[0m  [72/84], [94mLoss[0m : 2.48601
[1mStep[0m  [80/84], [94mLoss[0m : 2.34499

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22796
[1mStep[0m  [8/84], [94mLoss[0m : 2.41390
[1mStep[0m  [16/84], [94mLoss[0m : 2.11011
[1mStep[0m  [24/84], [94mLoss[0m : 2.16676
[1mStep[0m  [32/84], [94mLoss[0m : 2.47383
[1mStep[0m  [40/84], [94mLoss[0m : 2.46682
[1mStep[0m  [48/84], [94mLoss[0m : 2.47819
[1mStep[0m  [56/84], [94mLoss[0m : 2.35782
[1mStep[0m  [64/84], [94mLoss[0m : 2.44344
[1mStep[0m  [72/84], [94mLoss[0m : 2.42843
[1mStep[0m  [80/84], [94mLoss[0m : 2.19830

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18338
[1mStep[0m  [8/84], [94mLoss[0m : 2.35862
[1mStep[0m  [16/84], [94mLoss[0m : 2.23420
[1mStep[0m  [24/84], [94mLoss[0m : 1.96890
[1mStep[0m  [32/84], [94mLoss[0m : 2.24036
[1mStep[0m  [40/84], [94mLoss[0m : 2.11708
[1mStep[0m  [48/84], [94mLoss[0m : 2.25435
[1mStep[0m  [56/84], [94mLoss[0m : 2.61284
[1mStep[0m  [64/84], [94mLoss[0m : 2.54435
[1mStep[0m  [72/84], [94mLoss[0m : 2.23239
[1mStep[0m  [80/84], [94mLoss[0m : 2.41907

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48542
[1mStep[0m  [8/84], [94mLoss[0m : 2.07519
[1mStep[0m  [16/84], [94mLoss[0m : 2.25879
[1mStep[0m  [24/84], [94mLoss[0m : 2.35557
[1mStep[0m  [32/84], [94mLoss[0m : 2.08988
[1mStep[0m  [40/84], [94mLoss[0m : 2.20331
[1mStep[0m  [48/84], [94mLoss[0m : 2.24510
[1mStep[0m  [56/84], [94mLoss[0m : 2.47074
[1mStep[0m  [64/84], [94mLoss[0m : 2.40315
[1mStep[0m  [72/84], [94mLoss[0m : 2.23324
[1mStep[0m  [80/84], [94mLoss[0m : 2.23442

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94595
[1mStep[0m  [8/84], [94mLoss[0m : 2.26699
[1mStep[0m  [16/84], [94mLoss[0m : 2.27881
[1mStep[0m  [24/84], [94mLoss[0m : 2.41593
[1mStep[0m  [32/84], [94mLoss[0m : 2.10685
[1mStep[0m  [40/84], [94mLoss[0m : 2.11019
[1mStep[0m  [48/84], [94mLoss[0m : 2.20856
[1mStep[0m  [56/84], [94mLoss[0m : 1.98991
[1mStep[0m  [64/84], [94mLoss[0m : 2.03389
[1mStep[0m  [72/84], [94mLoss[0m : 2.16622
[1mStep[0m  [80/84], [94mLoss[0m : 2.24642

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00831
[1mStep[0m  [8/84], [94mLoss[0m : 2.16547
[1mStep[0m  [16/84], [94mLoss[0m : 2.33336
[1mStep[0m  [24/84], [94mLoss[0m : 2.29730
[1mStep[0m  [32/84], [94mLoss[0m : 2.16898
[1mStep[0m  [40/84], [94mLoss[0m : 2.25267
[1mStep[0m  [48/84], [94mLoss[0m : 2.16479
[1mStep[0m  [56/84], [94mLoss[0m : 2.21954
[1mStep[0m  [64/84], [94mLoss[0m : 2.28930
[1mStep[0m  [72/84], [94mLoss[0m : 2.25587
[1mStep[0m  [80/84], [94mLoss[0m : 2.43919

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06911
[1mStep[0m  [8/84], [94mLoss[0m : 2.24762
[1mStep[0m  [16/84], [94mLoss[0m : 2.09075
[1mStep[0m  [24/84], [94mLoss[0m : 2.01826
[1mStep[0m  [32/84], [94mLoss[0m : 2.01574
[1mStep[0m  [40/84], [94mLoss[0m : 2.35326
[1mStep[0m  [48/84], [94mLoss[0m : 2.37872
[1mStep[0m  [56/84], [94mLoss[0m : 1.91447
[1mStep[0m  [64/84], [94mLoss[0m : 1.93279
[1mStep[0m  [72/84], [94mLoss[0m : 1.99797
[1mStep[0m  [80/84], [94mLoss[0m : 2.03983

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16355
[1mStep[0m  [8/84], [94mLoss[0m : 2.13534
[1mStep[0m  [16/84], [94mLoss[0m : 2.22899
[1mStep[0m  [24/84], [94mLoss[0m : 2.19164
[1mStep[0m  [32/84], [94mLoss[0m : 2.26102
[1mStep[0m  [40/84], [94mLoss[0m : 1.99799
[1mStep[0m  [48/84], [94mLoss[0m : 2.21082
[1mStep[0m  [56/84], [94mLoss[0m : 2.24043
[1mStep[0m  [64/84], [94mLoss[0m : 2.03955
[1mStep[0m  [72/84], [94mLoss[0m : 2.50078
[1mStep[0m  [80/84], [94mLoss[0m : 2.01782

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82818
[1mStep[0m  [8/84], [94mLoss[0m : 1.88400
[1mStep[0m  [16/84], [94mLoss[0m : 2.10927
[1mStep[0m  [24/84], [94mLoss[0m : 2.11769
[1mStep[0m  [32/84], [94mLoss[0m : 2.19381
[1mStep[0m  [40/84], [94mLoss[0m : 2.15035
[1mStep[0m  [48/84], [94mLoss[0m : 1.92844
[1mStep[0m  [56/84], [94mLoss[0m : 2.00535
[1mStep[0m  [64/84], [94mLoss[0m : 2.19966
[1mStep[0m  [72/84], [94mLoss[0m : 2.04792
[1mStep[0m  [80/84], [94mLoss[0m : 1.99616

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15528
[1mStep[0m  [8/84], [94mLoss[0m : 2.02685
[1mStep[0m  [16/84], [94mLoss[0m : 2.03543
[1mStep[0m  [24/84], [94mLoss[0m : 2.15146
[1mStep[0m  [32/84], [94mLoss[0m : 1.98248
[1mStep[0m  [40/84], [94mLoss[0m : 2.30443
[1mStep[0m  [48/84], [94mLoss[0m : 2.03733
[1mStep[0m  [56/84], [94mLoss[0m : 2.10766
[1mStep[0m  [64/84], [94mLoss[0m : 1.81857
[1mStep[0m  [72/84], [94mLoss[0m : 2.00124
[1mStep[0m  [80/84], [94mLoss[0m : 2.16632

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15859
[1mStep[0m  [8/84], [94mLoss[0m : 2.20092
[1mStep[0m  [16/84], [94mLoss[0m : 1.78594
[1mStep[0m  [24/84], [94mLoss[0m : 1.99566
[1mStep[0m  [32/84], [94mLoss[0m : 1.89288
[1mStep[0m  [40/84], [94mLoss[0m : 1.90660
[1mStep[0m  [48/84], [94mLoss[0m : 1.85973
[1mStep[0m  [56/84], [94mLoss[0m : 2.05038
[1mStep[0m  [64/84], [94mLoss[0m : 1.94831
[1mStep[0m  [72/84], [94mLoss[0m : 2.15197
[1mStep[0m  [80/84], [94mLoss[0m : 1.95352

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.553, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76055
[1mStep[0m  [8/84], [94mLoss[0m : 2.06392
[1mStep[0m  [16/84], [94mLoss[0m : 2.20731
[1mStep[0m  [24/84], [94mLoss[0m : 2.19663
[1mStep[0m  [32/84], [94mLoss[0m : 1.98625
[1mStep[0m  [40/84], [94mLoss[0m : 1.75404
[1mStep[0m  [48/84], [94mLoss[0m : 1.91190
[1mStep[0m  [56/84], [94mLoss[0m : 1.98989
[1mStep[0m  [64/84], [94mLoss[0m : 2.20326
[1mStep[0m  [72/84], [94mLoss[0m : 2.11879
[1mStep[0m  [80/84], [94mLoss[0m : 2.03992

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92921
[1mStep[0m  [8/84], [94mLoss[0m : 2.07446
[1mStep[0m  [16/84], [94mLoss[0m : 2.22239
[1mStep[0m  [24/84], [94mLoss[0m : 2.24836
[1mStep[0m  [32/84], [94mLoss[0m : 1.87550
[1mStep[0m  [40/84], [94mLoss[0m : 2.19024
[1mStep[0m  [48/84], [94mLoss[0m : 1.87688
[1mStep[0m  [56/84], [94mLoss[0m : 2.07076
[1mStep[0m  [64/84], [94mLoss[0m : 1.94021
[1mStep[0m  [72/84], [94mLoss[0m : 1.82662
[1mStep[0m  [80/84], [94mLoss[0m : 2.28306

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04483
[1mStep[0m  [8/84], [94mLoss[0m : 1.97835
[1mStep[0m  [16/84], [94mLoss[0m : 1.94430
[1mStep[0m  [24/84], [94mLoss[0m : 1.84536
[1mStep[0m  [32/84], [94mLoss[0m : 2.16864
[1mStep[0m  [40/84], [94mLoss[0m : 1.92478
[1mStep[0m  [48/84], [94mLoss[0m : 2.20243
[1mStep[0m  [56/84], [94mLoss[0m : 1.98164
[1mStep[0m  [64/84], [94mLoss[0m : 2.31125
[1mStep[0m  [72/84], [94mLoss[0m : 2.07214
[1mStep[0m  [80/84], [94mLoss[0m : 2.02112

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82033
[1mStep[0m  [8/84], [94mLoss[0m : 2.07847
[1mStep[0m  [16/84], [94mLoss[0m : 1.97677
[1mStep[0m  [24/84], [94mLoss[0m : 2.03744
[1mStep[0m  [32/84], [94mLoss[0m : 2.13346
[1mStep[0m  [40/84], [94mLoss[0m : 2.12091
[1mStep[0m  [48/84], [94mLoss[0m : 1.85259
[1mStep[0m  [56/84], [94mLoss[0m : 1.98142
[1mStep[0m  [64/84], [94mLoss[0m : 2.11104
[1mStep[0m  [72/84], [94mLoss[0m : 1.98042
[1mStep[0m  [80/84], [94mLoss[0m : 2.03695

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.491957570825304
MAE score P1        2.36298
MAE score P2       2.491958
loss               1.965584
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.47781
[1mStep[0m  [4/42], [94mLoss[0m : 9.92532
[1mStep[0m  [8/42], [94mLoss[0m : 9.31695
[1mStep[0m  [12/42], [94mLoss[0m : 8.70842
[1mStep[0m  [16/42], [94mLoss[0m : 8.08448
[1mStep[0m  [20/42], [94mLoss[0m : 7.64478
[1mStep[0m  [24/42], [94mLoss[0m : 7.32975
[1mStep[0m  [28/42], [94mLoss[0m : 6.32920
[1mStep[0m  [32/42], [94mLoss[0m : 5.72639
[1mStep[0m  [36/42], [94mLoss[0m : 5.05006
[1mStep[0m  [40/42], [94mLoss[0m : 4.55608

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.351, [92mTest[0m: 10.758, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.70646
[1mStep[0m  [4/42], [94mLoss[0m : 3.82622
[1mStep[0m  [8/42], [94mLoss[0m : 3.68887
[1mStep[0m  [12/42], [94mLoss[0m : 3.15951
[1mStep[0m  [16/42], [94mLoss[0m : 3.15520
[1mStep[0m  [20/42], [94mLoss[0m : 3.15329
[1mStep[0m  [24/42], [94mLoss[0m : 3.14641
[1mStep[0m  [28/42], [94mLoss[0m : 2.81806
[1mStep[0m  [32/42], [94mLoss[0m : 2.77397
[1mStep[0m  [36/42], [94mLoss[0m : 2.54745
[1mStep[0m  [40/42], [94mLoss[0m : 2.60436

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.211, [92mTest[0m: 5.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51786
[1mStep[0m  [4/42], [94mLoss[0m : 2.71461
[1mStep[0m  [8/42], [94mLoss[0m : 2.64080
[1mStep[0m  [12/42], [94mLoss[0m : 2.61190
[1mStep[0m  [16/42], [94mLoss[0m : 2.74504
[1mStep[0m  [20/42], [94mLoss[0m : 2.83365
[1mStep[0m  [24/42], [94mLoss[0m : 2.69555
[1mStep[0m  [28/42], [94mLoss[0m : 2.69519
[1mStep[0m  [32/42], [94mLoss[0m : 2.75447
[1mStep[0m  [36/42], [94mLoss[0m : 2.60124
[1mStep[0m  [40/42], [94mLoss[0m : 2.51564

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.966, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70647
[1mStep[0m  [4/42], [94mLoss[0m : 2.48852
[1mStep[0m  [8/42], [94mLoss[0m : 2.52125
[1mStep[0m  [12/42], [94mLoss[0m : 2.57132
[1mStep[0m  [16/42], [94mLoss[0m : 2.31582
[1mStep[0m  [20/42], [94mLoss[0m : 2.74606
[1mStep[0m  [24/42], [94mLoss[0m : 2.60436
[1mStep[0m  [28/42], [94mLoss[0m : 2.46177
[1mStep[0m  [32/42], [94mLoss[0m : 2.42353
[1mStep[0m  [36/42], [94mLoss[0m : 2.45451
[1mStep[0m  [40/42], [94mLoss[0m : 2.58580

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.664, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63734
[1mStep[0m  [4/42], [94mLoss[0m : 2.40665
[1mStep[0m  [8/42], [94mLoss[0m : 2.81663
[1mStep[0m  [12/42], [94mLoss[0m : 2.55510
[1mStep[0m  [16/42], [94mLoss[0m : 2.43984
[1mStep[0m  [20/42], [94mLoss[0m : 2.58196
[1mStep[0m  [24/42], [94mLoss[0m : 2.40503
[1mStep[0m  [28/42], [94mLoss[0m : 2.47396
[1mStep[0m  [32/42], [94mLoss[0m : 2.60455
[1mStep[0m  [36/42], [94mLoss[0m : 2.57414
[1mStep[0m  [40/42], [94mLoss[0m : 2.45779

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56872
[1mStep[0m  [4/42], [94mLoss[0m : 2.46438
[1mStep[0m  [8/42], [94mLoss[0m : 2.57303
[1mStep[0m  [12/42], [94mLoss[0m : 2.33625
[1mStep[0m  [16/42], [94mLoss[0m : 2.61081
[1mStep[0m  [20/42], [94mLoss[0m : 2.68747
[1mStep[0m  [24/42], [94mLoss[0m : 2.50584
[1mStep[0m  [28/42], [94mLoss[0m : 2.46638
[1mStep[0m  [32/42], [94mLoss[0m : 2.53595
[1mStep[0m  [36/42], [94mLoss[0m : 2.60603
[1mStep[0m  [40/42], [94mLoss[0m : 2.32459

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80696
[1mStep[0m  [4/42], [94mLoss[0m : 2.80250
[1mStep[0m  [8/42], [94mLoss[0m : 2.54793
[1mStep[0m  [12/42], [94mLoss[0m : 2.68836
[1mStep[0m  [16/42], [94mLoss[0m : 2.49736
[1mStep[0m  [20/42], [94mLoss[0m : 2.54094
[1mStep[0m  [24/42], [94mLoss[0m : 2.44949
[1mStep[0m  [28/42], [94mLoss[0m : 2.56852
[1mStep[0m  [32/42], [94mLoss[0m : 2.46710
[1mStep[0m  [36/42], [94mLoss[0m : 2.44761
[1mStep[0m  [40/42], [94mLoss[0m : 2.51877

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66441
[1mStep[0m  [4/42], [94mLoss[0m : 2.48846
[1mStep[0m  [8/42], [94mLoss[0m : 2.34087
[1mStep[0m  [12/42], [94mLoss[0m : 2.68113
[1mStep[0m  [16/42], [94mLoss[0m : 2.45214
[1mStep[0m  [20/42], [94mLoss[0m : 2.46869
[1mStep[0m  [24/42], [94mLoss[0m : 2.60913
[1mStep[0m  [28/42], [94mLoss[0m : 2.64730
[1mStep[0m  [32/42], [94mLoss[0m : 2.46031
[1mStep[0m  [36/42], [94mLoss[0m : 2.52249
[1mStep[0m  [40/42], [94mLoss[0m : 2.53313

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60347
[1mStep[0m  [4/42], [94mLoss[0m : 2.51908
[1mStep[0m  [8/42], [94mLoss[0m : 2.43579
[1mStep[0m  [12/42], [94mLoss[0m : 2.55593
[1mStep[0m  [16/42], [94mLoss[0m : 2.48638
[1mStep[0m  [20/42], [94mLoss[0m : 2.56981
[1mStep[0m  [24/42], [94mLoss[0m : 2.61395
[1mStep[0m  [28/42], [94mLoss[0m : 2.42102
[1mStep[0m  [32/42], [94mLoss[0m : 2.33290
[1mStep[0m  [36/42], [94mLoss[0m : 2.45497
[1mStep[0m  [40/42], [94mLoss[0m : 2.65640

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52838
[1mStep[0m  [4/42], [94mLoss[0m : 2.36272
[1mStep[0m  [8/42], [94mLoss[0m : 2.35620
[1mStep[0m  [12/42], [94mLoss[0m : 2.71176
[1mStep[0m  [16/42], [94mLoss[0m : 2.53461
[1mStep[0m  [20/42], [94mLoss[0m : 2.64100
[1mStep[0m  [24/42], [94mLoss[0m : 2.49170
[1mStep[0m  [28/42], [94mLoss[0m : 2.54901
[1mStep[0m  [32/42], [94mLoss[0m : 2.39165
[1mStep[0m  [36/42], [94mLoss[0m : 2.76563
[1mStep[0m  [40/42], [94mLoss[0m : 2.63778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53721
[1mStep[0m  [4/42], [94mLoss[0m : 2.46084
[1mStep[0m  [8/42], [94mLoss[0m : 2.41000
[1mStep[0m  [12/42], [94mLoss[0m : 2.57388
[1mStep[0m  [16/42], [94mLoss[0m : 2.45999
[1mStep[0m  [20/42], [94mLoss[0m : 2.50204
[1mStep[0m  [24/42], [94mLoss[0m : 2.51187
[1mStep[0m  [28/42], [94mLoss[0m : 2.82205
[1mStep[0m  [32/42], [94mLoss[0m : 2.60395
[1mStep[0m  [36/42], [94mLoss[0m : 2.54295
[1mStep[0m  [40/42], [94mLoss[0m : 2.43298

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61131
[1mStep[0m  [4/42], [94mLoss[0m : 2.38904
[1mStep[0m  [8/42], [94mLoss[0m : 2.67909
[1mStep[0m  [12/42], [94mLoss[0m : 2.32425
[1mStep[0m  [16/42], [94mLoss[0m : 2.55836
[1mStep[0m  [20/42], [94mLoss[0m : 2.52774
[1mStep[0m  [24/42], [94mLoss[0m : 2.70793
[1mStep[0m  [28/42], [94mLoss[0m : 2.64807
[1mStep[0m  [32/42], [94mLoss[0m : 2.54040
[1mStep[0m  [36/42], [94mLoss[0m : 2.62163
[1mStep[0m  [40/42], [94mLoss[0m : 2.56111

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47965
[1mStep[0m  [4/42], [94mLoss[0m : 2.27704
[1mStep[0m  [8/42], [94mLoss[0m : 2.41363
[1mStep[0m  [12/42], [94mLoss[0m : 2.72963
[1mStep[0m  [16/42], [94mLoss[0m : 2.50890
[1mStep[0m  [20/42], [94mLoss[0m : 2.37075
[1mStep[0m  [24/42], [94mLoss[0m : 2.67439
[1mStep[0m  [28/42], [94mLoss[0m : 2.50995
[1mStep[0m  [32/42], [94mLoss[0m : 2.48691
[1mStep[0m  [36/42], [94mLoss[0m : 2.75529
[1mStep[0m  [40/42], [94mLoss[0m : 2.55369

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30239
[1mStep[0m  [4/42], [94mLoss[0m : 2.34029
[1mStep[0m  [8/42], [94mLoss[0m : 2.56352
[1mStep[0m  [12/42], [94mLoss[0m : 2.80265
[1mStep[0m  [16/42], [94mLoss[0m : 2.34905
[1mStep[0m  [20/42], [94mLoss[0m : 2.57003
[1mStep[0m  [24/42], [94mLoss[0m : 2.59762
[1mStep[0m  [28/42], [94mLoss[0m : 2.41417
[1mStep[0m  [32/42], [94mLoss[0m : 2.49893
[1mStep[0m  [36/42], [94mLoss[0m : 2.45682
[1mStep[0m  [40/42], [94mLoss[0m : 2.74339

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57111
[1mStep[0m  [4/42], [94mLoss[0m : 2.43044
[1mStep[0m  [8/42], [94mLoss[0m : 2.30656
[1mStep[0m  [12/42], [94mLoss[0m : 2.33485
[1mStep[0m  [16/42], [94mLoss[0m : 2.78207
[1mStep[0m  [20/42], [94mLoss[0m : 2.48104
[1mStep[0m  [24/42], [94mLoss[0m : 2.47311
[1mStep[0m  [28/42], [94mLoss[0m : 2.44517
[1mStep[0m  [32/42], [94mLoss[0m : 2.40866
[1mStep[0m  [36/42], [94mLoss[0m : 2.35413
[1mStep[0m  [40/42], [94mLoss[0m : 2.41975

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23758
[1mStep[0m  [4/42], [94mLoss[0m : 2.34593
[1mStep[0m  [8/42], [94mLoss[0m : 2.55671
[1mStep[0m  [12/42], [94mLoss[0m : 2.37798
[1mStep[0m  [16/42], [94mLoss[0m : 2.58584
[1mStep[0m  [20/42], [94mLoss[0m : 2.61723
[1mStep[0m  [24/42], [94mLoss[0m : 2.51138
[1mStep[0m  [28/42], [94mLoss[0m : 2.51969
[1mStep[0m  [32/42], [94mLoss[0m : 2.49063
[1mStep[0m  [36/42], [94mLoss[0m : 2.67817
[1mStep[0m  [40/42], [94mLoss[0m : 2.66028

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17224
[1mStep[0m  [4/42], [94mLoss[0m : 2.63313
[1mStep[0m  [8/42], [94mLoss[0m : 2.55958
[1mStep[0m  [12/42], [94mLoss[0m : 2.56044
[1mStep[0m  [16/42], [94mLoss[0m : 2.52183
[1mStep[0m  [20/42], [94mLoss[0m : 2.59742
[1mStep[0m  [24/42], [94mLoss[0m : 2.52413
[1mStep[0m  [28/42], [94mLoss[0m : 2.37531
[1mStep[0m  [32/42], [94mLoss[0m : 2.43243
[1mStep[0m  [36/42], [94mLoss[0m : 2.47490
[1mStep[0m  [40/42], [94mLoss[0m : 2.33867

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49546
[1mStep[0m  [4/42], [94mLoss[0m : 2.47225
[1mStep[0m  [8/42], [94mLoss[0m : 2.93869
[1mStep[0m  [12/42], [94mLoss[0m : 2.61092
[1mStep[0m  [16/42], [94mLoss[0m : 2.55917
[1mStep[0m  [20/42], [94mLoss[0m : 2.40948
[1mStep[0m  [24/42], [94mLoss[0m : 2.44195
[1mStep[0m  [28/42], [94mLoss[0m : 2.48138
[1mStep[0m  [32/42], [94mLoss[0m : 2.60311
[1mStep[0m  [36/42], [94mLoss[0m : 2.31488
[1mStep[0m  [40/42], [94mLoss[0m : 2.56411

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16652
[1mStep[0m  [4/42], [94mLoss[0m : 2.66484
[1mStep[0m  [8/42], [94mLoss[0m : 2.50240
[1mStep[0m  [12/42], [94mLoss[0m : 2.48345
[1mStep[0m  [16/42], [94mLoss[0m : 2.51141
[1mStep[0m  [20/42], [94mLoss[0m : 2.33488
[1mStep[0m  [24/42], [94mLoss[0m : 2.53966
[1mStep[0m  [28/42], [94mLoss[0m : 2.38132
[1mStep[0m  [32/42], [94mLoss[0m : 2.59017
[1mStep[0m  [36/42], [94mLoss[0m : 2.38285
[1mStep[0m  [40/42], [94mLoss[0m : 2.38475

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34612
[1mStep[0m  [4/42], [94mLoss[0m : 2.56533
[1mStep[0m  [8/42], [94mLoss[0m : 2.35645
[1mStep[0m  [12/42], [94mLoss[0m : 2.59842
[1mStep[0m  [16/42], [94mLoss[0m : 2.17416
[1mStep[0m  [20/42], [94mLoss[0m : 2.52782
[1mStep[0m  [24/42], [94mLoss[0m : 2.60584
[1mStep[0m  [28/42], [94mLoss[0m : 2.56117
[1mStep[0m  [32/42], [94mLoss[0m : 2.53811
[1mStep[0m  [36/42], [94mLoss[0m : 2.48714
[1mStep[0m  [40/42], [94mLoss[0m : 2.41582

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.385, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49477
[1mStep[0m  [4/42], [94mLoss[0m : 2.40718
[1mStep[0m  [8/42], [94mLoss[0m : 2.51864
[1mStep[0m  [12/42], [94mLoss[0m : 2.53169
[1mStep[0m  [16/42], [94mLoss[0m : 2.41681
[1mStep[0m  [20/42], [94mLoss[0m : 2.52430
[1mStep[0m  [24/42], [94mLoss[0m : 2.58763
[1mStep[0m  [28/42], [94mLoss[0m : 2.34385
[1mStep[0m  [32/42], [94mLoss[0m : 2.58232
[1mStep[0m  [36/42], [94mLoss[0m : 2.55699
[1mStep[0m  [40/42], [94mLoss[0m : 2.58098

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56810
[1mStep[0m  [4/42], [94mLoss[0m : 2.45718
[1mStep[0m  [8/42], [94mLoss[0m : 2.48621
[1mStep[0m  [12/42], [94mLoss[0m : 2.38022
[1mStep[0m  [16/42], [94mLoss[0m : 2.48935
[1mStep[0m  [20/42], [94mLoss[0m : 2.48784
[1mStep[0m  [24/42], [94mLoss[0m : 2.43970
[1mStep[0m  [28/42], [94mLoss[0m : 2.23350
[1mStep[0m  [32/42], [94mLoss[0m : 2.21207
[1mStep[0m  [36/42], [94mLoss[0m : 2.34787
[1mStep[0m  [40/42], [94mLoss[0m : 2.42716

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47135
[1mStep[0m  [4/42], [94mLoss[0m : 2.28466
[1mStep[0m  [8/42], [94mLoss[0m : 2.59972
[1mStep[0m  [12/42], [94mLoss[0m : 2.46944
[1mStep[0m  [16/42], [94mLoss[0m : 2.46736
[1mStep[0m  [20/42], [94mLoss[0m : 2.55876
[1mStep[0m  [24/42], [94mLoss[0m : 2.31908
[1mStep[0m  [28/42], [94mLoss[0m : 2.72214
[1mStep[0m  [32/42], [94mLoss[0m : 2.54141
[1mStep[0m  [36/42], [94mLoss[0m : 2.41826
[1mStep[0m  [40/42], [94mLoss[0m : 2.38056

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58931
[1mStep[0m  [4/42], [94mLoss[0m : 2.36798
[1mStep[0m  [8/42], [94mLoss[0m : 2.46565
[1mStep[0m  [12/42], [94mLoss[0m : 2.28385
[1mStep[0m  [16/42], [94mLoss[0m : 2.32724
[1mStep[0m  [20/42], [94mLoss[0m : 2.58878
[1mStep[0m  [24/42], [94mLoss[0m : 2.53256
[1mStep[0m  [28/42], [94mLoss[0m : 2.43289
[1mStep[0m  [32/42], [94mLoss[0m : 2.38956
[1mStep[0m  [36/42], [94mLoss[0m : 2.52298
[1mStep[0m  [40/42], [94mLoss[0m : 2.45777

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41926
[1mStep[0m  [4/42], [94mLoss[0m : 2.59278
[1mStep[0m  [8/42], [94mLoss[0m : 2.45612
[1mStep[0m  [12/42], [94mLoss[0m : 2.48352
[1mStep[0m  [16/42], [94mLoss[0m : 2.44045
[1mStep[0m  [20/42], [94mLoss[0m : 2.57920
[1mStep[0m  [24/42], [94mLoss[0m : 2.20929
[1mStep[0m  [28/42], [94mLoss[0m : 2.52722
[1mStep[0m  [32/42], [94mLoss[0m : 2.48116
[1mStep[0m  [36/42], [94mLoss[0m : 2.56623
[1mStep[0m  [40/42], [94mLoss[0m : 2.47723

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49766
[1mStep[0m  [4/42], [94mLoss[0m : 2.41963
[1mStep[0m  [8/42], [94mLoss[0m : 2.51070
[1mStep[0m  [12/42], [94mLoss[0m : 2.51237
[1mStep[0m  [16/42], [94mLoss[0m : 2.35424
[1mStep[0m  [20/42], [94mLoss[0m : 2.52318
[1mStep[0m  [24/42], [94mLoss[0m : 2.35758
[1mStep[0m  [28/42], [94mLoss[0m : 2.36115
[1mStep[0m  [32/42], [94mLoss[0m : 2.45544
[1mStep[0m  [36/42], [94mLoss[0m : 2.40476
[1mStep[0m  [40/42], [94mLoss[0m : 2.49252

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54830
[1mStep[0m  [4/42], [94mLoss[0m : 2.60676
[1mStep[0m  [8/42], [94mLoss[0m : 2.39025
[1mStep[0m  [12/42], [94mLoss[0m : 2.55125
[1mStep[0m  [16/42], [94mLoss[0m : 2.56316
[1mStep[0m  [20/42], [94mLoss[0m : 2.56771
[1mStep[0m  [24/42], [94mLoss[0m : 2.42766
[1mStep[0m  [28/42], [94mLoss[0m : 2.61563
[1mStep[0m  [32/42], [94mLoss[0m : 2.65497
[1mStep[0m  [36/42], [94mLoss[0m : 2.78691
[1mStep[0m  [40/42], [94mLoss[0m : 2.28254

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61511
[1mStep[0m  [4/42], [94mLoss[0m : 2.46157
[1mStep[0m  [8/42], [94mLoss[0m : 2.40144
[1mStep[0m  [12/42], [94mLoss[0m : 2.44199
[1mStep[0m  [16/42], [94mLoss[0m : 2.41896
[1mStep[0m  [20/42], [94mLoss[0m : 2.44489
[1mStep[0m  [24/42], [94mLoss[0m : 2.48463
[1mStep[0m  [28/42], [94mLoss[0m : 2.41609
[1mStep[0m  [32/42], [94mLoss[0m : 2.44849
[1mStep[0m  [36/42], [94mLoss[0m : 2.28745
[1mStep[0m  [40/42], [94mLoss[0m : 2.41867

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40120
[1mStep[0m  [4/42], [94mLoss[0m : 2.36033
[1mStep[0m  [8/42], [94mLoss[0m : 2.50223
[1mStep[0m  [12/42], [94mLoss[0m : 2.35951
[1mStep[0m  [16/42], [94mLoss[0m : 2.47031
[1mStep[0m  [20/42], [94mLoss[0m : 2.53667
[1mStep[0m  [24/42], [94mLoss[0m : 2.18477
[1mStep[0m  [28/42], [94mLoss[0m : 2.58765
[1mStep[0m  [32/42], [94mLoss[0m : 2.18625
[1mStep[0m  [36/42], [94mLoss[0m : 2.68656
[1mStep[0m  [40/42], [94mLoss[0m : 2.49659

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15951
[1mStep[0m  [4/42], [94mLoss[0m : 2.54334
[1mStep[0m  [8/42], [94mLoss[0m : 2.38618
[1mStep[0m  [12/42], [94mLoss[0m : 2.31791
[1mStep[0m  [16/42], [94mLoss[0m : 2.48782
[1mStep[0m  [20/42], [94mLoss[0m : 2.32643
[1mStep[0m  [24/42], [94mLoss[0m : 2.63674
[1mStep[0m  [28/42], [94mLoss[0m : 2.47726
[1mStep[0m  [32/42], [94mLoss[0m : 2.46130
[1mStep[0m  [36/42], [94mLoss[0m : 2.61536
[1mStep[0m  [40/42], [94mLoss[0m : 2.44121

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.355
====================================

Phase 1 - Evaluation MAE:  2.354653903416225
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.35612
[1mStep[0m  [4/42], [94mLoss[0m : 2.56614
[1mStep[0m  [8/42], [94mLoss[0m : 2.45296
[1mStep[0m  [12/42], [94mLoss[0m : 2.59207
[1mStep[0m  [16/42], [94mLoss[0m : 2.54725
[1mStep[0m  [20/42], [94mLoss[0m : 2.52272
[1mStep[0m  [24/42], [94mLoss[0m : 2.49590
[1mStep[0m  [28/42], [94mLoss[0m : 2.33778
[1mStep[0m  [32/42], [94mLoss[0m : 2.55839
[1mStep[0m  [36/42], [94mLoss[0m : 2.54657
[1mStep[0m  [40/42], [94mLoss[0m : 2.41258

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67080
[1mStep[0m  [4/42], [94mLoss[0m : 2.32746
[1mStep[0m  [8/42], [94mLoss[0m : 2.32174
[1mStep[0m  [12/42], [94mLoss[0m : 2.45159
[1mStep[0m  [16/42], [94mLoss[0m : 2.54903
[1mStep[0m  [20/42], [94mLoss[0m : 2.28862
[1mStep[0m  [24/42], [94mLoss[0m : 2.58206
[1mStep[0m  [28/42], [94mLoss[0m : 2.56358
[1mStep[0m  [32/42], [94mLoss[0m : 2.63580
[1mStep[0m  [36/42], [94mLoss[0m : 2.42854
[1mStep[0m  [40/42], [94mLoss[0m : 2.53211

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46942
[1mStep[0m  [4/42], [94mLoss[0m : 2.26671
[1mStep[0m  [8/42], [94mLoss[0m : 2.36143
[1mStep[0m  [12/42], [94mLoss[0m : 2.50590
[1mStep[0m  [16/42], [94mLoss[0m : 2.46724
[1mStep[0m  [20/42], [94mLoss[0m : 2.42798
[1mStep[0m  [24/42], [94mLoss[0m : 2.44011
[1mStep[0m  [28/42], [94mLoss[0m : 2.38216
[1mStep[0m  [32/42], [94mLoss[0m : 2.38050
[1mStep[0m  [36/42], [94mLoss[0m : 2.59012
[1mStep[0m  [40/42], [94mLoss[0m : 2.44413

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.610, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14344
[1mStep[0m  [4/42], [94mLoss[0m : 2.65565
[1mStep[0m  [8/42], [94mLoss[0m : 2.46470
[1mStep[0m  [12/42], [94mLoss[0m : 2.34573
[1mStep[0m  [16/42], [94mLoss[0m : 2.55207
[1mStep[0m  [20/42], [94mLoss[0m : 2.40797
[1mStep[0m  [24/42], [94mLoss[0m : 2.28391
[1mStep[0m  [28/42], [94mLoss[0m : 2.32504
[1mStep[0m  [32/42], [94mLoss[0m : 2.16821
[1mStep[0m  [36/42], [94mLoss[0m : 2.44899
[1mStep[0m  [40/42], [94mLoss[0m : 2.42542

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51936
[1mStep[0m  [4/42], [94mLoss[0m : 2.40859
[1mStep[0m  [8/42], [94mLoss[0m : 2.51818
[1mStep[0m  [12/42], [94mLoss[0m : 2.33506
[1mStep[0m  [16/42], [94mLoss[0m : 2.19788
[1mStep[0m  [20/42], [94mLoss[0m : 2.41043
[1mStep[0m  [24/42], [94mLoss[0m : 2.28305
[1mStep[0m  [28/42], [94mLoss[0m : 2.30693
[1mStep[0m  [32/42], [94mLoss[0m : 2.46734
[1mStep[0m  [36/42], [94mLoss[0m : 2.24744
[1mStep[0m  [40/42], [94mLoss[0m : 2.30964

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18750
[1mStep[0m  [4/42], [94mLoss[0m : 2.10913
[1mStep[0m  [8/42], [94mLoss[0m : 2.41586
[1mStep[0m  [12/42], [94mLoss[0m : 2.29647
[1mStep[0m  [16/42], [94mLoss[0m : 2.37897
[1mStep[0m  [20/42], [94mLoss[0m : 2.33950
[1mStep[0m  [24/42], [94mLoss[0m : 2.19943
[1mStep[0m  [28/42], [94mLoss[0m : 2.24975
[1mStep[0m  [32/42], [94mLoss[0m : 2.33931
[1mStep[0m  [36/42], [94mLoss[0m : 2.12501
[1mStep[0m  [40/42], [94mLoss[0m : 2.14536

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.582, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43658
[1mStep[0m  [4/42], [94mLoss[0m : 2.33122
[1mStep[0m  [8/42], [94mLoss[0m : 2.22465
[1mStep[0m  [12/42], [94mLoss[0m : 2.36731
[1mStep[0m  [16/42], [94mLoss[0m : 2.08607
[1mStep[0m  [20/42], [94mLoss[0m : 2.39526
[1mStep[0m  [24/42], [94mLoss[0m : 2.22390
[1mStep[0m  [28/42], [94mLoss[0m : 2.39675
[1mStep[0m  [32/42], [94mLoss[0m : 2.14707
[1mStep[0m  [36/42], [94mLoss[0m : 2.17712
[1mStep[0m  [40/42], [94mLoss[0m : 2.11799

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26811
[1mStep[0m  [4/42], [94mLoss[0m : 1.99525
[1mStep[0m  [8/42], [94mLoss[0m : 1.95659
[1mStep[0m  [12/42], [94mLoss[0m : 2.22971
[1mStep[0m  [16/42], [94mLoss[0m : 2.22103
[1mStep[0m  [20/42], [94mLoss[0m : 2.35061
[1mStep[0m  [24/42], [94mLoss[0m : 2.08655
[1mStep[0m  [28/42], [94mLoss[0m : 2.18030
[1mStep[0m  [32/42], [94mLoss[0m : 2.22721
[1mStep[0m  [36/42], [94mLoss[0m : 2.11120
[1mStep[0m  [40/42], [94mLoss[0m : 2.41238

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38715
[1mStep[0m  [4/42], [94mLoss[0m : 2.41475
[1mStep[0m  [8/42], [94mLoss[0m : 2.18251
[1mStep[0m  [12/42], [94mLoss[0m : 2.15332
[1mStep[0m  [16/42], [94mLoss[0m : 2.00304
[1mStep[0m  [20/42], [94mLoss[0m : 2.18181
[1mStep[0m  [24/42], [94mLoss[0m : 2.08165
[1mStep[0m  [28/42], [94mLoss[0m : 2.33957
[1mStep[0m  [32/42], [94mLoss[0m : 2.13389
[1mStep[0m  [36/42], [94mLoss[0m : 2.06075
[1mStep[0m  [40/42], [94mLoss[0m : 2.17157

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01998
[1mStep[0m  [4/42], [94mLoss[0m : 2.05948
[1mStep[0m  [8/42], [94mLoss[0m : 2.07251
[1mStep[0m  [12/42], [94mLoss[0m : 2.33287
[1mStep[0m  [16/42], [94mLoss[0m : 2.11856
[1mStep[0m  [20/42], [94mLoss[0m : 2.17188
[1mStep[0m  [24/42], [94mLoss[0m : 1.91923
[1mStep[0m  [28/42], [94mLoss[0m : 1.99469
[1mStep[0m  [32/42], [94mLoss[0m : 2.12640
[1mStep[0m  [36/42], [94mLoss[0m : 2.21874
[1mStep[0m  [40/42], [94mLoss[0m : 2.20843

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21320
[1mStep[0m  [4/42], [94mLoss[0m : 1.97131
[1mStep[0m  [8/42], [94mLoss[0m : 2.39650
[1mStep[0m  [12/42], [94mLoss[0m : 2.25497
[1mStep[0m  [16/42], [94mLoss[0m : 2.03894
[1mStep[0m  [20/42], [94mLoss[0m : 2.04601
[1mStep[0m  [24/42], [94mLoss[0m : 2.29395
[1mStep[0m  [28/42], [94mLoss[0m : 2.32048
[1mStep[0m  [32/42], [94mLoss[0m : 2.05049
[1mStep[0m  [36/42], [94mLoss[0m : 2.08665
[1mStep[0m  [40/42], [94mLoss[0m : 2.12609

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04270
[1mStep[0m  [4/42], [94mLoss[0m : 1.99228
[1mStep[0m  [8/42], [94mLoss[0m : 2.05898
[1mStep[0m  [12/42], [94mLoss[0m : 2.08442
[1mStep[0m  [16/42], [94mLoss[0m : 1.75226
[1mStep[0m  [20/42], [94mLoss[0m : 2.07218
[1mStep[0m  [24/42], [94mLoss[0m : 1.99407
[1mStep[0m  [28/42], [94mLoss[0m : 2.03630
[1mStep[0m  [32/42], [94mLoss[0m : 2.23861
[1mStep[0m  [36/42], [94mLoss[0m : 2.03002
[1mStep[0m  [40/42], [94mLoss[0m : 2.28783

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93059
[1mStep[0m  [4/42], [94mLoss[0m : 1.95634
[1mStep[0m  [8/42], [94mLoss[0m : 2.07098
[1mStep[0m  [12/42], [94mLoss[0m : 1.98881
[1mStep[0m  [16/42], [94mLoss[0m : 2.16078
[1mStep[0m  [20/42], [94mLoss[0m : 2.07582
[1mStep[0m  [24/42], [94mLoss[0m : 2.20894
[1mStep[0m  [28/42], [94mLoss[0m : 2.04154
[1mStep[0m  [32/42], [94mLoss[0m : 1.91978
[1mStep[0m  [36/42], [94mLoss[0m : 2.29375
[1mStep[0m  [40/42], [94mLoss[0m : 2.23083

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97521
[1mStep[0m  [4/42], [94mLoss[0m : 1.86082
[1mStep[0m  [8/42], [94mLoss[0m : 2.05108
[1mStep[0m  [12/42], [94mLoss[0m : 1.90491
[1mStep[0m  [16/42], [94mLoss[0m : 2.06079
[1mStep[0m  [20/42], [94mLoss[0m : 1.87705
[1mStep[0m  [24/42], [94mLoss[0m : 1.97563
[1mStep[0m  [28/42], [94mLoss[0m : 1.97298
[1mStep[0m  [32/42], [94mLoss[0m : 1.96443
[1mStep[0m  [36/42], [94mLoss[0m : 2.14310
[1mStep[0m  [40/42], [94mLoss[0m : 2.10153

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91219
[1mStep[0m  [4/42], [94mLoss[0m : 1.82206
[1mStep[0m  [8/42], [94mLoss[0m : 2.34535
[1mStep[0m  [12/42], [94mLoss[0m : 1.96250
[1mStep[0m  [16/42], [94mLoss[0m : 1.92643
[1mStep[0m  [20/42], [94mLoss[0m : 1.96271
[1mStep[0m  [24/42], [94mLoss[0m : 1.88205
[1mStep[0m  [28/42], [94mLoss[0m : 2.13440
[1mStep[0m  [32/42], [94mLoss[0m : 1.86382
[1mStep[0m  [36/42], [94mLoss[0m : 1.85989
[1mStep[0m  [40/42], [94mLoss[0m : 1.87521

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82294
[1mStep[0m  [4/42], [94mLoss[0m : 1.92575
[1mStep[0m  [8/42], [94mLoss[0m : 2.03216
[1mStep[0m  [12/42], [94mLoss[0m : 1.88975
[1mStep[0m  [16/42], [94mLoss[0m : 1.81000
[1mStep[0m  [20/42], [94mLoss[0m : 1.98672
[1mStep[0m  [24/42], [94mLoss[0m : 1.94834
[1mStep[0m  [28/42], [94mLoss[0m : 1.87019
[1mStep[0m  [32/42], [94mLoss[0m : 1.98449
[1mStep[0m  [36/42], [94mLoss[0m : 1.90387
[1mStep[0m  [40/42], [94mLoss[0m : 1.92616

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82645
[1mStep[0m  [4/42], [94mLoss[0m : 1.99228
[1mStep[0m  [8/42], [94mLoss[0m : 2.00944
[1mStep[0m  [12/42], [94mLoss[0m : 1.76596
[1mStep[0m  [16/42], [94mLoss[0m : 1.94147
[1mStep[0m  [20/42], [94mLoss[0m : 2.11619
[1mStep[0m  [24/42], [94mLoss[0m : 2.03449
[1mStep[0m  [28/42], [94mLoss[0m : 1.84995
[1mStep[0m  [32/42], [94mLoss[0m : 1.84254
[1mStep[0m  [36/42], [94mLoss[0m : 1.83436
[1mStep[0m  [40/42], [94mLoss[0m : 1.87568

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60291
[1mStep[0m  [4/42], [94mLoss[0m : 1.82125
[1mStep[0m  [8/42], [94mLoss[0m : 1.94305
[1mStep[0m  [12/42], [94mLoss[0m : 1.97115
[1mStep[0m  [16/42], [94mLoss[0m : 1.88287
[1mStep[0m  [20/42], [94mLoss[0m : 1.96308
[1mStep[0m  [24/42], [94mLoss[0m : 1.86160
[1mStep[0m  [28/42], [94mLoss[0m : 1.88195
[1mStep[0m  [32/42], [94mLoss[0m : 1.85277
[1mStep[0m  [36/42], [94mLoss[0m : 1.90347
[1mStep[0m  [40/42], [94mLoss[0m : 2.01039

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88068
[1mStep[0m  [4/42], [94mLoss[0m : 1.81458
[1mStep[0m  [8/42], [94mLoss[0m : 1.70925
[1mStep[0m  [12/42], [94mLoss[0m : 1.74040
[1mStep[0m  [16/42], [94mLoss[0m : 1.75024
[1mStep[0m  [20/42], [94mLoss[0m : 1.88001
[1mStep[0m  [24/42], [94mLoss[0m : 1.95161
[1mStep[0m  [28/42], [94mLoss[0m : 1.79307
[1mStep[0m  [32/42], [94mLoss[0m : 1.86042
[1mStep[0m  [36/42], [94mLoss[0m : 1.87783
[1mStep[0m  [40/42], [94mLoss[0m : 1.83370

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69379
[1mStep[0m  [4/42], [94mLoss[0m : 1.66829
[1mStep[0m  [8/42], [94mLoss[0m : 1.89152
[1mStep[0m  [12/42], [94mLoss[0m : 1.95881
[1mStep[0m  [16/42], [94mLoss[0m : 1.68779
[1mStep[0m  [20/42], [94mLoss[0m : 1.88403
[1mStep[0m  [24/42], [94mLoss[0m : 1.82642
[1mStep[0m  [28/42], [94mLoss[0m : 1.68721
[1mStep[0m  [32/42], [94mLoss[0m : 1.88523
[1mStep[0m  [36/42], [94mLoss[0m : 1.77917
[1mStep[0m  [40/42], [94mLoss[0m : 1.71706

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63040
[1mStep[0m  [4/42], [94mLoss[0m : 1.81788
[1mStep[0m  [8/42], [94mLoss[0m : 1.88289
[1mStep[0m  [12/42], [94mLoss[0m : 1.87035
[1mStep[0m  [16/42], [94mLoss[0m : 1.74549
[1mStep[0m  [20/42], [94mLoss[0m : 1.58303
[1mStep[0m  [24/42], [94mLoss[0m : 1.77066
[1mStep[0m  [28/42], [94mLoss[0m : 1.81300
[1mStep[0m  [32/42], [94mLoss[0m : 1.73425
[1mStep[0m  [36/42], [94mLoss[0m : 2.00125
[1mStep[0m  [40/42], [94mLoss[0m : 1.75759

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59287
[1mStep[0m  [4/42], [94mLoss[0m : 1.67631
[1mStep[0m  [8/42], [94mLoss[0m : 1.73582
[1mStep[0m  [12/42], [94mLoss[0m : 1.87123
[1mStep[0m  [16/42], [94mLoss[0m : 1.96389
[1mStep[0m  [20/42], [94mLoss[0m : 1.76852
[1mStep[0m  [24/42], [94mLoss[0m : 1.62613
[1mStep[0m  [28/42], [94mLoss[0m : 1.62223
[1mStep[0m  [32/42], [94mLoss[0m : 1.74367
[1mStep[0m  [36/42], [94mLoss[0m : 1.76538
[1mStep[0m  [40/42], [94mLoss[0m : 1.64541

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68392
[1mStep[0m  [4/42], [94mLoss[0m : 1.72246
[1mStep[0m  [8/42], [94mLoss[0m : 1.68695
[1mStep[0m  [12/42], [94mLoss[0m : 1.83103
[1mStep[0m  [16/42], [94mLoss[0m : 1.76676
[1mStep[0m  [20/42], [94mLoss[0m : 1.56721
[1mStep[0m  [24/42], [94mLoss[0m : 1.71167
[1mStep[0m  [28/42], [94mLoss[0m : 1.43926
[1mStep[0m  [32/42], [94mLoss[0m : 1.69501
[1mStep[0m  [36/42], [94mLoss[0m : 1.56957
[1mStep[0m  [40/42], [94mLoss[0m : 1.84603

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65555
[1mStep[0m  [4/42], [94mLoss[0m : 1.80775
[1mStep[0m  [8/42], [94mLoss[0m : 1.68143
[1mStep[0m  [12/42], [94mLoss[0m : 1.64381
[1mStep[0m  [16/42], [94mLoss[0m : 1.77508
[1mStep[0m  [20/42], [94mLoss[0m : 1.54612
[1mStep[0m  [24/42], [94mLoss[0m : 1.72637
[1mStep[0m  [28/42], [94mLoss[0m : 1.60659
[1mStep[0m  [32/42], [94mLoss[0m : 1.58504
[1mStep[0m  [36/42], [94mLoss[0m : 1.73605
[1mStep[0m  [40/42], [94mLoss[0m : 1.89356

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56313
[1mStep[0m  [4/42], [94mLoss[0m : 1.53583
[1mStep[0m  [8/42], [94mLoss[0m : 1.64082
[1mStep[0m  [12/42], [94mLoss[0m : 1.50566
[1mStep[0m  [16/42], [94mLoss[0m : 1.62160
[1mStep[0m  [20/42], [94mLoss[0m : 1.94217
[1mStep[0m  [24/42], [94mLoss[0m : 1.72489
[1mStep[0m  [28/42], [94mLoss[0m : 1.78049
[1mStep[0m  [32/42], [94mLoss[0m : 1.65186
[1mStep[0m  [36/42], [94mLoss[0m : 1.74522
[1mStep[0m  [40/42], [94mLoss[0m : 1.62742

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77677
[1mStep[0m  [4/42], [94mLoss[0m : 1.63973
[1mStep[0m  [8/42], [94mLoss[0m : 1.87238
[1mStep[0m  [12/42], [94mLoss[0m : 1.62193
[1mStep[0m  [16/42], [94mLoss[0m : 1.63975
[1mStep[0m  [20/42], [94mLoss[0m : 1.58597
[1mStep[0m  [24/42], [94mLoss[0m : 1.65609
[1mStep[0m  [28/42], [94mLoss[0m : 1.68002
[1mStep[0m  [32/42], [94mLoss[0m : 1.60846
[1mStep[0m  [36/42], [94mLoss[0m : 1.55539
[1mStep[0m  [40/42], [94mLoss[0m : 1.82645

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56065
[1mStep[0m  [4/42], [94mLoss[0m : 1.63294
[1mStep[0m  [8/42], [94mLoss[0m : 1.66646
[1mStep[0m  [12/42], [94mLoss[0m : 1.74798
[1mStep[0m  [16/42], [94mLoss[0m : 1.64387
[1mStep[0m  [20/42], [94mLoss[0m : 1.66218
[1mStep[0m  [24/42], [94mLoss[0m : 1.47497
[1mStep[0m  [28/42], [94mLoss[0m : 1.75728
[1mStep[0m  [32/42], [94mLoss[0m : 1.72332
[1mStep[0m  [36/42], [94mLoss[0m : 1.66078
[1mStep[0m  [40/42], [94mLoss[0m : 1.63404

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49371
[1mStep[0m  [4/42], [94mLoss[0m : 1.47853
[1mStep[0m  [8/42], [94mLoss[0m : 1.64744
[1mStep[0m  [12/42], [94mLoss[0m : 1.61824
[1mStep[0m  [16/42], [94mLoss[0m : 1.62602
[1mStep[0m  [20/42], [94mLoss[0m : 1.60177
[1mStep[0m  [24/42], [94mLoss[0m : 1.49000
[1mStep[0m  [28/42], [94mLoss[0m : 1.53224
[1mStep[0m  [32/42], [94mLoss[0m : 1.62399
[1mStep[0m  [36/42], [94mLoss[0m : 1.70049
[1mStep[0m  [40/42], [94mLoss[0m : 1.74123

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48562
[1mStep[0m  [4/42], [94mLoss[0m : 1.59671
[1mStep[0m  [8/42], [94mLoss[0m : 1.55844
[1mStep[0m  [12/42], [94mLoss[0m : 1.54745
[1mStep[0m  [16/42], [94mLoss[0m : 1.63815
[1mStep[0m  [20/42], [94mLoss[0m : 1.58104
[1mStep[0m  [24/42], [94mLoss[0m : 1.58743
[1mStep[0m  [28/42], [94mLoss[0m : 1.51622
[1mStep[0m  [32/42], [94mLoss[0m : 1.50877
[1mStep[0m  [36/42], [94mLoss[0m : 1.57639
[1mStep[0m  [40/42], [94mLoss[0m : 1.71231

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64996
[1mStep[0m  [4/42], [94mLoss[0m : 1.55072
[1mStep[0m  [8/42], [94mLoss[0m : 1.65741
[1mStep[0m  [12/42], [94mLoss[0m : 1.52206
[1mStep[0m  [16/42], [94mLoss[0m : 1.61264
[1mStep[0m  [20/42], [94mLoss[0m : 1.61445
[1mStep[0m  [24/42], [94mLoss[0m : 1.73361
[1mStep[0m  [28/42], [94mLoss[0m : 1.63706
[1mStep[0m  [32/42], [94mLoss[0m : 1.67299
[1mStep[0m  [36/42], [94mLoss[0m : 1.48790
[1mStep[0m  [40/42], [94mLoss[0m : 1.63823

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.498
====================================

Phase 2 - Evaluation MAE:  2.4984017269951955
MAE score P1      2.354654
MAE score P2      2.498402
loss              1.585356
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.93091
[1mStep[0m  [4/42], [94mLoss[0m : 10.79083
[1mStep[0m  [8/42], [94mLoss[0m : 11.04601
[1mStep[0m  [12/42], [94mLoss[0m : 10.66650
[1mStep[0m  [16/42], [94mLoss[0m : 10.94586
[1mStep[0m  [20/42], [94mLoss[0m : 10.80836
[1mStep[0m  [24/42], [94mLoss[0m : 10.34194
[1mStep[0m  [28/42], [94mLoss[0m : 10.61711
[1mStep[0m  [32/42], [94mLoss[0m : 10.50688
[1mStep[0m  [36/42], [94mLoss[0m : 10.42596
[1mStep[0m  [40/42], [94mLoss[0m : 10.19607

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.670, [92mTest[0m: 10.919, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.88948
[1mStep[0m  [4/42], [94mLoss[0m : 10.17558
[1mStep[0m  [8/42], [94mLoss[0m : 10.14720
[1mStep[0m  [12/42], [94mLoss[0m : 9.99793
[1mStep[0m  [16/42], [94mLoss[0m : 9.96381
[1mStep[0m  [20/42], [94mLoss[0m : 9.89360
[1mStep[0m  [24/42], [94mLoss[0m : 9.62426
[1mStep[0m  [28/42], [94mLoss[0m : 9.69004
[1mStep[0m  [32/42], [94mLoss[0m : 9.68426
[1mStep[0m  [36/42], [94mLoss[0m : 9.35982
[1mStep[0m  [40/42], [94mLoss[0m : 9.32254

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.910, [92mTest[0m: 10.163, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.34551
[1mStep[0m  [4/42], [94mLoss[0m : 9.52645
[1mStep[0m  [8/42], [94mLoss[0m : 9.41384
[1mStep[0m  [12/42], [94mLoss[0m : 9.32823
[1mStep[0m  [16/42], [94mLoss[0m : 9.00546
[1mStep[0m  [20/42], [94mLoss[0m : 8.89417
[1mStep[0m  [24/42], [94mLoss[0m : 8.78342
[1mStep[0m  [28/42], [94mLoss[0m : 8.69101
[1mStep[0m  [32/42], [94mLoss[0m : 8.70384
[1mStep[0m  [36/42], [94mLoss[0m : 8.90677
[1mStep[0m  [40/42], [94mLoss[0m : 8.98551

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.971, [92mTest[0m: 9.087, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.55811
[1mStep[0m  [4/42], [94mLoss[0m : 8.18617
[1mStep[0m  [8/42], [94mLoss[0m : 8.30663
[1mStep[0m  [12/42], [94mLoss[0m : 8.29795
[1mStep[0m  [16/42], [94mLoss[0m : 8.65722
[1mStep[0m  [20/42], [94mLoss[0m : 8.16166
[1mStep[0m  [24/42], [94mLoss[0m : 7.63235
[1mStep[0m  [28/42], [94mLoss[0m : 7.74131
[1mStep[0m  [32/42], [94mLoss[0m : 7.69621
[1mStep[0m  [36/42], [94mLoss[0m : 7.87952
[1mStep[0m  [40/42], [94mLoss[0m : 7.74813

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.941, [92mTest[0m: 7.860, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.11344
[1mStep[0m  [4/42], [94mLoss[0m : 7.44746
[1mStep[0m  [8/42], [94mLoss[0m : 7.23351
[1mStep[0m  [12/42], [94mLoss[0m : 7.23762
[1mStep[0m  [16/42], [94mLoss[0m : 7.11111
[1mStep[0m  [20/42], [94mLoss[0m : 7.01068
[1mStep[0m  [24/42], [94mLoss[0m : 7.30999
[1mStep[0m  [28/42], [94mLoss[0m : 7.11439
[1mStep[0m  [32/42], [94mLoss[0m : 6.71246
[1mStep[0m  [36/42], [94mLoss[0m : 6.69339
[1mStep[0m  [40/42], [94mLoss[0m : 6.76725

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.046, [92mTest[0m: 6.696, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.49230
[1mStep[0m  [4/42], [94mLoss[0m : 6.42618
[1mStep[0m  [8/42], [94mLoss[0m : 6.65777
[1mStep[0m  [12/42], [94mLoss[0m : 6.24174
[1mStep[0m  [16/42], [94mLoss[0m : 6.39160
[1mStep[0m  [20/42], [94mLoss[0m : 6.29783
[1mStep[0m  [24/42], [94mLoss[0m : 6.36893
[1mStep[0m  [28/42], [94mLoss[0m : 6.52136
[1mStep[0m  [32/42], [94mLoss[0m : 6.25175
[1mStep[0m  [36/42], [94mLoss[0m : 5.94691
[1mStep[0m  [40/42], [94mLoss[0m : 5.95097

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.295, [92mTest[0m: 5.771, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.82248
[1mStep[0m  [4/42], [94mLoss[0m : 5.97190
[1mStep[0m  [8/42], [94mLoss[0m : 5.76185
[1mStep[0m  [12/42], [94mLoss[0m : 5.54331
[1mStep[0m  [16/42], [94mLoss[0m : 5.89751
[1mStep[0m  [20/42], [94mLoss[0m : 5.67199
[1mStep[0m  [24/42], [94mLoss[0m : 5.77402
[1mStep[0m  [28/42], [94mLoss[0m : 5.45314
[1mStep[0m  [32/42], [94mLoss[0m : 5.11383
[1mStep[0m  [36/42], [94mLoss[0m : 4.84894
[1mStep[0m  [40/42], [94mLoss[0m : 5.21361

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.492, [92mTest[0m: 4.873, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.11817
[1mStep[0m  [4/42], [94mLoss[0m : 4.96840
[1mStep[0m  [8/42], [94mLoss[0m : 4.57969
[1mStep[0m  [12/42], [94mLoss[0m : 4.90096
[1mStep[0m  [16/42], [94mLoss[0m : 4.74884
[1mStep[0m  [20/42], [94mLoss[0m : 4.32297
[1mStep[0m  [24/42], [94mLoss[0m : 4.63042
[1mStep[0m  [28/42], [94mLoss[0m : 4.68737
[1mStep[0m  [32/42], [94mLoss[0m : 4.73147
[1mStep[0m  [36/42], [94mLoss[0m : 4.20692
[1mStep[0m  [40/42], [94mLoss[0m : 4.03612

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.607, [92mTest[0m: 4.020, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.17944
[1mStep[0m  [4/42], [94mLoss[0m : 3.79016
[1mStep[0m  [8/42], [94mLoss[0m : 4.18986
[1mStep[0m  [12/42], [94mLoss[0m : 3.83481
[1mStep[0m  [16/42], [94mLoss[0m : 3.71586
[1mStep[0m  [20/42], [94mLoss[0m : 3.98639
[1mStep[0m  [24/42], [94mLoss[0m : 4.15531
[1mStep[0m  [28/42], [94mLoss[0m : 3.82883
[1mStep[0m  [32/42], [94mLoss[0m : 3.32559
[1mStep[0m  [36/42], [94mLoss[0m : 3.55434
[1mStep[0m  [40/42], [94mLoss[0m : 3.33214

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.756, [92mTest[0m: 3.222, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.32616
[1mStep[0m  [4/42], [94mLoss[0m : 3.03728
[1mStep[0m  [8/42], [94mLoss[0m : 3.11397
[1mStep[0m  [12/42], [94mLoss[0m : 3.26159
[1mStep[0m  [16/42], [94mLoss[0m : 3.10390
[1mStep[0m  [20/42], [94mLoss[0m : 3.12938
[1mStep[0m  [24/42], [94mLoss[0m : 3.39681
[1mStep[0m  [28/42], [94mLoss[0m : 3.24304
[1mStep[0m  [32/42], [94mLoss[0m : 2.96520
[1mStep[0m  [36/42], [94mLoss[0m : 2.89484
[1mStep[0m  [40/42], [94mLoss[0m : 3.07983

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.095, [92mTest[0m: 2.617, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86092
[1mStep[0m  [4/42], [94mLoss[0m : 3.08819
[1mStep[0m  [8/42], [94mLoss[0m : 2.95125
[1mStep[0m  [12/42], [94mLoss[0m : 2.92254
[1mStep[0m  [16/42], [94mLoss[0m : 2.86919
[1mStep[0m  [20/42], [94mLoss[0m : 3.04012
[1mStep[0m  [24/42], [94mLoss[0m : 2.87747
[1mStep[0m  [28/42], [94mLoss[0m : 2.84543
[1mStep[0m  [32/42], [94mLoss[0m : 2.70327
[1mStep[0m  [36/42], [94mLoss[0m : 2.97928
[1mStep[0m  [40/42], [94mLoss[0m : 2.85847

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.843, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56583
[1mStep[0m  [4/42], [94mLoss[0m : 2.96747
[1mStep[0m  [8/42], [94mLoss[0m : 2.78876
[1mStep[0m  [12/42], [94mLoss[0m : 2.70774
[1mStep[0m  [16/42], [94mLoss[0m : 2.64728
[1mStep[0m  [20/42], [94mLoss[0m : 2.79701
[1mStep[0m  [24/42], [94mLoss[0m : 2.92584
[1mStep[0m  [28/42], [94mLoss[0m : 3.02970
[1mStep[0m  [32/42], [94mLoss[0m : 2.71114
[1mStep[0m  [36/42], [94mLoss[0m : 2.81771
[1mStep[0m  [40/42], [94mLoss[0m : 2.83311

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.770, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79422
[1mStep[0m  [4/42], [94mLoss[0m : 2.68397
[1mStep[0m  [8/42], [94mLoss[0m : 2.78738
[1mStep[0m  [12/42], [94mLoss[0m : 2.79280
[1mStep[0m  [16/42], [94mLoss[0m : 2.63277
[1mStep[0m  [20/42], [94mLoss[0m : 2.77594
[1mStep[0m  [24/42], [94mLoss[0m : 2.58749
[1mStep[0m  [28/42], [94mLoss[0m : 2.74701
[1mStep[0m  [32/42], [94mLoss[0m : 2.58811
[1mStep[0m  [36/42], [94mLoss[0m : 2.61273
[1mStep[0m  [40/42], [94mLoss[0m : 2.96474

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79509
[1mStep[0m  [4/42], [94mLoss[0m : 2.69632
[1mStep[0m  [8/42], [94mLoss[0m : 2.83760
[1mStep[0m  [12/42], [94mLoss[0m : 2.96076
[1mStep[0m  [16/42], [94mLoss[0m : 2.52450
[1mStep[0m  [20/42], [94mLoss[0m : 2.89121
[1mStep[0m  [24/42], [94mLoss[0m : 2.71425
[1mStep[0m  [28/42], [94mLoss[0m : 2.55686
[1mStep[0m  [32/42], [94mLoss[0m : 2.79332
[1mStep[0m  [36/42], [94mLoss[0m : 2.68545
[1mStep[0m  [40/42], [94mLoss[0m : 2.50397

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63107
[1mStep[0m  [4/42], [94mLoss[0m : 2.52055
[1mStep[0m  [8/42], [94mLoss[0m : 2.53346
[1mStep[0m  [12/42], [94mLoss[0m : 2.93789
[1mStep[0m  [16/42], [94mLoss[0m : 2.56920
[1mStep[0m  [20/42], [94mLoss[0m : 2.73332
[1mStep[0m  [24/42], [94mLoss[0m : 2.76818
[1mStep[0m  [28/42], [94mLoss[0m : 2.81246
[1mStep[0m  [32/42], [94mLoss[0m : 2.69142
[1mStep[0m  [36/42], [94mLoss[0m : 2.74753
[1mStep[0m  [40/42], [94mLoss[0m : 2.74447

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86138
[1mStep[0m  [4/42], [94mLoss[0m : 2.81221
[1mStep[0m  [8/42], [94mLoss[0m : 2.47537
[1mStep[0m  [12/42], [94mLoss[0m : 2.57622
[1mStep[0m  [16/42], [94mLoss[0m : 2.44717
[1mStep[0m  [20/42], [94mLoss[0m : 2.99019
[1mStep[0m  [24/42], [94mLoss[0m : 2.92617
[1mStep[0m  [28/42], [94mLoss[0m : 2.79202
[1mStep[0m  [32/42], [94mLoss[0m : 2.44658
[1mStep[0m  [36/42], [94mLoss[0m : 2.57831
[1mStep[0m  [40/42], [94mLoss[0m : 2.65430

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42846
[1mStep[0m  [4/42], [94mLoss[0m : 2.56936
[1mStep[0m  [8/42], [94mLoss[0m : 2.45848
[1mStep[0m  [12/42], [94mLoss[0m : 2.59571
[1mStep[0m  [16/42], [94mLoss[0m : 2.95579
[1mStep[0m  [20/42], [94mLoss[0m : 2.67699
[1mStep[0m  [24/42], [94mLoss[0m : 2.69611
[1mStep[0m  [28/42], [94mLoss[0m : 2.57723
[1mStep[0m  [32/42], [94mLoss[0m : 2.91019
[1mStep[0m  [36/42], [94mLoss[0m : 2.59128
[1mStep[0m  [40/42], [94mLoss[0m : 2.76613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77801
[1mStep[0m  [4/42], [94mLoss[0m : 2.63521
[1mStep[0m  [8/42], [94mLoss[0m : 2.60868
[1mStep[0m  [12/42], [94mLoss[0m : 2.61451
[1mStep[0m  [16/42], [94mLoss[0m : 2.84158
[1mStep[0m  [20/42], [94mLoss[0m : 2.69093
[1mStep[0m  [24/42], [94mLoss[0m : 2.67900
[1mStep[0m  [28/42], [94mLoss[0m : 2.78422
[1mStep[0m  [32/42], [94mLoss[0m : 2.94025
[1mStep[0m  [36/42], [94mLoss[0m : 2.67575
[1mStep[0m  [40/42], [94mLoss[0m : 2.52082

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69575
[1mStep[0m  [4/42], [94mLoss[0m : 2.68157
[1mStep[0m  [8/42], [94mLoss[0m : 2.57977
[1mStep[0m  [12/42], [94mLoss[0m : 2.74847
[1mStep[0m  [16/42], [94mLoss[0m : 2.52197
[1mStep[0m  [20/42], [94mLoss[0m : 2.71228
[1mStep[0m  [24/42], [94mLoss[0m : 2.81650
[1mStep[0m  [28/42], [94mLoss[0m : 2.74665
[1mStep[0m  [32/42], [94mLoss[0m : 2.52118
[1mStep[0m  [36/42], [94mLoss[0m : 2.73205
[1mStep[0m  [40/42], [94mLoss[0m : 2.67606

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73222
[1mStep[0m  [4/42], [94mLoss[0m : 2.75911
[1mStep[0m  [8/42], [94mLoss[0m : 2.69422
[1mStep[0m  [12/42], [94mLoss[0m : 2.59216
[1mStep[0m  [16/42], [94mLoss[0m : 2.90706
[1mStep[0m  [20/42], [94mLoss[0m : 2.84829
[1mStep[0m  [24/42], [94mLoss[0m : 2.51466
[1mStep[0m  [28/42], [94mLoss[0m : 2.87159
[1mStep[0m  [32/42], [94mLoss[0m : 2.82246
[1mStep[0m  [36/42], [94mLoss[0m : 2.69601
[1mStep[0m  [40/42], [94mLoss[0m : 2.77521

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.367, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63054
[1mStep[0m  [4/42], [94mLoss[0m : 2.49792
[1mStep[0m  [8/42], [94mLoss[0m : 2.51551
[1mStep[0m  [12/42], [94mLoss[0m : 2.72844
[1mStep[0m  [16/42], [94mLoss[0m : 2.67021
[1mStep[0m  [20/42], [94mLoss[0m : 2.68016
[1mStep[0m  [24/42], [94mLoss[0m : 2.51750
[1mStep[0m  [28/42], [94mLoss[0m : 2.87377
[1mStep[0m  [32/42], [94mLoss[0m : 2.60952
[1mStep[0m  [36/42], [94mLoss[0m : 2.63542
[1mStep[0m  [40/42], [94mLoss[0m : 2.83162

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54799
[1mStep[0m  [4/42], [94mLoss[0m : 2.64170
[1mStep[0m  [8/42], [94mLoss[0m : 2.65041
[1mStep[0m  [12/42], [94mLoss[0m : 2.71482
[1mStep[0m  [16/42], [94mLoss[0m : 2.72316
[1mStep[0m  [20/42], [94mLoss[0m : 2.71892
[1mStep[0m  [24/42], [94mLoss[0m : 2.71664
[1mStep[0m  [28/42], [94mLoss[0m : 2.67038
[1mStep[0m  [32/42], [94mLoss[0m : 2.59888
[1mStep[0m  [36/42], [94mLoss[0m : 2.59899
[1mStep[0m  [40/42], [94mLoss[0m : 2.60436

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79863
[1mStep[0m  [4/42], [94mLoss[0m : 2.67566
[1mStep[0m  [8/42], [94mLoss[0m : 2.68989
[1mStep[0m  [12/42], [94mLoss[0m : 2.53930
[1mStep[0m  [16/42], [94mLoss[0m : 2.43376
[1mStep[0m  [20/42], [94mLoss[0m : 2.78663
[1mStep[0m  [24/42], [94mLoss[0m : 2.54421
[1mStep[0m  [28/42], [94mLoss[0m : 2.63578
[1mStep[0m  [32/42], [94mLoss[0m : 2.57614
[1mStep[0m  [36/42], [94mLoss[0m : 2.65895
[1mStep[0m  [40/42], [94mLoss[0m : 2.55656

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55108
[1mStep[0m  [4/42], [94mLoss[0m : 2.67401
[1mStep[0m  [8/42], [94mLoss[0m : 2.57319
[1mStep[0m  [12/42], [94mLoss[0m : 2.33277
[1mStep[0m  [16/42], [94mLoss[0m : 2.41795
[1mStep[0m  [20/42], [94mLoss[0m : 2.56426
[1mStep[0m  [24/42], [94mLoss[0m : 2.53757
[1mStep[0m  [28/42], [94mLoss[0m : 2.75894
[1mStep[0m  [32/42], [94mLoss[0m : 2.41311
[1mStep[0m  [36/42], [94mLoss[0m : 3.04447
[1mStep[0m  [40/42], [94mLoss[0m : 2.52796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.357, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62002
[1mStep[0m  [4/42], [94mLoss[0m : 2.61451
[1mStep[0m  [8/42], [94mLoss[0m : 2.61145
[1mStep[0m  [12/42], [94mLoss[0m : 2.27534
[1mStep[0m  [16/42], [94mLoss[0m : 2.80681
[1mStep[0m  [20/42], [94mLoss[0m : 2.39276
[1mStep[0m  [24/42], [94mLoss[0m : 2.64214
[1mStep[0m  [28/42], [94mLoss[0m : 2.64877
[1mStep[0m  [32/42], [94mLoss[0m : 2.67327
[1mStep[0m  [36/42], [94mLoss[0m : 2.87347
[1mStep[0m  [40/42], [94mLoss[0m : 2.68768

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47535
[1mStep[0m  [4/42], [94mLoss[0m : 2.54179
[1mStep[0m  [8/42], [94mLoss[0m : 2.53187
[1mStep[0m  [12/42], [94mLoss[0m : 2.62575
[1mStep[0m  [16/42], [94mLoss[0m : 2.55253
[1mStep[0m  [20/42], [94mLoss[0m : 2.83182
[1mStep[0m  [24/42], [94mLoss[0m : 2.66851
[1mStep[0m  [28/42], [94mLoss[0m : 2.80588
[1mStep[0m  [32/42], [94mLoss[0m : 2.74641
[1mStep[0m  [36/42], [94mLoss[0m : 2.73486
[1mStep[0m  [40/42], [94mLoss[0m : 2.66016

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34620
[1mStep[0m  [4/42], [94mLoss[0m : 2.63962
[1mStep[0m  [8/42], [94mLoss[0m : 2.53465
[1mStep[0m  [12/42], [94mLoss[0m : 2.65795
[1mStep[0m  [16/42], [94mLoss[0m : 2.72767
[1mStep[0m  [20/42], [94mLoss[0m : 2.42670
[1mStep[0m  [24/42], [94mLoss[0m : 2.46739
[1mStep[0m  [28/42], [94mLoss[0m : 2.61656
[1mStep[0m  [32/42], [94mLoss[0m : 2.73914
[1mStep[0m  [36/42], [94mLoss[0m : 2.63678
[1mStep[0m  [40/42], [94mLoss[0m : 2.67671

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55168
[1mStep[0m  [4/42], [94mLoss[0m : 2.63034
[1mStep[0m  [8/42], [94mLoss[0m : 2.50411
[1mStep[0m  [12/42], [94mLoss[0m : 2.62648
[1mStep[0m  [16/42], [94mLoss[0m : 2.60179
[1mStep[0m  [20/42], [94mLoss[0m : 2.78647
[1mStep[0m  [24/42], [94mLoss[0m : 2.92658
[1mStep[0m  [28/42], [94mLoss[0m : 2.66717
[1mStep[0m  [32/42], [94mLoss[0m : 2.75803
[1mStep[0m  [36/42], [94mLoss[0m : 2.52766
[1mStep[0m  [40/42], [94mLoss[0m : 2.35892

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.369, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40924
[1mStep[0m  [4/42], [94mLoss[0m : 2.46097
[1mStep[0m  [8/42], [94mLoss[0m : 2.63171
[1mStep[0m  [12/42], [94mLoss[0m : 2.75984
[1mStep[0m  [16/42], [94mLoss[0m : 2.50871
[1mStep[0m  [20/42], [94mLoss[0m : 2.62679
[1mStep[0m  [24/42], [94mLoss[0m : 2.73252
[1mStep[0m  [28/42], [94mLoss[0m : 2.58474
[1mStep[0m  [32/42], [94mLoss[0m : 2.54384
[1mStep[0m  [36/42], [94mLoss[0m : 2.77583
[1mStep[0m  [40/42], [94mLoss[0m : 2.61849

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51097
[1mStep[0m  [4/42], [94mLoss[0m : 2.61678
[1mStep[0m  [8/42], [94mLoss[0m : 2.88448
[1mStep[0m  [12/42], [94mLoss[0m : 2.71395
[1mStep[0m  [16/42], [94mLoss[0m : 2.47119
[1mStep[0m  [20/42], [94mLoss[0m : 2.50689
[1mStep[0m  [24/42], [94mLoss[0m : 2.44736
[1mStep[0m  [28/42], [94mLoss[0m : 2.37931
[1mStep[0m  [32/42], [94mLoss[0m : 2.41019
[1mStep[0m  [36/42], [94mLoss[0m : 2.47440
[1mStep[0m  [40/42], [94mLoss[0m : 2.33536

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.357
====================================

Phase 1 - Evaluation MAE:  2.356877940041678
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.72427
[1mStep[0m  [4/42], [94mLoss[0m : 2.69106
[1mStep[0m  [8/42], [94mLoss[0m : 2.30819
[1mStep[0m  [12/42], [94mLoss[0m : 2.83569
[1mStep[0m  [16/42], [94mLoss[0m : 2.63004
[1mStep[0m  [20/42], [94mLoss[0m : 2.57912
[1mStep[0m  [24/42], [94mLoss[0m : 2.60502
[1mStep[0m  [28/42], [94mLoss[0m : 2.60030
[1mStep[0m  [32/42], [94mLoss[0m : 2.48073
[1mStep[0m  [36/42], [94mLoss[0m : 2.72356
[1mStep[0m  [40/42], [94mLoss[0m : 2.50703

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62079
[1mStep[0m  [4/42], [94mLoss[0m : 2.63401
[1mStep[0m  [8/42], [94mLoss[0m : 2.67815
[1mStep[0m  [12/42], [94mLoss[0m : 2.68216
[1mStep[0m  [16/42], [94mLoss[0m : 2.63450
[1mStep[0m  [20/42], [94mLoss[0m : 2.59993
[1mStep[0m  [24/42], [94mLoss[0m : 2.90341
[1mStep[0m  [28/42], [94mLoss[0m : 2.46512
[1mStep[0m  [32/42], [94mLoss[0m : 2.63867
[1mStep[0m  [36/42], [94mLoss[0m : 2.85316
[1mStep[0m  [40/42], [94mLoss[0m : 2.61844

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45018
[1mStep[0m  [4/42], [94mLoss[0m : 2.55190
[1mStep[0m  [8/42], [94mLoss[0m : 2.54273
[1mStep[0m  [12/42], [94mLoss[0m : 2.58492
[1mStep[0m  [16/42], [94mLoss[0m : 2.28920
[1mStep[0m  [20/42], [94mLoss[0m : 2.40446
[1mStep[0m  [24/42], [94mLoss[0m : 2.57986
[1mStep[0m  [28/42], [94mLoss[0m : 2.72156
[1mStep[0m  [32/42], [94mLoss[0m : 2.77145
[1mStep[0m  [36/42], [94mLoss[0m : 2.58220
[1mStep[0m  [40/42], [94mLoss[0m : 2.68650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38692
[1mStep[0m  [4/42], [94mLoss[0m : 2.43187
[1mStep[0m  [8/42], [94mLoss[0m : 2.38281
[1mStep[0m  [12/42], [94mLoss[0m : 2.54431
[1mStep[0m  [16/42], [94mLoss[0m : 2.48447
[1mStep[0m  [20/42], [94mLoss[0m : 2.67051
[1mStep[0m  [24/42], [94mLoss[0m : 2.38292
[1mStep[0m  [28/42], [94mLoss[0m : 2.37160
[1mStep[0m  [32/42], [94mLoss[0m : 2.32546
[1mStep[0m  [36/42], [94mLoss[0m : 2.58918
[1mStep[0m  [40/42], [94mLoss[0m : 2.59302

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.578, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54453
[1mStep[0m  [4/42], [94mLoss[0m : 2.36456
[1mStep[0m  [8/42], [94mLoss[0m : 2.45036
[1mStep[0m  [12/42], [94mLoss[0m : 2.41454
[1mStep[0m  [16/42], [94mLoss[0m : 2.64165
[1mStep[0m  [20/42], [94mLoss[0m : 2.51196
[1mStep[0m  [24/42], [94mLoss[0m : 2.62974
[1mStep[0m  [28/42], [94mLoss[0m : 2.45422
[1mStep[0m  [32/42], [94mLoss[0m : 2.48688
[1mStep[0m  [36/42], [94mLoss[0m : 2.66782
[1mStep[0m  [40/42], [94mLoss[0m : 2.55496

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38889
[1mStep[0m  [4/42], [94mLoss[0m : 2.64467
[1mStep[0m  [8/42], [94mLoss[0m : 2.43193
[1mStep[0m  [12/42], [94mLoss[0m : 2.50737
[1mStep[0m  [16/42], [94mLoss[0m : 2.36713
[1mStep[0m  [20/42], [94mLoss[0m : 2.44797
[1mStep[0m  [24/42], [94mLoss[0m : 2.56717
[1mStep[0m  [28/42], [94mLoss[0m : 2.41042
[1mStep[0m  [32/42], [94mLoss[0m : 2.40762
[1mStep[0m  [36/42], [94mLoss[0m : 2.21401
[1mStep[0m  [40/42], [94mLoss[0m : 2.51268

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.744, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38231
[1mStep[0m  [4/42], [94mLoss[0m : 2.38859
[1mStep[0m  [8/42], [94mLoss[0m : 2.47690
[1mStep[0m  [12/42], [94mLoss[0m : 2.29009
[1mStep[0m  [16/42], [94mLoss[0m : 2.44255
[1mStep[0m  [20/42], [94mLoss[0m : 2.31808
[1mStep[0m  [24/42], [94mLoss[0m : 2.36140
[1mStep[0m  [28/42], [94mLoss[0m : 2.37232
[1mStep[0m  [32/42], [94mLoss[0m : 2.30868
[1mStep[0m  [36/42], [94mLoss[0m : 2.36459
[1mStep[0m  [40/42], [94mLoss[0m : 2.42227

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.672, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35068
[1mStep[0m  [4/42], [94mLoss[0m : 2.55545
[1mStep[0m  [8/42], [94mLoss[0m : 2.48154
[1mStep[0m  [12/42], [94mLoss[0m : 2.25347
[1mStep[0m  [16/42], [94mLoss[0m : 2.46717
[1mStep[0m  [20/42], [94mLoss[0m : 1.98252
[1mStep[0m  [24/42], [94mLoss[0m : 2.52940
[1mStep[0m  [28/42], [94mLoss[0m : 2.43275
[1mStep[0m  [32/42], [94mLoss[0m : 2.32841
[1mStep[0m  [36/42], [94mLoss[0m : 2.17807
[1mStep[0m  [40/42], [94mLoss[0m : 2.53847

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.687, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28357
[1mStep[0m  [4/42], [94mLoss[0m : 2.46395
[1mStep[0m  [8/42], [94mLoss[0m : 2.40606
[1mStep[0m  [12/42], [94mLoss[0m : 2.35899
[1mStep[0m  [16/42], [94mLoss[0m : 2.32363
[1mStep[0m  [20/42], [94mLoss[0m : 2.41422
[1mStep[0m  [24/42], [94mLoss[0m : 2.30770
[1mStep[0m  [28/42], [94mLoss[0m : 2.29426
[1mStep[0m  [32/42], [94mLoss[0m : 2.43619
[1mStep[0m  [36/42], [94mLoss[0m : 2.24117
[1mStep[0m  [40/42], [94mLoss[0m : 2.39015

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22843
[1mStep[0m  [4/42], [94mLoss[0m : 2.27829
[1mStep[0m  [8/42], [94mLoss[0m : 2.34240
[1mStep[0m  [12/42], [94mLoss[0m : 2.06975
[1mStep[0m  [16/42], [94mLoss[0m : 2.37614
[1mStep[0m  [20/42], [94mLoss[0m : 2.18733
[1mStep[0m  [24/42], [94mLoss[0m : 2.39978
[1mStep[0m  [28/42], [94mLoss[0m : 2.22202
[1mStep[0m  [32/42], [94mLoss[0m : 2.40512
[1mStep[0m  [36/42], [94mLoss[0m : 2.34410
[1mStep[0m  [40/42], [94mLoss[0m : 2.39554

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.586, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43523
[1mStep[0m  [4/42], [94mLoss[0m : 2.26626
[1mStep[0m  [8/42], [94mLoss[0m : 2.13568
[1mStep[0m  [12/42], [94mLoss[0m : 2.15586
[1mStep[0m  [16/42], [94mLoss[0m : 2.35628
[1mStep[0m  [20/42], [94mLoss[0m : 2.21233
[1mStep[0m  [24/42], [94mLoss[0m : 2.30467
[1mStep[0m  [28/42], [94mLoss[0m : 2.24338
[1mStep[0m  [32/42], [94mLoss[0m : 2.43676
[1mStep[0m  [36/42], [94mLoss[0m : 2.36500
[1mStep[0m  [40/42], [94mLoss[0m : 2.08606

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.661, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36050
[1mStep[0m  [4/42], [94mLoss[0m : 2.20568
[1mStep[0m  [8/42], [94mLoss[0m : 2.24693
[1mStep[0m  [12/42], [94mLoss[0m : 2.19703
[1mStep[0m  [16/42], [94mLoss[0m : 2.18411
[1mStep[0m  [20/42], [94mLoss[0m : 2.13436
[1mStep[0m  [24/42], [94mLoss[0m : 2.44328
[1mStep[0m  [28/42], [94mLoss[0m : 2.26388
[1mStep[0m  [32/42], [94mLoss[0m : 2.20593
[1mStep[0m  [36/42], [94mLoss[0m : 2.40597
[1mStep[0m  [40/42], [94mLoss[0m : 2.10443

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.581, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18151
[1mStep[0m  [4/42], [94mLoss[0m : 2.14994
[1mStep[0m  [8/42], [94mLoss[0m : 2.16723
[1mStep[0m  [12/42], [94mLoss[0m : 2.12790
[1mStep[0m  [16/42], [94mLoss[0m : 2.15003
[1mStep[0m  [20/42], [94mLoss[0m : 2.31143
[1mStep[0m  [24/42], [94mLoss[0m : 2.20345
[1mStep[0m  [28/42], [94mLoss[0m : 2.28400
[1mStep[0m  [32/42], [94mLoss[0m : 2.11542
[1mStep[0m  [36/42], [94mLoss[0m : 2.21083
[1mStep[0m  [40/42], [94mLoss[0m : 2.04750

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11895
[1mStep[0m  [4/42], [94mLoss[0m : 2.08409
[1mStep[0m  [8/42], [94mLoss[0m : 2.09542
[1mStep[0m  [12/42], [94mLoss[0m : 2.06416
[1mStep[0m  [16/42], [94mLoss[0m : 2.24396
[1mStep[0m  [20/42], [94mLoss[0m : 2.02351
[1mStep[0m  [24/42], [94mLoss[0m : 2.10811
[1mStep[0m  [28/42], [94mLoss[0m : 2.31102
[1mStep[0m  [32/42], [94mLoss[0m : 2.15375
[1mStep[0m  [36/42], [94mLoss[0m : 2.14367
[1mStep[0m  [40/42], [94mLoss[0m : 2.31465

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07810
[1mStep[0m  [4/42], [94mLoss[0m : 2.23764
[1mStep[0m  [8/42], [94mLoss[0m : 1.94915
[1mStep[0m  [12/42], [94mLoss[0m : 2.17855
[1mStep[0m  [16/42], [94mLoss[0m : 2.30042
[1mStep[0m  [20/42], [94mLoss[0m : 2.32203
[1mStep[0m  [24/42], [94mLoss[0m : 2.01007
[1mStep[0m  [28/42], [94mLoss[0m : 2.09869
[1mStep[0m  [32/42], [94mLoss[0m : 2.16852
[1mStep[0m  [36/42], [94mLoss[0m : 2.24693
[1mStep[0m  [40/42], [94mLoss[0m : 2.20311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96096
[1mStep[0m  [4/42], [94mLoss[0m : 2.00119
[1mStep[0m  [8/42], [94mLoss[0m : 1.87208
[1mStep[0m  [12/42], [94mLoss[0m : 2.07415
[1mStep[0m  [16/42], [94mLoss[0m : 2.10844
[1mStep[0m  [20/42], [94mLoss[0m : 2.09088
[1mStep[0m  [24/42], [94mLoss[0m : 2.04360
[1mStep[0m  [28/42], [94mLoss[0m : 2.11567
[1mStep[0m  [32/42], [94mLoss[0m : 1.97038
[1mStep[0m  [36/42], [94mLoss[0m : 2.23117
[1mStep[0m  [40/42], [94mLoss[0m : 2.14713

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02153
[1mStep[0m  [4/42], [94mLoss[0m : 2.05754
[1mStep[0m  [8/42], [94mLoss[0m : 2.19906
[1mStep[0m  [12/42], [94mLoss[0m : 1.85405
[1mStep[0m  [16/42], [94mLoss[0m : 2.04146
[1mStep[0m  [20/42], [94mLoss[0m : 2.01232
[1mStep[0m  [24/42], [94mLoss[0m : 2.01328
[1mStep[0m  [28/42], [94mLoss[0m : 2.18195
[1mStep[0m  [32/42], [94mLoss[0m : 2.00375
[1mStep[0m  [36/42], [94mLoss[0m : 1.95329
[1mStep[0m  [40/42], [94mLoss[0m : 1.95831

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.038, [92mTest[0m: 2.498, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14699
[1mStep[0m  [4/42], [94mLoss[0m : 2.06696
[1mStep[0m  [8/42], [94mLoss[0m : 1.91838
[1mStep[0m  [12/42], [94mLoss[0m : 1.94930
[1mStep[0m  [16/42], [94mLoss[0m : 2.03781
[1mStep[0m  [20/42], [94mLoss[0m : 1.92346
[1mStep[0m  [24/42], [94mLoss[0m : 2.10003
[1mStep[0m  [28/42], [94mLoss[0m : 1.90590
[1mStep[0m  [32/42], [94mLoss[0m : 2.01934
[1mStep[0m  [36/42], [94mLoss[0m : 2.02342
[1mStep[0m  [40/42], [94mLoss[0m : 1.94814

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.570, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99641
[1mStep[0m  [4/42], [94mLoss[0m : 1.89552
[1mStep[0m  [8/42], [94mLoss[0m : 1.87862
[1mStep[0m  [12/42], [94mLoss[0m : 1.96103
[1mStep[0m  [16/42], [94mLoss[0m : 2.08263
[1mStep[0m  [20/42], [94mLoss[0m : 1.87161
[1mStep[0m  [24/42], [94mLoss[0m : 1.94599
[1mStep[0m  [28/42], [94mLoss[0m : 2.08766
[1mStep[0m  [32/42], [94mLoss[0m : 2.00889
[1mStep[0m  [36/42], [94mLoss[0m : 1.85137
[1mStep[0m  [40/42], [94mLoss[0m : 1.97880

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99499
[1mStep[0m  [4/42], [94mLoss[0m : 1.73055
[1mStep[0m  [8/42], [94mLoss[0m : 1.99405
[1mStep[0m  [12/42], [94mLoss[0m : 1.89169
[1mStep[0m  [16/42], [94mLoss[0m : 1.82811
[1mStep[0m  [20/42], [94mLoss[0m : 1.95226
[1mStep[0m  [24/42], [94mLoss[0m : 1.88672
[1mStep[0m  [28/42], [94mLoss[0m : 2.00112
[1mStep[0m  [32/42], [94mLoss[0m : 1.92126
[1mStep[0m  [36/42], [94mLoss[0m : 1.71833
[1mStep[0m  [40/42], [94mLoss[0m : 1.99771

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.589, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98723
[1mStep[0m  [4/42], [94mLoss[0m : 2.08208
[1mStep[0m  [8/42], [94mLoss[0m : 1.79353
[1mStep[0m  [12/42], [94mLoss[0m : 1.81949
[1mStep[0m  [16/42], [94mLoss[0m : 1.76482
[1mStep[0m  [20/42], [94mLoss[0m : 2.10026
[1mStep[0m  [24/42], [94mLoss[0m : 1.81123
[1mStep[0m  [28/42], [94mLoss[0m : 1.86083
[1mStep[0m  [32/42], [94mLoss[0m : 1.88594
[1mStep[0m  [36/42], [94mLoss[0m : 1.71562
[1mStep[0m  [40/42], [94mLoss[0m : 1.76926

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79975
[1mStep[0m  [4/42], [94mLoss[0m : 1.91000
[1mStep[0m  [8/42], [94mLoss[0m : 1.82025
[1mStep[0m  [12/42], [94mLoss[0m : 1.86216
[1mStep[0m  [16/42], [94mLoss[0m : 2.03722
[1mStep[0m  [20/42], [94mLoss[0m : 2.03772
[1mStep[0m  [24/42], [94mLoss[0m : 1.82416
[1mStep[0m  [28/42], [94mLoss[0m : 1.96431
[1mStep[0m  [32/42], [94mLoss[0m : 1.76991
[1mStep[0m  [36/42], [94mLoss[0m : 1.83160
[1mStep[0m  [40/42], [94mLoss[0m : 1.97090

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.569, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86194
[1mStep[0m  [4/42], [94mLoss[0m : 1.83756
[1mStep[0m  [8/42], [94mLoss[0m : 1.85364
[1mStep[0m  [12/42], [94mLoss[0m : 1.74442
[1mStep[0m  [16/42], [94mLoss[0m : 1.81894
[1mStep[0m  [20/42], [94mLoss[0m : 1.78926
[1mStep[0m  [24/42], [94mLoss[0m : 1.71640
[1mStep[0m  [28/42], [94mLoss[0m : 1.71194
[1mStep[0m  [32/42], [94mLoss[0m : 1.89992
[1mStep[0m  [36/42], [94mLoss[0m : 1.88295
[1mStep[0m  [40/42], [94mLoss[0m : 1.77921

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84743
[1mStep[0m  [4/42], [94mLoss[0m : 1.59619
[1mStep[0m  [8/42], [94mLoss[0m : 1.79635
[1mStep[0m  [12/42], [94mLoss[0m : 1.92836
[1mStep[0m  [16/42], [94mLoss[0m : 1.90828
[1mStep[0m  [20/42], [94mLoss[0m : 1.73134
[1mStep[0m  [24/42], [94mLoss[0m : 1.82381
[1mStep[0m  [28/42], [94mLoss[0m : 1.75602
[1mStep[0m  [32/42], [94mLoss[0m : 1.56425
[1mStep[0m  [36/42], [94mLoss[0m : 1.86564
[1mStep[0m  [40/42], [94mLoss[0m : 1.83599

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69898
[1mStep[0m  [4/42], [94mLoss[0m : 1.77450
[1mStep[0m  [8/42], [94mLoss[0m : 2.11642
[1mStep[0m  [12/42], [94mLoss[0m : 1.77406
[1mStep[0m  [16/42], [94mLoss[0m : 1.92449
[1mStep[0m  [20/42], [94mLoss[0m : 1.58823
[1mStep[0m  [24/42], [94mLoss[0m : 1.72208
[1mStep[0m  [28/42], [94mLoss[0m : 1.79574
[1mStep[0m  [32/42], [94mLoss[0m : 1.75570
[1mStep[0m  [36/42], [94mLoss[0m : 1.81855
[1mStep[0m  [40/42], [94mLoss[0m : 1.75854

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82888
[1mStep[0m  [4/42], [94mLoss[0m : 1.85539
[1mStep[0m  [8/42], [94mLoss[0m : 1.87036
[1mStep[0m  [12/42], [94mLoss[0m : 1.68924
[1mStep[0m  [16/42], [94mLoss[0m : 1.85035
[1mStep[0m  [20/42], [94mLoss[0m : 1.79327
[1mStep[0m  [24/42], [94mLoss[0m : 1.86483
[1mStep[0m  [28/42], [94mLoss[0m : 1.64065
[1mStep[0m  [32/42], [94mLoss[0m : 1.54667
[1mStep[0m  [36/42], [94mLoss[0m : 1.86796
[1mStep[0m  [40/42], [94mLoss[0m : 1.59925

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62137
[1mStep[0m  [4/42], [94mLoss[0m : 1.75050
[1mStep[0m  [8/42], [94mLoss[0m : 1.72232
[1mStep[0m  [12/42], [94mLoss[0m : 1.80722
[1mStep[0m  [16/42], [94mLoss[0m : 1.65396
[1mStep[0m  [20/42], [94mLoss[0m : 1.63670
[1mStep[0m  [24/42], [94mLoss[0m : 1.87317
[1mStep[0m  [28/42], [94mLoss[0m : 1.67604
[1mStep[0m  [32/42], [94mLoss[0m : 1.88380
[1mStep[0m  [36/42], [94mLoss[0m : 1.63578
[1mStep[0m  [40/42], [94mLoss[0m : 1.69703

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76995
[1mStep[0m  [4/42], [94mLoss[0m : 1.63461
[1mStep[0m  [8/42], [94mLoss[0m : 1.77425
[1mStep[0m  [12/42], [94mLoss[0m : 1.71096
[1mStep[0m  [16/42], [94mLoss[0m : 1.50553
[1mStep[0m  [20/42], [94mLoss[0m : 1.68443
[1mStep[0m  [24/42], [94mLoss[0m : 1.68560
[1mStep[0m  [28/42], [94mLoss[0m : 1.79995
[1mStep[0m  [32/42], [94mLoss[0m : 1.92979
[1mStep[0m  [36/42], [94mLoss[0m : 1.86180
[1mStep[0m  [40/42], [94mLoss[0m : 1.65386

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63801
[1mStep[0m  [4/42], [94mLoss[0m : 1.72919
[1mStep[0m  [8/42], [94mLoss[0m : 1.78157
[1mStep[0m  [12/42], [94mLoss[0m : 1.75602
[1mStep[0m  [16/42], [94mLoss[0m : 1.58187
[1mStep[0m  [20/42], [94mLoss[0m : 1.69057
[1mStep[0m  [24/42], [94mLoss[0m : 1.67951
[1mStep[0m  [28/42], [94mLoss[0m : 1.65377
[1mStep[0m  [32/42], [94mLoss[0m : 1.68034
[1mStep[0m  [36/42], [94mLoss[0m : 1.83551
[1mStep[0m  [40/42], [94mLoss[0m : 1.82681

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.590, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56638
[1mStep[0m  [4/42], [94mLoss[0m : 1.93407
[1mStep[0m  [8/42], [94mLoss[0m : 1.67677
[1mStep[0m  [12/42], [94mLoss[0m : 1.71081
[1mStep[0m  [16/42], [94mLoss[0m : 1.86875
[1mStep[0m  [20/42], [94mLoss[0m : 1.54228
[1mStep[0m  [24/42], [94mLoss[0m : 1.55469
[1mStep[0m  [28/42], [94mLoss[0m : 1.70071
[1mStep[0m  [32/42], [94mLoss[0m : 1.66397
[1mStep[0m  [36/42], [94mLoss[0m : 1.50790
[1mStep[0m  [40/42], [94mLoss[0m : 1.64578

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.537, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.531
====================================

Phase 2 - Evaluation MAE:  2.531127231461661
MAE score P1      2.356878
MAE score P2      2.531127
loss              1.674109
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.18769
[1mStep[0m  [8/84], [94mLoss[0m : 10.87567
[1mStep[0m  [16/84], [94mLoss[0m : 10.79001
[1mStep[0m  [24/84], [94mLoss[0m : 9.99440
[1mStep[0m  [32/84], [94mLoss[0m : 9.69190
[1mStep[0m  [40/84], [94mLoss[0m : 8.46044
[1mStep[0m  [48/84], [94mLoss[0m : 7.97849
[1mStep[0m  [56/84], [94mLoss[0m : 7.19403
[1mStep[0m  [64/84], [94mLoss[0m : 6.27386
[1mStep[0m  [72/84], [94mLoss[0m : 5.69573
[1mStep[0m  [80/84], [94mLoss[0m : 4.66557

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.329, [92mTest[0m: 10.819, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.62287
[1mStep[0m  [8/84], [94mLoss[0m : 3.14208
[1mStep[0m  [16/84], [94mLoss[0m : 3.15748
[1mStep[0m  [24/84], [94mLoss[0m : 2.58816
[1mStep[0m  [32/84], [94mLoss[0m : 2.50577
[1mStep[0m  [40/84], [94mLoss[0m : 2.80930
[1mStep[0m  [48/84], [94mLoss[0m : 2.55640
[1mStep[0m  [56/84], [94mLoss[0m : 2.51821
[1mStep[0m  [64/84], [94mLoss[0m : 2.35985
[1mStep[0m  [72/84], [94mLoss[0m : 2.62404
[1mStep[0m  [80/84], [94mLoss[0m : 2.63031

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.816, [92mTest[0m: 3.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53800
[1mStep[0m  [8/84], [94mLoss[0m : 2.32116
[1mStep[0m  [16/84], [94mLoss[0m : 2.33580
[1mStep[0m  [24/84], [94mLoss[0m : 2.13166
[1mStep[0m  [32/84], [94mLoss[0m : 2.53872
[1mStep[0m  [40/84], [94mLoss[0m : 2.44572
[1mStep[0m  [48/84], [94mLoss[0m : 2.52262
[1mStep[0m  [56/84], [94mLoss[0m : 2.87089
[1mStep[0m  [64/84], [94mLoss[0m : 2.86570
[1mStep[0m  [72/84], [94mLoss[0m : 2.85536
[1mStep[0m  [80/84], [94mLoss[0m : 2.69497

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87373
[1mStep[0m  [8/84], [94mLoss[0m : 2.36502
[1mStep[0m  [16/84], [94mLoss[0m : 2.57894
[1mStep[0m  [24/84], [94mLoss[0m : 2.48217
[1mStep[0m  [32/84], [94mLoss[0m : 2.38489
[1mStep[0m  [40/84], [94mLoss[0m : 2.49805
[1mStep[0m  [48/84], [94mLoss[0m : 2.40275
[1mStep[0m  [56/84], [94mLoss[0m : 2.68733
[1mStep[0m  [64/84], [94mLoss[0m : 2.47798
[1mStep[0m  [72/84], [94mLoss[0m : 2.48103
[1mStep[0m  [80/84], [94mLoss[0m : 2.88307

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55880
[1mStep[0m  [8/84], [94mLoss[0m : 2.73724
[1mStep[0m  [16/84], [94mLoss[0m : 2.80742
[1mStep[0m  [24/84], [94mLoss[0m : 2.79747
[1mStep[0m  [32/84], [94mLoss[0m : 2.33939
[1mStep[0m  [40/84], [94mLoss[0m : 2.47329
[1mStep[0m  [48/84], [94mLoss[0m : 2.42614
[1mStep[0m  [56/84], [94mLoss[0m : 2.32658
[1mStep[0m  [64/84], [94mLoss[0m : 2.82929
[1mStep[0m  [72/84], [94mLoss[0m : 2.45858
[1mStep[0m  [80/84], [94mLoss[0m : 2.76261

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10676
[1mStep[0m  [8/84], [94mLoss[0m : 2.60945
[1mStep[0m  [16/84], [94mLoss[0m : 2.24520
[1mStep[0m  [24/84], [94mLoss[0m : 2.33281
[1mStep[0m  [32/84], [94mLoss[0m : 2.36061
[1mStep[0m  [40/84], [94mLoss[0m : 2.42002
[1mStep[0m  [48/84], [94mLoss[0m : 2.52685
[1mStep[0m  [56/84], [94mLoss[0m : 2.50061
[1mStep[0m  [64/84], [94mLoss[0m : 2.59961
[1mStep[0m  [72/84], [94mLoss[0m : 2.52477
[1mStep[0m  [80/84], [94mLoss[0m : 2.50818

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33613
[1mStep[0m  [8/84], [94mLoss[0m : 2.26326
[1mStep[0m  [16/84], [94mLoss[0m : 2.13160
[1mStep[0m  [24/84], [94mLoss[0m : 2.45175
[1mStep[0m  [32/84], [94mLoss[0m : 2.42203
[1mStep[0m  [40/84], [94mLoss[0m : 2.50782
[1mStep[0m  [48/84], [94mLoss[0m : 2.51917
[1mStep[0m  [56/84], [94mLoss[0m : 2.82728
[1mStep[0m  [64/84], [94mLoss[0m : 2.85554
[1mStep[0m  [72/84], [94mLoss[0m : 2.50221
[1mStep[0m  [80/84], [94mLoss[0m : 2.50700

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13802
[1mStep[0m  [8/84], [94mLoss[0m : 2.20778
[1mStep[0m  [16/84], [94mLoss[0m : 2.48856
[1mStep[0m  [24/84], [94mLoss[0m : 2.51710
[1mStep[0m  [32/84], [94mLoss[0m : 2.54486
[1mStep[0m  [40/84], [94mLoss[0m : 2.20607
[1mStep[0m  [48/84], [94mLoss[0m : 2.74501
[1mStep[0m  [56/84], [94mLoss[0m : 2.43416
[1mStep[0m  [64/84], [94mLoss[0m : 2.27750
[1mStep[0m  [72/84], [94mLoss[0m : 2.49607
[1mStep[0m  [80/84], [94mLoss[0m : 2.36142

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35050
[1mStep[0m  [8/84], [94mLoss[0m : 2.70374
[1mStep[0m  [16/84], [94mLoss[0m : 2.16908
[1mStep[0m  [24/84], [94mLoss[0m : 2.39295
[1mStep[0m  [32/84], [94mLoss[0m : 2.58552
[1mStep[0m  [40/84], [94mLoss[0m : 2.26112
[1mStep[0m  [48/84], [94mLoss[0m : 2.50828
[1mStep[0m  [56/84], [94mLoss[0m : 2.45510
[1mStep[0m  [64/84], [94mLoss[0m : 2.40915
[1mStep[0m  [72/84], [94mLoss[0m : 2.98807
[1mStep[0m  [80/84], [94mLoss[0m : 2.24169

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66311
[1mStep[0m  [8/84], [94mLoss[0m : 2.22148
[1mStep[0m  [16/84], [94mLoss[0m : 2.53736
[1mStep[0m  [24/84], [94mLoss[0m : 2.57346
[1mStep[0m  [32/84], [94mLoss[0m : 2.17216
[1mStep[0m  [40/84], [94mLoss[0m : 2.45193
[1mStep[0m  [48/84], [94mLoss[0m : 2.31000
[1mStep[0m  [56/84], [94mLoss[0m : 2.32487
[1mStep[0m  [64/84], [94mLoss[0m : 2.62838
[1mStep[0m  [72/84], [94mLoss[0m : 2.42805
[1mStep[0m  [80/84], [94mLoss[0m : 2.39624

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38817
[1mStep[0m  [8/84], [94mLoss[0m : 2.38788
[1mStep[0m  [16/84], [94mLoss[0m : 2.07190
[1mStep[0m  [24/84], [94mLoss[0m : 2.68104
[1mStep[0m  [32/84], [94mLoss[0m : 2.04197
[1mStep[0m  [40/84], [94mLoss[0m : 2.73728
[1mStep[0m  [48/84], [94mLoss[0m : 2.25356
[1mStep[0m  [56/84], [94mLoss[0m : 2.49430
[1mStep[0m  [64/84], [94mLoss[0m : 2.10025
[1mStep[0m  [72/84], [94mLoss[0m : 2.27726
[1mStep[0m  [80/84], [94mLoss[0m : 2.29047

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36995
[1mStep[0m  [8/84], [94mLoss[0m : 2.34844
[1mStep[0m  [16/84], [94mLoss[0m : 2.42805
[1mStep[0m  [24/84], [94mLoss[0m : 2.56919
[1mStep[0m  [32/84], [94mLoss[0m : 2.29322
[1mStep[0m  [40/84], [94mLoss[0m : 2.68893
[1mStep[0m  [48/84], [94mLoss[0m : 2.59647
[1mStep[0m  [56/84], [94mLoss[0m : 2.51078
[1mStep[0m  [64/84], [94mLoss[0m : 2.76704
[1mStep[0m  [72/84], [94mLoss[0m : 2.46041
[1mStep[0m  [80/84], [94mLoss[0m : 2.27748

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50335
[1mStep[0m  [8/84], [94mLoss[0m : 2.39770
[1mStep[0m  [16/84], [94mLoss[0m : 2.23044
[1mStep[0m  [24/84], [94mLoss[0m : 2.48473
[1mStep[0m  [32/84], [94mLoss[0m : 2.39830
[1mStep[0m  [40/84], [94mLoss[0m : 2.38189
[1mStep[0m  [48/84], [94mLoss[0m : 2.13358
[1mStep[0m  [56/84], [94mLoss[0m : 2.39350
[1mStep[0m  [64/84], [94mLoss[0m : 2.33505
[1mStep[0m  [72/84], [94mLoss[0m : 2.52336
[1mStep[0m  [80/84], [94mLoss[0m : 2.40578

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13629
[1mStep[0m  [8/84], [94mLoss[0m : 2.47579
[1mStep[0m  [16/84], [94mLoss[0m : 2.26234
[1mStep[0m  [24/84], [94mLoss[0m : 2.61001
[1mStep[0m  [32/84], [94mLoss[0m : 2.38063
[1mStep[0m  [40/84], [94mLoss[0m : 2.46569
[1mStep[0m  [48/84], [94mLoss[0m : 2.51245
[1mStep[0m  [56/84], [94mLoss[0m : 2.28454
[1mStep[0m  [64/84], [94mLoss[0m : 2.30846
[1mStep[0m  [72/84], [94mLoss[0m : 2.62325
[1mStep[0m  [80/84], [94mLoss[0m : 2.21411

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36186
[1mStep[0m  [8/84], [94mLoss[0m : 2.71477
[1mStep[0m  [16/84], [94mLoss[0m : 2.31011
[1mStep[0m  [24/84], [94mLoss[0m : 2.70765
[1mStep[0m  [32/84], [94mLoss[0m : 2.53201
[1mStep[0m  [40/84], [94mLoss[0m : 2.40703
[1mStep[0m  [48/84], [94mLoss[0m : 2.60600
[1mStep[0m  [56/84], [94mLoss[0m : 2.61030
[1mStep[0m  [64/84], [94mLoss[0m : 3.02447
[1mStep[0m  [72/84], [94mLoss[0m : 2.30710
[1mStep[0m  [80/84], [94mLoss[0m : 2.44822

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13735
[1mStep[0m  [8/84], [94mLoss[0m : 2.51181
[1mStep[0m  [16/84], [94mLoss[0m : 2.38420
[1mStep[0m  [24/84], [94mLoss[0m : 2.61573
[1mStep[0m  [32/84], [94mLoss[0m : 2.28977
[1mStep[0m  [40/84], [94mLoss[0m : 2.63099
[1mStep[0m  [48/84], [94mLoss[0m : 2.58255
[1mStep[0m  [56/84], [94mLoss[0m : 2.55620
[1mStep[0m  [64/84], [94mLoss[0m : 2.41470
[1mStep[0m  [72/84], [94mLoss[0m : 2.23809
[1mStep[0m  [80/84], [94mLoss[0m : 2.44563

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.312, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54284
[1mStep[0m  [8/84], [94mLoss[0m : 2.49996
[1mStep[0m  [16/84], [94mLoss[0m : 2.42302
[1mStep[0m  [24/84], [94mLoss[0m : 2.55955
[1mStep[0m  [32/84], [94mLoss[0m : 2.59015
[1mStep[0m  [40/84], [94mLoss[0m : 2.06284
[1mStep[0m  [48/84], [94mLoss[0m : 2.31180
[1mStep[0m  [56/84], [94mLoss[0m : 2.40839
[1mStep[0m  [64/84], [94mLoss[0m : 1.98745
[1mStep[0m  [72/84], [94mLoss[0m : 2.56790
[1mStep[0m  [80/84], [94mLoss[0m : 2.18334

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42076
[1mStep[0m  [8/84], [94mLoss[0m : 2.26412
[1mStep[0m  [16/84], [94mLoss[0m : 2.16425
[1mStep[0m  [24/84], [94mLoss[0m : 2.34953
[1mStep[0m  [32/84], [94mLoss[0m : 2.35024
[1mStep[0m  [40/84], [94mLoss[0m : 2.33716
[1mStep[0m  [48/84], [94mLoss[0m : 2.24062
[1mStep[0m  [56/84], [94mLoss[0m : 2.57532
[1mStep[0m  [64/84], [94mLoss[0m : 2.37166
[1mStep[0m  [72/84], [94mLoss[0m : 2.39701
[1mStep[0m  [80/84], [94mLoss[0m : 2.40663

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.315, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37571
[1mStep[0m  [8/84], [94mLoss[0m : 2.31166
[1mStep[0m  [16/84], [94mLoss[0m : 2.48159
[1mStep[0m  [24/84], [94mLoss[0m : 2.40093
[1mStep[0m  [32/84], [94mLoss[0m : 2.17624
[1mStep[0m  [40/84], [94mLoss[0m : 2.37980
[1mStep[0m  [48/84], [94mLoss[0m : 2.31829
[1mStep[0m  [56/84], [94mLoss[0m : 2.16663
[1mStep[0m  [64/84], [94mLoss[0m : 2.41354
[1mStep[0m  [72/84], [94mLoss[0m : 2.27780
[1mStep[0m  [80/84], [94mLoss[0m : 2.46128

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54531
[1mStep[0m  [8/84], [94mLoss[0m : 2.38811
[1mStep[0m  [16/84], [94mLoss[0m : 2.42923
[1mStep[0m  [24/84], [94mLoss[0m : 2.12290
[1mStep[0m  [32/84], [94mLoss[0m : 2.38670
[1mStep[0m  [40/84], [94mLoss[0m : 2.25012
[1mStep[0m  [48/84], [94mLoss[0m : 2.54537
[1mStep[0m  [56/84], [94mLoss[0m : 2.49438
[1mStep[0m  [64/84], [94mLoss[0m : 2.07398
[1mStep[0m  [72/84], [94mLoss[0m : 2.32840
[1mStep[0m  [80/84], [94mLoss[0m : 2.29839

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67630
[1mStep[0m  [8/84], [94mLoss[0m : 2.44900
[1mStep[0m  [16/84], [94mLoss[0m : 2.31513
[1mStep[0m  [24/84], [94mLoss[0m : 2.28220
[1mStep[0m  [32/84], [94mLoss[0m : 2.42842
[1mStep[0m  [40/84], [94mLoss[0m : 2.37188
[1mStep[0m  [48/84], [94mLoss[0m : 2.59658
[1mStep[0m  [56/84], [94mLoss[0m : 2.46323
[1mStep[0m  [64/84], [94mLoss[0m : 2.60498
[1mStep[0m  [72/84], [94mLoss[0m : 2.74185
[1mStep[0m  [80/84], [94mLoss[0m : 2.25274

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25313
[1mStep[0m  [8/84], [94mLoss[0m : 2.45470
[1mStep[0m  [16/84], [94mLoss[0m : 2.17335
[1mStep[0m  [24/84], [94mLoss[0m : 2.49217
[1mStep[0m  [32/84], [94mLoss[0m : 2.54838
[1mStep[0m  [40/84], [94mLoss[0m : 2.38641
[1mStep[0m  [48/84], [94mLoss[0m : 2.31378
[1mStep[0m  [56/84], [94mLoss[0m : 2.24829
[1mStep[0m  [64/84], [94mLoss[0m : 2.40589
[1mStep[0m  [72/84], [94mLoss[0m : 2.28243
[1mStep[0m  [80/84], [94mLoss[0m : 2.86924

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55119
[1mStep[0m  [8/84], [94mLoss[0m : 2.24692
[1mStep[0m  [16/84], [94mLoss[0m : 2.46025
[1mStep[0m  [24/84], [94mLoss[0m : 2.45356
[1mStep[0m  [32/84], [94mLoss[0m : 2.41264
[1mStep[0m  [40/84], [94mLoss[0m : 2.20662
[1mStep[0m  [48/84], [94mLoss[0m : 2.47052
[1mStep[0m  [56/84], [94mLoss[0m : 2.12035
[1mStep[0m  [64/84], [94mLoss[0m : 2.66816
[1mStep[0m  [72/84], [94mLoss[0m : 2.22089
[1mStep[0m  [80/84], [94mLoss[0m : 2.57563

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.313, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37846
[1mStep[0m  [8/84], [94mLoss[0m : 2.55802
[1mStep[0m  [16/84], [94mLoss[0m : 2.19407
[1mStep[0m  [24/84], [94mLoss[0m : 2.35767
[1mStep[0m  [32/84], [94mLoss[0m : 1.91079
[1mStep[0m  [40/84], [94mLoss[0m : 2.25388
[1mStep[0m  [48/84], [94mLoss[0m : 2.21983
[1mStep[0m  [56/84], [94mLoss[0m : 2.01956
[1mStep[0m  [64/84], [94mLoss[0m : 2.35099
[1mStep[0m  [72/84], [94mLoss[0m : 2.55052
[1mStep[0m  [80/84], [94mLoss[0m : 2.51686

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53812
[1mStep[0m  [8/84], [94mLoss[0m : 2.36451
[1mStep[0m  [16/84], [94mLoss[0m : 2.05136
[1mStep[0m  [24/84], [94mLoss[0m : 2.45459
[1mStep[0m  [32/84], [94mLoss[0m : 2.25516
[1mStep[0m  [40/84], [94mLoss[0m : 2.24887
[1mStep[0m  [48/84], [94mLoss[0m : 2.53364
[1mStep[0m  [56/84], [94mLoss[0m : 2.23816
[1mStep[0m  [64/84], [94mLoss[0m : 2.30824
[1mStep[0m  [72/84], [94mLoss[0m : 2.23018
[1mStep[0m  [80/84], [94mLoss[0m : 2.57176

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46227
[1mStep[0m  [8/84], [94mLoss[0m : 2.67669
[1mStep[0m  [16/84], [94mLoss[0m : 2.61379
[1mStep[0m  [24/84], [94mLoss[0m : 2.09508
[1mStep[0m  [32/84], [94mLoss[0m : 2.43635
[1mStep[0m  [40/84], [94mLoss[0m : 2.58760
[1mStep[0m  [48/84], [94mLoss[0m : 2.31044
[1mStep[0m  [56/84], [94mLoss[0m : 2.50511
[1mStep[0m  [64/84], [94mLoss[0m : 2.44998
[1mStep[0m  [72/84], [94mLoss[0m : 2.23161
[1mStep[0m  [80/84], [94mLoss[0m : 2.39014

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60294
[1mStep[0m  [8/84], [94mLoss[0m : 2.28353
[1mStep[0m  [16/84], [94mLoss[0m : 2.46772
[1mStep[0m  [24/84], [94mLoss[0m : 2.40592
[1mStep[0m  [32/84], [94mLoss[0m : 2.58004
[1mStep[0m  [40/84], [94mLoss[0m : 2.43970
[1mStep[0m  [48/84], [94mLoss[0m : 2.25934
[1mStep[0m  [56/84], [94mLoss[0m : 2.19250
[1mStep[0m  [64/84], [94mLoss[0m : 2.71487
[1mStep[0m  [72/84], [94mLoss[0m : 2.76605
[1mStep[0m  [80/84], [94mLoss[0m : 2.40384

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29582
[1mStep[0m  [8/84], [94mLoss[0m : 2.37911
[1mStep[0m  [16/84], [94mLoss[0m : 2.43512
[1mStep[0m  [24/84], [94mLoss[0m : 2.55562
[1mStep[0m  [32/84], [94mLoss[0m : 2.39934
[1mStep[0m  [40/84], [94mLoss[0m : 2.54338
[1mStep[0m  [48/84], [94mLoss[0m : 2.66972
[1mStep[0m  [56/84], [94mLoss[0m : 2.12162
[1mStep[0m  [64/84], [94mLoss[0m : 2.18672
[1mStep[0m  [72/84], [94mLoss[0m : 2.61074
[1mStep[0m  [80/84], [94mLoss[0m : 2.33260

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24680
[1mStep[0m  [8/84], [94mLoss[0m : 2.36063
[1mStep[0m  [16/84], [94mLoss[0m : 2.43056
[1mStep[0m  [24/84], [94mLoss[0m : 2.46986
[1mStep[0m  [32/84], [94mLoss[0m : 2.40595
[1mStep[0m  [40/84], [94mLoss[0m : 2.58663
[1mStep[0m  [48/84], [94mLoss[0m : 2.34488
[1mStep[0m  [56/84], [94mLoss[0m : 2.62081
[1mStep[0m  [64/84], [94mLoss[0m : 2.30057
[1mStep[0m  [72/84], [94mLoss[0m : 2.35194
[1mStep[0m  [80/84], [94mLoss[0m : 2.34201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21684
[1mStep[0m  [8/84], [94mLoss[0m : 2.27520
[1mStep[0m  [16/84], [94mLoss[0m : 2.37500
[1mStep[0m  [24/84], [94mLoss[0m : 2.67689
[1mStep[0m  [32/84], [94mLoss[0m : 2.50235
[1mStep[0m  [40/84], [94mLoss[0m : 2.76454
[1mStep[0m  [48/84], [94mLoss[0m : 2.28282
[1mStep[0m  [56/84], [94mLoss[0m : 2.13742
[1mStep[0m  [64/84], [94mLoss[0m : 2.25526
[1mStep[0m  [72/84], [94mLoss[0m : 2.42251
[1mStep[0m  [80/84], [94mLoss[0m : 2.37351

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.3386520573071072
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.40731
[1mStep[0m  [8/84], [94mLoss[0m : 2.22395
[1mStep[0m  [16/84], [94mLoss[0m : 2.37598
[1mStep[0m  [24/84], [94mLoss[0m : 2.07831
[1mStep[0m  [32/84], [94mLoss[0m : 2.10987
[1mStep[0m  [40/84], [94mLoss[0m : 2.52658
[1mStep[0m  [48/84], [94mLoss[0m : 2.73488
[1mStep[0m  [56/84], [94mLoss[0m : 2.59543
[1mStep[0m  [64/84], [94mLoss[0m : 2.37432
[1mStep[0m  [72/84], [94mLoss[0m : 2.46980
[1mStep[0m  [80/84], [94mLoss[0m : 2.02285

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34203
[1mStep[0m  [8/84], [94mLoss[0m : 2.63333
[1mStep[0m  [16/84], [94mLoss[0m : 2.31174
[1mStep[0m  [24/84], [94mLoss[0m : 2.01310
[1mStep[0m  [32/84], [94mLoss[0m : 2.35180
[1mStep[0m  [40/84], [94mLoss[0m : 2.49375
[1mStep[0m  [48/84], [94mLoss[0m : 2.42245
[1mStep[0m  [56/84], [94mLoss[0m : 2.30267
[1mStep[0m  [64/84], [94mLoss[0m : 2.18184
[1mStep[0m  [72/84], [94mLoss[0m : 2.80097
[1mStep[0m  [80/84], [94mLoss[0m : 2.29228

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46779
[1mStep[0m  [8/84], [94mLoss[0m : 2.08557
[1mStep[0m  [16/84], [94mLoss[0m : 2.30390
[1mStep[0m  [24/84], [94mLoss[0m : 2.17601
[1mStep[0m  [32/84], [94mLoss[0m : 2.53276
[1mStep[0m  [40/84], [94mLoss[0m : 2.20313
[1mStep[0m  [48/84], [94mLoss[0m : 2.07397
[1mStep[0m  [56/84], [94mLoss[0m : 2.32647
[1mStep[0m  [64/84], [94mLoss[0m : 2.37307
[1mStep[0m  [72/84], [94mLoss[0m : 2.17990
[1mStep[0m  [80/84], [94mLoss[0m : 2.48107

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91002
[1mStep[0m  [8/84], [94mLoss[0m : 2.23867
[1mStep[0m  [16/84], [94mLoss[0m : 2.32090
[1mStep[0m  [24/84], [94mLoss[0m : 2.09567
[1mStep[0m  [32/84], [94mLoss[0m : 2.19739
[1mStep[0m  [40/84], [94mLoss[0m : 1.89715
[1mStep[0m  [48/84], [94mLoss[0m : 2.12597
[1mStep[0m  [56/84], [94mLoss[0m : 2.23246
[1mStep[0m  [64/84], [94mLoss[0m : 2.00247
[1mStep[0m  [72/84], [94mLoss[0m : 2.41159
[1mStep[0m  [80/84], [94mLoss[0m : 2.23232

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19681
[1mStep[0m  [8/84], [94mLoss[0m : 2.01599
[1mStep[0m  [16/84], [94mLoss[0m : 2.11720
[1mStep[0m  [24/84], [94mLoss[0m : 2.08365
[1mStep[0m  [32/84], [94mLoss[0m : 1.87873
[1mStep[0m  [40/84], [94mLoss[0m : 2.13799
[1mStep[0m  [48/84], [94mLoss[0m : 1.86232
[1mStep[0m  [56/84], [94mLoss[0m : 2.09277
[1mStep[0m  [64/84], [94mLoss[0m : 2.02356
[1mStep[0m  [72/84], [94mLoss[0m : 2.08933
[1mStep[0m  [80/84], [94mLoss[0m : 2.02613

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69187
[1mStep[0m  [8/84], [94mLoss[0m : 1.92292
[1mStep[0m  [16/84], [94mLoss[0m : 1.88631
[1mStep[0m  [24/84], [94mLoss[0m : 2.06239
[1mStep[0m  [32/84], [94mLoss[0m : 1.95674
[1mStep[0m  [40/84], [94mLoss[0m : 2.14699
[1mStep[0m  [48/84], [94mLoss[0m : 2.19211
[1mStep[0m  [56/84], [94mLoss[0m : 2.02660
[1mStep[0m  [64/84], [94mLoss[0m : 2.44070
[1mStep[0m  [72/84], [94mLoss[0m : 2.14273
[1mStep[0m  [80/84], [94mLoss[0m : 2.16334

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18554
[1mStep[0m  [8/84], [94mLoss[0m : 1.83109
[1mStep[0m  [16/84], [94mLoss[0m : 1.97438
[1mStep[0m  [24/84], [94mLoss[0m : 1.69048
[1mStep[0m  [32/84], [94mLoss[0m : 1.83730
[1mStep[0m  [40/84], [94mLoss[0m : 1.80020
[1mStep[0m  [48/84], [94mLoss[0m : 1.92766
[1mStep[0m  [56/84], [94mLoss[0m : 2.22269
[1mStep[0m  [64/84], [94mLoss[0m : 1.62440
[1mStep[0m  [72/84], [94mLoss[0m : 1.83732
[1mStep[0m  [80/84], [94mLoss[0m : 2.11237

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92686
[1mStep[0m  [8/84], [94mLoss[0m : 2.07584
[1mStep[0m  [16/84], [94mLoss[0m : 1.95005
[1mStep[0m  [24/84], [94mLoss[0m : 1.90494
[1mStep[0m  [32/84], [94mLoss[0m : 1.97511
[1mStep[0m  [40/84], [94mLoss[0m : 2.19772
[1mStep[0m  [48/84], [94mLoss[0m : 1.99612
[1mStep[0m  [56/84], [94mLoss[0m : 1.83678
[1mStep[0m  [64/84], [94mLoss[0m : 1.96240
[1mStep[0m  [72/84], [94mLoss[0m : 1.97089
[1mStep[0m  [80/84], [94mLoss[0m : 2.20910

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79865
[1mStep[0m  [8/84], [94mLoss[0m : 1.74623
[1mStep[0m  [16/84], [94mLoss[0m : 1.79880
[1mStep[0m  [24/84], [94mLoss[0m : 1.98434
[1mStep[0m  [32/84], [94mLoss[0m : 1.74193
[1mStep[0m  [40/84], [94mLoss[0m : 1.84968
[1mStep[0m  [48/84], [94mLoss[0m : 1.85439
[1mStep[0m  [56/84], [94mLoss[0m : 1.89465
[1mStep[0m  [64/84], [94mLoss[0m : 2.11231
[1mStep[0m  [72/84], [94mLoss[0m : 1.84121
[1mStep[0m  [80/84], [94mLoss[0m : 1.93837

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81153
[1mStep[0m  [8/84], [94mLoss[0m : 1.65875
[1mStep[0m  [16/84], [94mLoss[0m : 1.72860
[1mStep[0m  [24/84], [94mLoss[0m : 1.80860
[1mStep[0m  [32/84], [94mLoss[0m : 1.64037
[1mStep[0m  [40/84], [94mLoss[0m : 1.94602
[1mStep[0m  [48/84], [94mLoss[0m : 2.08698
[1mStep[0m  [56/84], [94mLoss[0m : 1.83625
[1mStep[0m  [64/84], [94mLoss[0m : 1.86774
[1mStep[0m  [72/84], [94mLoss[0m : 2.11583
[1mStep[0m  [80/84], [94mLoss[0m : 1.83113

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95960
[1mStep[0m  [8/84], [94mLoss[0m : 1.84242
[1mStep[0m  [16/84], [94mLoss[0m : 1.44278
[1mStep[0m  [24/84], [94mLoss[0m : 1.77564
[1mStep[0m  [32/84], [94mLoss[0m : 1.89771
[1mStep[0m  [40/84], [94mLoss[0m : 2.11002
[1mStep[0m  [48/84], [94mLoss[0m : 1.97207
[1mStep[0m  [56/84], [94mLoss[0m : 2.00493
[1mStep[0m  [64/84], [94mLoss[0m : 2.05150
[1mStep[0m  [72/84], [94mLoss[0m : 1.76139
[1mStep[0m  [80/84], [94mLoss[0m : 2.04802

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48572
[1mStep[0m  [8/84], [94mLoss[0m : 1.77914
[1mStep[0m  [16/84], [94mLoss[0m : 1.72976
[1mStep[0m  [24/84], [94mLoss[0m : 1.86607
[1mStep[0m  [32/84], [94mLoss[0m : 1.86662
[1mStep[0m  [40/84], [94mLoss[0m : 1.85755
[1mStep[0m  [48/84], [94mLoss[0m : 1.78586
[1mStep[0m  [56/84], [94mLoss[0m : 1.95162
[1mStep[0m  [64/84], [94mLoss[0m : 1.73762
[1mStep[0m  [72/84], [94mLoss[0m : 1.83677
[1mStep[0m  [80/84], [94mLoss[0m : 1.84030

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79061
[1mStep[0m  [8/84], [94mLoss[0m : 1.68921
[1mStep[0m  [16/84], [94mLoss[0m : 1.65073
[1mStep[0m  [24/84], [94mLoss[0m : 1.67314
[1mStep[0m  [32/84], [94mLoss[0m : 1.71578
[1mStep[0m  [40/84], [94mLoss[0m : 1.80883
[1mStep[0m  [48/84], [94mLoss[0m : 1.68740
[1mStep[0m  [56/84], [94mLoss[0m : 2.03902
[1mStep[0m  [64/84], [94mLoss[0m : 1.85128
[1mStep[0m  [72/84], [94mLoss[0m : 1.85183
[1mStep[0m  [80/84], [94mLoss[0m : 2.12638

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74750
[1mStep[0m  [8/84], [94mLoss[0m : 1.78365
[1mStep[0m  [16/84], [94mLoss[0m : 1.84665
[1mStep[0m  [24/84], [94mLoss[0m : 2.08188
[1mStep[0m  [32/84], [94mLoss[0m : 1.75650
[1mStep[0m  [40/84], [94mLoss[0m : 1.80978
[1mStep[0m  [48/84], [94mLoss[0m : 1.82193
[1mStep[0m  [56/84], [94mLoss[0m : 1.92870
[1mStep[0m  [64/84], [94mLoss[0m : 1.64590
[1mStep[0m  [72/84], [94mLoss[0m : 1.72421
[1mStep[0m  [80/84], [94mLoss[0m : 1.95343

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64965
[1mStep[0m  [8/84], [94mLoss[0m : 1.74507
[1mStep[0m  [16/84], [94mLoss[0m : 1.69408
[1mStep[0m  [24/84], [94mLoss[0m : 1.72542
[1mStep[0m  [32/84], [94mLoss[0m : 1.73055
[1mStep[0m  [40/84], [94mLoss[0m : 1.73154
[1mStep[0m  [48/84], [94mLoss[0m : 1.73185
[1mStep[0m  [56/84], [94mLoss[0m : 2.03710
[1mStep[0m  [64/84], [94mLoss[0m : 1.51895
[1mStep[0m  [72/84], [94mLoss[0m : 1.89254
[1mStep[0m  [80/84], [94mLoss[0m : 1.63522

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.503, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96334
[1mStep[0m  [8/84], [94mLoss[0m : 1.65985
[1mStep[0m  [16/84], [94mLoss[0m : 1.86497
[1mStep[0m  [24/84], [94mLoss[0m : 1.64878
[1mStep[0m  [32/84], [94mLoss[0m : 1.69991
[1mStep[0m  [40/84], [94mLoss[0m : 1.56926
[1mStep[0m  [48/84], [94mLoss[0m : 2.09603
[1mStep[0m  [56/84], [94mLoss[0m : 1.65194
[1mStep[0m  [64/84], [94mLoss[0m : 1.82292
[1mStep[0m  [72/84], [94mLoss[0m : 1.75356
[1mStep[0m  [80/84], [94mLoss[0m : 1.56065

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68824
[1mStep[0m  [8/84], [94mLoss[0m : 1.36949
[1mStep[0m  [16/84], [94mLoss[0m : 1.73308
[1mStep[0m  [24/84], [94mLoss[0m : 1.84880
[1mStep[0m  [32/84], [94mLoss[0m : 1.79381
[1mStep[0m  [40/84], [94mLoss[0m : 1.81866
[1mStep[0m  [48/84], [94mLoss[0m : 1.45091
[1mStep[0m  [56/84], [94mLoss[0m : 1.54898
[1mStep[0m  [64/84], [94mLoss[0m : 1.60283
[1mStep[0m  [72/84], [94mLoss[0m : 1.71250
[1mStep[0m  [80/84], [94mLoss[0m : 1.99918

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52632
[1mStep[0m  [8/84], [94mLoss[0m : 1.75612
[1mStep[0m  [16/84], [94mLoss[0m : 1.56959
[1mStep[0m  [24/84], [94mLoss[0m : 1.50887
[1mStep[0m  [32/84], [94mLoss[0m : 1.74779
[1mStep[0m  [40/84], [94mLoss[0m : 1.80098
[1mStep[0m  [48/84], [94mLoss[0m : 1.82134
[1mStep[0m  [56/84], [94mLoss[0m : 1.67403
[1mStep[0m  [64/84], [94mLoss[0m : 1.74314
[1mStep[0m  [72/84], [94mLoss[0m : 1.71198
[1mStep[0m  [80/84], [94mLoss[0m : 1.53552

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.545, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53453
[1mStep[0m  [8/84], [94mLoss[0m : 1.68519
[1mStep[0m  [16/84], [94mLoss[0m : 1.67859
[1mStep[0m  [24/84], [94mLoss[0m : 1.57201
[1mStep[0m  [32/84], [94mLoss[0m : 1.62750
[1mStep[0m  [40/84], [94mLoss[0m : 1.71637
[1mStep[0m  [48/84], [94mLoss[0m : 1.72788
[1mStep[0m  [56/84], [94mLoss[0m : 1.94511
[1mStep[0m  [64/84], [94mLoss[0m : 1.84393
[1mStep[0m  [72/84], [94mLoss[0m : 1.76031
[1mStep[0m  [80/84], [94mLoss[0m : 1.78443

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41664
[1mStep[0m  [8/84], [94mLoss[0m : 1.41849
[1mStep[0m  [16/84], [94mLoss[0m : 1.45190
[1mStep[0m  [24/84], [94mLoss[0m : 1.72156
[1mStep[0m  [32/84], [94mLoss[0m : 1.45685
[1mStep[0m  [40/84], [94mLoss[0m : 1.80198
[1mStep[0m  [48/84], [94mLoss[0m : 1.68755
[1mStep[0m  [56/84], [94mLoss[0m : 1.61443
[1mStep[0m  [64/84], [94mLoss[0m : 1.92514
[1mStep[0m  [72/84], [94mLoss[0m : 1.85827
[1mStep[0m  [80/84], [94mLoss[0m : 1.86298

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58234
[1mStep[0m  [8/84], [94mLoss[0m : 1.49336
[1mStep[0m  [16/84], [94mLoss[0m : 1.53112
[1mStep[0m  [24/84], [94mLoss[0m : 1.41373
[1mStep[0m  [32/84], [94mLoss[0m : 1.66819
[1mStep[0m  [40/84], [94mLoss[0m : 1.48098
[1mStep[0m  [48/84], [94mLoss[0m : 1.71194
[1mStep[0m  [56/84], [94mLoss[0m : 1.50961
[1mStep[0m  [64/84], [94mLoss[0m : 1.62161
[1mStep[0m  [72/84], [94mLoss[0m : 1.46873
[1mStep[0m  [80/84], [94mLoss[0m : 1.94753

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.615, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52914
[1mStep[0m  [8/84], [94mLoss[0m : 1.48340
[1mStep[0m  [16/84], [94mLoss[0m : 1.30345
[1mStep[0m  [24/84], [94mLoss[0m : 1.52249
[1mStep[0m  [32/84], [94mLoss[0m : 1.56281
[1mStep[0m  [40/84], [94mLoss[0m : 1.53405
[1mStep[0m  [48/84], [94mLoss[0m : 1.47584
[1mStep[0m  [56/84], [94mLoss[0m : 1.70805
[1mStep[0m  [64/84], [94mLoss[0m : 1.41006
[1mStep[0m  [72/84], [94mLoss[0m : 1.84456
[1mStep[0m  [80/84], [94mLoss[0m : 1.75838

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44339
[1mStep[0m  [8/84], [94mLoss[0m : 1.57471
[1mStep[0m  [16/84], [94mLoss[0m : 1.81129
[1mStep[0m  [24/84], [94mLoss[0m : 1.49423
[1mStep[0m  [32/84], [94mLoss[0m : 1.49346
[1mStep[0m  [40/84], [94mLoss[0m : 1.66583
[1mStep[0m  [48/84], [94mLoss[0m : 1.72005
[1mStep[0m  [56/84], [94mLoss[0m : 1.64699
[1mStep[0m  [64/84], [94mLoss[0m : 1.49586
[1mStep[0m  [72/84], [94mLoss[0m : 1.54379
[1mStep[0m  [80/84], [94mLoss[0m : 1.65286

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.533, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69768
[1mStep[0m  [8/84], [94mLoss[0m : 1.56762
[1mStep[0m  [16/84], [94mLoss[0m : 1.47765
[1mStep[0m  [24/84], [94mLoss[0m : 1.68922
[1mStep[0m  [32/84], [94mLoss[0m : 1.38706
[1mStep[0m  [40/84], [94mLoss[0m : 1.66694
[1mStep[0m  [48/84], [94mLoss[0m : 1.63884
[1mStep[0m  [56/84], [94mLoss[0m : 1.67814
[1mStep[0m  [64/84], [94mLoss[0m : 1.69964
[1mStep[0m  [72/84], [94mLoss[0m : 1.53829
[1mStep[0m  [80/84], [94mLoss[0m : 1.64724

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51879
[1mStep[0m  [8/84], [94mLoss[0m : 1.43389
[1mStep[0m  [16/84], [94mLoss[0m : 1.31541
[1mStep[0m  [24/84], [94mLoss[0m : 1.56251
[1mStep[0m  [32/84], [94mLoss[0m : 1.61150
[1mStep[0m  [40/84], [94mLoss[0m : 1.58478
[1mStep[0m  [48/84], [94mLoss[0m : 1.40599
[1mStep[0m  [56/84], [94mLoss[0m : 1.53891
[1mStep[0m  [64/84], [94mLoss[0m : 1.60883
[1mStep[0m  [72/84], [94mLoss[0m : 1.84265
[1mStep[0m  [80/84], [94mLoss[0m : 1.56703

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.539, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28801
[1mStep[0m  [8/84], [94mLoss[0m : 1.43447
[1mStep[0m  [16/84], [94mLoss[0m : 1.54380
[1mStep[0m  [24/84], [94mLoss[0m : 1.43317
[1mStep[0m  [32/84], [94mLoss[0m : 1.40763
[1mStep[0m  [40/84], [94mLoss[0m : 1.51165
[1mStep[0m  [48/84], [94mLoss[0m : 1.71934
[1mStep[0m  [56/84], [94mLoss[0m : 1.52429
[1mStep[0m  [64/84], [94mLoss[0m : 1.58800
[1mStep[0m  [72/84], [94mLoss[0m : 1.59832
[1mStep[0m  [80/84], [94mLoss[0m : 1.43359

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65144
[1mStep[0m  [8/84], [94mLoss[0m : 1.19143
[1mStep[0m  [16/84], [94mLoss[0m : 1.28819
[1mStep[0m  [24/84], [94mLoss[0m : 1.71322
[1mStep[0m  [32/84], [94mLoss[0m : 1.61776
[1mStep[0m  [40/84], [94mLoss[0m : 1.72864
[1mStep[0m  [48/84], [94mLoss[0m : 1.41819
[1mStep[0m  [56/84], [94mLoss[0m : 1.34751
[1mStep[0m  [64/84], [94mLoss[0m : 1.58793
[1mStep[0m  [72/84], [94mLoss[0m : 1.21346
[1mStep[0m  [80/84], [94mLoss[0m : 1.50604

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59816
[1mStep[0m  [8/84], [94mLoss[0m : 1.40496
[1mStep[0m  [16/84], [94mLoss[0m : 1.38203
[1mStep[0m  [24/84], [94mLoss[0m : 1.60975
[1mStep[0m  [32/84], [94mLoss[0m : 1.37380
[1mStep[0m  [40/84], [94mLoss[0m : 1.73554
[1mStep[0m  [48/84], [94mLoss[0m : 1.42828
[1mStep[0m  [56/84], [94mLoss[0m : 1.65562
[1mStep[0m  [64/84], [94mLoss[0m : 1.52854
[1mStep[0m  [72/84], [94mLoss[0m : 1.56510
[1mStep[0m  [80/84], [94mLoss[0m : 1.61839

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32224
[1mStep[0m  [8/84], [94mLoss[0m : 1.34847
[1mStep[0m  [16/84], [94mLoss[0m : 1.40280
[1mStep[0m  [24/84], [94mLoss[0m : 1.38503
[1mStep[0m  [32/84], [94mLoss[0m : 1.41866
[1mStep[0m  [40/84], [94mLoss[0m : 1.69698
[1mStep[0m  [48/84], [94mLoss[0m : 1.62954
[1mStep[0m  [56/84], [94mLoss[0m : 1.44037
[1mStep[0m  [64/84], [94mLoss[0m : 1.63970
[1mStep[0m  [72/84], [94mLoss[0m : 1.64720
[1mStep[0m  [80/84], [94mLoss[0m : 1.64085

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31074
[1mStep[0m  [8/84], [94mLoss[0m : 1.42120
[1mStep[0m  [16/84], [94mLoss[0m : 1.37962
[1mStep[0m  [24/84], [94mLoss[0m : 1.43976
[1mStep[0m  [32/84], [94mLoss[0m : 1.32873
[1mStep[0m  [40/84], [94mLoss[0m : 1.41534
[1mStep[0m  [48/84], [94mLoss[0m : 1.49630
[1mStep[0m  [56/84], [94mLoss[0m : 1.49168
[1mStep[0m  [64/84], [94mLoss[0m : 1.66458
[1mStep[0m  [72/84], [94mLoss[0m : 1.58470
[1mStep[0m  [80/84], [94mLoss[0m : 1.62169

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.522, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.547
====================================

Phase 2 - Evaluation MAE:  2.5468692013195584
MAE score P1        2.338652
MAE score P2        2.546869
loss                1.476077
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay            0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.07422
[1mStep[0m  [4/42], [94mLoss[0m : 9.81693
[1mStep[0m  [8/42], [94mLoss[0m : 8.87403
[1mStep[0m  [12/42], [94mLoss[0m : 8.30068
[1mStep[0m  [16/42], [94mLoss[0m : 7.00711
[1mStep[0m  [20/42], [94mLoss[0m : 5.75631
[1mStep[0m  [24/42], [94mLoss[0m : 5.11203
[1mStep[0m  [28/42], [94mLoss[0m : 4.84470
[1mStep[0m  [32/42], [94mLoss[0m : 3.63835
[1mStep[0m  [36/42], [94mLoss[0m : 3.75086
[1mStep[0m  [40/42], [94mLoss[0m : 2.82529

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.266, [92mTest[0m: 10.982, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78701
[1mStep[0m  [4/42], [94mLoss[0m : 2.79834
[1mStep[0m  [8/42], [94mLoss[0m : 2.59686
[1mStep[0m  [12/42], [94mLoss[0m : 2.64009
[1mStep[0m  [16/42], [94mLoss[0m : 2.58193
[1mStep[0m  [20/42], [94mLoss[0m : 2.60823
[1mStep[0m  [24/42], [94mLoss[0m : 2.68760
[1mStep[0m  [28/42], [94mLoss[0m : 2.42118
[1mStep[0m  [32/42], [94mLoss[0m : 2.45570
[1mStep[0m  [36/42], [94mLoss[0m : 2.49827
[1mStep[0m  [40/42], [94mLoss[0m : 2.53514

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.911, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47052
[1mStep[0m  [4/42], [94mLoss[0m : 2.49482
[1mStep[0m  [8/42], [94mLoss[0m : 2.55231
[1mStep[0m  [12/42], [94mLoss[0m : 2.61463
[1mStep[0m  [16/42], [94mLoss[0m : 2.65410
[1mStep[0m  [20/42], [94mLoss[0m : 2.39478
[1mStep[0m  [24/42], [94mLoss[0m : 2.59049
[1mStep[0m  [28/42], [94mLoss[0m : 2.44510
[1mStep[0m  [32/42], [94mLoss[0m : 2.63908
[1mStep[0m  [36/42], [94mLoss[0m : 2.57803
[1mStep[0m  [40/42], [94mLoss[0m : 2.61359

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41978
[1mStep[0m  [4/42], [94mLoss[0m : 2.46853
[1mStep[0m  [8/42], [94mLoss[0m : 2.45655
[1mStep[0m  [12/42], [94mLoss[0m : 2.41552
[1mStep[0m  [16/42], [94mLoss[0m : 2.46468
[1mStep[0m  [20/42], [94mLoss[0m : 2.48961
[1mStep[0m  [24/42], [94mLoss[0m : 2.50745
[1mStep[0m  [28/42], [94mLoss[0m : 2.36920
[1mStep[0m  [32/42], [94mLoss[0m : 2.37886
[1mStep[0m  [36/42], [94mLoss[0m : 2.40147
[1mStep[0m  [40/42], [94mLoss[0m : 2.38782

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50380
[1mStep[0m  [4/42], [94mLoss[0m : 2.46967
[1mStep[0m  [8/42], [94mLoss[0m : 2.57553
[1mStep[0m  [12/42], [94mLoss[0m : 2.44975
[1mStep[0m  [16/42], [94mLoss[0m : 2.39327
[1mStep[0m  [20/42], [94mLoss[0m : 2.46484
[1mStep[0m  [24/42], [94mLoss[0m : 2.59170
[1mStep[0m  [28/42], [94mLoss[0m : 2.48691
[1mStep[0m  [32/42], [94mLoss[0m : 2.33183
[1mStep[0m  [36/42], [94mLoss[0m : 2.32674
[1mStep[0m  [40/42], [94mLoss[0m : 2.50272

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39792
[1mStep[0m  [4/42], [94mLoss[0m : 2.40458
[1mStep[0m  [8/42], [94mLoss[0m : 2.57488
[1mStep[0m  [12/42], [94mLoss[0m : 2.32165
[1mStep[0m  [16/42], [94mLoss[0m : 2.28410
[1mStep[0m  [20/42], [94mLoss[0m : 2.58995
[1mStep[0m  [24/42], [94mLoss[0m : 2.48748
[1mStep[0m  [28/42], [94mLoss[0m : 2.50577
[1mStep[0m  [32/42], [94mLoss[0m : 2.71299
[1mStep[0m  [36/42], [94mLoss[0m : 2.51940
[1mStep[0m  [40/42], [94mLoss[0m : 2.40623

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27857
[1mStep[0m  [4/42], [94mLoss[0m : 2.50698
[1mStep[0m  [8/42], [94mLoss[0m : 2.59538
[1mStep[0m  [12/42], [94mLoss[0m : 2.65069
[1mStep[0m  [16/42], [94mLoss[0m : 2.34787
[1mStep[0m  [20/42], [94mLoss[0m : 2.29709
[1mStep[0m  [24/42], [94mLoss[0m : 2.39607
[1mStep[0m  [28/42], [94mLoss[0m : 2.32105
[1mStep[0m  [32/42], [94mLoss[0m : 2.37951
[1mStep[0m  [36/42], [94mLoss[0m : 2.76369
[1mStep[0m  [40/42], [94mLoss[0m : 2.39011

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22856
[1mStep[0m  [4/42], [94mLoss[0m : 2.39804
[1mStep[0m  [8/42], [94mLoss[0m : 2.17497
[1mStep[0m  [12/42], [94mLoss[0m : 2.50595
[1mStep[0m  [16/42], [94mLoss[0m : 2.38333
[1mStep[0m  [20/42], [94mLoss[0m : 2.46278
[1mStep[0m  [24/42], [94mLoss[0m : 2.32582
[1mStep[0m  [28/42], [94mLoss[0m : 2.63249
[1mStep[0m  [32/42], [94mLoss[0m : 2.56728
[1mStep[0m  [36/42], [94mLoss[0m : 2.31901
[1mStep[0m  [40/42], [94mLoss[0m : 2.41924

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55171
[1mStep[0m  [4/42], [94mLoss[0m : 2.53102
[1mStep[0m  [8/42], [94mLoss[0m : 2.67720
[1mStep[0m  [12/42], [94mLoss[0m : 2.48819
[1mStep[0m  [16/42], [94mLoss[0m : 2.56120
[1mStep[0m  [20/42], [94mLoss[0m : 2.43478
[1mStep[0m  [24/42], [94mLoss[0m : 2.31719
[1mStep[0m  [28/42], [94mLoss[0m : 2.42967
[1mStep[0m  [32/42], [94mLoss[0m : 2.45303
[1mStep[0m  [36/42], [94mLoss[0m : 2.40688
[1mStep[0m  [40/42], [94mLoss[0m : 2.45839

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40732
[1mStep[0m  [4/42], [94mLoss[0m : 2.42666
[1mStep[0m  [8/42], [94mLoss[0m : 2.47670
[1mStep[0m  [12/42], [94mLoss[0m : 2.52225
[1mStep[0m  [16/42], [94mLoss[0m : 2.57441
[1mStep[0m  [20/42], [94mLoss[0m : 2.59327
[1mStep[0m  [24/42], [94mLoss[0m : 2.51173
[1mStep[0m  [28/42], [94mLoss[0m : 2.38605
[1mStep[0m  [32/42], [94mLoss[0m : 2.33112
[1mStep[0m  [36/42], [94mLoss[0m : 2.34671
[1mStep[0m  [40/42], [94mLoss[0m : 2.46460

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39980
[1mStep[0m  [4/42], [94mLoss[0m : 2.44186
[1mStep[0m  [8/42], [94mLoss[0m : 2.49832
[1mStep[0m  [12/42], [94mLoss[0m : 2.50975
[1mStep[0m  [16/42], [94mLoss[0m : 2.63666
[1mStep[0m  [20/42], [94mLoss[0m : 2.43610
[1mStep[0m  [24/42], [94mLoss[0m : 2.32448
[1mStep[0m  [28/42], [94mLoss[0m : 2.45064
[1mStep[0m  [32/42], [94mLoss[0m : 2.55661
[1mStep[0m  [36/42], [94mLoss[0m : 2.50922
[1mStep[0m  [40/42], [94mLoss[0m : 2.59312

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50021
[1mStep[0m  [4/42], [94mLoss[0m : 2.53755
[1mStep[0m  [8/42], [94mLoss[0m : 2.32399
[1mStep[0m  [12/42], [94mLoss[0m : 2.39299
[1mStep[0m  [16/42], [94mLoss[0m : 2.54076
[1mStep[0m  [20/42], [94mLoss[0m : 2.39616
[1mStep[0m  [24/42], [94mLoss[0m : 2.34727
[1mStep[0m  [28/42], [94mLoss[0m : 2.36187
[1mStep[0m  [32/42], [94mLoss[0m : 2.24032
[1mStep[0m  [36/42], [94mLoss[0m : 2.74459
[1mStep[0m  [40/42], [94mLoss[0m : 2.52075

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41127
[1mStep[0m  [4/42], [94mLoss[0m : 2.42547
[1mStep[0m  [8/42], [94mLoss[0m : 2.31154
[1mStep[0m  [12/42], [94mLoss[0m : 2.53271
[1mStep[0m  [16/42], [94mLoss[0m : 2.38773
[1mStep[0m  [20/42], [94mLoss[0m : 2.62616
[1mStep[0m  [24/42], [94mLoss[0m : 2.49026
[1mStep[0m  [28/42], [94mLoss[0m : 2.30092
[1mStep[0m  [32/42], [94mLoss[0m : 2.82161
[1mStep[0m  [36/42], [94mLoss[0m : 2.44363
[1mStep[0m  [40/42], [94mLoss[0m : 2.49454

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44967
[1mStep[0m  [4/42], [94mLoss[0m : 2.44176
[1mStep[0m  [8/42], [94mLoss[0m : 2.47527
[1mStep[0m  [12/42], [94mLoss[0m : 2.51748
[1mStep[0m  [16/42], [94mLoss[0m : 2.51182
[1mStep[0m  [20/42], [94mLoss[0m : 2.58690
[1mStep[0m  [24/42], [94mLoss[0m : 2.37442
[1mStep[0m  [28/42], [94mLoss[0m : 2.28221
[1mStep[0m  [32/42], [94mLoss[0m : 2.46154
[1mStep[0m  [36/42], [94mLoss[0m : 2.39486
[1mStep[0m  [40/42], [94mLoss[0m : 2.51700

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50684
[1mStep[0m  [4/42], [94mLoss[0m : 2.38021
[1mStep[0m  [8/42], [94mLoss[0m : 2.25160
[1mStep[0m  [12/42], [94mLoss[0m : 2.33003
[1mStep[0m  [16/42], [94mLoss[0m : 2.33098
[1mStep[0m  [20/42], [94mLoss[0m : 2.39706
[1mStep[0m  [24/42], [94mLoss[0m : 2.40921
[1mStep[0m  [28/42], [94mLoss[0m : 2.42763
[1mStep[0m  [32/42], [94mLoss[0m : 2.39235
[1mStep[0m  [36/42], [94mLoss[0m : 2.37308
[1mStep[0m  [40/42], [94mLoss[0m : 2.25728

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87393
[1mStep[0m  [4/42], [94mLoss[0m : 2.42877
[1mStep[0m  [8/42], [94mLoss[0m : 2.24274
[1mStep[0m  [12/42], [94mLoss[0m : 2.60371
[1mStep[0m  [16/42], [94mLoss[0m : 2.38423
[1mStep[0m  [20/42], [94mLoss[0m : 2.35705
[1mStep[0m  [24/42], [94mLoss[0m : 2.42334
[1mStep[0m  [28/42], [94mLoss[0m : 2.33786
[1mStep[0m  [32/42], [94mLoss[0m : 2.47842
[1mStep[0m  [36/42], [94mLoss[0m : 2.32930
[1mStep[0m  [40/42], [94mLoss[0m : 2.27372

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56089
[1mStep[0m  [4/42], [94mLoss[0m : 2.65849
[1mStep[0m  [8/42], [94mLoss[0m : 2.29382
[1mStep[0m  [12/42], [94mLoss[0m : 2.36854
[1mStep[0m  [16/42], [94mLoss[0m : 2.49029
[1mStep[0m  [20/42], [94mLoss[0m : 2.50719
[1mStep[0m  [24/42], [94mLoss[0m : 2.62434
[1mStep[0m  [28/42], [94mLoss[0m : 2.37528
[1mStep[0m  [32/42], [94mLoss[0m : 2.37790
[1mStep[0m  [36/42], [94mLoss[0m : 2.32534
[1mStep[0m  [40/42], [94mLoss[0m : 2.38443

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33804
[1mStep[0m  [4/42], [94mLoss[0m : 2.40551
[1mStep[0m  [8/42], [94mLoss[0m : 2.63431
[1mStep[0m  [12/42], [94mLoss[0m : 2.29330
[1mStep[0m  [16/42], [94mLoss[0m : 2.64583
[1mStep[0m  [20/42], [94mLoss[0m : 2.32779
[1mStep[0m  [24/42], [94mLoss[0m : 2.46409
[1mStep[0m  [28/42], [94mLoss[0m : 2.22803
[1mStep[0m  [32/42], [94mLoss[0m : 2.34500
[1mStep[0m  [36/42], [94mLoss[0m : 2.38963
[1mStep[0m  [40/42], [94mLoss[0m : 2.30862

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24628
[1mStep[0m  [4/42], [94mLoss[0m : 2.63765
[1mStep[0m  [8/42], [94mLoss[0m : 2.47990
[1mStep[0m  [12/42], [94mLoss[0m : 2.46792
[1mStep[0m  [16/42], [94mLoss[0m : 2.51369
[1mStep[0m  [20/42], [94mLoss[0m : 2.47629
[1mStep[0m  [24/42], [94mLoss[0m : 2.52587
[1mStep[0m  [28/42], [94mLoss[0m : 2.50337
[1mStep[0m  [32/42], [94mLoss[0m : 2.48719
[1mStep[0m  [36/42], [94mLoss[0m : 2.29626
[1mStep[0m  [40/42], [94mLoss[0m : 2.53962

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62088
[1mStep[0m  [4/42], [94mLoss[0m : 2.32408
[1mStep[0m  [8/42], [94mLoss[0m : 2.30390
[1mStep[0m  [12/42], [94mLoss[0m : 2.26051
[1mStep[0m  [16/42], [94mLoss[0m : 2.35364
[1mStep[0m  [20/42], [94mLoss[0m : 2.45626
[1mStep[0m  [24/42], [94mLoss[0m : 2.67441
[1mStep[0m  [28/42], [94mLoss[0m : 2.38946
[1mStep[0m  [32/42], [94mLoss[0m : 2.19490
[1mStep[0m  [36/42], [94mLoss[0m : 2.43629
[1mStep[0m  [40/42], [94mLoss[0m : 2.44228

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19720
[1mStep[0m  [4/42], [94mLoss[0m : 2.42916
[1mStep[0m  [8/42], [94mLoss[0m : 2.41652
[1mStep[0m  [12/42], [94mLoss[0m : 2.70590
[1mStep[0m  [16/42], [94mLoss[0m : 2.57283
[1mStep[0m  [20/42], [94mLoss[0m : 2.34877
[1mStep[0m  [24/42], [94mLoss[0m : 2.44506
[1mStep[0m  [28/42], [94mLoss[0m : 2.37979
[1mStep[0m  [32/42], [94mLoss[0m : 2.35185
[1mStep[0m  [36/42], [94mLoss[0m : 2.37785
[1mStep[0m  [40/42], [94mLoss[0m : 2.39008

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49717
[1mStep[0m  [4/42], [94mLoss[0m : 2.46014
[1mStep[0m  [8/42], [94mLoss[0m : 2.45644
[1mStep[0m  [12/42], [94mLoss[0m : 2.54454
[1mStep[0m  [16/42], [94mLoss[0m : 2.30878
[1mStep[0m  [20/42], [94mLoss[0m : 2.58162
[1mStep[0m  [24/42], [94mLoss[0m : 2.43496
[1mStep[0m  [28/42], [94mLoss[0m : 2.46756
[1mStep[0m  [32/42], [94mLoss[0m : 2.42496
[1mStep[0m  [36/42], [94mLoss[0m : 2.20012
[1mStep[0m  [40/42], [94mLoss[0m : 2.61425

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36661
[1mStep[0m  [4/42], [94mLoss[0m : 2.44059
[1mStep[0m  [8/42], [94mLoss[0m : 2.30878
[1mStep[0m  [12/42], [94mLoss[0m : 2.39931
[1mStep[0m  [16/42], [94mLoss[0m : 2.45220
[1mStep[0m  [20/42], [94mLoss[0m : 2.46657
[1mStep[0m  [24/42], [94mLoss[0m : 2.29892
[1mStep[0m  [28/42], [94mLoss[0m : 2.53952
[1mStep[0m  [32/42], [94mLoss[0m : 2.38891
[1mStep[0m  [36/42], [94mLoss[0m : 2.28102
[1mStep[0m  [40/42], [94mLoss[0m : 2.44015

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37561
[1mStep[0m  [4/42], [94mLoss[0m : 2.44596
[1mStep[0m  [8/42], [94mLoss[0m : 2.36207
[1mStep[0m  [12/42], [94mLoss[0m : 2.43805
[1mStep[0m  [16/42], [94mLoss[0m : 2.36033
[1mStep[0m  [20/42], [94mLoss[0m : 2.30021
[1mStep[0m  [24/42], [94mLoss[0m : 2.33314
[1mStep[0m  [28/42], [94mLoss[0m : 2.58479
[1mStep[0m  [32/42], [94mLoss[0m : 2.37104
[1mStep[0m  [36/42], [94mLoss[0m : 2.52568
[1mStep[0m  [40/42], [94mLoss[0m : 2.40058

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42522
[1mStep[0m  [4/42], [94mLoss[0m : 2.25941
[1mStep[0m  [8/42], [94mLoss[0m : 2.42938
[1mStep[0m  [12/42], [94mLoss[0m : 2.38723
[1mStep[0m  [16/42], [94mLoss[0m : 2.65461
[1mStep[0m  [20/42], [94mLoss[0m : 2.36756
[1mStep[0m  [24/42], [94mLoss[0m : 2.59524
[1mStep[0m  [28/42], [94mLoss[0m : 2.49585
[1mStep[0m  [32/42], [94mLoss[0m : 2.32053
[1mStep[0m  [36/42], [94mLoss[0m : 2.56980
[1mStep[0m  [40/42], [94mLoss[0m : 2.49521

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42793
[1mStep[0m  [4/42], [94mLoss[0m : 2.25084
[1mStep[0m  [8/42], [94mLoss[0m : 2.30349
[1mStep[0m  [12/42], [94mLoss[0m : 2.51673
[1mStep[0m  [16/42], [94mLoss[0m : 2.40206
[1mStep[0m  [20/42], [94mLoss[0m : 2.29304
[1mStep[0m  [24/42], [94mLoss[0m : 2.48517
[1mStep[0m  [28/42], [94mLoss[0m : 2.43586
[1mStep[0m  [32/42], [94mLoss[0m : 2.45887
[1mStep[0m  [36/42], [94mLoss[0m : 2.26986
[1mStep[0m  [40/42], [94mLoss[0m : 2.62656

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39860
[1mStep[0m  [4/42], [94mLoss[0m : 2.48624
[1mStep[0m  [8/42], [94mLoss[0m : 2.45125
[1mStep[0m  [12/42], [94mLoss[0m : 2.50945
[1mStep[0m  [16/42], [94mLoss[0m : 2.36643
[1mStep[0m  [20/42], [94mLoss[0m : 2.59593
[1mStep[0m  [24/42], [94mLoss[0m : 2.48843
[1mStep[0m  [28/42], [94mLoss[0m : 2.35129
[1mStep[0m  [32/42], [94mLoss[0m : 2.47575
[1mStep[0m  [36/42], [94mLoss[0m : 2.37503
[1mStep[0m  [40/42], [94mLoss[0m : 2.30449

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35930
[1mStep[0m  [4/42], [94mLoss[0m : 2.55095
[1mStep[0m  [8/42], [94mLoss[0m : 2.66491
[1mStep[0m  [12/42], [94mLoss[0m : 2.46260
[1mStep[0m  [16/42], [94mLoss[0m : 2.40288
[1mStep[0m  [20/42], [94mLoss[0m : 2.31410
[1mStep[0m  [24/42], [94mLoss[0m : 2.59107
[1mStep[0m  [28/42], [94mLoss[0m : 2.48829
[1mStep[0m  [32/42], [94mLoss[0m : 2.52172
[1mStep[0m  [36/42], [94mLoss[0m : 2.39112
[1mStep[0m  [40/42], [94mLoss[0m : 2.24628

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48681
[1mStep[0m  [4/42], [94mLoss[0m : 2.36065
[1mStep[0m  [8/42], [94mLoss[0m : 2.29079
[1mStep[0m  [12/42], [94mLoss[0m : 2.36427
[1mStep[0m  [16/42], [94mLoss[0m : 2.38412
[1mStep[0m  [20/42], [94mLoss[0m : 2.57327
[1mStep[0m  [24/42], [94mLoss[0m : 2.52587
[1mStep[0m  [28/42], [94mLoss[0m : 2.41813
[1mStep[0m  [32/42], [94mLoss[0m : 2.47166
[1mStep[0m  [36/42], [94mLoss[0m : 2.46776
[1mStep[0m  [40/42], [94mLoss[0m : 2.46681

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41318
[1mStep[0m  [4/42], [94mLoss[0m : 2.65780
[1mStep[0m  [8/42], [94mLoss[0m : 2.34321
[1mStep[0m  [12/42], [94mLoss[0m : 2.32852
[1mStep[0m  [16/42], [94mLoss[0m : 2.52634
[1mStep[0m  [20/42], [94mLoss[0m : 2.60371
[1mStep[0m  [24/42], [94mLoss[0m : 2.47275
[1mStep[0m  [28/42], [94mLoss[0m : 2.37605
[1mStep[0m  [32/42], [94mLoss[0m : 2.40040
[1mStep[0m  [36/42], [94mLoss[0m : 2.30245
[1mStep[0m  [40/42], [94mLoss[0m : 2.42881

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3262148244040355
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.63806
[1mStep[0m  [4/42], [94mLoss[0m : 2.30258
[1mStep[0m  [8/42], [94mLoss[0m : 2.44795
[1mStep[0m  [12/42], [94mLoss[0m : 2.54119
[1mStep[0m  [16/42], [94mLoss[0m : 2.49635
[1mStep[0m  [20/42], [94mLoss[0m : 2.21823
[1mStep[0m  [24/42], [94mLoss[0m : 2.47087
[1mStep[0m  [28/42], [94mLoss[0m : 2.26196
[1mStep[0m  [32/42], [94mLoss[0m : 2.39854
[1mStep[0m  [36/42], [94mLoss[0m : 2.43524
[1mStep[0m  [40/42], [94mLoss[0m : 2.43415

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.321, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53985
[1mStep[0m  [4/42], [94mLoss[0m : 2.03914
[1mStep[0m  [8/42], [94mLoss[0m : 2.46415
[1mStep[0m  [12/42], [94mLoss[0m : 2.56582
[1mStep[0m  [16/42], [94mLoss[0m : 2.37524
[1mStep[0m  [20/42], [94mLoss[0m : 2.46214
[1mStep[0m  [24/42], [94mLoss[0m : 2.53489
[1mStep[0m  [28/42], [94mLoss[0m : 2.50682
[1mStep[0m  [32/42], [94mLoss[0m : 2.35009
[1mStep[0m  [36/42], [94mLoss[0m : 2.52840
[1mStep[0m  [40/42], [94mLoss[0m : 2.50291

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51493
[1mStep[0m  [4/42], [94mLoss[0m : 2.62971
[1mStep[0m  [8/42], [94mLoss[0m : 2.42601
[1mStep[0m  [12/42], [94mLoss[0m : 2.51407
[1mStep[0m  [16/42], [94mLoss[0m : 2.37905
[1mStep[0m  [20/42], [94mLoss[0m : 2.41091
[1mStep[0m  [24/42], [94mLoss[0m : 2.48179
[1mStep[0m  [28/42], [94mLoss[0m : 2.28222
[1mStep[0m  [32/42], [94mLoss[0m : 2.49088
[1mStep[0m  [36/42], [94mLoss[0m : 2.33653
[1mStep[0m  [40/42], [94mLoss[0m : 2.17161

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31532
[1mStep[0m  [4/42], [94mLoss[0m : 2.29781
[1mStep[0m  [8/42], [94mLoss[0m : 2.23964
[1mStep[0m  [12/42], [94mLoss[0m : 2.61654
[1mStep[0m  [16/42], [94mLoss[0m : 2.41676
[1mStep[0m  [20/42], [94mLoss[0m : 2.25379
[1mStep[0m  [24/42], [94mLoss[0m : 2.46077
[1mStep[0m  [28/42], [94mLoss[0m : 2.41692
[1mStep[0m  [32/42], [94mLoss[0m : 2.26832
[1mStep[0m  [36/42], [94mLoss[0m : 2.38555
[1mStep[0m  [40/42], [94mLoss[0m : 2.15950

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27226
[1mStep[0m  [4/42], [94mLoss[0m : 2.29490
[1mStep[0m  [8/42], [94mLoss[0m : 2.36310
[1mStep[0m  [12/42], [94mLoss[0m : 2.24026
[1mStep[0m  [16/42], [94mLoss[0m : 2.30346
[1mStep[0m  [20/42], [94mLoss[0m : 2.05161
[1mStep[0m  [24/42], [94mLoss[0m : 2.19876
[1mStep[0m  [28/42], [94mLoss[0m : 2.44383
[1mStep[0m  [32/42], [94mLoss[0m : 2.17105
[1mStep[0m  [36/42], [94mLoss[0m : 2.41376
[1mStep[0m  [40/42], [94mLoss[0m : 2.25812

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13413
[1mStep[0m  [4/42], [94mLoss[0m : 2.30200
[1mStep[0m  [8/42], [94mLoss[0m : 2.59033
[1mStep[0m  [12/42], [94mLoss[0m : 2.31340
[1mStep[0m  [16/42], [94mLoss[0m : 2.29698
[1mStep[0m  [20/42], [94mLoss[0m : 2.21571
[1mStep[0m  [24/42], [94mLoss[0m : 2.15921
[1mStep[0m  [28/42], [94mLoss[0m : 2.23081
[1mStep[0m  [32/42], [94mLoss[0m : 2.30770
[1mStep[0m  [36/42], [94mLoss[0m : 2.29367
[1mStep[0m  [40/42], [94mLoss[0m : 2.20190

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.307, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32237
[1mStep[0m  [4/42], [94mLoss[0m : 2.55596
[1mStep[0m  [8/42], [94mLoss[0m : 2.08544
[1mStep[0m  [12/42], [94mLoss[0m : 2.05170
[1mStep[0m  [16/42], [94mLoss[0m : 2.30960
[1mStep[0m  [20/42], [94mLoss[0m : 2.27147
[1mStep[0m  [24/42], [94mLoss[0m : 2.28026
[1mStep[0m  [28/42], [94mLoss[0m : 2.32437
[1mStep[0m  [32/42], [94mLoss[0m : 2.35986
[1mStep[0m  [36/42], [94mLoss[0m : 2.06349
[1mStep[0m  [40/42], [94mLoss[0m : 2.26537

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12224
[1mStep[0m  [4/42], [94mLoss[0m : 2.36538
[1mStep[0m  [8/42], [94mLoss[0m : 2.30329
[1mStep[0m  [12/42], [94mLoss[0m : 2.19009
[1mStep[0m  [16/42], [94mLoss[0m : 2.36611
[1mStep[0m  [20/42], [94mLoss[0m : 2.08244
[1mStep[0m  [24/42], [94mLoss[0m : 2.16835
[1mStep[0m  [28/42], [94mLoss[0m : 2.12606
[1mStep[0m  [32/42], [94mLoss[0m : 2.53475
[1mStep[0m  [36/42], [94mLoss[0m : 2.37981
[1mStep[0m  [40/42], [94mLoss[0m : 2.23528

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06219
[1mStep[0m  [4/42], [94mLoss[0m : 2.13277
[1mStep[0m  [8/42], [94mLoss[0m : 2.14524
[1mStep[0m  [12/42], [94mLoss[0m : 2.09801
[1mStep[0m  [16/42], [94mLoss[0m : 2.27446
[1mStep[0m  [20/42], [94mLoss[0m : 2.27897
[1mStep[0m  [24/42], [94mLoss[0m : 2.06732
[1mStep[0m  [28/42], [94mLoss[0m : 2.12293
[1mStep[0m  [32/42], [94mLoss[0m : 2.22385
[1mStep[0m  [36/42], [94mLoss[0m : 1.98189
[1mStep[0m  [40/42], [94mLoss[0m : 2.29915

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00461
[1mStep[0m  [4/42], [94mLoss[0m : 2.17267
[1mStep[0m  [8/42], [94mLoss[0m : 2.20964
[1mStep[0m  [12/42], [94mLoss[0m : 2.19587
[1mStep[0m  [16/42], [94mLoss[0m : 2.13296
[1mStep[0m  [20/42], [94mLoss[0m : 2.10799
[1mStep[0m  [24/42], [94mLoss[0m : 2.26841
[1mStep[0m  [28/42], [94mLoss[0m : 2.13720
[1mStep[0m  [32/42], [94mLoss[0m : 2.23471
[1mStep[0m  [36/42], [94mLoss[0m : 2.17776
[1mStep[0m  [40/42], [94mLoss[0m : 2.27173

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21554
[1mStep[0m  [4/42], [94mLoss[0m : 2.19246
[1mStep[0m  [8/42], [94mLoss[0m : 2.06151
[1mStep[0m  [12/42], [94mLoss[0m : 1.95473
[1mStep[0m  [16/42], [94mLoss[0m : 2.09840
[1mStep[0m  [20/42], [94mLoss[0m : 2.03004
[1mStep[0m  [24/42], [94mLoss[0m : 1.95839
[1mStep[0m  [28/42], [94mLoss[0m : 2.06913
[1mStep[0m  [32/42], [94mLoss[0m : 2.02233
[1mStep[0m  [36/42], [94mLoss[0m : 2.08007
[1mStep[0m  [40/42], [94mLoss[0m : 2.27372

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04286
[1mStep[0m  [4/42], [94mLoss[0m : 1.95261
[1mStep[0m  [8/42], [94mLoss[0m : 2.06624
[1mStep[0m  [12/42], [94mLoss[0m : 2.01861
[1mStep[0m  [16/42], [94mLoss[0m : 2.07249
[1mStep[0m  [20/42], [94mLoss[0m : 2.01690
[1mStep[0m  [24/42], [94mLoss[0m : 2.07689
[1mStep[0m  [28/42], [94mLoss[0m : 2.04354
[1mStep[0m  [32/42], [94mLoss[0m : 1.89364
[1mStep[0m  [36/42], [94mLoss[0m : 1.91041
[1mStep[0m  [40/42], [94mLoss[0m : 2.03044

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.055, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91801
[1mStep[0m  [4/42], [94mLoss[0m : 2.04384
[1mStep[0m  [8/42], [94mLoss[0m : 1.85536
[1mStep[0m  [12/42], [94mLoss[0m : 2.02786
[1mStep[0m  [16/42], [94mLoss[0m : 1.87122
[1mStep[0m  [20/42], [94mLoss[0m : 2.06362
[1mStep[0m  [24/42], [94mLoss[0m : 2.10713
[1mStep[0m  [28/42], [94mLoss[0m : 1.99953
[1mStep[0m  [32/42], [94mLoss[0m : 2.03301
[1mStep[0m  [36/42], [94mLoss[0m : 2.08650
[1mStep[0m  [40/42], [94mLoss[0m : 1.87776

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04477
[1mStep[0m  [4/42], [94mLoss[0m : 1.99447
[1mStep[0m  [8/42], [94mLoss[0m : 1.87181
[1mStep[0m  [12/42], [94mLoss[0m : 1.96387
[1mStep[0m  [16/42], [94mLoss[0m : 1.99369
[1mStep[0m  [20/42], [94mLoss[0m : 1.91804
[1mStep[0m  [24/42], [94mLoss[0m : 1.94835
[1mStep[0m  [28/42], [94mLoss[0m : 1.96705
[1mStep[0m  [32/42], [94mLoss[0m : 2.02260
[1mStep[0m  [36/42], [94mLoss[0m : 2.02964
[1mStep[0m  [40/42], [94mLoss[0m : 1.98569

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11154
[1mStep[0m  [4/42], [94mLoss[0m : 1.80040
[1mStep[0m  [8/42], [94mLoss[0m : 1.97415
[1mStep[0m  [12/42], [94mLoss[0m : 1.95133
[1mStep[0m  [16/42], [94mLoss[0m : 2.16404
[1mStep[0m  [20/42], [94mLoss[0m : 1.97330
[1mStep[0m  [24/42], [94mLoss[0m : 1.84099
[1mStep[0m  [28/42], [94mLoss[0m : 2.05122
[1mStep[0m  [32/42], [94mLoss[0m : 1.86033
[1mStep[0m  [36/42], [94mLoss[0m : 1.99033
[1mStep[0m  [40/42], [94mLoss[0m : 1.94231

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76445
[1mStep[0m  [4/42], [94mLoss[0m : 1.90634
[1mStep[0m  [8/42], [94mLoss[0m : 1.91430
[1mStep[0m  [12/42], [94mLoss[0m : 1.87814
[1mStep[0m  [16/42], [94mLoss[0m : 2.07552
[1mStep[0m  [20/42], [94mLoss[0m : 1.90447
[1mStep[0m  [24/42], [94mLoss[0m : 1.82364
[1mStep[0m  [28/42], [94mLoss[0m : 1.85824
[1mStep[0m  [32/42], [94mLoss[0m : 2.04431
[1mStep[0m  [36/42], [94mLoss[0m : 1.86821
[1mStep[0m  [40/42], [94mLoss[0m : 1.80026

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90544
[1mStep[0m  [4/42], [94mLoss[0m : 1.79672
[1mStep[0m  [8/42], [94mLoss[0m : 1.71725
[1mStep[0m  [12/42], [94mLoss[0m : 1.93843
[1mStep[0m  [16/42], [94mLoss[0m : 1.77247
[1mStep[0m  [20/42], [94mLoss[0m : 1.95081
[1mStep[0m  [24/42], [94mLoss[0m : 2.03266
[1mStep[0m  [28/42], [94mLoss[0m : 1.99017
[1mStep[0m  [32/42], [94mLoss[0m : 1.79616
[1mStep[0m  [36/42], [94mLoss[0m : 1.93414
[1mStep[0m  [40/42], [94mLoss[0m : 1.89498

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74064
[1mStep[0m  [4/42], [94mLoss[0m : 1.70070
[1mStep[0m  [8/42], [94mLoss[0m : 1.72623
[1mStep[0m  [12/42], [94mLoss[0m : 1.89024
[1mStep[0m  [16/42], [94mLoss[0m : 1.87793
[1mStep[0m  [20/42], [94mLoss[0m : 1.84714
[1mStep[0m  [24/42], [94mLoss[0m : 1.92548
[1mStep[0m  [28/42], [94mLoss[0m : 1.75560
[1mStep[0m  [32/42], [94mLoss[0m : 1.87477
[1mStep[0m  [36/42], [94mLoss[0m : 1.75012
[1mStep[0m  [40/42], [94mLoss[0m : 2.01401

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75193
[1mStep[0m  [4/42], [94mLoss[0m : 1.66538
[1mStep[0m  [8/42], [94mLoss[0m : 1.67054
[1mStep[0m  [12/42], [94mLoss[0m : 1.77336
[1mStep[0m  [16/42], [94mLoss[0m : 1.82372
[1mStep[0m  [20/42], [94mLoss[0m : 1.76178
[1mStep[0m  [24/42], [94mLoss[0m : 1.81644
[1mStep[0m  [28/42], [94mLoss[0m : 1.89473
[1mStep[0m  [32/42], [94mLoss[0m : 1.76122
[1mStep[0m  [36/42], [94mLoss[0m : 1.70071
[1mStep[0m  [40/42], [94mLoss[0m : 1.71923

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72797
[1mStep[0m  [4/42], [94mLoss[0m : 1.69624
[1mStep[0m  [8/42], [94mLoss[0m : 1.78484
[1mStep[0m  [12/42], [94mLoss[0m : 1.59165
[1mStep[0m  [16/42], [94mLoss[0m : 1.80339
[1mStep[0m  [20/42], [94mLoss[0m : 1.84963
[1mStep[0m  [24/42], [94mLoss[0m : 1.95003
[1mStep[0m  [28/42], [94mLoss[0m : 1.79373
[1mStep[0m  [32/42], [94mLoss[0m : 1.80233
[1mStep[0m  [36/42], [94mLoss[0m : 1.62490
[1mStep[0m  [40/42], [94mLoss[0m : 1.81901

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68972
[1mStep[0m  [4/42], [94mLoss[0m : 1.75670
[1mStep[0m  [8/42], [94mLoss[0m : 1.74906
[1mStep[0m  [12/42], [94mLoss[0m : 1.69676
[1mStep[0m  [16/42], [94mLoss[0m : 1.74196
[1mStep[0m  [20/42], [94mLoss[0m : 1.85623
[1mStep[0m  [24/42], [94mLoss[0m : 1.66086
[1mStep[0m  [28/42], [94mLoss[0m : 1.69473
[1mStep[0m  [32/42], [94mLoss[0m : 1.65878
[1mStep[0m  [36/42], [94mLoss[0m : 1.75645
[1mStep[0m  [40/42], [94mLoss[0m : 1.74072

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54329
[1mStep[0m  [4/42], [94mLoss[0m : 1.48853
[1mStep[0m  [8/42], [94mLoss[0m : 1.48495
[1mStep[0m  [12/42], [94mLoss[0m : 1.75323
[1mStep[0m  [16/42], [94mLoss[0m : 1.72767
[1mStep[0m  [20/42], [94mLoss[0m : 1.65088
[1mStep[0m  [24/42], [94mLoss[0m : 1.43341
[1mStep[0m  [28/42], [94mLoss[0m : 1.79747
[1mStep[0m  [32/42], [94mLoss[0m : 1.72872
[1mStep[0m  [36/42], [94mLoss[0m : 1.73636
[1mStep[0m  [40/42], [94mLoss[0m : 1.77067

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50862
[1mStep[0m  [4/42], [94mLoss[0m : 1.65527
[1mStep[0m  [8/42], [94mLoss[0m : 1.69181
[1mStep[0m  [12/42], [94mLoss[0m : 1.55030
[1mStep[0m  [16/42], [94mLoss[0m : 1.72725
[1mStep[0m  [20/42], [94mLoss[0m : 1.60234
[1mStep[0m  [24/42], [94mLoss[0m : 1.58316
[1mStep[0m  [28/42], [94mLoss[0m : 1.60202
[1mStep[0m  [32/42], [94mLoss[0m : 1.63668
[1mStep[0m  [36/42], [94mLoss[0m : 1.60457
[1mStep[0m  [40/42], [94mLoss[0m : 1.63630

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72506
[1mStep[0m  [4/42], [94mLoss[0m : 1.59161
[1mStep[0m  [8/42], [94mLoss[0m : 1.53654
[1mStep[0m  [12/42], [94mLoss[0m : 1.51742
[1mStep[0m  [16/42], [94mLoss[0m : 1.56684
[1mStep[0m  [20/42], [94mLoss[0m : 1.57216
[1mStep[0m  [24/42], [94mLoss[0m : 1.53937
[1mStep[0m  [28/42], [94mLoss[0m : 1.66503
[1mStep[0m  [32/42], [94mLoss[0m : 1.49531
[1mStep[0m  [36/42], [94mLoss[0m : 1.64038
[1mStep[0m  [40/42], [94mLoss[0m : 1.56400

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48370
[1mStep[0m  [4/42], [94mLoss[0m : 1.48709
[1mStep[0m  [8/42], [94mLoss[0m : 1.42407
[1mStep[0m  [12/42], [94mLoss[0m : 1.57201
[1mStep[0m  [16/42], [94mLoss[0m : 1.70944
[1mStep[0m  [20/42], [94mLoss[0m : 1.52708
[1mStep[0m  [24/42], [94mLoss[0m : 1.61885
[1mStep[0m  [28/42], [94mLoss[0m : 1.60648
[1mStep[0m  [32/42], [94mLoss[0m : 1.55280
[1mStep[0m  [36/42], [94mLoss[0m : 1.57318
[1mStep[0m  [40/42], [94mLoss[0m : 1.55858

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46822
[1mStep[0m  [4/42], [94mLoss[0m : 1.51057
[1mStep[0m  [8/42], [94mLoss[0m : 1.76267
[1mStep[0m  [12/42], [94mLoss[0m : 1.48907
[1mStep[0m  [16/42], [94mLoss[0m : 1.62865
[1mStep[0m  [20/42], [94mLoss[0m : 1.47498
[1mStep[0m  [24/42], [94mLoss[0m : 1.55791
[1mStep[0m  [28/42], [94mLoss[0m : 1.55282
[1mStep[0m  [32/42], [94mLoss[0m : 1.50345
[1mStep[0m  [36/42], [94mLoss[0m : 1.66589
[1mStep[0m  [40/42], [94mLoss[0m : 1.55709

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60388
[1mStep[0m  [4/42], [94mLoss[0m : 1.47133
[1mStep[0m  [8/42], [94mLoss[0m : 1.57334
[1mStep[0m  [12/42], [94mLoss[0m : 1.56663
[1mStep[0m  [16/42], [94mLoss[0m : 1.36586
[1mStep[0m  [20/42], [94mLoss[0m : 1.46372
[1mStep[0m  [24/42], [94mLoss[0m : 1.64589
[1mStep[0m  [28/42], [94mLoss[0m : 1.69067
[1mStep[0m  [32/42], [94mLoss[0m : 1.72898
[1mStep[0m  [36/42], [94mLoss[0m : 1.66000
[1mStep[0m  [40/42], [94mLoss[0m : 1.55652

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62669
[1mStep[0m  [4/42], [94mLoss[0m : 1.61805
[1mStep[0m  [8/42], [94mLoss[0m : 1.47121
[1mStep[0m  [12/42], [94mLoss[0m : 1.43126
[1mStep[0m  [16/42], [94mLoss[0m : 1.68859
[1mStep[0m  [20/42], [94mLoss[0m : 1.36964
[1mStep[0m  [24/42], [94mLoss[0m : 1.46325
[1mStep[0m  [28/42], [94mLoss[0m : 1.52338
[1mStep[0m  [32/42], [94mLoss[0m : 1.46818
[1mStep[0m  [36/42], [94mLoss[0m : 1.43580
[1mStep[0m  [40/42], [94mLoss[0m : 1.36772

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.524, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46835
[1mStep[0m  [4/42], [94mLoss[0m : 1.47784
[1mStep[0m  [8/42], [94mLoss[0m : 1.47207
[1mStep[0m  [12/42], [94mLoss[0m : 1.30132
[1mStep[0m  [16/42], [94mLoss[0m : 1.37520
[1mStep[0m  [20/42], [94mLoss[0m : 1.44209
[1mStep[0m  [24/42], [94mLoss[0m : 1.57806
[1mStep[0m  [28/42], [94mLoss[0m : 1.48648
[1mStep[0m  [32/42], [94mLoss[0m : 1.45932
[1mStep[0m  [36/42], [94mLoss[0m : 1.58840
[1mStep[0m  [40/42], [94mLoss[0m : 1.59363

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36204
[1mStep[0m  [4/42], [94mLoss[0m : 1.48361
[1mStep[0m  [8/42], [94mLoss[0m : 1.46721
[1mStep[0m  [12/42], [94mLoss[0m : 1.43199
[1mStep[0m  [16/42], [94mLoss[0m : 1.44760
[1mStep[0m  [20/42], [94mLoss[0m : 1.40341
[1mStep[0m  [24/42], [94mLoss[0m : 1.46356
[1mStep[0m  [28/42], [94mLoss[0m : 1.58566
[1mStep[0m  [32/42], [94mLoss[0m : 1.41984
[1mStep[0m  [36/42], [94mLoss[0m : 1.56108
[1mStep[0m  [40/42], [94mLoss[0m : 1.40890

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.480, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.465
====================================

Phase 2 - Evaluation MAE:  2.4649145092282976
MAE score P1      2.326215
MAE score P2      2.464915
loss              1.479792
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.18228
[1mStep[0m  [4/42], [94mLoss[0m : 9.90485
[1mStep[0m  [8/42], [94mLoss[0m : 9.31347
[1mStep[0m  [12/42], [94mLoss[0m : 8.70995
[1mStep[0m  [16/42], [94mLoss[0m : 7.92962
[1mStep[0m  [20/42], [94mLoss[0m : 7.02706
[1mStep[0m  [24/42], [94mLoss[0m : 6.81059
[1mStep[0m  [28/42], [94mLoss[0m : 6.05339
[1mStep[0m  [32/42], [94mLoss[0m : 5.57777
[1mStep[0m  [36/42], [94mLoss[0m : 5.00674
[1mStep[0m  [40/42], [94mLoss[0m : 4.25820

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.317, [92mTest[0m: 10.837, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.09408
[1mStep[0m  [4/42], [94mLoss[0m : 4.02670
[1mStep[0m  [8/42], [94mLoss[0m : 3.73572
[1mStep[0m  [12/42], [94mLoss[0m : 3.39706
[1mStep[0m  [16/42], [94mLoss[0m : 3.18622
[1mStep[0m  [20/42], [94mLoss[0m : 3.04925
[1mStep[0m  [24/42], [94mLoss[0m : 2.76562
[1mStep[0m  [28/42], [94mLoss[0m : 2.96203
[1mStep[0m  [32/42], [94mLoss[0m : 2.82729
[1mStep[0m  [36/42], [94mLoss[0m : 2.71603
[1mStep[0m  [40/42], [94mLoss[0m : 2.56632

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.185, [92mTest[0m: 6.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.93163
[1mStep[0m  [4/42], [94mLoss[0m : 2.80630
[1mStep[0m  [8/42], [94mLoss[0m : 2.92782
[1mStep[0m  [12/42], [94mLoss[0m : 2.69560
[1mStep[0m  [16/42], [94mLoss[0m : 2.74689
[1mStep[0m  [20/42], [94mLoss[0m : 2.70982
[1mStep[0m  [24/42], [94mLoss[0m : 2.62010
[1mStep[0m  [28/42], [94mLoss[0m : 2.59317
[1mStep[0m  [32/42], [94mLoss[0m : 2.49717
[1mStep[0m  [36/42], [94mLoss[0m : 2.78837
[1mStep[0m  [40/42], [94mLoss[0m : 2.59637

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.749, [92mTest[0m: 3.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64839
[1mStep[0m  [4/42], [94mLoss[0m : 2.60764
[1mStep[0m  [8/42], [94mLoss[0m : 2.93879
[1mStep[0m  [12/42], [94mLoss[0m : 2.60797
[1mStep[0m  [16/42], [94mLoss[0m : 2.71427
[1mStep[0m  [20/42], [94mLoss[0m : 2.57828
[1mStep[0m  [24/42], [94mLoss[0m : 2.59859
[1mStep[0m  [28/42], [94mLoss[0m : 2.63273
[1mStep[0m  [32/42], [94mLoss[0m : 2.42541
[1mStep[0m  [36/42], [94mLoss[0m : 2.66357
[1mStep[0m  [40/42], [94mLoss[0m : 2.61658

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.666, [92mTest[0m: 3.072, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70934
[1mStep[0m  [4/42], [94mLoss[0m : 2.68580
[1mStep[0m  [8/42], [94mLoss[0m : 2.40012
[1mStep[0m  [12/42], [94mLoss[0m : 2.69921
[1mStep[0m  [16/42], [94mLoss[0m : 2.54119
[1mStep[0m  [20/42], [94mLoss[0m : 2.60011
[1mStep[0m  [24/42], [94mLoss[0m : 2.46878
[1mStep[0m  [28/42], [94mLoss[0m : 2.54658
[1mStep[0m  [32/42], [94mLoss[0m : 2.62667
[1mStep[0m  [36/42], [94mLoss[0m : 2.84855
[1mStep[0m  [40/42], [94mLoss[0m : 2.47685

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.807, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48899
[1mStep[0m  [4/42], [94mLoss[0m : 2.53195
[1mStep[0m  [8/42], [94mLoss[0m : 2.59461
[1mStep[0m  [12/42], [94mLoss[0m : 2.54294
[1mStep[0m  [16/42], [94mLoss[0m : 2.64789
[1mStep[0m  [20/42], [94mLoss[0m : 2.41735
[1mStep[0m  [24/42], [94mLoss[0m : 2.84792
[1mStep[0m  [28/42], [94mLoss[0m : 2.74554
[1mStep[0m  [32/42], [94mLoss[0m : 2.57266
[1mStep[0m  [36/42], [94mLoss[0m : 2.77169
[1mStep[0m  [40/42], [94mLoss[0m : 2.69631

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.705, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84249
[1mStep[0m  [4/42], [94mLoss[0m : 2.50263
[1mStep[0m  [8/42], [94mLoss[0m : 2.57527
[1mStep[0m  [12/42], [94mLoss[0m : 2.53657
[1mStep[0m  [16/42], [94mLoss[0m : 2.62120
[1mStep[0m  [20/42], [94mLoss[0m : 2.52777
[1mStep[0m  [24/42], [94mLoss[0m : 2.74230
[1mStep[0m  [28/42], [94mLoss[0m : 2.68758
[1mStep[0m  [32/42], [94mLoss[0m : 2.53097
[1mStep[0m  [36/42], [94mLoss[0m : 2.62183
[1mStep[0m  [40/42], [94mLoss[0m : 2.56473

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.652, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78714
[1mStep[0m  [4/42], [94mLoss[0m : 2.71057
[1mStep[0m  [8/42], [94mLoss[0m : 2.57590
[1mStep[0m  [12/42], [94mLoss[0m : 2.53789
[1mStep[0m  [16/42], [94mLoss[0m : 2.49869
[1mStep[0m  [20/42], [94mLoss[0m : 2.49119
[1mStep[0m  [24/42], [94mLoss[0m : 2.43524
[1mStep[0m  [28/42], [94mLoss[0m : 2.63225
[1mStep[0m  [32/42], [94mLoss[0m : 2.46125
[1mStep[0m  [36/42], [94mLoss[0m : 2.64367
[1mStep[0m  [40/42], [94mLoss[0m : 2.66603

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.626, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56323
[1mStep[0m  [4/42], [94mLoss[0m : 2.49945
[1mStep[0m  [8/42], [94mLoss[0m : 2.52585
[1mStep[0m  [12/42], [94mLoss[0m : 2.61202
[1mStep[0m  [16/42], [94mLoss[0m : 2.50412
[1mStep[0m  [20/42], [94mLoss[0m : 2.50820
[1mStep[0m  [24/42], [94mLoss[0m : 2.52054
[1mStep[0m  [28/42], [94mLoss[0m : 2.63441
[1mStep[0m  [32/42], [94mLoss[0m : 2.59189
[1mStep[0m  [36/42], [94mLoss[0m : 2.42572
[1mStep[0m  [40/42], [94mLoss[0m : 2.59177

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48363
[1mStep[0m  [4/42], [94mLoss[0m : 2.37373
[1mStep[0m  [8/42], [94mLoss[0m : 2.67606
[1mStep[0m  [12/42], [94mLoss[0m : 2.54186
[1mStep[0m  [16/42], [94mLoss[0m : 2.52392
[1mStep[0m  [20/42], [94mLoss[0m : 2.72311
[1mStep[0m  [24/42], [94mLoss[0m : 2.72454
[1mStep[0m  [28/42], [94mLoss[0m : 2.44033
[1mStep[0m  [32/42], [94mLoss[0m : 2.44857
[1mStep[0m  [36/42], [94mLoss[0m : 2.60986
[1mStep[0m  [40/42], [94mLoss[0m : 2.46023

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60449
[1mStep[0m  [4/42], [94mLoss[0m : 2.59219
[1mStep[0m  [8/42], [94mLoss[0m : 2.65657
[1mStep[0m  [12/42], [94mLoss[0m : 2.73078
[1mStep[0m  [16/42], [94mLoss[0m : 2.49403
[1mStep[0m  [20/42], [94mLoss[0m : 2.54960
[1mStep[0m  [24/42], [94mLoss[0m : 2.77112
[1mStep[0m  [28/42], [94mLoss[0m : 2.53391
[1mStep[0m  [32/42], [94mLoss[0m : 2.52190
[1mStep[0m  [36/42], [94mLoss[0m : 2.48624
[1mStep[0m  [40/42], [94mLoss[0m : 2.70266

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73145
[1mStep[0m  [4/42], [94mLoss[0m : 2.74394
[1mStep[0m  [8/42], [94mLoss[0m : 2.38862
[1mStep[0m  [12/42], [94mLoss[0m : 2.36386
[1mStep[0m  [16/42], [94mLoss[0m : 2.55939
[1mStep[0m  [20/42], [94mLoss[0m : 2.61733
[1mStep[0m  [24/42], [94mLoss[0m : 2.54224
[1mStep[0m  [28/42], [94mLoss[0m : 2.51820
[1mStep[0m  [32/42], [94mLoss[0m : 2.39276
[1mStep[0m  [36/42], [94mLoss[0m : 2.76006
[1mStep[0m  [40/42], [94mLoss[0m : 2.70714

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.576, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65740
[1mStep[0m  [4/42], [94mLoss[0m : 2.52480
[1mStep[0m  [8/42], [94mLoss[0m : 2.42036
[1mStep[0m  [12/42], [94mLoss[0m : 2.53797
[1mStep[0m  [16/42], [94mLoss[0m : 2.54535
[1mStep[0m  [20/42], [94mLoss[0m : 2.46577
[1mStep[0m  [24/42], [94mLoss[0m : 2.68959
[1mStep[0m  [28/42], [94mLoss[0m : 2.70736
[1mStep[0m  [32/42], [94mLoss[0m : 2.55994
[1mStep[0m  [36/42], [94mLoss[0m : 2.53941
[1mStep[0m  [40/42], [94mLoss[0m : 2.46487

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59475
[1mStep[0m  [4/42], [94mLoss[0m : 2.56435
[1mStep[0m  [8/42], [94mLoss[0m : 2.72153
[1mStep[0m  [12/42], [94mLoss[0m : 2.68472
[1mStep[0m  [16/42], [94mLoss[0m : 2.59883
[1mStep[0m  [20/42], [94mLoss[0m : 2.41016
[1mStep[0m  [24/42], [94mLoss[0m : 2.55672
[1mStep[0m  [28/42], [94mLoss[0m : 2.20282
[1mStep[0m  [32/42], [94mLoss[0m : 2.45911
[1mStep[0m  [36/42], [94mLoss[0m : 2.68894
[1mStep[0m  [40/42], [94mLoss[0m : 2.53809

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46943
[1mStep[0m  [4/42], [94mLoss[0m : 2.54337
[1mStep[0m  [8/42], [94mLoss[0m : 2.73564
[1mStep[0m  [12/42], [94mLoss[0m : 2.49819
[1mStep[0m  [16/42], [94mLoss[0m : 2.57569
[1mStep[0m  [20/42], [94mLoss[0m : 2.34893
[1mStep[0m  [24/42], [94mLoss[0m : 2.43109
[1mStep[0m  [28/42], [94mLoss[0m : 2.41909
[1mStep[0m  [32/42], [94mLoss[0m : 2.66421
[1mStep[0m  [36/42], [94mLoss[0m : 2.36942
[1mStep[0m  [40/42], [94mLoss[0m : 2.62287

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52371
[1mStep[0m  [4/42], [94mLoss[0m : 2.54035
[1mStep[0m  [8/42], [94mLoss[0m : 2.38918
[1mStep[0m  [12/42], [94mLoss[0m : 2.43950
[1mStep[0m  [16/42], [94mLoss[0m : 2.59626
[1mStep[0m  [20/42], [94mLoss[0m : 2.45154
[1mStep[0m  [24/42], [94mLoss[0m : 2.44933
[1mStep[0m  [28/42], [94mLoss[0m : 2.32633
[1mStep[0m  [32/42], [94mLoss[0m : 2.66245
[1mStep[0m  [36/42], [94mLoss[0m : 2.65655
[1mStep[0m  [40/42], [94mLoss[0m : 2.54637

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59645
[1mStep[0m  [4/42], [94mLoss[0m : 2.63030
[1mStep[0m  [8/42], [94mLoss[0m : 2.34988
[1mStep[0m  [12/42], [94mLoss[0m : 2.52669
[1mStep[0m  [16/42], [94mLoss[0m : 2.43508
[1mStep[0m  [20/42], [94mLoss[0m : 2.38531
[1mStep[0m  [24/42], [94mLoss[0m : 2.48875
[1mStep[0m  [28/42], [94mLoss[0m : 2.60012
[1mStep[0m  [32/42], [94mLoss[0m : 2.69492
[1mStep[0m  [36/42], [94mLoss[0m : 2.73668
[1mStep[0m  [40/42], [94mLoss[0m : 2.59254

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59853
[1mStep[0m  [4/42], [94mLoss[0m : 2.45321
[1mStep[0m  [8/42], [94mLoss[0m : 2.59321
[1mStep[0m  [12/42], [94mLoss[0m : 2.27428
[1mStep[0m  [16/42], [94mLoss[0m : 2.30336
[1mStep[0m  [20/42], [94mLoss[0m : 2.51385
[1mStep[0m  [24/42], [94mLoss[0m : 2.86681
[1mStep[0m  [28/42], [94mLoss[0m : 2.69804
[1mStep[0m  [32/42], [94mLoss[0m : 2.45501
[1mStep[0m  [36/42], [94mLoss[0m : 2.26738
[1mStep[0m  [40/42], [94mLoss[0m : 2.38724

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45018
[1mStep[0m  [4/42], [94mLoss[0m : 2.44507
[1mStep[0m  [8/42], [94mLoss[0m : 2.45305
[1mStep[0m  [12/42], [94mLoss[0m : 2.46627
[1mStep[0m  [16/42], [94mLoss[0m : 2.59765
[1mStep[0m  [20/42], [94mLoss[0m : 2.35615
[1mStep[0m  [24/42], [94mLoss[0m : 2.38508
[1mStep[0m  [28/42], [94mLoss[0m : 2.59523
[1mStep[0m  [32/42], [94mLoss[0m : 2.61451
[1mStep[0m  [36/42], [94mLoss[0m : 2.70123
[1mStep[0m  [40/42], [94mLoss[0m : 2.73321

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68357
[1mStep[0m  [4/42], [94mLoss[0m : 2.46351
[1mStep[0m  [8/42], [94mLoss[0m : 2.54641
[1mStep[0m  [12/42], [94mLoss[0m : 2.37690
[1mStep[0m  [16/42], [94mLoss[0m : 2.56546
[1mStep[0m  [20/42], [94mLoss[0m : 2.49313
[1mStep[0m  [24/42], [94mLoss[0m : 2.59774
[1mStep[0m  [28/42], [94mLoss[0m : 2.30716
[1mStep[0m  [32/42], [94mLoss[0m : 2.55187
[1mStep[0m  [36/42], [94mLoss[0m : 2.43013
[1mStep[0m  [40/42], [94mLoss[0m : 2.45919

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53971
[1mStep[0m  [4/42], [94mLoss[0m : 2.53727
[1mStep[0m  [8/42], [94mLoss[0m : 2.26827
[1mStep[0m  [12/42], [94mLoss[0m : 2.27046
[1mStep[0m  [16/42], [94mLoss[0m : 2.68123
[1mStep[0m  [20/42], [94mLoss[0m : 2.46627
[1mStep[0m  [24/42], [94mLoss[0m : 2.45951
[1mStep[0m  [28/42], [94mLoss[0m : 2.56024
[1mStep[0m  [32/42], [94mLoss[0m : 2.59012
[1mStep[0m  [36/42], [94mLoss[0m : 2.39995
[1mStep[0m  [40/42], [94mLoss[0m : 2.40670

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41233
[1mStep[0m  [4/42], [94mLoss[0m : 2.74714
[1mStep[0m  [8/42], [94mLoss[0m : 2.54904
[1mStep[0m  [12/42], [94mLoss[0m : 2.47987
[1mStep[0m  [16/42], [94mLoss[0m : 2.48030
[1mStep[0m  [20/42], [94mLoss[0m : 2.30766
[1mStep[0m  [24/42], [94mLoss[0m : 2.63420
[1mStep[0m  [28/42], [94mLoss[0m : 2.45437
[1mStep[0m  [32/42], [94mLoss[0m : 2.34295
[1mStep[0m  [36/42], [94mLoss[0m : 2.63372
[1mStep[0m  [40/42], [94mLoss[0m : 2.66853

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38602
[1mStep[0m  [4/42], [94mLoss[0m : 2.49340
[1mStep[0m  [8/42], [94mLoss[0m : 2.61186
[1mStep[0m  [12/42], [94mLoss[0m : 2.57929
[1mStep[0m  [16/42], [94mLoss[0m : 2.45265
[1mStep[0m  [20/42], [94mLoss[0m : 2.57813
[1mStep[0m  [24/42], [94mLoss[0m : 2.55996
[1mStep[0m  [28/42], [94mLoss[0m : 2.38618
[1mStep[0m  [32/42], [94mLoss[0m : 2.39378
[1mStep[0m  [36/42], [94mLoss[0m : 2.52768
[1mStep[0m  [40/42], [94mLoss[0m : 2.48637

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70859
[1mStep[0m  [4/42], [94mLoss[0m : 2.49440
[1mStep[0m  [8/42], [94mLoss[0m : 2.43049
[1mStep[0m  [12/42], [94mLoss[0m : 2.42658
[1mStep[0m  [16/42], [94mLoss[0m : 2.53690
[1mStep[0m  [20/42], [94mLoss[0m : 2.42010
[1mStep[0m  [24/42], [94mLoss[0m : 2.40847
[1mStep[0m  [28/42], [94mLoss[0m : 2.31820
[1mStep[0m  [32/42], [94mLoss[0m : 2.61531
[1mStep[0m  [36/42], [94mLoss[0m : 2.49080
[1mStep[0m  [40/42], [94mLoss[0m : 2.57796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49435
[1mStep[0m  [4/42], [94mLoss[0m : 2.43381
[1mStep[0m  [8/42], [94mLoss[0m : 2.55918
[1mStep[0m  [12/42], [94mLoss[0m : 2.54005
[1mStep[0m  [16/42], [94mLoss[0m : 2.36959
[1mStep[0m  [20/42], [94mLoss[0m : 2.35946
[1mStep[0m  [24/42], [94mLoss[0m : 2.51997
[1mStep[0m  [28/42], [94mLoss[0m : 2.38200
[1mStep[0m  [32/42], [94mLoss[0m : 2.59958
[1mStep[0m  [36/42], [94mLoss[0m : 2.42898
[1mStep[0m  [40/42], [94mLoss[0m : 2.24111

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57969
[1mStep[0m  [4/42], [94mLoss[0m : 2.47859
[1mStep[0m  [8/42], [94mLoss[0m : 2.48422
[1mStep[0m  [12/42], [94mLoss[0m : 2.56263
[1mStep[0m  [16/42], [94mLoss[0m : 2.41155
[1mStep[0m  [20/42], [94mLoss[0m : 2.53654
[1mStep[0m  [24/42], [94mLoss[0m : 2.55139
[1mStep[0m  [28/42], [94mLoss[0m : 2.70953
[1mStep[0m  [32/42], [94mLoss[0m : 2.59699
[1mStep[0m  [36/42], [94mLoss[0m : 2.43657
[1mStep[0m  [40/42], [94mLoss[0m : 2.59498

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32410
[1mStep[0m  [4/42], [94mLoss[0m : 2.28606
[1mStep[0m  [8/42], [94mLoss[0m : 2.47241
[1mStep[0m  [12/42], [94mLoss[0m : 2.37322
[1mStep[0m  [16/42], [94mLoss[0m : 2.58937
[1mStep[0m  [20/42], [94mLoss[0m : 2.46860
[1mStep[0m  [24/42], [94mLoss[0m : 2.35085
[1mStep[0m  [28/42], [94mLoss[0m : 2.42087
[1mStep[0m  [32/42], [94mLoss[0m : 2.64368
[1mStep[0m  [36/42], [94mLoss[0m : 2.56221
[1mStep[0m  [40/42], [94mLoss[0m : 2.84556

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39888
[1mStep[0m  [4/42], [94mLoss[0m : 2.50432
[1mStep[0m  [8/42], [94mLoss[0m : 2.69543
[1mStep[0m  [12/42], [94mLoss[0m : 2.45324
[1mStep[0m  [16/42], [94mLoss[0m : 2.42221
[1mStep[0m  [20/42], [94mLoss[0m : 2.58527
[1mStep[0m  [24/42], [94mLoss[0m : 2.47891
[1mStep[0m  [28/42], [94mLoss[0m : 2.37937
[1mStep[0m  [32/42], [94mLoss[0m : 2.55769
[1mStep[0m  [36/42], [94mLoss[0m : 2.37957
[1mStep[0m  [40/42], [94mLoss[0m : 2.66241

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53827
[1mStep[0m  [4/42], [94mLoss[0m : 2.38704
[1mStep[0m  [8/42], [94mLoss[0m : 2.47579
[1mStep[0m  [12/42], [94mLoss[0m : 2.73209
[1mStep[0m  [16/42], [94mLoss[0m : 2.51890
[1mStep[0m  [20/42], [94mLoss[0m : 2.43145
[1mStep[0m  [24/42], [94mLoss[0m : 2.56827
[1mStep[0m  [28/42], [94mLoss[0m : 2.60842
[1mStep[0m  [32/42], [94mLoss[0m : 2.21300
[1mStep[0m  [36/42], [94mLoss[0m : 2.58949
[1mStep[0m  [40/42], [94mLoss[0m : 2.55957

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50357
[1mStep[0m  [4/42], [94mLoss[0m : 2.51453
[1mStep[0m  [8/42], [94mLoss[0m : 2.47452
[1mStep[0m  [12/42], [94mLoss[0m : 2.38636
[1mStep[0m  [16/42], [94mLoss[0m : 2.57881
[1mStep[0m  [20/42], [94mLoss[0m : 2.58480
[1mStep[0m  [24/42], [94mLoss[0m : 2.37552
[1mStep[0m  [28/42], [94mLoss[0m : 2.55978
[1mStep[0m  [32/42], [94mLoss[0m : 2.51840
[1mStep[0m  [36/42], [94mLoss[0m : 2.37464
[1mStep[0m  [40/42], [94mLoss[0m : 2.35115

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.375
====================================

Phase 1 - Evaluation MAE:  2.375029410634722
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.40149
[1mStep[0m  [4/42], [94mLoss[0m : 2.55541
[1mStep[0m  [8/42], [94mLoss[0m : 2.44544
[1mStep[0m  [12/42], [94mLoss[0m : 2.66143
[1mStep[0m  [16/42], [94mLoss[0m : 2.63794
[1mStep[0m  [20/42], [94mLoss[0m : 2.54350
[1mStep[0m  [24/42], [94mLoss[0m : 2.53768
[1mStep[0m  [28/42], [94mLoss[0m : 2.68399
[1mStep[0m  [32/42], [94mLoss[0m : 2.80038
[1mStep[0m  [36/42], [94mLoss[0m : 2.60940
[1mStep[0m  [40/42], [94mLoss[0m : 2.50464

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32419
[1mStep[0m  [4/42], [94mLoss[0m : 2.49074
[1mStep[0m  [8/42], [94mLoss[0m : 2.46768
[1mStep[0m  [12/42], [94mLoss[0m : 2.48496
[1mStep[0m  [16/42], [94mLoss[0m : 2.68895
[1mStep[0m  [20/42], [94mLoss[0m : 2.49737
[1mStep[0m  [24/42], [94mLoss[0m : 2.52217
[1mStep[0m  [28/42], [94mLoss[0m : 2.37029
[1mStep[0m  [32/42], [94mLoss[0m : 2.50446
[1mStep[0m  [36/42], [94mLoss[0m : 2.84169
[1mStep[0m  [40/42], [94mLoss[0m : 2.55328

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62072
[1mStep[0m  [4/42], [94mLoss[0m : 2.41986
[1mStep[0m  [8/42], [94mLoss[0m : 2.48935
[1mStep[0m  [12/42], [94mLoss[0m : 2.60990
[1mStep[0m  [16/42], [94mLoss[0m : 2.45738
[1mStep[0m  [20/42], [94mLoss[0m : 2.59233
[1mStep[0m  [24/42], [94mLoss[0m : 2.66237
[1mStep[0m  [28/42], [94mLoss[0m : 2.64335
[1mStep[0m  [32/42], [94mLoss[0m : 2.43881
[1mStep[0m  [36/42], [94mLoss[0m : 2.58883
[1mStep[0m  [40/42], [94mLoss[0m : 2.44382

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51664
[1mStep[0m  [4/42], [94mLoss[0m : 2.39387
[1mStep[0m  [8/42], [94mLoss[0m : 2.41686
[1mStep[0m  [12/42], [94mLoss[0m : 2.66070
[1mStep[0m  [16/42], [94mLoss[0m : 2.46687
[1mStep[0m  [20/42], [94mLoss[0m : 2.31476
[1mStep[0m  [24/42], [94mLoss[0m : 2.77455
[1mStep[0m  [28/42], [94mLoss[0m : 2.40135
[1mStep[0m  [32/42], [94mLoss[0m : 2.21627
[1mStep[0m  [36/42], [94mLoss[0m : 2.53841
[1mStep[0m  [40/42], [94mLoss[0m : 2.47794

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26221
[1mStep[0m  [4/42], [94mLoss[0m : 2.57279
[1mStep[0m  [8/42], [94mLoss[0m : 2.48115
[1mStep[0m  [12/42], [94mLoss[0m : 2.10063
[1mStep[0m  [16/42], [94mLoss[0m : 2.30260
[1mStep[0m  [20/42], [94mLoss[0m : 2.29002
[1mStep[0m  [24/42], [94mLoss[0m : 2.57158
[1mStep[0m  [28/42], [94mLoss[0m : 2.44121
[1mStep[0m  [32/42], [94mLoss[0m : 2.40870
[1mStep[0m  [36/42], [94mLoss[0m : 2.61625
[1mStep[0m  [40/42], [94mLoss[0m : 2.42132

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43506
[1mStep[0m  [4/42], [94mLoss[0m : 2.35208
[1mStep[0m  [8/42], [94mLoss[0m : 2.39725
[1mStep[0m  [12/42], [94mLoss[0m : 2.26249
[1mStep[0m  [16/42], [94mLoss[0m : 2.12028
[1mStep[0m  [20/42], [94mLoss[0m : 2.27551
[1mStep[0m  [24/42], [94mLoss[0m : 2.23778
[1mStep[0m  [28/42], [94mLoss[0m : 2.27822
[1mStep[0m  [32/42], [94mLoss[0m : 2.51832
[1mStep[0m  [36/42], [94mLoss[0m : 2.24713
[1mStep[0m  [40/42], [94mLoss[0m : 2.38816

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22063
[1mStep[0m  [4/42], [94mLoss[0m : 2.35643
[1mStep[0m  [8/42], [94mLoss[0m : 2.46253
[1mStep[0m  [12/42], [94mLoss[0m : 2.38168
[1mStep[0m  [16/42], [94mLoss[0m : 2.61351
[1mStep[0m  [20/42], [94mLoss[0m : 2.38746
[1mStep[0m  [24/42], [94mLoss[0m : 2.41104
[1mStep[0m  [28/42], [94mLoss[0m : 2.31904
[1mStep[0m  [32/42], [94mLoss[0m : 2.54363
[1mStep[0m  [36/42], [94mLoss[0m : 2.42091
[1mStep[0m  [40/42], [94mLoss[0m : 2.32807

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36380
[1mStep[0m  [4/42], [94mLoss[0m : 2.18839
[1mStep[0m  [8/42], [94mLoss[0m : 2.41983
[1mStep[0m  [12/42], [94mLoss[0m : 2.14686
[1mStep[0m  [16/42], [94mLoss[0m : 2.10770
[1mStep[0m  [20/42], [94mLoss[0m : 2.40726
[1mStep[0m  [24/42], [94mLoss[0m : 2.29892
[1mStep[0m  [28/42], [94mLoss[0m : 2.37589
[1mStep[0m  [32/42], [94mLoss[0m : 2.29836
[1mStep[0m  [36/42], [94mLoss[0m : 2.41892
[1mStep[0m  [40/42], [94mLoss[0m : 2.66926

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36542
[1mStep[0m  [4/42], [94mLoss[0m : 2.41696
[1mStep[0m  [8/42], [94mLoss[0m : 2.18232
[1mStep[0m  [12/42], [94mLoss[0m : 2.29773
[1mStep[0m  [16/42], [94mLoss[0m : 2.19412
[1mStep[0m  [20/42], [94mLoss[0m : 2.22020
[1mStep[0m  [24/42], [94mLoss[0m : 2.40360
[1mStep[0m  [28/42], [94mLoss[0m : 2.42554
[1mStep[0m  [32/42], [94mLoss[0m : 2.34463
[1mStep[0m  [36/42], [94mLoss[0m : 2.49509
[1mStep[0m  [40/42], [94mLoss[0m : 2.27118

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16335
[1mStep[0m  [4/42], [94mLoss[0m : 2.25162
[1mStep[0m  [8/42], [94mLoss[0m : 2.29894
[1mStep[0m  [12/42], [94mLoss[0m : 2.22307
[1mStep[0m  [16/42], [94mLoss[0m : 2.33681
[1mStep[0m  [20/42], [94mLoss[0m : 2.10859
[1mStep[0m  [24/42], [94mLoss[0m : 2.63892
[1mStep[0m  [28/42], [94mLoss[0m : 2.25892
[1mStep[0m  [32/42], [94mLoss[0m : 2.35480
[1mStep[0m  [36/42], [94mLoss[0m : 2.27679
[1mStep[0m  [40/42], [94mLoss[0m : 2.26393

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19498
[1mStep[0m  [4/42], [94mLoss[0m : 2.20967
[1mStep[0m  [8/42], [94mLoss[0m : 2.22974
[1mStep[0m  [12/42], [94mLoss[0m : 2.10262
[1mStep[0m  [16/42], [94mLoss[0m : 2.28230
[1mStep[0m  [20/42], [94mLoss[0m : 2.09730
[1mStep[0m  [24/42], [94mLoss[0m : 2.59581
[1mStep[0m  [28/42], [94mLoss[0m : 2.30368
[1mStep[0m  [32/42], [94mLoss[0m : 2.20444
[1mStep[0m  [36/42], [94mLoss[0m : 2.15539
[1mStep[0m  [40/42], [94mLoss[0m : 2.33636

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41360
[1mStep[0m  [4/42], [94mLoss[0m : 2.08691
[1mStep[0m  [8/42], [94mLoss[0m : 2.20572
[1mStep[0m  [12/42], [94mLoss[0m : 2.24413
[1mStep[0m  [16/42], [94mLoss[0m : 2.25191
[1mStep[0m  [20/42], [94mLoss[0m : 2.14215
[1mStep[0m  [24/42], [94mLoss[0m : 2.27843
[1mStep[0m  [28/42], [94mLoss[0m : 2.15850
[1mStep[0m  [32/42], [94mLoss[0m : 2.13106
[1mStep[0m  [36/42], [94mLoss[0m : 2.40708
[1mStep[0m  [40/42], [94mLoss[0m : 2.43897

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05298
[1mStep[0m  [4/42], [94mLoss[0m : 2.01835
[1mStep[0m  [8/42], [94mLoss[0m : 2.26326
[1mStep[0m  [12/42], [94mLoss[0m : 2.20938
[1mStep[0m  [16/42], [94mLoss[0m : 2.10999
[1mStep[0m  [20/42], [94mLoss[0m : 2.15147
[1mStep[0m  [24/42], [94mLoss[0m : 2.26263
[1mStep[0m  [28/42], [94mLoss[0m : 2.19834
[1mStep[0m  [32/42], [94mLoss[0m : 2.13115
[1mStep[0m  [36/42], [94mLoss[0m : 2.34399
[1mStep[0m  [40/42], [94mLoss[0m : 2.12370

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24189
[1mStep[0m  [4/42], [94mLoss[0m : 2.04373
[1mStep[0m  [8/42], [94mLoss[0m : 2.18426
[1mStep[0m  [12/42], [94mLoss[0m : 2.13451
[1mStep[0m  [16/42], [94mLoss[0m : 2.38004
[1mStep[0m  [20/42], [94mLoss[0m : 2.08220
[1mStep[0m  [24/42], [94mLoss[0m : 2.33211
[1mStep[0m  [28/42], [94mLoss[0m : 2.26265
[1mStep[0m  [32/42], [94mLoss[0m : 2.05957
[1mStep[0m  [36/42], [94mLoss[0m : 2.06728
[1mStep[0m  [40/42], [94mLoss[0m : 2.14643

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.518, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13291
[1mStep[0m  [4/42], [94mLoss[0m : 2.08361
[1mStep[0m  [8/42], [94mLoss[0m : 2.20300
[1mStep[0m  [12/42], [94mLoss[0m : 1.99174
[1mStep[0m  [16/42], [94mLoss[0m : 2.14632
[1mStep[0m  [20/42], [94mLoss[0m : 2.31645
[1mStep[0m  [24/42], [94mLoss[0m : 2.31562
[1mStep[0m  [28/42], [94mLoss[0m : 2.16050
[1mStep[0m  [32/42], [94mLoss[0m : 2.22827
[1mStep[0m  [36/42], [94mLoss[0m : 2.25027
[1mStep[0m  [40/42], [94mLoss[0m : 1.97250

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93440
[1mStep[0m  [4/42], [94mLoss[0m : 2.29776
[1mStep[0m  [8/42], [94mLoss[0m : 2.23924
[1mStep[0m  [12/42], [94mLoss[0m : 2.06533
[1mStep[0m  [16/42], [94mLoss[0m : 2.07340
[1mStep[0m  [20/42], [94mLoss[0m : 2.06211
[1mStep[0m  [24/42], [94mLoss[0m : 2.20069
[1mStep[0m  [28/42], [94mLoss[0m : 1.90205
[1mStep[0m  [32/42], [94mLoss[0m : 2.03728
[1mStep[0m  [36/42], [94mLoss[0m : 2.21989
[1mStep[0m  [40/42], [94mLoss[0m : 2.42308

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23599
[1mStep[0m  [4/42], [94mLoss[0m : 1.81452
[1mStep[0m  [8/42], [94mLoss[0m : 2.16192
[1mStep[0m  [12/42], [94mLoss[0m : 2.10263
[1mStep[0m  [16/42], [94mLoss[0m : 2.15115
[1mStep[0m  [20/42], [94mLoss[0m : 2.03766
[1mStep[0m  [24/42], [94mLoss[0m : 2.03036
[1mStep[0m  [28/42], [94mLoss[0m : 2.23613
[1mStep[0m  [32/42], [94mLoss[0m : 2.07746
[1mStep[0m  [36/42], [94mLoss[0m : 2.12329
[1mStep[0m  [40/42], [94mLoss[0m : 1.97324

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83488
[1mStep[0m  [4/42], [94mLoss[0m : 2.10978
[1mStep[0m  [8/42], [94mLoss[0m : 1.91471
[1mStep[0m  [12/42], [94mLoss[0m : 1.99340
[1mStep[0m  [16/42], [94mLoss[0m : 2.30925
[1mStep[0m  [20/42], [94mLoss[0m : 2.09012
[1mStep[0m  [24/42], [94mLoss[0m : 2.13496
[1mStep[0m  [28/42], [94mLoss[0m : 2.02683
[1mStep[0m  [32/42], [94mLoss[0m : 1.96510
[1mStep[0m  [36/42], [94mLoss[0m : 2.09914
[1mStep[0m  [40/42], [94mLoss[0m : 1.88737

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98643
[1mStep[0m  [4/42], [94mLoss[0m : 1.95851
[1mStep[0m  [8/42], [94mLoss[0m : 1.81927
[1mStep[0m  [12/42], [94mLoss[0m : 2.03404
[1mStep[0m  [16/42], [94mLoss[0m : 2.11876
[1mStep[0m  [20/42], [94mLoss[0m : 1.90073
[1mStep[0m  [24/42], [94mLoss[0m : 2.02310
[1mStep[0m  [28/42], [94mLoss[0m : 1.86809
[1mStep[0m  [32/42], [94mLoss[0m : 1.96730
[1mStep[0m  [36/42], [94mLoss[0m : 1.99327
[1mStep[0m  [40/42], [94mLoss[0m : 2.14198

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83197
[1mStep[0m  [4/42], [94mLoss[0m : 1.82781
[1mStep[0m  [8/42], [94mLoss[0m : 2.00271
[1mStep[0m  [12/42], [94mLoss[0m : 2.22187
[1mStep[0m  [16/42], [94mLoss[0m : 1.84086
[1mStep[0m  [20/42], [94mLoss[0m : 1.98012
[1mStep[0m  [24/42], [94mLoss[0m : 1.73419
[1mStep[0m  [28/42], [94mLoss[0m : 2.04418
[1mStep[0m  [32/42], [94mLoss[0m : 1.97029
[1mStep[0m  [36/42], [94mLoss[0m : 1.92084
[1mStep[0m  [40/42], [94mLoss[0m : 2.03373

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.511, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80970
[1mStep[0m  [4/42], [94mLoss[0m : 1.95906
[1mStep[0m  [8/42], [94mLoss[0m : 1.82911
[1mStep[0m  [12/42], [94mLoss[0m : 1.83604
[1mStep[0m  [16/42], [94mLoss[0m : 1.89027
[1mStep[0m  [20/42], [94mLoss[0m : 2.05452
[1mStep[0m  [24/42], [94mLoss[0m : 2.00319
[1mStep[0m  [28/42], [94mLoss[0m : 1.99909
[1mStep[0m  [32/42], [94mLoss[0m : 2.09506
[1mStep[0m  [36/42], [94mLoss[0m : 2.15760
[1mStep[0m  [40/42], [94mLoss[0m : 1.88898

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83484
[1mStep[0m  [4/42], [94mLoss[0m : 1.77647
[1mStep[0m  [8/42], [94mLoss[0m : 1.91553
[1mStep[0m  [12/42], [94mLoss[0m : 1.86319
[1mStep[0m  [16/42], [94mLoss[0m : 2.00269
[1mStep[0m  [20/42], [94mLoss[0m : 2.08585
[1mStep[0m  [24/42], [94mLoss[0m : 1.95537
[1mStep[0m  [28/42], [94mLoss[0m : 1.99650
[1mStep[0m  [32/42], [94mLoss[0m : 1.92002
[1mStep[0m  [36/42], [94mLoss[0m : 1.96804
[1mStep[0m  [40/42], [94mLoss[0m : 1.97273

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72126
[1mStep[0m  [4/42], [94mLoss[0m : 1.84768
[1mStep[0m  [8/42], [94mLoss[0m : 1.92971
[1mStep[0m  [12/42], [94mLoss[0m : 1.88113
[1mStep[0m  [16/42], [94mLoss[0m : 1.80404
[1mStep[0m  [20/42], [94mLoss[0m : 1.74305
[1mStep[0m  [24/42], [94mLoss[0m : 1.68481
[1mStep[0m  [28/42], [94mLoss[0m : 1.83851
[1mStep[0m  [32/42], [94mLoss[0m : 2.00855
[1mStep[0m  [36/42], [94mLoss[0m : 1.89905
[1mStep[0m  [40/42], [94mLoss[0m : 1.85080

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.629, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02611
[1mStep[0m  [4/42], [94mLoss[0m : 1.83579
[1mStep[0m  [8/42], [94mLoss[0m : 1.67705
[1mStep[0m  [12/42], [94mLoss[0m : 1.94592
[1mStep[0m  [16/42], [94mLoss[0m : 1.75915
[1mStep[0m  [20/42], [94mLoss[0m : 1.89158
[1mStep[0m  [24/42], [94mLoss[0m : 1.79541
[1mStep[0m  [28/42], [94mLoss[0m : 1.80963
[1mStep[0m  [32/42], [94mLoss[0m : 1.77881
[1mStep[0m  [36/42], [94mLoss[0m : 1.98866
[1mStep[0m  [40/42], [94mLoss[0m : 2.00977

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.585, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00560
[1mStep[0m  [4/42], [94mLoss[0m : 1.79469
[1mStep[0m  [8/42], [94mLoss[0m : 1.83270
[1mStep[0m  [12/42], [94mLoss[0m : 1.89336
[1mStep[0m  [16/42], [94mLoss[0m : 1.74888
[1mStep[0m  [20/42], [94mLoss[0m : 1.81009
[1mStep[0m  [24/42], [94mLoss[0m : 1.84456
[1mStep[0m  [28/42], [94mLoss[0m : 1.95208
[1mStep[0m  [32/42], [94mLoss[0m : 1.82119
[1mStep[0m  [36/42], [94mLoss[0m : 1.93198
[1mStep[0m  [40/42], [94mLoss[0m : 1.70632

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.585, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83799
[1mStep[0m  [4/42], [94mLoss[0m : 1.73177
[1mStep[0m  [8/42], [94mLoss[0m : 1.63094
[1mStep[0m  [12/42], [94mLoss[0m : 1.84723
[1mStep[0m  [16/42], [94mLoss[0m : 1.93435
[1mStep[0m  [20/42], [94mLoss[0m : 1.88720
[1mStep[0m  [24/42], [94mLoss[0m : 1.81236
[1mStep[0m  [28/42], [94mLoss[0m : 1.86583
[1mStep[0m  [32/42], [94mLoss[0m : 1.83062
[1mStep[0m  [36/42], [94mLoss[0m : 2.03754
[1mStep[0m  [40/42], [94mLoss[0m : 1.76372

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.572, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81250
[1mStep[0m  [4/42], [94mLoss[0m : 1.87645
[1mStep[0m  [8/42], [94mLoss[0m : 1.73945
[1mStep[0m  [12/42], [94mLoss[0m : 1.78817
[1mStep[0m  [16/42], [94mLoss[0m : 1.73048
[1mStep[0m  [20/42], [94mLoss[0m : 1.68377
[1mStep[0m  [24/42], [94mLoss[0m : 1.87897
[1mStep[0m  [28/42], [94mLoss[0m : 1.81033
[1mStep[0m  [32/42], [94mLoss[0m : 1.73025
[1mStep[0m  [36/42], [94mLoss[0m : 1.99229
[1mStep[0m  [40/42], [94mLoss[0m : 1.75480

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65712
[1mStep[0m  [4/42], [94mLoss[0m : 1.74683
[1mStep[0m  [8/42], [94mLoss[0m : 1.79382
[1mStep[0m  [12/42], [94mLoss[0m : 1.79188
[1mStep[0m  [16/42], [94mLoss[0m : 1.79303
[1mStep[0m  [20/42], [94mLoss[0m : 1.77311
[1mStep[0m  [24/42], [94mLoss[0m : 1.69684
[1mStep[0m  [28/42], [94mLoss[0m : 1.91516
[1mStep[0m  [32/42], [94mLoss[0m : 1.89366
[1mStep[0m  [36/42], [94mLoss[0m : 1.73935
[1mStep[0m  [40/42], [94mLoss[0m : 1.71244

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71026
[1mStep[0m  [4/42], [94mLoss[0m : 1.79147
[1mStep[0m  [8/42], [94mLoss[0m : 1.83844
[1mStep[0m  [12/42], [94mLoss[0m : 1.77124
[1mStep[0m  [16/42], [94mLoss[0m : 1.79917
[1mStep[0m  [20/42], [94mLoss[0m : 1.88916
[1mStep[0m  [24/42], [94mLoss[0m : 1.88607
[1mStep[0m  [28/42], [94mLoss[0m : 1.86615
[1mStep[0m  [32/42], [94mLoss[0m : 1.86928
[1mStep[0m  [36/42], [94mLoss[0m : 1.66028
[1mStep[0m  [40/42], [94mLoss[0m : 1.74895

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88805
[1mStep[0m  [4/42], [94mLoss[0m : 1.91148
[1mStep[0m  [8/42], [94mLoss[0m : 1.66747
[1mStep[0m  [12/42], [94mLoss[0m : 1.73781
[1mStep[0m  [16/42], [94mLoss[0m : 1.55701
[1mStep[0m  [20/42], [94mLoss[0m : 1.76600
[1mStep[0m  [24/42], [94mLoss[0m : 1.84288
[1mStep[0m  [28/42], [94mLoss[0m : 1.78258
[1mStep[0m  [32/42], [94mLoss[0m : 1.67849
[1mStep[0m  [36/42], [94mLoss[0m : 1.60352
[1mStep[0m  [40/42], [94mLoss[0m : 1.56552

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.552, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.541
====================================

Phase 2 - Evaluation MAE:  2.5405705315726146
MAE score P1        2.375029
MAE score P2        2.540571
loss                1.738796
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay           0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.13750
[1mStep[0m  [4/42], [94mLoss[0m : 9.42482
[1mStep[0m  [8/42], [94mLoss[0m : 7.56975
[1mStep[0m  [12/42], [94mLoss[0m : 6.37086
[1mStep[0m  [16/42], [94mLoss[0m : 4.59073
[1mStep[0m  [20/42], [94mLoss[0m : 3.66445
[1mStep[0m  [24/42], [94mLoss[0m : 3.09382
[1mStep[0m  [28/42], [94mLoss[0m : 3.01378
[1mStep[0m  [32/42], [94mLoss[0m : 2.75197
[1mStep[0m  [36/42], [94mLoss[0m : 2.89405
[1mStep[0m  [40/42], [94mLoss[0m : 2.80216

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.996, [92mTest[0m: 11.064, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86720
[1mStep[0m  [4/42], [94mLoss[0m : 2.72826
[1mStep[0m  [8/42], [94mLoss[0m : 2.80170
[1mStep[0m  [12/42], [94mLoss[0m : 2.65968
[1mStep[0m  [16/42], [94mLoss[0m : 2.60577
[1mStep[0m  [20/42], [94mLoss[0m : 2.71394
[1mStep[0m  [24/42], [94mLoss[0m : 2.58762
[1mStep[0m  [28/42], [94mLoss[0m : 2.70648
[1mStep[0m  [32/42], [94mLoss[0m : 2.43164
[1mStep[0m  [36/42], [94mLoss[0m : 2.60307
[1mStep[0m  [40/42], [94mLoss[0m : 2.69639

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.756, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58554
[1mStep[0m  [4/42], [94mLoss[0m : 2.63054
[1mStep[0m  [8/42], [94mLoss[0m : 2.50201
[1mStep[0m  [12/42], [94mLoss[0m : 2.44981
[1mStep[0m  [16/42], [94mLoss[0m : 2.50140
[1mStep[0m  [20/42], [94mLoss[0m : 2.67950
[1mStep[0m  [24/42], [94mLoss[0m : 2.47455
[1mStep[0m  [28/42], [94mLoss[0m : 2.59979
[1mStep[0m  [32/42], [94mLoss[0m : 2.66556
[1mStep[0m  [36/42], [94mLoss[0m : 2.66575
[1mStep[0m  [40/42], [94mLoss[0m : 2.40336

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55200
[1mStep[0m  [4/42], [94mLoss[0m : 2.49977
[1mStep[0m  [8/42], [94mLoss[0m : 2.64044
[1mStep[0m  [12/42], [94mLoss[0m : 2.67537
[1mStep[0m  [16/42], [94mLoss[0m : 2.54528
[1mStep[0m  [20/42], [94mLoss[0m : 2.51015
[1mStep[0m  [24/42], [94mLoss[0m : 2.79322
[1mStep[0m  [28/42], [94mLoss[0m : 2.60425
[1mStep[0m  [32/42], [94mLoss[0m : 2.35129
[1mStep[0m  [36/42], [94mLoss[0m : 2.51609
[1mStep[0m  [40/42], [94mLoss[0m : 2.61932

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56039
[1mStep[0m  [4/42], [94mLoss[0m : 2.70852
[1mStep[0m  [8/42], [94mLoss[0m : 2.46714
[1mStep[0m  [12/42], [94mLoss[0m : 2.48687
[1mStep[0m  [16/42], [94mLoss[0m : 2.60339
[1mStep[0m  [20/42], [94mLoss[0m : 2.33763
[1mStep[0m  [24/42], [94mLoss[0m : 2.36823
[1mStep[0m  [28/42], [94mLoss[0m : 2.27245
[1mStep[0m  [32/42], [94mLoss[0m : 2.40603
[1mStep[0m  [36/42], [94mLoss[0m : 2.45972
[1mStep[0m  [40/42], [94mLoss[0m : 2.62607

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56625
[1mStep[0m  [4/42], [94mLoss[0m : 2.48512
[1mStep[0m  [8/42], [94mLoss[0m : 2.55630
[1mStep[0m  [12/42], [94mLoss[0m : 2.57067
[1mStep[0m  [16/42], [94mLoss[0m : 2.56524
[1mStep[0m  [20/42], [94mLoss[0m : 2.43693
[1mStep[0m  [24/42], [94mLoss[0m : 2.33001
[1mStep[0m  [28/42], [94mLoss[0m : 2.30246
[1mStep[0m  [32/42], [94mLoss[0m : 2.50607
[1mStep[0m  [36/42], [94mLoss[0m : 2.44850
[1mStep[0m  [40/42], [94mLoss[0m : 2.49441

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66228
[1mStep[0m  [4/42], [94mLoss[0m : 2.36621
[1mStep[0m  [8/42], [94mLoss[0m : 2.51877
[1mStep[0m  [12/42], [94mLoss[0m : 2.64813
[1mStep[0m  [16/42], [94mLoss[0m : 2.69958
[1mStep[0m  [20/42], [94mLoss[0m : 2.40072
[1mStep[0m  [24/42], [94mLoss[0m : 2.31174
[1mStep[0m  [28/42], [94mLoss[0m : 2.49325
[1mStep[0m  [32/42], [94mLoss[0m : 2.46672
[1mStep[0m  [36/42], [94mLoss[0m : 2.68719
[1mStep[0m  [40/42], [94mLoss[0m : 2.48903

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66506
[1mStep[0m  [4/42], [94mLoss[0m : 2.46402
[1mStep[0m  [8/42], [94mLoss[0m : 2.58675
[1mStep[0m  [12/42], [94mLoss[0m : 2.55524
[1mStep[0m  [16/42], [94mLoss[0m : 2.50485
[1mStep[0m  [20/42], [94mLoss[0m : 2.28487
[1mStep[0m  [24/42], [94mLoss[0m : 2.53861
[1mStep[0m  [28/42], [94mLoss[0m : 2.34394
[1mStep[0m  [32/42], [94mLoss[0m : 2.63528
[1mStep[0m  [36/42], [94mLoss[0m : 2.63101
[1mStep[0m  [40/42], [94mLoss[0m : 2.30758

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61004
[1mStep[0m  [4/42], [94mLoss[0m : 2.37861
[1mStep[0m  [8/42], [94mLoss[0m : 2.58788
[1mStep[0m  [12/42], [94mLoss[0m : 2.51705
[1mStep[0m  [16/42], [94mLoss[0m : 2.42601
[1mStep[0m  [20/42], [94mLoss[0m : 2.33274
[1mStep[0m  [24/42], [94mLoss[0m : 2.59968
[1mStep[0m  [28/42], [94mLoss[0m : 2.66807
[1mStep[0m  [32/42], [94mLoss[0m : 2.57368
[1mStep[0m  [36/42], [94mLoss[0m : 2.57841
[1mStep[0m  [40/42], [94mLoss[0m : 2.48429

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42321
[1mStep[0m  [4/42], [94mLoss[0m : 2.66275
[1mStep[0m  [8/42], [94mLoss[0m : 2.66862
[1mStep[0m  [12/42], [94mLoss[0m : 2.62570
[1mStep[0m  [16/42], [94mLoss[0m : 2.53389
[1mStep[0m  [20/42], [94mLoss[0m : 2.37827
[1mStep[0m  [24/42], [94mLoss[0m : 2.37047
[1mStep[0m  [28/42], [94mLoss[0m : 2.41204
[1mStep[0m  [32/42], [94mLoss[0m : 2.36327
[1mStep[0m  [36/42], [94mLoss[0m : 2.59166
[1mStep[0m  [40/42], [94mLoss[0m : 2.61645

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46333
[1mStep[0m  [4/42], [94mLoss[0m : 2.58534
[1mStep[0m  [8/42], [94mLoss[0m : 2.49668
[1mStep[0m  [12/42], [94mLoss[0m : 2.33812
[1mStep[0m  [16/42], [94mLoss[0m : 2.57455
[1mStep[0m  [20/42], [94mLoss[0m : 2.39149
[1mStep[0m  [24/42], [94mLoss[0m : 2.59194
[1mStep[0m  [28/42], [94mLoss[0m : 2.34788
[1mStep[0m  [32/42], [94mLoss[0m : 2.52799
[1mStep[0m  [36/42], [94mLoss[0m : 2.46722
[1mStep[0m  [40/42], [94mLoss[0m : 2.45023

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60273
[1mStep[0m  [4/42], [94mLoss[0m : 2.42663
[1mStep[0m  [8/42], [94mLoss[0m : 2.37585
[1mStep[0m  [12/42], [94mLoss[0m : 2.26652
[1mStep[0m  [16/42], [94mLoss[0m : 2.61916
[1mStep[0m  [20/42], [94mLoss[0m : 2.73301
[1mStep[0m  [24/42], [94mLoss[0m : 2.54152
[1mStep[0m  [28/42], [94mLoss[0m : 2.74739
[1mStep[0m  [32/42], [94mLoss[0m : 2.60908
[1mStep[0m  [36/42], [94mLoss[0m : 2.57772
[1mStep[0m  [40/42], [94mLoss[0m : 2.56404

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26571
[1mStep[0m  [4/42], [94mLoss[0m : 2.49356
[1mStep[0m  [8/42], [94mLoss[0m : 2.55246
[1mStep[0m  [12/42], [94mLoss[0m : 2.57584
[1mStep[0m  [16/42], [94mLoss[0m : 2.68576
[1mStep[0m  [20/42], [94mLoss[0m : 2.44888
[1mStep[0m  [24/42], [94mLoss[0m : 2.33558
[1mStep[0m  [28/42], [94mLoss[0m : 2.58830
[1mStep[0m  [32/42], [94mLoss[0m : 2.40074
[1mStep[0m  [36/42], [94mLoss[0m : 2.58519
[1mStep[0m  [40/42], [94mLoss[0m : 2.44474

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36262
[1mStep[0m  [4/42], [94mLoss[0m : 2.56305
[1mStep[0m  [8/42], [94mLoss[0m : 2.24760
[1mStep[0m  [12/42], [94mLoss[0m : 2.64857
[1mStep[0m  [16/42], [94mLoss[0m : 2.56458
[1mStep[0m  [20/42], [94mLoss[0m : 2.59241
[1mStep[0m  [24/42], [94mLoss[0m : 2.28893
[1mStep[0m  [28/42], [94mLoss[0m : 2.63533
[1mStep[0m  [32/42], [94mLoss[0m : 2.35488
[1mStep[0m  [36/42], [94mLoss[0m : 2.47764
[1mStep[0m  [40/42], [94mLoss[0m : 2.40230

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49997
[1mStep[0m  [4/42], [94mLoss[0m : 2.34576
[1mStep[0m  [8/42], [94mLoss[0m : 2.46867
[1mStep[0m  [12/42], [94mLoss[0m : 2.45918
[1mStep[0m  [16/42], [94mLoss[0m : 2.28971
[1mStep[0m  [20/42], [94mLoss[0m : 2.26832
[1mStep[0m  [24/42], [94mLoss[0m : 2.60007
[1mStep[0m  [28/42], [94mLoss[0m : 2.34967
[1mStep[0m  [32/42], [94mLoss[0m : 2.43431
[1mStep[0m  [36/42], [94mLoss[0m : 2.28441
[1mStep[0m  [40/42], [94mLoss[0m : 2.38689

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65029
[1mStep[0m  [4/42], [94mLoss[0m : 2.40129
[1mStep[0m  [8/42], [94mLoss[0m : 2.34531
[1mStep[0m  [12/42], [94mLoss[0m : 2.42279
[1mStep[0m  [16/42], [94mLoss[0m : 2.46541
[1mStep[0m  [20/42], [94mLoss[0m : 2.52628
[1mStep[0m  [24/42], [94mLoss[0m : 2.32243
[1mStep[0m  [28/42], [94mLoss[0m : 2.65302
[1mStep[0m  [32/42], [94mLoss[0m : 2.46811
[1mStep[0m  [36/42], [94mLoss[0m : 2.40931
[1mStep[0m  [40/42], [94mLoss[0m : 2.49322

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40777
[1mStep[0m  [4/42], [94mLoss[0m : 2.47747
[1mStep[0m  [8/42], [94mLoss[0m : 2.43040
[1mStep[0m  [12/42], [94mLoss[0m : 2.45705
[1mStep[0m  [16/42], [94mLoss[0m : 2.33900
[1mStep[0m  [20/42], [94mLoss[0m : 2.46100
[1mStep[0m  [24/42], [94mLoss[0m : 2.34311
[1mStep[0m  [28/42], [94mLoss[0m : 2.38601
[1mStep[0m  [32/42], [94mLoss[0m : 2.39303
[1mStep[0m  [36/42], [94mLoss[0m : 2.47389
[1mStep[0m  [40/42], [94mLoss[0m : 2.34300

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36659
[1mStep[0m  [4/42], [94mLoss[0m : 2.45109
[1mStep[0m  [8/42], [94mLoss[0m : 2.52701
[1mStep[0m  [12/42], [94mLoss[0m : 2.60339
[1mStep[0m  [16/42], [94mLoss[0m : 2.46689
[1mStep[0m  [20/42], [94mLoss[0m : 2.58966
[1mStep[0m  [24/42], [94mLoss[0m : 2.35141
[1mStep[0m  [28/42], [94mLoss[0m : 2.14747
[1mStep[0m  [32/42], [94mLoss[0m : 2.53029
[1mStep[0m  [36/42], [94mLoss[0m : 2.61484
[1mStep[0m  [40/42], [94mLoss[0m : 2.41179

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53174
[1mStep[0m  [4/42], [94mLoss[0m : 2.33886
[1mStep[0m  [8/42], [94mLoss[0m : 2.52126
[1mStep[0m  [12/42], [94mLoss[0m : 2.48618
[1mStep[0m  [16/42], [94mLoss[0m : 2.42980
[1mStep[0m  [20/42], [94mLoss[0m : 2.64253
[1mStep[0m  [24/42], [94mLoss[0m : 2.33074
[1mStep[0m  [28/42], [94mLoss[0m : 2.51815
[1mStep[0m  [32/42], [94mLoss[0m : 2.44935
[1mStep[0m  [36/42], [94mLoss[0m : 2.58651
[1mStep[0m  [40/42], [94mLoss[0m : 2.18003

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56847
[1mStep[0m  [4/42], [94mLoss[0m : 2.35326
[1mStep[0m  [8/42], [94mLoss[0m : 2.46324
[1mStep[0m  [12/42], [94mLoss[0m : 2.36500
[1mStep[0m  [16/42], [94mLoss[0m : 2.53813
[1mStep[0m  [20/42], [94mLoss[0m : 2.48379
[1mStep[0m  [24/42], [94mLoss[0m : 2.34489
[1mStep[0m  [28/42], [94mLoss[0m : 2.70199
[1mStep[0m  [32/42], [94mLoss[0m : 2.37054
[1mStep[0m  [36/42], [94mLoss[0m : 2.23243
[1mStep[0m  [40/42], [94mLoss[0m : 2.23787

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31101
[1mStep[0m  [4/42], [94mLoss[0m : 2.42522
[1mStep[0m  [8/42], [94mLoss[0m : 2.49146
[1mStep[0m  [12/42], [94mLoss[0m : 2.36839
[1mStep[0m  [16/42], [94mLoss[0m : 2.43387
[1mStep[0m  [20/42], [94mLoss[0m : 2.35917
[1mStep[0m  [24/42], [94mLoss[0m : 2.45595
[1mStep[0m  [28/42], [94mLoss[0m : 2.45092
[1mStep[0m  [32/42], [94mLoss[0m : 2.53839
[1mStep[0m  [36/42], [94mLoss[0m : 2.35188
[1mStep[0m  [40/42], [94mLoss[0m : 2.53579

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36312
[1mStep[0m  [4/42], [94mLoss[0m : 2.55162
[1mStep[0m  [8/42], [94mLoss[0m : 2.35425
[1mStep[0m  [12/42], [94mLoss[0m : 2.51034
[1mStep[0m  [16/42], [94mLoss[0m : 2.43869
[1mStep[0m  [20/42], [94mLoss[0m : 2.54438
[1mStep[0m  [24/42], [94mLoss[0m : 2.39510
[1mStep[0m  [28/42], [94mLoss[0m : 2.44668
[1mStep[0m  [32/42], [94mLoss[0m : 2.49301
[1mStep[0m  [36/42], [94mLoss[0m : 2.40393
[1mStep[0m  [40/42], [94mLoss[0m : 2.58171

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66524
[1mStep[0m  [4/42], [94mLoss[0m : 2.34741
[1mStep[0m  [8/42], [94mLoss[0m : 2.47548
[1mStep[0m  [12/42], [94mLoss[0m : 2.60438
[1mStep[0m  [16/42], [94mLoss[0m : 2.46226
[1mStep[0m  [20/42], [94mLoss[0m : 2.83139
[1mStep[0m  [24/42], [94mLoss[0m : 2.43789
[1mStep[0m  [28/42], [94mLoss[0m : 2.37033
[1mStep[0m  [32/42], [94mLoss[0m : 2.53531
[1mStep[0m  [36/42], [94mLoss[0m : 2.51634
[1mStep[0m  [40/42], [94mLoss[0m : 2.28385

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17195
[1mStep[0m  [4/42], [94mLoss[0m : 2.43634
[1mStep[0m  [8/42], [94mLoss[0m : 2.47871
[1mStep[0m  [12/42], [94mLoss[0m : 2.48631
[1mStep[0m  [16/42], [94mLoss[0m : 2.52839
[1mStep[0m  [20/42], [94mLoss[0m : 2.49215
[1mStep[0m  [24/42], [94mLoss[0m : 2.63803
[1mStep[0m  [28/42], [94mLoss[0m : 2.51436
[1mStep[0m  [32/42], [94mLoss[0m : 2.67595
[1mStep[0m  [36/42], [94mLoss[0m : 2.56462
[1mStep[0m  [40/42], [94mLoss[0m : 2.44803

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22715
[1mStep[0m  [4/42], [94mLoss[0m : 2.42411
[1mStep[0m  [8/42], [94mLoss[0m : 2.68699
[1mStep[0m  [12/42], [94mLoss[0m : 2.43348
[1mStep[0m  [16/42], [94mLoss[0m : 2.31524
[1mStep[0m  [20/42], [94mLoss[0m : 2.41477
[1mStep[0m  [24/42], [94mLoss[0m : 2.61140
[1mStep[0m  [28/42], [94mLoss[0m : 2.67498
[1mStep[0m  [32/42], [94mLoss[0m : 2.36880
[1mStep[0m  [36/42], [94mLoss[0m : 2.38910
[1mStep[0m  [40/42], [94mLoss[0m : 2.49675

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55937
[1mStep[0m  [4/42], [94mLoss[0m : 2.75493
[1mStep[0m  [8/42], [94mLoss[0m : 2.52279
[1mStep[0m  [12/42], [94mLoss[0m : 2.67389
[1mStep[0m  [16/42], [94mLoss[0m : 2.73113
[1mStep[0m  [20/42], [94mLoss[0m : 2.50349
[1mStep[0m  [24/42], [94mLoss[0m : 2.42928
[1mStep[0m  [28/42], [94mLoss[0m : 2.37202
[1mStep[0m  [32/42], [94mLoss[0m : 2.51864
[1mStep[0m  [36/42], [94mLoss[0m : 2.56760
[1mStep[0m  [40/42], [94mLoss[0m : 2.57659

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45174
[1mStep[0m  [4/42], [94mLoss[0m : 2.63768
[1mStep[0m  [8/42], [94mLoss[0m : 2.41023
[1mStep[0m  [12/42], [94mLoss[0m : 2.53711
[1mStep[0m  [16/42], [94mLoss[0m : 2.51184
[1mStep[0m  [20/42], [94mLoss[0m : 2.39640
[1mStep[0m  [24/42], [94mLoss[0m : 2.30752
[1mStep[0m  [28/42], [94mLoss[0m : 2.46255
[1mStep[0m  [32/42], [94mLoss[0m : 2.41565
[1mStep[0m  [36/42], [94mLoss[0m : 2.43440
[1mStep[0m  [40/42], [94mLoss[0m : 2.43810

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37884
[1mStep[0m  [4/42], [94mLoss[0m : 2.24046
[1mStep[0m  [8/42], [94mLoss[0m : 2.60636
[1mStep[0m  [12/42], [94mLoss[0m : 2.47604
[1mStep[0m  [16/42], [94mLoss[0m : 2.28751
[1mStep[0m  [20/42], [94mLoss[0m : 2.50044
[1mStep[0m  [24/42], [94mLoss[0m : 2.27999
[1mStep[0m  [28/42], [94mLoss[0m : 2.50654
[1mStep[0m  [32/42], [94mLoss[0m : 2.46083
[1mStep[0m  [36/42], [94mLoss[0m : 2.67285
[1mStep[0m  [40/42], [94mLoss[0m : 2.43501

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42115
[1mStep[0m  [4/42], [94mLoss[0m : 2.72356
[1mStep[0m  [8/42], [94mLoss[0m : 2.42092
[1mStep[0m  [12/42], [94mLoss[0m : 2.40190
[1mStep[0m  [16/42], [94mLoss[0m : 2.40616
[1mStep[0m  [20/42], [94mLoss[0m : 2.33184
[1mStep[0m  [24/42], [94mLoss[0m : 2.44843
[1mStep[0m  [28/42], [94mLoss[0m : 2.33589
[1mStep[0m  [32/42], [94mLoss[0m : 2.31004
[1mStep[0m  [36/42], [94mLoss[0m : 2.72066
[1mStep[0m  [40/42], [94mLoss[0m : 2.46591

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58821
[1mStep[0m  [4/42], [94mLoss[0m : 2.21402
[1mStep[0m  [8/42], [94mLoss[0m : 2.40204
[1mStep[0m  [12/42], [94mLoss[0m : 2.38568
[1mStep[0m  [16/42], [94mLoss[0m : 2.41660
[1mStep[0m  [20/42], [94mLoss[0m : 2.43657
[1mStep[0m  [24/42], [94mLoss[0m : 2.53355
[1mStep[0m  [28/42], [94mLoss[0m : 2.49806
[1mStep[0m  [32/42], [94mLoss[0m : 2.42618
[1mStep[0m  [36/42], [94mLoss[0m : 2.45258
[1mStep[0m  [40/42], [94mLoss[0m : 2.30219

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3370443752833774
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.56637
[1mStep[0m  [4/42], [94mLoss[0m : 2.45737
[1mStep[0m  [8/42], [94mLoss[0m : 2.45456
[1mStep[0m  [12/42], [94mLoss[0m : 2.41422
[1mStep[0m  [16/42], [94mLoss[0m : 2.35148
[1mStep[0m  [20/42], [94mLoss[0m : 2.49262
[1mStep[0m  [24/42], [94mLoss[0m : 2.40086
[1mStep[0m  [28/42], [94mLoss[0m : 2.34591
[1mStep[0m  [32/42], [94mLoss[0m : 2.56752
[1mStep[0m  [36/42], [94mLoss[0m : 2.50859
[1mStep[0m  [40/42], [94mLoss[0m : 2.56338

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55941
[1mStep[0m  [4/42], [94mLoss[0m : 2.44234
[1mStep[0m  [8/42], [94mLoss[0m : 2.22461
[1mStep[0m  [12/42], [94mLoss[0m : 2.51989
[1mStep[0m  [16/42], [94mLoss[0m : 2.58348
[1mStep[0m  [20/42], [94mLoss[0m : 2.44241
[1mStep[0m  [24/42], [94mLoss[0m : 2.45963
[1mStep[0m  [28/42], [94mLoss[0m : 2.59064
[1mStep[0m  [32/42], [94mLoss[0m : 2.25201
[1mStep[0m  [36/42], [94mLoss[0m : 2.34435
[1mStep[0m  [40/42], [94mLoss[0m : 2.37828

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45452
[1mStep[0m  [4/42], [94mLoss[0m : 2.53284
[1mStep[0m  [8/42], [94mLoss[0m : 2.48973
[1mStep[0m  [12/42], [94mLoss[0m : 2.52527
[1mStep[0m  [16/42], [94mLoss[0m : 2.37625
[1mStep[0m  [20/42], [94mLoss[0m : 2.52012
[1mStep[0m  [24/42], [94mLoss[0m : 2.58624
[1mStep[0m  [28/42], [94mLoss[0m : 2.37486
[1mStep[0m  [32/42], [94mLoss[0m : 2.35982
[1mStep[0m  [36/42], [94mLoss[0m : 2.31583
[1mStep[0m  [40/42], [94mLoss[0m : 2.40432

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54970
[1mStep[0m  [4/42], [94mLoss[0m : 2.38820
[1mStep[0m  [8/42], [94mLoss[0m : 2.12680
[1mStep[0m  [12/42], [94mLoss[0m : 2.44688
[1mStep[0m  [16/42], [94mLoss[0m : 2.38331
[1mStep[0m  [20/42], [94mLoss[0m : 2.60353
[1mStep[0m  [24/42], [94mLoss[0m : 2.50464
[1mStep[0m  [28/42], [94mLoss[0m : 2.67007
[1mStep[0m  [32/42], [94mLoss[0m : 2.38092
[1mStep[0m  [36/42], [94mLoss[0m : 2.54320
[1mStep[0m  [40/42], [94mLoss[0m : 2.49981

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37540
[1mStep[0m  [4/42], [94mLoss[0m : 2.44412
[1mStep[0m  [8/42], [94mLoss[0m : 2.43048
[1mStep[0m  [12/42], [94mLoss[0m : 2.34479
[1mStep[0m  [16/42], [94mLoss[0m : 2.43532
[1mStep[0m  [20/42], [94mLoss[0m : 2.44362
[1mStep[0m  [24/42], [94mLoss[0m : 2.41409
[1mStep[0m  [28/42], [94mLoss[0m : 2.39082
[1mStep[0m  [32/42], [94mLoss[0m : 2.30773
[1mStep[0m  [36/42], [94mLoss[0m : 2.27297
[1mStep[0m  [40/42], [94mLoss[0m : 2.62358

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56232
[1mStep[0m  [4/42], [94mLoss[0m : 2.24887
[1mStep[0m  [8/42], [94mLoss[0m : 2.38129
[1mStep[0m  [12/42], [94mLoss[0m : 2.54320
[1mStep[0m  [16/42], [94mLoss[0m : 2.37395
[1mStep[0m  [20/42], [94mLoss[0m : 2.43628
[1mStep[0m  [24/42], [94mLoss[0m : 2.34827
[1mStep[0m  [28/42], [94mLoss[0m : 2.41928
[1mStep[0m  [32/42], [94mLoss[0m : 2.47608
[1mStep[0m  [36/42], [94mLoss[0m : 2.22826
[1mStep[0m  [40/42], [94mLoss[0m : 2.39740

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51755
[1mStep[0m  [4/42], [94mLoss[0m : 2.34449
[1mStep[0m  [8/42], [94mLoss[0m : 2.26061
[1mStep[0m  [12/42], [94mLoss[0m : 2.39645
[1mStep[0m  [16/42], [94mLoss[0m : 2.50106
[1mStep[0m  [20/42], [94mLoss[0m : 2.66494
[1mStep[0m  [24/42], [94mLoss[0m : 2.27331
[1mStep[0m  [28/42], [94mLoss[0m : 2.25926
[1mStep[0m  [32/42], [94mLoss[0m : 2.38872
[1mStep[0m  [36/42], [94mLoss[0m : 2.34314
[1mStep[0m  [40/42], [94mLoss[0m : 2.41648

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40908
[1mStep[0m  [4/42], [94mLoss[0m : 2.31204
[1mStep[0m  [8/42], [94mLoss[0m : 2.24155
[1mStep[0m  [12/42], [94mLoss[0m : 2.28463
[1mStep[0m  [16/42], [94mLoss[0m : 2.38904
[1mStep[0m  [20/42], [94mLoss[0m : 2.51244
[1mStep[0m  [24/42], [94mLoss[0m : 2.19098
[1mStep[0m  [28/42], [94mLoss[0m : 2.33693
[1mStep[0m  [32/42], [94mLoss[0m : 2.54802
[1mStep[0m  [36/42], [94mLoss[0m : 2.30807
[1mStep[0m  [40/42], [94mLoss[0m : 2.44237

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29720
[1mStep[0m  [4/42], [94mLoss[0m : 2.24667
[1mStep[0m  [8/42], [94mLoss[0m : 2.44659
[1mStep[0m  [12/42], [94mLoss[0m : 2.25404
[1mStep[0m  [16/42], [94mLoss[0m : 2.36332
[1mStep[0m  [20/42], [94mLoss[0m : 2.44071
[1mStep[0m  [24/42], [94mLoss[0m : 2.36102
[1mStep[0m  [28/42], [94mLoss[0m : 2.18775
[1mStep[0m  [32/42], [94mLoss[0m : 2.39149
[1mStep[0m  [36/42], [94mLoss[0m : 2.33968
[1mStep[0m  [40/42], [94mLoss[0m : 2.54580

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.583, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39530
[1mStep[0m  [4/42], [94mLoss[0m : 2.07300
[1mStep[0m  [8/42], [94mLoss[0m : 2.43784
[1mStep[0m  [12/42], [94mLoss[0m : 2.17457
[1mStep[0m  [16/42], [94mLoss[0m : 2.50135
[1mStep[0m  [20/42], [94mLoss[0m : 2.25507
[1mStep[0m  [24/42], [94mLoss[0m : 2.46169
[1mStep[0m  [28/42], [94mLoss[0m : 2.20959
[1mStep[0m  [32/42], [94mLoss[0m : 2.39646
[1mStep[0m  [36/42], [94mLoss[0m : 2.44215
[1mStep[0m  [40/42], [94mLoss[0m : 2.32094

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.594, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41687
[1mStep[0m  [4/42], [94mLoss[0m : 2.24810
[1mStep[0m  [8/42], [94mLoss[0m : 2.46700
[1mStep[0m  [12/42], [94mLoss[0m : 2.36248
[1mStep[0m  [16/42], [94mLoss[0m : 2.25352
[1mStep[0m  [20/42], [94mLoss[0m : 2.19169
[1mStep[0m  [24/42], [94mLoss[0m : 2.34576
[1mStep[0m  [28/42], [94mLoss[0m : 2.45420
[1mStep[0m  [32/42], [94mLoss[0m : 2.34956
[1mStep[0m  [36/42], [94mLoss[0m : 2.34906
[1mStep[0m  [40/42], [94mLoss[0m : 2.48660

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.590, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04724
[1mStep[0m  [4/42], [94mLoss[0m : 2.50192
[1mStep[0m  [8/42], [94mLoss[0m : 2.29358
[1mStep[0m  [12/42], [94mLoss[0m : 2.27146
[1mStep[0m  [16/42], [94mLoss[0m : 2.16816
[1mStep[0m  [20/42], [94mLoss[0m : 2.29960
[1mStep[0m  [24/42], [94mLoss[0m : 2.16336
[1mStep[0m  [28/42], [94mLoss[0m : 2.30467
[1mStep[0m  [32/42], [94mLoss[0m : 2.43085
[1mStep[0m  [36/42], [94mLoss[0m : 2.25179
[1mStep[0m  [40/42], [94mLoss[0m : 2.36798

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.640, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35304
[1mStep[0m  [4/42], [94mLoss[0m : 2.32996
[1mStep[0m  [8/42], [94mLoss[0m : 2.13668
[1mStep[0m  [12/42], [94mLoss[0m : 2.18879
[1mStep[0m  [16/42], [94mLoss[0m : 2.42501
[1mStep[0m  [20/42], [94mLoss[0m : 2.28049
[1mStep[0m  [24/42], [94mLoss[0m : 2.33814
[1mStep[0m  [28/42], [94mLoss[0m : 2.31775
[1mStep[0m  [32/42], [94mLoss[0m : 2.32122
[1mStep[0m  [36/42], [94mLoss[0m : 2.09686
[1mStep[0m  [40/42], [94mLoss[0m : 2.23353

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.643, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28229
[1mStep[0m  [4/42], [94mLoss[0m : 2.36049
[1mStep[0m  [8/42], [94mLoss[0m : 2.30830
[1mStep[0m  [12/42], [94mLoss[0m : 2.40722
[1mStep[0m  [16/42], [94mLoss[0m : 2.49937
[1mStep[0m  [20/42], [94mLoss[0m : 2.21970
[1mStep[0m  [24/42], [94mLoss[0m : 2.32987
[1mStep[0m  [28/42], [94mLoss[0m : 2.19800
[1mStep[0m  [32/42], [94mLoss[0m : 2.48953
[1mStep[0m  [36/42], [94mLoss[0m : 2.26635
[1mStep[0m  [40/42], [94mLoss[0m : 2.27929

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.575, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17447
[1mStep[0m  [4/42], [94mLoss[0m : 2.07487
[1mStep[0m  [8/42], [94mLoss[0m : 2.21328
[1mStep[0m  [12/42], [94mLoss[0m : 2.30852
[1mStep[0m  [16/42], [94mLoss[0m : 2.05572
[1mStep[0m  [20/42], [94mLoss[0m : 2.60004
[1mStep[0m  [24/42], [94mLoss[0m : 2.31515
[1mStep[0m  [28/42], [94mLoss[0m : 2.13593
[1mStep[0m  [32/42], [94mLoss[0m : 2.40918
[1mStep[0m  [36/42], [94mLoss[0m : 2.19379
[1mStep[0m  [40/42], [94mLoss[0m : 2.27677

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.640, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25432
[1mStep[0m  [4/42], [94mLoss[0m : 2.25008
[1mStep[0m  [8/42], [94mLoss[0m : 2.31019
[1mStep[0m  [12/42], [94mLoss[0m : 2.06613
[1mStep[0m  [16/42], [94mLoss[0m : 2.21421
[1mStep[0m  [20/42], [94mLoss[0m : 1.93002
[1mStep[0m  [24/42], [94mLoss[0m : 2.15614
[1mStep[0m  [28/42], [94mLoss[0m : 2.26603
[1mStep[0m  [32/42], [94mLoss[0m : 2.34249
[1mStep[0m  [36/42], [94mLoss[0m : 2.26511
[1mStep[0m  [40/42], [94mLoss[0m : 2.32478

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.671, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17184
[1mStep[0m  [4/42], [94mLoss[0m : 2.08914
[1mStep[0m  [8/42], [94mLoss[0m : 2.18312
[1mStep[0m  [12/42], [94mLoss[0m : 2.40060
[1mStep[0m  [16/42], [94mLoss[0m : 2.39375
[1mStep[0m  [20/42], [94mLoss[0m : 2.26225
[1mStep[0m  [24/42], [94mLoss[0m : 2.23222
[1mStep[0m  [28/42], [94mLoss[0m : 2.12340
[1mStep[0m  [32/42], [94mLoss[0m : 2.03154
[1mStep[0m  [36/42], [94mLoss[0m : 2.15531
[1mStep[0m  [40/42], [94mLoss[0m : 2.26878

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.621, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21293
[1mStep[0m  [4/42], [94mLoss[0m : 2.37971
[1mStep[0m  [8/42], [94mLoss[0m : 2.30933
[1mStep[0m  [12/42], [94mLoss[0m : 2.38373
[1mStep[0m  [16/42], [94mLoss[0m : 2.05166
[1mStep[0m  [20/42], [94mLoss[0m : 2.08421
[1mStep[0m  [24/42], [94mLoss[0m : 2.16501
[1mStep[0m  [28/42], [94mLoss[0m : 2.19165
[1mStep[0m  [32/42], [94mLoss[0m : 2.31516
[1mStep[0m  [36/42], [94mLoss[0m : 2.32039
[1mStep[0m  [40/42], [94mLoss[0m : 2.16538

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.648, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28307
[1mStep[0m  [4/42], [94mLoss[0m : 2.16834
[1mStep[0m  [8/42], [94mLoss[0m : 2.27162
[1mStep[0m  [12/42], [94mLoss[0m : 2.13161
[1mStep[0m  [16/42], [94mLoss[0m : 2.22083
[1mStep[0m  [20/42], [94mLoss[0m : 2.03954
[1mStep[0m  [24/42], [94mLoss[0m : 2.21205
[1mStep[0m  [28/42], [94mLoss[0m : 2.09679
[1mStep[0m  [32/42], [94mLoss[0m : 2.12274
[1mStep[0m  [36/42], [94mLoss[0m : 1.98374
[1mStep[0m  [40/42], [94mLoss[0m : 2.14961

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.624, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11825
[1mStep[0m  [4/42], [94mLoss[0m : 1.96881
[1mStep[0m  [8/42], [94mLoss[0m : 2.12038
[1mStep[0m  [12/42], [94mLoss[0m : 2.08892
[1mStep[0m  [16/42], [94mLoss[0m : 2.04732
[1mStep[0m  [20/42], [94mLoss[0m : 2.25679
[1mStep[0m  [24/42], [94mLoss[0m : 2.23202
[1mStep[0m  [28/42], [94mLoss[0m : 1.93030
[1mStep[0m  [32/42], [94mLoss[0m : 2.18365
[1mStep[0m  [36/42], [94mLoss[0m : 2.07000
[1mStep[0m  [40/42], [94mLoss[0m : 2.30806

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.664, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05557
[1mStep[0m  [4/42], [94mLoss[0m : 2.11208
[1mStep[0m  [8/42], [94mLoss[0m : 1.95612
[1mStep[0m  [12/42], [94mLoss[0m : 2.11513
[1mStep[0m  [16/42], [94mLoss[0m : 2.08013
[1mStep[0m  [20/42], [94mLoss[0m : 1.92690
[1mStep[0m  [24/42], [94mLoss[0m : 2.08435
[1mStep[0m  [28/42], [94mLoss[0m : 2.45617
[1mStep[0m  [32/42], [94mLoss[0m : 2.11916
[1mStep[0m  [36/42], [94mLoss[0m : 2.30156
[1mStep[0m  [40/42], [94mLoss[0m : 2.25892

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.591, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81570
[1mStep[0m  [4/42], [94mLoss[0m : 2.13504
[1mStep[0m  [8/42], [94mLoss[0m : 2.13158
[1mStep[0m  [12/42], [94mLoss[0m : 2.14158
[1mStep[0m  [16/42], [94mLoss[0m : 2.16328
[1mStep[0m  [20/42], [94mLoss[0m : 2.10674
[1mStep[0m  [24/42], [94mLoss[0m : 2.07467
[1mStep[0m  [28/42], [94mLoss[0m : 1.94757
[1mStep[0m  [32/42], [94mLoss[0m : 2.08151
[1mStep[0m  [36/42], [94mLoss[0m : 2.33909
[1mStep[0m  [40/42], [94mLoss[0m : 2.02459

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.619, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06211
[1mStep[0m  [4/42], [94mLoss[0m : 1.90642
[1mStep[0m  [8/42], [94mLoss[0m : 2.01833
[1mStep[0m  [12/42], [94mLoss[0m : 2.02258
[1mStep[0m  [16/42], [94mLoss[0m : 2.14933
[1mStep[0m  [20/42], [94mLoss[0m : 2.11544
[1mStep[0m  [24/42], [94mLoss[0m : 2.01761
[1mStep[0m  [28/42], [94mLoss[0m : 2.06828
[1mStep[0m  [32/42], [94mLoss[0m : 2.08714
[1mStep[0m  [36/42], [94mLoss[0m : 2.20637
[1mStep[0m  [40/42], [94mLoss[0m : 2.12061

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.559, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89079
[1mStep[0m  [4/42], [94mLoss[0m : 2.26724
[1mStep[0m  [8/42], [94mLoss[0m : 2.00196
[1mStep[0m  [12/42], [94mLoss[0m : 1.91830
[1mStep[0m  [16/42], [94mLoss[0m : 1.93340
[1mStep[0m  [20/42], [94mLoss[0m : 1.91678
[1mStep[0m  [24/42], [94mLoss[0m : 2.20242
[1mStep[0m  [28/42], [94mLoss[0m : 2.13898
[1mStep[0m  [32/42], [94mLoss[0m : 2.04402
[1mStep[0m  [36/42], [94mLoss[0m : 2.08763
[1mStep[0m  [40/42], [94mLoss[0m : 2.15598

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76201
[1mStep[0m  [4/42], [94mLoss[0m : 2.10646
[1mStep[0m  [8/42], [94mLoss[0m : 1.91915
[1mStep[0m  [12/42], [94mLoss[0m : 1.80915
[1mStep[0m  [16/42], [94mLoss[0m : 2.09015
[1mStep[0m  [20/42], [94mLoss[0m : 2.06111
[1mStep[0m  [24/42], [94mLoss[0m : 1.77828
[1mStep[0m  [28/42], [94mLoss[0m : 1.95872
[1mStep[0m  [32/42], [94mLoss[0m : 2.10533
[1mStep[0m  [36/42], [94mLoss[0m : 2.09534
[1mStep[0m  [40/42], [94mLoss[0m : 2.07740

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03967
[1mStep[0m  [4/42], [94mLoss[0m : 2.18071
[1mStep[0m  [8/42], [94mLoss[0m : 1.77495
[1mStep[0m  [12/42], [94mLoss[0m : 1.90004
[1mStep[0m  [16/42], [94mLoss[0m : 1.85402
[1mStep[0m  [20/42], [94mLoss[0m : 2.08982
[1mStep[0m  [24/42], [94mLoss[0m : 1.90899
[1mStep[0m  [28/42], [94mLoss[0m : 2.14701
[1mStep[0m  [32/42], [94mLoss[0m : 2.16376
[1mStep[0m  [36/42], [94mLoss[0m : 1.93629
[1mStep[0m  [40/42], [94mLoss[0m : 1.85168

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83120
[1mStep[0m  [4/42], [94mLoss[0m : 1.98794
[1mStep[0m  [8/42], [94mLoss[0m : 1.96296
[1mStep[0m  [12/42], [94mLoss[0m : 2.06103
[1mStep[0m  [16/42], [94mLoss[0m : 2.08421
[1mStep[0m  [20/42], [94mLoss[0m : 2.00512
[1mStep[0m  [24/42], [94mLoss[0m : 2.06962
[1mStep[0m  [28/42], [94mLoss[0m : 1.93576
[1mStep[0m  [32/42], [94mLoss[0m : 1.97469
[1mStep[0m  [36/42], [94mLoss[0m : 2.09721
[1mStep[0m  [40/42], [94mLoss[0m : 1.85880

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88461
[1mStep[0m  [4/42], [94mLoss[0m : 1.91964
[1mStep[0m  [8/42], [94mLoss[0m : 2.03378
[1mStep[0m  [12/42], [94mLoss[0m : 1.91155
[1mStep[0m  [16/42], [94mLoss[0m : 2.23019
[1mStep[0m  [20/42], [94mLoss[0m : 1.97026
[1mStep[0m  [24/42], [94mLoss[0m : 1.95443
[1mStep[0m  [28/42], [94mLoss[0m : 1.80445
[1mStep[0m  [32/42], [94mLoss[0m : 1.94679
[1mStep[0m  [36/42], [94mLoss[0m : 2.02214
[1mStep[0m  [40/42], [94mLoss[0m : 1.96412

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.549, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06025
[1mStep[0m  [4/42], [94mLoss[0m : 1.90392
[1mStep[0m  [8/42], [94mLoss[0m : 1.95829
[1mStep[0m  [12/42], [94mLoss[0m : 2.03476
[1mStep[0m  [16/42], [94mLoss[0m : 1.73994
[1mStep[0m  [20/42], [94mLoss[0m : 1.93991
[1mStep[0m  [24/42], [94mLoss[0m : 1.92745
[1mStep[0m  [28/42], [94mLoss[0m : 1.97668
[1mStep[0m  [32/42], [94mLoss[0m : 1.81015
[1mStep[0m  [36/42], [94mLoss[0m : 1.97329
[1mStep[0m  [40/42], [94mLoss[0m : 1.99115

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.501, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95752
[1mStep[0m  [4/42], [94mLoss[0m : 1.81621
[1mStep[0m  [8/42], [94mLoss[0m : 1.92097
[1mStep[0m  [12/42], [94mLoss[0m : 1.89209
[1mStep[0m  [16/42], [94mLoss[0m : 1.96918
[1mStep[0m  [20/42], [94mLoss[0m : 1.84140
[1mStep[0m  [24/42], [94mLoss[0m : 1.98446
[1mStep[0m  [28/42], [94mLoss[0m : 1.85267
[1mStep[0m  [32/42], [94mLoss[0m : 1.94762
[1mStep[0m  [36/42], [94mLoss[0m : 1.82034
[1mStep[0m  [40/42], [94mLoss[0m : 1.93557

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.579, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.475
====================================

Phase 2 - Evaluation MAE:  2.475439889090402
MAE score P1      2.337044
MAE score P2       2.47544
loss              1.912425
learning_rate      0.00505
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.47163
[1mStep[0m  [4/42], [94mLoss[0m : 11.09007
[1mStep[0m  [8/42], [94mLoss[0m : 10.59054
[1mStep[0m  [12/42], [94mLoss[0m : 10.58323
[1mStep[0m  [16/42], [94mLoss[0m : 10.00367
[1mStep[0m  [20/42], [94mLoss[0m : 9.96294
[1mStep[0m  [24/42], [94mLoss[0m : 9.26615
[1mStep[0m  [28/42], [94mLoss[0m : 8.90083
[1mStep[0m  [32/42], [94mLoss[0m : 8.75937
[1mStep[0m  [36/42], [94mLoss[0m : 8.70722
[1mStep[0m  [40/42], [94mLoss[0m : 8.21801

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.766, [92mTest[0m: 11.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.31966
[1mStep[0m  [4/42], [94mLoss[0m : 7.78957
[1mStep[0m  [8/42], [94mLoss[0m : 7.12123
[1mStep[0m  [12/42], [94mLoss[0m : 7.27428
[1mStep[0m  [16/42], [94mLoss[0m : 7.01259
[1mStep[0m  [20/42], [94mLoss[0m : 6.52200
[1mStep[0m  [24/42], [94mLoss[0m : 6.06558
[1mStep[0m  [28/42], [94mLoss[0m : 5.97423
[1mStep[0m  [32/42], [94mLoss[0m : 5.39766
[1mStep[0m  [36/42], [94mLoss[0m : 5.77936
[1mStep[0m  [40/42], [94mLoss[0m : 5.46846

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.578, [92mTest[0m: 8.105, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.94586
[1mStep[0m  [4/42], [94mLoss[0m : 4.69483
[1mStep[0m  [8/42], [94mLoss[0m : 4.72188
[1mStep[0m  [12/42], [94mLoss[0m : 4.10368
[1mStep[0m  [16/42], [94mLoss[0m : 4.23302
[1mStep[0m  [20/42], [94mLoss[0m : 4.12895
[1mStep[0m  [24/42], [94mLoss[0m : 3.58884
[1mStep[0m  [28/42], [94mLoss[0m : 3.78424
[1mStep[0m  [32/42], [94mLoss[0m : 3.50003
[1mStep[0m  [36/42], [94mLoss[0m : 3.57593
[1mStep[0m  [40/42], [94mLoss[0m : 3.35707

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.051, [92mTest[0m: 4.975, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.25860
[1mStep[0m  [4/42], [94mLoss[0m : 3.47332
[1mStep[0m  [8/42], [94mLoss[0m : 3.04647
[1mStep[0m  [12/42], [94mLoss[0m : 2.93416
[1mStep[0m  [16/42], [94mLoss[0m : 3.05672
[1mStep[0m  [20/42], [94mLoss[0m : 2.85579
[1mStep[0m  [24/42], [94mLoss[0m : 2.94837
[1mStep[0m  [28/42], [94mLoss[0m : 3.35182
[1mStep[0m  [32/42], [94mLoss[0m : 2.71095
[1mStep[0m  [36/42], [94mLoss[0m : 2.93454
[1mStep[0m  [40/42], [94mLoss[0m : 2.80151

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.020, [92mTest[0m: 3.198, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.22723
[1mStep[0m  [4/42], [94mLoss[0m : 2.54882
[1mStep[0m  [8/42], [94mLoss[0m : 2.60262
[1mStep[0m  [12/42], [94mLoss[0m : 2.74407
[1mStep[0m  [16/42], [94mLoss[0m : 2.96983
[1mStep[0m  [20/42], [94mLoss[0m : 2.61346
[1mStep[0m  [24/42], [94mLoss[0m : 2.84620
[1mStep[0m  [28/42], [94mLoss[0m : 2.61202
[1mStep[0m  [32/42], [94mLoss[0m : 2.79269
[1mStep[0m  [36/42], [94mLoss[0m : 2.80954
[1mStep[0m  [40/42], [94mLoss[0m : 2.84345

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.605, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42319
[1mStep[0m  [4/42], [94mLoss[0m : 2.46769
[1mStep[0m  [8/42], [94mLoss[0m : 2.71850
[1mStep[0m  [12/42], [94mLoss[0m : 2.75038
[1mStep[0m  [16/42], [94mLoss[0m : 2.58996
[1mStep[0m  [20/42], [94mLoss[0m : 2.69463
[1mStep[0m  [24/42], [94mLoss[0m : 2.58938
[1mStep[0m  [28/42], [94mLoss[0m : 2.58106
[1mStep[0m  [32/42], [94mLoss[0m : 2.77855
[1mStep[0m  [36/42], [94mLoss[0m : 2.57249
[1mStep[0m  [40/42], [94mLoss[0m : 2.75216

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26607
[1mStep[0m  [4/42], [94mLoss[0m : 2.55187
[1mStep[0m  [8/42], [94mLoss[0m : 2.49803
[1mStep[0m  [12/42], [94mLoss[0m : 2.49964
[1mStep[0m  [16/42], [94mLoss[0m : 2.56704
[1mStep[0m  [20/42], [94mLoss[0m : 2.51744
[1mStep[0m  [24/42], [94mLoss[0m : 2.72306
[1mStep[0m  [28/42], [94mLoss[0m : 2.50241
[1mStep[0m  [32/42], [94mLoss[0m : 2.53395
[1mStep[0m  [36/42], [94mLoss[0m : 2.60577
[1mStep[0m  [40/42], [94mLoss[0m : 2.56330

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56015
[1mStep[0m  [4/42], [94mLoss[0m : 2.58186
[1mStep[0m  [8/42], [94mLoss[0m : 2.61572
[1mStep[0m  [12/42], [94mLoss[0m : 2.26868
[1mStep[0m  [16/42], [94mLoss[0m : 2.66153
[1mStep[0m  [20/42], [94mLoss[0m : 2.56850
[1mStep[0m  [24/42], [94mLoss[0m : 2.68667
[1mStep[0m  [28/42], [94mLoss[0m : 2.71761
[1mStep[0m  [32/42], [94mLoss[0m : 2.55698
[1mStep[0m  [36/42], [94mLoss[0m : 2.69265
[1mStep[0m  [40/42], [94mLoss[0m : 2.47495

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47961
[1mStep[0m  [4/42], [94mLoss[0m : 2.72432
[1mStep[0m  [8/42], [94mLoss[0m : 2.80732
[1mStep[0m  [12/42], [94mLoss[0m : 2.56492
[1mStep[0m  [16/42], [94mLoss[0m : 2.32258
[1mStep[0m  [20/42], [94mLoss[0m : 2.35224
[1mStep[0m  [24/42], [94mLoss[0m : 2.51116
[1mStep[0m  [28/42], [94mLoss[0m : 2.69108
[1mStep[0m  [32/42], [94mLoss[0m : 2.48764
[1mStep[0m  [36/42], [94mLoss[0m : 2.56999
[1mStep[0m  [40/42], [94mLoss[0m : 2.58549

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44507
[1mStep[0m  [4/42], [94mLoss[0m : 2.84817
[1mStep[0m  [8/42], [94mLoss[0m : 2.52759
[1mStep[0m  [12/42], [94mLoss[0m : 2.51757
[1mStep[0m  [16/42], [94mLoss[0m : 2.47907
[1mStep[0m  [20/42], [94mLoss[0m : 2.39530
[1mStep[0m  [24/42], [94mLoss[0m : 2.49634
[1mStep[0m  [28/42], [94mLoss[0m : 2.48333
[1mStep[0m  [32/42], [94mLoss[0m : 2.43490
[1mStep[0m  [36/42], [94mLoss[0m : 2.44696
[1mStep[0m  [40/42], [94mLoss[0m : 2.50982

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70452
[1mStep[0m  [4/42], [94mLoss[0m : 2.43300
[1mStep[0m  [8/42], [94mLoss[0m : 2.59137
[1mStep[0m  [12/42], [94mLoss[0m : 2.54122
[1mStep[0m  [16/42], [94mLoss[0m : 2.70642
[1mStep[0m  [20/42], [94mLoss[0m : 2.65391
[1mStep[0m  [24/42], [94mLoss[0m : 2.41308
[1mStep[0m  [28/42], [94mLoss[0m : 2.80820
[1mStep[0m  [32/42], [94mLoss[0m : 2.58304
[1mStep[0m  [36/42], [94mLoss[0m : 2.40503
[1mStep[0m  [40/42], [94mLoss[0m : 2.34032

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47946
[1mStep[0m  [4/42], [94mLoss[0m : 2.63973
[1mStep[0m  [8/42], [94mLoss[0m : 2.65825
[1mStep[0m  [12/42], [94mLoss[0m : 2.51049
[1mStep[0m  [16/42], [94mLoss[0m : 2.45424
[1mStep[0m  [20/42], [94mLoss[0m : 2.57702
[1mStep[0m  [24/42], [94mLoss[0m : 2.27877
[1mStep[0m  [28/42], [94mLoss[0m : 2.67441
[1mStep[0m  [32/42], [94mLoss[0m : 2.67564
[1mStep[0m  [36/42], [94mLoss[0m : 2.51427
[1mStep[0m  [40/42], [94mLoss[0m : 2.66771

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51144
[1mStep[0m  [4/42], [94mLoss[0m : 2.48300
[1mStep[0m  [8/42], [94mLoss[0m : 2.61290
[1mStep[0m  [12/42], [94mLoss[0m : 2.44980
[1mStep[0m  [16/42], [94mLoss[0m : 2.36099
[1mStep[0m  [20/42], [94mLoss[0m : 2.45353
[1mStep[0m  [24/42], [94mLoss[0m : 2.59066
[1mStep[0m  [28/42], [94mLoss[0m : 2.54485
[1mStep[0m  [32/42], [94mLoss[0m : 2.56652
[1mStep[0m  [36/42], [94mLoss[0m : 2.47353
[1mStep[0m  [40/42], [94mLoss[0m : 2.66781

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47675
[1mStep[0m  [4/42], [94mLoss[0m : 2.33211
[1mStep[0m  [8/42], [94mLoss[0m : 2.43546
[1mStep[0m  [12/42], [94mLoss[0m : 2.55873
[1mStep[0m  [16/42], [94mLoss[0m : 2.57090
[1mStep[0m  [20/42], [94mLoss[0m : 2.53903
[1mStep[0m  [24/42], [94mLoss[0m : 2.34268
[1mStep[0m  [28/42], [94mLoss[0m : 2.52446
[1mStep[0m  [32/42], [94mLoss[0m : 2.56769
[1mStep[0m  [36/42], [94mLoss[0m : 2.29680
[1mStep[0m  [40/42], [94mLoss[0m : 2.54341

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38785
[1mStep[0m  [4/42], [94mLoss[0m : 2.43753
[1mStep[0m  [8/42], [94mLoss[0m : 2.25554
[1mStep[0m  [12/42], [94mLoss[0m : 2.28922
[1mStep[0m  [16/42], [94mLoss[0m : 2.33222
[1mStep[0m  [20/42], [94mLoss[0m : 2.81373
[1mStep[0m  [24/42], [94mLoss[0m : 2.60932
[1mStep[0m  [28/42], [94mLoss[0m : 2.63869
[1mStep[0m  [32/42], [94mLoss[0m : 2.49253
[1mStep[0m  [36/42], [94mLoss[0m : 2.43060
[1mStep[0m  [40/42], [94mLoss[0m : 2.46085

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48194
[1mStep[0m  [4/42], [94mLoss[0m : 2.43817
[1mStep[0m  [8/42], [94mLoss[0m : 2.66016
[1mStep[0m  [12/42], [94mLoss[0m : 2.59984
[1mStep[0m  [16/42], [94mLoss[0m : 2.55310
[1mStep[0m  [20/42], [94mLoss[0m : 2.46483
[1mStep[0m  [24/42], [94mLoss[0m : 2.49089
[1mStep[0m  [28/42], [94mLoss[0m : 2.50312
[1mStep[0m  [32/42], [94mLoss[0m : 2.61825
[1mStep[0m  [36/42], [94mLoss[0m : 2.49639
[1mStep[0m  [40/42], [94mLoss[0m : 2.52275

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36570
[1mStep[0m  [4/42], [94mLoss[0m : 2.46884
[1mStep[0m  [8/42], [94mLoss[0m : 2.18252
[1mStep[0m  [12/42], [94mLoss[0m : 2.68739
[1mStep[0m  [16/42], [94mLoss[0m : 2.54778
[1mStep[0m  [20/42], [94mLoss[0m : 2.72436
[1mStep[0m  [24/42], [94mLoss[0m : 2.82595
[1mStep[0m  [28/42], [94mLoss[0m : 2.46724
[1mStep[0m  [32/42], [94mLoss[0m : 2.66295
[1mStep[0m  [36/42], [94mLoss[0m : 2.71384
[1mStep[0m  [40/42], [94mLoss[0m : 2.52396

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79360
[1mStep[0m  [4/42], [94mLoss[0m : 2.60509
[1mStep[0m  [8/42], [94mLoss[0m : 2.53497
[1mStep[0m  [12/42], [94mLoss[0m : 2.47906
[1mStep[0m  [16/42], [94mLoss[0m : 2.58985
[1mStep[0m  [20/42], [94mLoss[0m : 2.50901
[1mStep[0m  [24/42], [94mLoss[0m : 2.56366
[1mStep[0m  [28/42], [94mLoss[0m : 2.52608
[1mStep[0m  [32/42], [94mLoss[0m : 2.55657
[1mStep[0m  [36/42], [94mLoss[0m : 2.62330
[1mStep[0m  [40/42], [94mLoss[0m : 2.58494

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50690
[1mStep[0m  [4/42], [94mLoss[0m : 2.51754
[1mStep[0m  [8/42], [94mLoss[0m : 2.53204
[1mStep[0m  [12/42], [94mLoss[0m : 2.37164
[1mStep[0m  [16/42], [94mLoss[0m : 2.26142
[1mStep[0m  [20/42], [94mLoss[0m : 2.27503
[1mStep[0m  [24/42], [94mLoss[0m : 2.65552
[1mStep[0m  [28/42], [94mLoss[0m : 2.47582
[1mStep[0m  [32/42], [94mLoss[0m : 2.45285
[1mStep[0m  [36/42], [94mLoss[0m : 2.48487
[1mStep[0m  [40/42], [94mLoss[0m : 2.59486

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26265
[1mStep[0m  [4/42], [94mLoss[0m : 2.59443
[1mStep[0m  [8/42], [94mLoss[0m : 2.37693
[1mStep[0m  [12/42], [94mLoss[0m : 2.55961
[1mStep[0m  [16/42], [94mLoss[0m : 2.52008
[1mStep[0m  [20/42], [94mLoss[0m : 2.46957
[1mStep[0m  [24/42], [94mLoss[0m : 2.46925
[1mStep[0m  [28/42], [94mLoss[0m : 2.87498
[1mStep[0m  [32/42], [94mLoss[0m : 2.61812
[1mStep[0m  [36/42], [94mLoss[0m : 2.81915
[1mStep[0m  [40/42], [94mLoss[0m : 2.60393

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56609
[1mStep[0m  [4/42], [94mLoss[0m : 2.47347
[1mStep[0m  [8/42], [94mLoss[0m : 2.84600
[1mStep[0m  [12/42], [94mLoss[0m : 2.52463
[1mStep[0m  [16/42], [94mLoss[0m : 2.77840
[1mStep[0m  [20/42], [94mLoss[0m : 2.51097
[1mStep[0m  [24/42], [94mLoss[0m : 2.40397
[1mStep[0m  [28/42], [94mLoss[0m : 2.37284
[1mStep[0m  [32/42], [94mLoss[0m : 2.45573
[1mStep[0m  [36/42], [94mLoss[0m : 2.49939
[1mStep[0m  [40/42], [94mLoss[0m : 2.59316

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37605
[1mStep[0m  [4/42], [94mLoss[0m : 2.41865
[1mStep[0m  [8/42], [94mLoss[0m : 2.28519
[1mStep[0m  [12/42], [94mLoss[0m : 2.63791
[1mStep[0m  [16/42], [94mLoss[0m : 2.72691
[1mStep[0m  [20/42], [94mLoss[0m : 2.40705
[1mStep[0m  [24/42], [94mLoss[0m : 2.44503
[1mStep[0m  [28/42], [94mLoss[0m : 2.65992
[1mStep[0m  [32/42], [94mLoss[0m : 2.35990
[1mStep[0m  [36/42], [94mLoss[0m : 2.50883
[1mStep[0m  [40/42], [94mLoss[0m : 2.47014

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54010
[1mStep[0m  [4/42], [94mLoss[0m : 2.67344
[1mStep[0m  [8/42], [94mLoss[0m : 2.54568
[1mStep[0m  [12/42], [94mLoss[0m : 2.55909
[1mStep[0m  [16/42], [94mLoss[0m : 2.35697
[1mStep[0m  [20/42], [94mLoss[0m : 2.49604
[1mStep[0m  [24/42], [94mLoss[0m : 2.42416
[1mStep[0m  [28/42], [94mLoss[0m : 2.55824
[1mStep[0m  [32/42], [94mLoss[0m : 2.51279
[1mStep[0m  [36/42], [94mLoss[0m : 2.64685
[1mStep[0m  [40/42], [94mLoss[0m : 2.59362

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49686
[1mStep[0m  [4/42], [94mLoss[0m : 2.45337
[1mStep[0m  [8/42], [94mLoss[0m : 2.40044
[1mStep[0m  [12/42], [94mLoss[0m : 2.53779
[1mStep[0m  [16/42], [94mLoss[0m : 2.39759
[1mStep[0m  [20/42], [94mLoss[0m : 2.53489
[1mStep[0m  [24/42], [94mLoss[0m : 2.50813
[1mStep[0m  [28/42], [94mLoss[0m : 2.60584
[1mStep[0m  [32/42], [94mLoss[0m : 2.45843
[1mStep[0m  [36/42], [94mLoss[0m : 2.53976
[1mStep[0m  [40/42], [94mLoss[0m : 2.40000

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42408
[1mStep[0m  [4/42], [94mLoss[0m : 2.62863
[1mStep[0m  [8/42], [94mLoss[0m : 2.44912
[1mStep[0m  [12/42], [94mLoss[0m : 2.59801
[1mStep[0m  [16/42], [94mLoss[0m : 2.37381
[1mStep[0m  [20/42], [94mLoss[0m : 2.71469
[1mStep[0m  [24/42], [94mLoss[0m : 2.73451
[1mStep[0m  [28/42], [94mLoss[0m : 2.61933
[1mStep[0m  [32/42], [94mLoss[0m : 2.43212
[1mStep[0m  [36/42], [94mLoss[0m : 2.57638
[1mStep[0m  [40/42], [94mLoss[0m : 2.43822

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56163
[1mStep[0m  [4/42], [94mLoss[0m : 2.74927
[1mStep[0m  [8/42], [94mLoss[0m : 2.75193
[1mStep[0m  [12/42], [94mLoss[0m : 2.48909
[1mStep[0m  [16/42], [94mLoss[0m : 2.58330
[1mStep[0m  [20/42], [94mLoss[0m : 2.38822
[1mStep[0m  [24/42], [94mLoss[0m : 2.34639
[1mStep[0m  [28/42], [94mLoss[0m : 2.56989
[1mStep[0m  [32/42], [94mLoss[0m : 2.46119
[1mStep[0m  [36/42], [94mLoss[0m : 2.61188
[1mStep[0m  [40/42], [94mLoss[0m : 2.42712

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76091
[1mStep[0m  [4/42], [94mLoss[0m : 2.52096
[1mStep[0m  [8/42], [94mLoss[0m : 2.40666
[1mStep[0m  [12/42], [94mLoss[0m : 2.62223
[1mStep[0m  [16/42], [94mLoss[0m : 2.63916
[1mStep[0m  [20/42], [94mLoss[0m : 2.44664
[1mStep[0m  [24/42], [94mLoss[0m : 2.71054
[1mStep[0m  [28/42], [94mLoss[0m : 2.35794
[1mStep[0m  [32/42], [94mLoss[0m : 2.74319
[1mStep[0m  [36/42], [94mLoss[0m : 2.58822
[1mStep[0m  [40/42], [94mLoss[0m : 2.52613

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36673
[1mStep[0m  [4/42], [94mLoss[0m : 2.37480
[1mStep[0m  [8/42], [94mLoss[0m : 2.39093
[1mStep[0m  [12/42], [94mLoss[0m : 2.34821
[1mStep[0m  [16/42], [94mLoss[0m : 2.53065
[1mStep[0m  [20/42], [94mLoss[0m : 2.30646
[1mStep[0m  [24/42], [94mLoss[0m : 2.26652
[1mStep[0m  [28/42], [94mLoss[0m : 2.59845
[1mStep[0m  [32/42], [94mLoss[0m : 2.47695
[1mStep[0m  [36/42], [94mLoss[0m : 2.54629
[1mStep[0m  [40/42], [94mLoss[0m : 2.65729

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61492
[1mStep[0m  [4/42], [94mLoss[0m : 2.58400
[1mStep[0m  [8/42], [94mLoss[0m : 2.49543
[1mStep[0m  [12/42], [94mLoss[0m : 2.58084
[1mStep[0m  [16/42], [94mLoss[0m : 2.47462
[1mStep[0m  [20/42], [94mLoss[0m : 2.52932
[1mStep[0m  [24/42], [94mLoss[0m : 2.56927
[1mStep[0m  [28/42], [94mLoss[0m : 2.51206
[1mStep[0m  [32/42], [94mLoss[0m : 2.29395
[1mStep[0m  [36/42], [94mLoss[0m : 2.43085
[1mStep[0m  [40/42], [94mLoss[0m : 2.56861

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45762
[1mStep[0m  [4/42], [94mLoss[0m : 2.45391
[1mStep[0m  [8/42], [94mLoss[0m : 2.63185
[1mStep[0m  [12/42], [94mLoss[0m : 2.44152
[1mStep[0m  [16/42], [94mLoss[0m : 2.53926
[1mStep[0m  [20/42], [94mLoss[0m : 2.32775
[1mStep[0m  [24/42], [94mLoss[0m : 2.36298
[1mStep[0m  [28/42], [94mLoss[0m : 2.45682
[1mStep[0m  [32/42], [94mLoss[0m : 2.40461
[1mStep[0m  [36/42], [94mLoss[0m : 2.42997
[1mStep[0m  [40/42], [94mLoss[0m : 2.39095

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3372979504721507
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.62097
[1mStep[0m  [4/42], [94mLoss[0m : 2.68203
[1mStep[0m  [8/42], [94mLoss[0m : 2.09679
[1mStep[0m  [12/42], [94mLoss[0m : 2.31823
[1mStep[0m  [16/42], [94mLoss[0m : 2.47991
[1mStep[0m  [20/42], [94mLoss[0m : 2.38235
[1mStep[0m  [24/42], [94mLoss[0m : 2.37165
[1mStep[0m  [28/42], [94mLoss[0m : 2.52802
[1mStep[0m  [32/42], [94mLoss[0m : 2.68412
[1mStep[0m  [36/42], [94mLoss[0m : 2.52414
[1mStep[0m  [40/42], [94mLoss[0m : 2.54731

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43514
[1mStep[0m  [4/42], [94mLoss[0m : 2.28845
[1mStep[0m  [8/42], [94mLoss[0m : 2.41320
[1mStep[0m  [12/42], [94mLoss[0m : 2.60506
[1mStep[0m  [16/42], [94mLoss[0m : 2.48945
[1mStep[0m  [20/42], [94mLoss[0m : 2.48134
[1mStep[0m  [24/42], [94mLoss[0m : 2.40475
[1mStep[0m  [28/42], [94mLoss[0m : 2.51902
[1mStep[0m  [32/42], [94mLoss[0m : 2.55544
[1mStep[0m  [36/42], [94mLoss[0m : 2.32484
[1mStep[0m  [40/42], [94mLoss[0m : 2.60003

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45304
[1mStep[0m  [4/42], [94mLoss[0m : 2.54139
[1mStep[0m  [8/42], [94mLoss[0m : 2.37587
[1mStep[0m  [12/42], [94mLoss[0m : 2.48313
[1mStep[0m  [16/42], [94mLoss[0m : 2.69357
[1mStep[0m  [20/42], [94mLoss[0m : 2.62065
[1mStep[0m  [24/42], [94mLoss[0m : 2.56470
[1mStep[0m  [28/42], [94mLoss[0m : 2.56221
[1mStep[0m  [32/42], [94mLoss[0m : 2.35059
[1mStep[0m  [36/42], [94mLoss[0m : 2.47575
[1mStep[0m  [40/42], [94mLoss[0m : 2.69601

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18135
[1mStep[0m  [4/42], [94mLoss[0m : 2.62441
[1mStep[0m  [8/42], [94mLoss[0m : 2.53308
[1mStep[0m  [12/42], [94mLoss[0m : 2.46330
[1mStep[0m  [16/42], [94mLoss[0m : 2.35725
[1mStep[0m  [20/42], [94mLoss[0m : 2.35479
[1mStep[0m  [24/42], [94mLoss[0m : 2.42208
[1mStep[0m  [28/42], [94mLoss[0m : 2.70718
[1mStep[0m  [32/42], [94mLoss[0m : 2.51055
[1mStep[0m  [36/42], [94mLoss[0m : 2.40663
[1mStep[0m  [40/42], [94mLoss[0m : 2.60792

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67740
[1mStep[0m  [4/42], [94mLoss[0m : 2.57017
[1mStep[0m  [8/42], [94mLoss[0m : 2.62199
[1mStep[0m  [12/42], [94mLoss[0m : 2.43633
[1mStep[0m  [16/42], [94mLoss[0m : 2.54778
[1mStep[0m  [20/42], [94mLoss[0m : 2.43204
[1mStep[0m  [24/42], [94mLoss[0m : 2.51627
[1mStep[0m  [28/42], [94mLoss[0m : 2.48049
[1mStep[0m  [32/42], [94mLoss[0m : 2.58642
[1mStep[0m  [36/42], [94mLoss[0m : 2.33753
[1mStep[0m  [40/42], [94mLoss[0m : 2.28176

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.625, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36521
[1mStep[0m  [4/42], [94mLoss[0m : 2.55240
[1mStep[0m  [8/42], [94mLoss[0m : 2.33511
[1mStep[0m  [12/42], [94mLoss[0m : 2.47681
[1mStep[0m  [16/42], [94mLoss[0m : 2.23209
[1mStep[0m  [20/42], [94mLoss[0m : 2.61499
[1mStep[0m  [24/42], [94mLoss[0m : 2.32148
[1mStep[0m  [28/42], [94mLoss[0m : 2.31983
[1mStep[0m  [32/42], [94mLoss[0m : 2.60117
[1mStep[0m  [36/42], [94mLoss[0m : 2.23603
[1mStep[0m  [40/42], [94mLoss[0m : 2.50133

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.637, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24445
[1mStep[0m  [4/42], [94mLoss[0m : 2.29742
[1mStep[0m  [8/42], [94mLoss[0m : 2.49831
[1mStep[0m  [12/42], [94mLoss[0m : 2.59677
[1mStep[0m  [16/42], [94mLoss[0m : 2.50497
[1mStep[0m  [20/42], [94mLoss[0m : 2.30758
[1mStep[0m  [24/42], [94mLoss[0m : 2.37240
[1mStep[0m  [28/42], [94mLoss[0m : 2.44005
[1mStep[0m  [32/42], [94mLoss[0m : 2.46353
[1mStep[0m  [36/42], [94mLoss[0m : 2.48643
[1mStep[0m  [40/42], [94mLoss[0m : 2.33516

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.608, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40880
[1mStep[0m  [4/42], [94mLoss[0m : 2.51967
[1mStep[0m  [8/42], [94mLoss[0m : 2.38420
[1mStep[0m  [12/42], [94mLoss[0m : 2.27608
[1mStep[0m  [16/42], [94mLoss[0m : 2.48485
[1mStep[0m  [20/42], [94mLoss[0m : 2.53746
[1mStep[0m  [24/42], [94mLoss[0m : 2.45248
[1mStep[0m  [28/42], [94mLoss[0m : 2.35733
[1mStep[0m  [32/42], [94mLoss[0m : 2.32253
[1mStep[0m  [36/42], [94mLoss[0m : 2.38221
[1mStep[0m  [40/42], [94mLoss[0m : 2.64972

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.621, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50590
[1mStep[0m  [4/42], [94mLoss[0m : 2.23993
[1mStep[0m  [8/42], [94mLoss[0m : 2.18372
[1mStep[0m  [12/42], [94mLoss[0m : 2.23166
[1mStep[0m  [16/42], [94mLoss[0m : 2.06805
[1mStep[0m  [20/42], [94mLoss[0m : 2.60540
[1mStep[0m  [24/42], [94mLoss[0m : 2.42383
[1mStep[0m  [28/42], [94mLoss[0m : 2.37152
[1mStep[0m  [32/42], [94mLoss[0m : 2.37341
[1mStep[0m  [36/42], [94mLoss[0m : 2.32548
[1mStep[0m  [40/42], [94mLoss[0m : 2.55432

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.630, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45879
[1mStep[0m  [4/42], [94mLoss[0m : 2.38651
[1mStep[0m  [8/42], [94mLoss[0m : 2.46550
[1mStep[0m  [12/42], [94mLoss[0m : 2.31624
[1mStep[0m  [16/42], [94mLoss[0m : 2.40531
[1mStep[0m  [20/42], [94mLoss[0m : 2.29116
[1mStep[0m  [24/42], [94mLoss[0m : 2.27050
[1mStep[0m  [28/42], [94mLoss[0m : 2.41758
[1mStep[0m  [32/42], [94mLoss[0m : 2.22780
[1mStep[0m  [36/42], [94mLoss[0m : 2.48766
[1mStep[0m  [40/42], [94mLoss[0m : 2.52070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.669, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46889
[1mStep[0m  [4/42], [94mLoss[0m : 2.45759
[1mStep[0m  [8/42], [94mLoss[0m : 2.25618
[1mStep[0m  [12/42], [94mLoss[0m : 2.39085
[1mStep[0m  [16/42], [94mLoss[0m : 2.37609
[1mStep[0m  [20/42], [94mLoss[0m : 2.14948
[1mStep[0m  [24/42], [94mLoss[0m : 2.32145
[1mStep[0m  [28/42], [94mLoss[0m : 2.47035
[1mStep[0m  [32/42], [94mLoss[0m : 2.44739
[1mStep[0m  [36/42], [94mLoss[0m : 2.48899
[1mStep[0m  [40/42], [94mLoss[0m : 2.45815

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.677, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46579
[1mStep[0m  [4/42], [94mLoss[0m : 2.32735
[1mStep[0m  [8/42], [94mLoss[0m : 2.33898
[1mStep[0m  [12/42], [94mLoss[0m : 2.23887
[1mStep[0m  [16/42], [94mLoss[0m : 2.31172
[1mStep[0m  [20/42], [94mLoss[0m : 2.18532
[1mStep[0m  [24/42], [94mLoss[0m : 2.37655
[1mStep[0m  [28/42], [94mLoss[0m : 2.32511
[1mStep[0m  [32/42], [94mLoss[0m : 2.37969
[1mStep[0m  [36/42], [94mLoss[0m : 2.57243
[1mStep[0m  [40/42], [94mLoss[0m : 2.28507

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.679, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45028
[1mStep[0m  [4/42], [94mLoss[0m : 2.26126
[1mStep[0m  [8/42], [94mLoss[0m : 2.27119
[1mStep[0m  [12/42], [94mLoss[0m : 2.47075
[1mStep[0m  [16/42], [94mLoss[0m : 2.30795
[1mStep[0m  [20/42], [94mLoss[0m : 2.51011
[1mStep[0m  [24/42], [94mLoss[0m : 2.25806
[1mStep[0m  [28/42], [94mLoss[0m : 2.44878
[1mStep[0m  [32/42], [94mLoss[0m : 2.28780
[1mStep[0m  [36/42], [94mLoss[0m : 2.24141
[1mStep[0m  [40/42], [94mLoss[0m : 2.33814

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.656, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36744
[1mStep[0m  [4/42], [94mLoss[0m : 2.23699
[1mStep[0m  [8/42], [94mLoss[0m : 2.29782
[1mStep[0m  [12/42], [94mLoss[0m : 2.13543
[1mStep[0m  [16/42], [94mLoss[0m : 2.22979
[1mStep[0m  [20/42], [94mLoss[0m : 2.49117
[1mStep[0m  [24/42], [94mLoss[0m : 2.39692
[1mStep[0m  [28/42], [94mLoss[0m : 2.16048
[1mStep[0m  [32/42], [94mLoss[0m : 2.24695
[1mStep[0m  [36/42], [94mLoss[0m : 2.21271
[1mStep[0m  [40/42], [94mLoss[0m : 2.36308

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.745, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37939
[1mStep[0m  [4/42], [94mLoss[0m : 2.37838
[1mStep[0m  [8/42], [94mLoss[0m : 2.37258
[1mStep[0m  [12/42], [94mLoss[0m : 2.40050
[1mStep[0m  [16/42], [94mLoss[0m : 2.22881
[1mStep[0m  [20/42], [94mLoss[0m : 2.25017
[1mStep[0m  [24/42], [94mLoss[0m : 2.28731
[1mStep[0m  [28/42], [94mLoss[0m : 2.43688
[1mStep[0m  [32/42], [94mLoss[0m : 1.98219
[1mStep[0m  [36/42], [94mLoss[0m : 2.31624
[1mStep[0m  [40/42], [94mLoss[0m : 2.37056

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.689, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24061
[1mStep[0m  [4/42], [94mLoss[0m : 2.28726
[1mStep[0m  [8/42], [94mLoss[0m : 2.21445
[1mStep[0m  [12/42], [94mLoss[0m : 2.47391
[1mStep[0m  [16/42], [94mLoss[0m : 2.02690
[1mStep[0m  [20/42], [94mLoss[0m : 2.16857
[1mStep[0m  [24/42], [94mLoss[0m : 2.37295
[1mStep[0m  [28/42], [94mLoss[0m : 2.25151
[1mStep[0m  [32/42], [94mLoss[0m : 2.46677
[1mStep[0m  [36/42], [94mLoss[0m : 2.14003
[1mStep[0m  [40/42], [94mLoss[0m : 2.20544

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.688, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29678
[1mStep[0m  [4/42], [94mLoss[0m : 2.22392
[1mStep[0m  [8/42], [94mLoss[0m : 2.11622
[1mStep[0m  [12/42], [94mLoss[0m : 2.42839
[1mStep[0m  [16/42], [94mLoss[0m : 2.30037
[1mStep[0m  [20/42], [94mLoss[0m : 2.14993
[1mStep[0m  [24/42], [94mLoss[0m : 2.18890
[1mStep[0m  [28/42], [94mLoss[0m : 2.26518
[1mStep[0m  [32/42], [94mLoss[0m : 2.39868
[1mStep[0m  [36/42], [94mLoss[0m : 2.37919
[1mStep[0m  [40/42], [94mLoss[0m : 2.34236

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.689, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12606
[1mStep[0m  [4/42], [94mLoss[0m : 2.35156
[1mStep[0m  [8/42], [94mLoss[0m : 2.22335
[1mStep[0m  [12/42], [94mLoss[0m : 2.43450
[1mStep[0m  [16/42], [94mLoss[0m : 2.24822
[1mStep[0m  [20/42], [94mLoss[0m : 2.17257
[1mStep[0m  [24/42], [94mLoss[0m : 2.11852
[1mStep[0m  [28/42], [94mLoss[0m : 2.08593
[1mStep[0m  [32/42], [94mLoss[0m : 2.32092
[1mStep[0m  [36/42], [94mLoss[0m : 2.14224
[1mStep[0m  [40/42], [94mLoss[0m : 2.31486

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.655, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02270
[1mStep[0m  [4/42], [94mLoss[0m : 2.25224
[1mStep[0m  [8/42], [94mLoss[0m : 2.09339
[1mStep[0m  [12/42], [94mLoss[0m : 2.35683
[1mStep[0m  [16/42], [94mLoss[0m : 2.07575
[1mStep[0m  [20/42], [94mLoss[0m : 2.15598
[1mStep[0m  [24/42], [94mLoss[0m : 2.29171
[1mStep[0m  [28/42], [94mLoss[0m : 2.01237
[1mStep[0m  [32/42], [94mLoss[0m : 2.17227
[1mStep[0m  [36/42], [94mLoss[0m : 1.95662
[1mStep[0m  [40/42], [94mLoss[0m : 2.12034

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.642, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12833
[1mStep[0m  [4/42], [94mLoss[0m : 2.14088
[1mStep[0m  [8/42], [94mLoss[0m : 1.96052
[1mStep[0m  [12/42], [94mLoss[0m : 2.28220
[1mStep[0m  [16/42], [94mLoss[0m : 2.34377
[1mStep[0m  [20/42], [94mLoss[0m : 2.06247
[1mStep[0m  [24/42], [94mLoss[0m : 2.30787
[1mStep[0m  [28/42], [94mLoss[0m : 1.93220
[1mStep[0m  [32/42], [94mLoss[0m : 1.96840
[1mStep[0m  [36/42], [94mLoss[0m : 2.20903
[1mStep[0m  [40/42], [94mLoss[0m : 2.29853

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.648, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05637
[1mStep[0m  [4/42], [94mLoss[0m : 2.30155
[1mStep[0m  [8/42], [94mLoss[0m : 2.01782
[1mStep[0m  [12/42], [94mLoss[0m : 2.22532
[1mStep[0m  [16/42], [94mLoss[0m : 2.20054
[1mStep[0m  [20/42], [94mLoss[0m : 2.23629
[1mStep[0m  [24/42], [94mLoss[0m : 2.07255
[1mStep[0m  [28/42], [94mLoss[0m : 2.19873
[1mStep[0m  [32/42], [94mLoss[0m : 2.11764
[1mStep[0m  [36/42], [94mLoss[0m : 2.17882
[1mStep[0m  [40/42], [94mLoss[0m : 2.09938

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.586, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20543
[1mStep[0m  [4/42], [94mLoss[0m : 2.12343
[1mStep[0m  [8/42], [94mLoss[0m : 2.33741
[1mStep[0m  [12/42], [94mLoss[0m : 2.13619
[1mStep[0m  [16/42], [94mLoss[0m : 1.93180
[1mStep[0m  [20/42], [94mLoss[0m : 2.13633
[1mStep[0m  [24/42], [94mLoss[0m : 2.18580
[1mStep[0m  [28/42], [94mLoss[0m : 2.13805
[1mStep[0m  [32/42], [94mLoss[0m : 2.12864
[1mStep[0m  [36/42], [94mLoss[0m : 2.14380
[1mStep[0m  [40/42], [94mLoss[0m : 2.21184

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01886
[1mStep[0m  [4/42], [94mLoss[0m : 1.97664
[1mStep[0m  [8/42], [94mLoss[0m : 2.02166
[1mStep[0m  [12/42], [94mLoss[0m : 2.31783
[1mStep[0m  [16/42], [94mLoss[0m : 1.99966
[1mStep[0m  [20/42], [94mLoss[0m : 2.16107
[1mStep[0m  [24/42], [94mLoss[0m : 2.08516
[1mStep[0m  [28/42], [94mLoss[0m : 1.96811
[1mStep[0m  [32/42], [94mLoss[0m : 2.20287
[1mStep[0m  [36/42], [94mLoss[0m : 2.14866
[1mStep[0m  [40/42], [94mLoss[0m : 2.37233

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.623, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06552
[1mStep[0m  [4/42], [94mLoss[0m : 1.93228
[1mStep[0m  [8/42], [94mLoss[0m : 1.97632
[1mStep[0m  [12/42], [94mLoss[0m : 2.23543
[1mStep[0m  [16/42], [94mLoss[0m : 1.95349
[1mStep[0m  [20/42], [94mLoss[0m : 1.97018
[1mStep[0m  [24/42], [94mLoss[0m : 2.03639
[1mStep[0m  [28/42], [94mLoss[0m : 2.24940
[1mStep[0m  [32/42], [94mLoss[0m : 2.03714
[1mStep[0m  [36/42], [94mLoss[0m : 2.18281
[1mStep[0m  [40/42], [94mLoss[0m : 2.03146

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.674, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13775
[1mStep[0m  [4/42], [94mLoss[0m : 1.96818
[1mStep[0m  [8/42], [94mLoss[0m : 1.95289
[1mStep[0m  [12/42], [94mLoss[0m : 2.14884
[1mStep[0m  [16/42], [94mLoss[0m : 2.03924
[1mStep[0m  [20/42], [94mLoss[0m : 2.03163
[1mStep[0m  [24/42], [94mLoss[0m : 2.12861
[1mStep[0m  [28/42], [94mLoss[0m : 1.91115
[1mStep[0m  [32/42], [94mLoss[0m : 1.98784
[1mStep[0m  [36/42], [94mLoss[0m : 1.95357
[1mStep[0m  [40/42], [94mLoss[0m : 1.99866

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.627, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00018
[1mStep[0m  [4/42], [94mLoss[0m : 2.41052
[1mStep[0m  [8/42], [94mLoss[0m : 2.25921
[1mStep[0m  [12/42], [94mLoss[0m : 2.13817
[1mStep[0m  [16/42], [94mLoss[0m : 2.06954
[1mStep[0m  [20/42], [94mLoss[0m : 2.07448
[1mStep[0m  [24/42], [94mLoss[0m : 1.76521
[1mStep[0m  [28/42], [94mLoss[0m : 2.18205
[1mStep[0m  [32/42], [94mLoss[0m : 2.01495
[1mStep[0m  [36/42], [94mLoss[0m : 2.00307
[1mStep[0m  [40/42], [94mLoss[0m : 2.03201

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.059, [92mTest[0m: 2.543, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07324
[1mStep[0m  [4/42], [94mLoss[0m : 1.92729
[1mStep[0m  [8/42], [94mLoss[0m : 2.07008
[1mStep[0m  [12/42], [94mLoss[0m : 2.10288
[1mStep[0m  [16/42], [94mLoss[0m : 1.82799
[1mStep[0m  [20/42], [94mLoss[0m : 1.95182
[1mStep[0m  [24/42], [94mLoss[0m : 1.75843
[1mStep[0m  [28/42], [94mLoss[0m : 2.03221
[1mStep[0m  [32/42], [94mLoss[0m : 2.13199
[1mStep[0m  [36/42], [94mLoss[0m : 1.84736
[1mStep[0m  [40/42], [94mLoss[0m : 2.00214

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.579, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86611
[1mStep[0m  [4/42], [94mLoss[0m : 1.89825
[1mStep[0m  [8/42], [94mLoss[0m : 1.88458
[1mStep[0m  [12/42], [94mLoss[0m : 2.01644
[1mStep[0m  [16/42], [94mLoss[0m : 1.93743
[1mStep[0m  [20/42], [94mLoss[0m : 1.79317
[1mStep[0m  [24/42], [94mLoss[0m : 2.26689
[1mStep[0m  [28/42], [94mLoss[0m : 1.86263
[1mStep[0m  [32/42], [94mLoss[0m : 1.84114
[1mStep[0m  [36/42], [94mLoss[0m : 1.90326
[1mStep[0m  [40/42], [94mLoss[0m : 2.09753

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07215
[1mStep[0m  [4/42], [94mLoss[0m : 1.91688
[1mStep[0m  [8/42], [94mLoss[0m : 1.81265
[1mStep[0m  [12/42], [94mLoss[0m : 1.87642
[1mStep[0m  [16/42], [94mLoss[0m : 2.13841
[1mStep[0m  [20/42], [94mLoss[0m : 1.82381
[1mStep[0m  [24/42], [94mLoss[0m : 2.02367
[1mStep[0m  [28/42], [94mLoss[0m : 1.82798
[1mStep[0m  [32/42], [94mLoss[0m : 2.04788
[1mStep[0m  [36/42], [94mLoss[0m : 2.24668
[1mStep[0m  [40/42], [94mLoss[0m : 1.95484

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94262
[1mStep[0m  [4/42], [94mLoss[0m : 1.83646
[1mStep[0m  [8/42], [94mLoss[0m : 1.99167
[1mStep[0m  [12/42], [94mLoss[0m : 1.92163
[1mStep[0m  [16/42], [94mLoss[0m : 1.94940
[1mStep[0m  [20/42], [94mLoss[0m : 1.97273
[1mStep[0m  [24/42], [94mLoss[0m : 1.87657
[1mStep[0m  [28/42], [94mLoss[0m : 2.00410
[1mStep[0m  [32/42], [94mLoss[0m : 2.04599
[1mStep[0m  [36/42], [94mLoss[0m : 1.92918
[1mStep[0m  [40/42], [94mLoss[0m : 1.95622

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.527
====================================

Phase 2 - Evaluation MAE:  2.526851330484663
MAE score P1       2.337298
MAE score P2       2.526851
loss               1.960037
learning_rate       0.00505
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.43250
[1mStep[0m  [4/42], [94mLoss[0m : 8.74066
[1mStep[0m  [8/42], [94mLoss[0m : 5.32577
[1mStep[0m  [12/42], [94mLoss[0m : 3.14507
[1mStep[0m  [16/42], [94mLoss[0m : 3.25808
[1mStep[0m  [20/42], [94mLoss[0m : 3.77741
[1mStep[0m  [24/42], [94mLoss[0m : 3.00640
[1mStep[0m  [28/42], [94mLoss[0m : 2.89894
[1mStep[0m  [32/42], [94mLoss[0m : 2.72227
[1mStep[0m  [36/42], [94mLoss[0m : 2.53465
[1mStep[0m  [40/42], [94mLoss[0m : 2.61063

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.224, [92mTest[0m: 10.851, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82836
[1mStep[0m  [4/42], [94mLoss[0m : 2.59432
[1mStep[0m  [8/42], [94mLoss[0m : 2.72385
[1mStep[0m  [12/42], [94mLoss[0m : 2.54673
[1mStep[0m  [16/42], [94mLoss[0m : 2.51426
[1mStep[0m  [20/42], [94mLoss[0m : 2.91921
[1mStep[0m  [24/42], [94mLoss[0m : 2.70337
[1mStep[0m  [28/42], [94mLoss[0m : 2.42551
[1mStep[0m  [32/42], [94mLoss[0m : 2.39251
[1mStep[0m  [36/42], [94mLoss[0m : 2.31571
[1mStep[0m  [40/42], [94mLoss[0m : 2.56387

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.762, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73022
[1mStep[0m  [4/42], [94mLoss[0m : 2.52507
[1mStep[0m  [8/42], [94mLoss[0m : 2.57657
[1mStep[0m  [12/42], [94mLoss[0m : 2.40236
[1mStep[0m  [16/42], [94mLoss[0m : 2.52108
[1mStep[0m  [20/42], [94mLoss[0m : 2.26541
[1mStep[0m  [24/42], [94mLoss[0m : 2.44208
[1mStep[0m  [28/42], [94mLoss[0m : 2.52649
[1mStep[0m  [32/42], [94mLoss[0m : 2.35024
[1mStep[0m  [36/42], [94mLoss[0m : 2.42280
[1mStep[0m  [40/42], [94mLoss[0m : 2.40773

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43847
[1mStep[0m  [4/42], [94mLoss[0m : 2.51474
[1mStep[0m  [8/42], [94mLoss[0m : 2.54455
[1mStep[0m  [12/42], [94mLoss[0m : 2.55985
[1mStep[0m  [16/42], [94mLoss[0m : 2.49713
[1mStep[0m  [20/42], [94mLoss[0m : 2.55016
[1mStep[0m  [24/42], [94mLoss[0m : 2.56899
[1mStep[0m  [28/42], [94mLoss[0m : 2.16891
[1mStep[0m  [32/42], [94mLoss[0m : 2.31069
[1mStep[0m  [36/42], [94mLoss[0m : 2.47878
[1mStep[0m  [40/42], [94mLoss[0m : 2.31204

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40333
[1mStep[0m  [4/42], [94mLoss[0m : 2.39899
[1mStep[0m  [8/42], [94mLoss[0m : 2.24469
[1mStep[0m  [12/42], [94mLoss[0m : 2.56521
[1mStep[0m  [16/42], [94mLoss[0m : 2.38364
[1mStep[0m  [20/42], [94mLoss[0m : 2.53837
[1mStep[0m  [24/42], [94mLoss[0m : 2.46724
[1mStep[0m  [28/42], [94mLoss[0m : 2.47592
[1mStep[0m  [32/42], [94mLoss[0m : 2.41883
[1mStep[0m  [36/42], [94mLoss[0m : 2.54476
[1mStep[0m  [40/42], [94mLoss[0m : 2.42357

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05162
[1mStep[0m  [4/42], [94mLoss[0m : 2.49010
[1mStep[0m  [8/42], [94mLoss[0m : 2.54197
[1mStep[0m  [12/42], [94mLoss[0m : 2.35777
[1mStep[0m  [16/42], [94mLoss[0m : 2.30067
[1mStep[0m  [20/42], [94mLoss[0m : 2.58422
[1mStep[0m  [24/42], [94mLoss[0m : 2.36474
[1mStep[0m  [28/42], [94mLoss[0m : 2.47647
[1mStep[0m  [32/42], [94mLoss[0m : 2.27390
[1mStep[0m  [36/42], [94mLoss[0m : 2.36995
[1mStep[0m  [40/42], [94mLoss[0m : 2.30014

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51448
[1mStep[0m  [4/42], [94mLoss[0m : 2.37403
[1mStep[0m  [8/42], [94mLoss[0m : 2.32889
[1mStep[0m  [12/42], [94mLoss[0m : 2.33680
[1mStep[0m  [16/42], [94mLoss[0m : 2.25438
[1mStep[0m  [20/42], [94mLoss[0m : 2.50226
[1mStep[0m  [24/42], [94mLoss[0m : 2.25471
[1mStep[0m  [28/42], [94mLoss[0m : 2.47449
[1mStep[0m  [32/42], [94mLoss[0m : 2.32853
[1mStep[0m  [36/42], [94mLoss[0m : 2.30968
[1mStep[0m  [40/42], [94mLoss[0m : 2.22761

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35490
[1mStep[0m  [4/42], [94mLoss[0m : 2.41425
[1mStep[0m  [8/42], [94mLoss[0m : 2.50219
[1mStep[0m  [12/42], [94mLoss[0m : 2.35033
[1mStep[0m  [16/42], [94mLoss[0m : 2.36984
[1mStep[0m  [20/42], [94mLoss[0m : 2.48124
[1mStep[0m  [24/42], [94mLoss[0m : 2.09913
[1mStep[0m  [28/42], [94mLoss[0m : 2.41016
[1mStep[0m  [32/42], [94mLoss[0m : 2.26927
[1mStep[0m  [36/42], [94mLoss[0m : 2.46872
[1mStep[0m  [40/42], [94mLoss[0m : 2.33068

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28298
[1mStep[0m  [4/42], [94mLoss[0m : 2.26289
[1mStep[0m  [8/42], [94mLoss[0m : 2.47442
[1mStep[0m  [12/42], [94mLoss[0m : 2.16990
[1mStep[0m  [16/42], [94mLoss[0m : 2.46401
[1mStep[0m  [20/42], [94mLoss[0m : 2.47181
[1mStep[0m  [24/42], [94mLoss[0m : 2.48305
[1mStep[0m  [28/42], [94mLoss[0m : 2.41602
[1mStep[0m  [32/42], [94mLoss[0m : 2.29723
[1mStep[0m  [36/42], [94mLoss[0m : 2.32431
[1mStep[0m  [40/42], [94mLoss[0m : 2.46373

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21089
[1mStep[0m  [4/42], [94mLoss[0m : 2.61405
[1mStep[0m  [8/42], [94mLoss[0m : 2.22688
[1mStep[0m  [12/42], [94mLoss[0m : 2.27823
[1mStep[0m  [16/42], [94mLoss[0m : 2.26577
[1mStep[0m  [20/42], [94mLoss[0m : 2.03620
[1mStep[0m  [24/42], [94mLoss[0m : 2.29316
[1mStep[0m  [28/42], [94mLoss[0m : 2.67036
[1mStep[0m  [32/42], [94mLoss[0m : 2.58639
[1mStep[0m  [36/42], [94mLoss[0m : 2.34119
[1mStep[0m  [40/42], [94mLoss[0m : 2.39204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42174
[1mStep[0m  [4/42], [94mLoss[0m : 2.37139
[1mStep[0m  [8/42], [94mLoss[0m : 2.37717
[1mStep[0m  [12/42], [94mLoss[0m : 2.23076
[1mStep[0m  [16/42], [94mLoss[0m : 2.33180
[1mStep[0m  [20/42], [94mLoss[0m : 2.25059
[1mStep[0m  [24/42], [94mLoss[0m : 2.24225
[1mStep[0m  [28/42], [94mLoss[0m : 2.39693
[1mStep[0m  [32/42], [94mLoss[0m : 2.26401
[1mStep[0m  [36/42], [94mLoss[0m : 2.26292
[1mStep[0m  [40/42], [94mLoss[0m : 2.34892

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46468
[1mStep[0m  [4/42], [94mLoss[0m : 2.38601
[1mStep[0m  [8/42], [94mLoss[0m : 2.57119
[1mStep[0m  [12/42], [94mLoss[0m : 2.30093
[1mStep[0m  [16/42], [94mLoss[0m : 2.13088
[1mStep[0m  [20/42], [94mLoss[0m : 2.40403
[1mStep[0m  [24/42], [94mLoss[0m : 2.45155
[1mStep[0m  [28/42], [94mLoss[0m : 2.44143
[1mStep[0m  [32/42], [94mLoss[0m : 2.19371
[1mStep[0m  [36/42], [94mLoss[0m : 2.41268
[1mStep[0m  [40/42], [94mLoss[0m : 2.29965

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33364
[1mStep[0m  [4/42], [94mLoss[0m : 2.25679
[1mStep[0m  [8/42], [94mLoss[0m : 2.51492
[1mStep[0m  [12/42], [94mLoss[0m : 2.08816
[1mStep[0m  [16/42], [94mLoss[0m : 2.42942
[1mStep[0m  [20/42], [94mLoss[0m : 2.41803
[1mStep[0m  [24/42], [94mLoss[0m : 2.32723
[1mStep[0m  [28/42], [94mLoss[0m : 2.48830
[1mStep[0m  [32/42], [94mLoss[0m : 2.52192
[1mStep[0m  [36/42], [94mLoss[0m : 2.36800
[1mStep[0m  [40/42], [94mLoss[0m : 2.28142

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52859
[1mStep[0m  [4/42], [94mLoss[0m : 2.61200
[1mStep[0m  [8/42], [94mLoss[0m : 2.38266
[1mStep[0m  [12/42], [94mLoss[0m : 2.10687
[1mStep[0m  [16/42], [94mLoss[0m : 2.24698
[1mStep[0m  [20/42], [94mLoss[0m : 2.52449
[1mStep[0m  [24/42], [94mLoss[0m : 2.48109
[1mStep[0m  [28/42], [94mLoss[0m : 2.05569
[1mStep[0m  [32/42], [94mLoss[0m : 2.22693
[1mStep[0m  [36/42], [94mLoss[0m : 2.31971
[1mStep[0m  [40/42], [94mLoss[0m : 2.32560

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31583
[1mStep[0m  [4/42], [94mLoss[0m : 2.17885
[1mStep[0m  [8/42], [94mLoss[0m : 2.22196
[1mStep[0m  [12/42], [94mLoss[0m : 2.34150
[1mStep[0m  [16/42], [94mLoss[0m : 2.67463
[1mStep[0m  [20/42], [94mLoss[0m : 2.47132
[1mStep[0m  [24/42], [94mLoss[0m : 2.34910
[1mStep[0m  [28/42], [94mLoss[0m : 2.39934
[1mStep[0m  [32/42], [94mLoss[0m : 2.37987
[1mStep[0m  [36/42], [94mLoss[0m : 2.60810
[1mStep[0m  [40/42], [94mLoss[0m : 2.33112

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42564
[1mStep[0m  [4/42], [94mLoss[0m : 2.16647
[1mStep[0m  [8/42], [94mLoss[0m : 2.34425
[1mStep[0m  [12/42], [94mLoss[0m : 2.33106
[1mStep[0m  [16/42], [94mLoss[0m : 2.33404
[1mStep[0m  [20/42], [94mLoss[0m : 2.34477
[1mStep[0m  [24/42], [94mLoss[0m : 2.38833
[1mStep[0m  [28/42], [94mLoss[0m : 2.50272
[1mStep[0m  [32/42], [94mLoss[0m : 2.30055
[1mStep[0m  [36/42], [94mLoss[0m : 2.30012
[1mStep[0m  [40/42], [94mLoss[0m : 2.06242

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10689
[1mStep[0m  [4/42], [94mLoss[0m : 2.30037
[1mStep[0m  [8/42], [94mLoss[0m : 2.47058
[1mStep[0m  [12/42], [94mLoss[0m : 2.26711
[1mStep[0m  [16/42], [94mLoss[0m : 2.23049
[1mStep[0m  [20/42], [94mLoss[0m : 2.22660
[1mStep[0m  [24/42], [94mLoss[0m : 2.21860
[1mStep[0m  [28/42], [94mLoss[0m : 2.36531
[1mStep[0m  [32/42], [94mLoss[0m : 2.17842
[1mStep[0m  [36/42], [94mLoss[0m : 2.08125
[1mStep[0m  [40/42], [94mLoss[0m : 2.50020

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24387
[1mStep[0m  [4/42], [94mLoss[0m : 2.40030
[1mStep[0m  [8/42], [94mLoss[0m : 2.18864
[1mStep[0m  [12/42], [94mLoss[0m : 2.43028
[1mStep[0m  [16/42], [94mLoss[0m : 2.28963
[1mStep[0m  [20/42], [94mLoss[0m : 2.12854
[1mStep[0m  [24/42], [94mLoss[0m : 2.38359
[1mStep[0m  [28/42], [94mLoss[0m : 2.30651
[1mStep[0m  [32/42], [94mLoss[0m : 2.26719
[1mStep[0m  [36/42], [94mLoss[0m : 2.29317
[1mStep[0m  [40/42], [94mLoss[0m : 2.27918

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.315, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37734
[1mStep[0m  [4/42], [94mLoss[0m : 2.35128
[1mStep[0m  [8/42], [94mLoss[0m : 2.51246
[1mStep[0m  [12/42], [94mLoss[0m : 2.14372
[1mStep[0m  [16/42], [94mLoss[0m : 2.41317
[1mStep[0m  [20/42], [94mLoss[0m : 2.44175
[1mStep[0m  [24/42], [94mLoss[0m : 2.43408
[1mStep[0m  [28/42], [94mLoss[0m : 2.38415
[1mStep[0m  [32/42], [94mLoss[0m : 2.44112
[1mStep[0m  [36/42], [94mLoss[0m : 2.28733
[1mStep[0m  [40/42], [94mLoss[0m : 2.39566

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20935
[1mStep[0m  [4/42], [94mLoss[0m : 2.19710
[1mStep[0m  [8/42], [94mLoss[0m : 2.42971
[1mStep[0m  [12/42], [94mLoss[0m : 2.27438
[1mStep[0m  [16/42], [94mLoss[0m : 2.20246
[1mStep[0m  [20/42], [94mLoss[0m : 2.39826
[1mStep[0m  [24/42], [94mLoss[0m : 2.29426
[1mStep[0m  [28/42], [94mLoss[0m : 2.39723
[1mStep[0m  [32/42], [94mLoss[0m : 2.13509
[1mStep[0m  [36/42], [94mLoss[0m : 2.20955
[1mStep[0m  [40/42], [94mLoss[0m : 2.37125

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15986
[1mStep[0m  [4/42], [94mLoss[0m : 2.03484
[1mStep[0m  [8/42], [94mLoss[0m : 2.35149
[1mStep[0m  [12/42], [94mLoss[0m : 2.49587
[1mStep[0m  [16/42], [94mLoss[0m : 2.34445
[1mStep[0m  [20/42], [94mLoss[0m : 2.30718
[1mStep[0m  [24/42], [94mLoss[0m : 2.02611
[1mStep[0m  [28/42], [94mLoss[0m : 2.34632
[1mStep[0m  [32/42], [94mLoss[0m : 2.41229
[1mStep[0m  [36/42], [94mLoss[0m : 2.50587
[1mStep[0m  [40/42], [94mLoss[0m : 2.22272

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.371, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20873
[1mStep[0m  [4/42], [94mLoss[0m : 2.28784
[1mStep[0m  [8/42], [94mLoss[0m : 2.33238
[1mStep[0m  [12/42], [94mLoss[0m : 2.17074
[1mStep[0m  [16/42], [94mLoss[0m : 2.31976
[1mStep[0m  [20/42], [94mLoss[0m : 2.41217
[1mStep[0m  [24/42], [94mLoss[0m : 2.08277
[1mStep[0m  [28/42], [94mLoss[0m : 2.36495
[1mStep[0m  [32/42], [94mLoss[0m : 2.45054
[1mStep[0m  [36/42], [94mLoss[0m : 2.31498
[1mStep[0m  [40/42], [94mLoss[0m : 2.50412

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43215
[1mStep[0m  [4/42], [94mLoss[0m : 2.11776
[1mStep[0m  [8/42], [94mLoss[0m : 2.20490
[1mStep[0m  [12/42], [94mLoss[0m : 2.14767
[1mStep[0m  [16/42], [94mLoss[0m : 2.27623
[1mStep[0m  [20/42], [94mLoss[0m : 2.24227
[1mStep[0m  [24/42], [94mLoss[0m : 2.44361
[1mStep[0m  [28/42], [94mLoss[0m : 2.20859
[1mStep[0m  [32/42], [94mLoss[0m : 2.35023
[1mStep[0m  [36/42], [94mLoss[0m : 2.32199
[1mStep[0m  [40/42], [94mLoss[0m : 2.40797

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36413
[1mStep[0m  [4/42], [94mLoss[0m : 2.23267
[1mStep[0m  [8/42], [94mLoss[0m : 2.39438
[1mStep[0m  [12/42], [94mLoss[0m : 2.51853
[1mStep[0m  [16/42], [94mLoss[0m : 2.29254
[1mStep[0m  [20/42], [94mLoss[0m : 2.14968
[1mStep[0m  [24/42], [94mLoss[0m : 2.21038
[1mStep[0m  [28/42], [94mLoss[0m : 2.47575
[1mStep[0m  [32/42], [94mLoss[0m : 2.38134
[1mStep[0m  [36/42], [94mLoss[0m : 2.34983
[1mStep[0m  [40/42], [94mLoss[0m : 2.36500

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.305, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11253
[1mStep[0m  [4/42], [94mLoss[0m : 1.93952
[1mStep[0m  [8/42], [94mLoss[0m : 2.44111
[1mStep[0m  [12/42], [94mLoss[0m : 2.21971
[1mStep[0m  [16/42], [94mLoss[0m : 2.31402
[1mStep[0m  [20/42], [94mLoss[0m : 2.07550
[1mStep[0m  [24/42], [94mLoss[0m : 2.08473
[1mStep[0m  [28/42], [94mLoss[0m : 2.34036
[1mStep[0m  [32/42], [94mLoss[0m : 2.20218
[1mStep[0m  [36/42], [94mLoss[0m : 2.15178
[1mStep[0m  [40/42], [94mLoss[0m : 2.36215

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.317, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49986
[1mStep[0m  [4/42], [94mLoss[0m : 2.26127
[1mStep[0m  [8/42], [94mLoss[0m : 2.11858
[1mStep[0m  [12/42], [94mLoss[0m : 2.37556
[1mStep[0m  [16/42], [94mLoss[0m : 2.26197
[1mStep[0m  [20/42], [94mLoss[0m : 2.19855
[1mStep[0m  [24/42], [94mLoss[0m : 2.23446
[1mStep[0m  [28/42], [94mLoss[0m : 2.38382
[1mStep[0m  [32/42], [94mLoss[0m : 2.16387
[1mStep[0m  [36/42], [94mLoss[0m : 2.19604
[1mStep[0m  [40/42], [94mLoss[0m : 2.31024

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.306, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90737
[1mStep[0m  [4/42], [94mLoss[0m : 2.39513
[1mStep[0m  [8/42], [94mLoss[0m : 2.28204
[1mStep[0m  [12/42], [94mLoss[0m : 2.13857
[1mStep[0m  [16/42], [94mLoss[0m : 2.22687
[1mStep[0m  [20/42], [94mLoss[0m : 2.15401
[1mStep[0m  [24/42], [94mLoss[0m : 2.11194
[1mStep[0m  [28/42], [94mLoss[0m : 2.15991
[1mStep[0m  [32/42], [94mLoss[0m : 2.23864
[1mStep[0m  [36/42], [94mLoss[0m : 2.18327
[1mStep[0m  [40/42], [94mLoss[0m : 2.37019

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18669
[1mStep[0m  [4/42], [94mLoss[0m : 2.29566
[1mStep[0m  [8/42], [94mLoss[0m : 2.17614
[1mStep[0m  [12/42], [94mLoss[0m : 2.18788
[1mStep[0m  [16/42], [94mLoss[0m : 2.18557
[1mStep[0m  [20/42], [94mLoss[0m : 2.20153
[1mStep[0m  [24/42], [94mLoss[0m : 2.24381
[1mStep[0m  [28/42], [94mLoss[0m : 2.26810
[1mStep[0m  [32/42], [94mLoss[0m : 2.11272
[1mStep[0m  [36/42], [94mLoss[0m : 2.23822
[1mStep[0m  [40/42], [94mLoss[0m : 2.25524

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.309, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14859
[1mStep[0m  [4/42], [94mLoss[0m : 2.24334
[1mStep[0m  [8/42], [94mLoss[0m : 2.28477
[1mStep[0m  [12/42], [94mLoss[0m : 2.22311
[1mStep[0m  [16/42], [94mLoss[0m : 2.50171
[1mStep[0m  [20/42], [94mLoss[0m : 2.20249
[1mStep[0m  [24/42], [94mLoss[0m : 2.23543
[1mStep[0m  [28/42], [94mLoss[0m : 2.25449
[1mStep[0m  [32/42], [94mLoss[0m : 2.17698
[1mStep[0m  [36/42], [94mLoss[0m : 2.38909
[1mStep[0m  [40/42], [94mLoss[0m : 2.36853

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25379
[1mStep[0m  [4/42], [94mLoss[0m : 2.31210
[1mStep[0m  [8/42], [94mLoss[0m : 2.18606
[1mStep[0m  [12/42], [94mLoss[0m : 2.56975
[1mStep[0m  [16/42], [94mLoss[0m : 2.16829
[1mStep[0m  [20/42], [94mLoss[0m : 2.41430
[1mStep[0m  [24/42], [94mLoss[0m : 2.36370
[1mStep[0m  [28/42], [94mLoss[0m : 2.27165
[1mStep[0m  [32/42], [94mLoss[0m : 2.44299
[1mStep[0m  [36/42], [94mLoss[0m : 2.26630
[1mStep[0m  [40/42], [94mLoss[0m : 2.31650

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.293, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.305
====================================

Phase 1 - Evaluation MAE:  2.304790564945766
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.20283
[1mStep[0m  [4/42], [94mLoss[0m : 2.26408
[1mStep[0m  [8/42], [94mLoss[0m : 2.19682
[1mStep[0m  [12/42], [94mLoss[0m : 2.17146
[1mStep[0m  [16/42], [94mLoss[0m : 2.35645
[1mStep[0m  [20/42], [94mLoss[0m : 2.53701
[1mStep[0m  [24/42], [94mLoss[0m : 2.45347
[1mStep[0m  [28/42], [94mLoss[0m : 2.76230
[1mStep[0m  [32/42], [94mLoss[0m : 2.25334
[1mStep[0m  [36/42], [94mLoss[0m : 2.26453
[1mStep[0m  [40/42], [94mLoss[0m : 2.47404

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.302, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24243
[1mStep[0m  [4/42], [94mLoss[0m : 2.24240
[1mStep[0m  [8/42], [94mLoss[0m : 2.53388
[1mStep[0m  [12/42], [94mLoss[0m : 2.55046
[1mStep[0m  [16/42], [94mLoss[0m : 2.21574
[1mStep[0m  [20/42], [94mLoss[0m : 2.24869
[1mStep[0m  [24/42], [94mLoss[0m : 2.48459
[1mStep[0m  [28/42], [94mLoss[0m : 2.38172
[1mStep[0m  [32/42], [94mLoss[0m : 2.18615
[1mStep[0m  [36/42], [94mLoss[0m : 2.19271
[1mStep[0m  [40/42], [94mLoss[0m : 2.34897

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15787
[1mStep[0m  [4/42], [94mLoss[0m : 2.02162
[1mStep[0m  [8/42], [94mLoss[0m : 2.16754
[1mStep[0m  [12/42], [94mLoss[0m : 2.17218
[1mStep[0m  [16/42], [94mLoss[0m : 2.24235
[1mStep[0m  [20/42], [94mLoss[0m : 2.14437
[1mStep[0m  [24/42], [94mLoss[0m : 2.17749
[1mStep[0m  [28/42], [94mLoss[0m : 2.14363
[1mStep[0m  [32/42], [94mLoss[0m : 2.30606
[1mStep[0m  [36/42], [94mLoss[0m : 2.12226
[1mStep[0m  [40/42], [94mLoss[0m : 2.15819

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01741
[1mStep[0m  [4/42], [94mLoss[0m : 1.94009
[1mStep[0m  [8/42], [94mLoss[0m : 2.04506
[1mStep[0m  [12/42], [94mLoss[0m : 2.02313
[1mStep[0m  [16/42], [94mLoss[0m : 1.95305
[1mStep[0m  [20/42], [94mLoss[0m : 2.10575
[1mStep[0m  [24/42], [94mLoss[0m : 2.05233
[1mStep[0m  [28/42], [94mLoss[0m : 2.18314
[1mStep[0m  [32/42], [94mLoss[0m : 2.19557
[1mStep[0m  [36/42], [94mLoss[0m : 2.25258
[1mStep[0m  [40/42], [94mLoss[0m : 2.14128

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73459
[1mStep[0m  [4/42], [94mLoss[0m : 1.99567
[1mStep[0m  [8/42], [94mLoss[0m : 1.90073
[1mStep[0m  [12/42], [94mLoss[0m : 2.16155
[1mStep[0m  [16/42], [94mLoss[0m : 1.76451
[1mStep[0m  [20/42], [94mLoss[0m : 1.85653
[1mStep[0m  [24/42], [94mLoss[0m : 1.94418
[1mStep[0m  [28/42], [94mLoss[0m : 1.96038
[1mStep[0m  [32/42], [94mLoss[0m : 1.94462
[1mStep[0m  [36/42], [94mLoss[0m : 2.16690
[1mStep[0m  [40/42], [94mLoss[0m : 2.13496

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74220
[1mStep[0m  [4/42], [94mLoss[0m : 1.79293
[1mStep[0m  [8/42], [94mLoss[0m : 1.84362
[1mStep[0m  [12/42], [94mLoss[0m : 1.72499
[1mStep[0m  [16/42], [94mLoss[0m : 1.96202
[1mStep[0m  [20/42], [94mLoss[0m : 1.96277
[1mStep[0m  [24/42], [94mLoss[0m : 1.94089
[1mStep[0m  [28/42], [94mLoss[0m : 1.96060
[1mStep[0m  [32/42], [94mLoss[0m : 1.95224
[1mStep[0m  [36/42], [94mLoss[0m : 2.07804
[1mStep[0m  [40/42], [94mLoss[0m : 1.92520

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70837
[1mStep[0m  [4/42], [94mLoss[0m : 1.72377
[1mStep[0m  [8/42], [94mLoss[0m : 1.82299
[1mStep[0m  [12/42], [94mLoss[0m : 1.81624
[1mStep[0m  [16/42], [94mLoss[0m : 1.92191
[1mStep[0m  [20/42], [94mLoss[0m : 1.85409
[1mStep[0m  [24/42], [94mLoss[0m : 1.81429
[1mStep[0m  [28/42], [94mLoss[0m : 1.70076
[1mStep[0m  [32/42], [94mLoss[0m : 1.87199
[1mStep[0m  [36/42], [94mLoss[0m : 1.83025
[1mStep[0m  [40/42], [94mLoss[0m : 1.95289

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66107
[1mStep[0m  [4/42], [94mLoss[0m : 1.82013
[1mStep[0m  [8/42], [94mLoss[0m : 1.77958
[1mStep[0m  [12/42], [94mLoss[0m : 1.60778
[1mStep[0m  [16/42], [94mLoss[0m : 1.66817
[1mStep[0m  [20/42], [94mLoss[0m : 1.70082
[1mStep[0m  [24/42], [94mLoss[0m : 1.61997
[1mStep[0m  [28/42], [94mLoss[0m : 1.89171
[1mStep[0m  [32/42], [94mLoss[0m : 1.72931
[1mStep[0m  [36/42], [94mLoss[0m : 1.61382
[1mStep[0m  [40/42], [94mLoss[0m : 1.97381

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63319
[1mStep[0m  [4/42], [94mLoss[0m : 1.73617
[1mStep[0m  [8/42], [94mLoss[0m : 1.45417
[1mStep[0m  [12/42], [94mLoss[0m : 1.77409
[1mStep[0m  [16/42], [94mLoss[0m : 1.73328
[1mStep[0m  [20/42], [94mLoss[0m : 1.74049
[1mStep[0m  [24/42], [94mLoss[0m : 1.93043
[1mStep[0m  [28/42], [94mLoss[0m : 1.74926
[1mStep[0m  [32/42], [94mLoss[0m : 1.75726
[1mStep[0m  [36/42], [94mLoss[0m : 1.58340
[1mStep[0m  [40/42], [94mLoss[0m : 1.65484

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47513
[1mStep[0m  [4/42], [94mLoss[0m : 1.58418
[1mStep[0m  [8/42], [94mLoss[0m : 1.63274
[1mStep[0m  [12/42], [94mLoss[0m : 1.64815
[1mStep[0m  [16/42], [94mLoss[0m : 1.69749
[1mStep[0m  [20/42], [94mLoss[0m : 1.64183
[1mStep[0m  [24/42], [94mLoss[0m : 1.79013
[1mStep[0m  [28/42], [94mLoss[0m : 1.77929
[1mStep[0m  [32/42], [94mLoss[0m : 1.64330
[1mStep[0m  [36/42], [94mLoss[0m : 1.61872
[1mStep[0m  [40/42], [94mLoss[0m : 1.65835

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69314
[1mStep[0m  [4/42], [94mLoss[0m : 1.52032
[1mStep[0m  [8/42], [94mLoss[0m : 1.58027
[1mStep[0m  [12/42], [94mLoss[0m : 1.64260
[1mStep[0m  [16/42], [94mLoss[0m : 1.61491
[1mStep[0m  [20/42], [94mLoss[0m : 1.45159
[1mStep[0m  [24/42], [94mLoss[0m : 1.61893
[1mStep[0m  [28/42], [94mLoss[0m : 1.51925
[1mStep[0m  [32/42], [94mLoss[0m : 1.56741
[1mStep[0m  [36/42], [94mLoss[0m : 1.55537
[1mStep[0m  [40/42], [94mLoss[0m : 1.65698

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42701
[1mStep[0m  [4/42], [94mLoss[0m : 1.54613
[1mStep[0m  [8/42], [94mLoss[0m : 1.46419
[1mStep[0m  [12/42], [94mLoss[0m : 1.41207
[1mStep[0m  [16/42], [94mLoss[0m : 1.43894
[1mStep[0m  [20/42], [94mLoss[0m : 1.65984
[1mStep[0m  [24/42], [94mLoss[0m : 1.41886
[1mStep[0m  [28/42], [94mLoss[0m : 1.57794
[1mStep[0m  [32/42], [94mLoss[0m : 1.60213
[1mStep[0m  [36/42], [94mLoss[0m : 1.74039
[1mStep[0m  [40/42], [94mLoss[0m : 1.53834

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42828
[1mStep[0m  [4/42], [94mLoss[0m : 1.37320
[1mStep[0m  [8/42], [94mLoss[0m : 1.49064
[1mStep[0m  [12/42], [94mLoss[0m : 1.48062
[1mStep[0m  [16/42], [94mLoss[0m : 1.69161
[1mStep[0m  [20/42], [94mLoss[0m : 1.59208
[1mStep[0m  [24/42], [94mLoss[0m : 1.43977
[1mStep[0m  [28/42], [94mLoss[0m : 1.49661
[1mStep[0m  [32/42], [94mLoss[0m : 1.54349
[1mStep[0m  [36/42], [94mLoss[0m : 1.52352
[1mStep[0m  [40/42], [94mLoss[0m : 1.60158

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.554, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.32025
[1mStep[0m  [4/42], [94mLoss[0m : 1.55737
[1mStep[0m  [8/42], [94mLoss[0m : 1.49768
[1mStep[0m  [12/42], [94mLoss[0m : 1.62622
[1mStep[0m  [16/42], [94mLoss[0m : 1.35205
[1mStep[0m  [20/42], [94mLoss[0m : 1.53505
[1mStep[0m  [24/42], [94mLoss[0m : 1.50984
[1mStep[0m  [28/42], [94mLoss[0m : 1.48653
[1mStep[0m  [32/42], [94mLoss[0m : 1.63733
[1mStep[0m  [36/42], [94mLoss[0m : 1.60963
[1mStep[0m  [40/42], [94mLoss[0m : 1.64745

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.504, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.34734
[1mStep[0m  [4/42], [94mLoss[0m : 1.44566
[1mStep[0m  [8/42], [94mLoss[0m : 1.47568
[1mStep[0m  [12/42], [94mLoss[0m : 1.28797
[1mStep[0m  [16/42], [94mLoss[0m : 1.51515
[1mStep[0m  [20/42], [94mLoss[0m : 1.48257
[1mStep[0m  [24/42], [94mLoss[0m : 1.54102
[1mStep[0m  [28/42], [94mLoss[0m : 1.55735
[1mStep[0m  [32/42], [94mLoss[0m : 1.42069
[1mStep[0m  [36/42], [94mLoss[0m : 1.32436
[1mStep[0m  [40/42], [94mLoss[0m : 1.35024

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35859
[1mStep[0m  [4/42], [94mLoss[0m : 1.43400
[1mStep[0m  [8/42], [94mLoss[0m : 1.37894
[1mStep[0m  [12/42], [94mLoss[0m : 1.37005
[1mStep[0m  [16/42], [94mLoss[0m : 1.64255
[1mStep[0m  [20/42], [94mLoss[0m : 1.33800
[1mStep[0m  [24/42], [94mLoss[0m : 1.55242
[1mStep[0m  [28/42], [94mLoss[0m : 1.35936
[1mStep[0m  [32/42], [94mLoss[0m : 1.45333
[1mStep[0m  [36/42], [94mLoss[0m : 1.48068
[1mStep[0m  [40/42], [94mLoss[0m : 1.50201

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35948
[1mStep[0m  [4/42], [94mLoss[0m : 1.45576
[1mStep[0m  [8/42], [94mLoss[0m : 1.53690
[1mStep[0m  [12/42], [94mLoss[0m : 1.47925
[1mStep[0m  [16/42], [94mLoss[0m : 1.31505
[1mStep[0m  [20/42], [94mLoss[0m : 1.38403
[1mStep[0m  [24/42], [94mLoss[0m : 1.45582
[1mStep[0m  [28/42], [94mLoss[0m : 1.50344
[1mStep[0m  [32/42], [94mLoss[0m : 1.47421
[1mStep[0m  [36/42], [94mLoss[0m : 1.29298
[1mStep[0m  [40/42], [94mLoss[0m : 1.47417

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.526, [96mlr[0m: 0.005050000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 16 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 2 - Evaluation MAE:  2.487261618886675
MAE score P1      2.304791
MAE score P2      2.487262
loss              1.427029
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.18640
[1mStep[0m  [8/84], [94mLoss[0m : 10.47283
[1mStep[0m  [16/84], [94mLoss[0m : 9.64244
[1mStep[0m  [24/84], [94mLoss[0m : 8.82319
[1mStep[0m  [32/84], [94mLoss[0m : 8.76174
[1mStep[0m  [40/84], [94mLoss[0m : 7.87912
[1mStep[0m  [48/84], [94mLoss[0m : 6.94755
[1mStep[0m  [56/84], [94mLoss[0m : 6.24645
[1mStep[0m  [64/84], [94mLoss[0m : 5.01265
[1mStep[0m  [72/84], [94mLoss[0m : 4.67110
[1mStep[0m  [80/84], [94mLoss[0m : 3.83260

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.448, [92mTest[0m: 10.857, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.77574
[1mStep[0m  [8/84], [94mLoss[0m : 3.41536
[1mStep[0m  [16/84], [94mLoss[0m : 3.23843
[1mStep[0m  [24/84], [94mLoss[0m : 3.26335
[1mStep[0m  [32/84], [94mLoss[0m : 2.77792
[1mStep[0m  [40/84], [94mLoss[0m : 2.88891
[1mStep[0m  [48/84], [94mLoss[0m : 2.72326
[1mStep[0m  [56/84], [94mLoss[0m : 2.63487
[1mStep[0m  [64/84], [94mLoss[0m : 2.60813
[1mStep[0m  [72/84], [94mLoss[0m : 2.91330
[1mStep[0m  [80/84], [94mLoss[0m : 3.20742

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.091, [92mTest[0m: 5.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.08294
[1mStep[0m  [8/84], [94mLoss[0m : 2.74121
[1mStep[0m  [16/84], [94mLoss[0m : 2.50249
[1mStep[0m  [24/84], [94mLoss[0m : 2.83740
[1mStep[0m  [32/84], [94mLoss[0m : 2.69938
[1mStep[0m  [40/84], [94mLoss[0m : 2.53019
[1mStep[0m  [48/84], [94mLoss[0m : 2.99120
[1mStep[0m  [56/84], [94mLoss[0m : 2.83487
[1mStep[0m  [64/84], [94mLoss[0m : 3.01340
[1mStep[0m  [72/84], [94mLoss[0m : 2.79139
[1mStep[0m  [80/84], [94mLoss[0m : 2.70117

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.814, [92mTest[0m: 2.827, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.91134
[1mStep[0m  [8/84], [94mLoss[0m : 2.53958
[1mStep[0m  [16/84], [94mLoss[0m : 3.05148
[1mStep[0m  [24/84], [94mLoss[0m : 2.66104
[1mStep[0m  [32/84], [94mLoss[0m : 2.75629
[1mStep[0m  [40/84], [94mLoss[0m : 2.75476
[1mStep[0m  [48/84], [94mLoss[0m : 2.79318
[1mStep[0m  [56/84], [94mLoss[0m : 2.71311
[1mStep[0m  [64/84], [94mLoss[0m : 3.02822
[1mStep[0m  [72/84], [94mLoss[0m : 2.78086
[1mStep[0m  [80/84], [94mLoss[0m : 2.76409

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.777, [92mTest[0m: 2.554, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67465
[1mStep[0m  [8/84], [94mLoss[0m : 2.52681
[1mStep[0m  [16/84], [94mLoss[0m : 2.83625
[1mStep[0m  [24/84], [94mLoss[0m : 2.19064
[1mStep[0m  [32/84], [94mLoss[0m : 3.17458
[1mStep[0m  [40/84], [94mLoss[0m : 2.70952
[1mStep[0m  [48/84], [94mLoss[0m : 2.73106
[1mStep[0m  [56/84], [94mLoss[0m : 2.92114
[1mStep[0m  [64/84], [94mLoss[0m : 2.60688
[1mStep[0m  [72/84], [94mLoss[0m : 3.10353
[1mStep[0m  [80/84], [94mLoss[0m : 2.73888

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82400
[1mStep[0m  [8/84], [94mLoss[0m : 2.63268
[1mStep[0m  [16/84], [94mLoss[0m : 2.60241
[1mStep[0m  [24/84], [94mLoss[0m : 2.87543
[1mStep[0m  [32/84], [94mLoss[0m : 2.77584
[1mStep[0m  [40/84], [94mLoss[0m : 3.37300
[1mStep[0m  [48/84], [94mLoss[0m : 2.92223
[1mStep[0m  [56/84], [94mLoss[0m : 2.69462
[1mStep[0m  [64/84], [94mLoss[0m : 2.94897
[1mStep[0m  [72/84], [94mLoss[0m : 2.78409
[1mStep[0m  [80/84], [94mLoss[0m : 2.60891

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37651
[1mStep[0m  [8/84], [94mLoss[0m : 2.93705
[1mStep[0m  [16/84], [94mLoss[0m : 2.06439
[1mStep[0m  [24/84], [94mLoss[0m : 2.58769
[1mStep[0m  [32/84], [94mLoss[0m : 2.65181
[1mStep[0m  [40/84], [94mLoss[0m : 2.72209
[1mStep[0m  [48/84], [94mLoss[0m : 2.71995
[1mStep[0m  [56/84], [94mLoss[0m : 2.57720
[1mStep[0m  [64/84], [94mLoss[0m : 2.84552
[1mStep[0m  [72/84], [94mLoss[0m : 2.77640
[1mStep[0m  [80/84], [94mLoss[0m : 2.49979

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71804
[1mStep[0m  [8/84], [94mLoss[0m : 2.80944
[1mStep[0m  [16/84], [94mLoss[0m : 2.57310
[1mStep[0m  [24/84], [94mLoss[0m : 2.68893
[1mStep[0m  [32/84], [94mLoss[0m : 2.91081
[1mStep[0m  [40/84], [94mLoss[0m : 2.58528
[1mStep[0m  [48/84], [94mLoss[0m : 2.87917
[1mStep[0m  [56/84], [94mLoss[0m : 2.64271
[1mStep[0m  [64/84], [94mLoss[0m : 3.05713
[1mStep[0m  [72/84], [94mLoss[0m : 2.80601
[1mStep[0m  [80/84], [94mLoss[0m : 2.44710

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45091
[1mStep[0m  [8/84], [94mLoss[0m : 2.63727
[1mStep[0m  [16/84], [94mLoss[0m : 3.24547
[1mStep[0m  [24/84], [94mLoss[0m : 2.77059
[1mStep[0m  [32/84], [94mLoss[0m : 2.47632
[1mStep[0m  [40/84], [94mLoss[0m : 2.48646
[1mStep[0m  [48/84], [94mLoss[0m : 2.45911
[1mStep[0m  [56/84], [94mLoss[0m : 2.54306
[1mStep[0m  [64/84], [94mLoss[0m : 2.40993
[1mStep[0m  [72/84], [94mLoss[0m : 2.33571
[1mStep[0m  [80/84], [94mLoss[0m : 2.57961

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61848
[1mStep[0m  [8/84], [94mLoss[0m : 2.68700
[1mStep[0m  [16/84], [94mLoss[0m : 2.71224
[1mStep[0m  [24/84], [94mLoss[0m : 2.54676
[1mStep[0m  [32/84], [94mLoss[0m : 2.47542
[1mStep[0m  [40/84], [94mLoss[0m : 2.52943
[1mStep[0m  [48/84], [94mLoss[0m : 2.69234
[1mStep[0m  [56/84], [94mLoss[0m : 2.51248
[1mStep[0m  [64/84], [94mLoss[0m : 2.95200
[1mStep[0m  [72/84], [94mLoss[0m : 2.60424
[1mStep[0m  [80/84], [94mLoss[0m : 2.35715

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66091
[1mStep[0m  [8/84], [94mLoss[0m : 2.34600
[1mStep[0m  [16/84], [94mLoss[0m : 2.55594
[1mStep[0m  [24/84], [94mLoss[0m : 2.54617
[1mStep[0m  [32/84], [94mLoss[0m : 2.59314
[1mStep[0m  [40/84], [94mLoss[0m : 2.71627
[1mStep[0m  [48/84], [94mLoss[0m : 2.28887
[1mStep[0m  [56/84], [94mLoss[0m : 2.61564
[1mStep[0m  [64/84], [94mLoss[0m : 2.68008
[1mStep[0m  [72/84], [94mLoss[0m : 2.72276
[1mStep[0m  [80/84], [94mLoss[0m : 2.68856

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43704
[1mStep[0m  [8/84], [94mLoss[0m : 2.49727
[1mStep[0m  [16/84], [94mLoss[0m : 2.81409
[1mStep[0m  [24/84], [94mLoss[0m : 2.46952
[1mStep[0m  [32/84], [94mLoss[0m : 2.34591
[1mStep[0m  [40/84], [94mLoss[0m : 2.71518
[1mStep[0m  [48/84], [94mLoss[0m : 2.65904
[1mStep[0m  [56/84], [94mLoss[0m : 2.54906
[1mStep[0m  [64/84], [94mLoss[0m : 2.73826
[1mStep[0m  [72/84], [94mLoss[0m : 2.60769
[1mStep[0m  [80/84], [94mLoss[0m : 2.42954

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67767
[1mStep[0m  [8/84], [94mLoss[0m : 2.80074
[1mStep[0m  [16/84], [94mLoss[0m : 2.54857
[1mStep[0m  [24/84], [94mLoss[0m : 2.56861
[1mStep[0m  [32/84], [94mLoss[0m : 2.32474
[1mStep[0m  [40/84], [94mLoss[0m : 2.45112
[1mStep[0m  [48/84], [94mLoss[0m : 2.56747
[1mStep[0m  [56/84], [94mLoss[0m : 2.47828
[1mStep[0m  [64/84], [94mLoss[0m : 2.55975
[1mStep[0m  [72/84], [94mLoss[0m : 2.97018
[1mStep[0m  [80/84], [94mLoss[0m : 2.60471

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71782
[1mStep[0m  [8/84], [94mLoss[0m : 2.84652
[1mStep[0m  [16/84], [94mLoss[0m : 2.77041
[1mStep[0m  [24/84], [94mLoss[0m : 2.47173
[1mStep[0m  [32/84], [94mLoss[0m : 2.70719
[1mStep[0m  [40/84], [94mLoss[0m : 2.66416
[1mStep[0m  [48/84], [94mLoss[0m : 2.58256
[1mStep[0m  [56/84], [94mLoss[0m : 2.91183
[1mStep[0m  [64/84], [94mLoss[0m : 2.36442
[1mStep[0m  [72/84], [94mLoss[0m : 2.74915
[1mStep[0m  [80/84], [94mLoss[0m : 2.50819

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56954
[1mStep[0m  [8/84], [94mLoss[0m : 2.34977
[1mStep[0m  [16/84], [94mLoss[0m : 2.53995
[1mStep[0m  [24/84], [94mLoss[0m : 2.72334
[1mStep[0m  [32/84], [94mLoss[0m : 2.45915
[1mStep[0m  [40/84], [94mLoss[0m : 2.50280
[1mStep[0m  [48/84], [94mLoss[0m : 2.86913
[1mStep[0m  [56/84], [94mLoss[0m : 2.46323
[1mStep[0m  [64/84], [94mLoss[0m : 2.81851
[1mStep[0m  [72/84], [94mLoss[0m : 2.68070
[1mStep[0m  [80/84], [94mLoss[0m : 2.78816

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31698
[1mStep[0m  [8/84], [94mLoss[0m : 2.51982
[1mStep[0m  [16/84], [94mLoss[0m : 2.93308
[1mStep[0m  [24/84], [94mLoss[0m : 2.64839
[1mStep[0m  [32/84], [94mLoss[0m : 2.54900
[1mStep[0m  [40/84], [94mLoss[0m : 2.19052
[1mStep[0m  [48/84], [94mLoss[0m : 2.61556
[1mStep[0m  [56/84], [94mLoss[0m : 2.55085
[1mStep[0m  [64/84], [94mLoss[0m : 2.39784
[1mStep[0m  [72/84], [94mLoss[0m : 2.48783
[1mStep[0m  [80/84], [94mLoss[0m : 2.68859

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31960
[1mStep[0m  [8/84], [94mLoss[0m : 2.21290
[1mStep[0m  [16/84], [94mLoss[0m : 2.68066
[1mStep[0m  [24/84], [94mLoss[0m : 2.59117
[1mStep[0m  [32/84], [94mLoss[0m : 2.56195
[1mStep[0m  [40/84], [94mLoss[0m : 2.50738
[1mStep[0m  [48/84], [94mLoss[0m : 2.34666
[1mStep[0m  [56/84], [94mLoss[0m : 2.67932
[1mStep[0m  [64/84], [94mLoss[0m : 2.72172
[1mStep[0m  [72/84], [94mLoss[0m : 2.62888
[1mStep[0m  [80/84], [94mLoss[0m : 2.40359

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89984
[1mStep[0m  [8/84], [94mLoss[0m : 2.34725
[1mStep[0m  [16/84], [94mLoss[0m : 2.59644
[1mStep[0m  [24/84], [94mLoss[0m : 2.61068
[1mStep[0m  [32/84], [94mLoss[0m : 2.32113
[1mStep[0m  [40/84], [94mLoss[0m : 2.65156
[1mStep[0m  [48/84], [94mLoss[0m : 2.28643
[1mStep[0m  [56/84], [94mLoss[0m : 2.62362
[1mStep[0m  [64/84], [94mLoss[0m : 2.59316
[1mStep[0m  [72/84], [94mLoss[0m : 2.78198
[1mStep[0m  [80/84], [94mLoss[0m : 2.42604

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64825
[1mStep[0m  [8/84], [94mLoss[0m : 2.39985
[1mStep[0m  [16/84], [94mLoss[0m : 2.76593
[1mStep[0m  [24/84], [94mLoss[0m : 2.70835
[1mStep[0m  [32/84], [94mLoss[0m : 2.55358
[1mStep[0m  [40/84], [94mLoss[0m : 2.59029
[1mStep[0m  [48/84], [94mLoss[0m : 2.82924
[1mStep[0m  [56/84], [94mLoss[0m : 2.54003
[1mStep[0m  [64/84], [94mLoss[0m : 2.41973
[1mStep[0m  [72/84], [94mLoss[0m : 2.53120
[1mStep[0m  [80/84], [94mLoss[0m : 2.70857

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37219
[1mStep[0m  [8/84], [94mLoss[0m : 2.65766
[1mStep[0m  [16/84], [94mLoss[0m : 2.57193
[1mStep[0m  [24/84], [94mLoss[0m : 3.02448
[1mStep[0m  [32/84], [94mLoss[0m : 2.40889
[1mStep[0m  [40/84], [94mLoss[0m : 2.52695
[1mStep[0m  [48/84], [94mLoss[0m : 2.61288
[1mStep[0m  [56/84], [94mLoss[0m : 2.14943
[1mStep[0m  [64/84], [94mLoss[0m : 2.60332
[1mStep[0m  [72/84], [94mLoss[0m : 2.44247
[1mStep[0m  [80/84], [94mLoss[0m : 2.54381

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35706
[1mStep[0m  [8/84], [94mLoss[0m : 2.77393
[1mStep[0m  [16/84], [94mLoss[0m : 2.52971
[1mStep[0m  [24/84], [94mLoss[0m : 2.73936
[1mStep[0m  [32/84], [94mLoss[0m : 2.38617
[1mStep[0m  [40/84], [94mLoss[0m : 2.46368
[1mStep[0m  [48/84], [94mLoss[0m : 2.48807
[1mStep[0m  [56/84], [94mLoss[0m : 2.49536
[1mStep[0m  [64/84], [94mLoss[0m : 2.07037
[1mStep[0m  [72/84], [94mLoss[0m : 2.55296
[1mStep[0m  [80/84], [94mLoss[0m : 2.60712

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68986
[1mStep[0m  [8/84], [94mLoss[0m : 2.22191
[1mStep[0m  [16/84], [94mLoss[0m : 2.25276
[1mStep[0m  [24/84], [94mLoss[0m : 2.22217
[1mStep[0m  [32/84], [94mLoss[0m : 2.55046
[1mStep[0m  [40/84], [94mLoss[0m : 2.62324
[1mStep[0m  [48/84], [94mLoss[0m : 2.31219
[1mStep[0m  [56/84], [94mLoss[0m : 2.53026
[1mStep[0m  [64/84], [94mLoss[0m : 2.67526
[1mStep[0m  [72/84], [94mLoss[0m : 2.60223
[1mStep[0m  [80/84], [94mLoss[0m : 2.51960

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22296
[1mStep[0m  [8/84], [94mLoss[0m : 2.38086
[1mStep[0m  [16/84], [94mLoss[0m : 2.53224
[1mStep[0m  [24/84], [94mLoss[0m : 2.32817
[1mStep[0m  [32/84], [94mLoss[0m : 2.43295
[1mStep[0m  [40/84], [94mLoss[0m : 2.62354
[1mStep[0m  [48/84], [94mLoss[0m : 2.26175
[1mStep[0m  [56/84], [94mLoss[0m : 2.67689
[1mStep[0m  [64/84], [94mLoss[0m : 2.62647
[1mStep[0m  [72/84], [94mLoss[0m : 2.58001
[1mStep[0m  [80/84], [94mLoss[0m : 2.89879

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11890
[1mStep[0m  [8/84], [94mLoss[0m : 2.45877
[1mStep[0m  [16/84], [94mLoss[0m : 2.74351
[1mStep[0m  [24/84], [94mLoss[0m : 2.21117
[1mStep[0m  [32/84], [94mLoss[0m : 2.73585
[1mStep[0m  [40/84], [94mLoss[0m : 2.61943
[1mStep[0m  [48/84], [94mLoss[0m : 2.54493
[1mStep[0m  [56/84], [94mLoss[0m : 2.49455
[1mStep[0m  [64/84], [94mLoss[0m : 2.38295
[1mStep[0m  [72/84], [94mLoss[0m : 2.41372
[1mStep[0m  [80/84], [94mLoss[0m : 2.38893

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35140
[1mStep[0m  [8/84], [94mLoss[0m : 2.33141
[1mStep[0m  [16/84], [94mLoss[0m : 2.57657
[1mStep[0m  [24/84], [94mLoss[0m : 2.33450
[1mStep[0m  [32/84], [94mLoss[0m : 2.65985
[1mStep[0m  [40/84], [94mLoss[0m : 2.48995
[1mStep[0m  [48/84], [94mLoss[0m : 2.94715
[1mStep[0m  [56/84], [94mLoss[0m : 2.62308
[1mStep[0m  [64/84], [94mLoss[0m : 2.60380
[1mStep[0m  [72/84], [94mLoss[0m : 2.36778
[1mStep[0m  [80/84], [94mLoss[0m : 2.42354

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57423
[1mStep[0m  [8/84], [94mLoss[0m : 2.42228
[1mStep[0m  [16/84], [94mLoss[0m : 2.63355
[1mStep[0m  [24/84], [94mLoss[0m : 2.60488
[1mStep[0m  [32/84], [94mLoss[0m : 2.29810
[1mStep[0m  [40/84], [94mLoss[0m : 2.09507
[1mStep[0m  [48/84], [94mLoss[0m : 2.51472
[1mStep[0m  [56/84], [94mLoss[0m : 2.75130
[1mStep[0m  [64/84], [94mLoss[0m : 2.46470
[1mStep[0m  [72/84], [94mLoss[0m : 2.39915
[1mStep[0m  [80/84], [94mLoss[0m : 2.39467

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33371
[1mStep[0m  [8/84], [94mLoss[0m : 2.47706
[1mStep[0m  [16/84], [94mLoss[0m : 2.44646
[1mStep[0m  [24/84], [94mLoss[0m : 2.48828
[1mStep[0m  [32/84], [94mLoss[0m : 2.16763
[1mStep[0m  [40/84], [94mLoss[0m : 2.48063
[1mStep[0m  [48/84], [94mLoss[0m : 2.53316
[1mStep[0m  [56/84], [94mLoss[0m : 2.30459
[1mStep[0m  [64/84], [94mLoss[0m : 2.53641
[1mStep[0m  [72/84], [94mLoss[0m : 2.53092
[1mStep[0m  [80/84], [94mLoss[0m : 2.54685

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67437
[1mStep[0m  [8/84], [94mLoss[0m : 2.32232
[1mStep[0m  [16/84], [94mLoss[0m : 2.64592
[1mStep[0m  [24/84], [94mLoss[0m : 2.40862
[1mStep[0m  [32/84], [94mLoss[0m : 2.55808
[1mStep[0m  [40/84], [94mLoss[0m : 2.40524
[1mStep[0m  [48/84], [94mLoss[0m : 2.38640
[1mStep[0m  [56/84], [94mLoss[0m : 2.59121
[1mStep[0m  [64/84], [94mLoss[0m : 2.25757
[1mStep[0m  [72/84], [94mLoss[0m : 2.27239
[1mStep[0m  [80/84], [94mLoss[0m : 2.32446

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74871
[1mStep[0m  [8/84], [94mLoss[0m : 2.79909
[1mStep[0m  [16/84], [94mLoss[0m : 2.48849
[1mStep[0m  [24/84], [94mLoss[0m : 2.15613
[1mStep[0m  [32/84], [94mLoss[0m : 2.41777
[1mStep[0m  [40/84], [94mLoss[0m : 2.63293
[1mStep[0m  [48/84], [94mLoss[0m : 2.60870
[1mStep[0m  [56/84], [94mLoss[0m : 2.63563
[1mStep[0m  [64/84], [94mLoss[0m : 2.39863
[1mStep[0m  [72/84], [94mLoss[0m : 2.61374
[1mStep[0m  [80/84], [94mLoss[0m : 2.39242

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61399
[1mStep[0m  [8/84], [94mLoss[0m : 2.52382
[1mStep[0m  [16/84], [94mLoss[0m : 2.42837
[1mStep[0m  [24/84], [94mLoss[0m : 2.47032
[1mStep[0m  [32/84], [94mLoss[0m : 2.55317
[1mStep[0m  [40/84], [94mLoss[0m : 2.37210
[1mStep[0m  [48/84], [94mLoss[0m : 2.27296
[1mStep[0m  [56/84], [94mLoss[0m : 2.53665
[1mStep[0m  [64/84], [94mLoss[0m : 2.46745
[1mStep[0m  [72/84], [94mLoss[0m : 2.42770
[1mStep[0m  [80/84], [94mLoss[0m : 2.29882

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.333123121942793
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.11778
[1mStep[0m  [8/84], [94mLoss[0m : 2.64258
[1mStep[0m  [16/84], [94mLoss[0m : 2.55939
[1mStep[0m  [24/84], [94mLoss[0m : 2.74052
[1mStep[0m  [32/84], [94mLoss[0m : 2.43641
[1mStep[0m  [40/84], [94mLoss[0m : 2.41003
[1mStep[0m  [48/84], [94mLoss[0m : 2.39556
[1mStep[0m  [56/84], [94mLoss[0m : 2.56855
[1mStep[0m  [64/84], [94mLoss[0m : 2.56246
[1mStep[0m  [72/84], [94mLoss[0m : 2.36824
[1mStep[0m  [80/84], [94mLoss[0m : 2.52201

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51462
[1mStep[0m  [8/84], [94mLoss[0m : 2.54104
[1mStep[0m  [16/84], [94mLoss[0m : 2.38225
[1mStep[0m  [24/84], [94mLoss[0m : 2.33360
[1mStep[0m  [32/84], [94mLoss[0m : 2.34550
[1mStep[0m  [40/84], [94mLoss[0m : 2.72494
[1mStep[0m  [48/84], [94mLoss[0m : 2.49411
[1mStep[0m  [56/84], [94mLoss[0m : 2.76215
[1mStep[0m  [64/84], [94mLoss[0m : 2.49765
[1mStep[0m  [72/84], [94mLoss[0m : 2.96475
[1mStep[0m  [80/84], [94mLoss[0m : 2.49707

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.542, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63767
[1mStep[0m  [8/84], [94mLoss[0m : 2.64188
[1mStep[0m  [16/84], [94mLoss[0m : 2.61101
[1mStep[0m  [24/84], [94mLoss[0m : 2.82475
[1mStep[0m  [32/84], [94mLoss[0m : 2.30576
[1mStep[0m  [40/84], [94mLoss[0m : 2.38776
[1mStep[0m  [48/84], [94mLoss[0m : 2.51038
[1mStep[0m  [56/84], [94mLoss[0m : 2.33117
[1mStep[0m  [64/84], [94mLoss[0m : 2.35032
[1mStep[0m  [72/84], [94mLoss[0m : 2.54514
[1mStep[0m  [80/84], [94mLoss[0m : 2.54047

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.751, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21799
[1mStep[0m  [8/84], [94mLoss[0m : 2.67755
[1mStep[0m  [16/84], [94mLoss[0m : 2.79121
[1mStep[0m  [24/84], [94mLoss[0m : 2.54321
[1mStep[0m  [32/84], [94mLoss[0m : 2.25672
[1mStep[0m  [40/84], [94mLoss[0m : 2.72744
[1mStep[0m  [48/84], [94mLoss[0m : 2.41819
[1mStep[0m  [56/84], [94mLoss[0m : 2.31464
[1mStep[0m  [64/84], [94mLoss[0m : 2.11092
[1mStep[0m  [72/84], [94mLoss[0m : 2.61408
[1mStep[0m  [80/84], [94mLoss[0m : 2.51736

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.635, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49751
[1mStep[0m  [8/84], [94mLoss[0m : 2.67665
[1mStep[0m  [16/84], [94mLoss[0m : 2.26604
[1mStep[0m  [24/84], [94mLoss[0m : 2.34443
[1mStep[0m  [32/84], [94mLoss[0m : 2.22219
[1mStep[0m  [40/84], [94mLoss[0m : 2.30406
[1mStep[0m  [48/84], [94mLoss[0m : 2.35879
[1mStep[0m  [56/84], [94mLoss[0m : 2.29981
[1mStep[0m  [64/84], [94mLoss[0m : 2.39182
[1mStep[0m  [72/84], [94mLoss[0m : 2.51086
[1mStep[0m  [80/84], [94mLoss[0m : 2.26986

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24869
[1mStep[0m  [8/84], [94mLoss[0m : 2.14164
[1mStep[0m  [16/84], [94mLoss[0m : 2.19148
[1mStep[0m  [24/84], [94mLoss[0m : 2.35253
[1mStep[0m  [32/84], [94mLoss[0m : 2.36174
[1mStep[0m  [40/84], [94mLoss[0m : 2.21327
[1mStep[0m  [48/84], [94mLoss[0m : 2.45477
[1mStep[0m  [56/84], [94mLoss[0m : 2.23248
[1mStep[0m  [64/84], [94mLoss[0m : 2.02620
[1mStep[0m  [72/84], [94mLoss[0m : 2.39666
[1mStep[0m  [80/84], [94mLoss[0m : 2.04967

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43882
[1mStep[0m  [8/84], [94mLoss[0m : 2.05368
[1mStep[0m  [16/84], [94mLoss[0m : 2.35671
[1mStep[0m  [24/84], [94mLoss[0m : 2.55256
[1mStep[0m  [32/84], [94mLoss[0m : 2.26737
[1mStep[0m  [40/84], [94mLoss[0m : 1.88960
[1mStep[0m  [48/84], [94mLoss[0m : 2.31764
[1mStep[0m  [56/84], [94mLoss[0m : 2.26777
[1mStep[0m  [64/84], [94mLoss[0m : 2.30011
[1mStep[0m  [72/84], [94mLoss[0m : 2.12854
[1mStep[0m  [80/84], [94mLoss[0m : 2.55796

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.568, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26149
[1mStep[0m  [8/84], [94mLoss[0m : 2.38463
[1mStep[0m  [16/84], [94mLoss[0m : 2.34195
[1mStep[0m  [24/84], [94mLoss[0m : 2.02636
[1mStep[0m  [32/84], [94mLoss[0m : 2.09685
[1mStep[0m  [40/84], [94mLoss[0m : 2.11477
[1mStep[0m  [48/84], [94mLoss[0m : 2.44201
[1mStep[0m  [56/84], [94mLoss[0m : 2.11223
[1mStep[0m  [64/84], [94mLoss[0m : 2.52007
[1mStep[0m  [72/84], [94mLoss[0m : 2.03614
[1mStep[0m  [80/84], [94mLoss[0m : 2.14674

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69556
[1mStep[0m  [8/84], [94mLoss[0m : 1.99680
[1mStep[0m  [16/84], [94mLoss[0m : 2.17341
[1mStep[0m  [24/84], [94mLoss[0m : 2.27043
[1mStep[0m  [32/84], [94mLoss[0m : 2.29125
[1mStep[0m  [40/84], [94mLoss[0m : 2.05021
[1mStep[0m  [48/84], [94mLoss[0m : 2.19471
[1mStep[0m  [56/84], [94mLoss[0m : 2.32169
[1mStep[0m  [64/84], [94mLoss[0m : 2.17232
[1mStep[0m  [72/84], [94mLoss[0m : 2.44918
[1mStep[0m  [80/84], [94mLoss[0m : 2.01149

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88827
[1mStep[0m  [8/84], [94mLoss[0m : 2.52459
[1mStep[0m  [16/84], [94mLoss[0m : 2.12668
[1mStep[0m  [24/84], [94mLoss[0m : 2.10834
[1mStep[0m  [32/84], [94mLoss[0m : 1.96309
[1mStep[0m  [40/84], [94mLoss[0m : 2.28469
[1mStep[0m  [48/84], [94mLoss[0m : 1.86046
[1mStep[0m  [56/84], [94mLoss[0m : 2.39434
[1mStep[0m  [64/84], [94mLoss[0m : 2.11386
[1mStep[0m  [72/84], [94mLoss[0m : 2.42966
[1mStep[0m  [80/84], [94mLoss[0m : 2.11182

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78513
[1mStep[0m  [8/84], [94mLoss[0m : 2.17280
[1mStep[0m  [16/84], [94mLoss[0m : 2.10977
[1mStep[0m  [24/84], [94mLoss[0m : 2.47000
[1mStep[0m  [32/84], [94mLoss[0m : 2.25433
[1mStep[0m  [40/84], [94mLoss[0m : 2.13964
[1mStep[0m  [48/84], [94mLoss[0m : 2.33338
[1mStep[0m  [56/84], [94mLoss[0m : 2.25970
[1mStep[0m  [64/84], [94mLoss[0m : 2.19173
[1mStep[0m  [72/84], [94mLoss[0m : 2.10014
[1mStep[0m  [80/84], [94mLoss[0m : 2.21001

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09462
[1mStep[0m  [8/84], [94mLoss[0m : 1.88318
[1mStep[0m  [16/84], [94mLoss[0m : 1.99894
[1mStep[0m  [24/84], [94mLoss[0m : 2.15864
[1mStep[0m  [32/84], [94mLoss[0m : 2.01074
[1mStep[0m  [40/84], [94mLoss[0m : 2.28881
[1mStep[0m  [48/84], [94mLoss[0m : 2.08482
[1mStep[0m  [56/84], [94mLoss[0m : 1.86195
[1mStep[0m  [64/84], [94mLoss[0m : 2.08345
[1mStep[0m  [72/84], [94mLoss[0m : 2.07263
[1mStep[0m  [80/84], [94mLoss[0m : 1.96955

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98918
[1mStep[0m  [8/84], [94mLoss[0m : 2.25671
[1mStep[0m  [16/84], [94mLoss[0m : 1.89885
[1mStep[0m  [24/84], [94mLoss[0m : 1.95823
[1mStep[0m  [32/84], [94mLoss[0m : 2.10844
[1mStep[0m  [40/84], [94mLoss[0m : 2.07110
[1mStep[0m  [48/84], [94mLoss[0m : 2.09528
[1mStep[0m  [56/84], [94mLoss[0m : 2.02620
[1mStep[0m  [64/84], [94mLoss[0m : 2.06070
[1mStep[0m  [72/84], [94mLoss[0m : 1.96741
[1mStep[0m  [80/84], [94mLoss[0m : 2.07388

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07059
[1mStep[0m  [8/84], [94mLoss[0m : 1.71266
[1mStep[0m  [16/84], [94mLoss[0m : 1.81812
[1mStep[0m  [24/84], [94mLoss[0m : 1.95880
[1mStep[0m  [32/84], [94mLoss[0m : 2.09295
[1mStep[0m  [40/84], [94mLoss[0m : 2.06366
[1mStep[0m  [48/84], [94mLoss[0m : 2.02954
[1mStep[0m  [56/84], [94mLoss[0m : 2.02291
[1mStep[0m  [64/84], [94mLoss[0m : 1.98343
[1mStep[0m  [72/84], [94mLoss[0m : 1.88374
[1mStep[0m  [80/84], [94mLoss[0m : 1.90839

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77259
[1mStep[0m  [8/84], [94mLoss[0m : 1.98487
[1mStep[0m  [16/84], [94mLoss[0m : 1.76407
[1mStep[0m  [24/84], [94mLoss[0m : 1.87074
[1mStep[0m  [32/84], [94mLoss[0m : 1.93807
[1mStep[0m  [40/84], [94mLoss[0m : 2.16868
[1mStep[0m  [48/84], [94mLoss[0m : 2.05541
[1mStep[0m  [56/84], [94mLoss[0m : 1.93348
[1mStep[0m  [64/84], [94mLoss[0m : 1.92846
[1mStep[0m  [72/84], [94mLoss[0m : 2.35188
[1mStep[0m  [80/84], [94mLoss[0m : 2.31542

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16214
[1mStep[0m  [8/84], [94mLoss[0m : 1.65115
[1mStep[0m  [16/84], [94mLoss[0m : 1.98659
[1mStep[0m  [24/84], [94mLoss[0m : 1.72086
[1mStep[0m  [32/84], [94mLoss[0m : 1.78539
[1mStep[0m  [40/84], [94mLoss[0m : 1.98617
[1mStep[0m  [48/84], [94mLoss[0m : 1.78789
[1mStep[0m  [56/84], [94mLoss[0m : 1.84765
[1mStep[0m  [64/84], [94mLoss[0m : 2.11132
[1mStep[0m  [72/84], [94mLoss[0m : 2.15442
[1mStep[0m  [80/84], [94mLoss[0m : 1.95166

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.941, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72565
[1mStep[0m  [8/84], [94mLoss[0m : 1.64683
[1mStep[0m  [16/84], [94mLoss[0m : 2.28664
[1mStep[0m  [24/84], [94mLoss[0m : 1.76200
[1mStep[0m  [32/84], [94mLoss[0m : 1.74620
[1mStep[0m  [40/84], [94mLoss[0m : 1.93323
[1mStep[0m  [48/84], [94mLoss[0m : 2.03374
[1mStep[0m  [56/84], [94mLoss[0m : 1.94005
[1mStep[0m  [64/84], [94mLoss[0m : 1.90426
[1mStep[0m  [72/84], [94mLoss[0m : 2.01523
[1mStep[0m  [80/84], [94mLoss[0m : 2.08562

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04829
[1mStep[0m  [8/84], [94mLoss[0m : 1.84806
[1mStep[0m  [16/84], [94mLoss[0m : 1.85317
[1mStep[0m  [24/84], [94mLoss[0m : 1.96745
[1mStep[0m  [32/84], [94mLoss[0m : 2.08469
[1mStep[0m  [40/84], [94mLoss[0m : 2.24039
[1mStep[0m  [48/84], [94mLoss[0m : 1.98600
[1mStep[0m  [56/84], [94mLoss[0m : 1.99201
[1mStep[0m  [64/84], [94mLoss[0m : 1.80037
[1mStep[0m  [72/84], [94mLoss[0m : 1.89112
[1mStep[0m  [80/84], [94mLoss[0m : 1.77718

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93152
[1mStep[0m  [8/84], [94mLoss[0m : 1.55680
[1mStep[0m  [16/84], [94mLoss[0m : 1.79465
[1mStep[0m  [24/84], [94mLoss[0m : 1.80734
[1mStep[0m  [32/84], [94mLoss[0m : 2.07736
[1mStep[0m  [40/84], [94mLoss[0m : 1.96405
[1mStep[0m  [48/84], [94mLoss[0m : 1.93499
[1mStep[0m  [56/84], [94mLoss[0m : 1.75734
[1mStep[0m  [64/84], [94mLoss[0m : 2.06782
[1mStep[0m  [72/84], [94mLoss[0m : 1.72875
[1mStep[0m  [80/84], [94mLoss[0m : 1.66209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.495, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77373
[1mStep[0m  [8/84], [94mLoss[0m : 1.85161
[1mStep[0m  [16/84], [94mLoss[0m : 1.84726
[1mStep[0m  [24/84], [94mLoss[0m : 1.97933
[1mStep[0m  [32/84], [94mLoss[0m : 1.67151
[1mStep[0m  [40/84], [94mLoss[0m : 1.71948
[1mStep[0m  [48/84], [94mLoss[0m : 1.96049
[1mStep[0m  [56/84], [94mLoss[0m : 1.94019
[1mStep[0m  [64/84], [94mLoss[0m : 1.70471
[1mStep[0m  [72/84], [94mLoss[0m : 1.79941
[1mStep[0m  [80/84], [94mLoss[0m : 1.52939

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85689
[1mStep[0m  [8/84], [94mLoss[0m : 1.73376
[1mStep[0m  [16/84], [94mLoss[0m : 1.56902
[1mStep[0m  [24/84], [94mLoss[0m : 1.92422
[1mStep[0m  [32/84], [94mLoss[0m : 1.68076
[1mStep[0m  [40/84], [94mLoss[0m : 1.69723
[1mStep[0m  [48/84], [94mLoss[0m : 1.93409
[1mStep[0m  [56/84], [94mLoss[0m : 1.97543
[1mStep[0m  [64/84], [94mLoss[0m : 2.11748
[1mStep[0m  [72/84], [94mLoss[0m : 1.69524
[1mStep[0m  [80/84], [94mLoss[0m : 1.94908

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63112
[1mStep[0m  [8/84], [94mLoss[0m : 1.83461
[1mStep[0m  [16/84], [94mLoss[0m : 1.84534
[1mStep[0m  [24/84], [94mLoss[0m : 1.62047
[1mStep[0m  [32/84], [94mLoss[0m : 1.66146
[1mStep[0m  [40/84], [94mLoss[0m : 1.59901
[1mStep[0m  [48/84], [94mLoss[0m : 1.66296
[1mStep[0m  [56/84], [94mLoss[0m : 1.99127
[1mStep[0m  [64/84], [94mLoss[0m : 1.73362
[1mStep[0m  [72/84], [94mLoss[0m : 1.76074
[1mStep[0m  [80/84], [94mLoss[0m : 1.72862

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58287
[1mStep[0m  [8/84], [94mLoss[0m : 1.52914
[1mStep[0m  [16/84], [94mLoss[0m : 1.79428
[1mStep[0m  [24/84], [94mLoss[0m : 1.56652
[1mStep[0m  [32/84], [94mLoss[0m : 1.78415
[1mStep[0m  [40/84], [94mLoss[0m : 1.88874
[1mStep[0m  [48/84], [94mLoss[0m : 1.72444
[1mStep[0m  [56/84], [94mLoss[0m : 1.68492
[1mStep[0m  [64/84], [94mLoss[0m : 1.85578
[1mStep[0m  [72/84], [94mLoss[0m : 1.80840
[1mStep[0m  [80/84], [94mLoss[0m : 1.57825

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97028
[1mStep[0m  [8/84], [94mLoss[0m : 1.66605
[1mStep[0m  [16/84], [94mLoss[0m : 1.75233
[1mStep[0m  [24/84], [94mLoss[0m : 1.86867
[1mStep[0m  [32/84], [94mLoss[0m : 1.62666
[1mStep[0m  [40/84], [94mLoss[0m : 1.81411
[1mStep[0m  [48/84], [94mLoss[0m : 1.66872
[1mStep[0m  [56/84], [94mLoss[0m : 1.64397
[1mStep[0m  [64/84], [94mLoss[0m : 1.89307
[1mStep[0m  [72/84], [94mLoss[0m : 1.84870
[1mStep[0m  [80/84], [94mLoss[0m : 1.72782

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51436
[1mStep[0m  [8/84], [94mLoss[0m : 1.69700
[1mStep[0m  [16/84], [94mLoss[0m : 1.86755
[1mStep[0m  [24/84], [94mLoss[0m : 1.70425
[1mStep[0m  [32/84], [94mLoss[0m : 1.50497
[1mStep[0m  [40/84], [94mLoss[0m : 1.68287
[1mStep[0m  [48/84], [94mLoss[0m : 1.77034
[1mStep[0m  [56/84], [94mLoss[0m : 1.86932
[1mStep[0m  [64/84], [94mLoss[0m : 1.66940
[1mStep[0m  [72/84], [94mLoss[0m : 1.84884
[1mStep[0m  [80/84], [94mLoss[0m : 1.73802

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.524, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62842
[1mStep[0m  [8/84], [94mLoss[0m : 1.76110
[1mStep[0m  [16/84], [94mLoss[0m : 1.59492
[1mStep[0m  [24/84], [94mLoss[0m : 1.58733
[1mStep[0m  [32/84], [94mLoss[0m : 1.54862
[1mStep[0m  [40/84], [94mLoss[0m : 1.58958
[1mStep[0m  [48/84], [94mLoss[0m : 1.90264
[1mStep[0m  [56/84], [94mLoss[0m : 1.67862
[1mStep[0m  [64/84], [94mLoss[0m : 1.86087
[1mStep[0m  [72/84], [94mLoss[0m : 1.77174
[1mStep[0m  [80/84], [94mLoss[0m : 1.57126

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85710
[1mStep[0m  [8/84], [94mLoss[0m : 1.59276
[1mStep[0m  [16/84], [94mLoss[0m : 1.66263
[1mStep[0m  [24/84], [94mLoss[0m : 1.58623
[1mStep[0m  [32/84], [94mLoss[0m : 1.67197
[1mStep[0m  [40/84], [94mLoss[0m : 1.59552
[1mStep[0m  [48/84], [94mLoss[0m : 1.73489
[1mStep[0m  [56/84], [94mLoss[0m : 1.50646
[1mStep[0m  [64/84], [94mLoss[0m : 1.70929
[1mStep[0m  [72/84], [94mLoss[0m : 1.74912
[1mStep[0m  [80/84], [94mLoss[0m : 1.86952

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41303
[1mStep[0m  [8/84], [94mLoss[0m : 1.64792
[1mStep[0m  [16/84], [94mLoss[0m : 1.76384
[1mStep[0m  [24/84], [94mLoss[0m : 1.59708
[1mStep[0m  [32/84], [94mLoss[0m : 1.56913
[1mStep[0m  [40/84], [94mLoss[0m : 1.40486
[1mStep[0m  [48/84], [94mLoss[0m : 1.71237
[1mStep[0m  [56/84], [94mLoss[0m : 1.52574
[1mStep[0m  [64/84], [94mLoss[0m : 1.83397
[1mStep[0m  [72/84], [94mLoss[0m : 1.87303
[1mStep[0m  [80/84], [94mLoss[0m : 1.55442

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71348
[1mStep[0m  [8/84], [94mLoss[0m : 1.49926
[1mStep[0m  [16/84], [94mLoss[0m : 1.62793
[1mStep[0m  [24/84], [94mLoss[0m : 1.74573
[1mStep[0m  [32/84], [94mLoss[0m : 1.57630
[1mStep[0m  [40/84], [94mLoss[0m : 1.49173
[1mStep[0m  [48/84], [94mLoss[0m : 1.52094
[1mStep[0m  [56/84], [94mLoss[0m : 1.69867
[1mStep[0m  [64/84], [94mLoss[0m : 1.45300
[1mStep[0m  [72/84], [94mLoss[0m : 1.69604
[1mStep[0m  [80/84], [94mLoss[0m : 1.51992

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.552, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42965
[1mStep[0m  [8/84], [94mLoss[0m : 1.71719
[1mStep[0m  [16/84], [94mLoss[0m : 1.52802
[1mStep[0m  [24/84], [94mLoss[0m : 1.56683
[1mStep[0m  [32/84], [94mLoss[0m : 1.54496
[1mStep[0m  [40/84], [94mLoss[0m : 1.47747
[1mStep[0m  [48/84], [94mLoss[0m : 1.68377
[1mStep[0m  [56/84], [94mLoss[0m : 1.53806
[1mStep[0m  [64/84], [94mLoss[0m : 1.57582
[1mStep[0m  [72/84], [94mLoss[0m : 1.64508
[1mStep[0m  [80/84], [94mLoss[0m : 1.47025

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.517
====================================

Phase 2 - Evaluation MAE:  2.517331429890224
MAE score P1       2.333123
MAE score P2       2.517331
loss               1.588804
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.08206
[1mStep[0m  [4/42], [94mLoss[0m : 10.78953
[1mStep[0m  [8/42], [94mLoss[0m : 10.19508
[1mStep[0m  [12/42], [94mLoss[0m : 9.66186
[1mStep[0m  [16/42], [94mLoss[0m : 8.82448
[1mStep[0m  [20/42], [94mLoss[0m : 8.67573
[1mStep[0m  [24/42], [94mLoss[0m : 7.73004
[1mStep[0m  [28/42], [94mLoss[0m : 7.47486
[1mStep[0m  [32/42], [94mLoss[0m : 6.79892
[1mStep[0m  [36/42], [94mLoss[0m : 6.36760
[1mStep[0m  [40/42], [94mLoss[0m : 6.07171

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.401, [92mTest[0m: 11.126, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.85376
[1mStep[0m  [4/42], [94mLoss[0m : 5.31052
[1mStep[0m  [8/42], [94mLoss[0m : 4.26296
[1mStep[0m  [12/42], [94mLoss[0m : 4.28963
[1mStep[0m  [16/42], [94mLoss[0m : 3.57989
[1mStep[0m  [20/42], [94mLoss[0m : 3.76993
[1mStep[0m  [24/42], [94mLoss[0m : 3.44573
[1mStep[0m  [28/42], [94mLoss[0m : 3.41303
[1mStep[0m  [32/42], [94mLoss[0m : 3.10893
[1mStep[0m  [36/42], [94mLoss[0m : 2.91006
[1mStep[0m  [40/42], [94mLoss[0m : 2.70772

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.836, [92mTest[0m: 5.495, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89302
[1mStep[0m  [4/42], [94mLoss[0m : 3.08515
[1mStep[0m  [8/42], [94mLoss[0m : 2.79262
[1mStep[0m  [12/42], [94mLoss[0m : 2.64964
[1mStep[0m  [16/42], [94mLoss[0m : 2.70737
[1mStep[0m  [20/42], [94mLoss[0m : 2.77429
[1mStep[0m  [24/42], [94mLoss[0m : 2.62848
[1mStep[0m  [28/42], [94mLoss[0m : 2.60575
[1mStep[0m  [32/42], [94mLoss[0m : 2.47472
[1mStep[0m  [36/42], [94mLoss[0m : 2.48427
[1mStep[0m  [40/42], [94mLoss[0m : 2.56776

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.733, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57849
[1mStep[0m  [4/42], [94mLoss[0m : 2.77297
[1mStep[0m  [8/42], [94mLoss[0m : 2.49019
[1mStep[0m  [12/42], [94mLoss[0m : 2.39194
[1mStep[0m  [16/42], [94mLoss[0m : 2.52340
[1mStep[0m  [20/42], [94mLoss[0m : 2.49994
[1mStep[0m  [24/42], [94mLoss[0m : 2.61729
[1mStep[0m  [28/42], [94mLoss[0m : 2.83432
[1mStep[0m  [32/42], [94mLoss[0m : 2.51464
[1mStep[0m  [36/42], [94mLoss[0m : 2.30215
[1mStep[0m  [40/42], [94mLoss[0m : 2.57164

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41563
[1mStep[0m  [4/42], [94mLoss[0m : 2.70799
[1mStep[0m  [8/42], [94mLoss[0m : 2.46537
[1mStep[0m  [12/42], [94mLoss[0m : 2.39516
[1mStep[0m  [16/42], [94mLoss[0m : 2.86577
[1mStep[0m  [20/42], [94mLoss[0m : 2.50719
[1mStep[0m  [24/42], [94mLoss[0m : 2.45957
[1mStep[0m  [28/42], [94mLoss[0m : 2.46401
[1mStep[0m  [32/42], [94mLoss[0m : 2.34888
[1mStep[0m  [36/42], [94mLoss[0m : 2.39652
[1mStep[0m  [40/42], [94mLoss[0m : 2.73699

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52901
[1mStep[0m  [4/42], [94mLoss[0m : 2.62210
[1mStep[0m  [8/42], [94mLoss[0m : 2.60731
[1mStep[0m  [12/42], [94mLoss[0m : 2.70555
[1mStep[0m  [16/42], [94mLoss[0m : 2.66996
[1mStep[0m  [20/42], [94mLoss[0m : 2.37862
[1mStep[0m  [24/42], [94mLoss[0m : 2.56520
[1mStep[0m  [28/42], [94mLoss[0m : 2.59530
[1mStep[0m  [32/42], [94mLoss[0m : 2.68279
[1mStep[0m  [36/42], [94mLoss[0m : 2.68367
[1mStep[0m  [40/42], [94mLoss[0m : 2.66640

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63809
[1mStep[0m  [4/42], [94mLoss[0m : 2.59713
[1mStep[0m  [8/42], [94mLoss[0m : 2.73395
[1mStep[0m  [12/42], [94mLoss[0m : 2.74755
[1mStep[0m  [16/42], [94mLoss[0m : 2.61418
[1mStep[0m  [20/42], [94mLoss[0m : 2.33914
[1mStep[0m  [24/42], [94mLoss[0m : 2.36659
[1mStep[0m  [28/42], [94mLoss[0m : 2.58148
[1mStep[0m  [32/42], [94mLoss[0m : 2.53611
[1mStep[0m  [36/42], [94mLoss[0m : 2.38710
[1mStep[0m  [40/42], [94mLoss[0m : 2.44106

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64840
[1mStep[0m  [4/42], [94mLoss[0m : 2.39037
[1mStep[0m  [8/42], [94mLoss[0m : 2.07754
[1mStep[0m  [12/42], [94mLoss[0m : 2.65346
[1mStep[0m  [16/42], [94mLoss[0m : 2.63328
[1mStep[0m  [20/42], [94mLoss[0m : 2.59586
[1mStep[0m  [24/42], [94mLoss[0m : 2.41172
[1mStep[0m  [28/42], [94mLoss[0m : 2.68764
[1mStep[0m  [32/42], [94mLoss[0m : 2.47693
[1mStep[0m  [36/42], [94mLoss[0m : 2.50628
[1mStep[0m  [40/42], [94mLoss[0m : 2.43973

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25802
[1mStep[0m  [4/42], [94mLoss[0m : 2.68217
[1mStep[0m  [8/42], [94mLoss[0m : 2.35956
[1mStep[0m  [12/42], [94mLoss[0m : 2.55078
[1mStep[0m  [16/42], [94mLoss[0m : 2.29887
[1mStep[0m  [20/42], [94mLoss[0m : 2.46025
[1mStep[0m  [24/42], [94mLoss[0m : 2.54261
[1mStep[0m  [28/42], [94mLoss[0m : 2.52360
[1mStep[0m  [32/42], [94mLoss[0m : 2.41654
[1mStep[0m  [36/42], [94mLoss[0m : 2.42462
[1mStep[0m  [40/42], [94mLoss[0m : 2.53720

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47443
[1mStep[0m  [4/42], [94mLoss[0m : 2.27762
[1mStep[0m  [8/42], [94mLoss[0m : 2.42243
[1mStep[0m  [12/42], [94mLoss[0m : 2.72075
[1mStep[0m  [16/42], [94mLoss[0m : 2.47480
[1mStep[0m  [20/42], [94mLoss[0m : 2.52196
[1mStep[0m  [24/42], [94mLoss[0m : 2.47922
[1mStep[0m  [28/42], [94mLoss[0m : 2.58616
[1mStep[0m  [32/42], [94mLoss[0m : 2.81373
[1mStep[0m  [36/42], [94mLoss[0m : 2.31962
[1mStep[0m  [40/42], [94mLoss[0m : 2.45544

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41173
[1mStep[0m  [4/42], [94mLoss[0m : 2.60295
[1mStep[0m  [8/42], [94mLoss[0m : 2.51517
[1mStep[0m  [12/42], [94mLoss[0m : 2.59245
[1mStep[0m  [16/42], [94mLoss[0m : 2.42910
[1mStep[0m  [20/42], [94mLoss[0m : 2.45077
[1mStep[0m  [24/42], [94mLoss[0m : 2.43102
[1mStep[0m  [28/42], [94mLoss[0m : 2.49171
[1mStep[0m  [32/42], [94mLoss[0m : 2.57531
[1mStep[0m  [36/42], [94mLoss[0m : 2.67892
[1mStep[0m  [40/42], [94mLoss[0m : 2.44492

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39292
[1mStep[0m  [4/42], [94mLoss[0m : 2.31476
[1mStep[0m  [8/42], [94mLoss[0m : 2.45963
[1mStep[0m  [12/42], [94mLoss[0m : 2.49034
[1mStep[0m  [16/42], [94mLoss[0m : 2.52403
[1mStep[0m  [20/42], [94mLoss[0m : 2.56595
[1mStep[0m  [24/42], [94mLoss[0m : 2.57804
[1mStep[0m  [28/42], [94mLoss[0m : 2.68065
[1mStep[0m  [32/42], [94mLoss[0m : 2.47808
[1mStep[0m  [36/42], [94mLoss[0m : 2.58016
[1mStep[0m  [40/42], [94mLoss[0m : 2.45488

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34789
[1mStep[0m  [4/42], [94mLoss[0m : 2.52020
[1mStep[0m  [8/42], [94mLoss[0m : 2.46024
[1mStep[0m  [12/42], [94mLoss[0m : 2.34350
[1mStep[0m  [16/42], [94mLoss[0m : 2.38745
[1mStep[0m  [20/42], [94mLoss[0m : 2.48692
[1mStep[0m  [24/42], [94mLoss[0m : 2.84951
[1mStep[0m  [28/42], [94mLoss[0m : 2.75436
[1mStep[0m  [32/42], [94mLoss[0m : 2.59473
[1mStep[0m  [36/42], [94mLoss[0m : 2.32184
[1mStep[0m  [40/42], [94mLoss[0m : 2.49145

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51995
[1mStep[0m  [4/42], [94mLoss[0m : 2.57677
[1mStep[0m  [8/42], [94mLoss[0m : 2.43637
[1mStep[0m  [12/42], [94mLoss[0m : 2.57881
[1mStep[0m  [16/42], [94mLoss[0m : 2.44390
[1mStep[0m  [20/42], [94mLoss[0m : 2.38434
[1mStep[0m  [24/42], [94mLoss[0m : 2.54190
[1mStep[0m  [28/42], [94mLoss[0m : 2.57446
[1mStep[0m  [32/42], [94mLoss[0m : 2.32430
[1mStep[0m  [36/42], [94mLoss[0m : 2.68454
[1mStep[0m  [40/42], [94mLoss[0m : 2.59758

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40573
[1mStep[0m  [4/42], [94mLoss[0m : 2.35460
[1mStep[0m  [8/42], [94mLoss[0m : 2.49778
[1mStep[0m  [12/42], [94mLoss[0m : 2.58497
[1mStep[0m  [16/42], [94mLoss[0m : 2.45418
[1mStep[0m  [20/42], [94mLoss[0m : 2.34876
[1mStep[0m  [24/42], [94mLoss[0m : 2.50358
[1mStep[0m  [28/42], [94mLoss[0m : 2.61542
[1mStep[0m  [32/42], [94mLoss[0m : 2.65787
[1mStep[0m  [36/42], [94mLoss[0m : 2.62097
[1mStep[0m  [40/42], [94mLoss[0m : 2.61012

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41202
[1mStep[0m  [4/42], [94mLoss[0m : 2.44179
[1mStep[0m  [8/42], [94mLoss[0m : 2.66414
[1mStep[0m  [12/42], [94mLoss[0m : 2.43617
[1mStep[0m  [16/42], [94mLoss[0m : 2.33778
[1mStep[0m  [20/42], [94mLoss[0m : 2.71474
[1mStep[0m  [24/42], [94mLoss[0m : 2.52390
[1mStep[0m  [28/42], [94mLoss[0m : 2.46216
[1mStep[0m  [32/42], [94mLoss[0m : 2.25345
[1mStep[0m  [36/42], [94mLoss[0m : 2.44654
[1mStep[0m  [40/42], [94mLoss[0m : 2.37131

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40636
[1mStep[0m  [4/42], [94mLoss[0m : 2.56693
[1mStep[0m  [8/42], [94mLoss[0m : 2.58875
[1mStep[0m  [12/42], [94mLoss[0m : 2.80535
[1mStep[0m  [16/42], [94mLoss[0m : 2.75914
[1mStep[0m  [20/42], [94mLoss[0m : 2.59030
[1mStep[0m  [24/42], [94mLoss[0m : 2.41352
[1mStep[0m  [28/42], [94mLoss[0m : 2.30885
[1mStep[0m  [32/42], [94mLoss[0m : 2.45284
[1mStep[0m  [36/42], [94mLoss[0m : 2.46120
[1mStep[0m  [40/42], [94mLoss[0m : 2.63228

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64986
[1mStep[0m  [4/42], [94mLoss[0m : 2.56066
[1mStep[0m  [8/42], [94mLoss[0m : 2.68241
[1mStep[0m  [12/42], [94mLoss[0m : 2.73860
[1mStep[0m  [16/42], [94mLoss[0m : 2.44985
[1mStep[0m  [20/42], [94mLoss[0m : 2.48645
[1mStep[0m  [24/42], [94mLoss[0m : 2.54438
[1mStep[0m  [28/42], [94mLoss[0m : 2.55020
[1mStep[0m  [32/42], [94mLoss[0m : 2.52491
[1mStep[0m  [36/42], [94mLoss[0m : 2.41860
[1mStep[0m  [40/42], [94mLoss[0m : 2.36250

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51040
[1mStep[0m  [4/42], [94mLoss[0m : 2.49726
[1mStep[0m  [8/42], [94mLoss[0m : 2.37294
[1mStep[0m  [12/42], [94mLoss[0m : 2.65303
[1mStep[0m  [16/42], [94mLoss[0m : 2.21951
[1mStep[0m  [20/42], [94mLoss[0m : 2.55009
[1mStep[0m  [24/42], [94mLoss[0m : 2.51009
[1mStep[0m  [28/42], [94mLoss[0m : 2.60491
[1mStep[0m  [32/42], [94mLoss[0m : 2.30164
[1mStep[0m  [36/42], [94mLoss[0m : 2.65995
[1mStep[0m  [40/42], [94mLoss[0m : 2.60705

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29623
[1mStep[0m  [4/42], [94mLoss[0m : 2.51154
[1mStep[0m  [8/42], [94mLoss[0m : 2.36326
[1mStep[0m  [12/42], [94mLoss[0m : 2.35886
[1mStep[0m  [16/42], [94mLoss[0m : 2.45482
[1mStep[0m  [20/42], [94mLoss[0m : 2.53956
[1mStep[0m  [24/42], [94mLoss[0m : 2.44400
[1mStep[0m  [28/42], [94mLoss[0m : 2.19195
[1mStep[0m  [32/42], [94mLoss[0m : 2.48119
[1mStep[0m  [36/42], [94mLoss[0m : 2.61866
[1mStep[0m  [40/42], [94mLoss[0m : 2.51678

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57851
[1mStep[0m  [4/42], [94mLoss[0m : 2.47255
[1mStep[0m  [8/42], [94mLoss[0m : 2.48030
[1mStep[0m  [12/42], [94mLoss[0m : 2.39255
[1mStep[0m  [16/42], [94mLoss[0m : 2.61286
[1mStep[0m  [20/42], [94mLoss[0m : 2.36657
[1mStep[0m  [24/42], [94mLoss[0m : 2.49452
[1mStep[0m  [28/42], [94mLoss[0m : 2.23704
[1mStep[0m  [32/42], [94mLoss[0m : 2.53540
[1mStep[0m  [36/42], [94mLoss[0m : 2.53501
[1mStep[0m  [40/42], [94mLoss[0m : 2.51397

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29866
[1mStep[0m  [4/42], [94mLoss[0m : 2.59004
[1mStep[0m  [8/42], [94mLoss[0m : 2.48167
[1mStep[0m  [12/42], [94mLoss[0m : 2.25829
[1mStep[0m  [16/42], [94mLoss[0m : 2.81387
[1mStep[0m  [20/42], [94mLoss[0m : 2.72059
[1mStep[0m  [24/42], [94mLoss[0m : 2.50277
[1mStep[0m  [28/42], [94mLoss[0m : 2.54954
[1mStep[0m  [32/42], [94mLoss[0m : 2.44885
[1mStep[0m  [36/42], [94mLoss[0m : 2.70734
[1mStep[0m  [40/42], [94mLoss[0m : 2.34404

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35147
[1mStep[0m  [4/42], [94mLoss[0m : 2.56680
[1mStep[0m  [8/42], [94mLoss[0m : 2.31965
[1mStep[0m  [12/42], [94mLoss[0m : 2.36580
[1mStep[0m  [16/42], [94mLoss[0m : 2.61314
[1mStep[0m  [20/42], [94mLoss[0m : 2.45805
[1mStep[0m  [24/42], [94mLoss[0m : 2.20123
[1mStep[0m  [28/42], [94mLoss[0m : 2.67919
[1mStep[0m  [32/42], [94mLoss[0m : 2.43493
[1mStep[0m  [36/42], [94mLoss[0m : 2.35703
[1mStep[0m  [40/42], [94mLoss[0m : 2.45836

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53762
[1mStep[0m  [4/42], [94mLoss[0m : 2.66141
[1mStep[0m  [8/42], [94mLoss[0m : 2.31935
[1mStep[0m  [12/42], [94mLoss[0m : 2.61243
[1mStep[0m  [16/42], [94mLoss[0m : 2.60300
[1mStep[0m  [20/42], [94mLoss[0m : 2.40468
[1mStep[0m  [24/42], [94mLoss[0m : 2.47221
[1mStep[0m  [28/42], [94mLoss[0m : 2.54491
[1mStep[0m  [32/42], [94mLoss[0m : 2.62426
[1mStep[0m  [36/42], [94mLoss[0m : 2.34594
[1mStep[0m  [40/42], [94mLoss[0m : 2.69728

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40717
[1mStep[0m  [4/42], [94mLoss[0m : 2.45570
[1mStep[0m  [8/42], [94mLoss[0m : 2.49093
[1mStep[0m  [12/42], [94mLoss[0m : 2.47426
[1mStep[0m  [16/42], [94mLoss[0m : 2.67714
[1mStep[0m  [20/42], [94mLoss[0m : 2.30001
[1mStep[0m  [24/42], [94mLoss[0m : 2.60411
[1mStep[0m  [28/42], [94mLoss[0m : 2.40706
[1mStep[0m  [32/42], [94mLoss[0m : 2.65096
[1mStep[0m  [36/42], [94mLoss[0m : 2.45110
[1mStep[0m  [40/42], [94mLoss[0m : 2.79920

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45956
[1mStep[0m  [4/42], [94mLoss[0m : 2.61477
[1mStep[0m  [8/42], [94mLoss[0m : 2.55417
[1mStep[0m  [12/42], [94mLoss[0m : 2.44186
[1mStep[0m  [16/42], [94mLoss[0m : 2.38071
[1mStep[0m  [20/42], [94mLoss[0m : 2.41993
[1mStep[0m  [24/42], [94mLoss[0m : 2.44608
[1mStep[0m  [28/42], [94mLoss[0m : 2.34021
[1mStep[0m  [32/42], [94mLoss[0m : 2.51667
[1mStep[0m  [36/42], [94mLoss[0m : 2.24867
[1mStep[0m  [40/42], [94mLoss[0m : 2.43862

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35407
[1mStep[0m  [4/42], [94mLoss[0m : 2.55688
[1mStep[0m  [8/42], [94mLoss[0m : 2.60144
[1mStep[0m  [12/42], [94mLoss[0m : 2.38039
[1mStep[0m  [16/42], [94mLoss[0m : 2.32880
[1mStep[0m  [20/42], [94mLoss[0m : 2.47712
[1mStep[0m  [24/42], [94mLoss[0m : 2.49341
[1mStep[0m  [28/42], [94mLoss[0m : 2.35285
[1mStep[0m  [32/42], [94mLoss[0m : 2.35889
[1mStep[0m  [36/42], [94mLoss[0m : 2.55840
[1mStep[0m  [40/42], [94mLoss[0m : 2.82270

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38600
[1mStep[0m  [4/42], [94mLoss[0m : 2.21707
[1mStep[0m  [8/42], [94mLoss[0m : 2.58973
[1mStep[0m  [12/42], [94mLoss[0m : 2.29174
[1mStep[0m  [16/42], [94mLoss[0m : 2.62963
[1mStep[0m  [20/42], [94mLoss[0m : 2.40975
[1mStep[0m  [24/42], [94mLoss[0m : 2.56646
[1mStep[0m  [28/42], [94mLoss[0m : 2.56807
[1mStep[0m  [32/42], [94mLoss[0m : 2.50438
[1mStep[0m  [36/42], [94mLoss[0m : 2.53916
[1mStep[0m  [40/42], [94mLoss[0m : 2.44078

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49402
[1mStep[0m  [4/42], [94mLoss[0m : 2.50749
[1mStep[0m  [8/42], [94mLoss[0m : 2.47106
[1mStep[0m  [12/42], [94mLoss[0m : 2.39739
[1mStep[0m  [16/42], [94mLoss[0m : 2.34000
[1mStep[0m  [20/42], [94mLoss[0m : 2.46364
[1mStep[0m  [24/42], [94mLoss[0m : 2.49478
[1mStep[0m  [28/42], [94mLoss[0m : 2.47343
[1mStep[0m  [32/42], [94mLoss[0m : 2.55797
[1mStep[0m  [36/42], [94mLoss[0m : 2.30292
[1mStep[0m  [40/42], [94mLoss[0m : 2.29848

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38412
[1mStep[0m  [4/42], [94mLoss[0m : 2.19753
[1mStep[0m  [8/42], [94mLoss[0m : 2.57193
[1mStep[0m  [12/42], [94mLoss[0m : 2.29467
[1mStep[0m  [16/42], [94mLoss[0m : 2.61583
[1mStep[0m  [20/42], [94mLoss[0m : 2.34299
[1mStep[0m  [24/42], [94mLoss[0m : 2.33119
[1mStep[0m  [28/42], [94mLoss[0m : 2.43682
[1mStep[0m  [32/42], [94mLoss[0m : 2.46239
[1mStep[0m  [36/42], [94mLoss[0m : 2.37334
[1mStep[0m  [40/42], [94mLoss[0m : 2.57684

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.322753276143755
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.40583
[1mStep[0m  [4/42], [94mLoss[0m : 2.46976
[1mStep[0m  [8/42], [94mLoss[0m : 2.33153
[1mStep[0m  [12/42], [94mLoss[0m : 2.54924
[1mStep[0m  [16/42], [94mLoss[0m : 2.41170
[1mStep[0m  [20/42], [94mLoss[0m : 2.74121
[1mStep[0m  [24/42], [94mLoss[0m : 2.55698
[1mStep[0m  [28/42], [94mLoss[0m : 2.44193
[1mStep[0m  [32/42], [94mLoss[0m : 2.47861
[1mStep[0m  [36/42], [94mLoss[0m : 2.59826
[1mStep[0m  [40/42], [94mLoss[0m : 2.55859

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60508
[1mStep[0m  [4/42], [94mLoss[0m : 2.41879
[1mStep[0m  [8/42], [94mLoss[0m : 2.59407
[1mStep[0m  [12/42], [94mLoss[0m : 2.42308
[1mStep[0m  [16/42], [94mLoss[0m : 2.64990
[1mStep[0m  [20/42], [94mLoss[0m : 2.47333
[1mStep[0m  [24/42], [94mLoss[0m : 2.35014
[1mStep[0m  [28/42], [94mLoss[0m : 2.48833
[1mStep[0m  [32/42], [94mLoss[0m : 2.50007
[1mStep[0m  [36/42], [94mLoss[0m : 2.48379
[1mStep[0m  [40/42], [94mLoss[0m : 2.45796

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48114
[1mStep[0m  [4/42], [94mLoss[0m : 2.20739
[1mStep[0m  [8/42], [94mLoss[0m : 2.26999
[1mStep[0m  [12/42], [94mLoss[0m : 2.49021
[1mStep[0m  [16/42], [94mLoss[0m : 2.40220
[1mStep[0m  [20/42], [94mLoss[0m : 2.52043
[1mStep[0m  [24/42], [94mLoss[0m : 2.75955
[1mStep[0m  [28/42], [94mLoss[0m : 2.59998
[1mStep[0m  [32/42], [94mLoss[0m : 2.28797
[1mStep[0m  [36/42], [94mLoss[0m : 2.43762
[1mStep[0m  [40/42], [94mLoss[0m : 2.24350

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28505
[1mStep[0m  [4/42], [94mLoss[0m : 2.46753
[1mStep[0m  [8/42], [94mLoss[0m : 2.29766
[1mStep[0m  [12/42], [94mLoss[0m : 2.76337
[1mStep[0m  [16/42], [94mLoss[0m : 2.24485
[1mStep[0m  [20/42], [94mLoss[0m : 2.30407
[1mStep[0m  [24/42], [94mLoss[0m : 2.37055
[1mStep[0m  [28/42], [94mLoss[0m : 2.54975
[1mStep[0m  [32/42], [94mLoss[0m : 2.50320
[1mStep[0m  [36/42], [94mLoss[0m : 2.29983
[1mStep[0m  [40/42], [94mLoss[0m : 2.51519

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.311, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39676
[1mStep[0m  [4/42], [94mLoss[0m : 2.50483
[1mStep[0m  [8/42], [94mLoss[0m : 2.44246
[1mStep[0m  [12/42], [94mLoss[0m : 2.30094
[1mStep[0m  [16/42], [94mLoss[0m : 2.44199
[1mStep[0m  [20/42], [94mLoss[0m : 2.18020
[1mStep[0m  [24/42], [94mLoss[0m : 2.12712
[1mStep[0m  [28/42], [94mLoss[0m : 2.49479
[1mStep[0m  [32/42], [94mLoss[0m : 2.26035
[1mStep[0m  [36/42], [94mLoss[0m : 2.31745
[1mStep[0m  [40/42], [94mLoss[0m : 2.36590

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.310, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41925
[1mStep[0m  [4/42], [94mLoss[0m : 2.33837
[1mStep[0m  [8/42], [94mLoss[0m : 2.19931
[1mStep[0m  [12/42], [94mLoss[0m : 2.31803
[1mStep[0m  [16/42], [94mLoss[0m : 2.56511
[1mStep[0m  [20/42], [94mLoss[0m : 2.26982
[1mStep[0m  [24/42], [94mLoss[0m : 2.48005
[1mStep[0m  [28/42], [94mLoss[0m : 2.45688
[1mStep[0m  [32/42], [94mLoss[0m : 2.43760
[1mStep[0m  [36/42], [94mLoss[0m : 2.39939
[1mStep[0m  [40/42], [94mLoss[0m : 2.26496

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20114
[1mStep[0m  [4/42], [94mLoss[0m : 2.30327
[1mStep[0m  [8/42], [94mLoss[0m : 2.09498
[1mStep[0m  [12/42], [94mLoss[0m : 2.29606
[1mStep[0m  [16/42], [94mLoss[0m : 2.36054
[1mStep[0m  [20/42], [94mLoss[0m : 2.27146
[1mStep[0m  [24/42], [94mLoss[0m : 2.29399
[1mStep[0m  [28/42], [94mLoss[0m : 2.51419
[1mStep[0m  [32/42], [94mLoss[0m : 2.27058
[1mStep[0m  [36/42], [94mLoss[0m : 2.29019
[1mStep[0m  [40/42], [94mLoss[0m : 2.40336

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32357
[1mStep[0m  [4/42], [94mLoss[0m : 2.32386
[1mStep[0m  [8/42], [94mLoss[0m : 2.38217
[1mStep[0m  [12/42], [94mLoss[0m : 2.29418
[1mStep[0m  [16/42], [94mLoss[0m : 2.33679
[1mStep[0m  [20/42], [94mLoss[0m : 2.17911
[1mStep[0m  [24/42], [94mLoss[0m : 2.39979
[1mStep[0m  [28/42], [94mLoss[0m : 2.26651
[1mStep[0m  [32/42], [94mLoss[0m : 2.03201
[1mStep[0m  [36/42], [94mLoss[0m : 2.12601
[1mStep[0m  [40/42], [94mLoss[0m : 2.12713

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34764
[1mStep[0m  [4/42], [94mLoss[0m : 2.33686
[1mStep[0m  [8/42], [94mLoss[0m : 2.28378
[1mStep[0m  [12/42], [94mLoss[0m : 2.15804
[1mStep[0m  [16/42], [94mLoss[0m : 2.36449
[1mStep[0m  [20/42], [94mLoss[0m : 2.32192
[1mStep[0m  [24/42], [94mLoss[0m : 2.23489
[1mStep[0m  [28/42], [94mLoss[0m : 2.33752
[1mStep[0m  [32/42], [94mLoss[0m : 2.03021
[1mStep[0m  [36/42], [94mLoss[0m : 2.23264
[1mStep[0m  [40/42], [94mLoss[0m : 2.27174

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43621
[1mStep[0m  [4/42], [94mLoss[0m : 2.40969
[1mStep[0m  [8/42], [94mLoss[0m : 2.14023
[1mStep[0m  [12/42], [94mLoss[0m : 2.29657
[1mStep[0m  [16/42], [94mLoss[0m : 2.24699
[1mStep[0m  [20/42], [94mLoss[0m : 2.31043
[1mStep[0m  [24/42], [94mLoss[0m : 2.10556
[1mStep[0m  [28/42], [94mLoss[0m : 2.24371
[1mStep[0m  [32/42], [94mLoss[0m : 2.38569
[1mStep[0m  [36/42], [94mLoss[0m : 2.18885
[1mStep[0m  [40/42], [94mLoss[0m : 2.32422

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00043
[1mStep[0m  [4/42], [94mLoss[0m : 2.23186
[1mStep[0m  [8/42], [94mLoss[0m : 2.37837
[1mStep[0m  [12/42], [94mLoss[0m : 2.18197
[1mStep[0m  [16/42], [94mLoss[0m : 2.29420
[1mStep[0m  [20/42], [94mLoss[0m : 2.05784
[1mStep[0m  [24/42], [94mLoss[0m : 2.14055
[1mStep[0m  [28/42], [94mLoss[0m : 2.25082
[1mStep[0m  [32/42], [94mLoss[0m : 2.25434
[1mStep[0m  [36/42], [94mLoss[0m : 2.16482
[1mStep[0m  [40/42], [94mLoss[0m : 2.43841

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13765
[1mStep[0m  [4/42], [94mLoss[0m : 2.01750
[1mStep[0m  [8/42], [94mLoss[0m : 2.05852
[1mStep[0m  [12/42], [94mLoss[0m : 2.14378
[1mStep[0m  [16/42], [94mLoss[0m : 2.42983
[1mStep[0m  [20/42], [94mLoss[0m : 2.03564
[1mStep[0m  [24/42], [94mLoss[0m : 2.16445
[1mStep[0m  [28/42], [94mLoss[0m : 2.37647
[1mStep[0m  [32/42], [94mLoss[0m : 2.19580
[1mStep[0m  [36/42], [94mLoss[0m : 1.96756
[1mStep[0m  [40/42], [94mLoss[0m : 2.12941

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03633
[1mStep[0m  [4/42], [94mLoss[0m : 2.10134
[1mStep[0m  [8/42], [94mLoss[0m : 2.14434
[1mStep[0m  [12/42], [94mLoss[0m : 1.90112
[1mStep[0m  [16/42], [94mLoss[0m : 2.27652
[1mStep[0m  [20/42], [94mLoss[0m : 2.05128
[1mStep[0m  [24/42], [94mLoss[0m : 2.04429
[1mStep[0m  [28/42], [94mLoss[0m : 2.10354
[1mStep[0m  [32/42], [94mLoss[0m : 2.31724
[1mStep[0m  [36/42], [94mLoss[0m : 2.07254
[1mStep[0m  [40/42], [94mLoss[0m : 2.26234

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14091
[1mStep[0m  [4/42], [94mLoss[0m : 2.11595
[1mStep[0m  [8/42], [94mLoss[0m : 2.04500
[1mStep[0m  [12/42], [94mLoss[0m : 2.03334
[1mStep[0m  [16/42], [94mLoss[0m : 2.21478
[1mStep[0m  [20/42], [94mLoss[0m : 1.98022
[1mStep[0m  [24/42], [94mLoss[0m : 2.09105
[1mStep[0m  [28/42], [94mLoss[0m : 2.00319
[1mStep[0m  [32/42], [94mLoss[0m : 2.03477
[1mStep[0m  [36/42], [94mLoss[0m : 2.08766
[1mStep[0m  [40/42], [94mLoss[0m : 1.99731

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07998
[1mStep[0m  [4/42], [94mLoss[0m : 1.98849
[1mStep[0m  [8/42], [94mLoss[0m : 1.91205
[1mStep[0m  [12/42], [94mLoss[0m : 2.12839
[1mStep[0m  [16/42], [94mLoss[0m : 2.13422
[1mStep[0m  [20/42], [94mLoss[0m : 2.03608
[1mStep[0m  [24/42], [94mLoss[0m : 2.15996
[1mStep[0m  [28/42], [94mLoss[0m : 2.14755
[1mStep[0m  [32/42], [94mLoss[0m : 1.76512
[1mStep[0m  [36/42], [94mLoss[0m : 2.20553
[1mStep[0m  [40/42], [94mLoss[0m : 2.42435

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98593
[1mStep[0m  [4/42], [94mLoss[0m : 1.97159
[1mStep[0m  [8/42], [94mLoss[0m : 2.19280
[1mStep[0m  [12/42], [94mLoss[0m : 2.09942
[1mStep[0m  [16/42], [94mLoss[0m : 2.00318
[1mStep[0m  [20/42], [94mLoss[0m : 1.84843
[1mStep[0m  [24/42], [94mLoss[0m : 2.08095
[1mStep[0m  [28/42], [94mLoss[0m : 2.08034
[1mStep[0m  [32/42], [94mLoss[0m : 1.98740
[1mStep[0m  [36/42], [94mLoss[0m : 2.07318
[1mStep[0m  [40/42], [94mLoss[0m : 1.98330

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94018
[1mStep[0m  [4/42], [94mLoss[0m : 1.89176
[1mStep[0m  [8/42], [94mLoss[0m : 1.82844
[1mStep[0m  [12/42], [94mLoss[0m : 1.95670
[1mStep[0m  [16/42], [94mLoss[0m : 2.16292
[1mStep[0m  [20/42], [94mLoss[0m : 2.04469
[1mStep[0m  [24/42], [94mLoss[0m : 2.02813
[1mStep[0m  [28/42], [94mLoss[0m : 2.02711
[1mStep[0m  [32/42], [94mLoss[0m : 1.97774
[1mStep[0m  [36/42], [94mLoss[0m : 1.96127
[1mStep[0m  [40/42], [94mLoss[0m : 2.06166

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00449
[1mStep[0m  [4/42], [94mLoss[0m : 1.75166
[1mStep[0m  [8/42], [94mLoss[0m : 1.93042
[1mStep[0m  [12/42], [94mLoss[0m : 1.85331
[1mStep[0m  [16/42], [94mLoss[0m : 1.93129
[1mStep[0m  [20/42], [94mLoss[0m : 1.86013
[1mStep[0m  [24/42], [94mLoss[0m : 2.01680
[1mStep[0m  [28/42], [94mLoss[0m : 2.03282
[1mStep[0m  [32/42], [94mLoss[0m : 1.85981
[1mStep[0m  [36/42], [94mLoss[0m : 1.95673
[1mStep[0m  [40/42], [94mLoss[0m : 1.99105

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00872
[1mStep[0m  [4/42], [94mLoss[0m : 1.84180
[1mStep[0m  [8/42], [94mLoss[0m : 1.81968
[1mStep[0m  [12/42], [94mLoss[0m : 1.82726
[1mStep[0m  [16/42], [94mLoss[0m : 1.98306
[1mStep[0m  [20/42], [94mLoss[0m : 1.95494
[1mStep[0m  [24/42], [94mLoss[0m : 1.92765
[1mStep[0m  [28/42], [94mLoss[0m : 1.79865
[1mStep[0m  [32/42], [94mLoss[0m : 1.95926
[1mStep[0m  [36/42], [94mLoss[0m : 2.19148
[1mStep[0m  [40/42], [94mLoss[0m : 1.95246

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83590
[1mStep[0m  [4/42], [94mLoss[0m : 1.94730
[1mStep[0m  [8/42], [94mLoss[0m : 1.74413
[1mStep[0m  [12/42], [94mLoss[0m : 1.64641
[1mStep[0m  [16/42], [94mLoss[0m : 1.95077
[1mStep[0m  [20/42], [94mLoss[0m : 2.14196
[1mStep[0m  [24/42], [94mLoss[0m : 1.94174
[1mStep[0m  [28/42], [94mLoss[0m : 1.80053
[1mStep[0m  [32/42], [94mLoss[0m : 1.90436
[1mStep[0m  [36/42], [94mLoss[0m : 1.88174
[1mStep[0m  [40/42], [94mLoss[0m : 1.80804

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96292
[1mStep[0m  [4/42], [94mLoss[0m : 1.97814
[1mStep[0m  [8/42], [94mLoss[0m : 1.83303
[1mStep[0m  [12/42], [94mLoss[0m : 2.00327
[1mStep[0m  [16/42], [94mLoss[0m : 1.74432
[1mStep[0m  [20/42], [94mLoss[0m : 1.77670
[1mStep[0m  [24/42], [94mLoss[0m : 1.88571
[1mStep[0m  [28/42], [94mLoss[0m : 1.80292
[1mStep[0m  [32/42], [94mLoss[0m : 1.86983
[1mStep[0m  [36/42], [94mLoss[0m : 1.79341
[1mStep[0m  [40/42], [94mLoss[0m : 1.99890

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.545, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65881
[1mStep[0m  [4/42], [94mLoss[0m : 1.93774
[1mStep[0m  [8/42], [94mLoss[0m : 1.79708
[1mStep[0m  [12/42], [94mLoss[0m : 1.81756
[1mStep[0m  [16/42], [94mLoss[0m : 1.68446
[1mStep[0m  [20/42], [94mLoss[0m : 1.95342
[1mStep[0m  [24/42], [94mLoss[0m : 1.69960
[1mStep[0m  [28/42], [94mLoss[0m : 1.80523
[1mStep[0m  [32/42], [94mLoss[0m : 1.63596
[1mStep[0m  [36/42], [94mLoss[0m : 1.70997
[1mStep[0m  [40/42], [94mLoss[0m : 2.01124

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69029
[1mStep[0m  [4/42], [94mLoss[0m : 1.84634
[1mStep[0m  [8/42], [94mLoss[0m : 1.72407
[1mStep[0m  [12/42], [94mLoss[0m : 2.01105
[1mStep[0m  [16/42], [94mLoss[0m : 1.92774
[1mStep[0m  [20/42], [94mLoss[0m : 1.73077
[1mStep[0m  [24/42], [94mLoss[0m : 1.78857
[1mStep[0m  [28/42], [94mLoss[0m : 1.85269
[1mStep[0m  [32/42], [94mLoss[0m : 1.59630
[1mStep[0m  [36/42], [94mLoss[0m : 1.70469
[1mStep[0m  [40/42], [94mLoss[0m : 1.85034

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95275
[1mStep[0m  [4/42], [94mLoss[0m : 1.69148
[1mStep[0m  [8/42], [94mLoss[0m : 1.70209
[1mStep[0m  [12/42], [94mLoss[0m : 1.74781
[1mStep[0m  [16/42], [94mLoss[0m : 1.80362
[1mStep[0m  [20/42], [94mLoss[0m : 1.96559
[1mStep[0m  [24/42], [94mLoss[0m : 1.78066
[1mStep[0m  [28/42], [94mLoss[0m : 1.75452
[1mStep[0m  [32/42], [94mLoss[0m : 1.80725
[1mStep[0m  [36/42], [94mLoss[0m : 1.79747
[1mStep[0m  [40/42], [94mLoss[0m : 1.70456

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61329
[1mStep[0m  [4/42], [94mLoss[0m : 1.63106
[1mStep[0m  [8/42], [94mLoss[0m : 1.75325
[1mStep[0m  [12/42], [94mLoss[0m : 1.95383
[1mStep[0m  [16/42], [94mLoss[0m : 1.48274
[1mStep[0m  [20/42], [94mLoss[0m : 1.48102
[1mStep[0m  [24/42], [94mLoss[0m : 1.63932
[1mStep[0m  [28/42], [94mLoss[0m : 1.94165
[1mStep[0m  [32/42], [94mLoss[0m : 1.79367
[1mStep[0m  [36/42], [94mLoss[0m : 1.73992
[1mStep[0m  [40/42], [94mLoss[0m : 1.80292

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63791
[1mStep[0m  [4/42], [94mLoss[0m : 1.61364
[1mStep[0m  [8/42], [94mLoss[0m : 1.84772
[1mStep[0m  [12/42], [94mLoss[0m : 1.56606
[1mStep[0m  [16/42], [94mLoss[0m : 1.67179
[1mStep[0m  [20/42], [94mLoss[0m : 1.59952
[1mStep[0m  [24/42], [94mLoss[0m : 1.82892
[1mStep[0m  [28/42], [94mLoss[0m : 1.77052
[1mStep[0m  [32/42], [94mLoss[0m : 1.68365
[1mStep[0m  [36/42], [94mLoss[0m : 1.71871
[1mStep[0m  [40/42], [94mLoss[0m : 1.71217

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74561
[1mStep[0m  [4/42], [94mLoss[0m : 1.66832
[1mStep[0m  [8/42], [94mLoss[0m : 1.72716
[1mStep[0m  [12/42], [94mLoss[0m : 1.70099
[1mStep[0m  [16/42], [94mLoss[0m : 1.61192
[1mStep[0m  [20/42], [94mLoss[0m : 1.59475
[1mStep[0m  [24/42], [94mLoss[0m : 1.64599
[1mStep[0m  [28/42], [94mLoss[0m : 1.85231
[1mStep[0m  [32/42], [94mLoss[0m : 1.66280
[1mStep[0m  [36/42], [94mLoss[0m : 1.58400
[1mStep[0m  [40/42], [94mLoss[0m : 1.69359

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69049
[1mStep[0m  [4/42], [94mLoss[0m : 1.76680
[1mStep[0m  [8/42], [94mLoss[0m : 1.54349
[1mStep[0m  [12/42], [94mLoss[0m : 1.69633
[1mStep[0m  [16/42], [94mLoss[0m : 1.58057
[1mStep[0m  [20/42], [94mLoss[0m : 1.58363
[1mStep[0m  [24/42], [94mLoss[0m : 1.80140
[1mStep[0m  [28/42], [94mLoss[0m : 1.51173
[1mStep[0m  [32/42], [94mLoss[0m : 1.64721
[1mStep[0m  [36/42], [94mLoss[0m : 1.73290
[1mStep[0m  [40/42], [94mLoss[0m : 1.73664

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66188
[1mStep[0m  [4/42], [94mLoss[0m : 1.53929
[1mStep[0m  [8/42], [94mLoss[0m : 1.53267
[1mStep[0m  [12/42], [94mLoss[0m : 1.46375
[1mStep[0m  [16/42], [94mLoss[0m : 1.51365
[1mStep[0m  [20/42], [94mLoss[0m : 1.56710
[1mStep[0m  [24/42], [94mLoss[0m : 1.64008
[1mStep[0m  [28/42], [94mLoss[0m : 1.57987
[1mStep[0m  [32/42], [94mLoss[0m : 1.60327
[1mStep[0m  [36/42], [94mLoss[0m : 1.52605
[1mStep[0m  [40/42], [94mLoss[0m : 1.74285

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56138
[1mStep[0m  [4/42], [94mLoss[0m : 1.68518
[1mStep[0m  [8/42], [94mLoss[0m : 1.58685
[1mStep[0m  [12/42], [94mLoss[0m : 1.68530
[1mStep[0m  [16/42], [94mLoss[0m : 1.63018
[1mStep[0m  [20/42], [94mLoss[0m : 1.66962
[1mStep[0m  [24/42], [94mLoss[0m : 1.56823
[1mStep[0m  [28/42], [94mLoss[0m : 1.72015
[1mStep[0m  [32/42], [94mLoss[0m : 1.67787
[1mStep[0m  [36/42], [94mLoss[0m : 1.70994
[1mStep[0m  [40/42], [94mLoss[0m : 1.86219

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.580, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.4950001750673567
MAE score P1       2.322753
MAE score P2          2.495
loss               1.622681
learning_rate       0.00505
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.75051
[1mStep[0m  [4/42], [94mLoss[0m : 10.12032
[1mStep[0m  [8/42], [94mLoss[0m : 9.28846
[1mStep[0m  [12/42], [94mLoss[0m : 9.24853
[1mStep[0m  [16/42], [94mLoss[0m : 8.69460
[1mStep[0m  [20/42], [94mLoss[0m : 7.75941
[1mStep[0m  [24/42], [94mLoss[0m : 7.20350
[1mStep[0m  [28/42], [94mLoss[0m : 6.79019
[1mStep[0m  [32/42], [94mLoss[0m : 6.13156
[1mStep[0m  [36/42], [94mLoss[0m : 5.23010
[1mStep[0m  [40/42], [94mLoss[0m : 5.03080

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.793, [92mTest[0m: 10.773, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.06485
[1mStep[0m  [4/42], [94mLoss[0m : 4.44454
[1mStep[0m  [8/42], [94mLoss[0m : 4.22112
[1mStep[0m  [12/42], [94mLoss[0m : 4.13240
[1mStep[0m  [16/42], [94mLoss[0m : 3.99499
[1mStep[0m  [20/42], [94mLoss[0m : 3.47054
[1mStep[0m  [24/42], [94mLoss[0m : 3.11545
[1mStep[0m  [28/42], [94mLoss[0m : 3.34143
[1mStep[0m  [32/42], [94mLoss[0m : 2.92783
[1mStep[0m  [36/42], [94mLoss[0m : 2.72816
[1mStep[0m  [40/42], [94mLoss[0m : 2.94157

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.538, [92mTest[0m: 4.797, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92554
[1mStep[0m  [4/42], [94mLoss[0m : 2.75864
[1mStep[0m  [8/42], [94mLoss[0m : 3.04010
[1mStep[0m  [12/42], [94mLoss[0m : 3.01452
[1mStep[0m  [16/42], [94mLoss[0m : 2.69660
[1mStep[0m  [20/42], [94mLoss[0m : 2.39522
[1mStep[0m  [24/42], [94mLoss[0m : 2.47535
[1mStep[0m  [28/42], [94mLoss[0m : 2.57222
[1mStep[0m  [32/42], [94mLoss[0m : 2.49015
[1mStep[0m  [36/42], [94mLoss[0m : 2.74911
[1mStep[0m  [40/42], [94mLoss[0m : 2.78596

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.718, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67952
[1mStep[0m  [4/42], [94mLoss[0m : 2.52171
[1mStep[0m  [8/42], [94mLoss[0m : 2.67397
[1mStep[0m  [12/42], [94mLoss[0m : 2.45554
[1mStep[0m  [16/42], [94mLoss[0m : 2.61260
[1mStep[0m  [20/42], [94mLoss[0m : 2.49525
[1mStep[0m  [24/42], [94mLoss[0m : 2.59164
[1mStep[0m  [28/42], [94mLoss[0m : 2.46369
[1mStep[0m  [32/42], [94mLoss[0m : 2.61304
[1mStep[0m  [36/42], [94mLoss[0m : 2.50905
[1mStep[0m  [40/42], [94mLoss[0m : 2.60447

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67513
[1mStep[0m  [4/42], [94mLoss[0m : 2.78765
[1mStep[0m  [8/42], [94mLoss[0m : 2.73986
[1mStep[0m  [12/42], [94mLoss[0m : 2.71910
[1mStep[0m  [16/42], [94mLoss[0m : 2.62873
[1mStep[0m  [20/42], [94mLoss[0m : 2.51300
[1mStep[0m  [24/42], [94mLoss[0m : 2.55377
[1mStep[0m  [28/42], [94mLoss[0m : 2.52349
[1mStep[0m  [32/42], [94mLoss[0m : 2.40463
[1mStep[0m  [36/42], [94mLoss[0m : 2.56589
[1mStep[0m  [40/42], [94mLoss[0m : 2.78783

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57319
[1mStep[0m  [4/42], [94mLoss[0m : 2.55755
[1mStep[0m  [8/42], [94mLoss[0m : 2.59028
[1mStep[0m  [12/42], [94mLoss[0m : 2.65788
[1mStep[0m  [16/42], [94mLoss[0m : 2.41452
[1mStep[0m  [20/42], [94mLoss[0m : 2.53091
[1mStep[0m  [24/42], [94mLoss[0m : 2.52690
[1mStep[0m  [28/42], [94mLoss[0m : 2.25715
[1mStep[0m  [32/42], [94mLoss[0m : 2.49949
[1mStep[0m  [36/42], [94mLoss[0m : 2.78427
[1mStep[0m  [40/42], [94mLoss[0m : 2.48887

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53730
[1mStep[0m  [4/42], [94mLoss[0m : 2.62564
[1mStep[0m  [8/42], [94mLoss[0m : 2.54379
[1mStep[0m  [12/42], [94mLoss[0m : 2.49939
[1mStep[0m  [16/42], [94mLoss[0m : 2.57125
[1mStep[0m  [20/42], [94mLoss[0m : 2.63662
[1mStep[0m  [24/42], [94mLoss[0m : 2.46391
[1mStep[0m  [28/42], [94mLoss[0m : 2.76909
[1mStep[0m  [32/42], [94mLoss[0m : 2.56532
[1mStep[0m  [36/42], [94mLoss[0m : 2.49174
[1mStep[0m  [40/42], [94mLoss[0m : 2.54229

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45337
[1mStep[0m  [4/42], [94mLoss[0m : 2.65246
[1mStep[0m  [8/42], [94mLoss[0m : 2.49924
[1mStep[0m  [12/42], [94mLoss[0m : 2.60145
[1mStep[0m  [16/42], [94mLoss[0m : 2.52142
[1mStep[0m  [20/42], [94mLoss[0m : 2.60464
[1mStep[0m  [24/42], [94mLoss[0m : 2.50580
[1mStep[0m  [28/42], [94mLoss[0m : 2.66009
[1mStep[0m  [32/42], [94mLoss[0m : 2.52676
[1mStep[0m  [36/42], [94mLoss[0m : 2.59093
[1mStep[0m  [40/42], [94mLoss[0m : 2.66428

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55382
[1mStep[0m  [4/42], [94mLoss[0m : 2.46477
[1mStep[0m  [8/42], [94mLoss[0m : 2.57649
[1mStep[0m  [12/42], [94mLoss[0m : 2.55332
[1mStep[0m  [16/42], [94mLoss[0m : 2.54813
[1mStep[0m  [20/42], [94mLoss[0m : 2.55792
[1mStep[0m  [24/42], [94mLoss[0m : 2.60104
[1mStep[0m  [28/42], [94mLoss[0m : 2.61009
[1mStep[0m  [32/42], [94mLoss[0m : 2.59341
[1mStep[0m  [36/42], [94mLoss[0m : 2.48354
[1mStep[0m  [40/42], [94mLoss[0m : 2.58451

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50525
[1mStep[0m  [4/42], [94mLoss[0m : 2.51697
[1mStep[0m  [8/42], [94mLoss[0m : 2.60531
[1mStep[0m  [12/42], [94mLoss[0m : 2.65225
[1mStep[0m  [16/42], [94mLoss[0m : 2.37681
[1mStep[0m  [20/42], [94mLoss[0m : 2.55673
[1mStep[0m  [24/42], [94mLoss[0m : 2.44943
[1mStep[0m  [28/42], [94mLoss[0m : 2.54953
[1mStep[0m  [32/42], [94mLoss[0m : 2.56511
[1mStep[0m  [36/42], [94mLoss[0m : 2.56555
[1mStep[0m  [40/42], [94mLoss[0m : 2.74981

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74262
[1mStep[0m  [4/42], [94mLoss[0m : 2.86321
[1mStep[0m  [8/42], [94mLoss[0m : 2.44905
[1mStep[0m  [12/42], [94mLoss[0m : 2.54698
[1mStep[0m  [16/42], [94mLoss[0m : 2.30787
[1mStep[0m  [20/42], [94mLoss[0m : 2.59406
[1mStep[0m  [24/42], [94mLoss[0m : 2.54386
[1mStep[0m  [28/42], [94mLoss[0m : 2.33718
[1mStep[0m  [32/42], [94mLoss[0m : 2.63943
[1mStep[0m  [36/42], [94mLoss[0m : 2.38187
[1mStep[0m  [40/42], [94mLoss[0m : 2.41804

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50222
[1mStep[0m  [4/42], [94mLoss[0m : 2.51499
[1mStep[0m  [8/42], [94mLoss[0m : 2.50206
[1mStep[0m  [12/42], [94mLoss[0m : 2.51915
[1mStep[0m  [16/42], [94mLoss[0m : 2.63182
[1mStep[0m  [20/42], [94mLoss[0m : 2.46384
[1mStep[0m  [24/42], [94mLoss[0m : 2.51864
[1mStep[0m  [28/42], [94mLoss[0m : 2.50313
[1mStep[0m  [32/42], [94mLoss[0m : 2.36358
[1mStep[0m  [36/42], [94mLoss[0m : 2.46253
[1mStep[0m  [40/42], [94mLoss[0m : 2.49036

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47225
[1mStep[0m  [4/42], [94mLoss[0m : 2.48779
[1mStep[0m  [8/42], [94mLoss[0m : 2.40381
[1mStep[0m  [12/42], [94mLoss[0m : 2.66678
[1mStep[0m  [16/42], [94mLoss[0m : 2.85305
[1mStep[0m  [20/42], [94mLoss[0m : 2.37740
[1mStep[0m  [24/42], [94mLoss[0m : 2.45685
[1mStep[0m  [28/42], [94mLoss[0m : 2.28392
[1mStep[0m  [32/42], [94mLoss[0m : 2.58632
[1mStep[0m  [36/42], [94mLoss[0m : 2.50754
[1mStep[0m  [40/42], [94mLoss[0m : 2.32127

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53432
[1mStep[0m  [4/42], [94mLoss[0m : 2.52119
[1mStep[0m  [8/42], [94mLoss[0m : 2.34180
[1mStep[0m  [12/42], [94mLoss[0m : 2.42289
[1mStep[0m  [16/42], [94mLoss[0m : 2.62457
[1mStep[0m  [20/42], [94mLoss[0m : 2.57852
[1mStep[0m  [24/42], [94mLoss[0m : 2.57966
[1mStep[0m  [28/42], [94mLoss[0m : 2.79096
[1mStep[0m  [32/42], [94mLoss[0m : 2.42701
[1mStep[0m  [36/42], [94mLoss[0m : 2.59729
[1mStep[0m  [40/42], [94mLoss[0m : 2.41898

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49633
[1mStep[0m  [4/42], [94mLoss[0m : 2.58744
[1mStep[0m  [8/42], [94mLoss[0m : 2.46640
[1mStep[0m  [12/42], [94mLoss[0m : 2.50039
[1mStep[0m  [16/42], [94mLoss[0m : 2.28446
[1mStep[0m  [20/42], [94mLoss[0m : 2.23367
[1mStep[0m  [24/42], [94mLoss[0m : 2.40403
[1mStep[0m  [28/42], [94mLoss[0m : 2.50079
[1mStep[0m  [32/42], [94mLoss[0m : 2.60282
[1mStep[0m  [36/42], [94mLoss[0m : 2.54400
[1mStep[0m  [40/42], [94mLoss[0m : 2.32398

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44531
[1mStep[0m  [4/42], [94mLoss[0m : 2.38283
[1mStep[0m  [8/42], [94mLoss[0m : 2.30408
[1mStep[0m  [12/42], [94mLoss[0m : 2.50581
[1mStep[0m  [16/42], [94mLoss[0m : 2.46137
[1mStep[0m  [20/42], [94mLoss[0m : 2.47270
[1mStep[0m  [24/42], [94mLoss[0m : 2.56314
[1mStep[0m  [28/42], [94mLoss[0m : 2.59335
[1mStep[0m  [32/42], [94mLoss[0m : 2.41296
[1mStep[0m  [36/42], [94mLoss[0m : 2.32040
[1mStep[0m  [40/42], [94mLoss[0m : 2.36780

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64103
[1mStep[0m  [4/42], [94mLoss[0m : 2.60920
[1mStep[0m  [8/42], [94mLoss[0m : 2.49246
[1mStep[0m  [12/42], [94mLoss[0m : 2.58663
[1mStep[0m  [16/42], [94mLoss[0m : 2.35287
[1mStep[0m  [20/42], [94mLoss[0m : 2.62377
[1mStep[0m  [24/42], [94mLoss[0m : 2.57015
[1mStep[0m  [28/42], [94mLoss[0m : 2.30756
[1mStep[0m  [32/42], [94mLoss[0m : 2.33365
[1mStep[0m  [36/42], [94mLoss[0m : 2.55638
[1mStep[0m  [40/42], [94mLoss[0m : 2.43443

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57516
[1mStep[0m  [4/42], [94mLoss[0m : 2.50564
[1mStep[0m  [8/42], [94mLoss[0m : 2.56106
[1mStep[0m  [12/42], [94mLoss[0m : 2.50539
[1mStep[0m  [16/42], [94mLoss[0m : 2.40775
[1mStep[0m  [20/42], [94mLoss[0m : 2.48490
[1mStep[0m  [24/42], [94mLoss[0m : 2.47077
[1mStep[0m  [28/42], [94mLoss[0m : 2.31304
[1mStep[0m  [32/42], [94mLoss[0m : 2.66242
[1mStep[0m  [36/42], [94mLoss[0m : 2.49769
[1mStep[0m  [40/42], [94mLoss[0m : 2.69307

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50453
[1mStep[0m  [4/42], [94mLoss[0m : 2.46683
[1mStep[0m  [8/42], [94mLoss[0m : 2.65961
[1mStep[0m  [12/42], [94mLoss[0m : 2.54818
[1mStep[0m  [16/42], [94mLoss[0m : 2.52543
[1mStep[0m  [20/42], [94mLoss[0m : 2.42140
[1mStep[0m  [24/42], [94mLoss[0m : 2.37888
[1mStep[0m  [28/42], [94mLoss[0m : 2.54149
[1mStep[0m  [32/42], [94mLoss[0m : 2.59982
[1mStep[0m  [36/42], [94mLoss[0m : 2.45382
[1mStep[0m  [40/42], [94mLoss[0m : 2.54134

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61131
[1mStep[0m  [4/42], [94mLoss[0m : 2.85286
[1mStep[0m  [8/42], [94mLoss[0m : 2.35240
[1mStep[0m  [12/42], [94mLoss[0m : 2.48716
[1mStep[0m  [16/42], [94mLoss[0m : 2.63737
[1mStep[0m  [20/42], [94mLoss[0m : 2.38778
[1mStep[0m  [24/42], [94mLoss[0m : 2.63973
[1mStep[0m  [28/42], [94mLoss[0m : 2.40789
[1mStep[0m  [32/42], [94mLoss[0m : 2.44454
[1mStep[0m  [36/42], [94mLoss[0m : 2.53322
[1mStep[0m  [40/42], [94mLoss[0m : 2.46615

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63912
[1mStep[0m  [4/42], [94mLoss[0m : 2.79512
[1mStep[0m  [8/42], [94mLoss[0m : 2.46498
[1mStep[0m  [12/42], [94mLoss[0m : 2.50230
[1mStep[0m  [16/42], [94mLoss[0m : 2.48341
[1mStep[0m  [20/42], [94mLoss[0m : 2.58556
[1mStep[0m  [24/42], [94mLoss[0m : 2.30394
[1mStep[0m  [28/42], [94mLoss[0m : 2.59114
[1mStep[0m  [32/42], [94mLoss[0m : 2.38312
[1mStep[0m  [36/42], [94mLoss[0m : 2.52066
[1mStep[0m  [40/42], [94mLoss[0m : 2.44594

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46788
[1mStep[0m  [4/42], [94mLoss[0m : 2.28247
[1mStep[0m  [8/42], [94mLoss[0m : 2.46403
[1mStep[0m  [12/42], [94mLoss[0m : 2.42757
[1mStep[0m  [16/42], [94mLoss[0m : 2.60629
[1mStep[0m  [20/42], [94mLoss[0m : 2.48249
[1mStep[0m  [24/42], [94mLoss[0m : 2.31712
[1mStep[0m  [28/42], [94mLoss[0m : 2.64908
[1mStep[0m  [32/42], [94mLoss[0m : 2.47777
[1mStep[0m  [36/42], [94mLoss[0m : 2.53780
[1mStep[0m  [40/42], [94mLoss[0m : 2.42260

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56118
[1mStep[0m  [4/42], [94mLoss[0m : 2.60494
[1mStep[0m  [8/42], [94mLoss[0m : 2.64996
[1mStep[0m  [12/42], [94mLoss[0m : 2.51986
[1mStep[0m  [16/42], [94mLoss[0m : 2.57515
[1mStep[0m  [20/42], [94mLoss[0m : 2.42702
[1mStep[0m  [24/42], [94mLoss[0m : 2.38118
[1mStep[0m  [28/42], [94mLoss[0m : 2.61041
[1mStep[0m  [32/42], [94mLoss[0m : 2.27651
[1mStep[0m  [36/42], [94mLoss[0m : 2.50109
[1mStep[0m  [40/42], [94mLoss[0m : 2.35656

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65299
[1mStep[0m  [4/42], [94mLoss[0m : 2.58194
[1mStep[0m  [8/42], [94mLoss[0m : 2.56835
[1mStep[0m  [12/42], [94mLoss[0m : 2.55835
[1mStep[0m  [16/42], [94mLoss[0m : 2.21118
[1mStep[0m  [20/42], [94mLoss[0m : 2.44956
[1mStep[0m  [24/42], [94mLoss[0m : 2.61230
[1mStep[0m  [28/42], [94mLoss[0m : 2.54112
[1mStep[0m  [32/42], [94mLoss[0m : 2.37463
[1mStep[0m  [36/42], [94mLoss[0m : 2.41466
[1mStep[0m  [40/42], [94mLoss[0m : 2.60610

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45001
[1mStep[0m  [4/42], [94mLoss[0m : 2.39596
[1mStep[0m  [8/42], [94mLoss[0m : 2.55243
[1mStep[0m  [12/42], [94mLoss[0m : 2.44437
[1mStep[0m  [16/42], [94mLoss[0m : 2.32488
[1mStep[0m  [20/42], [94mLoss[0m : 2.48032
[1mStep[0m  [24/42], [94mLoss[0m : 2.33564
[1mStep[0m  [28/42], [94mLoss[0m : 2.41667
[1mStep[0m  [32/42], [94mLoss[0m : 2.53950
[1mStep[0m  [36/42], [94mLoss[0m : 2.67166
[1mStep[0m  [40/42], [94mLoss[0m : 2.46079

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48620
[1mStep[0m  [4/42], [94mLoss[0m : 2.63669
[1mStep[0m  [8/42], [94mLoss[0m : 2.40241
[1mStep[0m  [12/42], [94mLoss[0m : 2.48122
[1mStep[0m  [16/42], [94mLoss[0m : 2.49367
[1mStep[0m  [20/42], [94mLoss[0m : 2.35896
[1mStep[0m  [24/42], [94mLoss[0m : 2.41541
[1mStep[0m  [28/42], [94mLoss[0m : 2.58160
[1mStep[0m  [32/42], [94mLoss[0m : 2.72471
[1mStep[0m  [36/42], [94mLoss[0m : 2.48197
[1mStep[0m  [40/42], [94mLoss[0m : 2.69833

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49648
[1mStep[0m  [4/42], [94mLoss[0m : 2.58660
[1mStep[0m  [8/42], [94mLoss[0m : 2.45166
[1mStep[0m  [12/42], [94mLoss[0m : 2.44695
[1mStep[0m  [16/42], [94mLoss[0m : 2.42496
[1mStep[0m  [20/42], [94mLoss[0m : 2.35187
[1mStep[0m  [24/42], [94mLoss[0m : 2.45602
[1mStep[0m  [28/42], [94mLoss[0m : 2.52307
[1mStep[0m  [32/42], [94mLoss[0m : 2.60175
[1mStep[0m  [36/42], [94mLoss[0m : 2.19816
[1mStep[0m  [40/42], [94mLoss[0m : 2.63758

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30911
[1mStep[0m  [4/42], [94mLoss[0m : 2.54310
[1mStep[0m  [8/42], [94mLoss[0m : 2.41277
[1mStep[0m  [12/42], [94mLoss[0m : 2.49702
[1mStep[0m  [16/42], [94mLoss[0m : 2.41089
[1mStep[0m  [20/42], [94mLoss[0m : 2.76019
[1mStep[0m  [24/42], [94mLoss[0m : 2.40982
[1mStep[0m  [28/42], [94mLoss[0m : 2.42025
[1mStep[0m  [32/42], [94mLoss[0m : 2.49521
[1mStep[0m  [36/42], [94mLoss[0m : 2.50032
[1mStep[0m  [40/42], [94mLoss[0m : 2.58875

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49735
[1mStep[0m  [4/42], [94mLoss[0m : 2.76468
[1mStep[0m  [8/42], [94mLoss[0m : 2.31514
[1mStep[0m  [12/42], [94mLoss[0m : 2.35426
[1mStep[0m  [16/42], [94mLoss[0m : 2.56282
[1mStep[0m  [20/42], [94mLoss[0m : 2.43360
[1mStep[0m  [24/42], [94mLoss[0m : 2.68534
[1mStep[0m  [28/42], [94mLoss[0m : 2.60696
[1mStep[0m  [32/42], [94mLoss[0m : 2.55157
[1mStep[0m  [36/42], [94mLoss[0m : 2.67746
[1mStep[0m  [40/42], [94mLoss[0m : 2.53193

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35343
[1mStep[0m  [4/42], [94mLoss[0m : 2.53671
[1mStep[0m  [8/42], [94mLoss[0m : 2.37555
[1mStep[0m  [12/42], [94mLoss[0m : 2.53081
[1mStep[0m  [16/42], [94mLoss[0m : 2.53438
[1mStep[0m  [20/42], [94mLoss[0m : 2.54068
[1mStep[0m  [24/42], [94mLoss[0m : 2.40748
[1mStep[0m  [28/42], [94mLoss[0m : 2.69258
[1mStep[0m  [32/42], [94mLoss[0m : 2.28721
[1mStep[0m  [36/42], [94mLoss[0m : 2.40256
[1mStep[0m  [40/42], [94mLoss[0m : 2.55213

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.329
====================================

Phase 1 - Evaluation MAE:  2.3294505902699063
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.43273
[1mStep[0m  [4/42], [94mLoss[0m : 2.53706
[1mStep[0m  [8/42], [94mLoss[0m : 2.55348
[1mStep[0m  [12/42], [94mLoss[0m : 2.35579
[1mStep[0m  [16/42], [94mLoss[0m : 2.46081
[1mStep[0m  [20/42], [94mLoss[0m : 2.31868
[1mStep[0m  [24/42], [94mLoss[0m : 2.50471
[1mStep[0m  [28/42], [94mLoss[0m : 2.42593
[1mStep[0m  [32/42], [94mLoss[0m : 2.47439
[1mStep[0m  [36/42], [94mLoss[0m : 2.57860
[1mStep[0m  [40/42], [94mLoss[0m : 2.53417

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27484
[1mStep[0m  [4/42], [94mLoss[0m : 2.47151
[1mStep[0m  [8/42], [94mLoss[0m : 2.57499
[1mStep[0m  [12/42], [94mLoss[0m : 2.50644
[1mStep[0m  [16/42], [94mLoss[0m : 2.51477
[1mStep[0m  [20/42], [94mLoss[0m : 2.67890
[1mStep[0m  [24/42], [94mLoss[0m : 2.31948
[1mStep[0m  [28/42], [94mLoss[0m : 2.17588
[1mStep[0m  [32/42], [94mLoss[0m : 2.46759
[1mStep[0m  [36/42], [94mLoss[0m : 2.43464
[1mStep[0m  [40/42], [94mLoss[0m : 2.55627

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44849
[1mStep[0m  [4/42], [94mLoss[0m : 2.51382
[1mStep[0m  [8/42], [94mLoss[0m : 2.36851
[1mStep[0m  [12/42], [94mLoss[0m : 2.44090
[1mStep[0m  [16/42], [94mLoss[0m : 2.39991
[1mStep[0m  [20/42], [94mLoss[0m : 2.43072
[1mStep[0m  [24/42], [94mLoss[0m : 2.43824
[1mStep[0m  [28/42], [94mLoss[0m : 2.57676
[1mStep[0m  [32/42], [94mLoss[0m : 2.31681
[1mStep[0m  [36/42], [94mLoss[0m : 2.52618
[1mStep[0m  [40/42], [94mLoss[0m : 2.58098

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29631
[1mStep[0m  [4/42], [94mLoss[0m : 2.22734
[1mStep[0m  [8/42], [94mLoss[0m : 2.35656
[1mStep[0m  [12/42], [94mLoss[0m : 2.32193
[1mStep[0m  [16/42], [94mLoss[0m : 2.61178
[1mStep[0m  [20/42], [94mLoss[0m : 2.37333
[1mStep[0m  [24/42], [94mLoss[0m : 2.38402
[1mStep[0m  [28/42], [94mLoss[0m : 2.34543
[1mStep[0m  [32/42], [94mLoss[0m : 2.57722
[1mStep[0m  [36/42], [94mLoss[0m : 2.55553
[1mStep[0m  [40/42], [94mLoss[0m : 2.48388

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63524
[1mStep[0m  [4/42], [94mLoss[0m : 2.47817
[1mStep[0m  [8/42], [94mLoss[0m : 2.12643
[1mStep[0m  [12/42], [94mLoss[0m : 2.40200
[1mStep[0m  [16/42], [94mLoss[0m : 2.39731
[1mStep[0m  [20/42], [94mLoss[0m : 2.41730
[1mStep[0m  [24/42], [94mLoss[0m : 2.26942
[1mStep[0m  [28/42], [94mLoss[0m : 2.49251
[1mStep[0m  [32/42], [94mLoss[0m : 2.51072
[1mStep[0m  [36/42], [94mLoss[0m : 2.18570
[1mStep[0m  [40/42], [94mLoss[0m : 2.45013

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56586
[1mStep[0m  [4/42], [94mLoss[0m : 2.55555
[1mStep[0m  [8/42], [94mLoss[0m : 2.41950
[1mStep[0m  [12/42], [94mLoss[0m : 2.44489
[1mStep[0m  [16/42], [94mLoss[0m : 2.25102
[1mStep[0m  [20/42], [94mLoss[0m : 2.46365
[1mStep[0m  [24/42], [94mLoss[0m : 2.22880
[1mStep[0m  [28/42], [94mLoss[0m : 2.32639
[1mStep[0m  [32/42], [94mLoss[0m : 2.56838
[1mStep[0m  [36/42], [94mLoss[0m : 2.37905
[1mStep[0m  [40/42], [94mLoss[0m : 2.30014

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53557
[1mStep[0m  [4/42], [94mLoss[0m : 2.37931
[1mStep[0m  [8/42], [94mLoss[0m : 2.34202
[1mStep[0m  [12/42], [94mLoss[0m : 2.39154
[1mStep[0m  [16/42], [94mLoss[0m : 2.36336
[1mStep[0m  [20/42], [94mLoss[0m : 2.30955
[1mStep[0m  [24/42], [94mLoss[0m : 2.78574
[1mStep[0m  [28/42], [94mLoss[0m : 2.29360
[1mStep[0m  [32/42], [94mLoss[0m : 2.30938
[1mStep[0m  [36/42], [94mLoss[0m : 2.26303
[1mStep[0m  [40/42], [94mLoss[0m : 2.31102

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27633
[1mStep[0m  [4/42], [94mLoss[0m : 2.39743
[1mStep[0m  [8/42], [94mLoss[0m : 2.61654
[1mStep[0m  [12/42], [94mLoss[0m : 2.25103
[1mStep[0m  [16/42], [94mLoss[0m : 2.61641
[1mStep[0m  [20/42], [94mLoss[0m : 2.42411
[1mStep[0m  [24/42], [94mLoss[0m : 2.35234
[1mStep[0m  [28/42], [94mLoss[0m : 2.22080
[1mStep[0m  [32/42], [94mLoss[0m : 2.37220
[1mStep[0m  [36/42], [94mLoss[0m : 2.27463
[1mStep[0m  [40/42], [94mLoss[0m : 2.39123

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37218
[1mStep[0m  [4/42], [94mLoss[0m : 2.42230
[1mStep[0m  [8/42], [94mLoss[0m : 2.29780
[1mStep[0m  [12/42], [94mLoss[0m : 2.42566
[1mStep[0m  [16/42], [94mLoss[0m : 2.39829
[1mStep[0m  [20/42], [94mLoss[0m : 2.30110
[1mStep[0m  [24/42], [94mLoss[0m : 2.21549
[1mStep[0m  [28/42], [94mLoss[0m : 2.45166
[1mStep[0m  [32/42], [94mLoss[0m : 2.24401
[1mStep[0m  [36/42], [94mLoss[0m : 2.43294
[1mStep[0m  [40/42], [94mLoss[0m : 2.42303

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39829
[1mStep[0m  [4/42], [94mLoss[0m : 2.42263
[1mStep[0m  [8/42], [94mLoss[0m : 2.27403
[1mStep[0m  [12/42], [94mLoss[0m : 2.48461
[1mStep[0m  [16/42], [94mLoss[0m : 2.74602
[1mStep[0m  [20/42], [94mLoss[0m : 2.30402
[1mStep[0m  [24/42], [94mLoss[0m : 2.44599
[1mStep[0m  [28/42], [94mLoss[0m : 2.24758
[1mStep[0m  [32/42], [94mLoss[0m : 2.57567
[1mStep[0m  [36/42], [94mLoss[0m : 2.41392
[1mStep[0m  [40/42], [94mLoss[0m : 2.31275

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19033
[1mStep[0m  [4/42], [94mLoss[0m : 2.27407
[1mStep[0m  [8/42], [94mLoss[0m : 2.38670
[1mStep[0m  [12/42], [94mLoss[0m : 2.10188
[1mStep[0m  [16/42], [94mLoss[0m : 2.37896
[1mStep[0m  [20/42], [94mLoss[0m : 2.46924
[1mStep[0m  [24/42], [94mLoss[0m : 2.38280
[1mStep[0m  [28/42], [94mLoss[0m : 2.24993
[1mStep[0m  [32/42], [94mLoss[0m : 2.60298
[1mStep[0m  [36/42], [94mLoss[0m : 2.42643
[1mStep[0m  [40/42], [94mLoss[0m : 2.44168

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25193
[1mStep[0m  [4/42], [94mLoss[0m : 2.32930
[1mStep[0m  [8/42], [94mLoss[0m : 2.30709
[1mStep[0m  [12/42], [94mLoss[0m : 2.31493
[1mStep[0m  [16/42], [94mLoss[0m : 2.26742
[1mStep[0m  [20/42], [94mLoss[0m : 2.06605
[1mStep[0m  [24/42], [94mLoss[0m : 2.46166
[1mStep[0m  [28/42], [94mLoss[0m : 2.49155
[1mStep[0m  [32/42], [94mLoss[0m : 2.47721
[1mStep[0m  [36/42], [94mLoss[0m : 2.34573
[1mStep[0m  [40/42], [94mLoss[0m : 2.67213

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21577
[1mStep[0m  [4/42], [94mLoss[0m : 2.31861
[1mStep[0m  [8/42], [94mLoss[0m : 2.61785
[1mStep[0m  [12/42], [94mLoss[0m : 2.14142
[1mStep[0m  [16/42], [94mLoss[0m : 2.24644
[1mStep[0m  [20/42], [94mLoss[0m : 2.39160
[1mStep[0m  [24/42], [94mLoss[0m : 2.16520
[1mStep[0m  [28/42], [94mLoss[0m : 2.33912
[1mStep[0m  [32/42], [94mLoss[0m : 2.27277
[1mStep[0m  [36/42], [94mLoss[0m : 2.37558
[1mStep[0m  [40/42], [94mLoss[0m : 2.15698

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30414
[1mStep[0m  [4/42], [94mLoss[0m : 2.34201
[1mStep[0m  [8/42], [94mLoss[0m : 2.42618
[1mStep[0m  [12/42], [94mLoss[0m : 2.45165
[1mStep[0m  [16/42], [94mLoss[0m : 2.38419
[1mStep[0m  [20/42], [94mLoss[0m : 2.38178
[1mStep[0m  [24/42], [94mLoss[0m : 2.22228
[1mStep[0m  [28/42], [94mLoss[0m : 2.29628
[1mStep[0m  [32/42], [94mLoss[0m : 2.28827
[1mStep[0m  [36/42], [94mLoss[0m : 2.15532
[1mStep[0m  [40/42], [94mLoss[0m : 2.29108

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21113
[1mStep[0m  [4/42], [94mLoss[0m : 2.42635
[1mStep[0m  [8/42], [94mLoss[0m : 2.22965
[1mStep[0m  [12/42], [94mLoss[0m : 2.29550
[1mStep[0m  [16/42], [94mLoss[0m : 2.27858
[1mStep[0m  [20/42], [94mLoss[0m : 2.46104
[1mStep[0m  [24/42], [94mLoss[0m : 2.26433
[1mStep[0m  [28/42], [94mLoss[0m : 2.13133
[1mStep[0m  [32/42], [94mLoss[0m : 2.25668
[1mStep[0m  [36/42], [94mLoss[0m : 2.26640
[1mStep[0m  [40/42], [94mLoss[0m : 2.23801

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24142
[1mStep[0m  [4/42], [94mLoss[0m : 2.39765
[1mStep[0m  [8/42], [94mLoss[0m : 2.28136
[1mStep[0m  [12/42], [94mLoss[0m : 2.37692
[1mStep[0m  [16/42], [94mLoss[0m : 2.20300
[1mStep[0m  [20/42], [94mLoss[0m : 2.21761
[1mStep[0m  [24/42], [94mLoss[0m : 2.31306
[1mStep[0m  [28/42], [94mLoss[0m : 2.23359
[1mStep[0m  [32/42], [94mLoss[0m : 2.28083
[1mStep[0m  [36/42], [94mLoss[0m : 2.18211
[1mStep[0m  [40/42], [94mLoss[0m : 2.22155

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05271
[1mStep[0m  [4/42], [94mLoss[0m : 2.17144
[1mStep[0m  [8/42], [94mLoss[0m : 2.33999
[1mStep[0m  [12/42], [94mLoss[0m : 2.08509
[1mStep[0m  [16/42], [94mLoss[0m : 2.36587
[1mStep[0m  [20/42], [94mLoss[0m : 2.01206
[1mStep[0m  [24/42], [94mLoss[0m : 2.14007
[1mStep[0m  [28/42], [94mLoss[0m : 2.17179
[1mStep[0m  [32/42], [94mLoss[0m : 2.16795
[1mStep[0m  [36/42], [94mLoss[0m : 2.23908
[1mStep[0m  [40/42], [94mLoss[0m : 2.31385

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25178
[1mStep[0m  [4/42], [94mLoss[0m : 2.37344
[1mStep[0m  [8/42], [94mLoss[0m : 2.35637
[1mStep[0m  [12/42], [94mLoss[0m : 2.44056
[1mStep[0m  [16/42], [94mLoss[0m : 2.27050
[1mStep[0m  [20/42], [94mLoss[0m : 2.30692
[1mStep[0m  [24/42], [94mLoss[0m : 2.23591
[1mStep[0m  [28/42], [94mLoss[0m : 2.15963
[1mStep[0m  [32/42], [94mLoss[0m : 2.16532
[1mStep[0m  [36/42], [94mLoss[0m : 2.19880
[1mStep[0m  [40/42], [94mLoss[0m : 2.43929

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05616
[1mStep[0m  [4/42], [94mLoss[0m : 2.20183
[1mStep[0m  [8/42], [94mLoss[0m : 2.15624
[1mStep[0m  [12/42], [94mLoss[0m : 2.20259
[1mStep[0m  [16/42], [94mLoss[0m : 2.12330
[1mStep[0m  [20/42], [94mLoss[0m : 2.15649
[1mStep[0m  [24/42], [94mLoss[0m : 2.18104
[1mStep[0m  [28/42], [94mLoss[0m : 2.19999
[1mStep[0m  [32/42], [94mLoss[0m : 2.27934
[1mStep[0m  [36/42], [94mLoss[0m : 2.11684
[1mStep[0m  [40/42], [94mLoss[0m : 2.16748

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21263
[1mStep[0m  [4/42], [94mLoss[0m : 2.17932
[1mStep[0m  [8/42], [94mLoss[0m : 2.22661
[1mStep[0m  [12/42], [94mLoss[0m : 1.89065
[1mStep[0m  [16/42], [94mLoss[0m : 2.27391
[1mStep[0m  [20/42], [94mLoss[0m : 1.99787
[1mStep[0m  [24/42], [94mLoss[0m : 2.02182
[1mStep[0m  [28/42], [94mLoss[0m : 2.31402
[1mStep[0m  [32/42], [94mLoss[0m : 2.00851
[1mStep[0m  [36/42], [94mLoss[0m : 2.02747
[1mStep[0m  [40/42], [94mLoss[0m : 2.24337

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13855
[1mStep[0m  [4/42], [94mLoss[0m : 1.93412
[1mStep[0m  [8/42], [94mLoss[0m : 2.10941
[1mStep[0m  [12/42], [94mLoss[0m : 1.89001
[1mStep[0m  [16/42], [94mLoss[0m : 2.06607
[1mStep[0m  [20/42], [94mLoss[0m : 2.12677
[1mStep[0m  [24/42], [94mLoss[0m : 2.07630
[1mStep[0m  [28/42], [94mLoss[0m : 2.05489
[1mStep[0m  [32/42], [94mLoss[0m : 2.17925
[1mStep[0m  [36/42], [94mLoss[0m : 2.20780
[1mStep[0m  [40/42], [94mLoss[0m : 2.25640

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09199
[1mStep[0m  [4/42], [94mLoss[0m : 2.18492
[1mStep[0m  [8/42], [94mLoss[0m : 2.18804
[1mStep[0m  [12/42], [94mLoss[0m : 2.31545
[1mStep[0m  [16/42], [94mLoss[0m : 2.02447
[1mStep[0m  [20/42], [94mLoss[0m : 2.14874
[1mStep[0m  [24/42], [94mLoss[0m : 2.28403
[1mStep[0m  [28/42], [94mLoss[0m : 2.08829
[1mStep[0m  [32/42], [94mLoss[0m : 2.09219
[1mStep[0m  [36/42], [94mLoss[0m : 1.97918
[1mStep[0m  [40/42], [94mLoss[0m : 2.29163

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.445, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21551
[1mStep[0m  [4/42], [94mLoss[0m : 2.04220
[1mStep[0m  [8/42], [94mLoss[0m : 1.96860
[1mStep[0m  [12/42], [94mLoss[0m : 1.96916
[1mStep[0m  [16/42], [94mLoss[0m : 2.18469
[1mStep[0m  [20/42], [94mLoss[0m : 2.20310
[1mStep[0m  [24/42], [94mLoss[0m : 2.03675
[1mStep[0m  [28/42], [94mLoss[0m : 2.03074
[1mStep[0m  [32/42], [94mLoss[0m : 2.13061
[1mStep[0m  [36/42], [94mLoss[0m : 1.93098
[1mStep[0m  [40/42], [94mLoss[0m : 2.03339

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99465
[1mStep[0m  [4/42], [94mLoss[0m : 2.04611
[1mStep[0m  [8/42], [94mLoss[0m : 2.10047
[1mStep[0m  [12/42], [94mLoss[0m : 2.08072
[1mStep[0m  [16/42], [94mLoss[0m : 1.89542
[1mStep[0m  [20/42], [94mLoss[0m : 2.00790
[1mStep[0m  [24/42], [94mLoss[0m : 2.11786
[1mStep[0m  [28/42], [94mLoss[0m : 1.93222
[1mStep[0m  [32/42], [94mLoss[0m : 2.02906
[1mStep[0m  [36/42], [94mLoss[0m : 2.17987
[1mStep[0m  [40/42], [94mLoss[0m : 2.09512

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97662
[1mStep[0m  [4/42], [94mLoss[0m : 1.78002
[1mStep[0m  [8/42], [94mLoss[0m : 1.85207
[1mStep[0m  [12/42], [94mLoss[0m : 1.94093
[1mStep[0m  [16/42], [94mLoss[0m : 2.12060
[1mStep[0m  [20/42], [94mLoss[0m : 2.17294
[1mStep[0m  [24/42], [94mLoss[0m : 1.95894
[1mStep[0m  [28/42], [94mLoss[0m : 2.24991
[1mStep[0m  [32/42], [94mLoss[0m : 1.99656
[1mStep[0m  [36/42], [94mLoss[0m : 1.85550
[1mStep[0m  [40/42], [94mLoss[0m : 2.35453

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82967
[1mStep[0m  [4/42], [94mLoss[0m : 1.91275
[1mStep[0m  [8/42], [94mLoss[0m : 1.97331
[1mStep[0m  [12/42], [94mLoss[0m : 1.92426
[1mStep[0m  [16/42], [94mLoss[0m : 1.88555
[1mStep[0m  [20/42], [94mLoss[0m : 1.93777
[1mStep[0m  [24/42], [94mLoss[0m : 2.27571
[1mStep[0m  [28/42], [94mLoss[0m : 1.89947
[1mStep[0m  [32/42], [94mLoss[0m : 1.85072
[1mStep[0m  [36/42], [94mLoss[0m : 1.92812
[1mStep[0m  [40/42], [94mLoss[0m : 1.81683

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12926
[1mStep[0m  [4/42], [94mLoss[0m : 2.04051
[1mStep[0m  [8/42], [94mLoss[0m : 1.90410
[1mStep[0m  [12/42], [94mLoss[0m : 1.97626
[1mStep[0m  [16/42], [94mLoss[0m : 1.88243
[1mStep[0m  [20/42], [94mLoss[0m : 1.98890
[1mStep[0m  [24/42], [94mLoss[0m : 1.97269
[1mStep[0m  [28/42], [94mLoss[0m : 1.87270
[1mStep[0m  [32/42], [94mLoss[0m : 2.13488
[1mStep[0m  [36/42], [94mLoss[0m : 1.92279
[1mStep[0m  [40/42], [94mLoss[0m : 1.88608

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87648
[1mStep[0m  [4/42], [94mLoss[0m : 2.01338
[1mStep[0m  [8/42], [94mLoss[0m : 2.09750
[1mStep[0m  [12/42], [94mLoss[0m : 1.93255
[1mStep[0m  [16/42], [94mLoss[0m : 2.22457
[1mStep[0m  [20/42], [94mLoss[0m : 1.95218
[1mStep[0m  [24/42], [94mLoss[0m : 1.93864
[1mStep[0m  [28/42], [94mLoss[0m : 1.87853
[1mStep[0m  [32/42], [94mLoss[0m : 1.97241
[1mStep[0m  [36/42], [94mLoss[0m : 1.95303
[1mStep[0m  [40/42], [94mLoss[0m : 1.86186

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92887
[1mStep[0m  [4/42], [94mLoss[0m : 2.04941
[1mStep[0m  [8/42], [94mLoss[0m : 1.89065
[1mStep[0m  [12/42], [94mLoss[0m : 2.04468
[1mStep[0m  [16/42], [94mLoss[0m : 1.72193
[1mStep[0m  [20/42], [94mLoss[0m : 1.82631
[1mStep[0m  [24/42], [94mLoss[0m : 1.97623
[1mStep[0m  [28/42], [94mLoss[0m : 1.81466
[1mStep[0m  [32/42], [94mLoss[0m : 1.77697
[1mStep[0m  [36/42], [94mLoss[0m : 1.96762
[1mStep[0m  [40/42], [94mLoss[0m : 2.06086

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06965
[1mStep[0m  [4/42], [94mLoss[0m : 1.82004
[1mStep[0m  [8/42], [94mLoss[0m : 1.72436
[1mStep[0m  [12/42], [94mLoss[0m : 1.72056
[1mStep[0m  [16/42], [94mLoss[0m : 1.86278
[1mStep[0m  [20/42], [94mLoss[0m : 2.00892
[1mStep[0m  [24/42], [94mLoss[0m : 1.84572
[1mStep[0m  [28/42], [94mLoss[0m : 2.12554
[1mStep[0m  [32/42], [94mLoss[0m : 2.00438
[1mStep[0m  [36/42], [94mLoss[0m : 1.79081
[1mStep[0m  [40/42], [94mLoss[0m : 1.90365

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.429
====================================

Phase 2 - Evaluation MAE:  2.429424524307251
MAE score P1      2.329451
MAE score P2      2.429425
loss              1.898207
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.02812
[1mStep[0m  [8/84], [94mLoss[0m : 8.47603
[1mStep[0m  [16/84], [94mLoss[0m : 7.27308
[1mStep[0m  [24/84], [94mLoss[0m : 4.23205
[1mStep[0m  [32/84], [94mLoss[0m : 3.92401
[1mStep[0m  [40/84], [94mLoss[0m : 3.43165
[1mStep[0m  [48/84], [94mLoss[0m : 2.91077
[1mStep[0m  [56/84], [94mLoss[0m : 3.04392
[1mStep[0m  [64/84], [94mLoss[0m : 2.54850
[1mStep[0m  [72/84], [94mLoss[0m : 2.88148
[1mStep[0m  [80/84], [94mLoss[0m : 2.21502

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.396, [92mTest[0m: 10.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59394
[1mStep[0m  [8/84], [94mLoss[0m : 2.77259
[1mStep[0m  [16/84], [94mLoss[0m : 2.41671
[1mStep[0m  [24/84], [94mLoss[0m : 2.66431
[1mStep[0m  [32/84], [94mLoss[0m : 2.80840
[1mStep[0m  [40/84], [94mLoss[0m : 2.50922
[1mStep[0m  [48/84], [94mLoss[0m : 2.75485
[1mStep[0m  [56/84], [94mLoss[0m : 2.45917
[1mStep[0m  [64/84], [94mLoss[0m : 2.53912
[1mStep[0m  [72/84], [94mLoss[0m : 2.73545
[1mStep[0m  [80/84], [94mLoss[0m : 2.53590

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.523, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98656
[1mStep[0m  [8/84], [94mLoss[0m : 2.43170
[1mStep[0m  [16/84], [94mLoss[0m : 2.69599
[1mStep[0m  [24/84], [94mLoss[0m : 2.56251
[1mStep[0m  [32/84], [94mLoss[0m : 2.69408
[1mStep[0m  [40/84], [94mLoss[0m : 2.70880
[1mStep[0m  [48/84], [94mLoss[0m : 2.46767
[1mStep[0m  [56/84], [94mLoss[0m : 2.63923
[1mStep[0m  [64/84], [94mLoss[0m : 2.70204
[1mStep[0m  [72/84], [94mLoss[0m : 2.71054
[1mStep[0m  [80/84], [94mLoss[0m : 2.94544

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84068
[1mStep[0m  [8/84], [94mLoss[0m : 2.68928
[1mStep[0m  [16/84], [94mLoss[0m : 2.78917
[1mStep[0m  [24/84], [94mLoss[0m : 2.59345
[1mStep[0m  [32/84], [94mLoss[0m : 2.36753
[1mStep[0m  [40/84], [94mLoss[0m : 2.61197
[1mStep[0m  [48/84], [94mLoss[0m : 2.76384
[1mStep[0m  [56/84], [94mLoss[0m : 2.45426
[1mStep[0m  [64/84], [94mLoss[0m : 2.72290
[1mStep[0m  [72/84], [94mLoss[0m : 2.40346
[1mStep[0m  [80/84], [94mLoss[0m : 2.38158

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76080
[1mStep[0m  [8/84], [94mLoss[0m : 2.72593
[1mStep[0m  [16/84], [94mLoss[0m : 2.63261
[1mStep[0m  [24/84], [94mLoss[0m : 2.78364
[1mStep[0m  [32/84], [94mLoss[0m : 2.38808
[1mStep[0m  [40/84], [94mLoss[0m : 2.71119
[1mStep[0m  [48/84], [94mLoss[0m : 2.59201
[1mStep[0m  [56/84], [94mLoss[0m : 2.77614
[1mStep[0m  [64/84], [94mLoss[0m : 2.60223
[1mStep[0m  [72/84], [94mLoss[0m : 2.52669
[1mStep[0m  [80/84], [94mLoss[0m : 2.58435

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39381
[1mStep[0m  [8/84], [94mLoss[0m : 2.66718
[1mStep[0m  [16/84], [94mLoss[0m : 2.47294
[1mStep[0m  [24/84], [94mLoss[0m : 2.55353
[1mStep[0m  [32/84], [94mLoss[0m : 2.42411
[1mStep[0m  [40/84], [94mLoss[0m : 2.62286
[1mStep[0m  [48/84], [94mLoss[0m : 2.35099
[1mStep[0m  [56/84], [94mLoss[0m : 2.60451
[1mStep[0m  [64/84], [94mLoss[0m : 2.70534
[1mStep[0m  [72/84], [94mLoss[0m : 2.82676
[1mStep[0m  [80/84], [94mLoss[0m : 2.56418

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.03623
[1mStep[0m  [8/84], [94mLoss[0m : 2.32673
[1mStep[0m  [16/84], [94mLoss[0m : 2.50157
[1mStep[0m  [24/84], [94mLoss[0m : 2.77313
[1mStep[0m  [32/84], [94mLoss[0m : 2.41950
[1mStep[0m  [40/84], [94mLoss[0m : 2.46041
[1mStep[0m  [48/84], [94mLoss[0m : 2.56471
[1mStep[0m  [56/84], [94mLoss[0m : 2.29567
[1mStep[0m  [64/84], [94mLoss[0m : 2.35965
[1mStep[0m  [72/84], [94mLoss[0m : 2.24025
[1mStep[0m  [80/84], [94mLoss[0m : 2.25471

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54039
[1mStep[0m  [8/84], [94mLoss[0m : 2.78194
[1mStep[0m  [16/84], [94mLoss[0m : 2.47077
[1mStep[0m  [24/84], [94mLoss[0m : 2.83600
[1mStep[0m  [32/84], [94mLoss[0m : 2.57855
[1mStep[0m  [40/84], [94mLoss[0m : 2.74293
[1mStep[0m  [48/84], [94mLoss[0m : 2.59650
[1mStep[0m  [56/84], [94mLoss[0m : 2.50395
[1mStep[0m  [64/84], [94mLoss[0m : 2.43426
[1mStep[0m  [72/84], [94mLoss[0m : 2.60141
[1mStep[0m  [80/84], [94mLoss[0m : 2.50506

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48523
[1mStep[0m  [8/84], [94mLoss[0m : 2.68421
[1mStep[0m  [16/84], [94mLoss[0m : 2.67077
[1mStep[0m  [24/84], [94mLoss[0m : 2.40525
[1mStep[0m  [32/84], [94mLoss[0m : 2.67227
[1mStep[0m  [40/84], [94mLoss[0m : 2.63809
[1mStep[0m  [48/84], [94mLoss[0m : 2.97413
[1mStep[0m  [56/84], [94mLoss[0m : 2.49026
[1mStep[0m  [64/84], [94mLoss[0m : 2.67344
[1mStep[0m  [72/84], [94mLoss[0m : 2.59332
[1mStep[0m  [80/84], [94mLoss[0m : 2.29287

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58070
[1mStep[0m  [8/84], [94mLoss[0m : 3.07395
[1mStep[0m  [16/84], [94mLoss[0m : 2.49013
[1mStep[0m  [24/84], [94mLoss[0m : 2.46116
[1mStep[0m  [32/84], [94mLoss[0m : 2.74132
[1mStep[0m  [40/84], [94mLoss[0m : 2.63153
[1mStep[0m  [48/84], [94mLoss[0m : 2.54333
[1mStep[0m  [56/84], [94mLoss[0m : 2.81547
[1mStep[0m  [64/84], [94mLoss[0m : 2.36254
[1mStep[0m  [72/84], [94mLoss[0m : 2.68132
[1mStep[0m  [80/84], [94mLoss[0m : 2.54332

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68390
[1mStep[0m  [8/84], [94mLoss[0m : 2.38091
[1mStep[0m  [16/84], [94mLoss[0m : 2.40116
[1mStep[0m  [24/84], [94mLoss[0m : 2.29530
[1mStep[0m  [32/84], [94mLoss[0m : 2.50584
[1mStep[0m  [40/84], [94mLoss[0m : 2.38756
[1mStep[0m  [48/84], [94mLoss[0m : 2.53481
[1mStep[0m  [56/84], [94mLoss[0m : 2.51922
[1mStep[0m  [64/84], [94mLoss[0m : 2.34702
[1mStep[0m  [72/84], [94mLoss[0m : 2.56018
[1mStep[0m  [80/84], [94mLoss[0m : 2.72329

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47295
[1mStep[0m  [8/84], [94mLoss[0m : 2.41467
[1mStep[0m  [16/84], [94mLoss[0m : 2.25961
[1mStep[0m  [24/84], [94mLoss[0m : 2.62690
[1mStep[0m  [32/84], [94mLoss[0m : 2.21767
[1mStep[0m  [40/84], [94mLoss[0m : 2.55231
[1mStep[0m  [48/84], [94mLoss[0m : 2.43406
[1mStep[0m  [56/84], [94mLoss[0m : 2.58983
[1mStep[0m  [64/84], [94mLoss[0m : 2.71358
[1mStep[0m  [72/84], [94mLoss[0m : 2.51747
[1mStep[0m  [80/84], [94mLoss[0m : 2.48093

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52135
[1mStep[0m  [8/84], [94mLoss[0m : 2.51365
[1mStep[0m  [16/84], [94mLoss[0m : 2.59825
[1mStep[0m  [24/84], [94mLoss[0m : 2.51093
[1mStep[0m  [32/84], [94mLoss[0m : 2.72204
[1mStep[0m  [40/84], [94mLoss[0m : 2.39015
[1mStep[0m  [48/84], [94mLoss[0m : 2.32312
[1mStep[0m  [56/84], [94mLoss[0m : 2.69212
[1mStep[0m  [64/84], [94mLoss[0m : 2.38024
[1mStep[0m  [72/84], [94mLoss[0m : 2.73816
[1mStep[0m  [80/84], [94mLoss[0m : 2.31254

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38168
[1mStep[0m  [8/84], [94mLoss[0m : 2.38232
[1mStep[0m  [16/84], [94mLoss[0m : 2.64122
[1mStep[0m  [24/84], [94mLoss[0m : 2.34595
[1mStep[0m  [32/84], [94mLoss[0m : 2.83545
[1mStep[0m  [40/84], [94mLoss[0m : 2.63653
[1mStep[0m  [48/84], [94mLoss[0m : 2.62969
[1mStep[0m  [56/84], [94mLoss[0m : 2.77935
[1mStep[0m  [64/84], [94mLoss[0m : 2.85119
[1mStep[0m  [72/84], [94mLoss[0m : 2.48997
[1mStep[0m  [80/84], [94mLoss[0m : 2.44034

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54837
[1mStep[0m  [8/84], [94mLoss[0m : 2.40925
[1mStep[0m  [16/84], [94mLoss[0m : 2.73888
[1mStep[0m  [24/84], [94mLoss[0m : 2.50082
[1mStep[0m  [32/84], [94mLoss[0m : 2.59196
[1mStep[0m  [40/84], [94mLoss[0m : 2.50443
[1mStep[0m  [48/84], [94mLoss[0m : 2.43170
[1mStep[0m  [56/84], [94mLoss[0m : 2.63575
[1mStep[0m  [64/84], [94mLoss[0m : 2.31764
[1mStep[0m  [72/84], [94mLoss[0m : 2.49624
[1mStep[0m  [80/84], [94mLoss[0m : 2.80116

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60558
[1mStep[0m  [8/84], [94mLoss[0m : 2.61250
[1mStep[0m  [16/84], [94mLoss[0m : 2.48058
[1mStep[0m  [24/84], [94mLoss[0m : 2.72687
[1mStep[0m  [32/84], [94mLoss[0m : 2.61840
[1mStep[0m  [40/84], [94mLoss[0m : 2.62637
[1mStep[0m  [48/84], [94mLoss[0m : 2.46420
[1mStep[0m  [56/84], [94mLoss[0m : 2.43665
[1mStep[0m  [64/84], [94mLoss[0m : 2.75785
[1mStep[0m  [72/84], [94mLoss[0m : 2.45778
[1mStep[0m  [80/84], [94mLoss[0m : 2.48561

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76993
[1mStep[0m  [8/84], [94mLoss[0m : 2.40342
[1mStep[0m  [16/84], [94mLoss[0m : 2.57994
[1mStep[0m  [24/84], [94mLoss[0m : 2.36201
[1mStep[0m  [32/84], [94mLoss[0m : 2.54093
[1mStep[0m  [40/84], [94mLoss[0m : 2.62882
[1mStep[0m  [48/84], [94mLoss[0m : 2.35280
[1mStep[0m  [56/84], [94mLoss[0m : 2.43199
[1mStep[0m  [64/84], [94mLoss[0m : 2.62682
[1mStep[0m  [72/84], [94mLoss[0m : 2.63516
[1mStep[0m  [80/84], [94mLoss[0m : 2.32215

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30451
[1mStep[0m  [8/84], [94mLoss[0m : 2.66441
[1mStep[0m  [16/84], [94mLoss[0m : 2.58850
[1mStep[0m  [24/84], [94mLoss[0m : 2.53430
[1mStep[0m  [32/84], [94mLoss[0m : 2.76803
[1mStep[0m  [40/84], [94mLoss[0m : 2.40145
[1mStep[0m  [48/84], [94mLoss[0m : 2.48944
[1mStep[0m  [56/84], [94mLoss[0m : 2.29419
[1mStep[0m  [64/84], [94mLoss[0m : 2.25169
[1mStep[0m  [72/84], [94mLoss[0m : 2.38503
[1mStep[0m  [80/84], [94mLoss[0m : 2.49484

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76295
[1mStep[0m  [8/84], [94mLoss[0m : 2.32045
[1mStep[0m  [16/84], [94mLoss[0m : 2.32009
[1mStep[0m  [24/84], [94mLoss[0m : 2.68207
[1mStep[0m  [32/84], [94mLoss[0m : 2.57059
[1mStep[0m  [40/84], [94mLoss[0m : 2.49307
[1mStep[0m  [48/84], [94mLoss[0m : 2.37924
[1mStep[0m  [56/84], [94mLoss[0m : 2.80847
[1mStep[0m  [64/84], [94mLoss[0m : 2.47063
[1mStep[0m  [72/84], [94mLoss[0m : 2.45494
[1mStep[0m  [80/84], [94mLoss[0m : 2.30527

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36283
[1mStep[0m  [8/84], [94mLoss[0m : 2.53409
[1mStep[0m  [16/84], [94mLoss[0m : 2.55531
[1mStep[0m  [24/84], [94mLoss[0m : 2.53861
[1mStep[0m  [32/84], [94mLoss[0m : 2.10506
[1mStep[0m  [40/84], [94mLoss[0m : 2.75647
[1mStep[0m  [48/84], [94mLoss[0m : 2.42291
[1mStep[0m  [56/84], [94mLoss[0m : 2.24174
[1mStep[0m  [64/84], [94mLoss[0m : 2.61557
[1mStep[0m  [72/84], [94mLoss[0m : 2.61356
[1mStep[0m  [80/84], [94mLoss[0m : 2.64246

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33494
[1mStep[0m  [8/84], [94mLoss[0m : 2.53539
[1mStep[0m  [16/84], [94mLoss[0m : 2.55901
[1mStep[0m  [24/84], [94mLoss[0m : 2.39789
[1mStep[0m  [32/84], [94mLoss[0m : 2.65959
[1mStep[0m  [40/84], [94mLoss[0m : 2.51964
[1mStep[0m  [48/84], [94mLoss[0m : 2.52939
[1mStep[0m  [56/84], [94mLoss[0m : 2.40243
[1mStep[0m  [64/84], [94mLoss[0m : 2.70918
[1mStep[0m  [72/84], [94mLoss[0m : 2.60663
[1mStep[0m  [80/84], [94mLoss[0m : 2.38973

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55428
[1mStep[0m  [8/84], [94mLoss[0m : 2.70007
[1mStep[0m  [16/84], [94mLoss[0m : 2.43596
[1mStep[0m  [24/84], [94mLoss[0m : 2.35441
[1mStep[0m  [32/84], [94mLoss[0m : 2.67082
[1mStep[0m  [40/84], [94mLoss[0m : 2.44430
[1mStep[0m  [48/84], [94mLoss[0m : 2.33404
[1mStep[0m  [56/84], [94mLoss[0m : 2.67458
[1mStep[0m  [64/84], [94mLoss[0m : 2.62882
[1mStep[0m  [72/84], [94mLoss[0m : 2.69503
[1mStep[0m  [80/84], [94mLoss[0m : 2.58877

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62853
[1mStep[0m  [8/84], [94mLoss[0m : 2.12867
[1mStep[0m  [16/84], [94mLoss[0m : 2.40187
[1mStep[0m  [24/84], [94mLoss[0m : 2.26589
[1mStep[0m  [32/84], [94mLoss[0m : 2.47162
[1mStep[0m  [40/84], [94mLoss[0m : 2.76323
[1mStep[0m  [48/84], [94mLoss[0m : 2.45059
[1mStep[0m  [56/84], [94mLoss[0m : 2.36501
[1mStep[0m  [64/84], [94mLoss[0m : 3.02890
[1mStep[0m  [72/84], [94mLoss[0m : 2.73277
[1mStep[0m  [80/84], [94mLoss[0m : 2.84971

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46338
[1mStep[0m  [8/84], [94mLoss[0m : 2.32706
[1mStep[0m  [16/84], [94mLoss[0m : 2.56574
[1mStep[0m  [24/84], [94mLoss[0m : 2.56659
[1mStep[0m  [32/84], [94mLoss[0m : 2.61336
[1mStep[0m  [40/84], [94mLoss[0m : 2.25852
[1mStep[0m  [48/84], [94mLoss[0m : 2.60959
[1mStep[0m  [56/84], [94mLoss[0m : 2.35969
[1mStep[0m  [64/84], [94mLoss[0m : 2.61169
[1mStep[0m  [72/84], [94mLoss[0m : 1.97287
[1mStep[0m  [80/84], [94mLoss[0m : 2.22636

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37876
[1mStep[0m  [8/84], [94mLoss[0m : 2.42091
[1mStep[0m  [16/84], [94mLoss[0m : 2.30536
[1mStep[0m  [24/84], [94mLoss[0m : 2.53381
[1mStep[0m  [32/84], [94mLoss[0m : 2.55706
[1mStep[0m  [40/84], [94mLoss[0m : 2.40977
[1mStep[0m  [48/84], [94mLoss[0m : 2.83352
[1mStep[0m  [56/84], [94mLoss[0m : 2.39135
[1mStep[0m  [64/84], [94mLoss[0m : 2.71883
[1mStep[0m  [72/84], [94mLoss[0m : 2.50092
[1mStep[0m  [80/84], [94mLoss[0m : 2.21177

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64720
[1mStep[0m  [8/84], [94mLoss[0m : 2.40804
[1mStep[0m  [16/84], [94mLoss[0m : 2.29761
[1mStep[0m  [24/84], [94mLoss[0m : 2.45233
[1mStep[0m  [32/84], [94mLoss[0m : 2.15253
[1mStep[0m  [40/84], [94mLoss[0m : 2.63989
[1mStep[0m  [48/84], [94mLoss[0m : 2.48454
[1mStep[0m  [56/84], [94mLoss[0m : 2.67695
[1mStep[0m  [64/84], [94mLoss[0m : 2.58938
[1mStep[0m  [72/84], [94mLoss[0m : 2.44874
[1mStep[0m  [80/84], [94mLoss[0m : 2.45164

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66406
[1mStep[0m  [8/84], [94mLoss[0m : 2.22553
[1mStep[0m  [16/84], [94mLoss[0m : 2.84622
[1mStep[0m  [24/84], [94mLoss[0m : 2.46083
[1mStep[0m  [32/84], [94mLoss[0m : 2.32281
[1mStep[0m  [40/84], [94mLoss[0m : 2.57856
[1mStep[0m  [48/84], [94mLoss[0m : 2.68956
[1mStep[0m  [56/84], [94mLoss[0m : 2.38693
[1mStep[0m  [64/84], [94mLoss[0m : 2.54321
[1mStep[0m  [72/84], [94mLoss[0m : 2.37036
[1mStep[0m  [80/84], [94mLoss[0m : 2.59842

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70875
[1mStep[0m  [8/84], [94mLoss[0m : 2.30662
[1mStep[0m  [16/84], [94mLoss[0m : 2.28015
[1mStep[0m  [24/84], [94mLoss[0m : 2.35162
[1mStep[0m  [32/84], [94mLoss[0m : 2.30684
[1mStep[0m  [40/84], [94mLoss[0m : 2.57761
[1mStep[0m  [48/84], [94mLoss[0m : 2.47955
[1mStep[0m  [56/84], [94mLoss[0m : 2.42983
[1mStep[0m  [64/84], [94mLoss[0m : 2.45707
[1mStep[0m  [72/84], [94mLoss[0m : 2.50784
[1mStep[0m  [80/84], [94mLoss[0m : 2.41519

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80621
[1mStep[0m  [8/84], [94mLoss[0m : 2.50807
[1mStep[0m  [16/84], [94mLoss[0m : 2.33324
[1mStep[0m  [24/84], [94mLoss[0m : 2.36775
[1mStep[0m  [32/84], [94mLoss[0m : 2.76135
[1mStep[0m  [40/84], [94mLoss[0m : 2.51398
[1mStep[0m  [48/84], [94mLoss[0m : 2.18831
[1mStep[0m  [56/84], [94mLoss[0m : 2.63838
[1mStep[0m  [64/84], [94mLoss[0m : 2.50043
[1mStep[0m  [72/84], [94mLoss[0m : 2.77205
[1mStep[0m  [80/84], [94mLoss[0m : 2.71718

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35114
[1mStep[0m  [8/84], [94mLoss[0m : 2.48518
[1mStep[0m  [16/84], [94mLoss[0m : 2.60455
[1mStep[0m  [24/84], [94mLoss[0m : 2.65157
[1mStep[0m  [32/84], [94mLoss[0m : 2.55432
[1mStep[0m  [40/84], [94mLoss[0m : 2.43066
[1mStep[0m  [48/84], [94mLoss[0m : 2.56930
[1mStep[0m  [56/84], [94mLoss[0m : 2.58635
[1mStep[0m  [64/84], [94mLoss[0m : 2.46294
[1mStep[0m  [72/84], [94mLoss[0m : 2.68605
[1mStep[0m  [80/84], [94mLoss[0m : 2.27586

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.325334153005055
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.25697
[1mStep[0m  [8/84], [94mLoss[0m : 2.62894
[1mStep[0m  [16/84], [94mLoss[0m : 2.64709
[1mStep[0m  [24/84], [94mLoss[0m : 2.34132
[1mStep[0m  [32/84], [94mLoss[0m : 2.57145
[1mStep[0m  [40/84], [94mLoss[0m : 2.94827
[1mStep[0m  [48/84], [94mLoss[0m : 2.39938
[1mStep[0m  [56/84], [94mLoss[0m : 2.29489
[1mStep[0m  [64/84], [94mLoss[0m : 2.65242
[1mStep[0m  [72/84], [94mLoss[0m : 2.78056
[1mStep[0m  [80/84], [94mLoss[0m : 2.61351

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18354
[1mStep[0m  [8/84], [94mLoss[0m : 2.16709
[1mStep[0m  [16/84], [94mLoss[0m : 2.12685
[1mStep[0m  [24/84], [94mLoss[0m : 2.19879
[1mStep[0m  [32/84], [94mLoss[0m : 2.69007
[1mStep[0m  [40/84], [94mLoss[0m : 2.52049
[1mStep[0m  [48/84], [94mLoss[0m : 2.24048
[1mStep[0m  [56/84], [94mLoss[0m : 2.23265
[1mStep[0m  [64/84], [94mLoss[0m : 2.58307
[1mStep[0m  [72/84], [94mLoss[0m : 2.36368
[1mStep[0m  [80/84], [94mLoss[0m : 2.69313

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78041
[1mStep[0m  [8/84], [94mLoss[0m : 2.39945
[1mStep[0m  [16/84], [94mLoss[0m : 2.52583
[1mStep[0m  [24/84], [94mLoss[0m : 2.50768
[1mStep[0m  [32/84], [94mLoss[0m : 2.49781
[1mStep[0m  [40/84], [94mLoss[0m : 2.70750
[1mStep[0m  [48/84], [94mLoss[0m : 1.94748
[1mStep[0m  [56/84], [94mLoss[0m : 2.31878
[1mStep[0m  [64/84], [94mLoss[0m : 2.44436
[1mStep[0m  [72/84], [94mLoss[0m : 2.50388
[1mStep[0m  [80/84], [94mLoss[0m : 2.21326

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50106
[1mStep[0m  [8/84], [94mLoss[0m : 2.51981
[1mStep[0m  [16/84], [94mLoss[0m : 2.79042
[1mStep[0m  [24/84], [94mLoss[0m : 2.34179
[1mStep[0m  [32/84], [94mLoss[0m : 2.49463
[1mStep[0m  [40/84], [94mLoss[0m : 2.32737
[1mStep[0m  [48/84], [94mLoss[0m : 2.26634
[1mStep[0m  [56/84], [94mLoss[0m : 2.29536
[1mStep[0m  [64/84], [94mLoss[0m : 2.46463
[1mStep[0m  [72/84], [94mLoss[0m : 2.43753
[1mStep[0m  [80/84], [94mLoss[0m : 2.49125

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19900
[1mStep[0m  [8/84], [94mLoss[0m : 2.33807
[1mStep[0m  [16/84], [94mLoss[0m : 2.26778
[1mStep[0m  [24/84], [94mLoss[0m : 2.40751
[1mStep[0m  [32/84], [94mLoss[0m : 2.58673
[1mStep[0m  [40/84], [94mLoss[0m : 2.41878
[1mStep[0m  [48/84], [94mLoss[0m : 2.43972
[1mStep[0m  [56/84], [94mLoss[0m : 2.47517
[1mStep[0m  [64/84], [94mLoss[0m : 2.47114
[1mStep[0m  [72/84], [94mLoss[0m : 2.74009
[1mStep[0m  [80/84], [94mLoss[0m : 2.29045

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35709
[1mStep[0m  [8/84], [94mLoss[0m : 2.28578
[1mStep[0m  [16/84], [94mLoss[0m : 2.26892
[1mStep[0m  [24/84], [94mLoss[0m : 2.17744
[1mStep[0m  [32/84], [94mLoss[0m : 2.33446
[1mStep[0m  [40/84], [94mLoss[0m : 2.18046
[1mStep[0m  [48/84], [94mLoss[0m : 2.11720
[1mStep[0m  [56/84], [94mLoss[0m : 2.09289
[1mStep[0m  [64/84], [94mLoss[0m : 2.27669
[1mStep[0m  [72/84], [94mLoss[0m : 2.36224
[1mStep[0m  [80/84], [94mLoss[0m : 2.41209

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09636
[1mStep[0m  [8/84], [94mLoss[0m : 2.58035
[1mStep[0m  [16/84], [94mLoss[0m : 2.28192
[1mStep[0m  [24/84], [94mLoss[0m : 2.22774
[1mStep[0m  [32/84], [94mLoss[0m : 2.32001
[1mStep[0m  [40/84], [94mLoss[0m : 2.53906
[1mStep[0m  [48/84], [94mLoss[0m : 2.16963
[1mStep[0m  [56/84], [94mLoss[0m : 2.15658
[1mStep[0m  [64/84], [94mLoss[0m : 2.14907
[1mStep[0m  [72/84], [94mLoss[0m : 2.39537
[1mStep[0m  [80/84], [94mLoss[0m : 2.50815

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21063
[1mStep[0m  [8/84], [94mLoss[0m : 2.05978
[1mStep[0m  [16/84], [94mLoss[0m : 2.35767
[1mStep[0m  [24/84], [94mLoss[0m : 2.22853
[1mStep[0m  [32/84], [94mLoss[0m : 2.44127
[1mStep[0m  [40/84], [94mLoss[0m : 2.25834
[1mStep[0m  [48/84], [94mLoss[0m : 2.20992
[1mStep[0m  [56/84], [94mLoss[0m : 2.49940
[1mStep[0m  [64/84], [94mLoss[0m : 2.32845
[1mStep[0m  [72/84], [94mLoss[0m : 2.11087
[1mStep[0m  [80/84], [94mLoss[0m : 2.14587

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32710
[1mStep[0m  [8/84], [94mLoss[0m : 2.02350
[1mStep[0m  [16/84], [94mLoss[0m : 2.62441
[1mStep[0m  [24/84], [94mLoss[0m : 2.13880
[1mStep[0m  [32/84], [94mLoss[0m : 2.13005
[1mStep[0m  [40/84], [94mLoss[0m : 2.29649
[1mStep[0m  [48/84], [94mLoss[0m : 2.20202
[1mStep[0m  [56/84], [94mLoss[0m : 2.07027
[1mStep[0m  [64/84], [94mLoss[0m : 2.47010
[1mStep[0m  [72/84], [94mLoss[0m : 2.15995
[1mStep[0m  [80/84], [94mLoss[0m : 2.39115

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36394
[1mStep[0m  [8/84], [94mLoss[0m : 2.25551
[1mStep[0m  [16/84], [94mLoss[0m : 2.03297
[1mStep[0m  [24/84], [94mLoss[0m : 2.04094
[1mStep[0m  [32/84], [94mLoss[0m : 2.21767
[1mStep[0m  [40/84], [94mLoss[0m : 2.17477
[1mStep[0m  [48/84], [94mLoss[0m : 2.18638
[1mStep[0m  [56/84], [94mLoss[0m : 1.96499
[1mStep[0m  [64/84], [94mLoss[0m : 2.14688
[1mStep[0m  [72/84], [94mLoss[0m : 2.22029
[1mStep[0m  [80/84], [94mLoss[0m : 2.50270

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07492
[1mStep[0m  [8/84], [94mLoss[0m : 2.23331
[1mStep[0m  [16/84], [94mLoss[0m : 1.97295
[1mStep[0m  [24/84], [94mLoss[0m : 2.16793
[1mStep[0m  [32/84], [94mLoss[0m : 2.39468
[1mStep[0m  [40/84], [94mLoss[0m : 2.08194
[1mStep[0m  [48/84], [94mLoss[0m : 2.23904
[1mStep[0m  [56/84], [94mLoss[0m : 2.33908
[1mStep[0m  [64/84], [94mLoss[0m : 2.05819
[1mStep[0m  [72/84], [94mLoss[0m : 2.30819
[1mStep[0m  [80/84], [94mLoss[0m : 2.17681

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02551
[1mStep[0m  [8/84], [94mLoss[0m : 2.12150
[1mStep[0m  [16/84], [94mLoss[0m : 2.17875
[1mStep[0m  [24/84], [94mLoss[0m : 1.93390
[1mStep[0m  [32/84], [94mLoss[0m : 2.11065
[1mStep[0m  [40/84], [94mLoss[0m : 2.21244
[1mStep[0m  [48/84], [94mLoss[0m : 2.33378
[1mStep[0m  [56/84], [94mLoss[0m : 2.09961
[1mStep[0m  [64/84], [94mLoss[0m : 1.93310
[1mStep[0m  [72/84], [94mLoss[0m : 2.39774
[1mStep[0m  [80/84], [94mLoss[0m : 2.18009

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.569, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69901
[1mStep[0m  [8/84], [94mLoss[0m : 2.19299
[1mStep[0m  [16/84], [94mLoss[0m : 2.00897
[1mStep[0m  [24/84], [94mLoss[0m : 2.07435
[1mStep[0m  [32/84], [94mLoss[0m : 2.11606
[1mStep[0m  [40/84], [94mLoss[0m : 1.98263
[1mStep[0m  [48/84], [94mLoss[0m : 2.13305
[1mStep[0m  [56/84], [94mLoss[0m : 2.17407
[1mStep[0m  [64/84], [94mLoss[0m : 2.21236
[1mStep[0m  [72/84], [94mLoss[0m : 2.27535
[1mStep[0m  [80/84], [94mLoss[0m : 1.80402

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02465
[1mStep[0m  [8/84], [94mLoss[0m : 2.14947
[1mStep[0m  [16/84], [94mLoss[0m : 2.09620
[1mStep[0m  [24/84], [94mLoss[0m : 2.10625
[1mStep[0m  [32/84], [94mLoss[0m : 2.00919
[1mStep[0m  [40/84], [94mLoss[0m : 1.99994
[1mStep[0m  [48/84], [94mLoss[0m : 1.92718
[1mStep[0m  [56/84], [94mLoss[0m : 2.12273
[1mStep[0m  [64/84], [94mLoss[0m : 2.05595
[1mStep[0m  [72/84], [94mLoss[0m : 1.85832
[1mStep[0m  [80/84], [94mLoss[0m : 1.87855

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90361
[1mStep[0m  [8/84], [94mLoss[0m : 1.90329
[1mStep[0m  [16/84], [94mLoss[0m : 2.10408
[1mStep[0m  [24/84], [94mLoss[0m : 1.87838
[1mStep[0m  [32/84], [94mLoss[0m : 1.78048
[1mStep[0m  [40/84], [94mLoss[0m : 2.16395
[1mStep[0m  [48/84], [94mLoss[0m : 1.95996
[1mStep[0m  [56/84], [94mLoss[0m : 1.71833
[1mStep[0m  [64/84], [94mLoss[0m : 1.97235
[1mStep[0m  [72/84], [94mLoss[0m : 1.92728
[1mStep[0m  [80/84], [94mLoss[0m : 2.12339

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16576
[1mStep[0m  [8/84], [94mLoss[0m : 1.83455
[1mStep[0m  [16/84], [94mLoss[0m : 1.83983
[1mStep[0m  [24/84], [94mLoss[0m : 2.04797
[1mStep[0m  [32/84], [94mLoss[0m : 1.89587
[1mStep[0m  [40/84], [94mLoss[0m : 1.80121
[1mStep[0m  [48/84], [94mLoss[0m : 1.83270
[1mStep[0m  [56/84], [94mLoss[0m : 1.76006
[1mStep[0m  [64/84], [94mLoss[0m : 2.10960
[1mStep[0m  [72/84], [94mLoss[0m : 2.18367
[1mStep[0m  [80/84], [94mLoss[0m : 2.09371

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93275
[1mStep[0m  [8/84], [94mLoss[0m : 1.81538
[1mStep[0m  [16/84], [94mLoss[0m : 2.00031
[1mStep[0m  [24/84], [94mLoss[0m : 2.02662
[1mStep[0m  [32/84], [94mLoss[0m : 1.97461
[1mStep[0m  [40/84], [94mLoss[0m : 1.67709
[1mStep[0m  [48/84], [94mLoss[0m : 2.32504
[1mStep[0m  [56/84], [94mLoss[0m : 2.02204
[1mStep[0m  [64/84], [94mLoss[0m : 1.99812
[1mStep[0m  [72/84], [94mLoss[0m : 2.28442
[1mStep[0m  [80/84], [94mLoss[0m : 1.90292

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68506
[1mStep[0m  [8/84], [94mLoss[0m : 2.04636
[1mStep[0m  [16/84], [94mLoss[0m : 1.71073
[1mStep[0m  [24/84], [94mLoss[0m : 1.91583
[1mStep[0m  [32/84], [94mLoss[0m : 1.89453
[1mStep[0m  [40/84], [94mLoss[0m : 2.00105
[1mStep[0m  [48/84], [94mLoss[0m : 2.23297
[1mStep[0m  [56/84], [94mLoss[0m : 1.93598
[1mStep[0m  [64/84], [94mLoss[0m : 1.87392
[1mStep[0m  [72/84], [94mLoss[0m : 1.98593
[1mStep[0m  [80/84], [94mLoss[0m : 1.76634

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71897
[1mStep[0m  [8/84], [94mLoss[0m : 1.95727
[1mStep[0m  [16/84], [94mLoss[0m : 1.89911
[1mStep[0m  [24/84], [94mLoss[0m : 1.82400
[1mStep[0m  [32/84], [94mLoss[0m : 1.51930
[1mStep[0m  [40/84], [94mLoss[0m : 1.63438
[1mStep[0m  [48/84], [94mLoss[0m : 1.83793
[1mStep[0m  [56/84], [94mLoss[0m : 1.91904
[1mStep[0m  [64/84], [94mLoss[0m : 1.81860
[1mStep[0m  [72/84], [94mLoss[0m : 1.73339
[1mStep[0m  [80/84], [94mLoss[0m : 2.13436

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63400
[1mStep[0m  [8/84], [94mLoss[0m : 1.87014
[1mStep[0m  [16/84], [94mLoss[0m : 1.83013
[1mStep[0m  [24/84], [94mLoss[0m : 1.86836
[1mStep[0m  [32/84], [94mLoss[0m : 1.71821
[1mStep[0m  [40/84], [94mLoss[0m : 1.99373
[1mStep[0m  [48/84], [94mLoss[0m : 1.89282
[1mStep[0m  [56/84], [94mLoss[0m : 1.73665
[1mStep[0m  [64/84], [94mLoss[0m : 1.88311
[1mStep[0m  [72/84], [94mLoss[0m : 1.99467
[1mStep[0m  [80/84], [94mLoss[0m : 1.89833

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90519
[1mStep[0m  [8/84], [94mLoss[0m : 2.09070
[1mStep[0m  [16/84], [94mLoss[0m : 1.76803
[1mStep[0m  [24/84], [94mLoss[0m : 1.62319
[1mStep[0m  [32/84], [94mLoss[0m : 1.64040
[1mStep[0m  [40/84], [94mLoss[0m : 1.80540
[1mStep[0m  [48/84], [94mLoss[0m : 1.77456
[1mStep[0m  [56/84], [94mLoss[0m : 1.81553
[1mStep[0m  [64/84], [94mLoss[0m : 1.88432
[1mStep[0m  [72/84], [94mLoss[0m : 1.68742
[1mStep[0m  [80/84], [94mLoss[0m : 1.79202

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67280
[1mStep[0m  [8/84], [94mLoss[0m : 1.73133
[1mStep[0m  [16/84], [94mLoss[0m : 1.76419
[1mStep[0m  [24/84], [94mLoss[0m : 1.80790
[1mStep[0m  [32/84], [94mLoss[0m : 1.77029
[1mStep[0m  [40/84], [94mLoss[0m : 1.66700
[1mStep[0m  [48/84], [94mLoss[0m : 1.79945
[1mStep[0m  [56/84], [94mLoss[0m : 1.79715
[1mStep[0m  [64/84], [94mLoss[0m : 2.03607
[1mStep[0m  [72/84], [94mLoss[0m : 1.85887
[1mStep[0m  [80/84], [94mLoss[0m : 1.74403

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80315
[1mStep[0m  [8/84], [94mLoss[0m : 1.76856
[1mStep[0m  [16/84], [94mLoss[0m : 1.68984
[1mStep[0m  [24/84], [94mLoss[0m : 1.67114
[1mStep[0m  [32/84], [94mLoss[0m : 1.90881
[1mStep[0m  [40/84], [94mLoss[0m : 1.67389
[1mStep[0m  [48/84], [94mLoss[0m : 1.87115
[1mStep[0m  [56/84], [94mLoss[0m : 1.96309
[1mStep[0m  [64/84], [94mLoss[0m : 2.12932
[1mStep[0m  [72/84], [94mLoss[0m : 1.73386
[1mStep[0m  [80/84], [94mLoss[0m : 1.61208

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94534
[1mStep[0m  [8/84], [94mLoss[0m : 1.63742
[1mStep[0m  [16/84], [94mLoss[0m : 1.94631
[1mStep[0m  [24/84], [94mLoss[0m : 1.66255
[1mStep[0m  [32/84], [94mLoss[0m : 1.63327
[1mStep[0m  [40/84], [94mLoss[0m : 1.71422
[1mStep[0m  [48/84], [94mLoss[0m : 1.50005
[1mStep[0m  [56/84], [94mLoss[0m : 1.70655
[1mStep[0m  [64/84], [94mLoss[0m : 1.61978
[1mStep[0m  [72/84], [94mLoss[0m : 1.65265
[1mStep[0m  [80/84], [94mLoss[0m : 1.72243

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65079
[1mStep[0m  [8/84], [94mLoss[0m : 1.80942
[1mStep[0m  [16/84], [94mLoss[0m : 1.62182
[1mStep[0m  [24/84], [94mLoss[0m : 1.53372
[1mStep[0m  [32/84], [94mLoss[0m : 1.74068
[1mStep[0m  [40/84], [94mLoss[0m : 1.63025
[1mStep[0m  [48/84], [94mLoss[0m : 1.63369
[1mStep[0m  [56/84], [94mLoss[0m : 1.80302
[1mStep[0m  [64/84], [94mLoss[0m : 1.61398
[1mStep[0m  [72/84], [94mLoss[0m : 1.58944
[1mStep[0m  [80/84], [94mLoss[0m : 1.58653

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.545, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57564
[1mStep[0m  [8/84], [94mLoss[0m : 1.42780
[1mStep[0m  [16/84], [94mLoss[0m : 1.53145
[1mStep[0m  [24/84], [94mLoss[0m : 1.59479
[1mStep[0m  [32/84], [94mLoss[0m : 1.80205
[1mStep[0m  [40/84], [94mLoss[0m : 1.77873
[1mStep[0m  [48/84], [94mLoss[0m : 1.88184
[1mStep[0m  [56/84], [94mLoss[0m : 1.67626
[1mStep[0m  [64/84], [94mLoss[0m : 1.53808
[1mStep[0m  [72/84], [94mLoss[0m : 1.76037
[1mStep[0m  [80/84], [94mLoss[0m : 1.53684

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58284
[1mStep[0m  [8/84], [94mLoss[0m : 1.67822
[1mStep[0m  [16/84], [94mLoss[0m : 1.69025
[1mStep[0m  [24/84], [94mLoss[0m : 1.53581
[1mStep[0m  [32/84], [94mLoss[0m : 1.55605
[1mStep[0m  [40/84], [94mLoss[0m : 1.57746
[1mStep[0m  [48/84], [94mLoss[0m : 1.64559
[1mStep[0m  [56/84], [94mLoss[0m : 1.86808
[1mStep[0m  [64/84], [94mLoss[0m : 1.56523
[1mStep[0m  [72/84], [94mLoss[0m : 1.54483
[1mStep[0m  [80/84], [94mLoss[0m : 1.67529

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61086
[1mStep[0m  [8/84], [94mLoss[0m : 1.68371
[1mStep[0m  [16/84], [94mLoss[0m : 1.68809
[1mStep[0m  [24/84], [94mLoss[0m : 1.59392
[1mStep[0m  [32/84], [94mLoss[0m : 1.88334
[1mStep[0m  [40/84], [94mLoss[0m : 1.65005
[1mStep[0m  [48/84], [94mLoss[0m : 1.50939
[1mStep[0m  [56/84], [94mLoss[0m : 1.74653
[1mStep[0m  [64/84], [94mLoss[0m : 1.48183
[1mStep[0m  [72/84], [94mLoss[0m : 1.68644
[1mStep[0m  [80/84], [94mLoss[0m : 1.75925

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56810
[1mStep[0m  [8/84], [94mLoss[0m : 1.59526
[1mStep[0m  [16/84], [94mLoss[0m : 1.50363
[1mStep[0m  [24/84], [94mLoss[0m : 1.75199
[1mStep[0m  [32/84], [94mLoss[0m : 1.51338
[1mStep[0m  [40/84], [94mLoss[0m : 1.59143
[1mStep[0m  [48/84], [94mLoss[0m : 1.69315
[1mStep[0m  [56/84], [94mLoss[0m : 1.62507
[1mStep[0m  [64/84], [94mLoss[0m : 1.56649
[1mStep[0m  [72/84], [94mLoss[0m : 1.68538
[1mStep[0m  [80/84], [94mLoss[0m : 1.53106

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67440
[1mStep[0m  [8/84], [94mLoss[0m : 1.69194
[1mStep[0m  [16/84], [94mLoss[0m : 1.62380
[1mStep[0m  [24/84], [94mLoss[0m : 1.49184
[1mStep[0m  [32/84], [94mLoss[0m : 1.64016
[1mStep[0m  [40/84], [94mLoss[0m : 1.71602
[1mStep[0m  [48/84], [94mLoss[0m : 1.47403
[1mStep[0m  [56/84], [94mLoss[0m : 1.59382
[1mStep[0m  [64/84], [94mLoss[0m : 1.62856
[1mStep[0m  [72/84], [94mLoss[0m : 1.58911
[1mStep[0m  [80/84], [94mLoss[0m : 1.40431

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.574
====================================

Phase 2 - Evaluation MAE:  2.5740975056375777
MAE score P1        2.325334
MAE score P2        2.574098
loss                1.639129
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.5
weight_decay            0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.95311
[1mStep[0m  [8/84], [94mLoss[0m : 10.28654
[1mStep[0m  [16/84], [94mLoss[0m : 9.08459
[1mStep[0m  [24/84], [94mLoss[0m : 7.83752
[1mStep[0m  [32/84], [94mLoss[0m : 7.09762
[1mStep[0m  [40/84], [94mLoss[0m : 5.52925
[1mStep[0m  [48/84], [94mLoss[0m : 4.77135
[1mStep[0m  [56/84], [94mLoss[0m : 3.58111
[1mStep[0m  [64/84], [94mLoss[0m : 3.43891
[1mStep[0m  [72/84], [94mLoss[0m : 3.58158
[1mStep[0m  [80/84], [94mLoss[0m : 3.23022

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.110, [92mTest[0m: 10.982, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.01102
[1mStep[0m  [8/84], [94mLoss[0m : 2.71647
[1mStep[0m  [16/84], [94mLoss[0m : 2.69134
[1mStep[0m  [24/84], [94mLoss[0m : 2.57751
[1mStep[0m  [32/84], [94mLoss[0m : 2.71864
[1mStep[0m  [40/84], [94mLoss[0m : 2.80782
[1mStep[0m  [48/84], [94mLoss[0m : 2.84309
[1mStep[0m  [56/84], [94mLoss[0m : 2.59574
[1mStep[0m  [64/84], [94mLoss[0m : 2.88106
[1mStep[0m  [72/84], [94mLoss[0m : 2.76205
[1mStep[0m  [80/84], [94mLoss[0m : 3.05518

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.926, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46250
[1mStep[0m  [8/84], [94mLoss[0m : 2.74824
[1mStep[0m  [16/84], [94mLoss[0m : 2.75736
[1mStep[0m  [24/84], [94mLoss[0m : 2.46830
[1mStep[0m  [32/84], [94mLoss[0m : 2.90351
[1mStep[0m  [40/84], [94mLoss[0m : 2.73668
[1mStep[0m  [48/84], [94mLoss[0m : 2.68620
[1mStep[0m  [56/84], [94mLoss[0m : 2.38900
[1mStep[0m  [64/84], [94mLoss[0m : 2.81706
[1mStep[0m  [72/84], [94mLoss[0m : 2.88492
[1mStep[0m  [80/84], [94mLoss[0m : 2.91074

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72815
[1mStep[0m  [8/84], [94mLoss[0m : 2.59177
[1mStep[0m  [16/84], [94mLoss[0m : 2.40636
[1mStep[0m  [24/84], [94mLoss[0m : 2.87731
[1mStep[0m  [32/84], [94mLoss[0m : 2.79997
[1mStep[0m  [40/84], [94mLoss[0m : 2.94426
[1mStep[0m  [48/84], [94mLoss[0m : 2.91573
[1mStep[0m  [56/84], [94mLoss[0m : 2.90717
[1mStep[0m  [64/84], [94mLoss[0m : 2.87569
[1mStep[0m  [72/84], [94mLoss[0m : 2.96665
[1mStep[0m  [80/84], [94mLoss[0m : 2.42325

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72660
[1mStep[0m  [8/84], [94mLoss[0m : 2.69654
[1mStep[0m  [16/84], [94mLoss[0m : 3.08359
[1mStep[0m  [24/84], [94mLoss[0m : 2.54824
[1mStep[0m  [32/84], [94mLoss[0m : 2.59687
[1mStep[0m  [40/84], [94mLoss[0m : 2.61101
[1mStep[0m  [48/84], [94mLoss[0m : 2.87906
[1mStep[0m  [56/84], [94mLoss[0m : 2.39547
[1mStep[0m  [64/84], [94mLoss[0m : 2.69433
[1mStep[0m  [72/84], [94mLoss[0m : 2.44134
[1mStep[0m  [80/84], [94mLoss[0m : 2.40510

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83430
[1mStep[0m  [8/84], [94mLoss[0m : 2.87549
[1mStep[0m  [16/84], [94mLoss[0m : 2.47732
[1mStep[0m  [24/84], [94mLoss[0m : 2.89180
[1mStep[0m  [32/84], [94mLoss[0m : 2.94666
[1mStep[0m  [40/84], [94mLoss[0m : 2.54566
[1mStep[0m  [48/84], [94mLoss[0m : 2.68051
[1mStep[0m  [56/84], [94mLoss[0m : 2.46989
[1mStep[0m  [64/84], [94mLoss[0m : 2.57378
[1mStep[0m  [72/84], [94mLoss[0m : 2.89865
[1mStep[0m  [80/84], [94mLoss[0m : 2.56534

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96059
[1mStep[0m  [8/84], [94mLoss[0m : 2.59467
[1mStep[0m  [16/84], [94mLoss[0m : 3.04706
[1mStep[0m  [24/84], [94mLoss[0m : 2.64272
[1mStep[0m  [32/84], [94mLoss[0m : 2.80484
[1mStep[0m  [40/84], [94mLoss[0m : 3.01540
[1mStep[0m  [48/84], [94mLoss[0m : 2.20036
[1mStep[0m  [56/84], [94mLoss[0m : 2.62208
[1mStep[0m  [64/84], [94mLoss[0m : 2.75593
[1mStep[0m  [72/84], [94mLoss[0m : 2.66897
[1mStep[0m  [80/84], [94mLoss[0m : 2.84020

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80399
[1mStep[0m  [8/84], [94mLoss[0m : 2.32621
[1mStep[0m  [16/84], [94mLoss[0m : 2.66107
[1mStep[0m  [24/84], [94mLoss[0m : 2.47949
[1mStep[0m  [32/84], [94mLoss[0m : 2.74638
[1mStep[0m  [40/84], [94mLoss[0m : 2.58763
[1mStep[0m  [48/84], [94mLoss[0m : 2.62295
[1mStep[0m  [56/84], [94mLoss[0m : 2.57424
[1mStep[0m  [64/84], [94mLoss[0m : 2.67087
[1mStep[0m  [72/84], [94mLoss[0m : 2.57313
[1mStep[0m  [80/84], [94mLoss[0m : 2.63057

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81880
[1mStep[0m  [8/84], [94mLoss[0m : 3.16459
[1mStep[0m  [16/84], [94mLoss[0m : 2.52551
[1mStep[0m  [24/84], [94mLoss[0m : 2.78236
[1mStep[0m  [32/84], [94mLoss[0m : 2.40612
[1mStep[0m  [40/84], [94mLoss[0m : 2.55174
[1mStep[0m  [48/84], [94mLoss[0m : 2.40973
[1mStep[0m  [56/84], [94mLoss[0m : 2.60100
[1mStep[0m  [64/84], [94mLoss[0m : 2.61564
[1mStep[0m  [72/84], [94mLoss[0m : 2.39659
[1mStep[0m  [80/84], [94mLoss[0m : 2.70420

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33476
[1mStep[0m  [8/84], [94mLoss[0m : 2.32645
[1mStep[0m  [16/84], [94mLoss[0m : 2.79574
[1mStep[0m  [24/84], [94mLoss[0m : 2.75105
[1mStep[0m  [32/84], [94mLoss[0m : 2.63360
[1mStep[0m  [40/84], [94mLoss[0m : 2.42700
[1mStep[0m  [48/84], [94mLoss[0m : 2.46343
[1mStep[0m  [56/84], [94mLoss[0m : 2.83947
[1mStep[0m  [64/84], [94mLoss[0m : 2.99574
[1mStep[0m  [72/84], [94mLoss[0m : 2.43952
[1mStep[0m  [80/84], [94mLoss[0m : 2.56041

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46516
[1mStep[0m  [8/84], [94mLoss[0m : 2.88662
[1mStep[0m  [16/84], [94mLoss[0m : 2.65986
[1mStep[0m  [24/84], [94mLoss[0m : 2.31684
[1mStep[0m  [32/84], [94mLoss[0m : 2.52565
[1mStep[0m  [40/84], [94mLoss[0m : 2.44829
[1mStep[0m  [48/84], [94mLoss[0m : 2.62795
[1mStep[0m  [56/84], [94mLoss[0m : 2.83794
[1mStep[0m  [64/84], [94mLoss[0m : 2.50297
[1mStep[0m  [72/84], [94mLoss[0m : 2.34109
[1mStep[0m  [80/84], [94mLoss[0m : 2.78206

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51223
[1mStep[0m  [8/84], [94mLoss[0m : 2.68893
[1mStep[0m  [16/84], [94mLoss[0m : 2.34026
[1mStep[0m  [24/84], [94mLoss[0m : 2.81515
[1mStep[0m  [32/84], [94mLoss[0m : 2.79795
[1mStep[0m  [40/84], [94mLoss[0m : 2.75964
[1mStep[0m  [48/84], [94mLoss[0m : 2.84793
[1mStep[0m  [56/84], [94mLoss[0m : 2.76969
[1mStep[0m  [64/84], [94mLoss[0m : 2.54533
[1mStep[0m  [72/84], [94mLoss[0m : 2.79179
[1mStep[0m  [80/84], [94mLoss[0m : 2.80934

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95908
[1mStep[0m  [8/84], [94mLoss[0m : 2.67546
[1mStep[0m  [16/84], [94mLoss[0m : 2.34686
[1mStep[0m  [24/84], [94mLoss[0m : 2.31658
[1mStep[0m  [32/84], [94mLoss[0m : 2.52428
[1mStep[0m  [40/84], [94mLoss[0m : 2.51883
[1mStep[0m  [48/84], [94mLoss[0m : 2.66557
[1mStep[0m  [56/84], [94mLoss[0m : 2.39288
[1mStep[0m  [64/84], [94mLoss[0m : 2.31900
[1mStep[0m  [72/84], [94mLoss[0m : 2.47066
[1mStep[0m  [80/84], [94mLoss[0m : 2.82171

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23138
[1mStep[0m  [8/84], [94mLoss[0m : 2.62182
[1mStep[0m  [16/84], [94mLoss[0m : 2.59361
[1mStep[0m  [24/84], [94mLoss[0m : 2.40565
[1mStep[0m  [32/84], [94mLoss[0m : 2.51947
[1mStep[0m  [40/84], [94mLoss[0m : 2.54404
[1mStep[0m  [48/84], [94mLoss[0m : 2.78427
[1mStep[0m  [56/84], [94mLoss[0m : 2.42575
[1mStep[0m  [64/84], [94mLoss[0m : 2.66405
[1mStep[0m  [72/84], [94mLoss[0m : 2.73796
[1mStep[0m  [80/84], [94mLoss[0m : 2.28254

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73332
[1mStep[0m  [8/84], [94mLoss[0m : 2.90084
[1mStep[0m  [16/84], [94mLoss[0m : 2.49585
[1mStep[0m  [24/84], [94mLoss[0m : 2.54455
[1mStep[0m  [32/84], [94mLoss[0m : 2.79222
[1mStep[0m  [40/84], [94mLoss[0m : 2.81167
[1mStep[0m  [48/84], [94mLoss[0m : 2.77310
[1mStep[0m  [56/84], [94mLoss[0m : 2.39141
[1mStep[0m  [64/84], [94mLoss[0m : 2.73382
[1mStep[0m  [72/84], [94mLoss[0m : 2.59271
[1mStep[0m  [80/84], [94mLoss[0m : 2.52531

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34035
[1mStep[0m  [8/84], [94mLoss[0m : 2.53147
[1mStep[0m  [16/84], [94mLoss[0m : 2.65054
[1mStep[0m  [24/84], [94mLoss[0m : 2.65222
[1mStep[0m  [32/84], [94mLoss[0m : 2.46354
[1mStep[0m  [40/84], [94mLoss[0m : 2.59001
[1mStep[0m  [48/84], [94mLoss[0m : 2.70245
[1mStep[0m  [56/84], [94mLoss[0m : 2.59675
[1mStep[0m  [64/84], [94mLoss[0m : 2.49475
[1mStep[0m  [72/84], [94mLoss[0m : 2.54542
[1mStep[0m  [80/84], [94mLoss[0m : 2.67216

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77777
[1mStep[0m  [8/84], [94mLoss[0m : 2.54042
[1mStep[0m  [16/84], [94mLoss[0m : 2.79745
[1mStep[0m  [24/84], [94mLoss[0m : 2.52251
[1mStep[0m  [32/84], [94mLoss[0m : 2.39197
[1mStep[0m  [40/84], [94mLoss[0m : 2.76189
[1mStep[0m  [48/84], [94mLoss[0m : 2.50379
[1mStep[0m  [56/84], [94mLoss[0m : 2.12201
[1mStep[0m  [64/84], [94mLoss[0m : 2.75462
[1mStep[0m  [72/84], [94mLoss[0m : 2.68461
[1mStep[0m  [80/84], [94mLoss[0m : 3.12613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35113
[1mStep[0m  [8/84], [94mLoss[0m : 2.61314
[1mStep[0m  [16/84], [94mLoss[0m : 2.82554
[1mStep[0m  [24/84], [94mLoss[0m : 2.64042
[1mStep[0m  [32/84], [94mLoss[0m : 2.43081
[1mStep[0m  [40/84], [94mLoss[0m : 2.72840
[1mStep[0m  [48/84], [94mLoss[0m : 2.51611
[1mStep[0m  [56/84], [94mLoss[0m : 2.68433
[1mStep[0m  [64/84], [94mLoss[0m : 2.84351
[1mStep[0m  [72/84], [94mLoss[0m : 2.92566
[1mStep[0m  [80/84], [94mLoss[0m : 2.62355

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97870
[1mStep[0m  [8/84], [94mLoss[0m : 2.73580
[1mStep[0m  [16/84], [94mLoss[0m : 2.57320
[1mStep[0m  [24/84], [94mLoss[0m : 2.46543
[1mStep[0m  [32/84], [94mLoss[0m : 3.00223
[1mStep[0m  [40/84], [94mLoss[0m : 2.56827
[1mStep[0m  [48/84], [94mLoss[0m : 2.70319
[1mStep[0m  [56/84], [94mLoss[0m : 2.70766
[1mStep[0m  [64/84], [94mLoss[0m : 2.59944
[1mStep[0m  [72/84], [94mLoss[0m : 2.40971
[1mStep[0m  [80/84], [94mLoss[0m : 2.42209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55913
[1mStep[0m  [8/84], [94mLoss[0m : 2.42039
[1mStep[0m  [16/84], [94mLoss[0m : 2.75696
[1mStep[0m  [24/84], [94mLoss[0m : 2.79797
[1mStep[0m  [32/84], [94mLoss[0m : 2.12579
[1mStep[0m  [40/84], [94mLoss[0m : 2.95826
[1mStep[0m  [48/84], [94mLoss[0m : 2.53018
[1mStep[0m  [56/84], [94mLoss[0m : 2.24057
[1mStep[0m  [64/84], [94mLoss[0m : 2.72149
[1mStep[0m  [72/84], [94mLoss[0m : 2.39585
[1mStep[0m  [80/84], [94mLoss[0m : 2.48033

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72425
[1mStep[0m  [8/84], [94mLoss[0m : 2.48873
[1mStep[0m  [16/84], [94mLoss[0m : 2.16415
[1mStep[0m  [24/84], [94mLoss[0m : 2.60817
[1mStep[0m  [32/84], [94mLoss[0m : 2.64531
[1mStep[0m  [40/84], [94mLoss[0m : 2.45668
[1mStep[0m  [48/84], [94mLoss[0m : 2.59840
[1mStep[0m  [56/84], [94mLoss[0m : 2.38774
[1mStep[0m  [64/84], [94mLoss[0m : 2.66793
[1mStep[0m  [72/84], [94mLoss[0m : 2.37289
[1mStep[0m  [80/84], [94mLoss[0m : 2.74949

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35237
[1mStep[0m  [8/84], [94mLoss[0m : 2.65193
[1mStep[0m  [16/84], [94mLoss[0m : 2.79301
[1mStep[0m  [24/84], [94mLoss[0m : 2.75749
[1mStep[0m  [32/84], [94mLoss[0m : 2.45052
[1mStep[0m  [40/84], [94mLoss[0m : 2.86030
[1mStep[0m  [48/84], [94mLoss[0m : 2.62962
[1mStep[0m  [56/84], [94mLoss[0m : 2.77125
[1mStep[0m  [64/84], [94mLoss[0m : 2.60393
[1mStep[0m  [72/84], [94mLoss[0m : 2.56784
[1mStep[0m  [80/84], [94mLoss[0m : 2.58719

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65447
[1mStep[0m  [8/84], [94mLoss[0m : 2.28938
[1mStep[0m  [16/84], [94mLoss[0m : 2.92209
[1mStep[0m  [24/84], [94mLoss[0m : 2.60251
[1mStep[0m  [32/84], [94mLoss[0m : 2.62974
[1mStep[0m  [40/84], [94mLoss[0m : 2.50331
[1mStep[0m  [48/84], [94mLoss[0m : 2.74092
[1mStep[0m  [56/84], [94mLoss[0m : 2.78079
[1mStep[0m  [64/84], [94mLoss[0m : 2.49937
[1mStep[0m  [72/84], [94mLoss[0m : 2.48261
[1mStep[0m  [80/84], [94mLoss[0m : 3.02588

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38235
[1mStep[0m  [8/84], [94mLoss[0m : 2.39398
[1mStep[0m  [16/84], [94mLoss[0m : 2.61780
[1mStep[0m  [24/84], [94mLoss[0m : 2.73131
[1mStep[0m  [32/84], [94mLoss[0m : 2.72535
[1mStep[0m  [40/84], [94mLoss[0m : 2.50601
[1mStep[0m  [48/84], [94mLoss[0m : 2.18732
[1mStep[0m  [56/84], [94mLoss[0m : 2.64937
[1mStep[0m  [64/84], [94mLoss[0m : 2.72240
[1mStep[0m  [72/84], [94mLoss[0m : 2.47311
[1mStep[0m  [80/84], [94mLoss[0m : 2.81857

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59472
[1mStep[0m  [8/84], [94mLoss[0m : 2.44037
[1mStep[0m  [16/84], [94mLoss[0m : 2.78571
[1mStep[0m  [24/84], [94mLoss[0m : 2.91039
[1mStep[0m  [32/84], [94mLoss[0m : 3.05884
[1mStep[0m  [40/84], [94mLoss[0m : 2.65215
[1mStep[0m  [48/84], [94mLoss[0m : 2.78004
[1mStep[0m  [56/84], [94mLoss[0m : 2.41189
[1mStep[0m  [64/84], [94mLoss[0m : 2.38382
[1mStep[0m  [72/84], [94mLoss[0m : 2.80540
[1mStep[0m  [80/84], [94mLoss[0m : 2.23389

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62105
[1mStep[0m  [8/84], [94mLoss[0m : 2.67536
[1mStep[0m  [16/84], [94mLoss[0m : 2.74975
[1mStep[0m  [24/84], [94mLoss[0m : 2.50667
[1mStep[0m  [32/84], [94mLoss[0m : 2.74005
[1mStep[0m  [40/84], [94mLoss[0m : 2.57747
[1mStep[0m  [48/84], [94mLoss[0m : 2.54419
[1mStep[0m  [56/84], [94mLoss[0m : 2.40434
[1mStep[0m  [64/84], [94mLoss[0m : 2.69203
[1mStep[0m  [72/84], [94mLoss[0m : 2.46308
[1mStep[0m  [80/84], [94mLoss[0m : 2.67432

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60809
[1mStep[0m  [8/84], [94mLoss[0m : 2.78165
[1mStep[0m  [16/84], [94mLoss[0m : 2.59843
[1mStep[0m  [24/84], [94mLoss[0m : 2.88544
[1mStep[0m  [32/84], [94mLoss[0m : 2.39295
[1mStep[0m  [40/84], [94mLoss[0m : 2.44671
[1mStep[0m  [48/84], [94mLoss[0m : 2.32889
[1mStep[0m  [56/84], [94mLoss[0m : 2.22868
[1mStep[0m  [64/84], [94mLoss[0m : 2.37412
[1mStep[0m  [72/84], [94mLoss[0m : 2.87407
[1mStep[0m  [80/84], [94mLoss[0m : 2.37119

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67172
[1mStep[0m  [8/84], [94mLoss[0m : 2.37936
[1mStep[0m  [16/84], [94mLoss[0m : 2.69630
[1mStep[0m  [24/84], [94mLoss[0m : 2.72193
[1mStep[0m  [32/84], [94mLoss[0m : 2.57101
[1mStep[0m  [40/84], [94mLoss[0m : 2.73402
[1mStep[0m  [48/84], [94mLoss[0m : 2.54165
[1mStep[0m  [56/84], [94mLoss[0m : 2.03902
[1mStep[0m  [64/84], [94mLoss[0m : 2.82529
[1mStep[0m  [72/84], [94mLoss[0m : 2.61788
[1mStep[0m  [80/84], [94mLoss[0m : 2.74083

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52496
[1mStep[0m  [8/84], [94mLoss[0m : 2.30656
[1mStep[0m  [16/84], [94mLoss[0m : 2.51327
[1mStep[0m  [24/84], [94mLoss[0m : 2.55873
[1mStep[0m  [32/84], [94mLoss[0m : 2.23604
[1mStep[0m  [40/84], [94mLoss[0m : 2.62195
[1mStep[0m  [48/84], [94mLoss[0m : 2.94066
[1mStep[0m  [56/84], [94mLoss[0m : 2.62707
[1mStep[0m  [64/84], [94mLoss[0m : 2.02636
[1mStep[0m  [72/84], [94mLoss[0m : 2.51562
[1mStep[0m  [80/84], [94mLoss[0m : 2.78019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80124
[1mStep[0m  [8/84], [94mLoss[0m : 2.52876
[1mStep[0m  [16/84], [94mLoss[0m : 2.76365
[1mStep[0m  [24/84], [94mLoss[0m : 2.59674
[1mStep[0m  [32/84], [94mLoss[0m : 2.62951
[1mStep[0m  [40/84], [94mLoss[0m : 2.56167
[1mStep[0m  [48/84], [94mLoss[0m : 2.33043
[1mStep[0m  [56/84], [94mLoss[0m : 2.71627
[1mStep[0m  [64/84], [94mLoss[0m : 2.48643
[1mStep[0m  [72/84], [94mLoss[0m : 2.52414
[1mStep[0m  [80/84], [94mLoss[0m : 2.39365

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3317534582955495
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.51500
[1mStep[0m  [8/84], [94mLoss[0m : 2.74660
[1mStep[0m  [16/84], [94mLoss[0m : 2.30963
[1mStep[0m  [24/84], [94mLoss[0m : 2.07406
[1mStep[0m  [32/84], [94mLoss[0m : 2.40087
[1mStep[0m  [40/84], [94mLoss[0m : 2.59111
[1mStep[0m  [48/84], [94mLoss[0m : 2.56686
[1mStep[0m  [56/84], [94mLoss[0m : 2.72593
[1mStep[0m  [64/84], [94mLoss[0m : 2.46665
[1mStep[0m  [72/84], [94mLoss[0m : 2.37752
[1mStep[0m  [80/84], [94mLoss[0m : 2.63028

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51111
[1mStep[0m  [8/84], [94mLoss[0m : 2.69871
[1mStep[0m  [16/84], [94mLoss[0m : 2.34829
[1mStep[0m  [24/84], [94mLoss[0m : 2.50604
[1mStep[0m  [32/84], [94mLoss[0m : 2.52084
[1mStep[0m  [40/84], [94mLoss[0m : 2.69585
[1mStep[0m  [48/84], [94mLoss[0m : 2.27866
[1mStep[0m  [56/84], [94mLoss[0m : 2.59392
[1mStep[0m  [64/84], [94mLoss[0m : 2.77260
[1mStep[0m  [72/84], [94mLoss[0m : 2.58747
[1mStep[0m  [80/84], [94mLoss[0m : 2.42062

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61477
[1mStep[0m  [8/84], [94mLoss[0m : 2.60920
[1mStep[0m  [16/84], [94mLoss[0m : 2.48487
[1mStep[0m  [24/84], [94mLoss[0m : 2.50650
[1mStep[0m  [32/84], [94mLoss[0m : 2.39495
[1mStep[0m  [40/84], [94mLoss[0m : 2.80773
[1mStep[0m  [48/84], [94mLoss[0m : 2.19591
[1mStep[0m  [56/84], [94mLoss[0m : 2.53717
[1mStep[0m  [64/84], [94mLoss[0m : 2.65915
[1mStep[0m  [72/84], [94mLoss[0m : 2.63331
[1mStep[0m  [80/84], [94mLoss[0m : 2.30056

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54965
[1mStep[0m  [8/84], [94mLoss[0m : 2.53794
[1mStep[0m  [16/84], [94mLoss[0m : 2.56763
[1mStep[0m  [24/84], [94mLoss[0m : 2.28512
[1mStep[0m  [32/84], [94mLoss[0m : 2.58495
[1mStep[0m  [40/84], [94mLoss[0m : 2.59492
[1mStep[0m  [48/84], [94mLoss[0m : 2.26692
[1mStep[0m  [56/84], [94mLoss[0m : 2.44555
[1mStep[0m  [64/84], [94mLoss[0m : 2.46007
[1mStep[0m  [72/84], [94mLoss[0m : 2.28665
[1mStep[0m  [80/84], [94mLoss[0m : 2.75205

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79644
[1mStep[0m  [8/84], [94mLoss[0m : 2.31974
[1mStep[0m  [16/84], [94mLoss[0m : 2.32506
[1mStep[0m  [24/84], [94mLoss[0m : 2.45442
[1mStep[0m  [32/84], [94mLoss[0m : 2.81828
[1mStep[0m  [40/84], [94mLoss[0m : 2.12546
[1mStep[0m  [48/84], [94mLoss[0m : 2.43657
[1mStep[0m  [56/84], [94mLoss[0m : 2.66464
[1mStep[0m  [64/84], [94mLoss[0m : 2.35961
[1mStep[0m  [72/84], [94mLoss[0m : 2.62505
[1mStep[0m  [80/84], [94mLoss[0m : 2.34144

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34873
[1mStep[0m  [8/84], [94mLoss[0m : 2.35903
[1mStep[0m  [16/84], [94mLoss[0m : 2.21731
[1mStep[0m  [24/84], [94mLoss[0m : 2.10219
[1mStep[0m  [32/84], [94mLoss[0m : 2.27933
[1mStep[0m  [40/84], [94mLoss[0m : 2.28271
[1mStep[0m  [48/84], [94mLoss[0m : 2.37307
[1mStep[0m  [56/84], [94mLoss[0m : 2.48292
[1mStep[0m  [64/84], [94mLoss[0m : 2.40603
[1mStep[0m  [72/84], [94mLoss[0m : 2.41659
[1mStep[0m  [80/84], [94mLoss[0m : 2.36834

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33745
[1mStep[0m  [8/84], [94mLoss[0m : 2.56822
[1mStep[0m  [16/84], [94mLoss[0m : 2.15196
[1mStep[0m  [24/84], [94mLoss[0m : 2.41176
[1mStep[0m  [32/84], [94mLoss[0m : 2.70733
[1mStep[0m  [40/84], [94mLoss[0m : 2.40166
[1mStep[0m  [48/84], [94mLoss[0m : 2.31211
[1mStep[0m  [56/84], [94mLoss[0m : 2.69308
[1mStep[0m  [64/84], [94mLoss[0m : 2.40570
[1mStep[0m  [72/84], [94mLoss[0m : 2.39201
[1mStep[0m  [80/84], [94mLoss[0m : 2.08535

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25741
[1mStep[0m  [8/84], [94mLoss[0m : 2.15298
[1mStep[0m  [16/84], [94mLoss[0m : 2.25997
[1mStep[0m  [24/84], [94mLoss[0m : 2.40078
[1mStep[0m  [32/84], [94mLoss[0m : 2.45888
[1mStep[0m  [40/84], [94mLoss[0m : 1.96081
[1mStep[0m  [48/84], [94mLoss[0m : 2.42426
[1mStep[0m  [56/84], [94mLoss[0m : 2.51043
[1mStep[0m  [64/84], [94mLoss[0m : 2.32828
[1mStep[0m  [72/84], [94mLoss[0m : 2.29528
[1mStep[0m  [80/84], [94mLoss[0m : 2.15709

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23842
[1mStep[0m  [8/84], [94mLoss[0m : 2.36009
[1mStep[0m  [16/84], [94mLoss[0m : 1.95127
[1mStep[0m  [24/84], [94mLoss[0m : 2.26870
[1mStep[0m  [32/84], [94mLoss[0m : 2.65137
[1mStep[0m  [40/84], [94mLoss[0m : 2.49193
[1mStep[0m  [48/84], [94mLoss[0m : 2.26488
[1mStep[0m  [56/84], [94mLoss[0m : 2.39563
[1mStep[0m  [64/84], [94mLoss[0m : 2.55505
[1mStep[0m  [72/84], [94mLoss[0m : 2.39464
[1mStep[0m  [80/84], [94mLoss[0m : 2.53354

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03384
[1mStep[0m  [8/84], [94mLoss[0m : 2.08815
[1mStep[0m  [16/84], [94mLoss[0m : 2.09937
[1mStep[0m  [24/84], [94mLoss[0m : 2.12308
[1mStep[0m  [32/84], [94mLoss[0m : 2.56746
[1mStep[0m  [40/84], [94mLoss[0m : 2.39321
[1mStep[0m  [48/84], [94mLoss[0m : 2.27516
[1mStep[0m  [56/84], [94mLoss[0m : 2.38622
[1mStep[0m  [64/84], [94mLoss[0m : 2.54436
[1mStep[0m  [72/84], [94mLoss[0m : 2.19457
[1mStep[0m  [80/84], [94mLoss[0m : 2.22762

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30319
[1mStep[0m  [8/84], [94mLoss[0m : 1.84050
[1mStep[0m  [16/84], [94mLoss[0m : 1.96661
[1mStep[0m  [24/84], [94mLoss[0m : 2.40165
[1mStep[0m  [32/84], [94mLoss[0m : 2.41427
[1mStep[0m  [40/84], [94mLoss[0m : 2.33177
[1mStep[0m  [48/84], [94mLoss[0m : 2.26278
[1mStep[0m  [56/84], [94mLoss[0m : 2.37370
[1mStep[0m  [64/84], [94mLoss[0m : 1.87858
[1mStep[0m  [72/84], [94mLoss[0m : 2.07520
[1mStep[0m  [80/84], [94mLoss[0m : 2.45125

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10585
[1mStep[0m  [8/84], [94mLoss[0m : 1.78495
[1mStep[0m  [16/84], [94mLoss[0m : 2.09089
[1mStep[0m  [24/84], [94mLoss[0m : 2.50773
[1mStep[0m  [32/84], [94mLoss[0m : 2.37630
[1mStep[0m  [40/84], [94mLoss[0m : 2.32021
[1mStep[0m  [48/84], [94mLoss[0m : 2.14775
[1mStep[0m  [56/84], [94mLoss[0m : 2.20610
[1mStep[0m  [64/84], [94mLoss[0m : 2.07947
[1mStep[0m  [72/84], [94mLoss[0m : 2.61313
[1mStep[0m  [80/84], [94mLoss[0m : 2.31914

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.197, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11869
[1mStep[0m  [8/84], [94mLoss[0m : 2.20958
[1mStep[0m  [16/84], [94mLoss[0m : 2.51956
[1mStep[0m  [24/84], [94mLoss[0m : 2.15085
[1mStep[0m  [32/84], [94mLoss[0m : 2.34298
[1mStep[0m  [40/84], [94mLoss[0m : 2.27557
[1mStep[0m  [48/84], [94mLoss[0m : 2.16908
[1mStep[0m  [56/84], [94mLoss[0m : 2.06154
[1mStep[0m  [64/84], [94mLoss[0m : 2.31358
[1mStep[0m  [72/84], [94mLoss[0m : 1.89197
[1mStep[0m  [80/84], [94mLoss[0m : 2.26282

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18224
[1mStep[0m  [8/84], [94mLoss[0m : 1.95071
[1mStep[0m  [16/84], [94mLoss[0m : 2.32498
[1mStep[0m  [24/84], [94mLoss[0m : 2.26607
[1mStep[0m  [32/84], [94mLoss[0m : 2.11714
[1mStep[0m  [40/84], [94mLoss[0m : 2.02396
[1mStep[0m  [48/84], [94mLoss[0m : 2.14276
[1mStep[0m  [56/84], [94mLoss[0m : 2.19558
[1mStep[0m  [64/84], [94mLoss[0m : 2.17796
[1mStep[0m  [72/84], [94mLoss[0m : 2.04988
[1mStep[0m  [80/84], [94mLoss[0m : 1.90558

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.566, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77041
[1mStep[0m  [8/84], [94mLoss[0m : 2.10894
[1mStep[0m  [16/84], [94mLoss[0m : 2.10475
[1mStep[0m  [24/84], [94mLoss[0m : 2.17136
[1mStep[0m  [32/84], [94mLoss[0m : 2.07095
[1mStep[0m  [40/84], [94mLoss[0m : 2.22960
[1mStep[0m  [48/84], [94mLoss[0m : 2.05756
[1mStep[0m  [56/84], [94mLoss[0m : 1.99683
[1mStep[0m  [64/84], [94mLoss[0m : 1.82980
[1mStep[0m  [72/84], [94mLoss[0m : 1.94607
[1mStep[0m  [80/84], [94mLoss[0m : 2.03398

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84906
[1mStep[0m  [8/84], [94mLoss[0m : 2.40170
[1mStep[0m  [16/84], [94mLoss[0m : 2.29615
[1mStep[0m  [24/84], [94mLoss[0m : 2.06288
[1mStep[0m  [32/84], [94mLoss[0m : 1.91752
[1mStep[0m  [40/84], [94mLoss[0m : 2.20827
[1mStep[0m  [48/84], [94mLoss[0m : 2.26633
[1mStep[0m  [56/84], [94mLoss[0m : 2.23875
[1mStep[0m  [64/84], [94mLoss[0m : 1.79730
[1mStep[0m  [72/84], [94mLoss[0m : 2.11095
[1mStep[0m  [80/84], [94mLoss[0m : 2.03621

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16929
[1mStep[0m  [8/84], [94mLoss[0m : 1.84824
[1mStep[0m  [16/84], [94mLoss[0m : 1.99786
[1mStep[0m  [24/84], [94mLoss[0m : 2.10257
[1mStep[0m  [32/84], [94mLoss[0m : 1.86685
[1mStep[0m  [40/84], [94mLoss[0m : 2.15559
[1mStep[0m  [48/84], [94mLoss[0m : 1.97218
[1mStep[0m  [56/84], [94mLoss[0m : 1.82684
[1mStep[0m  [64/84], [94mLoss[0m : 1.95762
[1mStep[0m  [72/84], [94mLoss[0m : 1.99865
[1mStep[0m  [80/84], [94mLoss[0m : 1.98543

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97523
[1mStep[0m  [8/84], [94mLoss[0m : 2.17147
[1mStep[0m  [16/84], [94mLoss[0m : 1.96576
[1mStep[0m  [24/84], [94mLoss[0m : 1.97083
[1mStep[0m  [32/84], [94mLoss[0m : 2.20854
[1mStep[0m  [40/84], [94mLoss[0m : 2.02909
[1mStep[0m  [48/84], [94mLoss[0m : 1.97063
[1mStep[0m  [56/84], [94mLoss[0m : 1.86649
[1mStep[0m  [64/84], [94mLoss[0m : 2.29436
[1mStep[0m  [72/84], [94mLoss[0m : 2.09263
[1mStep[0m  [80/84], [94mLoss[0m : 1.99107

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07474
[1mStep[0m  [8/84], [94mLoss[0m : 1.71854
[1mStep[0m  [16/84], [94mLoss[0m : 1.71119
[1mStep[0m  [24/84], [94mLoss[0m : 2.04097
[1mStep[0m  [32/84], [94mLoss[0m : 2.17237
[1mStep[0m  [40/84], [94mLoss[0m : 2.13256
[1mStep[0m  [48/84], [94mLoss[0m : 2.04665
[1mStep[0m  [56/84], [94mLoss[0m : 1.95084
[1mStep[0m  [64/84], [94mLoss[0m : 1.94765
[1mStep[0m  [72/84], [94mLoss[0m : 1.79639
[1mStep[0m  [80/84], [94mLoss[0m : 2.13895

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94753
[1mStep[0m  [8/84], [94mLoss[0m : 2.12868
[1mStep[0m  [16/84], [94mLoss[0m : 1.64447
[1mStep[0m  [24/84], [94mLoss[0m : 2.19798
[1mStep[0m  [32/84], [94mLoss[0m : 1.84509
[1mStep[0m  [40/84], [94mLoss[0m : 2.01810
[1mStep[0m  [48/84], [94mLoss[0m : 2.11450
[1mStep[0m  [56/84], [94mLoss[0m : 1.89541
[1mStep[0m  [64/84], [94mLoss[0m : 1.77194
[1mStep[0m  [72/84], [94mLoss[0m : 2.18937
[1mStep[0m  [80/84], [94mLoss[0m : 2.11948

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.551, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69835
[1mStep[0m  [8/84], [94mLoss[0m : 1.76430
[1mStep[0m  [16/84], [94mLoss[0m : 1.94675
[1mStep[0m  [24/84], [94mLoss[0m : 2.02573
[1mStep[0m  [32/84], [94mLoss[0m : 1.87369
[1mStep[0m  [40/84], [94mLoss[0m : 2.13025
[1mStep[0m  [48/84], [94mLoss[0m : 1.83861
[1mStep[0m  [56/84], [94mLoss[0m : 2.00079
[1mStep[0m  [64/84], [94mLoss[0m : 1.94021
[1mStep[0m  [72/84], [94mLoss[0m : 2.15706
[1mStep[0m  [80/84], [94mLoss[0m : 1.81206

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92598
[1mStep[0m  [8/84], [94mLoss[0m : 1.80227
[1mStep[0m  [16/84], [94mLoss[0m : 1.50978
[1mStep[0m  [24/84], [94mLoss[0m : 1.80808
[1mStep[0m  [32/84], [94mLoss[0m : 1.89288
[1mStep[0m  [40/84], [94mLoss[0m : 1.85993
[1mStep[0m  [48/84], [94mLoss[0m : 2.11597
[1mStep[0m  [56/84], [94mLoss[0m : 1.83142
[1mStep[0m  [64/84], [94mLoss[0m : 2.15327
[1mStep[0m  [72/84], [94mLoss[0m : 1.95097
[1mStep[0m  [80/84], [94mLoss[0m : 2.07591

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77864
[1mStep[0m  [8/84], [94mLoss[0m : 1.82872
[1mStep[0m  [16/84], [94mLoss[0m : 1.81476
[1mStep[0m  [24/84], [94mLoss[0m : 1.78370
[1mStep[0m  [32/84], [94mLoss[0m : 1.80171
[1mStep[0m  [40/84], [94mLoss[0m : 1.74296
[1mStep[0m  [48/84], [94mLoss[0m : 1.76190
[1mStep[0m  [56/84], [94mLoss[0m : 1.91479
[1mStep[0m  [64/84], [94mLoss[0m : 1.76095
[1mStep[0m  [72/84], [94mLoss[0m : 1.68973
[1mStep[0m  [80/84], [94mLoss[0m : 1.90354

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01247
[1mStep[0m  [8/84], [94mLoss[0m : 1.79104
[1mStep[0m  [16/84], [94mLoss[0m : 1.75212
[1mStep[0m  [24/84], [94mLoss[0m : 1.77337
[1mStep[0m  [32/84], [94mLoss[0m : 2.12127
[1mStep[0m  [40/84], [94mLoss[0m : 1.72804
[1mStep[0m  [48/84], [94mLoss[0m : 2.05604
[1mStep[0m  [56/84], [94mLoss[0m : 1.69362
[1mStep[0m  [64/84], [94mLoss[0m : 1.72307
[1mStep[0m  [72/84], [94mLoss[0m : 1.81540
[1mStep[0m  [80/84], [94mLoss[0m : 1.85434

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79207
[1mStep[0m  [8/84], [94mLoss[0m : 1.79851
[1mStep[0m  [16/84], [94mLoss[0m : 1.70279
[1mStep[0m  [24/84], [94mLoss[0m : 1.89786
[1mStep[0m  [32/84], [94mLoss[0m : 1.77988
[1mStep[0m  [40/84], [94mLoss[0m : 1.93082
[1mStep[0m  [48/84], [94mLoss[0m : 2.04510
[1mStep[0m  [56/84], [94mLoss[0m : 1.89280
[1mStep[0m  [64/84], [94mLoss[0m : 1.45076
[1mStep[0m  [72/84], [94mLoss[0m : 1.92008
[1mStep[0m  [80/84], [94mLoss[0m : 1.97443

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00864
[1mStep[0m  [8/84], [94mLoss[0m : 1.58688
[1mStep[0m  [16/84], [94mLoss[0m : 2.03086
[1mStep[0m  [24/84], [94mLoss[0m : 2.13029
[1mStep[0m  [32/84], [94mLoss[0m : 1.69392
[1mStep[0m  [40/84], [94mLoss[0m : 1.86386
[1mStep[0m  [48/84], [94mLoss[0m : 1.67998
[1mStep[0m  [56/84], [94mLoss[0m : 1.79626
[1mStep[0m  [64/84], [94mLoss[0m : 1.67728
[1mStep[0m  [72/84], [94mLoss[0m : 1.74522
[1mStep[0m  [80/84], [94mLoss[0m : 1.70969

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.600, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87461
[1mStep[0m  [8/84], [94mLoss[0m : 1.85066
[1mStep[0m  [16/84], [94mLoss[0m : 1.86824
[1mStep[0m  [24/84], [94mLoss[0m : 2.04271
[1mStep[0m  [32/84], [94mLoss[0m : 1.72801
[1mStep[0m  [40/84], [94mLoss[0m : 1.59816
[1mStep[0m  [48/84], [94mLoss[0m : 2.00759
[1mStep[0m  [56/84], [94mLoss[0m : 1.78902
[1mStep[0m  [64/84], [94mLoss[0m : 1.72212
[1mStep[0m  [72/84], [94mLoss[0m : 1.69802
[1mStep[0m  [80/84], [94mLoss[0m : 1.88690

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76615
[1mStep[0m  [8/84], [94mLoss[0m : 1.77848
[1mStep[0m  [16/84], [94mLoss[0m : 1.93668
[1mStep[0m  [24/84], [94mLoss[0m : 1.51307
[1mStep[0m  [32/84], [94mLoss[0m : 1.81573
[1mStep[0m  [40/84], [94mLoss[0m : 1.76739
[1mStep[0m  [48/84], [94mLoss[0m : 1.80728
[1mStep[0m  [56/84], [94mLoss[0m : 1.77990
[1mStep[0m  [64/84], [94mLoss[0m : 1.67898
[1mStep[0m  [72/84], [94mLoss[0m : 1.77746
[1mStep[0m  [80/84], [94mLoss[0m : 1.87791

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88070
[1mStep[0m  [8/84], [94mLoss[0m : 2.03469
[1mStep[0m  [16/84], [94mLoss[0m : 1.57434
[1mStep[0m  [24/84], [94mLoss[0m : 1.61489
[1mStep[0m  [32/84], [94mLoss[0m : 1.73449
[1mStep[0m  [40/84], [94mLoss[0m : 2.04064
[1mStep[0m  [48/84], [94mLoss[0m : 1.83742
[1mStep[0m  [56/84], [94mLoss[0m : 1.88177
[1mStep[0m  [64/84], [94mLoss[0m : 1.55547
[1mStep[0m  [72/84], [94mLoss[0m : 1.72410
[1mStep[0m  [80/84], [94mLoss[0m : 1.72092

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73513
[1mStep[0m  [8/84], [94mLoss[0m : 1.97484
[1mStep[0m  [16/84], [94mLoss[0m : 1.51138
[1mStep[0m  [24/84], [94mLoss[0m : 1.74444
[1mStep[0m  [32/84], [94mLoss[0m : 2.06559
[1mStep[0m  [40/84], [94mLoss[0m : 1.61054
[1mStep[0m  [48/84], [94mLoss[0m : 1.89476
[1mStep[0m  [56/84], [94mLoss[0m : 1.80687
[1mStep[0m  [64/84], [94mLoss[0m : 2.07715
[1mStep[0m  [72/84], [94mLoss[0m : 2.07082
[1mStep[0m  [80/84], [94mLoss[0m : 1.93313

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.565
====================================

Phase 2 - Evaluation MAE:  2.5651930400303433
MAE score P1       2.331753
MAE score P2       2.565193
loss               1.750395
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.64833
[1mStep[0m  [4/42], [94mLoss[0m : 10.34892
[1mStep[0m  [8/42], [94mLoss[0m : 9.92317
[1mStep[0m  [12/42], [94mLoss[0m : 9.60893
[1mStep[0m  [16/42], [94mLoss[0m : 9.08541
[1mStep[0m  [20/42], [94mLoss[0m : 8.66184
[1mStep[0m  [24/42], [94mLoss[0m : 8.35023
[1mStep[0m  [28/42], [94mLoss[0m : 8.19089
[1mStep[0m  [32/42], [94mLoss[0m : 7.94696
[1mStep[0m  [36/42], [94mLoss[0m : 7.25993
[1mStep[0m  [40/42], [94mLoss[0m : 7.01205

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.763, [92mTest[0m: 10.865, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.62235
[1mStep[0m  [4/42], [94mLoss[0m : 6.99218
[1mStep[0m  [8/42], [94mLoss[0m : 5.95165
[1mStep[0m  [12/42], [94mLoss[0m : 5.96285
[1mStep[0m  [16/42], [94mLoss[0m : 5.38934
[1mStep[0m  [20/42], [94mLoss[0m : 5.45676
[1mStep[0m  [24/42], [94mLoss[0m : 4.69226
[1mStep[0m  [28/42], [94mLoss[0m : 4.32004
[1mStep[0m  [32/42], [94mLoss[0m : 4.39759
[1mStep[0m  [36/42], [94mLoss[0m : 4.08599
[1mStep[0m  [40/42], [94mLoss[0m : 4.13768

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.222, [92mTest[0m: 8.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.81068
[1mStep[0m  [4/42], [94mLoss[0m : 3.34356
[1mStep[0m  [8/42], [94mLoss[0m : 3.65189
[1mStep[0m  [12/42], [94mLoss[0m : 3.66236
[1mStep[0m  [16/42], [94mLoss[0m : 3.13085
[1mStep[0m  [20/42], [94mLoss[0m : 3.21363
[1mStep[0m  [24/42], [94mLoss[0m : 3.05863
[1mStep[0m  [28/42], [94mLoss[0m : 2.99305
[1mStep[0m  [32/42], [94mLoss[0m : 2.82293
[1mStep[0m  [36/42], [94mLoss[0m : 3.10090
[1mStep[0m  [40/42], [94mLoss[0m : 2.97159

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.228, [92mTest[0m: 5.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71604
[1mStep[0m  [4/42], [94mLoss[0m : 2.76068
[1mStep[0m  [8/42], [94mLoss[0m : 3.00009
[1mStep[0m  [12/42], [94mLoss[0m : 2.65857
[1mStep[0m  [16/42], [94mLoss[0m : 3.07397
[1mStep[0m  [20/42], [94mLoss[0m : 2.60649
[1mStep[0m  [24/42], [94mLoss[0m : 3.17804
[1mStep[0m  [28/42], [94mLoss[0m : 2.73850
[1mStep[0m  [32/42], [94mLoss[0m : 2.56685
[1mStep[0m  [36/42], [94mLoss[0m : 2.69720
[1mStep[0m  [40/42], [94mLoss[0m : 2.50064

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.784, [92mTest[0m: 3.873, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66303
[1mStep[0m  [4/42], [94mLoss[0m : 2.72534
[1mStep[0m  [8/42], [94mLoss[0m : 2.66277
[1mStep[0m  [12/42], [94mLoss[0m : 2.65107
[1mStep[0m  [16/42], [94mLoss[0m : 2.79582
[1mStep[0m  [20/42], [94mLoss[0m : 2.87506
[1mStep[0m  [24/42], [94mLoss[0m : 2.60757
[1mStep[0m  [28/42], [94mLoss[0m : 2.73435
[1mStep[0m  [32/42], [94mLoss[0m : 2.59236
[1mStep[0m  [36/42], [94mLoss[0m : 2.51234
[1mStep[0m  [40/42], [94mLoss[0m : 2.74444

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.724, [92mTest[0m: 3.240, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85314
[1mStep[0m  [4/42], [94mLoss[0m : 2.23407
[1mStep[0m  [8/42], [94mLoss[0m : 2.93087
[1mStep[0m  [12/42], [94mLoss[0m : 2.64490
[1mStep[0m  [16/42], [94mLoss[0m : 2.62343
[1mStep[0m  [20/42], [94mLoss[0m : 2.62181
[1mStep[0m  [24/42], [94mLoss[0m : 2.87777
[1mStep[0m  [28/42], [94mLoss[0m : 2.38613
[1mStep[0m  [32/42], [94mLoss[0m : 2.63617
[1mStep[0m  [36/42], [94mLoss[0m : 2.78656
[1mStep[0m  [40/42], [94mLoss[0m : 2.67304

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.671, [92mTest[0m: 3.024, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50985
[1mStep[0m  [4/42], [94mLoss[0m : 2.74524
[1mStep[0m  [8/42], [94mLoss[0m : 2.76465
[1mStep[0m  [12/42], [94mLoss[0m : 2.62434
[1mStep[0m  [16/42], [94mLoss[0m : 2.73836
[1mStep[0m  [20/42], [94mLoss[0m : 2.65466
[1mStep[0m  [24/42], [94mLoss[0m : 2.45600
[1mStep[0m  [28/42], [94mLoss[0m : 2.64638
[1mStep[0m  [32/42], [94mLoss[0m : 2.63235
[1mStep[0m  [36/42], [94mLoss[0m : 2.54546
[1mStep[0m  [40/42], [94mLoss[0m : 2.52747

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.880, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63621
[1mStep[0m  [4/42], [94mLoss[0m : 2.51464
[1mStep[0m  [8/42], [94mLoss[0m : 2.75114
[1mStep[0m  [12/42], [94mLoss[0m : 2.61410
[1mStep[0m  [16/42], [94mLoss[0m : 2.41435
[1mStep[0m  [20/42], [94mLoss[0m : 2.80689
[1mStep[0m  [24/42], [94mLoss[0m : 2.70051
[1mStep[0m  [28/42], [94mLoss[0m : 2.70174
[1mStep[0m  [32/42], [94mLoss[0m : 2.54542
[1mStep[0m  [36/42], [94mLoss[0m : 2.71547
[1mStep[0m  [40/42], [94mLoss[0m : 2.72952

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.787, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56867
[1mStep[0m  [4/42], [94mLoss[0m : 2.78940
[1mStep[0m  [8/42], [94mLoss[0m : 2.56943
[1mStep[0m  [12/42], [94mLoss[0m : 2.84229
[1mStep[0m  [16/42], [94mLoss[0m : 2.68683
[1mStep[0m  [20/42], [94mLoss[0m : 2.79641
[1mStep[0m  [24/42], [94mLoss[0m : 3.02815
[1mStep[0m  [28/42], [94mLoss[0m : 2.57844
[1mStep[0m  [32/42], [94mLoss[0m : 2.83273
[1mStep[0m  [36/42], [94mLoss[0m : 2.60682
[1mStep[0m  [40/42], [94mLoss[0m : 2.51881

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.742, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72364
[1mStep[0m  [4/42], [94mLoss[0m : 2.69105
[1mStep[0m  [8/42], [94mLoss[0m : 2.46111
[1mStep[0m  [12/42], [94mLoss[0m : 2.63190
[1mStep[0m  [16/42], [94mLoss[0m : 2.39708
[1mStep[0m  [20/42], [94mLoss[0m : 2.55487
[1mStep[0m  [24/42], [94mLoss[0m : 2.42261
[1mStep[0m  [28/42], [94mLoss[0m : 2.55699
[1mStep[0m  [32/42], [94mLoss[0m : 2.83106
[1mStep[0m  [36/42], [94mLoss[0m : 2.60866
[1mStep[0m  [40/42], [94mLoss[0m : 2.68188

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.721, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52418
[1mStep[0m  [4/42], [94mLoss[0m : 2.72531
[1mStep[0m  [8/42], [94mLoss[0m : 2.66641
[1mStep[0m  [12/42], [94mLoss[0m : 2.55046
[1mStep[0m  [16/42], [94mLoss[0m : 2.78787
[1mStep[0m  [20/42], [94mLoss[0m : 2.68936
[1mStep[0m  [24/42], [94mLoss[0m : 2.50304
[1mStep[0m  [28/42], [94mLoss[0m : 2.55874
[1mStep[0m  [32/42], [94mLoss[0m : 2.67388
[1mStep[0m  [36/42], [94mLoss[0m : 2.55405
[1mStep[0m  [40/42], [94mLoss[0m : 2.76594

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.641, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62719
[1mStep[0m  [4/42], [94mLoss[0m : 2.65535
[1mStep[0m  [8/42], [94mLoss[0m : 2.60161
[1mStep[0m  [12/42], [94mLoss[0m : 2.74161
[1mStep[0m  [16/42], [94mLoss[0m : 2.59032
[1mStep[0m  [20/42], [94mLoss[0m : 2.45948
[1mStep[0m  [24/42], [94mLoss[0m : 2.41671
[1mStep[0m  [28/42], [94mLoss[0m : 2.66687
[1mStep[0m  [32/42], [94mLoss[0m : 2.34530
[1mStep[0m  [36/42], [94mLoss[0m : 2.51442
[1mStep[0m  [40/42], [94mLoss[0m : 2.65016

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.678, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58150
[1mStep[0m  [4/42], [94mLoss[0m : 2.49651
[1mStep[0m  [8/42], [94mLoss[0m : 2.70760
[1mStep[0m  [12/42], [94mLoss[0m : 2.71065
[1mStep[0m  [16/42], [94mLoss[0m : 2.44433
[1mStep[0m  [20/42], [94mLoss[0m : 2.73273
[1mStep[0m  [24/42], [94mLoss[0m : 2.50816
[1mStep[0m  [28/42], [94mLoss[0m : 2.40408
[1mStep[0m  [32/42], [94mLoss[0m : 2.84779
[1mStep[0m  [36/42], [94mLoss[0m : 2.48313
[1mStep[0m  [40/42], [94mLoss[0m : 2.49050

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.685, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46183
[1mStep[0m  [4/42], [94mLoss[0m : 2.49362
[1mStep[0m  [8/42], [94mLoss[0m : 2.82342
[1mStep[0m  [12/42], [94mLoss[0m : 2.72894
[1mStep[0m  [16/42], [94mLoss[0m : 2.62797
[1mStep[0m  [20/42], [94mLoss[0m : 2.49997
[1mStep[0m  [24/42], [94mLoss[0m : 2.51116
[1mStep[0m  [28/42], [94mLoss[0m : 3.03225
[1mStep[0m  [32/42], [94mLoss[0m : 2.69246
[1mStep[0m  [36/42], [94mLoss[0m : 2.64767
[1mStep[0m  [40/42], [94mLoss[0m : 2.48073

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.608, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47588
[1mStep[0m  [4/42], [94mLoss[0m : 2.57845
[1mStep[0m  [8/42], [94mLoss[0m : 2.64269
[1mStep[0m  [12/42], [94mLoss[0m : 2.72780
[1mStep[0m  [16/42], [94mLoss[0m : 2.57551
[1mStep[0m  [20/42], [94mLoss[0m : 2.62722
[1mStep[0m  [24/42], [94mLoss[0m : 2.43533
[1mStep[0m  [28/42], [94mLoss[0m : 2.74596
[1mStep[0m  [32/42], [94mLoss[0m : 2.66705
[1mStep[0m  [36/42], [94mLoss[0m : 2.58502
[1mStep[0m  [40/42], [94mLoss[0m : 2.71760

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.575, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55762
[1mStep[0m  [4/42], [94mLoss[0m : 2.51418
[1mStep[0m  [8/42], [94mLoss[0m : 2.50027
[1mStep[0m  [12/42], [94mLoss[0m : 2.61897
[1mStep[0m  [16/42], [94mLoss[0m : 2.65446
[1mStep[0m  [20/42], [94mLoss[0m : 2.53813
[1mStep[0m  [24/42], [94mLoss[0m : 2.59782
[1mStep[0m  [28/42], [94mLoss[0m : 2.39139
[1mStep[0m  [32/42], [94mLoss[0m : 2.68914
[1mStep[0m  [36/42], [94mLoss[0m : 2.61753
[1mStep[0m  [40/42], [94mLoss[0m : 2.63220

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.586, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78316
[1mStep[0m  [4/42], [94mLoss[0m : 2.44865
[1mStep[0m  [8/42], [94mLoss[0m : 2.84704
[1mStep[0m  [12/42], [94mLoss[0m : 2.56351
[1mStep[0m  [16/42], [94mLoss[0m : 2.41062
[1mStep[0m  [20/42], [94mLoss[0m : 2.69715
[1mStep[0m  [24/42], [94mLoss[0m : 2.43637
[1mStep[0m  [28/42], [94mLoss[0m : 2.48439
[1mStep[0m  [32/42], [94mLoss[0m : 2.62447
[1mStep[0m  [36/42], [94mLoss[0m : 2.60496
[1mStep[0m  [40/42], [94mLoss[0m : 2.64496

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.558, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72778
[1mStep[0m  [4/42], [94mLoss[0m : 2.63377
[1mStep[0m  [8/42], [94mLoss[0m : 2.67691
[1mStep[0m  [12/42], [94mLoss[0m : 2.59313
[1mStep[0m  [16/42], [94mLoss[0m : 2.58026
[1mStep[0m  [20/42], [94mLoss[0m : 2.66213
[1mStep[0m  [24/42], [94mLoss[0m : 2.59701
[1mStep[0m  [28/42], [94mLoss[0m : 2.62207
[1mStep[0m  [32/42], [94mLoss[0m : 2.55576
[1mStep[0m  [36/42], [94mLoss[0m : 2.45750
[1mStep[0m  [40/42], [94mLoss[0m : 2.62289

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.546, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40609
[1mStep[0m  [4/42], [94mLoss[0m : 2.43853
[1mStep[0m  [8/42], [94mLoss[0m : 2.67568
[1mStep[0m  [12/42], [94mLoss[0m : 2.57169
[1mStep[0m  [16/42], [94mLoss[0m : 2.68062
[1mStep[0m  [20/42], [94mLoss[0m : 2.51980
[1mStep[0m  [24/42], [94mLoss[0m : 2.48023
[1mStep[0m  [28/42], [94mLoss[0m : 2.59544
[1mStep[0m  [32/42], [94mLoss[0m : 2.59089
[1mStep[0m  [36/42], [94mLoss[0m : 2.71019
[1mStep[0m  [40/42], [94mLoss[0m : 2.34727

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.508, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60455
[1mStep[0m  [4/42], [94mLoss[0m : 2.36026
[1mStep[0m  [8/42], [94mLoss[0m : 2.71655
[1mStep[0m  [12/42], [94mLoss[0m : 2.58142
[1mStep[0m  [16/42], [94mLoss[0m : 2.71544
[1mStep[0m  [20/42], [94mLoss[0m : 2.33297
[1mStep[0m  [24/42], [94mLoss[0m : 2.57642
[1mStep[0m  [28/42], [94mLoss[0m : 2.82593
[1mStep[0m  [32/42], [94mLoss[0m : 2.45610
[1mStep[0m  [36/42], [94mLoss[0m : 2.38157
[1mStep[0m  [40/42], [94mLoss[0m : 2.74143

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54544
[1mStep[0m  [4/42], [94mLoss[0m : 2.44937
[1mStep[0m  [8/42], [94mLoss[0m : 2.50475
[1mStep[0m  [12/42], [94mLoss[0m : 2.63174
[1mStep[0m  [16/42], [94mLoss[0m : 2.41233
[1mStep[0m  [20/42], [94mLoss[0m : 2.56140
[1mStep[0m  [24/42], [94mLoss[0m : 2.58779
[1mStep[0m  [28/42], [94mLoss[0m : 2.36956
[1mStep[0m  [32/42], [94mLoss[0m : 2.49415
[1mStep[0m  [36/42], [94mLoss[0m : 2.60404
[1mStep[0m  [40/42], [94mLoss[0m : 2.34801

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38776
[1mStep[0m  [4/42], [94mLoss[0m : 2.40256
[1mStep[0m  [8/42], [94mLoss[0m : 2.61828
[1mStep[0m  [12/42], [94mLoss[0m : 2.37939
[1mStep[0m  [16/42], [94mLoss[0m : 2.48489
[1mStep[0m  [20/42], [94mLoss[0m : 2.46590
[1mStep[0m  [24/42], [94mLoss[0m : 2.60368
[1mStep[0m  [28/42], [94mLoss[0m : 2.41138
[1mStep[0m  [32/42], [94mLoss[0m : 2.50491
[1mStep[0m  [36/42], [94mLoss[0m : 2.50049
[1mStep[0m  [40/42], [94mLoss[0m : 2.63752

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55828
[1mStep[0m  [4/42], [94mLoss[0m : 2.53503
[1mStep[0m  [8/42], [94mLoss[0m : 2.63027
[1mStep[0m  [12/42], [94mLoss[0m : 2.56627
[1mStep[0m  [16/42], [94mLoss[0m : 2.43129
[1mStep[0m  [20/42], [94mLoss[0m : 2.43502
[1mStep[0m  [24/42], [94mLoss[0m : 2.68393
[1mStep[0m  [28/42], [94mLoss[0m : 2.45848
[1mStep[0m  [32/42], [94mLoss[0m : 2.39265
[1mStep[0m  [36/42], [94mLoss[0m : 2.36095
[1mStep[0m  [40/42], [94mLoss[0m : 2.57395

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62141
[1mStep[0m  [4/42], [94mLoss[0m : 2.53833
[1mStep[0m  [8/42], [94mLoss[0m : 2.39804
[1mStep[0m  [12/42], [94mLoss[0m : 2.58856
[1mStep[0m  [16/42], [94mLoss[0m : 2.59917
[1mStep[0m  [20/42], [94mLoss[0m : 2.70248
[1mStep[0m  [24/42], [94mLoss[0m : 2.56249
[1mStep[0m  [28/42], [94mLoss[0m : 2.46578
[1mStep[0m  [32/42], [94mLoss[0m : 2.47279
[1mStep[0m  [36/42], [94mLoss[0m : 2.46933
[1mStep[0m  [40/42], [94mLoss[0m : 2.69833

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46074
[1mStep[0m  [4/42], [94mLoss[0m : 2.71687
[1mStep[0m  [8/42], [94mLoss[0m : 2.72942
[1mStep[0m  [12/42], [94mLoss[0m : 2.53139
[1mStep[0m  [16/42], [94mLoss[0m : 2.46623
[1mStep[0m  [20/42], [94mLoss[0m : 2.56140
[1mStep[0m  [24/42], [94mLoss[0m : 2.56779
[1mStep[0m  [28/42], [94mLoss[0m : 2.52239
[1mStep[0m  [32/42], [94mLoss[0m : 2.50594
[1mStep[0m  [36/42], [94mLoss[0m : 2.46772
[1mStep[0m  [40/42], [94mLoss[0m : 2.04639

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47473
[1mStep[0m  [4/42], [94mLoss[0m : 2.67611
[1mStep[0m  [8/42], [94mLoss[0m : 2.62336
[1mStep[0m  [12/42], [94mLoss[0m : 2.59531
[1mStep[0m  [16/42], [94mLoss[0m : 2.45597
[1mStep[0m  [20/42], [94mLoss[0m : 2.71973
[1mStep[0m  [24/42], [94mLoss[0m : 2.74608
[1mStep[0m  [28/42], [94mLoss[0m : 2.48570
[1mStep[0m  [32/42], [94mLoss[0m : 2.41021
[1mStep[0m  [36/42], [94mLoss[0m : 2.60568
[1mStep[0m  [40/42], [94mLoss[0m : 2.39693

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72654
[1mStep[0m  [4/42], [94mLoss[0m : 2.32411
[1mStep[0m  [8/42], [94mLoss[0m : 2.41466
[1mStep[0m  [12/42], [94mLoss[0m : 2.46423
[1mStep[0m  [16/42], [94mLoss[0m : 2.53966
[1mStep[0m  [20/42], [94mLoss[0m : 2.39736
[1mStep[0m  [24/42], [94mLoss[0m : 2.55895
[1mStep[0m  [28/42], [94mLoss[0m : 2.55763
[1mStep[0m  [32/42], [94mLoss[0m : 2.56015
[1mStep[0m  [36/42], [94mLoss[0m : 2.50098
[1mStep[0m  [40/42], [94mLoss[0m : 2.46849

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48041
[1mStep[0m  [4/42], [94mLoss[0m : 2.34216
[1mStep[0m  [8/42], [94mLoss[0m : 2.39162
[1mStep[0m  [12/42], [94mLoss[0m : 2.69607
[1mStep[0m  [16/42], [94mLoss[0m : 2.48707
[1mStep[0m  [20/42], [94mLoss[0m : 2.56208
[1mStep[0m  [24/42], [94mLoss[0m : 2.43764
[1mStep[0m  [28/42], [94mLoss[0m : 2.69334
[1mStep[0m  [32/42], [94mLoss[0m : 2.45505
[1mStep[0m  [36/42], [94mLoss[0m : 2.33482
[1mStep[0m  [40/42], [94mLoss[0m : 2.45702

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44480
[1mStep[0m  [4/42], [94mLoss[0m : 2.37009
[1mStep[0m  [8/42], [94mLoss[0m : 2.59517
[1mStep[0m  [12/42], [94mLoss[0m : 2.56511
[1mStep[0m  [16/42], [94mLoss[0m : 2.41800
[1mStep[0m  [20/42], [94mLoss[0m : 2.54944
[1mStep[0m  [24/42], [94mLoss[0m : 2.84720
[1mStep[0m  [28/42], [94mLoss[0m : 2.42707
[1mStep[0m  [32/42], [94mLoss[0m : 2.69835
[1mStep[0m  [36/42], [94mLoss[0m : 2.41627
[1mStep[0m  [40/42], [94mLoss[0m : 2.43667

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67343
[1mStep[0m  [4/42], [94mLoss[0m : 2.47480
[1mStep[0m  [8/42], [94mLoss[0m : 2.34353
[1mStep[0m  [12/42], [94mLoss[0m : 2.53480
[1mStep[0m  [16/42], [94mLoss[0m : 2.17356
[1mStep[0m  [20/42], [94mLoss[0m : 2.37841
[1mStep[0m  [24/42], [94mLoss[0m : 2.56126
[1mStep[0m  [28/42], [94mLoss[0m : 2.45174
[1mStep[0m  [32/42], [94mLoss[0m : 2.60208
[1mStep[0m  [36/42], [94mLoss[0m : 2.55153
[1mStep[0m  [40/42], [94mLoss[0m : 2.49449

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.444
====================================

Phase 1 - Evaluation MAE:  2.44390036378588
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.69551
[1mStep[0m  [4/42], [94mLoss[0m : 2.30327
[1mStep[0m  [8/42], [94mLoss[0m : 2.58249
[1mStep[0m  [12/42], [94mLoss[0m : 2.51680
[1mStep[0m  [16/42], [94mLoss[0m : 2.80354
[1mStep[0m  [20/42], [94mLoss[0m : 2.49765
[1mStep[0m  [24/42], [94mLoss[0m : 2.45864
[1mStep[0m  [28/42], [94mLoss[0m : 2.57552
[1mStep[0m  [32/42], [94mLoss[0m : 2.46849
[1mStep[0m  [36/42], [94mLoss[0m : 2.42242
[1mStep[0m  [40/42], [94mLoss[0m : 2.71146

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43660
[1mStep[0m  [4/42], [94mLoss[0m : 2.48548
[1mStep[0m  [8/42], [94mLoss[0m : 2.51628
[1mStep[0m  [12/42], [94mLoss[0m : 2.49849
[1mStep[0m  [16/42], [94mLoss[0m : 2.50129
[1mStep[0m  [20/42], [94mLoss[0m : 2.48209
[1mStep[0m  [24/42], [94mLoss[0m : 2.58764
[1mStep[0m  [28/42], [94mLoss[0m : 2.33846
[1mStep[0m  [32/42], [94mLoss[0m : 2.61190
[1mStep[0m  [36/42], [94mLoss[0m : 2.64649
[1mStep[0m  [40/42], [94mLoss[0m : 2.80581

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65074
[1mStep[0m  [4/42], [94mLoss[0m : 2.48132
[1mStep[0m  [8/42], [94mLoss[0m : 2.98682
[1mStep[0m  [12/42], [94mLoss[0m : 2.61045
[1mStep[0m  [16/42], [94mLoss[0m : 2.57804
[1mStep[0m  [20/42], [94mLoss[0m : 2.41698
[1mStep[0m  [24/42], [94mLoss[0m : 2.39123
[1mStep[0m  [28/42], [94mLoss[0m : 2.45778
[1mStep[0m  [32/42], [94mLoss[0m : 2.62344
[1mStep[0m  [36/42], [94mLoss[0m : 2.53229
[1mStep[0m  [40/42], [94mLoss[0m : 2.46117

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34290
[1mStep[0m  [4/42], [94mLoss[0m : 2.65456
[1mStep[0m  [8/42], [94mLoss[0m : 2.58103
[1mStep[0m  [12/42], [94mLoss[0m : 2.55652
[1mStep[0m  [16/42], [94mLoss[0m : 2.23849
[1mStep[0m  [20/42], [94mLoss[0m : 2.31659
[1mStep[0m  [24/42], [94mLoss[0m : 2.70601
[1mStep[0m  [28/42], [94mLoss[0m : 2.58911
[1mStep[0m  [32/42], [94mLoss[0m : 2.48715
[1mStep[0m  [36/42], [94mLoss[0m : 2.84623
[1mStep[0m  [40/42], [94mLoss[0m : 2.65482

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43563
[1mStep[0m  [4/42], [94mLoss[0m : 2.45871
[1mStep[0m  [8/42], [94mLoss[0m : 2.47176
[1mStep[0m  [12/42], [94mLoss[0m : 2.34589
[1mStep[0m  [16/42], [94mLoss[0m : 2.65790
[1mStep[0m  [20/42], [94mLoss[0m : 2.53420
[1mStep[0m  [24/42], [94mLoss[0m : 2.70529
[1mStep[0m  [28/42], [94mLoss[0m : 2.30444
[1mStep[0m  [32/42], [94mLoss[0m : 2.56199
[1mStep[0m  [36/42], [94mLoss[0m : 2.64197
[1mStep[0m  [40/42], [94mLoss[0m : 2.26200

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38124
[1mStep[0m  [4/42], [94mLoss[0m : 2.58845
[1mStep[0m  [8/42], [94mLoss[0m : 2.53846
[1mStep[0m  [12/42], [94mLoss[0m : 2.43723
[1mStep[0m  [16/42], [94mLoss[0m : 2.52199
[1mStep[0m  [20/42], [94mLoss[0m : 2.34406
[1mStep[0m  [24/42], [94mLoss[0m : 2.41536
[1mStep[0m  [28/42], [94mLoss[0m : 2.39820
[1mStep[0m  [32/42], [94mLoss[0m : 2.73426
[1mStep[0m  [36/42], [94mLoss[0m : 2.62925
[1mStep[0m  [40/42], [94mLoss[0m : 2.34295

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33109
[1mStep[0m  [4/42], [94mLoss[0m : 2.48789
[1mStep[0m  [8/42], [94mLoss[0m : 2.33736
[1mStep[0m  [12/42], [94mLoss[0m : 2.43942
[1mStep[0m  [16/42], [94mLoss[0m : 2.62414
[1mStep[0m  [20/42], [94mLoss[0m : 2.67902
[1mStep[0m  [24/42], [94mLoss[0m : 2.69350
[1mStep[0m  [28/42], [94mLoss[0m : 2.56190
[1mStep[0m  [32/42], [94mLoss[0m : 2.43527
[1mStep[0m  [36/42], [94mLoss[0m : 2.46885
[1mStep[0m  [40/42], [94mLoss[0m : 2.58505

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30648
[1mStep[0m  [4/42], [94mLoss[0m : 2.69481
[1mStep[0m  [8/42], [94mLoss[0m : 2.32242
[1mStep[0m  [12/42], [94mLoss[0m : 2.40356
[1mStep[0m  [16/42], [94mLoss[0m : 2.45516
[1mStep[0m  [20/42], [94mLoss[0m : 2.28842
[1mStep[0m  [24/42], [94mLoss[0m : 2.62093
[1mStep[0m  [28/42], [94mLoss[0m : 2.64380
[1mStep[0m  [32/42], [94mLoss[0m : 2.55348
[1mStep[0m  [36/42], [94mLoss[0m : 2.22957
[1mStep[0m  [40/42], [94mLoss[0m : 2.42130

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56416
[1mStep[0m  [4/42], [94mLoss[0m : 2.26272
[1mStep[0m  [8/42], [94mLoss[0m : 2.50372
[1mStep[0m  [12/42], [94mLoss[0m : 2.63057
[1mStep[0m  [16/42], [94mLoss[0m : 2.36897
[1mStep[0m  [20/42], [94mLoss[0m : 2.45268
[1mStep[0m  [24/42], [94mLoss[0m : 2.58211
[1mStep[0m  [28/42], [94mLoss[0m : 2.50099
[1mStep[0m  [32/42], [94mLoss[0m : 2.62462
[1mStep[0m  [36/42], [94mLoss[0m : 2.64527
[1mStep[0m  [40/42], [94mLoss[0m : 2.47430

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39745
[1mStep[0m  [4/42], [94mLoss[0m : 2.40970
[1mStep[0m  [8/42], [94mLoss[0m : 2.43455
[1mStep[0m  [12/42], [94mLoss[0m : 2.67464
[1mStep[0m  [16/42], [94mLoss[0m : 2.43473
[1mStep[0m  [20/42], [94mLoss[0m : 2.31749
[1mStep[0m  [24/42], [94mLoss[0m : 2.49667
[1mStep[0m  [28/42], [94mLoss[0m : 2.25690
[1mStep[0m  [32/42], [94mLoss[0m : 2.55880
[1mStep[0m  [36/42], [94mLoss[0m : 2.35800
[1mStep[0m  [40/42], [94mLoss[0m : 2.32908

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34700
[1mStep[0m  [4/42], [94mLoss[0m : 2.42685
[1mStep[0m  [8/42], [94mLoss[0m : 2.59477
[1mStep[0m  [12/42], [94mLoss[0m : 2.32538
[1mStep[0m  [16/42], [94mLoss[0m : 2.08605
[1mStep[0m  [20/42], [94mLoss[0m : 2.35224
[1mStep[0m  [24/42], [94mLoss[0m : 2.36877
[1mStep[0m  [28/42], [94mLoss[0m : 2.48787
[1mStep[0m  [32/42], [94mLoss[0m : 2.31705
[1mStep[0m  [36/42], [94mLoss[0m : 2.31778
[1mStep[0m  [40/42], [94mLoss[0m : 2.33148

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61158
[1mStep[0m  [4/42], [94mLoss[0m : 2.43895
[1mStep[0m  [8/42], [94mLoss[0m : 2.41715
[1mStep[0m  [12/42], [94mLoss[0m : 2.37229
[1mStep[0m  [16/42], [94mLoss[0m : 2.44478
[1mStep[0m  [20/42], [94mLoss[0m : 2.12153
[1mStep[0m  [24/42], [94mLoss[0m : 2.38288
[1mStep[0m  [28/42], [94mLoss[0m : 2.59585
[1mStep[0m  [32/42], [94mLoss[0m : 2.43764
[1mStep[0m  [36/42], [94mLoss[0m : 2.43391
[1mStep[0m  [40/42], [94mLoss[0m : 2.36019

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25692
[1mStep[0m  [4/42], [94mLoss[0m : 2.34808
[1mStep[0m  [8/42], [94mLoss[0m : 2.32022
[1mStep[0m  [12/42], [94mLoss[0m : 2.42345
[1mStep[0m  [16/42], [94mLoss[0m : 2.48667
[1mStep[0m  [20/42], [94mLoss[0m : 2.37474
[1mStep[0m  [24/42], [94mLoss[0m : 2.13115
[1mStep[0m  [28/42], [94mLoss[0m : 2.26071
[1mStep[0m  [32/42], [94mLoss[0m : 2.54159
[1mStep[0m  [36/42], [94mLoss[0m : 2.44917
[1mStep[0m  [40/42], [94mLoss[0m : 2.21738

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13796
[1mStep[0m  [4/42], [94mLoss[0m : 2.34597
[1mStep[0m  [8/42], [94mLoss[0m : 2.33696
[1mStep[0m  [12/42], [94mLoss[0m : 2.34229
[1mStep[0m  [16/42], [94mLoss[0m : 2.57301
[1mStep[0m  [20/42], [94mLoss[0m : 2.20760
[1mStep[0m  [24/42], [94mLoss[0m : 2.20663
[1mStep[0m  [28/42], [94mLoss[0m : 2.58629
[1mStep[0m  [32/42], [94mLoss[0m : 2.52878
[1mStep[0m  [36/42], [94mLoss[0m : 2.31474
[1mStep[0m  [40/42], [94mLoss[0m : 2.37140

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.548, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16430
[1mStep[0m  [4/42], [94mLoss[0m : 2.22693
[1mStep[0m  [8/42], [94mLoss[0m : 2.25964
[1mStep[0m  [12/42], [94mLoss[0m : 2.30335
[1mStep[0m  [16/42], [94mLoss[0m : 2.35954
[1mStep[0m  [20/42], [94mLoss[0m : 2.37094
[1mStep[0m  [24/42], [94mLoss[0m : 2.32836
[1mStep[0m  [28/42], [94mLoss[0m : 2.35396
[1mStep[0m  [32/42], [94mLoss[0m : 2.08789
[1mStep[0m  [36/42], [94mLoss[0m : 2.42663
[1mStep[0m  [40/42], [94mLoss[0m : 2.56075

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29153
[1mStep[0m  [4/42], [94mLoss[0m : 2.24872
[1mStep[0m  [8/42], [94mLoss[0m : 2.44332
[1mStep[0m  [12/42], [94mLoss[0m : 2.50909
[1mStep[0m  [16/42], [94mLoss[0m : 2.10729
[1mStep[0m  [20/42], [94mLoss[0m : 2.32355
[1mStep[0m  [24/42], [94mLoss[0m : 2.32434
[1mStep[0m  [28/42], [94mLoss[0m : 2.19227
[1mStep[0m  [32/42], [94mLoss[0m : 2.21812
[1mStep[0m  [36/42], [94mLoss[0m : 2.32721
[1mStep[0m  [40/42], [94mLoss[0m : 2.34141

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.553, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24779
[1mStep[0m  [4/42], [94mLoss[0m : 2.29040
[1mStep[0m  [8/42], [94mLoss[0m : 2.23574
[1mStep[0m  [12/42], [94mLoss[0m : 2.08940
[1mStep[0m  [16/42], [94mLoss[0m : 2.33411
[1mStep[0m  [20/42], [94mLoss[0m : 2.16353
[1mStep[0m  [24/42], [94mLoss[0m : 2.33189
[1mStep[0m  [28/42], [94mLoss[0m : 2.14870
[1mStep[0m  [32/42], [94mLoss[0m : 2.31415
[1mStep[0m  [36/42], [94mLoss[0m : 2.38322
[1mStep[0m  [40/42], [94mLoss[0m : 2.18685

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20984
[1mStep[0m  [4/42], [94mLoss[0m : 2.22243
[1mStep[0m  [8/42], [94mLoss[0m : 2.24660
[1mStep[0m  [12/42], [94mLoss[0m : 2.33637
[1mStep[0m  [16/42], [94mLoss[0m : 2.29459
[1mStep[0m  [20/42], [94mLoss[0m : 2.59549
[1mStep[0m  [24/42], [94mLoss[0m : 2.15361
[1mStep[0m  [28/42], [94mLoss[0m : 2.20247
[1mStep[0m  [32/42], [94mLoss[0m : 2.17399
[1mStep[0m  [36/42], [94mLoss[0m : 2.42998
[1mStep[0m  [40/42], [94mLoss[0m : 2.34881

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.567, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31723
[1mStep[0m  [4/42], [94mLoss[0m : 2.31443
[1mStep[0m  [8/42], [94mLoss[0m : 2.14173
[1mStep[0m  [12/42], [94mLoss[0m : 2.17076
[1mStep[0m  [16/42], [94mLoss[0m : 2.34887
[1mStep[0m  [20/42], [94mLoss[0m : 2.06732
[1mStep[0m  [24/42], [94mLoss[0m : 2.02992
[1mStep[0m  [28/42], [94mLoss[0m : 2.21864
[1mStep[0m  [32/42], [94mLoss[0m : 2.45458
[1mStep[0m  [36/42], [94mLoss[0m : 2.18196
[1mStep[0m  [40/42], [94mLoss[0m : 2.31708

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.583, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36166
[1mStep[0m  [4/42], [94mLoss[0m : 2.26360
[1mStep[0m  [8/42], [94mLoss[0m : 2.26796
[1mStep[0m  [12/42], [94mLoss[0m : 2.28500
[1mStep[0m  [16/42], [94mLoss[0m : 2.34651
[1mStep[0m  [20/42], [94mLoss[0m : 2.39585
[1mStep[0m  [24/42], [94mLoss[0m : 2.28541
[1mStep[0m  [28/42], [94mLoss[0m : 2.05210
[1mStep[0m  [32/42], [94mLoss[0m : 2.07220
[1mStep[0m  [36/42], [94mLoss[0m : 2.13959
[1mStep[0m  [40/42], [94mLoss[0m : 2.16452

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37235
[1mStep[0m  [4/42], [94mLoss[0m : 2.08440
[1mStep[0m  [8/42], [94mLoss[0m : 1.99051
[1mStep[0m  [12/42], [94mLoss[0m : 2.19844
[1mStep[0m  [16/42], [94mLoss[0m : 2.26132
[1mStep[0m  [20/42], [94mLoss[0m : 2.21521
[1mStep[0m  [24/42], [94mLoss[0m : 2.01974
[1mStep[0m  [28/42], [94mLoss[0m : 2.11527
[1mStep[0m  [32/42], [94mLoss[0m : 2.27758
[1mStep[0m  [36/42], [94mLoss[0m : 2.27177
[1mStep[0m  [40/42], [94mLoss[0m : 2.17773

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.573, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03878
[1mStep[0m  [4/42], [94mLoss[0m : 1.95604
[1mStep[0m  [8/42], [94mLoss[0m : 2.25919
[1mStep[0m  [12/42], [94mLoss[0m : 2.11647
[1mStep[0m  [16/42], [94mLoss[0m : 2.13185
[1mStep[0m  [20/42], [94mLoss[0m : 1.94995
[1mStep[0m  [24/42], [94mLoss[0m : 2.26390
[1mStep[0m  [28/42], [94mLoss[0m : 2.22735
[1mStep[0m  [32/42], [94mLoss[0m : 2.03564
[1mStep[0m  [36/42], [94mLoss[0m : 2.06359
[1mStep[0m  [40/42], [94mLoss[0m : 1.99428

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.554, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10871
[1mStep[0m  [4/42], [94mLoss[0m : 2.18761
[1mStep[0m  [8/42], [94mLoss[0m : 2.24813
[1mStep[0m  [12/42], [94mLoss[0m : 1.91155
[1mStep[0m  [16/42], [94mLoss[0m : 2.18456
[1mStep[0m  [20/42], [94mLoss[0m : 2.09472
[1mStep[0m  [24/42], [94mLoss[0m : 2.42383
[1mStep[0m  [28/42], [94mLoss[0m : 2.35572
[1mStep[0m  [32/42], [94mLoss[0m : 2.21987
[1mStep[0m  [36/42], [94mLoss[0m : 2.28421
[1mStep[0m  [40/42], [94mLoss[0m : 2.04501

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.165, [92mTest[0m: 2.589, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95730
[1mStep[0m  [4/42], [94mLoss[0m : 2.28053
[1mStep[0m  [8/42], [94mLoss[0m : 2.16265
[1mStep[0m  [12/42], [94mLoss[0m : 2.08042
[1mStep[0m  [16/42], [94mLoss[0m : 2.09942
[1mStep[0m  [20/42], [94mLoss[0m : 2.18690
[1mStep[0m  [24/42], [94mLoss[0m : 2.27549
[1mStep[0m  [28/42], [94mLoss[0m : 2.01191
[1mStep[0m  [32/42], [94mLoss[0m : 2.16467
[1mStep[0m  [36/42], [94mLoss[0m : 2.17610
[1mStep[0m  [40/42], [94mLoss[0m : 1.95951

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.604, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10626
[1mStep[0m  [4/42], [94mLoss[0m : 2.01093
[1mStep[0m  [8/42], [94mLoss[0m : 2.14820
[1mStep[0m  [12/42], [94mLoss[0m : 2.24656
[1mStep[0m  [16/42], [94mLoss[0m : 2.08684
[1mStep[0m  [20/42], [94mLoss[0m : 2.05265
[1mStep[0m  [24/42], [94mLoss[0m : 2.14214
[1mStep[0m  [28/42], [94mLoss[0m : 2.10035
[1mStep[0m  [32/42], [94mLoss[0m : 2.12599
[1mStep[0m  [36/42], [94mLoss[0m : 2.22155
[1mStep[0m  [40/42], [94mLoss[0m : 2.14127

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.569, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03541
[1mStep[0m  [4/42], [94mLoss[0m : 2.09117
[1mStep[0m  [8/42], [94mLoss[0m : 2.06830
[1mStep[0m  [12/42], [94mLoss[0m : 2.24336
[1mStep[0m  [16/42], [94mLoss[0m : 2.07291
[1mStep[0m  [20/42], [94mLoss[0m : 2.05159
[1mStep[0m  [24/42], [94mLoss[0m : 2.10004
[1mStep[0m  [28/42], [94mLoss[0m : 2.30322
[1mStep[0m  [32/42], [94mLoss[0m : 2.14655
[1mStep[0m  [36/42], [94mLoss[0m : 2.24274
[1mStep[0m  [40/42], [94mLoss[0m : 1.96380

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.589, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99970
[1mStep[0m  [4/42], [94mLoss[0m : 2.06326
[1mStep[0m  [8/42], [94mLoss[0m : 2.20144
[1mStep[0m  [12/42], [94mLoss[0m : 2.16830
[1mStep[0m  [16/42], [94mLoss[0m : 2.11705
[1mStep[0m  [20/42], [94mLoss[0m : 2.28323
[1mStep[0m  [24/42], [94mLoss[0m : 2.17008
[1mStep[0m  [28/42], [94mLoss[0m : 2.06885
[1mStep[0m  [32/42], [94mLoss[0m : 2.01780
[1mStep[0m  [36/42], [94mLoss[0m : 2.10751
[1mStep[0m  [40/42], [94mLoss[0m : 2.07953

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06282
[1mStep[0m  [4/42], [94mLoss[0m : 2.02322
[1mStep[0m  [8/42], [94mLoss[0m : 2.03210
[1mStep[0m  [12/42], [94mLoss[0m : 2.01863
[1mStep[0m  [16/42], [94mLoss[0m : 1.89409
[1mStep[0m  [20/42], [94mLoss[0m : 2.14537
[1mStep[0m  [24/42], [94mLoss[0m : 2.20814
[1mStep[0m  [28/42], [94mLoss[0m : 2.09760
[1mStep[0m  [32/42], [94mLoss[0m : 2.19366
[1mStep[0m  [36/42], [94mLoss[0m : 2.10598
[1mStep[0m  [40/42], [94mLoss[0m : 2.20587

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.059, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78593
[1mStep[0m  [4/42], [94mLoss[0m : 1.94601
[1mStep[0m  [8/42], [94mLoss[0m : 2.01939
[1mStep[0m  [12/42], [94mLoss[0m : 2.10571
[1mStep[0m  [16/42], [94mLoss[0m : 1.83252
[1mStep[0m  [20/42], [94mLoss[0m : 2.02913
[1mStep[0m  [24/42], [94mLoss[0m : 2.08242
[1mStep[0m  [28/42], [94mLoss[0m : 1.85236
[1mStep[0m  [32/42], [94mLoss[0m : 2.03457
[1mStep[0m  [36/42], [94mLoss[0m : 2.18696
[1mStep[0m  [40/42], [94mLoss[0m : 1.79562

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00632
[1mStep[0m  [4/42], [94mLoss[0m : 1.93341
[1mStep[0m  [8/42], [94mLoss[0m : 1.88453
[1mStep[0m  [12/42], [94mLoss[0m : 1.95856
[1mStep[0m  [16/42], [94mLoss[0m : 1.93855
[1mStep[0m  [20/42], [94mLoss[0m : 1.99792
[1mStep[0m  [24/42], [94mLoss[0m : 2.00884
[1mStep[0m  [28/42], [94mLoss[0m : 1.88210
[1mStep[0m  [32/42], [94mLoss[0m : 1.96105
[1mStep[0m  [36/42], [94mLoss[0m : 1.91411
[1mStep[0m  [40/42], [94mLoss[0m : 1.90512

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.649, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.579
====================================

Phase 2 - Evaluation MAE:  2.5794354677200317
MAE score P1          2.4439
MAE score P2        2.579435
loss                1.991197
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay           0.001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
