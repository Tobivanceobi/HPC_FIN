no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  8
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.65455
[1mStep[0m  [4/42], [94mLoss[0m : 9.53774
[1mStep[0m  [8/42], [94mLoss[0m : 6.89105
[1mStep[0m  [12/42], [94mLoss[0m : 3.88212
[1mStep[0m  [16/42], [94mLoss[0m : 2.76407
[1mStep[0m  [20/42], [94mLoss[0m : 3.25151
[1mStep[0m  [24/42], [94mLoss[0m : 3.59166
[1mStep[0m  [28/42], [94mLoss[0m : 3.19884
[1mStep[0m  [32/42], [94mLoss[0m : 2.54273
[1mStep[0m  [36/42], [94mLoss[0m : 2.58063
[1mStep[0m  [40/42], [94mLoss[0m : 2.69051

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.513, [92mTest[0m: 10.719, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.98192
[1mStep[0m  [4/42], [94mLoss[0m : 2.71200
[1mStep[0m  [8/42], [94mLoss[0m : 2.52565
[1mStep[0m  [12/42], [94mLoss[0m : 2.37576
[1mStep[0m  [16/42], [94mLoss[0m : 2.81566
[1mStep[0m  [20/42], [94mLoss[0m : 2.43297
[1mStep[0m  [24/42], [94mLoss[0m : 2.38674
[1mStep[0m  [28/42], [94mLoss[0m : 2.52446
[1mStep[0m  [32/42], [94mLoss[0m : 2.55447
[1mStep[0m  [36/42], [94mLoss[0m : 2.65434
[1mStep[0m  [40/42], [94mLoss[0m : 2.52560

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66818
[1mStep[0m  [4/42], [94mLoss[0m : 2.51288
[1mStep[0m  [8/42], [94mLoss[0m : 2.48976
[1mStep[0m  [12/42], [94mLoss[0m : 2.49947
[1mStep[0m  [16/42], [94mLoss[0m : 2.62396
[1mStep[0m  [20/42], [94mLoss[0m : 2.47492
[1mStep[0m  [24/42], [94mLoss[0m : 2.43607
[1mStep[0m  [28/42], [94mLoss[0m : 2.85240
[1mStep[0m  [32/42], [94mLoss[0m : 2.67487
[1mStep[0m  [36/42], [94mLoss[0m : 2.64736
[1mStep[0m  [40/42], [94mLoss[0m : 2.67403

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40709
[1mStep[0m  [4/42], [94mLoss[0m : 2.54736
[1mStep[0m  [8/42], [94mLoss[0m : 2.40660
[1mStep[0m  [12/42], [94mLoss[0m : 2.57829
[1mStep[0m  [16/42], [94mLoss[0m : 2.61845
[1mStep[0m  [20/42], [94mLoss[0m : 2.64163
[1mStep[0m  [24/42], [94mLoss[0m : 2.48158
[1mStep[0m  [28/42], [94mLoss[0m : 2.49092
[1mStep[0m  [32/42], [94mLoss[0m : 2.49654
[1mStep[0m  [36/42], [94mLoss[0m : 2.34291
[1mStep[0m  [40/42], [94mLoss[0m : 2.38994

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57709
[1mStep[0m  [4/42], [94mLoss[0m : 2.58857
[1mStep[0m  [8/42], [94mLoss[0m : 2.29143
[1mStep[0m  [12/42], [94mLoss[0m : 2.63540
[1mStep[0m  [16/42], [94mLoss[0m : 2.75607
[1mStep[0m  [20/42], [94mLoss[0m : 2.35032
[1mStep[0m  [24/42], [94mLoss[0m : 2.67942
[1mStep[0m  [28/42], [94mLoss[0m : 2.55033
[1mStep[0m  [32/42], [94mLoss[0m : 2.37217
[1mStep[0m  [36/42], [94mLoss[0m : 2.94093
[1mStep[0m  [40/42], [94mLoss[0m : 2.39344

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45712
[1mStep[0m  [4/42], [94mLoss[0m : 2.36138
[1mStep[0m  [8/42], [94mLoss[0m : 2.36925
[1mStep[0m  [12/42], [94mLoss[0m : 2.35571
[1mStep[0m  [16/42], [94mLoss[0m : 2.56380
[1mStep[0m  [20/42], [94mLoss[0m : 2.49302
[1mStep[0m  [24/42], [94mLoss[0m : 2.63946
[1mStep[0m  [28/42], [94mLoss[0m : 2.55972
[1mStep[0m  [32/42], [94mLoss[0m : 2.48236
[1mStep[0m  [36/42], [94mLoss[0m : 2.39889
[1mStep[0m  [40/42], [94mLoss[0m : 2.54494

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39773
[1mStep[0m  [4/42], [94mLoss[0m : 2.65110
[1mStep[0m  [8/42], [94mLoss[0m : 2.61278
[1mStep[0m  [12/42], [94mLoss[0m : 2.33599
[1mStep[0m  [16/42], [94mLoss[0m : 2.35228
[1mStep[0m  [20/42], [94mLoss[0m : 2.36650
[1mStep[0m  [24/42], [94mLoss[0m : 2.43073
[1mStep[0m  [28/42], [94mLoss[0m : 2.48244
[1mStep[0m  [32/42], [94mLoss[0m : 2.71675
[1mStep[0m  [36/42], [94mLoss[0m : 2.37084
[1mStep[0m  [40/42], [94mLoss[0m : 2.41611

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42057
[1mStep[0m  [4/42], [94mLoss[0m : 2.32715
[1mStep[0m  [8/42], [94mLoss[0m : 2.56583
[1mStep[0m  [12/42], [94mLoss[0m : 2.54051
[1mStep[0m  [16/42], [94mLoss[0m : 2.53301
[1mStep[0m  [20/42], [94mLoss[0m : 2.48224
[1mStep[0m  [24/42], [94mLoss[0m : 2.64950
[1mStep[0m  [28/42], [94mLoss[0m : 2.35909
[1mStep[0m  [32/42], [94mLoss[0m : 2.48136
[1mStep[0m  [36/42], [94mLoss[0m : 2.62661
[1mStep[0m  [40/42], [94mLoss[0m : 2.49520

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38251
[1mStep[0m  [4/42], [94mLoss[0m : 2.47625
[1mStep[0m  [8/42], [94mLoss[0m : 2.35976
[1mStep[0m  [12/42], [94mLoss[0m : 2.44637
[1mStep[0m  [16/42], [94mLoss[0m : 2.53147
[1mStep[0m  [20/42], [94mLoss[0m : 2.46291
[1mStep[0m  [24/42], [94mLoss[0m : 2.43088
[1mStep[0m  [28/42], [94mLoss[0m : 2.49191
[1mStep[0m  [32/42], [94mLoss[0m : 2.35375
[1mStep[0m  [36/42], [94mLoss[0m : 2.51018
[1mStep[0m  [40/42], [94mLoss[0m : 2.51576

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46004
[1mStep[0m  [4/42], [94mLoss[0m : 2.16471
[1mStep[0m  [8/42], [94mLoss[0m : 2.36320
[1mStep[0m  [12/42], [94mLoss[0m : 2.60331
[1mStep[0m  [16/42], [94mLoss[0m : 2.37290
[1mStep[0m  [20/42], [94mLoss[0m : 2.38759
[1mStep[0m  [24/42], [94mLoss[0m : 2.52096
[1mStep[0m  [28/42], [94mLoss[0m : 2.37969
[1mStep[0m  [32/42], [94mLoss[0m : 2.48081
[1mStep[0m  [36/42], [94mLoss[0m : 2.59820
[1mStep[0m  [40/42], [94mLoss[0m : 2.56022

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14999
[1mStep[0m  [4/42], [94mLoss[0m : 2.42476
[1mStep[0m  [8/42], [94mLoss[0m : 2.48532
[1mStep[0m  [12/42], [94mLoss[0m : 2.37543
[1mStep[0m  [16/42], [94mLoss[0m : 2.60053
[1mStep[0m  [20/42], [94mLoss[0m : 2.46029
[1mStep[0m  [24/42], [94mLoss[0m : 2.39023
[1mStep[0m  [28/42], [94mLoss[0m : 2.39022
[1mStep[0m  [32/42], [94mLoss[0m : 2.30255
[1mStep[0m  [36/42], [94mLoss[0m : 2.40341
[1mStep[0m  [40/42], [94mLoss[0m : 2.44382

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64342
[1mStep[0m  [4/42], [94mLoss[0m : 2.54139
[1mStep[0m  [8/42], [94mLoss[0m : 2.32167
[1mStep[0m  [12/42], [94mLoss[0m : 2.56170
[1mStep[0m  [16/42], [94mLoss[0m : 2.30641
[1mStep[0m  [20/42], [94mLoss[0m : 2.66358
[1mStep[0m  [24/42], [94mLoss[0m : 2.50202
[1mStep[0m  [28/42], [94mLoss[0m : 2.32576
[1mStep[0m  [32/42], [94mLoss[0m : 2.46096
[1mStep[0m  [36/42], [94mLoss[0m : 2.40464
[1mStep[0m  [40/42], [94mLoss[0m : 2.52637

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31681
[1mStep[0m  [4/42], [94mLoss[0m : 2.45424
[1mStep[0m  [8/42], [94mLoss[0m : 2.47288
[1mStep[0m  [12/42], [94mLoss[0m : 2.40629
[1mStep[0m  [16/42], [94mLoss[0m : 2.63693
[1mStep[0m  [20/42], [94mLoss[0m : 2.58117
[1mStep[0m  [24/42], [94mLoss[0m : 2.46839
[1mStep[0m  [28/42], [94mLoss[0m : 2.71660
[1mStep[0m  [32/42], [94mLoss[0m : 2.41098
[1mStep[0m  [36/42], [94mLoss[0m : 2.45308
[1mStep[0m  [40/42], [94mLoss[0m : 2.36715

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63665
[1mStep[0m  [4/42], [94mLoss[0m : 2.28468
[1mStep[0m  [8/42], [94mLoss[0m : 2.25350
[1mStep[0m  [12/42], [94mLoss[0m : 2.50559
[1mStep[0m  [16/42], [94mLoss[0m : 2.51915
[1mStep[0m  [20/42], [94mLoss[0m : 2.36542
[1mStep[0m  [24/42], [94mLoss[0m : 2.48857
[1mStep[0m  [28/42], [94mLoss[0m : 2.45209
[1mStep[0m  [32/42], [94mLoss[0m : 2.57918
[1mStep[0m  [36/42], [94mLoss[0m : 2.37247
[1mStep[0m  [40/42], [94mLoss[0m : 2.39077

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71607
[1mStep[0m  [4/42], [94mLoss[0m : 2.60019
[1mStep[0m  [8/42], [94mLoss[0m : 2.43201
[1mStep[0m  [12/42], [94mLoss[0m : 2.25496
[1mStep[0m  [16/42], [94mLoss[0m : 2.73851
[1mStep[0m  [20/42], [94mLoss[0m : 2.33199
[1mStep[0m  [24/42], [94mLoss[0m : 2.60462
[1mStep[0m  [28/42], [94mLoss[0m : 2.47705
[1mStep[0m  [32/42], [94mLoss[0m : 2.62048
[1mStep[0m  [36/42], [94mLoss[0m : 2.50058
[1mStep[0m  [40/42], [94mLoss[0m : 2.50693

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46212
[1mStep[0m  [4/42], [94mLoss[0m : 2.51604
[1mStep[0m  [8/42], [94mLoss[0m : 2.39518
[1mStep[0m  [12/42], [94mLoss[0m : 2.38272
[1mStep[0m  [16/42], [94mLoss[0m : 2.34195
[1mStep[0m  [20/42], [94mLoss[0m : 2.52158
[1mStep[0m  [24/42], [94mLoss[0m : 2.53516
[1mStep[0m  [28/42], [94mLoss[0m : 2.54587
[1mStep[0m  [32/42], [94mLoss[0m : 2.30918
[1mStep[0m  [36/42], [94mLoss[0m : 2.43101
[1mStep[0m  [40/42], [94mLoss[0m : 2.65736

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35737
[1mStep[0m  [4/42], [94mLoss[0m : 2.48345
[1mStep[0m  [8/42], [94mLoss[0m : 2.39606
[1mStep[0m  [12/42], [94mLoss[0m : 2.38080
[1mStep[0m  [16/42], [94mLoss[0m : 2.40135
[1mStep[0m  [20/42], [94mLoss[0m : 2.40496
[1mStep[0m  [24/42], [94mLoss[0m : 2.58667
[1mStep[0m  [28/42], [94mLoss[0m : 2.51755
[1mStep[0m  [32/42], [94mLoss[0m : 2.40597
[1mStep[0m  [36/42], [94mLoss[0m : 2.46049
[1mStep[0m  [40/42], [94mLoss[0m : 2.43084

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44977
[1mStep[0m  [4/42], [94mLoss[0m : 2.18492
[1mStep[0m  [8/42], [94mLoss[0m : 2.62211
[1mStep[0m  [12/42], [94mLoss[0m : 2.47976
[1mStep[0m  [16/42], [94mLoss[0m : 2.44475
[1mStep[0m  [20/42], [94mLoss[0m : 2.50238
[1mStep[0m  [24/42], [94mLoss[0m : 2.31514
[1mStep[0m  [28/42], [94mLoss[0m : 2.51503
[1mStep[0m  [32/42], [94mLoss[0m : 2.38890
[1mStep[0m  [36/42], [94mLoss[0m : 2.48287
[1mStep[0m  [40/42], [94mLoss[0m : 2.49078

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58543
[1mStep[0m  [4/42], [94mLoss[0m : 2.47214
[1mStep[0m  [8/42], [94mLoss[0m : 2.68656
[1mStep[0m  [12/42], [94mLoss[0m : 2.31920
[1mStep[0m  [16/42], [94mLoss[0m : 2.51196
[1mStep[0m  [20/42], [94mLoss[0m : 2.38004
[1mStep[0m  [24/42], [94mLoss[0m : 2.57879
[1mStep[0m  [28/42], [94mLoss[0m : 2.65586
[1mStep[0m  [32/42], [94mLoss[0m : 2.25192
[1mStep[0m  [36/42], [94mLoss[0m : 2.24663
[1mStep[0m  [40/42], [94mLoss[0m : 2.40832

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51980
[1mStep[0m  [4/42], [94mLoss[0m : 2.50912
[1mStep[0m  [8/42], [94mLoss[0m : 2.69486
[1mStep[0m  [12/42], [94mLoss[0m : 2.36116
[1mStep[0m  [16/42], [94mLoss[0m : 2.39307
[1mStep[0m  [20/42], [94mLoss[0m : 2.47520
[1mStep[0m  [24/42], [94mLoss[0m : 2.54995
[1mStep[0m  [28/42], [94mLoss[0m : 2.34491
[1mStep[0m  [32/42], [94mLoss[0m : 2.78595
[1mStep[0m  [36/42], [94mLoss[0m : 2.43858
[1mStep[0m  [40/42], [94mLoss[0m : 2.28708

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52127
[1mStep[0m  [4/42], [94mLoss[0m : 2.54517
[1mStep[0m  [8/42], [94mLoss[0m : 2.44734
[1mStep[0m  [12/42], [94mLoss[0m : 2.49088
[1mStep[0m  [16/42], [94mLoss[0m : 2.47014
[1mStep[0m  [20/42], [94mLoss[0m : 2.71710
[1mStep[0m  [24/42], [94mLoss[0m : 2.43593
[1mStep[0m  [28/42], [94mLoss[0m : 2.53839
[1mStep[0m  [32/42], [94mLoss[0m : 2.37150
[1mStep[0m  [36/42], [94mLoss[0m : 2.65994
[1mStep[0m  [40/42], [94mLoss[0m : 2.45071

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47388
[1mStep[0m  [4/42], [94mLoss[0m : 2.47205
[1mStep[0m  [8/42], [94mLoss[0m : 2.35139
[1mStep[0m  [12/42], [94mLoss[0m : 2.54984
[1mStep[0m  [16/42], [94mLoss[0m : 2.50161
[1mStep[0m  [20/42], [94mLoss[0m : 2.19166
[1mStep[0m  [24/42], [94mLoss[0m : 2.59543
[1mStep[0m  [28/42], [94mLoss[0m : 2.47320
[1mStep[0m  [32/42], [94mLoss[0m : 2.55383
[1mStep[0m  [36/42], [94mLoss[0m : 2.56142
[1mStep[0m  [40/42], [94mLoss[0m : 2.52990

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44347
[1mStep[0m  [4/42], [94mLoss[0m : 2.43017
[1mStep[0m  [8/42], [94mLoss[0m : 2.71862
[1mStep[0m  [12/42], [94mLoss[0m : 2.39207
[1mStep[0m  [16/42], [94mLoss[0m : 2.61955
[1mStep[0m  [20/42], [94mLoss[0m : 2.44133
[1mStep[0m  [24/42], [94mLoss[0m : 2.49790
[1mStep[0m  [28/42], [94mLoss[0m : 2.49171
[1mStep[0m  [32/42], [94mLoss[0m : 2.37696
[1mStep[0m  [36/42], [94mLoss[0m : 2.29089
[1mStep[0m  [40/42], [94mLoss[0m : 2.44263

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41622
[1mStep[0m  [4/42], [94mLoss[0m : 2.37111
[1mStep[0m  [8/42], [94mLoss[0m : 2.16885
[1mStep[0m  [12/42], [94mLoss[0m : 2.51637
[1mStep[0m  [16/42], [94mLoss[0m : 2.21668
[1mStep[0m  [20/42], [94mLoss[0m : 2.66521
[1mStep[0m  [24/42], [94mLoss[0m : 2.29279
[1mStep[0m  [28/42], [94mLoss[0m : 2.32905
[1mStep[0m  [32/42], [94mLoss[0m : 2.59330
[1mStep[0m  [36/42], [94mLoss[0m : 2.52732
[1mStep[0m  [40/42], [94mLoss[0m : 2.35691

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67091
[1mStep[0m  [4/42], [94mLoss[0m : 2.43997
[1mStep[0m  [8/42], [94mLoss[0m : 2.44833
[1mStep[0m  [12/42], [94mLoss[0m : 2.53454
[1mStep[0m  [16/42], [94mLoss[0m : 2.36018
[1mStep[0m  [20/42], [94mLoss[0m : 2.54674
[1mStep[0m  [24/42], [94mLoss[0m : 2.55543
[1mStep[0m  [28/42], [94mLoss[0m : 2.72325
[1mStep[0m  [32/42], [94mLoss[0m : 2.28652
[1mStep[0m  [36/42], [94mLoss[0m : 2.67268
[1mStep[0m  [40/42], [94mLoss[0m : 2.33227

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52331
[1mStep[0m  [4/42], [94mLoss[0m : 2.55184
[1mStep[0m  [8/42], [94mLoss[0m : 2.29069
[1mStep[0m  [12/42], [94mLoss[0m : 2.27008
[1mStep[0m  [16/42], [94mLoss[0m : 2.38665
[1mStep[0m  [20/42], [94mLoss[0m : 2.34273
[1mStep[0m  [24/42], [94mLoss[0m : 2.41134
[1mStep[0m  [28/42], [94mLoss[0m : 2.59516
[1mStep[0m  [32/42], [94mLoss[0m : 2.59330
[1mStep[0m  [36/42], [94mLoss[0m : 2.39742
[1mStep[0m  [40/42], [94mLoss[0m : 2.24943

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51562
[1mStep[0m  [4/42], [94mLoss[0m : 2.43702
[1mStep[0m  [8/42], [94mLoss[0m : 2.44590
[1mStep[0m  [12/42], [94mLoss[0m : 2.35584
[1mStep[0m  [16/42], [94mLoss[0m : 2.49000
[1mStep[0m  [20/42], [94mLoss[0m : 2.53535
[1mStep[0m  [24/42], [94mLoss[0m : 2.30754
[1mStep[0m  [28/42], [94mLoss[0m : 2.51558
[1mStep[0m  [32/42], [94mLoss[0m : 2.46727
[1mStep[0m  [36/42], [94mLoss[0m : 2.62423
[1mStep[0m  [40/42], [94mLoss[0m : 2.47476

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67197
[1mStep[0m  [4/42], [94mLoss[0m : 2.43416
[1mStep[0m  [8/42], [94mLoss[0m : 2.38188
[1mStep[0m  [12/42], [94mLoss[0m : 2.57572
[1mStep[0m  [16/42], [94mLoss[0m : 2.41387
[1mStep[0m  [20/42], [94mLoss[0m : 2.16260
[1mStep[0m  [24/42], [94mLoss[0m : 2.42797
[1mStep[0m  [28/42], [94mLoss[0m : 2.41041
[1mStep[0m  [32/42], [94mLoss[0m : 2.30904
[1mStep[0m  [36/42], [94mLoss[0m : 2.62018
[1mStep[0m  [40/42], [94mLoss[0m : 2.36711

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42788
[1mStep[0m  [4/42], [94mLoss[0m : 2.76405
[1mStep[0m  [8/42], [94mLoss[0m : 2.51543
[1mStep[0m  [12/42], [94mLoss[0m : 2.69555
[1mStep[0m  [16/42], [94mLoss[0m : 2.34506
[1mStep[0m  [20/42], [94mLoss[0m : 2.44242
[1mStep[0m  [24/42], [94mLoss[0m : 2.45900
[1mStep[0m  [28/42], [94mLoss[0m : 2.39849
[1mStep[0m  [32/42], [94mLoss[0m : 2.49606
[1mStep[0m  [36/42], [94mLoss[0m : 2.56278
[1mStep[0m  [40/42], [94mLoss[0m : 2.23199

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43879
[1mStep[0m  [4/42], [94mLoss[0m : 2.45497
[1mStep[0m  [8/42], [94mLoss[0m : 2.39148
[1mStep[0m  [12/42], [94mLoss[0m : 2.34338
[1mStep[0m  [16/42], [94mLoss[0m : 2.40254
[1mStep[0m  [20/42], [94mLoss[0m : 2.33432
[1mStep[0m  [24/42], [94mLoss[0m : 2.52287
[1mStep[0m  [28/42], [94mLoss[0m : 2.30890
[1mStep[0m  [32/42], [94mLoss[0m : 2.51139
[1mStep[0m  [36/42], [94mLoss[0m : 2.50718
[1mStep[0m  [40/42], [94mLoss[0m : 2.41272

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3314088753291538
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.39961
[1mStep[0m  [4/42], [94mLoss[0m : 2.53244
[1mStep[0m  [8/42], [94mLoss[0m : 2.17765
[1mStep[0m  [12/42], [94mLoss[0m : 2.43485
[1mStep[0m  [16/42], [94mLoss[0m : 2.41251
[1mStep[0m  [20/42], [94mLoss[0m : 2.51221
[1mStep[0m  [24/42], [94mLoss[0m : 2.43196
[1mStep[0m  [28/42], [94mLoss[0m : 2.47134
[1mStep[0m  [32/42], [94mLoss[0m : 2.32287
[1mStep[0m  [36/42], [94mLoss[0m : 2.35748
[1mStep[0m  [40/42], [94mLoss[0m : 2.44353

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43057
[1mStep[0m  [4/42], [94mLoss[0m : 2.40180
[1mStep[0m  [8/42], [94mLoss[0m : 2.31219
[1mStep[0m  [12/42], [94mLoss[0m : 2.44292
[1mStep[0m  [16/42], [94mLoss[0m : 2.49038
[1mStep[0m  [20/42], [94mLoss[0m : 2.17887
[1mStep[0m  [24/42], [94mLoss[0m : 2.39003
[1mStep[0m  [28/42], [94mLoss[0m : 2.23499
[1mStep[0m  [32/42], [94mLoss[0m : 2.31793
[1mStep[0m  [36/42], [94mLoss[0m : 2.61990
[1mStep[0m  [40/42], [94mLoss[0m : 2.37052

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37021
[1mStep[0m  [4/42], [94mLoss[0m : 2.24022
[1mStep[0m  [8/42], [94mLoss[0m : 2.26682
[1mStep[0m  [12/42], [94mLoss[0m : 2.27600
[1mStep[0m  [16/42], [94mLoss[0m : 2.45098
[1mStep[0m  [20/42], [94mLoss[0m : 2.30637
[1mStep[0m  [24/42], [94mLoss[0m : 2.19467
[1mStep[0m  [28/42], [94mLoss[0m : 2.24987
[1mStep[0m  [32/42], [94mLoss[0m : 2.34925
[1mStep[0m  [36/42], [94mLoss[0m : 2.24928
[1mStep[0m  [40/42], [94mLoss[0m : 2.41412

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26495
[1mStep[0m  [4/42], [94mLoss[0m : 2.22477
[1mStep[0m  [8/42], [94mLoss[0m : 2.26327
[1mStep[0m  [12/42], [94mLoss[0m : 2.21790
[1mStep[0m  [16/42], [94mLoss[0m : 2.09642
[1mStep[0m  [20/42], [94mLoss[0m : 2.31869
[1mStep[0m  [24/42], [94mLoss[0m : 2.29220
[1mStep[0m  [28/42], [94mLoss[0m : 2.36201
[1mStep[0m  [32/42], [94mLoss[0m : 2.33129
[1mStep[0m  [36/42], [94mLoss[0m : 2.47604
[1mStep[0m  [40/42], [94mLoss[0m : 2.42577

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03943
[1mStep[0m  [4/42], [94mLoss[0m : 2.16991
[1mStep[0m  [8/42], [94mLoss[0m : 2.33932
[1mStep[0m  [12/42], [94mLoss[0m : 2.38627
[1mStep[0m  [16/42], [94mLoss[0m : 2.20250
[1mStep[0m  [20/42], [94mLoss[0m : 2.04164
[1mStep[0m  [24/42], [94mLoss[0m : 2.11839
[1mStep[0m  [28/42], [94mLoss[0m : 2.13488
[1mStep[0m  [32/42], [94mLoss[0m : 2.28951
[1mStep[0m  [36/42], [94mLoss[0m : 2.20184
[1mStep[0m  [40/42], [94mLoss[0m : 2.07334

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31617
[1mStep[0m  [4/42], [94mLoss[0m : 2.06326
[1mStep[0m  [8/42], [94mLoss[0m : 1.98746
[1mStep[0m  [12/42], [94mLoss[0m : 2.02664
[1mStep[0m  [16/42], [94mLoss[0m : 1.97482
[1mStep[0m  [20/42], [94mLoss[0m : 2.06147
[1mStep[0m  [24/42], [94mLoss[0m : 1.88869
[1mStep[0m  [28/42], [94mLoss[0m : 2.07399
[1mStep[0m  [32/42], [94mLoss[0m : 2.35637
[1mStep[0m  [36/42], [94mLoss[0m : 2.31303
[1mStep[0m  [40/42], [94mLoss[0m : 2.23417

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99885
[1mStep[0m  [4/42], [94mLoss[0m : 2.12284
[1mStep[0m  [8/42], [94mLoss[0m : 2.24301
[1mStep[0m  [12/42], [94mLoss[0m : 1.95183
[1mStep[0m  [16/42], [94mLoss[0m : 2.09861
[1mStep[0m  [20/42], [94mLoss[0m : 2.08304
[1mStep[0m  [24/42], [94mLoss[0m : 1.95289
[1mStep[0m  [28/42], [94mLoss[0m : 2.15832
[1mStep[0m  [32/42], [94mLoss[0m : 1.98909
[1mStep[0m  [36/42], [94mLoss[0m : 1.95324
[1mStep[0m  [40/42], [94mLoss[0m : 2.27930

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.588, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91836
[1mStep[0m  [4/42], [94mLoss[0m : 1.93565
[1mStep[0m  [8/42], [94mLoss[0m : 1.99617
[1mStep[0m  [12/42], [94mLoss[0m : 2.02343
[1mStep[0m  [16/42], [94mLoss[0m : 1.80348
[1mStep[0m  [20/42], [94mLoss[0m : 2.11063
[1mStep[0m  [24/42], [94mLoss[0m : 2.17575
[1mStep[0m  [28/42], [94mLoss[0m : 2.16704
[1mStep[0m  [32/42], [94mLoss[0m : 2.03309
[1mStep[0m  [36/42], [94mLoss[0m : 2.01827
[1mStep[0m  [40/42], [94mLoss[0m : 1.98535

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83746
[1mStep[0m  [4/42], [94mLoss[0m : 1.96593
[1mStep[0m  [8/42], [94mLoss[0m : 1.80889
[1mStep[0m  [12/42], [94mLoss[0m : 1.86374
[1mStep[0m  [16/42], [94mLoss[0m : 2.04213
[1mStep[0m  [20/42], [94mLoss[0m : 2.09781
[1mStep[0m  [24/42], [94mLoss[0m : 2.00486
[1mStep[0m  [28/42], [94mLoss[0m : 1.85797
[1mStep[0m  [32/42], [94mLoss[0m : 2.22485
[1mStep[0m  [36/42], [94mLoss[0m : 2.13070
[1mStep[0m  [40/42], [94mLoss[0m : 2.11062

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76274
[1mStep[0m  [4/42], [94mLoss[0m : 1.64267
[1mStep[0m  [8/42], [94mLoss[0m : 1.90204
[1mStep[0m  [12/42], [94mLoss[0m : 2.02882
[1mStep[0m  [16/42], [94mLoss[0m : 1.81391
[1mStep[0m  [20/42], [94mLoss[0m : 1.76161
[1mStep[0m  [24/42], [94mLoss[0m : 2.03175
[1mStep[0m  [28/42], [94mLoss[0m : 1.79868
[1mStep[0m  [32/42], [94mLoss[0m : 1.99245
[1mStep[0m  [36/42], [94mLoss[0m : 2.15775
[1mStep[0m  [40/42], [94mLoss[0m : 2.20748

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93524
[1mStep[0m  [4/42], [94mLoss[0m : 1.71981
[1mStep[0m  [8/42], [94mLoss[0m : 1.84806
[1mStep[0m  [12/42], [94mLoss[0m : 1.83036
[1mStep[0m  [16/42], [94mLoss[0m : 1.92254
[1mStep[0m  [20/42], [94mLoss[0m : 1.94004
[1mStep[0m  [24/42], [94mLoss[0m : 2.05383
[1mStep[0m  [28/42], [94mLoss[0m : 1.86739
[1mStep[0m  [32/42], [94mLoss[0m : 1.88013
[1mStep[0m  [36/42], [94mLoss[0m : 1.85204
[1mStep[0m  [40/42], [94mLoss[0m : 2.12491

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.904, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77969
[1mStep[0m  [4/42], [94mLoss[0m : 1.75970
[1mStep[0m  [8/42], [94mLoss[0m : 1.87729
[1mStep[0m  [12/42], [94mLoss[0m : 1.95557
[1mStep[0m  [16/42], [94mLoss[0m : 1.89348
[1mStep[0m  [20/42], [94mLoss[0m : 1.92692
[1mStep[0m  [24/42], [94mLoss[0m : 1.88299
[1mStep[0m  [28/42], [94mLoss[0m : 1.90330
[1mStep[0m  [32/42], [94mLoss[0m : 1.78905
[1mStep[0m  [36/42], [94mLoss[0m : 1.83286
[1mStep[0m  [40/42], [94mLoss[0m : 1.83617

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68677
[1mStep[0m  [4/42], [94mLoss[0m : 1.56122
[1mStep[0m  [8/42], [94mLoss[0m : 1.95587
[1mStep[0m  [12/42], [94mLoss[0m : 1.63298
[1mStep[0m  [16/42], [94mLoss[0m : 1.82095
[1mStep[0m  [20/42], [94mLoss[0m : 1.94988
[1mStep[0m  [24/42], [94mLoss[0m : 1.86363
[1mStep[0m  [28/42], [94mLoss[0m : 1.98043
[1mStep[0m  [32/42], [94mLoss[0m : 2.10495
[1mStep[0m  [36/42], [94mLoss[0m : 1.83099
[1mStep[0m  [40/42], [94mLoss[0m : 1.72651

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.85100
[1mStep[0m  [4/42], [94mLoss[0m : 1.88755
[1mStep[0m  [8/42], [94mLoss[0m : 1.77683
[1mStep[0m  [12/42], [94mLoss[0m : 1.65884
[1mStep[0m  [16/42], [94mLoss[0m : 1.85340
[1mStep[0m  [20/42], [94mLoss[0m : 1.72620
[1mStep[0m  [24/42], [94mLoss[0m : 1.76170
[1mStep[0m  [28/42], [94mLoss[0m : 1.77387
[1mStep[0m  [32/42], [94mLoss[0m : 1.93530
[1mStep[0m  [36/42], [94mLoss[0m : 2.04254
[1mStep[0m  [40/42], [94mLoss[0m : 1.96450

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84882
[1mStep[0m  [4/42], [94mLoss[0m : 1.77420
[1mStep[0m  [8/42], [94mLoss[0m : 1.63452
[1mStep[0m  [12/42], [94mLoss[0m : 1.89618
[1mStep[0m  [16/42], [94mLoss[0m : 1.69276
[1mStep[0m  [20/42], [94mLoss[0m : 1.73092
[1mStep[0m  [24/42], [94mLoss[0m : 1.76254
[1mStep[0m  [28/42], [94mLoss[0m : 1.74895
[1mStep[0m  [32/42], [94mLoss[0m : 1.67899
[1mStep[0m  [36/42], [94mLoss[0m : 1.68820
[1mStep[0m  [40/42], [94mLoss[0m : 1.90095

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79863
[1mStep[0m  [4/42], [94mLoss[0m : 1.64536
[1mStep[0m  [8/42], [94mLoss[0m : 1.86383
[1mStep[0m  [12/42], [94mLoss[0m : 1.78237
[1mStep[0m  [16/42], [94mLoss[0m : 1.85000
[1mStep[0m  [20/42], [94mLoss[0m : 1.80362
[1mStep[0m  [24/42], [94mLoss[0m : 1.60884
[1mStep[0m  [28/42], [94mLoss[0m : 1.78272
[1mStep[0m  [32/42], [94mLoss[0m : 1.68510
[1mStep[0m  [36/42], [94mLoss[0m : 1.64800
[1mStep[0m  [40/42], [94mLoss[0m : 1.75185

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78628
[1mStep[0m  [4/42], [94mLoss[0m : 1.64965
[1mStep[0m  [8/42], [94mLoss[0m : 1.70886
[1mStep[0m  [12/42], [94mLoss[0m : 1.90668
[1mStep[0m  [16/42], [94mLoss[0m : 1.58687
[1mStep[0m  [20/42], [94mLoss[0m : 1.78893
[1mStep[0m  [24/42], [94mLoss[0m : 1.66631
[1mStep[0m  [28/42], [94mLoss[0m : 1.82773
[1mStep[0m  [32/42], [94mLoss[0m : 1.86023
[1mStep[0m  [36/42], [94mLoss[0m : 1.75200
[1mStep[0m  [40/42], [94mLoss[0m : 1.90244

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60646
[1mStep[0m  [4/42], [94mLoss[0m : 1.68879
[1mStep[0m  [8/42], [94mLoss[0m : 1.60793
[1mStep[0m  [12/42], [94mLoss[0m : 1.79806
[1mStep[0m  [16/42], [94mLoss[0m : 1.57867
[1mStep[0m  [20/42], [94mLoss[0m : 1.76355
[1mStep[0m  [24/42], [94mLoss[0m : 1.76792
[1mStep[0m  [28/42], [94mLoss[0m : 1.82006
[1mStep[0m  [32/42], [94mLoss[0m : 1.70333
[1mStep[0m  [36/42], [94mLoss[0m : 1.78282
[1mStep[0m  [40/42], [94mLoss[0m : 1.88554

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.57421
[1mStep[0m  [4/42], [94mLoss[0m : 1.81097
[1mStep[0m  [8/42], [94mLoss[0m : 1.61657
[1mStep[0m  [12/42], [94mLoss[0m : 1.61595
[1mStep[0m  [16/42], [94mLoss[0m : 1.70952
[1mStep[0m  [20/42], [94mLoss[0m : 2.05257
[1mStep[0m  [24/42], [94mLoss[0m : 1.81279
[1mStep[0m  [28/42], [94mLoss[0m : 1.78026
[1mStep[0m  [32/42], [94mLoss[0m : 1.61678
[1mStep[0m  [36/42], [94mLoss[0m : 1.71634
[1mStep[0m  [40/42], [94mLoss[0m : 1.60850

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64702
[1mStep[0m  [4/42], [94mLoss[0m : 1.61928
[1mStep[0m  [8/42], [94mLoss[0m : 1.60746
[1mStep[0m  [12/42], [94mLoss[0m : 1.67201
[1mStep[0m  [16/42], [94mLoss[0m : 1.83974
[1mStep[0m  [20/42], [94mLoss[0m : 1.71308
[1mStep[0m  [24/42], [94mLoss[0m : 1.52093
[1mStep[0m  [28/42], [94mLoss[0m : 1.81788
[1mStep[0m  [32/42], [94mLoss[0m : 1.86495
[1mStep[0m  [36/42], [94mLoss[0m : 1.69173
[1mStep[0m  [40/42], [94mLoss[0m : 1.72659

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61665
[1mStep[0m  [4/42], [94mLoss[0m : 1.62311
[1mStep[0m  [8/42], [94mLoss[0m : 1.55287
[1mStep[0m  [12/42], [94mLoss[0m : 1.69763
[1mStep[0m  [16/42], [94mLoss[0m : 1.78443
[1mStep[0m  [20/42], [94mLoss[0m : 1.70542
[1mStep[0m  [24/42], [94mLoss[0m : 1.74062
[1mStep[0m  [28/42], [94mLoss[0m : 1.70193
[1mStep[0m  [32/42], [94mLoss[0m : 1.64101
[1mStep[0m  [36/42], [94mLoss[0m : 1.61184
[1mStep[0m  [40/42], [94mLoss[0m : 1.64845

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54558
[1mStep[0m  [4/42], [94mLoss[0m : 1.51064
[1mStep[0m  [8/42], [94mLoss[0m : 1.48142
[1mStep[0m  [12/42], [94mLoss[0m : 1.64319
[1mStep[0m  [16/42], [94mLoss[0m : 1.64739
[1mStep[0m  [20/42], [94mLoss[0m : 1.62775
[1mStep[0m  [24/42], [94mLoss[0m : 1.59624
[1mStep[0m  [28/42], [94mLoss[0m : 1.70532
[1mStep[0m  [32/42], [94mLoss[0m : 1.61705
[1mStep[0m  [36/42], [94mLoss[0m : 1.55820
[1mStep[0m  [40/42], [94mLoss[0m : 1.70721

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64056
[1mStep[0m  [4/42], [94mLoss[0m : 1.70036
[1mStep[0m  [8/42], [94mLoss[0m : 1.67491
[1mStep[0m  [12/42], [94mLoss[0m : 1.61284
[1mStep[0m  [16/42], [94mLoss[0m : 1.60998
[1mStep[0m  [20/42], [94mLoss[0m : 1.80004
[1mStep[0m  [24/42], [94mLoss[0m : 1.43260
[1mStep[0m  [28/42], [94mLoss[0m : 1.66814
[1mStep[0m  [32/42], [94mLoss[0m : 1.67407
[1mStep[0m  [36/42], [94mLoss[0m : 1.48704
[1mStep[0m  [40/42], [94mLoss[0m : 1.63613

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69862
[1mStep[0m  [4/42], [94mLoss[0m : 1.76696
[1mStep[0m  [8/42], [94mLoss[0m : 1.66557
[1mStep[0m  [12/42], [94mLoss[0m : 1.69123
[1mStep[0m  [16/42], [94mLoss[0m : 1.65937
[1mStep[0m  [20/42], [94mLoss[0m : 1.42877
[1mStep[0m  [24/42], [94mLoss[0m : 1.65642
[1mStep[0m  [28/42], [94mLoss[0m : 1.63863
[1mStep[0m  [32/42], [94mLoss[0m : 1.51573
[1mStep[0m  [36/42], [94mLoss[0m : 1.67478
[1mStep[0m  [40/42], [94mLoss[0m : 1.80381

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51383
[1mStep[0m  [4/42], [94mLoss[0m : 1.53111
[1mStep[0m  [8/42], [94mLoss[0m : 1.66286
[1mStep[0m  [12/42], [94mLoss[0m : 1.65185
[1mStep[0m  [16/42], [94mLoss[0m : 1.66792
[1mStep[0m  [20/42], [94mLoss[0m : 1.49962
[1mStep[0m  [24/42], [94mLoss[0m : 1.60109
[1mStep[0m  [28/42], [94mLoss[0m : 1.74089
[1mStep[0m  [32/42], [94mLoss[0m : 1.62290
[1mStep[0m  [36/42], [94mLoss[0m : 1.66965
[1mStep[0m  [40/42], [94mLoss[0m : 1.65865

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.51936
[1mStep[0m  [4/42], [94mLoss[0m : 1.51544
[1mStep[0m  [8/42], [94mLoss[0m : 1.49115
[1mStep[0m  [12/42], [94mLoss[0m : 1.53644
[1mStep[0m  [16/42], [94mLoss[0m : 1.77130
[1mStep[0m  [20/42], [94mLoss[0m : 1.67777
[1mStep[0m  [24/42], [94mLoss[0m : 1.44920
[1mStep[0m  [28/42], [94mLoss[0m : 1.64909
[1mStep[0m  [32/42], [94mLoss[0m : 1.45323
[1mStep[0m  [36/42], [94mLoss[0m : 1.53162
[1mStep[0m  [40/42], [94mLoss[0m : 1.60167

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.538, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78826
[1mStep[0m  [4/42], [94mLoss[0m : 1.55326
[1mStep[0m  [8/42], [94mLoss[0m : 1.76279
[1mStep[0m  [12/42], [94mLoss[0m : 1.53613
[1mStep[0m  [16/42], [94mLoss[0m : 1.45019
[1mStep[0m  [20/42], [94mLoss[0m : 1.75700
[1mStep[0m  [24/42], [94mLoss[0m : 1.69242
[1mStep[0m  [28/42], [94mLoss[0m : 1.72235
[1mStep[0m  [32/42], [94mLoss[0m : 1.48846
[1mStep[0m  [36/42], [94mLoss[0m : 1.75543
[1mStep[0m  [40/42], [94mLoss[0m : 1.57614

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59328
[1mStep[0m  [4/42], [94mLoss[0m : 1.72736
[1mStep[0m  [8/42], [94mLoss[0m : 1.56870
[1mStep[0m  [12/42], [94mLoss[0m : 1.64476
[1mStep[0m  [16/42], [94mLoss[0m : 1.66063
[1mStep[0m  [20/42], [94mLoss[0m : 1.57432
[1mStep[0m  [24/42], [94mLoss[0m : 1.65603
[1mStep[0m  [28/42], [94mLoss[0m : 1.53968
[1mStep[0m  [32/42], [94mLoss[0m : 1.64865
[1mStep[0m  [36/42], [94mLoss[0m : 1.63938
[1mStep[0m  [40/42], [94mLoss[0m : 1.64698

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49850
[1mStep[0m  [4/42], [94mLoss[0m : 1.44147
[1mStep[0m  [8/42], [94mLoss[0m : 1.44463
[1mStep[0m  [12/42], [94mLoss[0m : 1.37683
[1mStep[0m  [16/42], [94mLoss[0m : 1.54744
[1mStep[0m  [20/42], [94mLoss[0m : 1.51902
[1mStep[0m  [24/42], [94mLoss[0m : 1.61701
[1mStep[0m  [28/42], [94mLoss[0m : 1.52886
[1mStep[0m  [32/42], [94mLoss[0m : 1.70804
[1mStep[0m  [36/42], [94mLoss[0m : 1.55635
[1mStep[0m  [40/42], [94mLoss[0m : 1.63454

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47715
[1mStep[0m  [4/42], [94mLoss[0m : 1.46835
[1mStep[0m  [8/42], [94mLoss[0m : 1.47859
[1mStep[0m  [12/42], [94mLoss[0m : 1.55930
[1mStep[0m  [16/42], [94mLoss[0m : 1.59505
[1mStep[0m  [20/42], [94mLoss[0m : 1.53460
[1mStep[0m  [24/42], [94mLoss[0m : 1.59761
[1mStep[0m  [28/42], [94mLoss[0m : 1.55766
[1mStep[0m  [32/42], [94mLoss[0m : 1.54975
[1mStep[0m  [36/42], [94mLoss[0m : 1.62898
[1mStep[0m  [40/42], [94mLoss[0m : 1.58475

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.555, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.4935071808951244
MAE score P1        2.331409
MAE score P2        2.493507
loss                1.554979
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay            0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.24375
[1mStep[0m  [8/84], [94mLoss[0m : 10.23195
[1mStep[0m  [16/84], [94mLoss[0m : 9.81770
[1mStep[0m  [24/84], [94mLoss[0m : 9.57229
[1mStep[0m  [32/84], [94mLoss[0m : 9.24912
[1mStep[0m  [40/84], [94mLoss[0m : 8.60641
[1mStep[0m  [48/84], [94mLoss[0m : 8.15046
[1mStep[0m  [56/84], [94mLoss[0m : 7.45900
[1mStep[0m  [64/84], [94mLoss[0m : 7.53394
[1mStep[0m  [72/84], [94mLoss[0m : 6.93345
[1mStep[0m  [80/84], [94mLoss[0m : 6.42715

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.660, [92mTest[0m: 10.885, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.54585
[1mStep[0m  [8/84], [94mLoss[0m : 6.15358
[1mStep[0m  [16/84], [94mLoss[0m : 5.33332
[1mStep[0m  [24/84], [94mLoss[0m : 4.94056
[1mStep[0m  [32/84], [94mLoss[0m : 4.76062
[1mStep[0m  [40/84], [94mLoss[0m : 4.17719
[1mStep[0m  [48/84], [94mLoss[0m : 4.42079
[1mStep[0m  [56/84], [94mLoss[0m : 4.35160
[1mStep[0m  [64/84], [94mLoss[0m : 3.82553
[1mStep[0m  [72/84], [94mLoss[0m : 3.40316
[1mStep[0m  [80/84], [94mLoss[0m : 3.87278

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.661, [92mTest[0m: 7.811, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.45637
[1mStep[0m  [8/84], [94mLoss[0m : 3.34742
[1mStep[0m  [16/84], [94mLoss[0m : 3.51393
[1mStep[0m  [24/84], [94mLoss[0m : 3.42591
[1mStep[0m  [32/84], [94mLoss[0m : 3.38267
[1mStep[0m  [40/84], [94mLoss[0m : 2.83040
[1mStep[0m  [48/84], [94mLoss[0m : 3.08547
[1mStep[0m  [56/84], [94mLoss[0m : 3.00573
[1mStep[0m  [64/84], [94mLoss[0m : 2.95175
[1mStep[0m  [72/84], [94mLoss[0m : 2.85505
[1mStep[0m  [80/84], [94mLoss[0m : 3.00868

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.180, [92mTest[0m: 4.253, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88853
[1mStep[0m  [8/84], [94mLoss[0m : 2.58722
[1mStep[0m  [16/84], [94mLoss[0m : 2.93935
[1mStep[0m  [24/84], [94mLoss[0m : 2.91127
[1mStep[0m  [32/84], [94mLoss[0m : 2.83420
[1mStep[0m  [40/84], [94mLoss[0m : 3.18261
[1mStep[0m  [48/84], [94mLoss[0m : 2.83769
[1mStep[0m  [56/84], [94mLoss[0m : 2.92522
[1mStep[0m  [64/84], [94mLoss[0m : 2.87207
[1mStep[0m  [72/84], [94mLoss[0m : 2.98252
[1mStep[0m  [80/84], [94mLoss[0m : 2.96396

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.983, [92mTest[0m: 3.218, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93379
[1mStep[0m  [8/84], [94mLoss[0m : 3.00291
[1mStep[0m  [16/84], [94mLoss[0m : 2.79822
[1mStep[0m  [24/84], [94mLoss[0m : 2.85718
[1mStep[0m  [32/84], [94mLoss[0m : 2.99073
[1mStep[0m  [40/84], [94mLoss[0m : 3.00508
[1mStep[0m  [48/84], [94mLoss[0m : 3.03981
[1mStep[0m  [56/84], [94mLoss[0m : 2.74641
[1mStep[0m  [64/84], [94mLoss[0m : 3.12379
[1mStep[0m  [72/84], [94mLoss[0m : 3.00181
[1mStep[0m  [80/84], [94mLoss[0m : 2.70564

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.917, [92mTest[0m: 2.824, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86346
[1mStep[0m  [8/84], [94mLoss[0m : 2.99975
[1mStep[0m  [16/84], [94mLoss[0m : 2.90140
[1mStep[0m  [24/84], [94mLoss[0m : 3.33174
[1mStep[0m  [32/84], [94mLoss[0m : 2.71990
[1mStep[0m  [40/84], [94mLoss[0m : 2.86734
[1mStep[0m  [48/84], [94mLoss[0m : 3.18045
[1mStep[0m  [56/84], [94mLoss[0m : 2.79115
[1mStep[0m  [64/84], [94mLoss[0m : 3.33078
[1mStep[0m  [72/84], [94mLoss[0m : 2.86307
[1mStep[0m  [80/84], [94mLoss[0m : 2.58257

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.914, [92mTest[0m: 2.702, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82102
[1mStep[0m  [8/84], [94mLoss[0m : 3.04048
[1mStep[0m  [16/84], [94mLoss[0m : 3.07152
[1mStep[0m  [24/84], [94mLoss[0m : 2.60384
[1mStep[0m  [32/84], [94mLoss[0m : 3.09552
[1mStep[0m  [40/84], [94mLoss[0m : 2.88353
[1mStep[0m  [48/84], [94mLoss[0m : 2.94977
[1mStep[0m  [56/84], [94mLoss[0m : 3.02383
[1mStep[0m  [64/84], [94mLoss[0m : 3.23660
[1mStep[0m  [72/84], [94mLoss[0m : 2.54820
[1mStep[0m  [80/84], [94mLoss[0m : 2.90972

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.879, [92mTest[0m: 2.712, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58764
[1mStep[0m  [8/84], [94mLoss[0m : 2.55156
[1mStep[0m  [16/84], [94mLoss[0m : 2.98252
[1mStep[0m  [24/84], [94mLoss[0m : 2.93279
[1mStep[0m  [32/84], [94mLoss[0m : 3.25924
[1mStep[0m  [40/84], [94mLoss[0m : 2.53616
[1mStep[0m  [48/84], [94mLoss[0m : 2.78088
[1mStep[0m  [56/84], [94mLoss[0m : 2.94598
[1mStep[0m  [64/84], [94mLoss[0m : 2.95969
[1mStep[0m  [72/84], [94mLoss[0m : 2.97060
[1mStep[0m  [80/84], [94mLoss[0m : 3.01082

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.866, [92mTest[0m: 2.636, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.10660
[1mStep[0m  [8/84], [94mLoss[0m : 2.99999
[1mStep[0m  [16/84], [94mLoss[0m : 2.76782
[1mStep[0m  [24/84], [94mLoss[0m : 2.71511
[1mStep[0m  [32/84], [94mLoss[0m : 3.25758
[1mStep[0m  [40/84], [94mLoss[0m : 2.47106
[1mStep[0m  [48/84], [94mLoss[0m : 2.68519
[1mStep[0m  [56/84], [94mLoss[0m : 3.05414
[1mStep[0m  [64/84], [94mLoss[0m : 2.64575
[1mStep[0m  [72/84], [94mLoss[0m : 3.15122
[1mStep[0m  [80/84], [94mLoss[0m : 2.88245

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.844, [92mTest[0m: 2.562, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81606
[1mStep[0m  [8/84], [94mLoss[0m : 2.98621
[1mStep[0m  [16/84], [94mLoss[0m : 2.74585
[1mStep[0m  [24/84], [94mLoss[0m : 2.77966
[1mStep[0m  [32/84], [94mLoss[0m : 2.85454
[1mStep[0m  [40/84], [94mLoss[0m : 3.22350
[1mStep[0m  [48/84], [94mLoss[0m : 3.00999
[1mStep[0m  [56/84], [94mLoss[0m : 3.02458
[1mStep[0m  [64/84], [94mLoss[0m : 3.00579
[1mStep[0m  [72/84], [94mLoss[0m : 3.06657
[1mStep[0m  [80/84], [94mLoss[0m : 2.59877

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.835, [92mTest[0m: 2.562, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.11093
[1mStep[0m  [8/84], [94mLoss[0m : 2.71986
[1mStep[0m  [16/84], [94mLoss[0m : 2.29136
[1mStep[0m  [24/84], [94mLoss[0m : 2.59030
[1mStep[0m  [32/84], [94mLoss[0m : 2.61243
[1mStep[0m  [40/84], [94mLoss[0m : 2.74740
[1mStep[0m  [48/84], [94mLoss[0m : 3.23292
[1mStep[0m  [56/84], [94mLoss[0m : 2.56163
[1mStep[0m  [64/84], [94mLoss[0m : 2.61587
[1mStep[0m  [72/84], [94mLoss[0m : 2.51340
[1mStep[0m  [80/84], [94mLoss[0m : 2.86578

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.840, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68537
[1mStep[0m  [8/84], [94mLoss[0m : 2.87554
[1mStep[0m  [16/84], [94mLoss[0m : 2.82291
[1mStep[0m  [24/84], [94mLoss[0m : 2.69525
[1mStep[0m  [32/84], [94mLoss[0m : 3.16699
[1mStep[0m  [40/84], [94mLoss[0m : 2.92315
[1mStep[0m  [48/84], [94mLoss[0m : 2.74296
[1mStep[0m  [56/84], [94mLoss[0m : 2.65447
[1mStep[0m  [64/84], [94mLoss[0m : 2.93866
[1mStep[0m  [72/84], [94mLoss[0m : 2.80667
[1mStep[0m  [80/84], [94mLoss[0m : 2.88062

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.831, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.13842
[1mStep[0m  [8/84], [94mLoss[0m : 2.78950
[1mStep[0m  [16/84], [94mLoss[0m : 2.75799
[1mStep[0m  [24/84], [94mLoss[0m : 2.91193
[1mStep[0m  [32/84], [94mLoss[0m : 2.64604
[1mStep[0m  [40/84], [94mLoss[0m : 2.83480
[1mStep[0m  [48/84], [94mLoss[0m : 2.32913
[1mStep[0m  [56/84], [94mLoss[0m : 2.83274
[1mStep[0m  [64/84], [94mLoss[0m : 2.79932
[1mStep[0m  [72/84], [94mLoss[0m : 2.60791
[1mStep[0m  [80/84], [94mLoss[0m : 2.96358

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.785, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.33462
[1mStep[0m  [8/84], [94mLoss[0m : 2.93123
[1mStep[0m  [16/84], [94mLoss[0m : 2.83263
[1mStep[0m  [24/84], [94mLoss[0m : 2.88190
[1mStep[0m  [32/84], [94mLoss[0m : 2.75679
[1mStep[0m  [40/84], [94mLoss[0m : 2.75378
[1mStep[0m  [48/84], [94mLoss[0m : 2.93077
[1mStep[0m  [56/84], [94mLoss[0m : 2.78040
[1mStep[0m  [64/84], [94mLoss[0m : 2.37110
[1mStep[0m  [72/84], [94mLoss[0m : 2.77014
[1mStep[0m  [80/84], [94mLoss[0m : 2.84070

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.760, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57561
[1mStep[0m  [8/84], [94mLoss[0m : 2.59108
[1mStep[0m  [16/84], [94mLoss[0m : 3.03911
[1mStep[0m  [24/84], [94mLoss[0m : 2.54578
[1mStep[0m  [32/84], [94mLoss[0m : 2.65437
[1mStep[0m  [40/84], [94mLoss[0m : 2.81787
[1mStep[0m  [48/84], [94mLoss[0m : 2.65953
[1mStep[0m  [56/84], [94mLoss[0m : 2.35148
[1mStep[0m  [64/84], [94mLoss[0m : 2.86490
[1mStep[0m  [72/84], [94mLoss[0m : 3.02263
[1mStep[0m  [80/84], [94mLoss[0m : 2.76148

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.754, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47630
[1mStep[0m  [8/84], [94mLoss[0m : 2.79381
[1mStep[0m  [16/84], [94mLoss[0m : 2.80194
[1mStep[0m  [24/84], [94mLoss[0m : 2.77228
[1mStep[0m  [32/84], [94mLoss[0m : 3.06987
[1mStep[0m  [40/84], [94mLoss[0m : 2.65931
[1mStep[0m  [48/84], [94mLoss[0m : 2.82636
[1mStep[0m  [56/84], [94mLoss[0m : 2.82303
[1mStep[0m  [64/84], [94mLoss[0m : 2.41174
[1mStep[0m  [72/84], [94mLoss[0m : 2.75781
[1mStep[0m  [80/84], [94mLoss[0m : 2.63747

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.787, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61960
[1mStep[0m  [8/84], [94mLoss[0m : 2.76575
[1mStep[0m  [16/84], [94mLoss[0m : 2.66298
[1mStep[0m  [24/84], [94mLoss[0m : 2.76295
[1mStep[0m  [32/84], [94mLoss[0m : 2.58348
[1mStep[0m  [40/84], [94mLoss[0m : 2.71397
[1mStep[0m  [48/84], [94mLoss[0m : 2.36701
[1mStep[0m  [56/84], [94mLoss[0m : 2.53102
[1mStep[0m  [64/84], [94mLoss[0m : 2.66873
[1mStep[0m  [72/84], [94mLoss[0m : 2.78465
[1mStep[0m  [80/84], [94mLoss[0m : 2.36187

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.730, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62357
[1mStep[0m  [8/84], [94mLoss[0m : 2.42513
[1mStep[0m  [16/84], [94mLoss[0m : 2.82034
[1mStep[0m  [24/84], [94mLoss[0m : 2.88221
[1mStep[0m  [32/84], [94mLoss[0m : 2.79632
[1mStep[0m  [40/84], [94mLoss[0m : 2.68247
[1mStep[0m  [48/84], [94mLoss[0m : 3.12355
[1mStep[0m  [56/84], [94mLoss[0m : 2.46972
[1mStep[0m  [64/84], [94mLoss[0m : 2.86997
[1mStep[0m  [72/84], [94mLoss[0m : 2.66113
[1mStep[0m  [80/84], [94mLoss[0m : 2.96456

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42517
[1mStep[0m  [8/84], [94mLoss[0m : 3.05142
[1mStep[0m  [16/84], [94mLoss[0m : 2.65208
[1mStep[0m  [24/84], [94mLoss[0m : 2.97001
[1mStep[0m  [32/84], [94mLoss[0m : 2.54417
[1mStep[0m  [40/84], [94mLoss[0m : 2.49478
[1mStep[0m  [48/84], [94mLoss[0m : 2.73181
[1mStep[0m  [56/84], [94mLoss[0m : 2.69288
[1mStep[0m  [64/84], [94mLoss[0m : 2.65797
[1mStep[0m  [72/84], [94mLoss[0m : 3.12122
[1mStep[0m  [80/84], [94mLoss[0m : 2.45343

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70352
[1mStep[0m  [8/84], [94mLoss[0m : 2.68236
[1mStep[0m  [16/84], [94mLoss[0m : 2.88215
[1mStep[0m  [24/84], [94mLoss[0m : 2.59958
[1mStep[0m  [32/84], [94mLoss[0m : 2.77237
[1mStep[0m  [40/84], [94mLoss[0m : 2.73225
[1mStep[0m  [48/84], [94mLoss[0m : 2.53288
[1mStep[0m  [56/84], [94mLoss[0m : 2.83674
[1mStep[0m  [64/84], [94mLoss[0m : 2.54409
[1mStep[0m  [72/84], [94mLoss[0m : 2.61633
[1mStep[0m  [80/84], [94mLoss[0m : 2.61426

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61143
[1mStep[0m  [8/84], [94mLoss[0m : 2.95879
[1mStep[0m  [16/84], [94mLoss[0m : 2.60090
[1mStep[0m  [24/84], [94mLoss[0m : 2.64161
[1mStep[0m  [32/84], [94mLoss[0m : 2.88296
[1mStep[0m  [40/84], [94mLoss[0m : 2.66707
[1mStep[0m  [48/84], [94mLoss[0m : 2.96039
[1mStep[0m  [56/84], [94mLoss[0m : 2.97970
[1mStep[0m  [64/84], [94mLoss[0m : 3.21961
[1mStep[0m  [72/84], [94mLoss[0m : 2.81067
[1mStep[0m  [80/84], [94mLoss[0m : 2.89211

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.393, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90881
[1mStep[0m  [8/84], [94mLoss[0m : 2.44035
[1mStep[0m  [16/84], [94mLoss[0m : 2.58013
[1mStep[0m  [24/84], [94mLoss[0m : 2.48363
[1mStep[0m  [32/84], [94mLoss[0m : 2.46854
[1mStep[0m  [40/84], [94mLoss[0m : 2.84750
[1mStep[0m  [48/84], [94mLoss[0m : 2.85676
[1mStep[0m  [56/84], [94mLoss[0m : 2.73399
[1mStep[0m  [64/84], [94mLoss[0m : 2.72988
[1mStep[0m  [72/84], [94mLoss[0m : 2.53155
[1mStep[0m  [80/84], [94mLoss[0m : 2.91413

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99912
[1mStep[0m  [8/84], [94mLoss[0m : 2.54058
[1mStep[0m  [16/84], [94mLoss[0m : 2.48805
[1mStep[0m  [24/84], [94mLoss[0m : 2.91715
[1mStep[0m  [32/84], [94mLoss[0m : 2.41302
[1mStep[0m  [40/84], [94mLoss[0m : 2.83703
[1mStep[0m  [48/84], [94mLoss[0m : 2.50260
[1mStep[0m  [56/84], [94mLoss[0m : 2.47665
[1mStep[0m  [64/84], [94mLoss[0m : 2.47375
[1mStep[0m  [72/84], [94mLoss[0m : 2.59995
[1mStep[0m  [80/84], [94mLoss[0m : 2.37142

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44397
[1mStep[0m  [8/84], [94mLoss[0m : 2.91080
[1mStep[0m  [16/84], [94mLoss[0m : 2.59675
[1mStep[0m  [24/84], [94mLoss[0m : 2.77316
[1mStep[0m  [32/84], [94mLoss[0m : 2.60554
[1mStep[0m  [40/84], [94mLoss[0m : 2.98551
[1mStep[0m  [48/84], [94mLoss[0m : 2.65072
[1mStep[0m  [56/84], [94mLoss[0m : 2.63816
[1mStep[0m  [64/84], [94mLoss[0m : 2.74650
[1mStep[0m  [72/84], [94mLoss[0m : 2.55577
[1mStep[0m  [80/84], [94mLoss[0m : 2.79749

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.702, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67973
[1mStep[0m  [8/84], [94mLoss[0m : 2.54156
[1mStep[0m  [16/84], [94mLoss[0m : 2.79437
[1mStep[0m  [24/84], [94mLoss[0m : 2.38759
[1mStep[0m  [32/84], [94mLoss[0m : 2.97631
[1mStep[0m  [40/84], [94mLoss[0m : 2.65150
[1mStep[0m  [48/84], [94mLoss[0m : 2.64953
[1mStep[0m  [56/84], [94mLoss[0m : 2.78156
[1mStep[0m  [64/84], [94mLoss[0m : 2.68753
[1mStep[0m  [72/84], [94mLoss[0m : 2.54177
[1mStep[0m  [80/84], [94mLoss[0m : 2.52128

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63984
[1mStep[0m  [8/84], [94mLoss[0m : 2.94233
[1mStep[0m  [16/84], [94mLoss[0m : 2.67966
[1mStep[0m  [24/84], [94mLoss[0m : 2.49013
[1mStep[0m  [32/84], [94mLoss[0m : 2.71398
[1mStep[0m  [40/84], [94mLoss[0m : 2.36451
[1mStep[0m  [48/84], [94mLoss[0m : 2.97119
[1mStep[0m  [56/84], [94mLoss[0m : 2.82204
[1mStep[0m  [64/84], [94mLoss[0m : 2.70847
[1mStep[0m  [72/84], [94mLoss[0m : 2.85011
[1mStep[0m  [80/84], [94mLoss[0m : 2.73342

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68924
[1mStep[0m  [8/84], [94mLoss[0m : 2.74386
[1mStep[0m  [16/84], [94mLoss[0m : 2.78354
[1mStep[0m  [24/84], [94mLoss[0m : 2.70425
[1mStep[0m  [32/84], [94mLoss[0m : 2.62560
[1mStep[0m  [40/84], [94mLoss[0m : 2.59496
[1mStep[0m  [48/84], [94mLoss[0m : 2.62229
[1mStep[0m  [56/84], [94mLoss[0m : 2.48758
[1mStep[0m  [64/84], [94mLoss[0m : 2.64426
[1mStep[0m  [72/84], [94mLoss[0m : 2.96286
[1mStep[0m  [80/84], [94mLoss[0m : 2.68468

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57362
[1mStep[0m  [8/84], [94mLoss[0m : 2.65478
[1mStep[0m  [16/84], [94mLoss[0m : 2.57679
[1mStep[0m  [24/84], [94mLoss[0m : 2.34872
[1mStep[0m  [32/84], [94mLoss[0m : 3.00331
[1mStep[0m  [40/84], [94mLoss[0m : 2.81356
[1mStep[0m  [48/84], [94mLoss[0m : 3.04219
[1mStep[0m  [56/84], [94mLoss[0m : 2.45110
[1mStep[0m  [64/84], [94mLoss[0m : 3.08307
[1mStep[0m  [72/84], [94mLoss[0m : 2.39751
[1mStep[0m  [80/84], [94mLoss[0m : 2.59846

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98223
[1mStep[0m  [8/84], [94mLoss[0m : 2.53320
[1mStep[0m  [16/84], [94mLoss[0m : 2.72667
[1mStep[0m  [24/84], [94mLoss[0m : 2.56435
[1mStep[0m  [32/84], [94mLoss[0m : 2.73397
[1mStep[0m  [40/84], [94mLoss[0m : 2.73143
[1mStep[0m  [48/84], [94mLoss[0m : 2.55994
[1mStep[0m  [56/84], [94mLoss[0m : 3.36789
[1mStep[0m  [64/84], [94mLoss[0m : 2.69642
[1mStep[0m  [72/84], [94mLoss[0m : 2.38810
[1mStep[0m  [80/84], [94mLoss[0m : 2.72089

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96965
[1mStep[0m  [8/84], [94mLoss[0m : 2.77250
[1mStep[0m  [16/84], [94mLoss[0m : 2.84278
[1mStep[0m  [24/84], [94mLoss[0m : 2.59102
[1mStep[0m  [32/84], [94mLoss[0m : 2.70551
[1mStep[0m  [40/84], [94mLoss[0m : 2.78167
[1mStep[0m  [48/84], [94mLoss[0m : 2.61397
[1mStep[0m  [56/84], [94mLoss[0m : 2.53024
[1mStep[0m  [64/84], [94mLoss[0m : 2.39863
[1mStep[0m  [72/84], [94mLoss[0m : 2.73850
[1mStep[0m  [80/84], [94mLoss[0m : 2.69818

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.363
====================================

Phase 1 - Evaluation MAE:  2.3629795483180454
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.79781
[1mStep[0m  [8/84], [94mLoss[0m : 2.86334
[1mStep[0m  [16/84], [94mLoss[0m : 2.64358
[1mStep[0m  [24/84], [94mLoss[0m : 2.72978
[1mStep[0m  [32/84], [94mLoss[0m : 2.86365
[1mStep[0m  [40/84], [94mLoss[0m : 2.20576
[1mStep[0m  [48/84], [94mLoss[0m : 2.51493
[1mStep[0m  [56/84], [94mLoss[0m : 2.77870
[1mStep[0m  [64/84], [94mLoss[0m : 2.85706
[1mStep[0m  [72/84], [94mLoss[0m : 2.98941
[1mStep[0m  [80/84], [94mLoss[0m : 2.95547

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76002
[1mStep[0m  [8/84], [94mLoss[0m : 2.60695
[1mStep[0m  [16/84], [94mLoss[0m : 2.55533
[1mStep[0m  [24/84], [94mLoss[0m : 2.58346
[1mStep[0m  [32/84], [94mLoss[0m : 2.91832
[1mStep[0m  [40/84], [94mLoss[0m : 2.73714
[1mStep[0m  [48/84], [94mLoss[0m : 2.40580
[1mStep[0m  [56/84], [94mLoss[0m : 2.50257
[1mStep[0m  [64/84], [94mLoss[0m : 2.89467
[1mStep[0m  [72/84], [94mLoss[0m : 3.09254
[1mStep[0m  [80/84], [94mLoss[0m : 2.86550

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.762, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74099
[1mStep[0m  [8/84], [94mLoss[0m : 2.83499
[1mStep[0m  [16/84], [94mLoss[0m : 2.69301
[1mStep[0m  [24/84], [94mLoss[0m : 2.46292
[1mStep[0m  [32/84], [94mLoss[0m : 2.75297
[1mStep[0m  [40/84], [94mLoss[0m : 2.69709
[1mStep[0m  [48/84], [94mLoss[0m : 2.43653
[1mStep[0m  [56/84], [94mLoss[0m : 2.85880
[1mStep[0m  [64/84], [94mLoss[0m : 2.68810
[1mStep[0m  [72/84], [94mLoss[0m : 2.52139
[1mStep[0m  [80/84], [94mLoss[0m : 2.68013

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.708, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54600
[1mStep[0m  [8/84], [94mLoss[0m : 3.10946
[1mStep[0m  [16/84], [94mLoss[0m : 2.96652
[1mStep[0m  [24/84], [94mLoss[0m : 2.48882
[1mStep[0m  [32/84], [94mLoss[0m : 2.63226
[1mStep[0m  [40/84], [94mLoss[0m : 3.24760
[1mStep[0m  [48/84], [94mLoss[0m : 2.37272
[1mStep[0m  [56/84], [94mLoss[0m : 2.58462
[1mStep[0m  [64/84], [94mLoss[0m : 2.46313
[1mStep[0m  [72/84], [94mLoss[0m : 2.39947
[1mStep[0m  [80/84], [94mLoss[0m : 2.98588

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.696, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73754
[1mStep[0m  [8/84], [94mLoss[0m : 2.56766
[1mStep[0m  [16/84], [94mLoss[0m : 2.38584
[1mStep[0m  [24/84], [94mLoss[0m : 2.68237
[1mStep[0m  [32/84], [94mLoss[0m : 2.39395
[1mStep[0m  [40/84], [94mLoss[0m : 2.61191
[1mStep[0m  [48/84], [94mLoss[0m : 2.53869
[1mStep[0m  [56/84], [94mLoss[0m : 2.73936
[1mStep[0m  [64/84], [94mLoss[0m : 2.39142
[1mStep[0m  [72/84], [94mLoss[0m : 2.43931
[1mStep[0m  [80/84], [94mLoss[0m : 2.51324

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.557, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77051
[1mStep[0m  [8/84], [94mLoss[0m : 2.78650
[1mStep[0m  [16/84], [94mLoss[0m : 2.94594
[1mStep[0m  [24/84], [94mLoss[0m : 2.34032
[1mStep[0m  [32/84], [94mLoss[0m : 2.57720
[1mStep[0m  [40/84], [94mLoss[0m : 2.81110
[1mStep[0m  [48/84], [94mLoss[0m : 2.58624
[1mStep[0m  [56/84], [94mLoss[0m : 2.72230
[1mStep[0m  [64/84], [94mLoss[0m : 2.53087
[1mStep[0m  [72/84], [94mLoss[0m : 2.57050
[1mStep[0m  [80/84], [94mLoss[0m : 2.77381

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48555
[1mStep[0m  [8/84], [94mLoss[0m : 2.38679
[1mStep[0m  [16/84], [94mLoss[0m : 2.28452
[1mStep[0m  [24/84], [94mLoss[0m : 2.51317
[1mStep[0m  [32/84], [94mLoss[0m : 2.46170
[1mStep[0m  [40/84], [94mLoss[0m : 2.56146
[1mStep[0m  [48/84], [94mLoss[0m : 2.67915
[1mStep[0m  [56/84], [94mLoss[0m : 2.55176
[1mStep[0m  [64/84], [94mLoss[0m : 2.69036
[1mStep[0m  [72/84], [94mLoss[0m : 2.64116
[1mStep[0m  [80/84], [94mLoss[0m : 2.36963

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26703
[1mStep[0m  [8/84], [94mLoss[0m : 2.63505
[1mStep[0m  [16/84], [94mLoss[0m : 2.55729
[1mStep[0m  [24/84], [94mLoss[0m : 2.57404
[1mStep[0m  [32/84], [94mLoss[0m : 2.74617
[1mStep[0m  [40/84], [94mLoss[0m : 2.51035
[1mStep[0m  [48/84], [94mLoss[0m : 2.44816
[1mStep[0m  [56/84], [94mLoss[0m : 2.54187
[1mStep[0m  [64/84], [94mLoss[0m : 2.28662
[1mStep[0m  [72/84], [94mLoss[0m : 2.45141
[1mStep[0m  [80/84], [94mLoss[0m : 2.60269

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60789
[1mStep[0m  [8/84], [94mLoss[0m : 2.58714
[1mStep[0m  [16/84], [94mLoss[0m : 2.62458
[1mStep[0m  [24/84], [94mLoss[0m : 2.38303
[1mStep[0m  [32/84], [94mLoss[0m : 2.64311
[1mStep[0m  [40/84], [94mLoss[0m : 2.41121
[1mStep[0m  [48/84], [94mLoss[0m : 2.46243
[1mStep[0m  [56/84], [94mLoss[0m : 2.38848
[1mStep[0m  [64/84], [94mLoss[0m : 2.56575
[1mStep[0m  [72/84], [94mLoss[0m : 2.86378
[1mStep[0m  [80/84], [94mLoss[0m : 2.33558

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41327
[1mStep[0m  [8/84], [94mLoss[0m : 2.66775
[1mStep[0m  [16/84], [94mLoss[0m : 2.23854
[1mStep[0m  [24/84], [94mLoss[0m : 2.47592
[1mStep[0m  [32/84], [94mLoss[0m : 2.43264
[1mStep[0m  [40/84], [94mLoss[0m : 2.47548
[1mStep[0m  [48/84], [94mLoss[0m : 2.49272
[1mStep[0m  [56/84], [94mLoss[0m : 2.21904
[1mStep[0m  [64/84], [94mLoss[0m : 2.54631
[1mStep[0m  [72/84], [94mLoss[0m : 2.54641
[1mStep[0m  [80/84], [94mLoss[0m : 2.37608

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42367
[1mStep[0m  [8/84], [94mLoss[0m : 2.28293
[1mStep[0m  [16/84], [94mLoss[0m : 2.43067
[1mStep[0m  [24/84], [94mLoss[0m : 2.29869
[1mStep[0m  [32/84], [94mLoss[0m : 2.36499
[1mStep[0m  [40/84], [94mLoss[0m : 2.54463
[1mStep[0m  [48/84], [94mLoss[0m : 2.24465
[1mStep[0m  [56/84], [94mLoss[0m : 2.54855
[1mStep[0m  [64/84], [94mLoss[0m : 2.53866
[1mStep[0m  [72/84], [94mLoss[0m : 2.52192
[1mStep[0m  [80/84], [94mLoss[0m : 2.38344

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30011
[1mStep[0m  [8/84], [94mLoss[0m : 2.68327
[1mStep[0m  [16/84], [94mLoss[0m : 2.42203
[1mStep[0m  [24/84], [94mLoss[0m : 2.23650
[1mStep[0m  [32/84], [94mLoss[0m : 2.59138
[1mStep[0m  [40/84], [94mLoss[0m : 2.58250
[1mStep[0m  [48/84], [94mLoss[0m : 2.50405
[1mStep[0m  [56/84], [94mLoss[0m : 2.44083
[1mStep[0m  [64/84], [94mLoss[0m : 2.50847
[1mStep[0m  [72/84], [94mLoss[0m : 2.36961
[1mStep[0m  [80/84], [94mLoss[0m : 2.58481

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19157
[1mStep[0m  [8/84], [94mLoss[0m : 2.28228
[1mStep[0m  [16/84], [94mLoss[0m : 2.52264
[1mStep[0m  [24/84], [94mLoss[0m : 2.59655
[1mStep[0m  [32/84], [94mLoss[0m : 2.46346
[1mStep[0m  [40/84], [94mLoss[0m : 2.60589
[1mStep[0m  [48/84], [94mLoss[0m : 2.37618
[1mStep[0m  [56/84], [94mLoss[0m : 2.33277
[1mStep[0m  [64/84], [94mLoss[0m : 2.16030
[1mStep[0m  [72/84], [94mLoss[0m : 2.25255
[1mStep[0m  [80/84], [94mLoss[0m : 2.42536

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43017
[1mStep[0m  [8/84], [94mLoss[0m : 2.24949
[1mStep[0m  [16/84], [94mLoss[0m : 2.56645
[1mStep[0m  [24/84], [94mLoss[0m : 2.41135
[1mStep[0m  [32/84], [94mLoss[0m : 2.38985
[1mStep[0m  [40/84], [94mLoss[0m : 2.65710
[1mStep[0m  [48/84], [94mLoss[0m : 2.46162
[1mStep[0m  [56/84], [94mLoss[0m : 2.22320
[1mStep[0m  [64/84], [94mLoss[0m : 2.55830
[1mStep[0m  [72/84], [94mLoss[0m : 2.35910
[1mStep[0m  [80/84], [94mLoss[0m : 2.08352

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36040
[1mStep[0m  [8/84], [94mLoss[0m : 2.13197
[1mStep[0m  [16/84], [94mLoss[0m : 2.08264
[1mStep[0m  [24/84], [94mLoss[0m : 2.11022
[1mStep[0m  [32/84], [94mLoss[0m : 2.26947
[1mStep[0m  [40/84], [94mLoss[0m : 2.21603
[1mStep[0m  [48/84], [94mLoss[0m : 2.05976
[1mStep[0m  [56/84], [94mLoss[0m : 2.36689
[1mStep[0m  [64/84], [94mLoss[0m : 2.29107
[1mStep[0m  [72/84], [94mLoss[0m : 2.65680
[1mStep[0m  [80/84], [94mLoss[0m : 2.59411

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32870
[1mStep[0m  [8/84], [94mLoss[0m : 2.50686
[1mStep[0m  [16/84], [94mLoss[0m : 2.41947
[1mStep[0m  [24/84], [94mLoss[0m : 2.15408
[1mStep[0m  [32/84], [94mLoss[0m : 2.38891
[1mStep[0m  [40/84], [94mLoss[0m : 2.12594
[1mStep[0m  [48/84], [94mLoss[0m : 2.33092
[1mStep[0m  [56/84], [94mLoss[0m : 2.30182
[1mStep[0m  [64/84], [94mLoss[0m : 2.30119
[1mStep[0m  [72/84], [94mLoss[0m : 2.48601
[1mStep[0m  [80/84], [94mLoss[0m : 2.34499

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22796
[1mStep[0m  [8/84], [94mLoss[0m : 2.41390
[1mStep[0m  [16/84], [94mLoss[0m : 2.11011
[1mStep[0m  [24/84], [94mLoss[0m : 2.16676
[1mStep[0m  [32/84], [94mLoss[0m : 2.47383
[1mStep[0m  [40/84], [94mLoss[0m : 2.46682
[1mStep[0m  [48/84], [94mLoss[0m : 2.47819
[1mStep[0m  [56/84], [94mLoss[0m : 2.35782
[1mStep[0m  [64/84], [94mLoss[0m : 2.44344
[1mStep[0m  [72/84], [94mLoss[0m : 2.42843
[1mStep[0m  [80/84], [94mLoss[0m : 2.19830

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18338
[1mStep[0m  [8/84], [94mLoss[0m : 2.35862
[1mStep[0m  [16/84], [94mLoss[0m : 2.23420
[1mStep[0m  [24/84], [94mLoss[0m : 1.96890
[1mStep[0m  [32/84], [94mLoss[0m : 2.24036
[1mStep[0m  [40/84], [94mLoss[0m : 2.11708
[1mStep[0m  [48/84], [94mLoss[0m : 2.25435
[1mStep[0m  [56/84], [94mLoss[0m : 2.61284
[1mStep[0m  [64/84], [94mLoss[0m : 2.54435
[1mStep[0m  [72/84], [94mLoss[0m : 2.23239
[1mStep[0m  [80/84], [94mLoss[0m : 2.41907

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48542
[1mStep[0m  [8/84], [94mLoss[0m : 2.07519
[1mStep[0m  [16/84], [94mLoss[0m : 2.25879
[1mStep[0m  [24/84], [94mLoss[0m : 2.35557
[1mStep[0m  [32/84], [94mLoss[0m : 2.08988
[1mStep[0m  [40/84], [94mLoss[0m : 2.20331
[1mStep[0m  [48/84], [94mLoss[0m : 2.24510
[1mStep[0m  [56/84], [94mLoss[0m : 2.47074
[1mStep[0m  [64/84], [94mLoss[0m : 2.40315
[1mStep[0m  [72/84], [94mLoss[0m : 2.23324
[1mStep[0m  [80/84], [94mLoss[0m : 2.23442

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94595
[1mStep[0m  [8/84], [94mLoss[0m : 2.26699
[1mStep[0m  [16/84], [94mLoss[0m : 2.27881
[1mStep[0m  [24/84], [94mLoss[0m : 2.41593
[1mStep[0m  [32/84], [94mLoss[0m : 2.10685
[1mStep[0m  [40/84], [94mLoss[0m : 2.11019
[1mStep[0m  [48/84], [94mLoss[0m : 2.20856
[1mStep[0m  [56/84], [94mLoss[0m : 1.98991
[1mStep[0m  [64/84], [94mLoss[0m : 2.03389
[1mStep[0m  [72/84], [94mLoss[0m : 2.16622
[1mStep[0m  [80/84], [94mLoss[0m : 2.24642

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00831
[1mStep[0m  [8/84], [94mLoss[0m : 2.16547
[1mStep[0m  [16/84], [94mLoss[0m : 2.33336
[1mStep[0m  [24/84], [94mLoss[0m : 2.29730
[1mStep[0m  [32/84], [94mLoss[0m : 2.16898
[1mStep[0m  [40/84], [94mLoss[0m : 2.25267
[1mStep[0m  [48/84], [94mLoss[0m : 2.16479
[1mStep[0m  [56/84], [94mLoss[0m : 2.21954
[1mStep[0m  [64/84], [94mLoss[0m : 2.28930
[1mStep[0m  [72/84], [94mLoss[0m : 2.25587
[1mStep[0m  [80/84], [94mLoss[0m : 2.43919

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06911
[1mStep[0m  [8/84], [94mLoss[0m : 2.24762
[1mStep[0m  [16/84], [94mLoss[0m : 2.09075
[1mStep[0m  [24/84], [94mLoss[0m : 2.01826
[1mStep[0m  [32/84], [94mLoss[0m : 2.01574
[1mStep[0m  [40/84], [94mLoss[0m : 2.35326
[1mStep[0m  [48/84], [94mLoss[0m : 2.37872
[1mStep[0m  [56/84], [94mLoss[0m : 1.91447
[1mStep[0m  [64/84], [94mLoss[0m : 1.93279
[1mStep[0m  [72/84], [94mLoss[0m : 1.99797
[1mStep[0m  [80/84], [94mLoss[0m : 2.03983

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16355
[1mStep[0m  [8/84], [94mLoss[0m : 2.13534
[1mStep[0m  [16/84], [94mLoss[0m : 2.22899
[1mStep[0m  [24/84], [94mLoss[0m : 2.19164
[1mStep[0m  [32/84], [94mLoss[0m : 2.26102
[1mStep[0m  [40/84], [94mLoss[0m : 1.99799
[1mStep[0m  [48/84], [94mLoss[0m : 2.21082
[1mStep[0m  [56/84], [94mLoss[0m : 2.24043
[1mStep[0m  [64/84], [94mLoss[0m : 2.03955
[1mStep[0m  [72/84], [94mLoss[0m : 2.50078
[1mStep[0m  [80/84], [94mLoss[0m : 2.01782

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82818
[1mStep[0m  [8/84], [94mLoss[0m : 1.88400
[1mStep[0m  [16/84], [94mLoss[0m : 2.10927
[1mStep[0m  [24/84], [94mLoss[0m : 2.11769
[1mStep[0m  [32/84], [94mLoss[0m : 2.19381
[1mStep[0m  [40/84], [94mLoss[0m : 2.15035
[1mStep[0m  [48/84], [94mLoss[0m : 1.92844
[1mStep[0m  [56/84], [94mLoss[0m : 2.00535
[1mStep[0m  [64/84], [94mLoss[0m : 2.19966
[1mStep[0m  [72/84], [94mLoss[0m : 2.04792
[1mStep[0m  [80/84], [94mLoss[0m : 1.99616

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15528
[1mStep[0m  [8/84], [94mLoss[0m : 2.02685
[1mStep[0m  [16/84], [94mLoss[0m : 2.03543
[1mStep[0m  [24/84], [94mLoss[0m : 2.15146
[1mStep[0m  [32/84], [94mLoss[0m : 1.98248
[1mStep[0m  [40/84], [94mLoss[0m : 2.30443
[1mStep[0m  [48/84], [94mLoss[0m : 2.03733
[1mStep[0m  [56/84], [94mLoss[0m : 2.10766
[1mStep[0m  [64/84], [94mLoss[0m : 1.81857
[1mStep[0m  [72/84], [94mLoss[0m : 2.00124
[1mStep[0m  [80/84], [94mLoss[0m : 2.16632

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15859
[1mStep[0m  [8/84], [94mLoss[0m : 2.20092
[1mStep[0m  [16/84], [94mLoss[0m : 1.78594
[1mStep[0m  [24/84], [94mLoss[0m : 1.99566
[1mStep[0m  [32/84], [94mLoss[0m : 1.89288
[1mStep[0m  [40/84], [94mLoss[0m : 1.90660
[1mStep[0m  [48/84], [94mLoss[0m : 1.85973
[1mStep[0m  [56/84], [94mLoss[0m : 2.05038
[1mStep[0m  [64/84], [94mLoss[0m : 1.94831
[1mStep[0m  [72/84], [94mLoss[0m : 2.15197
[1mStep[0m  [80/84], [94mLoss[0m : 1.95352

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.553, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76055
[1mStep[0m  [8/84], [94mLoss[0m : 2.06392
[1mStep[0m  [16/84], [94mLoss[0m : 2.20731
[1mStep[0m  [24/84], [94mLoss[0m : 2.19663
[1mStep[0m  [32/84], [94mLoss[0m : 1.98625
[1mStep[0m  [40/84], [94mLoss[0m : 1.75404
[1mStep[0m  [48/84], [94mLoss[0m : 1.91190
[1mStep[0m  [56/84], [94mLoss[0m : 1.98989
[1mStep[0m  [64/84], [94mLoss[0m : 2.20326
[1mStep[0m  [72/84], [94mLoss[0m : 2.11879
[1mStep[0m  [80/84], [94mLoss[0m : 2.03992

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92921
[1mStep[0m  [8/84], [94mLoss[0m : 2.07446
[1mStep[0m  [16/84], [94mLoss[0m : 2.22239
[1mStep[0m  [24/84], [94mLoss[0m : 2.24836
[1mStep[0m  [32/84], [94mLoss[0m : 1.87550
[1mStep[0m  [40/84], [94mLoss[0m : 2.19024
[1mStep[0m  [48/84], [94mLoss[0m : 1.87688
[1mStep[0m  [56/84], [94mLoss[0m : 2.07076
[1mStep[0m  [64/84], [94mLoss[0m : 1.94021
[1mStep[0m  [72/84], [94mLoss[0m : 1.82662
[1mStep[0m  [80/84], [94mLoss[0m : 2.28306

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04483
[1mStep[0m  [8/84], [94mLoss[0m : 1.97835
[1mStep[0m  [16/84], [94mLoss[0m : 1.94430
[1mStep[0m  [24/84], [94mLoss[0m : 1.84536
[1mStep[0m  [32/84], [94mLoss[0m : 2.16864
[1mStep[0m  [40/84], [94mLoss[0m : 1.92478
[1mStep[0m  [48/84], [94mLoss[0m : 2.20243
[1mStep[0m  [56/84], [94mLoss[0m : 1.98164
[1mStep[0m  [64/84], [94mLoss[0m : 2.31125
[1mStep[0m  [72/84], [94mLoss[0m : 2.07214
[1mStep[0m  [80/84], [94mLoss[0m : 2.02112

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82033
[1mStep[0m  [8/84], [94mLoss[0m : 2.07847
[1mStep[0m  [16/84], [94mLoss[0m : 1.97677
[1mStep[0m  [24/84], [94mLoss[0m : 2.03744
[1mStep[0m  [32/84], [94mLoss[0m : 2.13346
[1mStep[0m  [40/84], [94mLoss[0m : 2.12091
[1mStep[0m  [48/84], [94mLoss[0m : 1.85259
[1mStep[0m  [56/84], [94mLoss[0m : 1.98142
[1mStep[0m  [64/84], [94mLoss[0m : 2.11104
[1mStep[0m  [72/84], [94mLoss[0m : 1.98042
[1mStep[0m  [80/84], [94mLoss[0m : 2.03695

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.491957570825304
MAE score P1        2.36298
MAE score P2       2.491958
loss               1.965584
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.47781
[1mStep[0m  [4/42], [94mLoss[0m : 9.92532
[1mStep[0m  [8/42], [94mLoss[0m : 9.31695
[1mStep[0m  [12/42], [94mLoss[0m : 8.70842
[1mStep[0m  [16/42], [94mLoss[0m : 8.08448
[1mStep[0m  [20/42], [94mLoss[0m : 7.64478
[1mStep[0m  [24/42], [94mLoss[0m : 7.32975
[1mStep[0m  [28/42], [94mLoss[0m : 6.32920
[1mStep[0m  [32/42], [94mLoss[0m : 5.72639
[1mStep[0m  [36/42], [94mLoss[0m : 5.05006
[1mStep[0m  [40/42], [94mLoss[0m : 4.55608

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.351, [92mTest[0m: 10.758, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.70646
[1mStep[0m  [4/42], [94mLoss[0m : 3.82622
[1mStep[0m  [8/42], [94mLoss[0m : 3.68887
[1mStep[0m  [12/42], [94mLoss[0m : 3.15951
[1mStep[0m  [16/42], [94mLoss[0m : 3.15520
[1mStep[0m  [20/42], [94mLoss[0m : 3.15329
[1mStep[0m  [24/42], [94mLoss[0m : 3.14641
[1mStep[0m  [28/42], [94mLoss[0m : 2.81806
[1mStep[0m  [32/42], [94mLoss[0m : 2.77397
[1mStep[0m  [36/42], [94mLoss[0m : 2.54745
[1mStep[0m  [40/42], [94mLoss[0m : 2.60436

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.211, [92mTest[0m: 5.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51786
[1mStep[0m  [4/42], [94mLoss[0m : 2.71461
[1mStep[0m  [8/42], [94mLoss[0m : 2.64080
[1mStep[0m  [12/42], [94mLoss[0m : 2.61190
[1mStep[0m  [16/42], [94mLoss[0m : 2.74504
[1mStep[0m  [20/42], [94mLoss[0m : 2.83365
[1mStep[0m  [24/42], [94mLoss[0m : 2.69555
[1mStep[0m  [28/42], [94mLoss[0m : 2.69519
[1mStep[0m  [32/42], [94mLoss[0m : 2.75447
[1mStep[0m  [36/42], [94mLoss[0m : 2.60124
[1mStep[0m  [40/42], [94mLoss[0m : 2.51564

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.966, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70647
[1mStep[0m  [4/42], [94mLoss[0m : 2.48852
[1mStep[0m  [8/42], [94mLoss[0m : 2.52125
[1mStep[0m  [12/42], [94mLoss[0m : 2.57132
[1mStep[0m  [16/42], [94mLoss[0m : 2.31582
[1mStep[0m  [20/42], [94mLoss[0m : 2.74606
[1mStep[0m  [24/42], [94mLoss[0m : 2.60436
[1mStep[0m  [28/42], [94mLoss[0m : 2.46177
[1mStep[0m  [32/42], [94mLoss[0m : 2.42353
[1mStep[0m  [36/42], [94mLoss[0m : 2.45451
[1mStep[0m  [40/42], [94mLoss[0m : 2.58580

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.664, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63734
[1mStep[0m  [4/42], [94mLoss[0m : 2.40665
[1mStep[0m  [8/42], [94mLoss[0m : 2.81663
[1mStep[0m  [12/42], [94mLoss[0m : 2.55510
[1mStep[0m  [16/42], [94mLoss[0m : 2.43984
[1mStep[0m  [20/42], [94mLoss[0m : 2.58196
[1mStep[0m  [24/42], [94mLoss[0m : 2.40503
[1mStep[0m  [28/42], [94mLoss[0m : 2.47396
[1mStep[0m  [32/42], [94mLoss[0m : 2.60455
[1mStep[0m  [36/42], [94mLoss[0m : 2.57414
[1mStep[0m  [40/42], [94mLoss[0m : 2.45779

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56872
[1mStep[0m  [4/42], [94mLoss[0m : 2.46438
[1mStep[0m  [8/42], [94mLoss[0m : 2.57303
[1mStep[0m  [12/42], [94mLoss[0m : 2.33625
[1mStep[0m  [16/42], [94mLoss[0m : 2.61081
[1mStep[0m  [20/42], [94mLoss[0m : 2.68747
[1mStep[0m  [24/42], [94mLoss[0m : 2.50584
[1mStep[0m  [28/42], [94mLoss[0m : 2.46638
[1mStep[0m  [32/42], [94mLoss[0m : 2.53595
[1mStep[0m  [36/42], [94mLoss[0m : 2.60603
[1mStep[0m  [40/42], [94mLoss[0m : 2.32459

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80696
[1mStep[0m  [4/42], [94mLoss[0m : 2.80250
[1mStep[0m  [8/42], [94mLoss[0m : 2.54793
[1mStep[0m  [12/42], [94mLoss[0m : 2.68836
[1mStep[0m  [16/42], [94mLoss[0m : 2.49736
[1mStep[0m  [20/42], [94mLoss[0m : 2.54094
[1mStep[0m  [24/42], [94mLoss[0m : 2.44949
[1mStep[0m  [28/42], [94mLoss[0m : 2.56852
[1mStep[0m  [32/42], [94mLoss[0m : 2.46710
[1mStep[0m  [36/42], [94mLoss[0m : 2.44761
[1mStep[0m  [40/42], [94mLoss[0m : 2.51877

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66441
[1mStep[0m  [4/42], [94mLoss[0m : 2.48846
[1mStep[0m  [8/42], [94mLoss[0m : 2.34087
[1mStep[0m  [12/42], [94mLoss[0m : 2.68113
[1mStep[0m  [16/42], [94mLoss[0m : 2.45214
[1mStep[0m  [20/42], [94mLoss[0m : 2.46869
[1mStep[0m  [24/42], [94mLoss[0m : 2.60913
[1mStep[0m  [28/42], [94mLoss[0m : 2.64730
[1mStep[0m  [32/42], [94mLoss[0m : 2.46031
[1mStep[0m  [36/42], [94mLoss[0m : 2.52249
[1mStep[0m  [40/42], [94mLoss[0m : 2.53313

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60347
[1mStep[0m  [4/42], [94mLoss[0m : 2.51908
[1mStep[0m  [8/42], [94mLoss[0m : 2.43579
[1mStep[0m  [12/42], [94mLoss[0m : 2.55593
[1mStep[0m  [16/42], [94mLoss[0m : 2.48638
[1mStep[0m  [20/42], [94mLoss[0m : 2.56981
[1mStep[0m  [24/42], [94mLoss[0m : 2.61395
[1mStep[0m  [28/42], [94mLoss[0m : 2.42102
[1mStep[0m  [32/42], [94mLoss[0m : 2.33290
[1mStep[0m  [36/42], [94mLoss[0m : 2.45497
[1mStep[0m  [40/42], [94mLoss[0m : 2.65640

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52838
[1mStep[0m  [4/42], [94mLoss[0m : 2.36272
[1mStep[0m  [8/42], [94mLoss[0m : 2.35620
[1mStep[0m  [12/42], [94mLoss[0m : 2.71176
[1mStep[0m  [16/42], [94mLoss[0m : 2.53461
[1mStep[0m  [20/42], [94mLoss[0m : 2.64100
[1mStep[0m  [24/42], [94mLoss[0m : 2.49170
[1mStep[0m  [28/42], [94mLoss[0m : 2.54901
[1mStep[0m  [32/42], [94mLoss[0m : 2.39165
[1mStep[0m  [36/42], [94mLoss[0m : 2.76563
[1mStep[0m  [40/42], [94mLoss[0m : 2.63778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53721
[1mStep[0m  [4/42], [94mLoss[0m : 2.46084
[1mStep[0m  [8/42], [94mLoss[0m : 2.41000
[1mStep[0m  [12/42], [94mLoss[0m : 2.57388
[1mStep[0m  [16/42], [94mLoss[0m : 2.45999
[1mStep[0m  [20/42], [94mLoss[0m : 2.50204
[1mStep[0m  [24/42], [94mLoss[0m : 2.51187
[1mStep[0m  [28/42], [94mLoss[0m : 2.82205
[1mStep[0m  [32/42], [94mLoss[0m : 2.60395
[1mStep[0m  [36/42], [94mLoss[0m : 2.54295
[1mStep[0m  [40/42], [94mLoss[0m : 2.43298

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61131
[1mStep[0m  [4/42], [94mLoss[0m : 2.38904
[1mStep[0m  [8/42], [94mLoss[0m : 2.67909
[1mStep[0m  [12/42], [94mLoss[0m : 2.32425
[1mStep[0m  [16/42], [94mLoss[0m : 2.55836
[1mStep[0m  [20/42], [94mLoss[0m : 2.52774
[1mStep[0m  [24/42], [94mLoss[0m : 2.70793
[1mStep[0m  [28/42], [94mLoss[0m : 2.64807
[1mStep[0m  [32/42], [94mLoss[0m : 2.54040
[1mStep[0m  [36/42], [94mLoss[0m : 2.62163
[1mStep[0m  [40/42], [94mLoss[0m : 2.56111

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47965
[1mStep[0m  [4/42], [94mLoss[0m : 2.27704
[1mStep[0m  [8/42], [94mLoss[0m : 2.41363
[1mStep[0m  [12/42], [94mLoss[0m : 2.72963
[1mStep[0m  [16/42], [94mLoss[0m : 2.50890
[1mStep[0m  [20/42], [94mLoss[0m : 2.37075
[1mStep[0m  [24/42], [94mLoss[0m : 2.67439
[1mStep[0m  [28/42], [94mLoss[0m : 2.50995
[1mStep[0m  [32/42], [94mLoss[0m : 2.48691
[1mStep[0m  [36/42], [94mLoss[0m : 2.75529
[1mStep[0m  [40/42], [94mLoss[0m : 2.55369

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30239
[1mStep[0m  [4/42], [94mLoss[0m : 2.34029
[1mStep[0m  [8/42], [94mLoss[0m : 2.56352
[1mStep[0m  [12/42], [94mLoss[0m : 2.80265
[1mStep[0m  [16/42], [94mLoss[0m : 2.34905
[1mStep[0m  [20/42], [94mLoss[0m : 2.57003
[1mStep[0m  [24/42], [94mLoss[0m : 2.59762
[1mStep[0m  [28/42], [94mLoss[0m : 2.41417
[1mStep[0m  [32/42], [94mLoss[0m : 2.49893
[1mStep[0m  [36/42], [94mLoss[0m : 2.45682
[1mStep[0m  [40/42], [94mLoss[0m : 2.74339

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57111
[1mStep[0m  [4/42], [94mLoss[0m : 2.43044
[1mStep[0m  [8/42], [94mLoss[0m : 2.30656
[1mStep[0m  [12/42], [94mLoss[0m : 2.33485
[1mStep[0m  [16/42], [94mLoss[0m : 2.78207
[1mStep[0m  [20/42], [94mLoss[0m : 2.48104
[1mStep[0m  [24/42], [94mLoss[0m : 2.47311
[1mStep[0m  [28/42], [94mLoss[0m : 2.44517
[1mStep[0m  [32/42], [94mLoss[0m : 2.40866
[1mStep[0m  [36/42], [94mLoss[0m : 2.35413
[1mStep[0m  [40/42], [94mLoss[0m : 2.41975

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23758
[1mStep[0m  [4/42], [94mLoss[0m : 2.34593
[1mStep[0m  [8/42], [94mLoss[0m : 2.55671
[1mStep[0m  [12/42], [94mLoss[0m : 2.37798
[1mStep[0m  [16/42], [94mLoss[0m : 2.58584
[1mStep[0m  [20/42], [94mLoss[0m : 2.61723
[1mStep[0m  [24/42], [94mLoss[0m : 2.51138
[1mStep[0m  [28/42], [94mLoss[0m : 2.51969
[1mStep[0m  [32/42], [94mLoss[0m : 2.49063
[1mStep[0m  [36/42], [94mLoss[0m : 2.67817
[1mStep[0m  [40/42], [94mLoss[0m : 2.66028

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17224
[1mStep[0m  [4/42], [94mLoss[0m : 2.63313
[1mStep[0m  [8/42], [94mLoss[0m : 2.55958
[1mStep[0m  [12/42], [94mLoss[0m : 2.56044
[1mStep[0m  [16/42], [94mLoss[0m : 2.52183
[1mStep[0m  [20/42], [94mLoss[0m : 2.59742
[1mStep[0m  [24/42], [94mLoss[0m : 2.52413
[1mStep[0m  [28/42], [94mLoss[0m : 2.37531
[1mStep[0m  [32/42], [94mLoss[0m : 2.43243
[1mStep[0m  [36/42], [94mLoss[0m : 2.47490
[1mStep[0m  [40/42], [94mLoss[0m : 2.33867

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49546
[1mStep[0m  [4/42], [94mLoss[0m : 2.47225
[1mStep[0m  [8/42], [94mLoss[0m : 2.93869
[1mStep[0m  [12/42], [94mLoss[0m : 2.61092
[1mStep[0m  [16/42], [94mLoss[0m : 2.55917
[1mStep[0m  [20/42], [94mLoss[0m : 2.40948
[1mStep[0m  [24/42], [94mLoss[0m : 2.44195
[1mStep[0m  [28/42], [94mLoss[0m : 2.48138
[1mStep[0m  [32/42], [94mLoss[0m : 2.60311
[1mStep[0m  [36/42], [94mLoss[0m : 2.31488
[1mStep[0m  [40/42], [94mLoss[0m : 2.56411

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16652
[1mStep[0m  [4/42], [94mLoss[0m : 2.66484
[1mStep[0m  [8/42], [94mLoss[0m : 2.50240
[1mStep[0m  [12/42], [94mLoss[0m : 2.48345
[1mStep[0m  [16/42], [94mLoss[0m : 2.51141
[1mStep[0m  [20/42], [94mLoss[0m : 2.33488
[1mStep[0m  [24/42], [94mLoss[0m : 2.53966
[1mStep[0m  [28/42], [94mLoss[0m : 2.38132
[1mStep[0m  [32/42], [94mLoss[0m : 2.59017
[1mStep[0m  [36/42], [94mLoss[0m : 2.38285
[1mStep[0m  [40/42], [94mLoss[0m : 2.38475

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34612
[1mStep[0m  [4/42], [94mLoss[0m : 2.56533
[1mStep[0m  [8/42], [94mLoss[0m : 2.35645
[1mStep[0m  [12/42], [94mLoss[0m : 2.59842
[1mStep[0m  [16/42], [94mLoss[0m : 2.17416
[1mStep[0m  [20/42], [94mLoss[0m : 2.52782
[1mStep[0m  [24/42], [94mLoss[0m : 2.60584
[1mStep[0m  [28/42], [94mLoss[0m : 2.56117
[1mStep[0m  [32/42], [94mLoss[0m : 2.53811
[1mStep[0m  [36/42], [94mLoss[0m : 2.48714
[1mStep[0m  [40/42], [94mLoss[0m : 2.41582

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.385, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49477
[1mStep[0m  [4/42], [94mLoss[0m : 2.40718
[1mStep[0m  [8/42], [94mLoss[0m : 2.51864
[1mStep[0m  [12/42], [94mLoss[0m : 2.53169
[1mStep[0m  [16/42], [94mLoss[0m : 2.41681
[1mStep[0m  [20/42], [94mLoss[0m : 2.52430
[1mStep[0m  [24/42], [94mLoss[0m : 2.58763
[1mStep[0m  [28/42], [94mLoss[0m : 2.34385
[1mStep[0m  [32/42], [94mLoss[0m : 2.58232
[1mStep[0m  [36/42], [94mLoss[0m : 2.55699
[1mStep[0m  [40/42], [94mLoss[0m : 2.58098

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56810
[1mStep[0m  [4/42], [94mLoss[0m : 2.45718
[1mStep[0m  [8/42], [94mLoss[0m : 2.48621
[1mStep[0m  [12/42], [94mLoss[0m : 2.38022
[1mStep[0m  [16/42], [94mLoss[0m : 2.48935
[1mStep[0m  [20/42], [94mLoss[0m : 2.48784
[1mStep[0m  [24/42], [94mLoss[0m : 2.43970
[1mStep[0m  [28/42], [94mLoss[0m : 2.23350
[1mStep[0m  [32/42], [94mLoss[0m : 2.21207
[1mStep[0m  [36/42], [94mLoss[0m : 2.34787
[1mStep[0m  [40/42], [94mLoss[0m : 2.42716

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47135
[1mStep[0m  [4/42], [94mLoss[0m : 2.28466
[1mStep[0m  [8/42], [94mLoss[0m : 2.59972
[1mStep[0m  [12/42], [94mLoss[0m : 2.46944
[1mStep[0m  [16/42], [94mLoss[0m : 2.46736
[1mStep[0m  [20/42], [94mLoss[0m : 2.55876
[1mStep[0m  [24/42], [94mLoss[0m : 2.31908
[1mStep[0m  [28/42], [94mLoss[0m : 2.72214
[1mStep[0m  [32/42], [94mLoss[0m : 2.54141
[1mStep[0m  [36/42], [94mLoss[0m : 2.41826
[1mStep[0m  [40/42], [94mLoss[0m : 2.38056

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58931
[1mStep[0m  [4/42], [94mLoss[0m : 2.36798
[1mStep[0m  [8/42], [94mLoss[0m : 2.46565
[1mStep[0m  [12/42], [94mLoss[0m : 2.28385
[1mStep[0m  [16/42], [94mLoss[0m : 2.32724
[1mStep[0m  [20/42], [94mLoss[0m : 2.58878
[1mStep[0m  [24/42], [94mLoss[0m : 2.53256
[1mStep[0m  [28/42], [94mLoss[0m : 2.43289
[1mStep[0m  [32/42], [94mLoss[0m : 2.38956
[1mStep[0m  [36/42], [94mLoss[0m : 2.52298
[1mStep[0m  [40/42], [94mLoss[0m : 2.45777

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41926
[1mStep[0m  [4/42], [94mLoss[0m : 2.59278
[1mStep[0m  [8/42], [94mLoss[0m : 2.45612
[1mStep[0m  [12/42], [94mLoss[0m : 2.48352
[1mStep[0m  [16/42], [94mLoss[0m : 2.44045
[1mStep[0m  [20/42], [94mLoss[0m : 2.57920
[1mStep[0m  [24/42], [94mLoss[0m : 2.20929
[1mStep[0m  [28/42], [94mLoss[0m : 2.52722
[1mStep[0m  [32/42], [94mLoss[0m : 2.48116
[1mStep[0m  [36/42], [94mLoss[0m : 2.56623
[1mStep[0m  [40/42], [94mLoss[0m : 2.47723

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.379, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49766
[1mStep[0m  [4/42], [94mLoss[0m : 2.41963
[1mStep[0m  [8/42], [94mLoss[0m : 2.51070
[1mStep[0m  [12/42], [94mLoss[0m : 2.51237
[1mStep[0m  [16/42], [94mLoss[0m : 2.35424
[1mStep[0m  [20/42], [94mLoss[0m : 2.52318
[1mStep[0m  [24/42], [94mLoss[0m : 2.35758
[1mStep[0m  [28/42], [94mLoss[0m : 2.36115
[1mStep[0m  [32/42], [94mLoss[0m : 2.45544
[1mStep[0m  [36/42], [94mLoss[0m : 2.40476
[1mStep[0m  [40/42], [94mLoss[0m : 2.49252

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.378, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54830
[1mStep[0m  [4/42], [94mLoss[0m : 2.60676
[1mStep[0m  [8/42], [94mLoss[0m : 2.39025
[1mStep[0m  [12/42], [94mLoss[0m : 2.55125
[1mStep[0m  [16/42], [94mLoss[0m : 2.56316
[1mStep[0m  [20/42], [94mLoss[0m : 2.56771
[1mStep[0m  [24/42], [94mLoss[0m : 2.42766
[1mStep[0m  [28/42], [94mLoss[0m : 2.61563
[1mStep[0m  [32/42], [94mLoss[0m : 2.65497
[1mStep[0m  [36/42], [94mLoss[0m : 2.78691
[1mStep[0m  [40/42], [94mLoss[0m : 2.28254

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61511
[1mStep[0m  [4/42], [94mLoss[0m : 2.46157
[1mStep[0m  [8/42], [94mLoss[0m : 2.40144
[1mStep[0m  [12/42], [94mLoss[0m : 2.44199
[1mStep[0m  [16/42], [94mLoss[0m : 2.41896
[1mStep[0m  [20/42], [94mLoss[0m : 2.44489
[1mStep[0m  [24/42], [94mLoss[0m : 2.48463
[1mStep[0m  [28/42], [94mLoss[0m : 2.41609
[1mStep[0m  [32/42], [94mLoss[0m : 2.44849
[1mStep[0m  [36/42], [94mLoss[0m : 2.28745
[1mStep[0m  [40/42], [94mLoss[0m : 2.41867

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40120
[1mStep[0m  [4/42], [94mLoss[0m : 2.36033
[1mStep[0m  [8/42], [94mLoss[0m : 2.50223
[1mStep[0m  [12/42], [94mLoss[0m : 2.35951
[1mStep[0m  [16/42], [94mLoss[0m : 2.47031
[1mStep[0m  [20/42], [94mLoss[0m : 2.53667
[1mStep[0m  [24/42], [94mLoss[0m : 2.18477
[1mStep[0m  [28/42], [94mLoss[0m : 2.58765
[1mStep[0m  [32/42], [94mLoss[0m : 2.18625
[1mStep[0m  [36/42], [94mLoss[0m : 2.68656
[1mStep[0m  [40/42], [94mLoss[0m : 2.49659

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15951
[1mStep[0m  [4/42], [94mLoss[0m : 2.54334
[1mStep[0m  [8/42], [94mLoss[0m : 2.38618
[1mStep[0m  [12/42], [94mLoss[0m : 2.31791
[1mStep[0m  [16/42], [94mLoss[0m : 2.48782
[1mStep[0m  [20/42], [94mLoss[0m : 2.32643
[1mStep[0m  [24/42], [94mLoss[0m : 2.63674
[1mStep[0m  [28/42], [94mLoss[0m : 2.47726
[1mStep[0m  [32/42], [94mLoss[0m : 2.46130
[1mStep[0m  [36/42], [94mLoss[0m : 2.61536
[1mStep[0m  [40/42], [94mLoss[0m : 2.44121

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.380, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.355
====================================

Phase 1 - Evaluation MAE:  2.354653903416225
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.35612
[1mStep[0m  [4/42], [94mLoss[0m : 2.56614
[1mStep[0m  [8/42], [94mLoss[0m : 2.45296
[1mStep[0m  [12/42], [94mLoss[0m : 2.59207
[1mStep[0m  [16/42], [94mLoss[0m : 2.54725
[1mStep[0m  [20/42], [94mLoss[0m : 2.52272
[1mStep[0m  [24/42], [94mLoss[0m : 2.49590
[1mStep[0m  [28/42], [94mLoss[0m : 2.33778
[1mStep[0m  [32/42], [94mLoss[0m : 2.55839
[1mStep[0m  [36/42], [94mLoss[0m : 2.54657
[1mStep[0m  [40/42], [94mLoss[0m : 2.41258

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67080
[1mStep[0m  [4/42], [94mLoss[0m : 2.32746
[1mStep[0m  [8/42], [94mLoss[0m : 2.32174
[1mStep[0m  [12/42], [94mLoss[0m : 2.45159
[1mStep[0m  [16/42], [94mLoss[0m : 2.54903
[1mStep[0m  [20/42], [94mLoss[0m : 2.28862
[1mStep[0m  [24/42], [94mLoss[0m : 2.58206
[1mStep[0m  [28/42], [94mLoss[0m : 2.56358
[1mStep[0m  [32/42], [94mLoss[0m : 2.63580
[1mStep[0m  [36/42], [94mLoss[0m : 2.42854
[1mStep[0m  [40/42], [94mLoss[0m : 2.53211

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46942
[1mStep[0m  [4/42], [94mLoss[0m : 2.26671
[1mStep[0m  [8/42], [94mLoss[0m : 2.36143
[1mStep[0m  [12/42], [94mLoss[0m : 2.50590
[1mStep[0m  [16/42], [94mLoss[0m : 2.46724
[1mStep[0m  [20/42], [94mLoss[0m : 2.42798
[1mStep[0m  [24/42], [94mLoss[0m : 2.44011
[1mStep[0m  [28/42], [94mLoss[0m : 2.38216
[1mStep[0m  [32/42], [94mLoss[0m : 2.38050
[1mStep[0m  [36/42], [94mLoss[0m : 2.59012
[1mStep[0m  [40/42], [94mLoss[0m : 2.44413

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.610, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14344
[1mStep[0m  [4/42], [94mLoss[0m : 2.65565
[1mStep[0m  [8/42], [94mLoss[0m : 2.46470
[1mStep[0m  [12/42], [94mLoss[0m : 2.34573
[1mStep[0m  [16/42], [94mLoss[0m : 2.55207
[1mStep[0m  [20/42], [94mLoss[0m : 2.40797
[1mStep[0m  [24/42], [94mLoss[0m : 2.28391
[1mStep[0m  [28/42], [94mLoss[0m : 2.32504
[1mStep[0m  [32/42], [94mLoss[0m : 2.16821
[1mStep[0m  [36/42], [94mLoss[0m : 2.44899
[1mStep[0m  [40/42], [94mLoss[0m : 2.42542

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51936
[1mStep[0m  [4/42], [94mLoss[0m : 2.40859
[1mStep[0m  [8/42], [94mLoss[0m : 2.51818
[1mStep[0m  [12/42], [94mLoss[0m : 2.33506
[1mStep[0m  [16/42], [94mLoss[0m : 2.19788
[1mStep[0m  [20/42], [94mLoss[0m : 2.41043
[1mStep[0m  [24/42], [94mLoss[0m : 2.28305
[1mStep[0m  [28/42], [94mLoss[0m : 2.30693
[1mStep[0m  [32/42], [94mLoss[0m : 2.46734
[1mStep[0m  [36/42], [94mLoss[0m : 2.24744
[1mStep[0m  [40/42], [94mLoss[0m : 2.30964

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18750
[1mStep[0m  [4/42], [94mLoss[0m : 2.10913
[1mStep[0m  [8/42], [94mLoss[0m : 2.41586
[1mStep[0m  [12/42], [94mLoss[0m : 2.29647
[1mStep[0m  [16/42], [94mLoss[0m : 2.37897
[1mStep[0m  [20/42], [94mLoss[0m : 2.33950
[1mStep[0m  [24/42], [94mLoss[0m : 2.19943
[1mStep[0m  [28/42], [94mLoss[0m : 2.24975
[1mStep[0m  [32/42], [94mLoss[0m : 2.33931
[1mStep[0m  [36/42], [94mLoss[0m : 2.12501
[1mStep[0m  [40/42], [94mLoss[0m : 2.14536

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.582, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43658
[1mStep[0m  [4/42], [94mLoss[0m : 2.33122
[1mStep[0m  [8/42], [94mLoss[0m : 2.22465
[1mStep[0m  [12/42], [94mLoss[0m : 2.36731
[1mStep[0m  [16/42], [94mLoss[0m : 2.08607
[1mStep[0m  [20/42], [94mLoss[0m : 2.39526
[1mStep[0m  [24/42], [94mLoss[0m : 2.22390
[1mStep[0m  [28/42], [94mLoss[0m : 2.39675
[1mStep[0m  [32/42], [94mLoss[0m : 2.14707
[1mStep[0m  [36/42], [94mLoss[0m : 2.17712
[1mStep[0m  [40/42], [94mLoss[0m : 2.11799

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26811
[1mStep[0m  [4/42], [94mLoss[0m : 1.99525
[1mStep[0m  [8/42], [94mLoss[0m : 1.95659
[1mStep[0m  [12/42], [94mLoss[0m : 2.22971
[1mStep[0m  [16/42], [94mLoss[0m : 2.22103
[1mStep[0m  [20/42], [94mLoss[0m : 2.35061
[1mStep[0m  [24/42], [94mLoss[0m : 2.08655
[1mStep[0m  [28/42], [94mLoss[0m : 2.18030
[1mStep[0m  [32/42], [94mLoss[0m : 2.22721
[1mStep[0m  [36/42], [94mLoss[0m : 2.11120
[1mStep[0m  [40/42], [94mLoss[0m : 2.41238

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38715
[1mStep[0m  [4/42], [94mLoss[0m : 2.41475
[1mStep[0m  [8/42], [94mLoss[0m : 2.18251
[1mStep[0m  [12/42], [94mLoss[0m : 2.15332
[1mStep[0m  [16/42], [94mLoss[0m : 2.00304
[1mStep[0m  [20/42], [94mLoss[0m : 2.18181
[1mStep[0m  [24/42], [94mLoss[0m : 2.08165
[1mStep[0m  [28/42], [94mLoss[0m : 2.33957
[1mStep[0m  [32/42], [94mLoss[0m : 2.13389
[1mStep[0m  [36/42], [94mLoss[0m : 2.06075
[1mStep[0m  [40/42], [94mLoss[0m : 2.17157

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01998
[1mStep[0m  [4/42], [94mLoss[0m : 2.05948
[1mStep[0m  [8/42], [94mLoss[0m : 2.07251
[1mStep[0m  [12/42], [94mLoss[0m : 2.33287
[1mStep[0m  [16/42], [94mLoss[0m : 2.11856
[1mStep[0m  [20/42], [94mLoss[0m : 2.17188
[1mStep[0m  [24/42], [94mLoss[0m : 1.91923
[1mStep[0m  [28/42], [94mLoss[0m : 1.99469
[1mStep[0m  [32/42], [94mLoss[0m : 2.12640
[1mStep[0m  [36/42], [94mLoss[0m : 2.21874
[1mStep[0m  [40/42], [94mLoss[0m : 2.20843

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21320
[1mStep[0m  [4/42], [94mLoss[0m : 1.97131
[1mStep[0m  [8/42], [94mLoss[0m : 2.39650
[1mStep[0m  [12/42], [94mLoss[0m : 2.25497
[1mStep[0m  [16/42], [94mLoss[0m : 2.03894
[1mStep[0m  [20/42], [94mLoss[0m : 2.04601
[1mStep[0m  [24/42], [94mLoss[0m : 2.29395
[1mStep[0m  [28/42], [94mLoss[0m : 2.32048
[1mStep[0m  [32/42], [94mLoss[0m : 2.05049
[1mStep[0m  [36/42], [94mLoss[0m : 2.08665
[1mStep[0m  [40/42], [94mLoss[0m : 2.12609

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04270
[1mStep[0m  [4/42], [94mLoss[0m : 1.99228
[1mStep[0m  [8/42], [94mLoss[0m : 2.05898
[1mStep[0m  [12/42], [94mLoss[0m : 2.08442
[1mStep[0m  [16/42], [94mLoss[0m : 1.75226
[1mStep[0m  [20/42], [94mLoss[0m : 2.07218
[1mStep[0m  [24/42], [94mLoss[0m : 1.99407
[1mStep[0m  [28/42], [94mLoss[0m : 2.03630
[1mStep[0m  [32/42], [94mLoss[0m : 2.23861
[1mStep[0m  [36/42], [94mLoss[0m : 2.03002
[1mStep[0m  [40/42], [94mLoss[0m : 2.28783

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93059
[1mStep[0m  [4/42], [94mLoss[0m : 1.95634
[1mStep[0m  [8/42], [94mLoss[0m : 2.07098
[1mStep[0m  [12/42], [94mLoss[0m : 1.98881
[1mStep[0m  [16/42], [94mLoss[0m : 2.16078
[1mStep[0m  [20/42], [94mLoss[0m : 2.07582
[1mStep[0m  [24/42], [94mLoss[0m : 2.20894
[1mStep[0m  [28/42], [94mLoss[0m : 2.04154
[1mStep[0m  [32/42], [94mLoss[0m : 1.91978
[1mStep[0m  [36/42], [94mLoss[0m : 2.29375
[1mStep[0m  [40/42], [94mLoss[0m : 2.23083

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97521
[1mStep[0m  [4/42], [94mLoss[0m : 1.86082
[1mStep[0m  [8/42], [94mLoss[0m : 2.05108
[1mStep[0m  [12/42], [94mLoss[0m : 1.90491
[1mStep[0m  [16/42], [94mLoss[0m : 2.06079
[1mStep[0m  [20/42], [94mLoss[0m : 1.87705
[1mStep[0m  [24/42], [94mLoss[0m : 1.97563
[1mStep[0m  [28/42], [94mLoss[0m : 1.97298
[1mStep[0m  [32/42], [94mLoss[0m : 1.96443
[1mStep[0m  [36/42], [94mLoss[0m : 2.14310
[1mStep[0m  [40/42], [94mLoss[0m : 2.10153

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91219
[1mStep[0m  [4/42], [94mLoss[0m : 1.82206
[1mStep[0m  [8/42], [94mLoss[0m : 2.34535
[1mStep[0m  [12/42], [94mLoss[0m : 1.96250
[1mStep[0m  [16/42], [94mLoss[0m : 1.92643
[1mStep[0m  [20/42], [94mLoss[0m : 1.96271
[1mStep[0m  [24/42], [94mLoss[0m : 1.88205
[1mStep[0m  [28/42], [94mLoss[0m : 2.13440
[1mStep[0m  [32/42], [94mLoss[0m : 1.86382
[1mStep[0m  [36/42], [94mLoss[0m : 1.85989
[1mStep[0m  [40/42], [94mLoss[0m : 1.87521

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82294
[1mStep[0m  [4/42], [94mLoss[0m : 1.92575
[1mStep[0m  [8/42], [94mLoss[0m : 2.03216
[1mStep[0m  [12/42], [94mLoss[0m : 1.88975
[1mStep[0m  [16/42], [94mLoss[0m : 1.81000
[1mStep[0m  [20/42], [94mLoss[0m : 1.98672
[1mStep[0m  [24/42], [94mLoss[0m : 1.94834
[1mStep[0m  [28/42], [94mLoss[0m : 1.87019
[1mStep[0m  [32/42], [94mLoss[0m : 1.98449
[1mStep[0m  [36/42], [94mLoss[0m : 1.90387
[1mStep[0m  [40/42], [94mLoss[0m : 1.92616

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82645
[1mStep[0m  [4/42], [94mLoss[0m : 1.99228
[1mStep[0m  [8/42], [94mLoss[0m : 2.00944
[1mStep[0m  [12/42], [94mLoss[0m : 1.76596
[1mStep[0m  [16/42], [94mLoss[0m : 1.94147
[1mStep[0m  [20/42], [94mLoss[0m : 2.11619
[1mStep[0m  [24/42], [94mLoss[0m : 2.03449
[1mStep[0m  [28/42], [94mLoss[0m : 1.84995
[1mStep[0m  [32/42], [94mLoss[0m : 1.84254
[1mStep[0m  [36/42], [94mLoss[0m : 1.83436
[1mStep[0m  [40/42], [94mLoss[0m : 1.87568

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60291
[1mStep[0m  [4/42], [94mLoss[0m : 1.82125
[1mStep[0m  [8/42], [94mLoss[0m : 1.94305
[1mStep[0m  [12/42], [94mLoss[0m : 1.97115
[1mStep[0m  [16/42], [94mLoss[0m : 1.88287
[1mStep[0m  [20/42], [94mLoss[0m : 1.96308
[1mStep[0m  [24/42], [94mLoss[0m : 1.86160
[1mStep[0m  [28/42], [94mLoss[0m : 1.88195
[1mStep[0m  [32/42], [94mLoss[0m : 1.85277
[1mStep[0m  [36/42], [94mLoss[0m : 1.90347
[1mStep[0m  [40/42], [94mLoss[0m : 2.01039

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88068
[1mStep[0m  [4/42], [94mLoss[0m : 1.81458
[1mStep[0m  [8/42], [94mLoss[0m : 1.70925
[1mStep[0m  [12/42], [94mLoss[0m : 1.74040
[1mStep[0m  [16/42], [94mLoss[0m : 1.75024
[1mStep[0m  [20/42], [94mLoss[0m : 1.88001
[1mStep[0m  [24/42], [94mLoss[0m : 1.95161
[1mStep[0m  [28/42], [94mLoss[0m : 1.79307
[1mStep[0m  [32/42], [94mLoss[0m : 1.86042
[1mStep[0m  [36/42], [94mLoss[0m : 1.87783
[1mStep[0m  [40/42], [94mLoss[0m : 1.83370

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69379
[1mStep[0m  [4/42], [94mLoss[0m : 1.66829
[1mStep[0m  [8/42], [94mLoss[0m : 1.89152
[1mStep[0m  [12/42], [94mLoss[0m : 1.95881
[1mStep[0m  [16/42], [94mLoss[0m : 1.68779
[1mStep[0m  [20/42], [94mLoss[0m : 1.88403
[1mStep[0m  [24/42], [94mLoss[0m : 1.82642
[1mStep[0m  [28/42], [94mLoss[0m : 1.68721
[1mStep[0m  [32/42], [94mLoss[0m : 1.88523
[1mStep[0m  [36/42], [94mLoss[0m : 1.77917
[1mStep[0m  [40/42], [94mLoss[0m : 1.71706

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63040
[1mStep[0m  [4/42], [94mLoss[0m : 1.81788
[1mStep[0m  [8/42], [94mLoss[0m : 1.88289
[1mStep[0m  [12/42], [94mLoss[0m : 1.87035
[1mStep[0m  [16/42], [94mLoss[0m : 1.74549
[1mStep[0m  [20/42], [94mLoss[0m : 1.58303
[1mStep[0m  [24/42], [94mLoss[0m : 1.77066
[1mStep[0m  [28/42], [94mLoss[0m : 1.81300
[1mStep[0m  [32/42], [94mLoss[0m : 1.73425
[1mStep[0m  [36/42], [94mLoss[0m : 2.00125
[1mStep[0m  [40/42], [94mLoss[0m : 1.75759

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59287
[1mStep[0m  [4/42], [94mLoss[0m : 1.67631
[1mStep[0m  [8/42], [94mLoss[0m : 1.73582
[1mStep[0m  [12/42], [94mLoss[0m : 1.87123
[1mStep[0m  [16/42], [94mLoss[0m : 1.96389
[1mStep[0m  [20/42], [94mLoss[0m : 1.76852
[1mStep[0m  [24/42], [94mLoss[0m : 1.62613
[1mStep[0m  [28/42], [94mLoss[0m : 1.62223
[1mStep[0m  [32/42], [94mLoss[0m : 1.74367
[1mStep[0m  [36/42], [94mLoss[0m : 1.76538
[1mStep[0m  [40/42], [94mLoss[0m : 1.64541

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68392
[1mStep[0m  [4/42], [94mLoss[0m : 1.72246
[1mStep[0m  [8/42], [94mLoss[0m : 1.68695
[1mStep[0m  [12/42], [94mLoss[0m : 1.83103
[1mStep[0m  [16/42], [94mLoss[0m : 1.76676
[1mStep[0m  [20/42], [94mLoss[0m : 1.56721
[1mStep[0m  [24/42], [94mLoss[0m : 1.71167
[1mStep[0m  [28/42], [94mLoss[0m : 1.43926
[1mStep[0m  [32/42], [94mLoss[0m : 1.69501
[1mStep[0m  [36/42], [94mLoss[0m : 1.56957
[1mStep[0m  [40/42], [94mLoss[0m : 1.84603

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65555
[1mStep[0m  [4/42], [94mLoss[0m : 1.80775
[1mStep[0m  [8/42], [94mLoss[0m : 1.68143
[1mStep[0m  [12/42], [94mLoss[0m : 1.64381
[1mStep[0m  [16/42], [94mLoss[0m : 1.77508
[1mStep[0m  [20/42], [94mLoss[0m : 1.54612
[1mStep[0m  [24/42], [94mLoss[0m : 1.72637
[1mStep[0m  [28/42], [94mLoss[0m : 1.60659
[1mStep[0m  [32/42], [94mLoss[0m : 1.58504
[1mStep[0m  [36/42], [94mLoss[0m : 1.73605
[1mStep[0m  [40/42], [94mLoss[0m : 1.89356

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56313
[1mStep[0m  [4/42], [94mLoss[0m : 1.53583
[1mStep[0m  [8/42], [94mLoss[0m : 1.64082
[1mStep[0m  [12/42], [94mLoss[0m : 1.50566
[1mStep[0m  [16/42], [94mLoss[0m : 1.62160
[1mStep[0m  [20/42], [94mLoss[0m : 1.94217
[1mStep[0m  [24/42], [94mLoss[0m : 1.72489
[1mStep[0m  [28/42], [94mLoss[0m : 1.78049
[1mStep[0m  [32/42], [94mLoss[0m : 1.65186
[1mStep[0m  [36/42], [94mLoss[0m : 1.74522
[1mStep[0m  [40/42], [94mLoss[0m : 1.62742

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77677
[1mStep[0m  [4/42], [94mLoss[0m : 1.63973
[1mStep[0m  [8/42], [94mLoss[0m : 1.87238
[1mStep[0m  [12/42], [94mLoss[0m : 1.62193
[1mStep[0m  [16/42], [94mLoss[0m : 1.63975
[1mStep[0m  [20/42], [94mLoss[0m : 1.58597
[1mStep[0m  [24/42], [94mLoss[0m : 1.65609
[1mStep[0m  [28/42], [94mLoss[0m : 1.68002
[1mStep[0m  [32/42], [94mLoss[0m : 1.60846
[1mStep[0m  [36/42], [94mLoss[0m : 1.55539
[1mStep[0m  [40/42], [94mLoss[0m : 1.82645

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56065
[1mStep[0m  [4/42], [94mLoss[0m : 1.63294
[1mStep[0m  [8/42], [94mLoss[0m : 1.66646
[1mStep[0m  [12/42], [94mLoss[0m : 1.74798
[1mStep[0m  [16/42], [94mLoss[0m : 1.64387
[1mStep[0m  [20/42], [94mLoss[0m : 1.66218
[1mStep[0m  [24/42], [94mLoss[0m : 1.47497
[1mStep[0m  [28/42], [94mLoss[0m : 1.75728
[1mStep[0m  [32/42], [94mLoss[0m : 1.72332
[1mStep[0m  [36/42], [94mLoss[0m : 1.66078
[1mStep[0m  [40/42], [94mLoss[0m : 1.63404

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49371
[1mStep[0m  [4/42], [94mLoss[0m : 1.47853
[1mStep[0m  [8/42], [94mLoss[0m : 1.64744
[1mStep[0m  [12/42], [94mLoss[0m : 1.61824
[1mStep[0m  [16/42], [94mLoss[0m : 1.62602
[1mStep[0m  [20/42], [94mLoss[0m : 1.60177
[1mStep[0m  [24/42], [94mLoss[0m : 1.49000
[1mStep[0m  [28/42], [94mLoss[0m : 1.53224
[1mStep[0m  [32/42], [94mLoss[0m : 1.62399
[1mStep[0m  [36/42], [94mLoss[0m : 1.70049
[1mStep[0m  [40/42], [94mLoss[0m : 1.74123

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48562
[1mStep[0m  [4/42], [94mLoss[0m : 1.59671
[1mStep[0m  [8/42], [94mLoss[0m : 1.55844
[1mStep[0m  [12/42], [94mLoss[0m : 1.54745
[1mStep[0m  [16/42], [94mLoss[0m : 1.63815
[1mStep[0m  [20/42], [94mLoss[0m : 1.58104
[1mStep[0m  [24/42], [94mLoss[0m : 1.58743
[1mStep[0m  [28/42], [94mLoss[0m : 1.51622
[1mStep[0m  [32/42], [94mLoss[0m : 1.50877
[1mStep[0m  [36/42], [94mLoss[0m : 1.57639
[1mStep[0m  [40/42], [94mLoss[0m : 1.71231

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.64996
[1mStep[0m  [4/42], [94mLoss[0m : 1.55072
[1mStep[0m  [8/42], [94mLoss[0m : 1.65741
[1mStep[0m  [12/42], [94mLoss[0m : 1.52206
[1mStep[0m  [16/42], [94mLoss[0m : 1.61264
[1mStep[0m  [20/42], [94mLoss[0m : 1.61445
[1mStep[0m  [24/42], [94mLoss[0m : 1.73361
[1mStep[0m  [28/42], [94mLoss[0m : 1.63706
[1mStep[0m  [32/42], [94mLoss[0m : 1.67299
[1mStep[0m  [36/42], [94mLoss[0m : 1.48790
[1mStep[0m  [40/42], [94mLoss[0m : 1.63823

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.498
====================================

Phase 2 - Evaluation MAE:  2.4984017269951955
MAE score P1      2.354654
MAE score P2      2.498402
loss              1.585356
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.93091
[1mStep[0m  [4/42], [94mLoss[0m : 10.79083
[1mStep[0m  [8/42], [94mLoss[0m : 11.04601
[1mStep[0m  [12/42], [94mLoss[0m : 10.66650
[1mStep[0m  [16/42], [94mLoss[0m : 10.94586
[1mStep[0m  [20/42], [94mLoss[0m : 10.80836
[1mStep[0m  [24/42], [94mLoss[0m : 10.34194
[1mStep[0m  [28/42], [94mLoss[0m : 10.61711
[1mStep[0m  [32/42], [94mLoss[0m : 10.50688
[1mStep[0m  [36/42], [94mLoss[0m : 10.42596
[1mStep[0m  [40/42], [94mLoss[0m : 10.19607

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.670, [92mTest[0m: 10.919, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.88948
[1mStep[0m  [4/42], [94mLoss[0m : 10.17558
[1mStep[0m  [8/42], [94mLoss[0m : 10.14720
[1mStep[0m  [12/42], [94mLoss[0m : 9.99793
[1mStep[0m  [16/42], [94mLoss[0m : 9.96381
[1mStep[0m  [20/42], [94mLoss[0m : 9.89360
[1mStep[0m  [24/42], [94mLoss[0m : 9.62426
[1mStep[0m  [28/42], [94mLoss[0m : 9.69004
[1mStep[0m  [32/42], [94mLoss[0m : 9.68426
[1mStep[0m  [36/42], [94mLoss[0m : 9.35982
[1mStep[0m  [40/42], [94mLoss[0m : 9.32254

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.910, [92mTest[0m: 10.163, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.34551
[1mStep[0m  [4/42], [94mLoss[0m : 9.52645
[1mStep[0m  [8/42], [94mLoss[0m : 9.41384
[1mStep[0m  [12/42], [94mLoss[0m : 9.32823
[1mStep[0m  [16/42], [94mLoss[0m : 9.00546
[1mStep[0m  [20/42], [94mLoss[0m : 8.89417
[1mStep[0m  [24/42], [94mLoss[0m : 8.78342
[1mStep[0m  [28/42], [94mLoss[0m : 8.69101
[1mStep[0m  [32/42], [94mLoss[0m : 8.70384
[1mStep[0m  [36/42], [94mLoss[0m : 8.90677
[1mStep[0m  [40/42], [94mLoss[0m : 8.98551

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.971, [92mTest[0m: 9.087, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.55811
[1mStep[0m  [4/42], [94mLoss[0m : 8.18617
[1mStep[0m  [8/42], [94mLoss[0m : 8.30663
[1mStep[0m  [12/42], [94mLoss[0m : 8.29795
[1mStep[0m  [16/42], [94mLoss[0m : 8.65722
[1mStep[0m  [20/42], [94mLoss[0m : 8.16166
[1mStep[0m  [24/42], [94mLoss[0m : 7.63235
[1mStep[0m  [28/42], [94mLoss[0m : 7.74131
[1mStep[0m  [32/42], [94mLoss[0m : 7.69621
[1mStep[0m  [36/42], [94mLoss[0m : 7.87952
[1mStep[0m  [40/42], [94mLoss[0m : 7.74813

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.941, [92mTest[0m: 7.860, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.11344
[1mStep[0m  [4/42], [94mLoss[0m : 7.44746
[1mStep[0m  [8/42], [94mLoss[0m : 7.23351
[1mStep[0m  [12/42], [94mLoss[0m : 7.23762
[1mStep[0m  [16/42], [94mLoss[0m : 7.11111
[1mStep[0m  [20/42], [94mLoss[0m : 7.01068
[1mStep[0m  [24/42], [94mLoss[0m : 7.30999
[1mStep[0m  [28/42], [94mLoss[0m : 7.11439
[1mStep[0m  [32/42], [94mLoss[0m : 6.71246
[1mStep[0m  [36/42], [94mLoss[0m : 6.69339
[1mStep[0m  [40/42], [94mLoss[0m : 6.76725

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.046, [92mTest[0m: 6.696, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.49230
[1mStep[0m  [4/42], [94mLoss[0m : 6.42618
[1mStep[0m  [8/42], [94mLoss[0m : 6.65777
[1mStep[0m  [12/42], [94mLoss[0m : 6.24174
[1mStep[0m  [16/42], [94mLoss[0m : 6.39160
[1mStep[0m  [20/42], [94mLoss[0m : 6.29783
[1mStep[0m  [24/42], [94mLoss[0m : 6.36893
[1mStep[0m  [28/42], [94mLoss[0m : 6.52136
[1mStep[0m  [32/42], [94mLoss[0m : 6.25175
[1mStep[0m  [36/42], [94mLoss[0m : 5.94691
[1mStep[0m  [40/42], [94mLoss[0m : 5.95097

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.295, [92mTest[0m: 5.771, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.82248
[1mStep[0m  [4/42], [94mLoss[0m : 5.97190
[1mStep[0m  [8/42], [94mLoss[0m : 5.76185
[1mStep[0m  [12/42], [94mLoss[0m : 5.54331
[1mStep[0m  [16/42], [94mLoss[0m : 5.89751
[1mStep[0m  [20/42], [94mLoss[0m : 5.67199
[1mStep[0m  [24/42], [94mLoss[0m : 5.77402
[1mStep[0m  [28/42], [94mLoss[0m : 5.45314
[1mStep[0m  [32/42], [94mLoss[0m : 5.11383
[1mStep[0m  [36/42], [94mLoss[0m : 4.84894
[1mStep[0m  [40/42], [94mLoss[0m : 5.21361

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.492, [92mTest[0m: 4.873, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.11817
[1mStep[0m  [4/42], [94mLoss[0m : 4.96840
[1mStep[0m  [8/42], [94mLoss[0m : 4.57969
[1mStep[0m  [12/42], [94mLoss[0m : 4.90096
[1mStep[0m  [16/42], [94mLoss[0m : 4.74884
[1mStep[0m  [20/42], [94mLoss[0m : 4.32297
[1mStep[0m  [24/42], [94mLoss[0m : 4.63042
[1mStep[0m  [28/42], [94mLoss[0m : 4.68737
[1mStep[0m  [32/42], [94mLoss[0m : 4.73147
[1mStep[0m  [36/42], [94mLoss[0m : 4.20692
[1mStep[0m  [40/42], [94mLoss[0m : 4.03612

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.607, [92mTest[0m: 4.020, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.17944
[1mStep[0m  [4/42], [94mLoss[0m : 3.79016
[1mStep[0m  [8/42], [94mLoss[0m : 4.18986
[1mStep[0m  [12/42], [94mLoss[0m : 3.83481
[1mStep[0m  [16/42], [94mLoss[0m : 3.71586
[1mStep[0m  [20/42], [94mLoss[0m : 3.98639
[1mStep[0m  [24/42], [94mLoss[0m : 4.15531
[1mStep[0m  [28/42], [94mLoss[0m : 3.82883
[1mStep[0m  [32/42], [94mLoss[0m : 3.32559
[1mStep[0m  [36/42], [94mLoss[0m : 3.55434
[1mStep[0m  [40/42], [94mLoss[0m : 3.33214

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.756, [92mTest[0m: 3.222, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.32616
[1mStep[0m  [4/42], [94mLoss[0m : 3.03728
[1mStep[0m  [8/42], [94mLoss[0m : 3.11397
[1mStep[0m  [12/42], [94mLoss[0m : 3.26159
[1mStep[0m  [16/42], [94mLoss[0m : 3.10390
[1mStep[0m  [20/42], [94mLoss[0m : 3.12938
[1mStep[0m  [24/42], [94mLoss[0m : 3.39681
[1mStep[0m  [28/42], [94mLoss[0m : 3.24304
[1mStep[0m  [32/42], [94mLoss[0m : 2.96520
[1mStep[0m  [36/42], [94mLoss[0m : 2.89484
[1mStep[0m  [40/42], [94mLoss[0m : 3.07983

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.095, [92mTest[0m: 2.617, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86092
[1mStep[0m  [4/42], [94mLoss[0m : 3.08819
[1mStep[0m  [8/42], [94mLoss[0m : 2.95125
[1mStep[0m  [12/42], [94mLoss[0m : 2.92254
[1mStep[0m  [16/42], [94mLoss[0m : 2.86919
[1mStep[0m  [20/42], [94mLoss[0m : 3.04012
[1mStep[0m  [24/42], [94mLoss[0m : 2.87747
[1mStep[0m  [28/42], [94mLoss[0m : 2.84543
[1mStep[0m  [32/42], [94mLoss[0m : 2.70327
[1mStep[0m  [36/42], [94mLoss[0m : 2.97928
[1mStep[0m  [40/42], [94mLoss[0m : 2.85847

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.843, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56583
[1mStep[0m  [4/42], [94mLoss[0m : 2.96747
[1mStep[0m  [8/42], [94mLoss[0m : 2.78876
[1mStep[0m  [12/42], [94mLoss[0m : 2.70774
[1mStep[0m  [16/42], [94mLoss[0m : 2.64728
[1mStep[0m  [20/42], [94mLoss[0m : 2.79701
[1mStep[0m  [24/42], [94mLoss[0m : 2.92584
[1mStep[0m  [28/42], [94mLoss[0m : 3.02970
[1mStep[0m  [32/42], [94mLoss[0m : 2.71114
[1mStep[0m  [36/42], [94mLoss[0m : 2.81771
[1mStep[0m  [40/42], [94mLoss[0m : 2.83311

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.770, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79422
[1mStep[0m  [4/42], [94mLoss[0m : 2.68397
[1mStep[0m  [8/42], [94mLoss[0m : 2.78738
[1mStep[0m  [12/42], [94mLoss[0m : 2.79280
[1mStep[0m  [16/42], [94mLoss[0m : 2.63277
[1mStep[0m  [20/42], [94mLoss[0m : 2.77594
[1mStep[0m  [24/42], [94mLoss[0m : 2.58749
[1mStep[0m  [28/42], [94mLoss[0m : 2.74701
[1mStep[0m  [32/42], [94mLoss[0m : 2.58811
[1mStep[0m  [36/42], [94mLoss[0m : 2.61273
[1mStep[0m  [40/42], [94mLoss[0m : 2.96474

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79509
[1mStep[0m  [4/42], [94mLoss[0m : 2.69632
[1mStep[0m  [8/42], [94mLoss[0m : 2.83760
[1mStep[0m  [12/42], [94mLoss[0m : 2.96076
[1mStep[0m  [16/42], [94mLoss[0m : 2.52450
[1mStep[0m  [20/42], [94mLoss[0m : 2.89121
[1mStep[0m  [24/42], [94mLoss[0m : 2.71425
[1mStep[0m  [28/42], [94mLoss[0m : 2.55686
[1mStep[0m  [32/42], [94mLoss[0m : 2.79332
[1mStep[0m  [36/42], [94mLoss[0m : 2.68545
[1mStep[0m  [40/42], [94mLoss[0m : 2.50397

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63107
[1mStep[0m  [4/42], [94mLoss[0m : 2.52055
[1mStep[0m  [8/42], [94mLoss[0m : 2.53346
[1mStep[0m  [12/42], [94mLoss[0m : 2.93789
[1mStep[0m  [16/42], [94mLoss[0m : 2.56920
[1mStep[0m  [20/42], [94mLoss[0m : 2.73332
[1mStep[0m  [24/42], [94mLoss[0m : 2.76818
[1mStep[0m  [28/42], [94mLoss[0m : 2.81246
[1mStep[0m  [32/42], [94mLoss[0m : 2.69142
[1mStep[0m  [36/42], [94mLoss[0m : 2.74753
[1mStep[0m  [40/42], [94mLoss[0m : 2.74447

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86138
[1mStep[0m  [4/42], [94mLoss[0m : 2.81221
[1mStep[0m  [8/42], [94mLoss[0m : 2.47537
[1mStep[0m  [12/42], [94mLoss[0m : 2.57622
[1mStep[0m  [16/42], [94mLoss[0m : 2.44717
[1mStep[0m  [20/42], [94mLoss[0m : 2.99019
[1mStep[0m  [24/42], [94mLoss[0m : 2.92617
[1mStep[0m  [28/42], [94mLoss[0m : 2.79202
[1mStep[0m  [32/42], [94mLoss[0m : 2.44658
[1mStep[0m  [36/42], [94mLoss[0m : 2.57831
[1mStep[0m  [40/42], [94mLoss[0m : 2.65430

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42846
[1mStep[0m  [4/42], [94mLoss[0m : 2.56936
[1mStep[0m  [8/42], [94mLoss[0m : 2.45848
[1mStep[0m  [12/42], [94mLoss[0m : 2.59571
[1mStep[0m  [16/42], [94mLoss[0m : 2.95579
[1mStep[0m  [20/42], [94mLoss[0m : 2.67699
[1mStep[0m  [24/42], [94mLoss[0m : 2.69611
[1mStep[0m  [28/42], [94mLoss[0m : 2.57723
[1mStep[0m  [32/42], [94mLoss[0m : 2.91019
[1mStep[0m  [36/42], [94mLoss[0m : 2.59128
[1mStep[0m  [40/42], [94mLoss[0m : 2.76613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77801
[1mStep[0m  [4/42], [94mLoss[0m : 2.63521
[1mStep[0m  [8/42], [94mLoss[0m : 2.60868
[1mStep[0m  [12/42], [94mLoss[0m : 2.61451
[1mStep[0m  [16/42], [94mLoss[0m : 2.84158
[1mStep[0m  [20/42], [94mLoss[0m : 2.69093
[1mStep[0m  [24/42], [94mLoss[0m : 2.67900
[1mStep[0m  [28/42], [94mLoss[0m : 2.78422
[1mStep[0m  [32/42], [94mLoss[0m : 2.94025
[1mStep[0m  [36/42], [94mLoss[0m : 2.67575
[1mStep[0m  [40/42], [94mLoss[0m : 2.52082

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69575
[1mStep[0m  [4/42], [94mLoss[0m : 2.68157
[1mStep[0m  [8/42], [94mLoss[0m : 2.57977
[1mStep[0m  [12/42], [94mLoss[0m : 2.74847
[1mStep[0m  [16/42], [94mLoss[0m : 2.52197
[1mStep[0m  [20/42], [94mLoss[0m : 2.71228
[1mStep[0m  [24/42], [94mLoss[0m : 2.81650
[1mStep[0m  [28/42], [94mLoss[0m : 2.74665
[1mStep[0m  [32/42], [94mLoss[0m : 2.52118
[1mStep[0m  [36/42], [94mLoss[0m : 2.73205
[1mStep[0m  [40/42], [94mLoss[0m : 2.67606

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73222
[1mStep[0m  [4/42], [94mLoss[0m : 2.75911
[1mStep[0m  [8/42], [94mLoss[0m : 2.69422
[1mStep[0m  [12/42], [94mLoss[0m : 2.59216
[1mStep[0m  [16/42], [94mLoss[0m : 2.90706
[1mStep[0m  [20/42], [94mLoss[0m : 2.84829
[1mStep[0m  [24/42], [94mLoss[0m : 2.51466
[1mStep[0m  [28/42], [94mLoss[0m : 2.87159
[1mStep[0m  [32/42], [94mLoss[0m : 2.82246
[1mStep[0m  [36/42], [94mLoss[0m : 2.69601
[1mStep[0m  [40/42], [94mLoss[0m : 2.77521

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.367, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63054
[1mStep[0m  [4/42], [94mLoss[0m : 2.49792
[1mStep[0m  [8/42], [94mLoss[0m : 2.51551
[1mStep[0m  [12/42], [94mLoss[0m : 2.72844
[1mStep[0m  [16/42], [94mLoss[0m : 2.67021
[1mStep[0m  [20/42], [94mLoss[0m : 2.68016
[1mStep[0m  [24/42], [94mLoss[0m : 2.51750
[1mStep[0m  [28/42], [94mLoss[0m : 2.87377
[1mStep[0m  [32/42], [94mLoss[0m : 2.60952
[1mStep[0m  [36/42], [94mLoss[0m : 2.63542
[1mStep[0m  [40/42], [94mLoss[0m : 2.83162

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54799
[1mStep[0m  [4/42], [94mLoss[0m : 2.64170
[1mStep[0m  [8/42], [94mLoss[0m : 2.65041
[1mStep[0m  [12/42], [94mLoss[0m : 2.71482
[1mStep[0m  [16/42], [94mLoss[0m : 2.72316
[1mStep[0m  [20/42], [94mLoss[0m : 2.71892
[1mStep[0m  [24/42], [94mLoss[0m : 2.71664
[1mStep[0m  [28/42], [94mLoss[0m : 2.67038
[1mStep[0m  [32/42], [94mLoss[0m : 2.59888
[1mStep[0m  [36/42], [94mLoss[0m : 2.59899
[1mStep[0m  [40/42], [94mLoss[0m : 2.60436

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79863
[1mStep[0m  [4/42], [94mLoss[0m : 2.67566
[1mStep[0m  [8/42], [94mLoss[0m : 2.68989
[1mStep[0m  [12/42], [94mLoss[0m : 2.53930
[1mStep[0m  [16/42], [94mLoss[0m : 2.43376
[1mStep[0m  [20/42], [94mLoss[0m : 2.78663
[1mStep[0m  [24/42], [94mLoss[0m : 2.54421
[1mStep[0m  [28/42], [94mLoss[0m : 2.63578
[1mStep[0m  [32/42], [94mLoss[0m : 2.57614
[1mStep[0m  [36/42], [94mLoss[0m : 2.65895
[1mStep[0m  [40/42], [94mLoss[0m : 2.55656

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55108
[1mStep[0m  [4/42], [94mLoss[0m : 2.67401
[1mStep[0m  [8/42], [94mLoss[0m : 2.57319
[1mStep[0m  [12/42], [94mLoss[0m : 2.33277
[1mStep[0m  [16/42], [94mLoss[0m : 2.41795
[1mStep[0m  [20/42], [94mLoss[0m : 2.56426
[1mStep[0m  [24/42], [94mLoss[0m : 2.53757
[1mStep[0m  [28/42], [94mLoss[0m : 2.75894
[1mStep[0m  [32/42], [94mLoss[0m : 2.41311
[1mStep[0m  [36/42], [94mLoss[0m : 3.04447
[1mStep[0m  [40/42], [94mLoss[0m : 2.52796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.357, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62002
[1mStep[0m  [4/42], [94mLoss[0m : 2.61451
[1mStep[0m  [8/42], [94mLoss[0m : 2.61145
[1mStep[0m  [12/42], [94mLoss[0m : 2.27534
[1mStep[0m  [16/42], [94mLoss[0m : 2.80681
[1mStep[0m  [20/42], [94mLoss[0m : 2.39276
[1mStep[0m  [24/42], [94mLoss[0m : 2.64214
[1mStep[0m  [28/42], [94mLoss[0m : 2.64877
[1mStep[0m  [32/42], [94mLoss[0m : 2.67327
[1mStep[0m  [36/42], [94mLoss[0m : 2.87347
[1mStep[0m  [40/42], [94mLoss[0m : 2.68768

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47535
[1mStep[0m  [4/42], [94mLoss[0m : 2.54179
[1mStep[0m  [8/42], [94mLoss[0m : 2.53187
[1mStep[0m  [12/42], [94mLoss[0m : 2.62575
[1mStep[0m  [16/42], [94mLoss[0m : 2.55253
[1mStep[0m  [20/42], [94mLoss[0m : 2.83182
[1mStep[0m  [24/42], [94mLoss[0m : 2.66851
[1mStep[0m  [28/42], [94mLoss[0m : 2.80588
[1mStep[0m  [32/42], [94mLoss[0m : 2.74641
[1mStep[0m  [36/42], [94mLoss[0m : 2.73486
[1mStep[0m  [40/42], [94mLoss[0m : 2.66016

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34620
[1mStep[0m  [4/42], [94mLoss[0m : 2.63962
[1mStep[0m  [8/42], [94mLoss[0m : 2.53465
[1mStep[0m  [12/42], [94mLoss[0m : 2.65795
[1mStep[0m  [16/42], [94mLoss[0m : 2.72767
[1mStep[0m  [20/42], [94mLoss[0m : 2.42670
[1mStep[0m  [24/42], [94mLoss[0m : 2.46739
[1mStep[0m  [28/42], [94mLoss[0m : 2.61656
[1mStep[0m  [32/42], [94mLoss[0m : 2.73914
[1mStep[0m  [36/42], [94mLoss[0m : 2.63678
[1mStep[0m  [40/42], [94mLoss[0m : 2.67671

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55168
[1mStep[0m  [4/42], [94mLoss[0m : 2.63034
[1mStep[0m  [8/42], [94mLoss[0m : 2.50411
[1mStep[0m  [12/42], [94mLoss[0m : 2.62648
[1mStep[0m  [16/42], [94mLoss[0m : 2.60179
[1mStep[0m  [20/42], [94mLoss[0m : 2.78647
[1mStep[0m  [24/42], [94mLoss[0m : 2.92658
[1mStep[0m  [28/42], [94mLoss[0m : 2.66717
[1mStep[0m  [32/42], [94mLoss[0m : 2.75803
[1mStep[0m  [36/42], [94mLoss[0m : 2.52766
[1mStep[0m  [40/42], [94mLoss[0m : 2.35892

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.369, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40924
[1mStep[0m  [4/42], [94mLoss[0m : 2.46097
[1mStep[0m  [8/42], [94mLoss[0m : 2.63171
[1mStep[0m  [12/42], [94mLoss[0m : 2.75984
[1mStep[0m  [16/42], [94mLoss[0m : 2.50871
[1mStep[0m  [20/42], [94mLoss[0m : 2.62679
[1mStep[0m  [24/42], [94mLoss[0m : 2.73252
[1mStep[0m  [28/42], [94mLoss[0m : 2.58474
[1mStep[0m  [32/42], [94mLoss[0m : 2.54384
[1mStep[0m  [36/42], [94mLoss[0m : 2.77583
[1mStep[0m  [40/42], [94mLoss[0m : 2.61849

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51097
[1mStep[0m  [4/42], [94mLoss[0m : 2.61678
[1mStep[0m  [8/42], [94mLoss[0m : 2.88448
[1mStep[0m  [12/42], [94mLoss[0m : 2.71395
[1mStep[0m  [16/42], [94mLoss[0m : 2.47119
[1mStep[0m  [20/42], [94mLoss[0m : 2.50689
[1mStep[0m  [24/42], [94mLoss[0m : 2.44736
[1mStep[0m  [28/42], [94mLoss[0m : 2.37931
[1mStep[0m  [32/42], [94mLoss[0m : 2.41019
[1mStep[0m  [36/42], [94mLoss[0m : 2.47440
[1mStep[0m  [40/42], [94mLoss[0m : 2.33536

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.357
====================================

Phase 1 - Evaluation MAE:  2.356877940041678
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.72427
[1mStep[0m  [4/42], [94mLoss[0m : 2.69106
[1mStep[0m  [8/42], [94mLoss[0m : 2.30819
[1mStep[0m  [12/42], [94mLoss[0m : 2.83569
[1mStep[0m  [16/42], [94mLoss[0m : 2.63004
[1mStep[0m  [20/42], [94mLoss[0m : 2.57912
[1mStep[0m  [24/42], [94mLoss[0m : 2.60502
[1mStep[0m  [28/42], [94mLoss[0m : 2.60030
[1mStep[0m  [32/42], [94mLoss[0m : 2.48073
[1mStep[0m  [36/42], [94mLoss[0m : 2.72356
[1mStep[0m  [40/42], [94mLoss[0m : 2.50703

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62079
[1mStep[0m  [4/42], [94mLoss[0m : 2.63401
[1mStep[0m  [8/42], [94mLoss[0m : 2.67815
[1mStep[0m  [12/42], [94mLoss[0m : 2.68216
[1mStep[0m  [16/42], [94mLoss[0m : 2.63450
[1mStep[0m  [20/42], [94mLoss[0m : 2.59993
[1mStep[0m  [24/42], [94mLoss[0m : 2.90341
[1mStep[0m  [28/42], [94mLoss[0m : 2.46512
[1mStep[0m  [32/42], [94mLoss[0m : 2.63867
[1mStep[0m  [36/42], [94mLoss[0m : 2.85316
[1mStep[0m  [40/42], [94mLoss[0m : 2.61844

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45018
[1mStep[0m  [4/42], [94mLoss[0m : 2.55190
[1mStep[0m  [8/42], [94mLoss[0m : 2.54273
[1mStep[0m  [12/42], [94mLoss[0m : 2.58492
[1mStep[0m  [16/42], [94mLoss[0m : 2.28920
[1mStep[0m  [20/42], [94mLoss[0m : 2.40446
[1mStep[0m  [24/42], [94mLoss[0m : 2.57986
[1mStep[0m  [28/42], [94mLoss[0m : 2.72156
[1mStep[0m  [32/42], [94mLoss[0m : 2.77145
[1mStep[0m  [36/42], [94mLoss[0m : 2.58220
[1mStep[0m  [40/42], [94mLoss[0m : 2.68650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38692
[1mStep[0m  [4/42], [94mLoss[0m : 2.43187
[1mStep[0m  [8/42], [94mLoss[0m : 2.38281
[1mStep[0m  [12/42], [94mLoss[0m : 2.54431
[1mStep[0m  [16/42], [94mLoss[0m : 2.48447
[1mStep[0m  [20/42], [94mLoss[0m : 2.67051
[1mStep[0m  [24/42], [94mLoss[0m : 2.38292
[1mStep[0m  [28/42], [94mLoss[0m : 2.37160
[1mStep[0m  [32/42], [94mLoss[0m : 2.32546
[1mStep[0m  [36/42], [94mLoss[0m : 2.58918
[1mStep[0m  [40/42], [94mLoss[0m : 2.59302

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.578, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54453
[1mStep[0m  [4/42], [94mLoss[0m : 2.36456
[1mStep[0m  [8/42], [94mLoss[0m : 2.45036
[1mStep[0m  [12/42], [94mLoss[0m : 2.41454
[1mStep[0m  [16/42], [94mLoss[0m : 2.64165
[1mStep[0m  [20/42], [94mLoss[0m : 2.51196
[1mStep[0m  [24/42], [94mLoss[0m : 2.62974
[1mStep[0m  [28/42], [94mLoss[0m : 2.45422
[1mStep[0m  [32/42], [94mLoss[0m : 2.48688
[1mStep[0m  [36/42], [94mLoss[0m : 2.66782
[1mStep[0m  [40/42], [94mLoss[0m : 2.55496

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38889
[1mStep[0m  [4/42], [94mLoss[0m : 2.64467
[1mStep[0m  [8/42], [94mLoss[0m : 2.43193
[1mStep[0m  [12/42], [94mLoss[0m : 2.50737
[1mStep[0m  [16/42], [94mLoss[0m : 2.36713
[1mStep[0m  [20/42], [94mLoss[0m : 2.44797
[1mStep[0m  [24/42], [94mLoss[0m : 2.56717
[1mStep[0m  [28/42], [94mLoss[0m : 2.41042
[1mStep[0m  [32/42], [94mLoss[0m : 2.40762
[1mStep[0m  [36/42], [94mLoss[0m : 2.21401
[1mStep[0m  [40/42], [94mLoss[0m : 2.51268

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.744, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38231
[1mStep[0m  [4/42], [94mLoss[0m : 2.38859
[1mStep[0m  [8/42], [94mLoss[0m : 2.47690
[1mStep[0m  [12/42], [94mLoss[0m : 2.29009
[1mStep[0m  [16/42], [94mLoss[0m : 2.44255
[1mStep[0m  [20/42], [94mLoss[0m : 2.31808
[1mStep[0m  [24/42], [94mLoss[0m : 2.36140
[1mStep[0m  [28/42], [94mLoss[0m : 2.37232
[1mStep[0m  [32/42], [94mLoss[0m : 2.30868
[1mStep[0m  [36/42], [94mLoss[0m : 2.36459
[1mStep[0m  [40/42], [94mLoss[0m : 2.42227

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.672, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35068
[1mStep[0m  [4/42], [94mLoss[0m : 2.55545
[1mStep[0m  [8/42], [94mLoss[0m : 2.48154
[1mStep[0m  [12/42], [94mLoss[0m : 2.25347
[1mStep[0m  [16/42], [94mLoss[0m : 2.46717
[1mStep[0m  [20/42], [94mLoss[0m : 1.98252
[1mStep[0m  [24/42], [94mLoss[0m : 2.52940
[1mStep[0m  [28/42], [94mLoss[0m : 2.43275
[1mStep[0m  [32/42], [94mLoss[0m : 2.32841
[1mStep[0m  [36/42], [94mLoss[0m : 2.17807
[1mStep[0m  [40/42], [94mLoss[0m : 2.53847

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.687, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28357
[1mStep[0m  [4/42], [94mLoss[0m : 2.46395
[1mStep[0m  [8/42], [94mLoss[0m : 2.40606
[1mStep[0m  [12/42], [94mLoss[0m : 2.35899
[1mStep[0m  [16/42], [94mLoss[0m : 2.32363
[1mStep[0m  [20/42], [94mLoss[0m : 2.41422
[1mStep[0m  [24/42], [94mLoss[0m : 2.30770
[1mStep[0m  [28/42], [94mLoss[0m : 2.29426
[1mStep[0m  [32/42], [94mLoss[0m : 2.43619
[1mStep[0m  [36/42], [94mLoss[0m : 2.24117
[1mStep[0m  [40/42], [94mLoss[0m : 2.39015

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22843
[1mStep[0m  [4/42], [94mLoss[0m : 2.27829
[1mStep[0m  [8/42], [94mLoss[0m : 2.34240
[1mStep[0m  [12/42], [94mLoss[0m : 2.06975
[1mStep[0m  [16/42], [94mLoss[0m : 2.37614
[1mStep[0m  [20/42], [94mLoss[0m : 2.18733
[1mStep[0m  [24/42], [94mLoss[0m : 2.39978
[1mStep[0m  [28/42], [94mLoss[0m : 2.22202
[1mStep[0m  [32/42], [94mLoss[0m : 2.40512
[1mStep[0m  [36/42], [94mLoss[0m : 2.34410
[1mStep[0m  [40/42], [94mLoss[0m : 2.39554

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.586, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43523
[1mStep[0m  [4/42], [94mLoss[0m : 2.26626
[1mStep[0m  [8/42], [94mLoss[0m : 2.13568
[1mStep[0m  [12/42], [94mLoss[0m : 2.15586
[1mStep[0m  [16/42], [94mLoss[0m : 2.35628
[1mStep[0m  [20/42], [94mLoss[0m : 2.21233
[1mStep[0m  [24/42], [94mLoss[0m : 2.30467
[1mStep[0m  [28/42], [94mLoss[0m : 2.24338
[1mStep[0m  [32/42], [94mLoss[0m : 2.43676
[1mStep[0m  [36/42], [94mLoss[0m : 2.36500
[1mStep[0m  [40/42], [94mLoss[0m : 2.08606

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.661, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36050
[1mStep[0m  [4/42], [94mLoss[0m : 2.20568
[1mStep[0m  [8/42], [94mLoss[0m : 2.24693
[1mStep[0m  [12/42], [94mLoss[0m : 2.19703
[1mStep[0m  [16/42], [94mLoss[0m : 2.18411
[1mStep[0m  [20/42], [94mLoss[0m : 2.13436
[1mStep[0m  [24/42], [94mLoss[0m : 2.44328
[1mStep[0m  [28/42], [94mLoss[0m : 2.26388
[1mStep[0m  [32/42], [94mLoss[0m : 2.20593
[1mStep[0m  [36/42], [94mLoss[0m : 2.40597
[1mStep[0m  [40/42], [94mLoss[0m : 2.10443

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.581, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18151
[1mStep[0m  [4/42], [94mLoss[0m : 2.14994
[1mStep[0m  [8/42], [94mLoss[0m : 2.16723
[1mStep[0m  [12/42], [94mLoss[0m : 2.12790
[1mStep[0m  [16/42], [94mLoss[0m : 2.15003
[1mStep[0m  [20/42], [94mLoss[0m : 2.31143
[1mStep[0m  [24/42], [94mLoss[0m : 2.20345
[1mStep[0m  [28/42], [94mLoss[0m : 2.28400
[1mStep[0m  [32/42], [94mLoss[0m : 2.11542
[1mStep[0m  [36/42], [94mLoss[0m : 2.21083
[1mStep[0m  [40/42], [94mLoss[0m : 2.04750

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11895
[1mStep[0m  [4/42], [94mLoss[0m : 2.08409
[1mStep[0m  [8/42], [94mLoss[0m : 2.09542
[1mStep[0m  [12/42], [94mLoss[0m : 2.06416
[1mStep[0m  [16/42], [94mLoss[0m : 2.24396
[1mStep[0m  [20/42], [94mLoss[0m : 2.02351
[1mStep[0m  [24/42], [94mLoss[0m : 2.10811
[1mStep[0m  [28/42], [94mLoss[0m : 2.31102
[1mStep[0m  [32/42], [94mLoss[0m : 2.15375
[1mStep[0m  [36/42], [94mLoss[0m : 2.14367
[1mStep[0m  [40/42], [94mLoss[0m : 2.31465

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07810
[1mStep[0m  [4/42], [94mLoss[0m : 2.23764
[1mStep[0m  [8/42], [94mLoss[0m : 1.94915
[1mStep[0m  [12/42], [94mLoss[0m : 2.17855
[1mStep[0m  [16/42], [94mLoss[0m : 2.30042
[1mStep[0m  [20/42], [94mLoss[0m : 2.32203
[1mStep[0m  [24/42], [94mLoss[0m : 2.01007
[1mStep[0m  [28/42], [94mLoss[0m : 2.09869
[1mStep[0m  [32/42], [94mLoss[0m : 2.16852
[1mStep[0m  [36/42], [94mLoss[0m : 2.24693
[1mStep[0m  [40/42], [94mLoss[0m : 2.20311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96096
[1mStep[0m  [4/42], [94mLoss[0m : 2.00119
[1mStep[0m  [8/42], [94mLoss[0m : 1.87208
[1mStep[0m  [12/42], [94mLoss[0m : 2.07415
[1mStep[0m  [16/42], [94mLoss[0m : 2.10844
[1mStep[0m  [20/42], [94mLoss[0m : 2.09088
[1mStep[0m  [24/42], [94mLoss[0m : 2.04360
[1mStep[0m  [28/42], [94mLoss[0m : 2.11567
[1mStep[0m  [32/42], [94mLoss[0m : 1.97038
[1mStep[0m  [36/42], [94mLoss[0m : 2.23117
[1mStep[0m  [40/42], [94mLoss[0m : 2.14713

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02153
[1mStep[0m  [4/42], [94mLoss[0m : 2.05754
[1mStep[0m  [8/42], [94mLoss[0m : 2.19906
[1mStep[0m  [12/42], [94mLoss[0m : 1.85405
[1mStep[0m  [16/42], [94mLoss[0m : 2.04146
[1mStep[0m  [20/42], [94mLoss[0m : 2.01232
[1mStep[0m  [24/42], [94mLoss[0m : 2.01328
[1mStep[0m  [28/42], [94mLoss[0m : 2.18195
[1mStep[0m  [32/42], [94mLoss[0m : 2.00375
[1mStep[0m  [36/42], [94mLoss[0m : 1.95329
[1mStep[0m  [40/42], [94mLoss[0m : 1.95831

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.038, [92mTest[0m: 2.498, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14699
[1mStep[0m  [4/42], [94mLoss[0m : 2.06696
[1mStep[0m  [8/42], [94mLoss[0m : 1.91838
[1mStep[0m  [12/42], [94mLoss[0m : 1.94930
[1mStep[0m  [16/42], [94mLoss[0m : 2.03781
[1mStep[0m  [20/42], [94mLoss[0m : 1.92346
[1mStep[0m  [24/42], [94mLoss[0m : 2.10003
[1mStep[0m  [28/42], [94mLoss[0m : 1.90590
[1mStep[0m  [32/42], [94mLoss[0m : 2.01934
[1mStep[0m  [36/42], [94mLoss[0m : 2.02342
[1mStep[0m  [40/42], [94mLoss[0m : 1.94814

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.570, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99641
[1mStep[0m  [4/42], [94mLoss[0m : 1.89552
[1mStep[0m  [8/42], [94mLoss[0m : 1.87862
[1mStep[0m  [12/42], [94mLoss[0m : 1.96103
[1mStep[0m  [16/42], [94mLoss[0m : 2.08263
[1mStep[0m  [20/42], [94mLoss[0m : 1.87161
[1mStep[0m  [24/42], [94mLoss[0m : 1.94599
[1mStep[0m  [28/42], [94mLoss[0m : 2.08766
[1mStep[0m  [32/42], [94mLoss[0m : 2.00889
[1mStep[0m  [36/42], [94mLoss[0m : 1.85137
[1mStep[0m  [40/42], [94mLoss[0m : 1.97880

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99499
[1mStep[0m  [4/42], [94mLoss[0m : 1.73055
[1mStep[0m  [8/42], [94mLoss[0m : 1.99405
[1mStep[0m  [12/42], [94mLoss[0m : 1.89169
[1mStep[0m  [16/42], [94mLoss[0m : 1.82811
[1mStep[0m  [20/42], [94mLoss[0m : 1.95226
[1mStep[0m  [24/42], [94mLoss[0m : 1.88672
[1mStep[0m  [28/42], [94mLoss[0m : 2.00112
[1mStep[0m  [32/42], [94mLoss[0m : 1.92126
[1mStep[0m  [36/42], [94mLoss[0m : 1.71833
[1mStep[0m  [40/42], [94mLoss[0m : 1.99771

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.589, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98723
[1mStep[0m  [4/42], [94mLoss[0m : 2.08208
[1mStep[0m  [8/42], [94mLoss[0m : 1.79353
[1mStep[0m  [12/42], [94mLoss[0m : 1.81949
[1mStep[0m  [16/42], [94mLoss[0m : 1.76482
[1mStep[0m  [20/42], [94mLoss[0m : 2.10026
[1mStep[0m  [24/42], [94mLoss[0m : 1.81123
[1mStep[0m  [28/42], [94mLoss[0m : 1.86083
[1mStep[0m  [32/42], [94mLoss[0m : 1.88594
[1mStep[0m  [36/42], [94mLoss[0m : 1.71562
[1mStep[0m  [40/42], [94mLoss[0m : 1.76926

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.79975
[1mStep[0m  [4/42], [94mLoss[0m : 1.91000
[1mStep[0m  [8/42], [94mLoss[0m : 1.82025
[1mStep[0m  [12/42], [94mLoss[0m : 1.86216
[1mStep[0m  [16/42], [94mLoss[0m : 2.03722
[1mStep[0m  [20/42], [94mLoss[0m : 2.03772
[1mStep[0m  [24/42], [94mLoss[0m : 1.82416
[1mStep[0m  [28/42], [94mLoss[0m : 1.96431
[1mStep[0m  [32/42], [94mLoss[0m : 1.76991
[1mStep[0m  [36/42], [94mLoss[0m : 1.83160
[1mStep[0m  [40/42], [94mLoss[0m : 1.97090

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.569, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86194
[1mStep[0m  [4/42], [94mLoss[0m : 1.83756
[1mStep[0m  [8/42], [94mLoss[0m : 1.85364
[1mStep[0m  [12/42], [94mLoss[0m : 1.74442
[1mStep[0m  [16/42], [94mLoss[0m : 1.81894
[1mStep[0m  [20/42], [94mLoss[0m : 1.78926
[1mStep[0m  [24/42], [94mLoss[0m : 1.71640
[1mStep[0m  [28/42], [94mLoss[0m : 1.71194
[1mStep[0m  [32/42], [94mLoss[0m : 1.89992
[1mStep[0m  [36/42], [94mLoss[0m : 1.88295
[1mStep[0m  [40/42], [94mLoss[0m : 1.77921

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84743
[1mStep[0m  [4/42], [94mLoss[0m : 1.59619
[1mStep[0m  [8/42], [94mLoss[0m : 1.79635
[1mStep[0m  [12/42], [94mLoss[0m : 1.92836
[1mStep[0m  [16/42], [94mLoss[0m : 1.90828
[1mStep[0m  [20/42], [94mLoss[0m : 1.73134
[1mStep[0m  [24/42], [94mLoss[0m : 1.82381
[1mStep[0m  [28/42], [94mLoss[0m : 1.75602
[1mStep[0m  [32/42], [94mLoss[0m : 1.56425
[1mStep[0m  [36/42], [94mLoss[0m : 1.86564
[1mStep[0m  [40/42], [94mLoss[0m : 1.83599

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69898
[1mStep[0m  [4/42], [94mLoss[0m : 1.77450
[1mStep[0m  [8/42], [94mLoss[0m : 2.11642
[1mStep[0m  [12/42], [94mLoss[0m : 1.77406
[1mStep[0m  [16/42], [94mLoss[0m : 1.92449
[1mStep[0m  [20/42], [94mLoss[0m : 1.58823
[1mStep[0m  [24/42], [94mLoss[0m : 1.72208
[1mStep[0m  [28/42], [94mLoss[0m : 1.79574
[1mStep[0m  [32/42], [94mLoss[0m : 1.75570
[1mStep[0m  [36/42], [94mLoss[0m : 1.81855
[1mStep[0m  [40/42], [94mLoss[0m : 1.75854

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82888
[1mStep[0m  [4/42], [94mLoss[0m : 1.85539
[1mStep[0m  [8/42], [94mLoss[0m : 1.87036
[1mStep[0m  [12/42], [94mLoss[0m : 1.68924
[1mStep[0m  [16/42], [94mLoss[0m : 1.85035
[1mStep[0m  [20/42], [94mLoss[0m : 1.79327
[1mStep[0m  [24/42], [94mLoss[0m : 1.86483
[1mStep[0m  [28/42], [94mLoss[0m : 1.64065
[1mStep[0m  [32/42], [94mLoss[0m : 1.54667
[1mStep[0m  [36/42], [94mLoss[0m : 1.86796
[1mStep[0m  [40/42], [94mLoss[0m : 1.59925

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62137
[1mStep[0m  [4/42], [94mLoss[0m : 1.75050
[1mStep[0m  [8/42], [94mLoss[0m : 1.72232
[1mStep[0m  [12/42], [94mLoss[0m : 1.80722
[1mStep[0m  [16/42], [94mLoss[0m : 1.65396
[1mStep[0m  [20/42], [94mLoss[0m : 1.63670
[1mStep[0m  [24/42], [94mLoss[0m : 1.87317
[1mStep[0m  [28/42], [94mLoss[0m : 1.67604
[1mStep[0m  [32/42], [94mLoss[0m : 1.88380
[1mStep[0m  [36/42], [94mLoss[0m : 1.63578
[1mStep[0m  [40/42], [94mLoss[0m : 1.69703

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76995
[1mStep[0m  [4/42], [94mLoss[0m : 1.63461
[1mStep[0m  [8/42], [94mLoss[0m : 1.77425
[1mStep[0m  [12/42], [94mLoss[0m : 1.71096
[1mStep[0m  [16/42], [94mLoss[0m : 1.50553
[1mStep[0m  [20/42], [94mLoss[0m : 1.68443
[1mStep[0m  [24/42], [94mLoss[0m : 1.68560
[1mStep[0m  [28/42], [94mLoss[0m : 1.79995
[1mStep[0m  [32/42], [94mLoss[0m : 1.92979
[1mStep[0m  [36/42], [94mLoss[0m : 1.86180
[1mStep[0m  [40/42], [94mLoss[0m : 1.65386

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63801
[1mStep[0m  [4/42], [94mLoss[0m : 1.72919
[1mStep[0m  [8/42], [94mLoss[0m : 1.78157
[1mStep[0m  [12/42], [94mLoss[0m : 1.75602
[1mStep[0m  [16/42], [94mLoss[0m : 1.58187
[1mStep[0m  [20/42], [94mLoss[0m : 1.69057
[1mStep[0m  [24/42], [94mLoss[0m : 1.67951
[1mStep[0m  [28/42], [94mLoss[0m : 1.65377
[1mStep[0m  [32/42], [94mLoss[0m : 1.68034
[1mStep[0m  [36/42], [94mLoss[0m : 1.83551
[1mStep[0m  [40/42], [94mLoss[0m : 1.82681

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.590, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56638
[1mStep[0m  [4/42], [94mLoss[0m : 1.93407
[1mStep[0m  [8/42], [94mLoss[0m : 1.67677
[1mStep[0m  [12/42], [94mLoss[0m : 1.71081
[1mStep[0m  [16/42], [94mLoss[0m : 1.86875
[1mStep[0m  [20/42], [94mLoss[0m : 1.54228
[1mStep[0m  [24/42], [94mLoss[0m : 1.55469
[1mStep[0m  [28/42], [94mLoss[0m : 1.70071
[1mStep[0m  [32/42], [94mLoss[0m : 1.66397
[1mStep[0m  [36/42], [94mLoss[0m : 1.50790
[1mStep[0m  [40/42], [94mLoss[0m : 1.64578

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.537, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.531
====================================

Phase 2 - Evaluation MAE:  2.531127231461661
MAE score P1      2.356878
MAE score P2      2.531127
loss              1.674109
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.18769
[1mStep[0m  [8/84], [94mLoss[0m : 10.87567
[1mStep[0m  [16/84], [94mLoss[0m : 10.79001
[1mStep[0m  [24/84], [94mLoss[0m : 9.99440
[1mStep[0m  [32/84], [94mLoss[0m : 9.69190
[1mStep[0m  [40/84], [94mLoss[0m : 8.46044
[1mStep[0m  [48/84], [94mLoss[0m : 7.97849
[1mStep[0m  [56/84], [94mLoss[0m : 7.19403
[1mStep[0m  [64/84], [94mLoss[0m : 6.27386
[1mStep[0m  [72/84], [94mLoss[0m : 5.69573
[1mStep[0m  [80/84], [94mLoss[0m : 4.66557

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.329, [92mTest[0m: 10.819, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.62287
[1mStep[0m  [8/84], [94mLoss[0m : 3.14208
[1mStep[0m  [16/84], [94mLoss[0m : 3.15748
[1mStep[0m  [24/84], [94mLoss[0m : 2.58816
[1mStep[0m  [32/84], [94mLoss[0m : 2.50577
[1mStep[0m  [40/84], [94mLoss[0m : 2.80930
[1mStep[0m  [48/84], [94mLoss[0m : 2.55640
[1mStep[0m  [56/84], [94mLoss[0m : 2.51821
[1mStep[0m  [64/84], [94mLoss[0m : 2.35985
[1mStep[0m  [72/84], [94mLoss[0m : 2.62404
[1mStep[0m  [80/84], [94mLoss[0m : 2.63031

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.816, [92mTest[0m: 3.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53800
[1mStep[0m  [8/84], [94mLoss[0m : 2.32116
[1mStep[0m  [16/84], [94mLoss[0m : 2.33580
[1mStep[0m  [24/84], [94mLoss[0m : 2.13166
[1mStep[0m  [32/84], [94mLoss[0m : 2.53872
[1mStep[0m  [40/84], [94mLoss[0m : 2.44572
[1mStep[0m  [48/84], [94mLoss[0m : 2.52262
[1mStep[0m  [56/84], [94mLoss[0m : 2.87089
[1mStep[0m  [64/84], [94mLoss[0m : 2.86570
[1mStep[0m  [72/84], [94mLoss[0m : 2.85536
[1mStep[0m  [80/84], [94mLoss[0m : 2.69497

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87373
[1mStep[0m  [8/84], [94mLoss[0m : 2.36502
[1mStep[0m  [16/84], [94mLoss[0m : 2.57894
[1mStep[0m  [24/84], [94mLoss[0m : 2.48217
[1mStep[0m  [32/84], [94mLoss[0m : 2.38489
[1mStep[0m  [40/84], [94mLoss[0m : 2.49805
[1mStep[0m  [48/84], [94mLoss[0m : 2.40275
[1mStep[0m  [56/84], [94mLoss[0m : 2.68733
[1mStep[0m  [64/84], [94mLoss[0m : 2.47798
[1mStep[0m  [72/84], [94mLoss[0m : 2.48103
[1mStep[0m  [80/84], [94mLoss[0m : 2.88307

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55880
[1mStep[0m  [8/84], [94mLoss[0m : 2.73724
[1mStep[0m  [16/84], [94mLoss[0m : 2.80742
[1mStep[0m  [24/84], [94mLoss[0m : 2.79747
[1mStep[0m  [32/84], [94mLoss[0m : 2.33939
[1mStep[0m  [40/84], [94mLoss[0m : 2.47329
[1mStep[0m  [48/84], [94mLoss[0m : 2.42614
[1mStep[0m  [56/84], [94mLoss[0m : 2.32658
[1mStep[0m  [64/84], [94mLoss[0m : 2.82929
[1mStep[0m  [72/84], [94mLoss[0m : 2.45858
[1mStep[0m  [80/84], [94mLoss[0m : 2.76261

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10676
[1mStep[0m  [8/84], [94mLoss[0m : 2.60945
[1mStep[0m  [16/84], [94mLoss[0m : 2.24520
[1mStep[0m  [24/84], [94mLoss[0m : 2.33281
[1mStep[0m  [32/84], [94mLoss[0m : 2.36061
[1mStep[0m  [40/84], [94mLoss[0m : 2.42002
[1mStep[0m  [48/84], [94mLoss[0m : 2.52685
[1mStep[0m  [56/84], [94mLoss[0m : 2.50061
[1mStep[0m  [64/84], [94mLoss[0m : 2.59961
[1mStep[0m  [72/84], [94mLoss[0m : 2.52477
[1mStep[0m  [80/84], [94mLoss[0m : 2.50818

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33613
[1mStep[0m  [8/84], [94mLoss[0m : 2.26326
[1mStep[0m  [16/84], [94mLoss[0m : 2.13160
[1mStep[0m  [24/84], [94mLoss[0m : 2.45175
[1mStep[0m  [32/84], [94mLoss[0m : 2.42203
[1mStep[0m  [40/84], [94mLoss[0m : 2.50782
[1mStep[0m  [48/84], [94mLoss[0m : 2.51917
[1mStep[0m  [56/84], [94mLoss[0m : 2.82728
[1mStep[0m  [64/84], [94mLoss[0m : 2.85554
[1mStep[0m  [72/84], [94mLoss[0m : 2.50221
[1mStep[0m  [80/84], [94mLoss[0m : 2.50700

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13802
[1mStep[0m  [8/84], [94mLoss[0m : 2.20778
[1mStep[0m  [16/84], [94mLoss[0m : 2.48856
[1mStep[0m  [24/84], [94mLoss[0m : 2.51710
[1mStep[0m  [32/84], [94mLoss[0m : 2.54486
[1mStep[0m  [40/84], [94mLoss[0m : 2.20607
[1mStep[0m  [48/84], [94mLoss[0m : 2.74501
[1mStep[0m  [56/84], [94mLoss[0m : 2.43416
[1mStep[0m  [64/84], [94mLoss[0m : 2.27750
[1mStep[0m  [72/84], [94mLoss[0m : 2.49607
[1mStep[0m  [80/84], [94mLoss[0m : 2.36142

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35050
[1mStep[0m  [8/84], [94mLoss[0m : 2.70374
[1mStep[0m  [16/84], [94mLoss[0m : 2.16908
[1mStep[0m  [24/84], [94mLoss[0m : 2.39295
[1mStep[0m  [32/84], [94mLoss[0m : 2.58552
[1mStep[0m  [40/84], [94mLoss[0m : 2.26112
[1mStep[0m  [48/84], [94mLoss[0m : 2.50828
[1mStep[0m  [56/84], [94mLoss[0m : 2.45510
[1mStep[0m  [64/84], [94mLoss[0m : 2.40915
[1mStep[0m  [72/84], [94mLoss[0m : 2.98807
[1mStep[0m  [80/84], [94mLoss[0m : 2.24169

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66311
[1mStep[0m  [8/84], [94mLoss[0m : 2.22148
[1mStep[0m  [16/84], [94mLoss[0m : 2.53736
[1mStep[0m  [24/84], [94mLoss[0m : 2.57346
[1mStep[0m  [32/84], [94mLoss[0m : 2.17216
[1mStep[0m  [40/84], [94mLoss[0m : 2.45193
[1mStep[0m  [48/84], [94mLoss[0m : 2.31000
[1mStep[0m  [56/84], [94mLoss[0m : 2.32487
[1mStep[0m  [64/84], [94mLoss[0m : 2.62838
[1mStep[0m  [72/84], [94mLoss[0m : 2.42805
[1mStep[0m  [80/84], [94mLoss[0m : 2.39624

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38817
[1mStep[0m  [8/84], [94mLoss[0m : 2.38788
[1mStep[0m  [16/84], [94mLoss[0m : 2.07190
[1mStep[0m  [24/84], [94mLoss[0m : 2.68104
[1mStep[0m  [32/84], [94mLoss[0m : 2.04197
[1mStep[0m  [40/84], [94mLoss[0m : 2.73728
[1mStep[0m  [48/84], [94mLoss[0m : 2.25356
[1mStep[0m  [56/84], [94mLoss[0m : 2.49430
[1mStep[0m  [64/84], [94mLoss[0m : 2.10025
[1mStep[0m  [72/84], [94mLoss[0m : 2.27726
[1mStep[0m  [80/84], [94mLoss[0m : 2.29047

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36995
[1mStep[0m  [8/84], [94mLoss[0m : 2.34844
[1mStep[0m  [16/84], [94mLoss[0m : 2.42805
[1mStep[0m  [24/84], [94mLoss[0m : 2.56919
[1mStep[0m  [32/84], [94mLoss[0m : 2.29322
[1mStep[0m  [40/84], [94mLoss[0m : 2.68893
[1mStep[0m  [48/84], [94mLoss[0m : 2.59647
[1mStep[0m  [56/84], [94mLoss[0m : 2.51078
[1mStep[0m  [64/84], [94mLoss[0m : 2.76704
[1mStep[0m  [72/84], [94mLoss[0m : 2.46041
[1mStep[0m  [80/84], [94mLoss[0m : 2.27748

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50335
[1mStep[0m  [8/84], [94mLoss[0m : 2.39770
[1mStep[0m  [16/84], [94mLoss[0m : 2.23044
[1mStep[0m  [24/84], [94mLoss[0m : 2.48473
[1mStep[0m  [32/84], [94mLoss[0m : 2.39830
[1mStep[0m  [40/84], [94mLoss[0m : 2.38189
[1mStep[0m  [48/84], [94mLoss[0m : 2.13358
[1mStep[0m  [56/84], [94mLoss[0m : 2.39350
[1mStep[0m  [64/84], [94mLoss[0m : 2.33505
[1mStep[0m  [72/84], [94mLoss[0m : 2.52336
[1mStep[0m  [80/84], [94mLoss[0m : 2.40578

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13629
[1mStep[0m  [8/84], [94mLoss[0m : 2.47579
[1mStep[0m  [16/84], [94mLoss[0m : 2.26234
[1mStep[0m  [24/84], [94mLoss[0m : 2.61001
[1mStep[0m  [32/84], [94mLoss[0m : 2.38063
[1mStep[0m  [40/84], [94mLoss[0m : 2.46569
[1mStep[0m  [48/84], [94mLoss[0m : 2.51245
[1mStep[0m  [56/84], [94mLoss[0m : 2.28454
[1mStep[0m  [64/84], [94mLoss[0m : 2.30846
[1mStep[0m  [72/84], [94mLoss[0m : 2.62325
[1mStep[0m  [80/84], [94mLoss[0m : 2.21411

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36186
[1mStep[0m  [8/84], [94mLoss[0m : 2.71477
[1mStep[0m  [16/84], [94mLoss[0m : 2.31011
[1mStep[0m  [24/84], [94mLoss[0m : 2.70765
[1mStep[0m  [32/84], [94mLoss[0m : 2.53201
[1mStep[0m  [40/84], [94mLoss[0m : 2.40703
[1mStep[0m  [48/84], [94mLoss[0m : 2.60600
[1mStep[0m  [56/84], [94mLoss[0m : 2.61030
[1mStep[0m  [64/84], [94mLoss[0m : 3.02447
[1mStep[0m  [72/84], [94mLoss[0m : 2.30710
[1mStep[0m  [80/84], [94mLoss[0m : 2.44822

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13735
[1mStep[0m  [8/84], [94mLoss[0m : 2.51181
[1mStep[0m  [16/84], [94mLoss[0m : 2.38420
[1mStep[0m  [24/84], [94mLoss[0m : 2.61573
[1mStep[0m  [32/84], [94mLoss[0m : 2.28977
[1mStep[0m  [40/84], [94mLoss[0m : 2.63099
[1mStep[0m  [48/84], [94mLoss[0m : 2.58255
[1mStep[0m  [56/84], [94mLoss[0m : 2.55620
[1mStep[0m  [64/84], [94mLoss[0m : 2.41470
[1mStep[0m  [72/84], [94mLoss[0m : 2.23809
[1mStep[0m  [80/84], [94mLoss[0m : 2.44563

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.312, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54284
[1mStep[0m  [8/84], [94mLoss[0m : 2.49996
[1mStep[0m  [16/84], [94mLoss[0m : 2.42302
[1mStep[0m  [24/84], [94mLoss[0m : 2.55955
[1mStep[0m  [32/84], [94mLoss[0m : 2.59015
[1mStep[0m  [40/84], [94mLoss[0m : 2.06284
[1mStep[0m  [48/84], [94mLoss[0m : 2.31180
[1mStep[0m  [56/84], [94mLoss[0m : 2.40839
[1mStep[0m  [64/84], [94mLoss[0m : 1.98745
[1mStep[0m  [72/84], [94mLoss[0m : 2.56790
[1mStep[0m  [80/84], [94mLoss[0m : 2.18334

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42076
[1mStep[0m  [8/84], [94mLoss[0m : 2.26412
[1mStep[0m  [16/84], [94mLoss[0m : 2.16425
[1mStep[0m  [24/84], [94mLoss[0m : 2.34953
[1mStep[0m  [32/84], [94mLoss[0m : 2.35024
[1mStep[0m  [40/84], [94mLoss[0m : 2.33716
[1mStep[0m  [48/84], [94mLoss[0m : 2.24062
[1mStep[0m  [56/84], [94mLoss[0m : 2.57532
[1mStep[0m  [64/84], [94mLoss[0m : 2.37166
[1mStep[0m  [72/84], [94mLoss[0m : 2.39701
[1mStep[0m  [80/84], [94mLoss[0m : 2.40663

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.315, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37571
[1mStep[0m  [8/84], [94mLoss[0m : 2.31166
[1mStep[0m  [16/84], [94mLoss[0m : 2.48159
[1mStep[0m  [24/84], [94mLoss[0m : 2.40093
[1mStep[0m  [32/84], [94mLoss[0m : 2.17624
[1mStep[0m  [40/84], [94mLoss[0m : 2.37980
[1mStep[0m  [48/84], [94mLoss[0m : 2.31829
[1mStep[0m  [56/84], [94mLoss[0m : 2.16663
[1mStep[0m  [64/84], [94mLoss[0m : 2.41354
[1mStep[0m  [72/84], [94mLoss[0m : 2.27780
[1mStep[0m  [80/84], [94mLoss[0m : 2.46128

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54531
[1mStep[0m  [8/84], [94mLoss[0m : 2.38811
[1mStep[0m  [16/84], [94mLoss[0m : 2.42923
[1mStep[0m  [24/84], [94mLoss[0m : 2.12290
[1mStep[0m  [32/84], [94mLoss[0m : 2.38670
[1mStep[0m  [40/84], [94mLoss[0m : 2.25012
[1mStep[0m  [48/84], [94mLoss[0m : 2.54537
[1mStep[0m  [56/84], [94mLoss[0m : 2.49438
[1mStep[0m  [64/84], [94mLoss[0m : 2.07398
[1mStep[0m  [72/84], [94mLoss[0m : 2.32840
[1mStep[0m  [80/84], [94mLoss[0m : 2.29839

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67630
[1mStep[0m  [8/84], [94mLoss[0m : 2.44900
[1mStep[0m  [16/84], [94mLoss[0m : 2.31513
[1mStep[0m  [24/84], [94mLoss[0m : 2.28220
[1mStep[0m  [32/84], [94mLoss[0m : 2.42842
[1mStep[0m  [40/84], [94mLoss[0m : 2.37188
[1mStep[0m  [48/84], [94mLoss[0m : 2.59658
[1mStep[0m  [56/84], [94mLoss[0m : 2.46323
[1mStep[0m  [64/84], [94mLoss[0m : 2.60498
[1mStep[0m  [72/84], [94mLoss[0m : 2.74185
[1mStep[0m  [80/84], [94mLoss[0m : 2.25274

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25313
[1mStep[0m  [8/84], [94mLoss[0m : 2.45470
[1mStep[0m  [16/84], [94mLoss[0m : 2.17335
[1mStep[0m  [24/84], [94mLoss[0m : 2.49217
[1mStep[0m  [32/84], [94mLoss[0m : 2.54838
[1mStep[0m  [40/84], [94mLoss[0m : 2.38641
[1mStep[0m  [48/84], [94mLoss[0m : 2.31378
[1mStep[0m  [56/84], [94mLoss[0m : 2.24829
[1mStep[0m  [64/84], [94mLoss[0m : 2.40589
[1mStep[0m  [72/84], [94mLoss[0m : 2.28243
[1mStep[0m  [80/84], [94mLoss[0m : 2.86924

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55119
[1mStep[0m  [8/84], [94mLoss[0m : 2.24692
[1mStep[0m  [16/84], [94mLoss[0m : 2.46025
[1mStep[0m  [24/84], [94mLoss[0m : 2.45356
[1mStep[0m  [32/84], [94mLoss[0m : 2.41264
[1mStep[0m  [40/84], [94mLoss[0m : 2.20662
[1mStep[0m  [48/84], [94mLoss[0m : 2.47052
[1mStep[0m  [56/84], [94mLoss[0m : 2.12035
[1mStep[0m  [64/84], [94mLoss[0m : 2.66816
[1mStep[0m  [72/84], [94mLoss[0m : 2.22089
[1mStep[0m  [80/84], [94mLoss[0m : 2.57563

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.313, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37846
[1mStep[0m  [8/84], [94mLoss[0m : 2.55802
[1mStep[0m  [16/84], [94mLoss[0m : 2.19407
[1mStep[0m  [24/84], [94mLoss[0m : 2.35767
[1mStep[0m  [32/84], [94mLoss[0m : 1.91079
[1mStep[0m  [40/84], [94mLoss[0m : 2.25388
[1mStep[0m  [48/84], [94mLoss[0m : 2.21983
[1mStep[0m  [56/84], [94mLoss[0m : 2.01956
[1mStep[0m  [64/84], [94mLoss[0m : 2.35099
[1mStep[0m  [72/84], [94mLoss[0m : 2.55052
[1mStep[0m  [80/84], [94mLoss[0m : 2.51686

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53812
[1mStep[0m  [8/84], [94mLoss[0m : 2.36451
[1mStep[0m  [16/84], [94mLoss[0m : 2.05136
[1mStep[0m  [24/84], [94mLoss[0m : 2.45459
[1mStep[0m  [32/84], [94mLoss[0m : 2.25516
[1mStep[0m  [40/84], [94mLoss[0m : 2.24887
[1mStep[0m  [48/84], [94mLoss[0m : 2.53364
[1mStep[0m  [56/84], [94mLoss[0m : 2.23816
[1mStep[0m  [64/84], [94mLoss[0m : 2.30824
[1mStep[0m  [72/84], [94mLoss[0m : 2.23018
[1mStep[0m  [80/84], [94mLoss[0m : 2.57176

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46227
[1mStep[0m  [8/84], [94mLoss[0m : 2.67669
[1mStep[0m  [16/84], [94mLoss[0m : 2.61379
[1mStep[0m  [24/84], [94mLoss[0m : 2.09508
[1mStep[0m  [32/84], [94mLoss[0m : 2.43635
[1mStep[0m  [40/84], [94mLoss[0m : 2.58760
[1mStep[0m  [48/84], [94mLoss[0m : 2.31044
[1mStep[0m  [56/84], [94mLoss[0m : 2.50511
[1mStep[0m  [64/84], [94mLoss[0m : 2.44998
[1mStep[0m  [72/84], [94mLoss[0m : 2.23161
[1mStep[0m  [80/84], [94mLoss[0m : 2.39014

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60294
[1mStep[0m  [8/84], [94mLoss[0m : 2.28353
[1mStep[0m  [16/84], [94mLoss[0m : 2.46772
[1mStep[0m  [24/84], [94mLoss[0m : 2.40592
[1mStep[0m  [32/84], [94mLoss[0m : 2.58004
[1mStep[0m  [40/84], [94mLoss[0m : 2.43970
[1mStep[0m  [48/84], [94mLoss[0m : 2.25934
[1mStep[0m  [56/84], [94mLoss[0m : 2.19250
[1mStep[0m  [64/84], [94mLoss[0m : 2.71487
[1mStep[0m  [72/84], [94mLoss[0m : 2.76605
[1mStep[0m  [80/84], [94mLoss[0m : 2.40384

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29582
[1mStep[0m  [8/84], [94mLoss[0m : 2.37911
[1mStep[0m  [16/84], [94mLoss[0m : 2.43512
[1mStep[0m  [24/84], [94mLoss[0m : 2.55562
[1mStep[0m  [32/84], [94mLoss[0m : 2.39934
[1mStep[0m  [40/84], [94mLoss[0m : 2.54338
[1mStep[0m  [48/84], [94mLoss[0m : 2.66972
[1mStep[0m  [56/84], [94mLoss[0m : 2.12162
[1mStep[0m  [64/84], [94mLoss[0m : 2.18672
[1mStep[0m  [72/84], [94mLoss[0m : 2.61074
[1mStep[0m  [80/84], [94mLoss[0m : 2.33260

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24680
[1mStep[0m  [8/84], [94mLoss[0m : 2.36063
[1mStep[0m  [16/84], [94mLoss[0m : 2.43056
[1mStep[0m  [24/84], [94mLoss[0m : 2.46986
[1mStep[0m  [32/84], [94mLoss[0m : 2.40595
[1mStep[0m  [40/84], [94mLoss[0m : 2.58663
[1mStep[0m  [48/84], [94mLoss[0m : 2.34488
[1mStep[0m  [56/84], [94mLoss[0m : 2.62081
[1mStep[0m  [64/84], [94mLoss[0m : 2.30057
[1mStep[0m  [72/84], [94mLoss[0m : 2.35194
[1mStep[0m  [80/84], [94mLoss[0m : 2.34201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21684
[1mStep[0m  [8/84], [94mLoss[0m : 2.27520
[1mStep[0m  [16/84], [94mLoss[0m : 2.37500
[1mStep[0m  [24/84], [94mLoss[0m : 2.67689
[1mStep[0m  [32/84], [94mLoss[0m : 2.50235
[1mStep[0m  [40/84], [94mLoss[0m : 2.76454
[1mStep[0m  [48/84], [94mLoss[0m : 2.28282
[1mStep[0m  [56/84], [94mLoss[0m : 2.13742
[1mStep[0m  [64/84], [94mLoss[0m : 2.25526
[1mStep[0m  [72/84], [94mLoss[0m : 2.42251
[1mStep[0m  [80/84], [94mLoss[0m : 2.37351

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.3386520573071072
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.40731
[1mStep[0m  [8/84], [94mLoss[0m : 2.22395
[1mStep[0m  [16/84], [94mLoss[0m : 2.37598
[1mStep[0m  [24/84], [94mLoss[0m : 2.07831
[1mStep[0m  [32/84], [94mLoss[0m : 2.10987
[1mStep[0m  [40/84], [94mLoss[0m : 2.52658
[1mStep[0m  [48/84], [94mLoss[0m : 2.73488
[1mStep[0m  [56/84], [94mLoss[0m : 2.59543
[1mStep[0m  [64/84], [94mLoss[0m : 2.37432
[1mStep[0m  [72/84], [94mLoss[0m : 2.46980
[1mStep[0m  [80/84], [94mLoss[0m : 2.02285

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34203
[1mStep[0m  [8/84], [94mLoss[0m : 2.63333
[1mStep[0m  [16/84], [94mLoss[0m : 2.31174
[1mStep[0m  [24/84], [94mLoss[0m : 2.01310
[1mStep[0m  [32/84], [94mLoss[0m : 2.35180
[1mStep[0m  [40/84], [94mLoss[0m : 2.49375
[1mStep[0m  [48/84], [94mLoss[0m : 2.42245
[1mStep[0m  [56/84], [94mLoss[0m : 2.30267
[1mStep[0m  [64/84], [94mLoss[0m : 2.18184
[1mStep[0m  [72/84], [94mLoss[0m : 2.80097
[1mStep[0m  [80/84], [94mLoss[0m : 2.29228

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46779
[1mStep[0m  [8/84], [94mLoss[0m : 2.08557
[1mStep[0m  [16/84], [94mLoss[0m : 2.30390
[1mStep[0m  [24/84], [94mLoss[0m : 2.17601
[1mStep[0m  [32/84], [94mLoss[0m : 2.53276
[1mStep[0m  [40/84], [94mLoss[0m : 2.20313
[1mStep[0m  [48/84], [94mLoss[0m : 2.07397
[1mStep[0m  [56/84], [94mLoss[0m : 2.32647
[1mStep[0m  [64/84], [94mLoss[0m : 2.37307
[1mStep[0m  [72/84], [94mLoss[0m : 2.17990
[1mStep[0m  [80/84], [94mLoss[0m : 2.48107

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91002
[1mStep[0m  [8/84], [94mLoss[0m : 2.23867
[1mStep[0m  [16/84], [94mLoss[0m : 2.32090
[1mStep[0m  [24/84], [94mLoss[0m : 2.09567
[1mStep[0m  [32/84], [94mLoss[0m : 2.19739
[1mStep[0m  [40/84], [94mLoss[0m : 1.89715
[1mStep[0m  [48/84], [94mLoss[0m : 2.12597
[1mStep[0m  [56/84], [94mLoss[0m : 2.23246
[1mStep[0m  [64/84], [94mLoss[0m : 2.00247
[1mStep[0m  [72/84], [94mLoss[0m : 2.41159
[1mStep[0m  [80/84], [94mLoss[0m : 2.23232

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19681
[1mStep[0m  [8/84], [94mLoss[0m : 2.01599
[1mStep[0m  [16/84], [94mLoss[0m : 2.11720
[1mStep[0m  [24/84], [94mLoss[0m : 2.08365
[1mStep[0m  [32/84], [94mLoss[0m : 1.87873
[1mStep[0m  [40/84], [94mLoss[0m : 2.13799
[1mStep[0m  [48/84], [94mLoss[0m : 1.86232
[1mStep[0m  [56/84], [94mLoss[0m : 2.09277
[1mStep[0m  [64/84], [94mLoss[0m : 2.02356
[1mStep[0m  [72/84], [94mLoss[0m : 2.08933
[1mStep[0m  [80/84], [94mLoss[0m : 2.02613

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69187
[1mStep[0m  [8/84], [94mLoss[0m : 1.92292
[1mStep[0m  [16/84], [94mLoss[0m : 1.88631
[1mStep[0m  [24/84], [94mLoss[0m : 2.06239
[1mStep[0m  [32/84], [94mLoss[0m : 1.95674
[1mStep[0m  [40/84], [94mLoss[0m : 2.14699
[1mStep[0m  [48/84], [94mLoss[0m : 2.19211
[1mStep[0m  [56/84], [94mLoss[0m : 2.02660
[1mStep[0m  [64/84], [94mLoss[0m : 2.44070
[1mStep[0m  [72/84], [94mLoss[0m : 2.14273
[1mStep[0m  [80/84], [94mLoss[0m : 2.16334

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18554
[1mStep[0m  [8/84], [94mLoss[0m : 1.83109
[1mStep[0m  [16/84], [94mLoss[0m : 1.97438
[1mStep[0m  [24/84], [94mLoss[0m : 1.69048
[1mStep[0m  [32/84], [94mLoss[0m : 1.83730
[1mStep[0m  [40/84], [94mLoss[0m : 1.80020
[1mStep[0m  [48/84], [94mLoss[0m : 1.92766
[1mStep[0m  [56/84], [94mLoss[0m : 2.22269
[1mStep[0m  [64/84], [94mLoss[0m : 1.62440
[1mStep[0m  [72/84], [94mLoss[0m : 1.83732
[1mStep[0m  [80/84], [94mLoss[0m : 2.11237

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92686
[1mStep[0m  [8/84], [94mLoss[0m : 2.07584
[1mStep[0m  [16/84], [94mLoss[0m : 1.95005
[1mStep[0m  [24/84], [94mLoss[0m : 1.90494
[1mStep[0m  [32/84], [94mLoss[0m : 1.97511
[1mStep[0m  [40/84], [94mLoss[0m : 2.19772
[1mStep[0m  [48/84], [94mLoss[0m : 1.99612
[1mStep[0m  [56/84], [94mLoss[0m : 1.83678
[1mStep[0m  [64/84], [94mLoss[0m : 1.96240
[1mStep[0m  [72/84], [94mLoss[0m : 1.97089
[1mStep[0m  [80/84], [94mLoss[0m : 2.20910

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79865
[1mStep[0m  [8/84], [94mLoss[0m : 1.74623
[1mStep[0m  [16/84], [94mLoss[0m : 1.79880
[1mStep[0m  [24/84], [94mLoss[0m : 1.98434
[1mStep[0m  [32/84], [94mLoss[0m : 1.74193
[1mStep[0m  [40/84], [94mLoss[0m : 1.84968
[1mStep[0m  [48/84], [94mLoss[0m : 1.85439
[1mStep[0m  [56/84], [94mLoss[0m : 1.89465
[1mStep[0m  [64/84], [94mLoss[0m : 2.11231
[1mStep[0m  [72/84], [94mLoss[0m : 1.84121
[1mStep[0m  [80/84], [94mLoss[0m : 1.93837

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81153
[1mStep[0m  [8/84], [94mLoss[0m : 1.65875
[1mStep[0m  [16/84], [94mLoss[0m : 1.72860
[1mStep[0m  [24/84], [94mLoss[0m : 1.80860
[1mStep[0m  [32/84], [94mLoss[0m : 1.64037
[1mStep[0m  [40/84], [94mLoss[0m : 1.94602
[1mStep[0m  [48/84], [94mLoss[0m : 2.08698
[1mStep[0m  [56/84], [94mLoss[0m : 1.83625
[1mStep[0m  [64/84], [94mLoss[0m : 1.86774
[1mStep[0m  [72/84], [94mLoss[0m : 2.11583
[1mStep[0m  [80/84], [94mLoss[0m : 1.83113

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95960
[1mStep[0m  [8/84], [94mLoss[0m : 1.84242
[1mStep[0m  [16/84], [94mLoss[0m : 1.44278
[1mStep[0m  [24/84], [94mLoss[0m : 1.77564
[1mStep[0m  [32/84], [94mLoss[0m : 1.89771
[1mStep[0m  [40/84], [94mLoss[0m : 2.11002
[1mStep[0m  [48/84], [94mLoss[0m : 1.97207
[1mStep[0m  [56/84], [94mLoss[0m : 2.00493
[1mStep[0m  [64/84], [94mLoss[0m : 2.05150
[1mStep[0m  [72/84], [94mLoss[0m : 1.76139
[1mStep[0m  [80/84], [94mLoss[0m : 2.04802

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48572
[1mStep[0m  [8/84], [94mLoss[0m : 1.77914
[1mStep[0m  [16/84], [94mLoss[0m : 1.72976
[1mStep[0m  [24/84], [94mLoss[0m : 1.86607
[1mStep[0m  [32/84], [94mLoss[0m : 1.86662
[1mStep[0m  [40/84], [94mLoss[0m : 1.85755
[1mStep[0m  [48/84], [94mLoss[0m : 1.78586
[1mStep[0m  [56/84], [94mLoss[0m : 1.95162
[1mStep[0m  [64/84], [94mLoss[0m : 1.73762
[1mStep[0m  [72/84], [94mLoss[0m : 1.83677
[1mStep[0m  [80/84], [94mLoss[0m : 1.84030

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79061
[1mStep[0m  [8/84], [94mLoss[0m : 1.68921
[1mStep[0m  [16/84], [94mLoss[0m : 1.65073
[1mStep[0m  [24/84], [94mLoss[0m : 1.67314
[1mStep[0m  [32/84], [94mLoss[0m : 1.71578
[1mStep[0m  [40/84], [94mLoss[0m : 1.80883
[1mStep[0m  [48/84], [94mLoss[0m : 1.68740
[1mStep[0m  [56/84], [94mLoss[0m : 2.03902
[1mStep[0m  [64/84], [94mLoss[0m : 1.85128
[1mStep[0m  [72/84], [94mLoss[0m : 1.85183
[1mStep[0m  [80/84], [94mLoss[0m : 2.12638

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74750
[1mStep[0m  [8/84], [94mLoss[0m : 1.78365
[1mStep[0m  [16/84], [94mLoss[0m : 1.84665
[1mStep[0m  [24/84], [94mLoss[0m : 2.08188
[1mStep[0m  [32/84], [94mLoss[0m : 1.75650
[1mStep[0m  [40/84], [94mLoss[0m : 1.80978
[1mStep[0m  [48/84], [94mLoss[0m : 1.82193
[1mStep[0m  [56/84], [94mLoss[0m : 1.92870
[1mStep[0m  [64/84], [94mLoss[0m : 1.64590
[1mStep[0m  [72/84], [94mLoss[0m : 1.72421
[1mStep[0m  [80/84], [94mLoss[0m : 1.95343

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64965
[1mStep[0m  [8/84], [94mLoss[0m : 1.74507
[1mStep[0m  [16/84], [94mLoss[0m : 1.69408
[1mStep[0m  [24/84], [94mLoss[0m : 1.72542
[1mStep[0m  [32/84], [94mLoss[0m : 1.73055
[1mStep[0m  [40/84], [94mLoss[0m : 1.73154
[1mStep[0m  [48/84], [94mLoss[0m : 1.73185
[1mStep[0m  [56/84], [94mLoss[0m : 2.03710
[1mStep[0m  [64/84], [94mLoss[0m : 1.51895
[1mStep[0m  [72/84], [94mLoss[0m : 1.89254
[1mStep[0m  [80/84], [94mLoss[0m : 1.63522

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.503, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96334
[1mStep[0m  [8/84], [94mLoss[0m : 1.65985
[1mStep[0m  [16/84], [94mLoss[0m : 1.86497
[1mStep[0m  [24/84], [94mLoss[0m : 1.64878
[1mStep[0m  [32/84], [94mLoss[0m : 1.69991
[1mStep[0m  [40/84], [94mLoss[0m : 1.56926
[1mStep[0m  [48/84], [94mLoss[0m : 2.09603
[1mStep[0m  [56/84], [94mLoss[0m : 1.65194
[1mStep[0m  [64/84], [94mLoss[0m : 1.82292
[1mStep[0m  [72/84], [94mLoss[0m : 1.75356
[1mStep[0m  [80/84], [94mLoss[0m : 1.56065

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68824
[1mStep[0m  [8/84], [94mLoss[0m : 1.36949
[1mStep[0m  [16/84], [94mLoss[0m : 1.73308
[1mStep[0m  [24/84], [94mLoss[0m : 1.84880
[1mStep[0m  [32/84], [94mLoss[0m : 1.79381
[1mStep[0m  [40/84], [94mLoss[0m : 1.81866
[1mStep[0m  [48/84], [94mLoss[0m : 1.45091
[1mStep[0m  [56/84], [94mLoss[0m : 1.54898
[1mStep[0m  [64/84], [94mLoss[0m : 1.60283
[1mStep[0m  [72/84], [94mLoss[0m : 1.71250
[1mStep[0m  [80/84], [94mLoss[0m : 1.99918

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52632
[1mStep[0m  [8/84], [94mLoss[0m : 1.75612
[1mStep[0m  [16/84], [94mLoss[0m : 1.56959
[1mStep[0m  [24/84], [94mLoss[0m : 1.50887
[1mStep[0m  [32/84], [94mLoss[0m : 1.74779
[1mStep[0m  [40/84], [94mLoss[0m : 1.80098
[1mStep[0m  [48/84], [94mLoss[0m : 1.82134
[1mStep[0m  [56/84], [94mLoss[0m : 1.67403
[1mStep[0m  [64/84], [94mLoss[0m : 1.74314
[1mStep[0m  [72/84], [94mLoss[0m : 1.71198
[1mStep[0m  [80/84], [94mLoss[0m : 1.53552

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.545, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53453
[1mStep[0m  [8/84], [94mLoss[0m : 1.68519
[1mStep[0m  [16/84], [94mLoss[0m : 1.67859
[1mStep[0m  [24/84], [94mLoss[0m : 1.57201
[1mStep[0m  [32/84], [94mLoss[0m : 1.62750
[1mStep[0m  [40/84], [94mLoss[0m : 1.71637
[1mStep[0m  [48/84], [94mLoss[0m : 1.72788
[1mStep[0m  [56/84], [94mLoss[0m : 1.94511
[1mStep[0m  [64/84], [94mLoss[0m : 1.84393
[1mStep[0m  [72/84], [94mLoss[0m : 1.76031
[1mStep[0m  [80/84], [94mLoss[0m : 1.78443

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41664
[1mStep[0m  [8/84], [94mLoss[0m : 1.41849
[1mStep[0m  [16/84], [94mLoss[0m : 1.45190
[1mStep[0m  [24/84], [94mLoss[0m : 1.72156
[1mStep[0m  [32/84], [94mLoss[0m : 1.45685
[1mStep[0m  [40/84], [94mLoss[0m : 1.80198
[1mStep[0m  [48/84], [94mLoss[0m : 1.68755
[1mStep[0m  [56/84], [94mLoss[0m : 1.61443
[1mStep[0m  [64/84], [94mLoss[0m : 1.92514
[1mStep[0m  [72/84], [94mLoss[0m : 1.85827
[1mStep[0m  [80/84], [94mLoss[0m : 1.86298

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58234
[1mStep[0m  [8/84], [94mLoss[0m : 1.49336
[1mStep[0m  [16/84], [94mLoss[0m : 1.53112
[1mStep[0m  [24/84], [94mLoss[0m : 1.41373
[1mStep[0m  [32/84], [94mLoss[0m : 1.66819
[1mStep[0m  [40/84], [94mLoss[0m : 1.48098
[1mStep[0m  [48/84], [94mLoss[0m : 1.71194
[1mStep[0m  [56/84], [94mLoss[0m : 1.50961
[1mStep[0m  [64/84], [94mLoss[0m : 1.62161
[1mStep[0m  [72/84], [94mLoss[0m : 1.46873
[1mStep[0m  [80/84], [94mLoss[0m : 1.94753

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.615, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52914
[1mStep[0m  [8/84], [94mLoss[0m : 1.48340
[1mStep[0m  [16/84], [94mLoss[0m : 1.30345
[1mStep[0m  [24/84], [94mLoss[0m : 1.52249
[1mStep[0m  [32/84], [94mLoss[0m : 1.56281
[1mStep[0m  [40/84], [94mLoss[0m : 1.53405
[1mStep[0m  [48/84], [94mLoss[0m : 1.47584
[1mStep[0m  [56/84], [94mLoss[0m : 1.70805
[1mStep[0m  [64/84], [94mLoss[0m : 1.41006
[1mStep[0m  [72/84], [94mLoss[0m : 1.84456
[1mStep[0m  [80/84], [94mLoss[0m : 1.75838

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44339
[1mStep[0m  [8/84], [94mLoss[0m : 1.57471
[1mStep[0m  [16/84], [94mLoss[0m : 1.81129
[1mStep[0m  [24/84], [94mLoss[0m : 1.49423
[1mStep[0m  [32/84], [94mLoss[0m : 1.49346
[1mStep[0m  [40/84], [94mLoss[0m : 1.66583
[1mStep[0m  [48/84], [94mLoss[0m : 1.72005
[1mStep[0m  [56/84], [94mLoss[0m : 1.64699
[1mStep[0m  [64/84], [94mLoss[0m : 1.49586
[1mStep[0m  [72/84], [94mLoss[0m : 1.54379
[1mStep[0m  [80/84], [94mLoss[0m : 1.65286

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.533, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69768
[1mStep[0m  [8/84], [94mLoss[0m : 1.56762
[1mStep[0m  [16/84], [94mLoss[0m : 1.47765
[1mStep[0m  [24/84], [94mLoss[0m : 1.68922
[1mStep[0m  [32/84], [94mLoss[0m : 1.38706
[1mStep[0m  [40/84], [94mLoss[0m : 1.66694
[1mStep[0m  [48/84], [94mLoss[0m : 1.63884
[1mStep[0m  [56/84], [94mLoss[0m : 1.67814
[1mStep[0m  [64/84], [94mLoss[0m : 1.69964
[1mStep[0m  [72/84], [94mLoss[0m : 1.53829
[1mStep[0m  [80/84], [94mLoss[0m : 1.64724

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51879
[1mStep[0m  [8/84], [94mLoss[0m : 1.43389
[1mStep[0m  [16/84], [94mLoss[0m : 1.31541
[1mStep[0m  [24/84], [94mLoss[0m : 1.56251
[1mStep[0m  [32/84], [94mLoss[0m : 1.61150
[1mStep[0m  [40/84], [94mLoss[0m : 1.58478
[1mStep[0m  [48/84], [94mLoss[0m : 1.40599
[1mStep[0m  [56/84], [94mLoss[0m : 1.53891
[1mStep[0m  [64/84], [94mLoss[0m : 1.60883
[1mStep[0m  [72/84], [94mLoss[0m : 1.84265
[1mStep[0m  [80/84], [94mLoss[0m : 1.56703

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.539, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.28801
[1mStep[0m  [8/84], [94mLoss[0m : 1.43447
[1mStep[0m  [16/84], [94mLoss[0m : 1.54380
[1mStep[0m  [24/84], [94mLoss[0m : 1.43317
[1mStep[0m  [32/84], [94mLoss[0m : 1.40763
[1mStep[0m  [40/84], [94mLoss[0m : 1.51165
[1mStep[0m  [48/84], [94mLoss[0m : 1.71934
[1mStep[0m  [56/84], [94mLoss[0m : 1.52429
[1mStep[0m  [64/84], [94mLoss[0m : 1.58800
[1mStep[0m  [72/84], [94mLoss[0m : 1.59832
[1mStep[0m  [80/84], [94mLoss[0m : 1.43359

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65144
[1mStep[0m  [8/84], [94mLoss[0m : 1.19143
[1mStep[0m  [16/84], [94mLoss[0m : 1.28819
[1mStep[0m  [24/84], [94mLoss[0m : 1.71322
[1mStep[0m  [32/84], [94mLoss[0m : 1.61776
[1mStep[0m  [40/84], [94mLoss[0m : 1.72864
[1mStep[0m  [48/84], [94mLoss[0m : 1.41819
[1mStep[0m  [56/84], [94mLoss[0m : 1.34751
[1mStep[0m  [64/84], [94mLoss[0m : 1.58793
[1mStep[0m  [72/84], [94mLoss[0m : 1.21346
[1mStep[0m  [80/84], [94mLoss[0m : 1.50604

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59816
[1mStep[0m  [8/84], [94mLoss[0m : 1.40496
[1mStep[0m  [16/84], [94mLoss[0m : 1.38203
[1mStep[0m  [24/84], [94mLoss[0m : 1.60975
[1mStep[0m  [32/84], [94mLoss[0m : 1.37380
[1mStep[0m  [40/84], [94mLoss[0m : 1.73554
[1mStep[0m  [48/84], [94mLoss[0m : 1.42828
[1mStep[0m  [56/84], [94mLoss[0m : 1.65562
[1mStep[0m  [64/84], [94mLoss[0m : 1.52854
[1mStep[0m  [72/84], [94mLoss[0m : 1.56510
[1mStep[0m  [80/84], [94mLoss[0m : 1.61839

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32224
[1mStep[0m  [8/84], [94mLoss[0m : 1.34847
[1mStep[0m  [16/84], [94mLoss[0m : 1.40280
[1mStep[0m  [24/84], [94mLoss[0m : 1.38503
[1mStep[0m  [32/84], [94mLoss[0m : 1.41866
[1mStep[0m  [40/84], [94mLoss[0m : 1.69698
[1mStep[0m  [48/84], [94mLoss[0m : 1.62954
[1mStep[0m  [56/84], [94mLoss[0m : 1.44037
[1mStep[0m  [64/84], [94mLoss[0m : 1.63970
[1mStep[0m  [72/84], [94mLoss[0m : 1.64720
[1mStep[0m  [80/84], [94mLoss[0m : 1.64085

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31074
[1mStep[0m  [8/84], [94mLoss[0m : 1.42120
[1mStep[0m  [16/84], [94mLoss[0m : 1.37962
[1mStep[0m  [24/84], [94mLoss[0m : 1.43976
[1mStep[0m  [32/84], [94mLoss[0m : 1.32873
[1mStep[0m  [40/84], [94mLoss[0m : 1.41534
[1mStep[0m  [48/84], [94mLoss[0m : 1.49630
[1mStep[0m  [56/84], [94mLoss[0m : 1.49168
[1mStep[0m  [64/84], [94mLoss[0m : 1.66458
[1mStep[0m  [72/84], [94mLoss[0m : 1.58470
[1mStep[0m  [80/84], [94mLoss[0m : 1.62169

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.522, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.547
====================================

Phase 2 - Evaluation MAE:  2.5468692013195584
MAE score P1        2.338652
MAE score P2        2.546869
loss                1.476077
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay            0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.07422
[1mStep[0m  [4/42], [94mLoss[0m : 9.81693
[1mStep[0m  [8/42], [94mLoss[0m : 8.87403
[1mStep[0m  [12/42], [94mLoss[0m : 8.30068
[1mStep[0m  [16/42], [94mLoss[0m : 7.00711
[1mStep[0m  [20/42], [94mLoss[0m : 5.75631
[1mStep[0m  [24/42], [94mLoss[0m : 5.11203
[1mStep[0m  [28/42], [94mLoss[0m : 4.84470
[1mStep[0m  [32/42], [94mLoss[0m : 3.63835
[1mStep[0m  [36/42], [94mLoss[0m : 3.75086
[1mStep[0m  [40/42], [94mLoss[0m : 2.82529

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.266, [92mTest[0m: 10.982, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78701
[1mStep[0m  [4/42], [94mLoss[0m : 2.79834
[1mStep[0m  [8/42], [94mLoss[0m : 2.59686
[1mStep[0m  [12/42], [94mLoss[0m : 2.64009
[1mStep[0m  [16/42], [94mLoss[0m : 2.58193
[1mStep[0m  [20/42], [94mLoss[0m : 2.60823
[1mStep[0m  [24/42], [94mLoss[0m : 2.68760
[1mStep[0m  [28/42], [94mLoss[0m : 2.42118
[1mStep[0m  [32/42], [94mLoss[0m : 2.45570
[1mStep[0m  [36/42], [94mLoss[0m : 2.49827
[1mStep[0m  [40/42], [94mLoss[0m : 2.53514

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.911, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47052
[1mStep[0m  [4/42], [94mLoss[0m : 2.49482
[1mStep[0m  [8/42], [94mLoss[0m : 2.55231
[1mStep[0m  [12/42], [94mLoss[0m : 2.61463
[1mStep[0m  [16/42], [94mLoss[0m : 2.65410
[1mStep[0m  [20/42], [94mLoss[0m : 2.39478
[1mStep[0m  [24/42], [94mLoss[0m : 2.59049
[1mStep[0m  [28/42], [94mLoss[0m : 2.44510
[1mStep[0m  [32/42], [94mLoss[0m : 2.63908
[1mStep[0m  [36/42], [94mLoss[0m : 2.57803
[1mStep[0m  [40/42], [94mLoss[0m : 2.61359

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41978
[1mStep[0m  [4/42], [94mLoss[0m : 2.46853
[1mStep[0m  [8/42], [94mLoss[0m : 2.45655
[1mStep[0m  [12/42], [94mLoss[0m : 2.41552
[1mStep[0m  [16/42], [94mLoss[0m : 2.46468
[1mStep[0m  [20/42], [94mLoss[0m : 2.48961
[1mStep[0m  [24/42], [94mLoss[0m : 2.50745
[1mStep[0m  [28/42], [94mLoss[0m : 2.36920
[1mStep[0m  [32/42], [94mLoss[0m : 2.37886
[1mStep[0m  [36/42], [94mLoss[0m : 2.40147
[1mStep[0m  [40/42], [94mLoss[0m : 2.38782

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50380
[1mStep[0m  [4/42], [94mLoss[0m : 2.46967
[1mStep[0m  [8/42], [94mLoss[0m : 2.57553
[1mStep[0m  [12/42], [94mLoss[0m : 2.44975
[1mStep[0m  [16/42], [94mLoss[0m : 2.39327
[1mStep[0m  [20/42], [94mLoss[0m : 2.46484
[1mStep[0m  [24/42], [94mLoss[0m : 2.59170
[1mStep[0m  [28/42], [94mLoss[0m : 2.48691
[1mStep[0m  [32/42], [94mLoss[0m : 2.33183
[1mStep[0m  [36/42], [94mLoss[0m : 2.32674
[1mStep[0m  [40/42], [94mLoss[0m : 2.50272

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39792
[1mStep[0m  [4/42], [94mLoss[0m : 2.40458
[1mStep[0m  [8/42], [94mLoss[0m : 2.57488
[1mStep[0m  [12/42], [94mLoss[0m : 2.32165
[1mStep[0m  [16/42], [94mLoss[0m : 2.28410
[1mStep[0m  [20/42], [94mLoss[0m : 2.58995
[1mStep[0m  [24/42], [94mLoss[0m : 2.48748
[1mStep[0m  [28/42], [94mLoss[0m : 2.50577
[1mStep[0m  [32/42], [94mLoss[0m : 2.71299
[1mStep[0m  [36/42], [94mLoss[0m : 2.51940
[1mStep[0m  [40/42], [94mLoss[0m : 2.40623

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27857
[1mStep[0m  [4/42], [94mLoss[0m : 2.50698
[1mStep[0m  [8/42], [94mLoss[0m : 2.59538
[1mStep[0m  [12/42], [94mLoss[0m : 2.65069
[1mStep[0m  [16/42], [94mLoss[0m : 2.34787
[1mStep[0m  [20/42], [94mLoss[0m : 2.29709
[1mStep[0m  [24/42], [94mLoss[0m : 2.39607
[1mStep[0m  [28/42], [94mLoss[0m : 2.32105
[1mStep[0m  [32/42], [94mLoss[0m : 2.37951
[1mStep[0m  [36/42], [94mLoss[0m : 2.76369
[1mStep[0m  [40/42], [94mLoss[0m : 2.39011

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22856
[1mStep[0m  [4/42], [94mLoss[0m : 2.39804
[1mStep[0m  [8/42], [94mLoss[0m : 2.17497
[1mStep[0m  [12/42], [94mLoss[0m : 2.50595
[1mStep[0m  [16/42], [94mLoss[0m : 2.38333
[1mStep[0m  [20/42], [94mLoss[0m : 2.46278
[1mStep[0m  [24/42], [94mLoss[0m : 2.32582
[1mStep[0m  [28/42], [94mLoss[0m : 2.63249
[1mStep[0m  [32/42], [94mLoss[0m : 2.56728
[1mStep[0m  [36/42], [94mLoss[0m : 2.31901
[1mStep[0m  [40/42], [94mLoss[0m : 2.41924

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55171
[1mStep[0m  [4/42], [94mLoss[0m : 2.53102
[1mStep[0m  [8/42], [94mLoss[0m : 2.67720
[1mStep[0m  [12/42], [94mLoss[0m : 2.48819
[1mStep[0m  [16/42], [94mLoss[0m : 2.56120
[1mStep[0m  [20/42], [94mLoss[0m : 2.43478
[1mStep[0m  [24/42], [94mLoss[0m : 2.31719
[1mStep[0m  [28/42], [94mLoss[0m : 2.42967
[1mStep[0m  [32/42], [94mLoss[0m : 2.45303
[1mStep[0m  [36/42], [94mLoss[0m : 2.40688
[1mStep[0m  [40/42], [94mLoss[0m : 2.45839

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40732
[1mStep[0m  [4/42], [94mLoss[0m : 2.42666
[1mStep[0m  [8/42], [94mLoss[0m : 2.47670
[1mStep[0m  [12/42], [94mLoss[0m : 2.52225
[1mStep[0m  [16/42], [94mLoss[0m : 2.57441
[1mStep[0m  [20/42], [94mLoss[0m : 2.59327
[1mStep[0m  [24/42], [94mLoss[0m : 2.51173
[1mStep[0m  [28/42], [94mLoss[0m : 2.38605
[1mStep[0m  [32/42], [94mLoss[0m : 2.33112
[1mStep[0m  [36/42], [94mLoss[0m : 2.34671
[1mStep[0m  [40/42], [94mLoss[0m : 2.46460

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39980
[1mStep[0m  [4/42], [94mLoss[0m : 2.44186
[1mStep[0m  [8/42], [94mLoss[0m : 2.49832
[1mStep[0m  [12/42], [94mLoss[0m : 2.50975
[1mStep[0m  [16/42], [94mLoss[0m : 2.63666
[1mStep[0m  [20/42], [94mLoss[0m : 2.43610
[1mStep[0m  [24/42], [94mLoss[0m : 2.32448
[1mStep[0m  [28/42], [94mLoss[0m : 2.45064
[1mStep[0m  [32/42], [94mLoss[0m : 2.55661
[1mStep[0m  [36/42], [94mLoss[0m : 2.50922
[1mStep[0m  [40/42], [94mLoss[0m : 2.59312

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50021
[1mStep[0m  [4/42], [94mLoss[0m : 2.53755
[1mStep[0m  [8/42], [94mLoss[0m : 2.32399
[1mStep[0m  [12/42], [94mLoss[0m : 2.39299
[1mStep[0m  [16/42], [94mLoss[0m : 2.54076
[1mStep[0m  [20/42], [94mLoss[0m : 2.39616
[1mStep[0m  [24/42], [94mLoss[0m : 2.34727
[1mStep[0m  [28/42], [94mLoss[0m : 2.36187
[1mStep[0m  [32/42], [94mLoss[0m : 2.24032
[1mStep[0m  [36/42], [94mLoss[0m : 2.74459
[1mStep[0m  [40/42], [94mLoss[0m : 2.52075

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41127
[1mStep[0m  [4/42], [94mLoss[0m : 2.42547
[1mStep[0m  [8/42], [94mLoss[0m : 2.31154
[1mStep[0m  [12/42], [94mLoss[0m : 2.53271
[1mStep[0m  [16/42], [94mLoss[0m : 2.38773
[1mStep[0m  [20/42], [94mLoss[0m : 2.62616
[1mStep[0m  [24/42], [94mLoss[0m : 2.49026
[1mStep[0m  [28/42], [94mLoss[0m : 2.30092
[1mStep[0m  [32/42], [94mLoss[0m : 2.82161
[1mStep[0m  [36/42], [94mLoss[0m : 2.44363
[1mStep[0m  [40/42], [94mLoss[0m : 2.49454

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44967
[1mStep[0m  [4/42], [94mLoss[0m : 2.44176
[1mStep[0m  [8/42], [94mLoss[0m : 2.47527
[1mStep[0m  [12/42], [94mLoss[0m : 2.51748
[1mStep[0m  [16/42], [94mLoss[0m : 2.51182
[1mStep[0m  [20/42], [94mLoss[0m : 2.58690
[1mStep[0m  [24/42], [94mLoss[0m : 2.37442
[1mStep[0m  [28/42], [94mLoss[0m : 2.28221
[1mStep[0m  [32/42], [94mLoss[0m : 2.46154
[1mStep[0m  [36/42], [94mLoss[0m : 2.39486
[1mStep[0m  [40/42], [94mLoss[0m : 2.51700

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50684
[1mStep[0m  [4/42], [94mLoss[0m : 2.38021
[1mStep[0m  [8/42], [94mLoss[0m : 2.25160
[1mStep[0m  [12/42], [94mLoss[0m : 2.33003
[1mStep[0m  [16/42], [94mLoss[0m : 2.33098
[1mStep[0m  [20/42], [94mLoss[0m : 2.39706
[1mStep[0m  [24/42], [94mLoss[0m : 2.40921
[1mStep[0m  [28/42], [94mLoss[0m : 2.42763
[1mStep[0m  [32/42], [94mLoss[0m : 2.39235
[1mStep[0m  [36/42], [94mLoss[0m : 2.37308
[1mStep[0m  [40/42], [94mLoss[0m : 2.25728

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87393
[1mStep[0m  [4/42], [94mLoss[0m : 2.42877
[1mStep[0m  [8/42], [94mLoss[0m : 2.24274
[1mStep[0m  [12/42], [94mLoss[0m : 2.60371
[1mStep[0m  [16/42], [94mLoss[0m : 2.38423
[1mStep[0m  [20/42], [94mLoss[0m : 2.35705
[1mStep[0m  [24/42], [94mLoss[0m : 2.42334
[1mStep[0m  [28/42], [94mLoss[0m : 2.33786
[1mStep[0m  [32/42], [94mLoss[0m : 2.47842
[1mStep[0m  [36/42], [94mLoss[0m : 2.32930
[1mStep[0m  [40/42], [94mLoss[0m : 2.27372

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56089
[1mStep[0m  [4/42], [94mLoss[0m : 2.65849
[1mStep[0m  [8/42], [94mLoss[0m : 2.29382
[1mStep[0m  [12/42], [94mLoss[0m : 2.36854
[1mStep[0m  [16/42], [94mLoss[0m : 2.49029
[1mStep[0m  [20/42], [94mLoss[0m : 2.50719
[1mStep[0m  [24/42], [94mLoss[0m : 2.62434
[1mStep[0m  [28/42], [94mLoss[0m : 2.37528
[1mStep[0m  [32/42], [94mLoss[0m : 2.37790
[1mStep[0m  [36/42], [94mLoss[0m : 2.32534
[1mStep[0m  [40/42], [94mLoss[0m : 2.38443

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33804
[1mStep[0m  [4/42], [94mLoss[0m : 2.40551
[1mStep[0m  [8/42], [94mLoss[0m : 2.63431
[1mStep[0m  [12/42], [94mLoss[0m : 2.29330
[1mStep[0m  [16/42], [94mLoss[0m : 2.64583
[1mStep[0m  [20/42], [94mLoss[0m : 2.32779
[1mStep[0m  [24/42], [94mLoss[0m : 2.46409
[1mStep[0m  [28/42], [94mLoss[0m : 2.22803
[1mStep[0m  [32/42], [94mLoss[0m : 2.34500
[1mStep[0m  [36/42], [94mLoss[0m : 2.38963
[1mStep[0m  [40/42], [94mLoss[0m : 2.30862

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24628
[1mStep[0m  [4/42], [94mLoss[0m : 2.63765
[1mStep[0m  [8/42], [94mLoss[0m : 2.47990
[1mStep[0m  [12/42], [94mLoss[0m : 2.46792
[1mStep[0m  [16/42], [94mLoss[0m : 2.51369
[1mStep[0m  [20/42], [94mLoss[0m : 2.47629
[1mStep[0m  [24/42], [94mLoss[0m : 2.52587
[1mStep[0m  [28/42], [94mLoss[0m : 2.50337
[1mStep[0m  [32/42], [94mLoss[0m : 2.48719
[1mStep[0m  [36/42], [94mLoss[0m : 2.29626
[1mStep[0m  [40/42], [94mLoss[0m : 2.53962

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62088
[1mStep[0m  [4/42], [94mLoss[0m : 2.32408
[1mStep[0m  [8/42], [94mLoss[0m : 2.30390
[1mStep[0m  [12/42], [94mLoss[0m : 2.26051
[1mStep[0m  [16/42], [94mLoss[0m : 2.35364
[1mStep[0m  [20/42], [94mLoss[0m : 2.45626
[1mStep[0m  [24/42], [94mLoss[0m : 2.67441
[1mStep[0m  [28/42], [94mLoss[0m : 2.38946
[1mStep[0m  [32/42], [94mLoss[0m : 2.19490
[1mStep[0m  [36/42], [94mLoss[0m : 2.43629
[1mStep[0m  [40/42], [94mLoss[0m : 2.44228

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19720
[1mStep[0m  [4/42], [94mLoss[0m : 2.42916
[1mStep[0m  [8/42], [94mLoss[0m : 2.41652
[1mStep[0m  [12/42], [94mLoss[0m : 2.70590
[1mStep[0m  [16/42], [94mLoss[0m : 2.57283
[1mStep[0m  [20/42], [94mLoss[0m : 2.34877
[1mStep[0m  [24/42], [94mLoss[0m : 2.44506
[1mStep[0m  [28/42], [94mLoss[0m : 2.37979
[1mStep[0m  [32/42], [94mLoss[0m : 2.35185
[1mStep[0m  [36/42], [94mLoss[0m : 2.37785
[1mStep[0m  [40/42], [94mLoss[0m : 2.39008

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49717
[1mStep[0m  [4/42], [94mLoss[0m : 2.46014
[1mStep[0m  [8/42], [94mLoss[0m : 2.45644
[1mStep[0m  [12/42], [94mLoss[0m : 2.54454
[1mStep[0m  [16/42], [94mLoss[0m : 2.30878
[1mStep[0m  [20/42], [94mLoss[0m : 2.58162
[1mStep[0m  [24/42], [94mLoss[0m : 2.43496
[1mStep[0m  [28/42], [94mLoss[0m : 2.46756
[1mStep[0m  [32/42], [94mLoss[0m : 2.42496
[1mStep[0m  [36/42], [94mLoss[0m : 2.20012
[1mStep[0m  [40/42], [94mLoss[0m : 2.61425

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36661
[1mStep[0m  [4/42], [94mLoss[0m : 2.44059
[1mStep[0m  [8/42], [94mLoss[0m : 2.30878
[1mStep[0m  [12/42], [94mLoss[0m : 2.39931
[1mStep[0m  [16/42], [94mLoss[0m : 2.45220
[1mStep[0m  [20/42], [94mLoss[0m : 2.46657
[1mStep[0m  [24/42], [94mLoss[0m : 2.29892
[1mStep[0m  [28/42], [94mLoss[0m : 2.53952
[1mStep[0m  [32/42], [94mLoss[0m : 2.38891
[1mStep[0m  [36/42], [94mLoss[0m : 2.28102
[1mStep[0m  [40/42], [94mLoss[0m : 2.44015

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37561
[1mStep[0m  [4/42], [94mLoss[0m : 2.44596
[1mStep[0m  [8/42], [94mLoss[0m : 2.36207
[1mStep[0m  [12/42], [94mLoss[0m : 2.43805
[1mStep[0m  [16/42], [94mLoss[0m : 2.36033
[1mStep[0m  [20/42], [94mLoss[0m : 2.30021
[1mStep[0m  [24/42], [94mLoss[0m : 2.33314
[1mStep[0m  [28/42], [94mLoss[0m : 2.58479
[1mStep[0m  [32/42], [94mLoss[0m : 2.37104
[1mStep[0m  [36/42], [94mLoss[0m : 2.52568
[1mStep[0m  [40/42], [94mLoss[0m : 2.40058

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42522
[1mStep[0m  [4/42], [94mLoss[0m : 2.25941
[1mStep[0m  [8/42], [94mLoss[0m : 2.42938
[1mStep[0m  [12/42], [94mLoss[0m : 2.38723
[1mStep[0m  [16/42], [94mLoss[0m : 2.65461
[1mStep[0m  [20/42], [94mLoss[0m : 2.36756
[1mStep[0m  [24/42], [94mLoss[0m : 2.59524
[1mStep[0m  [28/42], [94mLoss[0m : 2.49585
[1mStep[0m  [32/42], [94mLoss[0m : 2.32053
[1mStep[0m  [36/42], [94mLoss[0m : 2.56980
[1mStep[0m  [40/42], [94mLoss[0m : 2.49521

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42793
[1mStep[0m  [4/42], [94mLoss[0m : 2.25084
[1mStep[0m  [8/42], [94mLoss[0m : 2.30349
[1mStep[0m  [12/42], [94mLoss[0m : 2.51673
[1mStep[0m  [16/42], [94mLoss[0m : 2.40206
[1mStep[0m  [20/42], [94mLoss[0m : 2.29304
[1mStep[0m  [24/42], [94mLoss[0m : 2.48517
[1mStep[0m  [28/42], [94mLoss[0m : 2.43586
[1mStep[0m  [32/42], [94mLoss[0m : 2.45887
[1mStep[0m  [36/42], [94mLoss[0m : 2.26986
[1mStep[0m  [40/42], [94mLoss[0m : 2.62656

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39860
[1mStep[0m  [4/42], [94mLoss[0m : 2.48624
[1mStep[0m  [8/42], [94mLoss[0m : 2.45125
[1mStep[0m  [12/42], [94mLoss[0m : 2.50945
[1mStep[0m  [16/42], [94mLoss[0m : 2.36643
[1mStep[0m  [20/42], [94mLoss[0m : 2.59593
[1mStep[0m  [24/42], [94mLoss[0m : 2.48843
[1mStep[0m  [28/42], [94mLoss[0m : 2.35129
[1mStep[0m  [32/42], [94mLoss[0m : 2.47575
[1mStep[0m  [36/42], [94mLoss[0m : 2.37503
[1mStep[0m  [40/42], [94mLoss[0m : 2.30449

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35930
[1mStep[0m  [4/42], [94mLoss[0m : 2.55095
[1mStep[0m  [8/42], [94mLoss[0m : 2.66491
[1mStep[0m  [12/42], [94mLoss[0m : 2.46260
[1mStep[0m  [16/42], [94mLoss[0m : 2.40288
[1mStep[0m  [20/42], [94mLoss[0m : 2.31410
[1mStep[0m  [24/42], [94mLoss[0m : 2.59107
[1mStep[0m  [28/42], [94mLoss[0m : 2.48829
[1mStep[0m  [32/42], [94mLoss[0m : 2.52172
[1mStep[0m  [36/42], [94mLoss[0m : 2.39112
[1mStep[0m  [40/42], [94mLoss[0m : 2.24628

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48681
[1mStep[0m  [4/42], [94mLoss[0m : 2.36065
[1mStep[0m  [8/42], [94mLoss[0m : 2.29079
[1mStep[0m  [12/42], [94mLoss[0m : 2.36427
[1mStep[0m  [16/42], [94mLoss[0m : 2.38412
[1mStep[0m  [20/42], [94mLoss[0m : 2.57327
[1mStep[0m  [24/42], [94mLoss[0m : 2.52587
[1mStep[0m  [28/42], [94mLoss[0m : 2.41813
[1mStep[0m  [32/42], [94mLoss[0m : 2.47166
[1mStep[0m  [36/42], [94mLoss[0m : 2.46776
[1mStep[0m  [40/42], [94mLoss[0m : 2.46681

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41318
[1mStep[0m  [4/42], [94mLoss[0m : 2.65780
[1mStep[0m  [8/42], [94mLoss[0m : 2.34321
[1mStep[0m  [12/42], [94mLoss[0m : 2.32852
[1mStep[0m  [16/42], [94mLoss[0m : 2.52634
[1mStep[0m  [20/42], [94mLoss[0m : 2.60371
[1mStep[0m  [24/42], [94mLoss[0m : 2.47275
[1mStep[0m  [28/42], [94mLoss[0m : 2.37605
[1mStep[0m  [32/42], [94mLoss[0m : 2.40040
[1mStep[0m  [36/42], [94mLoss[0m : 2.30245
[1mStep[0m  [40/42], [94mLoss[0m : 2.42881

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3262148244040355
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.63806
[1mStep[0m  [4/42], [94mLoss[0m : 2.30258
[1mStep[0m  [8/42], [94mLoss[0m : 2.44795
[1mStep[0m  [12/42], [94mLoss[0m : 2.54119
[1mStep[0m  [16/42], [94mLoss[0m : 2.49635
[1mStep[0m  [20/42], [94mLoss[0m : 2.21823
[1mStep[0m  [24/42], [94mLoss[0m : 2.47087
[1mStep[0m  [28/42], [94mLoss[0m : 2.26196
[1mStep[0m  [32/42], [94mLoss[0m : 2.39854
[1mStep[0m  [36/42], [94mLoss[0m : 2.43524
[1mStep[0m  [40/42], [94mLoss[0m : 2.43415

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.321, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53985
[1mStep[0m  [4/42], [94mLoss[0m : 2.03914
[1mStep[0m  [8/42], [94mLoss[0m : 2.46415
[1mStep[0m  [12/42], [94mLoss[0m : 2.56582
[1mStep[0m  [16/42], [94mLoss[0m : 2.37524
[1mStep[0m  [20/42], [94mLoss[0m : 2.46214
[1mStep[0m  [24/42], [94mLoss[0m : 2.53489
[1mStep[0m  [28/42], [94mLoss[0m : 2.50682
[1mStep[0m  [32/42], [94mLoss[0m : 2.35009
[1mStep[0m  [36/42], [94mLoss[0m : 2.52840
[1mStep[0m  [40/42], [94mLoss[0m : 2.50291

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51493
[1mStep[0m  [4/42], [94mLoss[0m : 2.62971
[1mStep[0m  [8/42], [94mLoss[0m : 2.42601
[1mStep[0m  [12/42], [94mLoss[0m : 2.51407
[1mStep[0m  [16/42], [94mLoss[0m : 2.37905
[1mStep[0m  [20/42], [94mLoss[0m : 2.41091
[1mStep[0m  [24/42], [94mLoss[0m : 2.48179
[1mStep[0m  [28/42], [94mLoss[0m : 2.28222
[1mStep[0m  [32/42], [94mLoss[0m : 2.49088
[1mStep[0m  [36/42], [94mLoss[0m : 2.33653
[1mStep[0m  [40/42], [94mLoss[0m : 2.17161

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31532
[1mStep[0m  [4/42], [94mLoss[0m : 2.29781
[1mStep[0m  [8/42], [94mLoss[0m : 2.23964
[1mStep[0m  [12/42], [94mLoss[0m : 2.61654
[1mStep[0m  [16/42], [94mLoss[0m : 2.41676
[1mStep[0m  [20/42], [94mLoss[0m : 2.25379
[1mStep[0m  [24/42], [94mLoss[0m : 2.46077
[1mStep[0m  [28/42], [94mLoss[0m : 2.41692
[1mStep[0m  [32/42], [94mLoss[0m : 2.26832
[1mStep[0m  [36/42], [94mLoss[0m : 2.38555
[1mStep[0m  [40/42], [94mLoss[0m : 2.15950

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27226
[1mStep[0m  [4/42], [94mLoss[0m : 2.29490
[1mStep[0m  [8/42], [94mLoss[0m : 2.36310
[1mStep[0m  [12/42], [94mLoss[0m : 2.24026
[1mStep[0m  [16/42], [94mLoss[0m : 2.30346
[1mStep[0m  [20/42], [94mLoss[0m : 2.05161
[1mStep[0m  [24/42], [94mLoss[0m : 2.19876
[1mStep[0m  [28/42], [94mLoss[0m : 2.44383
[1mStep[0m  [32/42], [94mLoss[0m : 2.17105
[1mStep[0m  [36/42], [94mLoss[0m : 2.41376
[1mStep[0m  [40/42], [94mLoss[0m : 2.25812

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13413
[1mStep[0m  [4/42], [94mLoss[0m : 2.30200
[1mStep[0m  [8/42], [94mLoss[0m : 2.59033
[1mStep[0m  [12/42], [94mLoss[0m : 2.31340
[1mStep[0m  [16/42], [94mLoss[0m : 2.29698
[1mStep[0m  [20/42], [94mLoss[0m : 2.21571
[1mStep[0m  [24/42], [94mLoss[0m : 2.15921
[1mStep[0m  [28/42], [94mLoss[0m : 2.23081
[1mStep[0m  [32/42], [94mLoss[0m : 2.30770
[1mStep[0m  [36/42], [94mLoss[0m : 2.29367
[1mStep[0m  [40/42], [94mLoss[0m : 2.20190

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.307, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32237
[1mStep[0m  [4/42], [94mLoss[0m : 2.55596
[1mStep[0m  [8/42], [94mLoss[0m : 2.08544
[1mStep[0m  [12/42], [94mLoss[0m : 2.05170
[1mStep[0m  [16/42], [94mLoss[0m : 2.30960
[1mStep[0m  [20/42], [94mLoss[0m : 2.27147
[1mStep[0m  [24/42], [94mLoss[0m : 2.28026
[1mStep[0m  [28/42], [94mLoss[0m : 2.32437
[1mStep[0m  [32/42], [94mLoss[0m : 2.35986
[1mStep[0m  [36/42], [94mLoss[0m : 2.06349
[1mStep[0m  [40/42], [94mLoss[0m : 2.26537

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12224
[1mStep[0m  [4/42], [94mLoss[0m : 2.36538
[1mStep[0m  [8/42], [94mLoss[0m : 2.30329
[1mStep[0m  [12/42], [94mLoss[0m : 2.19009
[1mStep[0m  [16/42], [94mLoss[0m : 2.36611
[1mStep[0m  [20/42], [94mLoss[0m : 2.08244
[1mStep[0m  [24/42], [94mLoss[0m : 2.16835
[1mStep[0m  [28/42], [94mLoss[0m : 2.12606
[1mStep[0m  [32/42], [94mLoss[0m : 2.53475
[1mStep[0m  [36/42], [94mLoss[0m : 2.37981
[1mStep[0m  [40/42], [94mLoss[0m : 2.23528

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06219
[1mStep[0m  [4/42], [94mLoss[0m : 2.13277
[1mStep[0m  [8/42], [94mLoss[0m : 2.14524
[1mStep[0m  [12/42], [94mLoss[0m : 2.09801
[1mStep[0m  [16/42], [94mLoss[0m : 2.27446
[1mStep[0m  [20/42], [94mLoss[0m : 2.27897
[1mStep[0m  [24/42], [94mLoss[0m : 2.06732
[1mStep[0m  [28/42], [94mLoss[0m : 2.12293
[1mStep[0m  [32/42], [94mLoss[0m : 2.22385
[1mStep[0m  [36/42], [94mLoss[0m : 1.98189
[1mStep[0m  [40/42], [94mLoss[0m : 2.29915

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00461
[1mStep[0m  [4/42], [94mLoss[0m : 2.17267
[1mStep[0m  [8/42], [94mLoss[0m : 2.20964
[1mStep[0m  [12/42], [94mLoss[0m : 2.19587
[1mStep[0m  [16/42], [94mLoss[0m : 2.13296
[1mStep[0m  [20/42], [94mLoss[0m : 2.10799
[1mStep[0m  [24/42], [94mLoss[0m : 2.26841
[1mStep[0m  [28/42], [94mLoss[0m : 2.13720
[1mStep[0m  [32/42], [94mLoss[0m : 2.23471
[1mStep[0m  [36/42], [94mLoss[0m : 2.17776
[1mStep[0m  [40/42], [94mLoss[0m : 2.27173

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21554
[1mStep[0m  [4/42], [94mLoss[0m : 2.19246
[1mStep[0m  [8/42], [94mLoss[0m : 2.06151
[1mStep[0m  [12/42], [94mLoss[0m : 1.95473
[1mStep[0m  [16/42], [94mLoss[0m : 2.09840
[1mStep[0m  [20/42], [94mLoss[0m : 2.03004
[1mStep[0m  [24/42], [94mLoss[0m : 1.95839
[1mStep[0m  [28/42], [94mLoss[0m : 2.06913
[1mStep[0m  [32/42], [94mLoss[0m : 2.02233
[1mStep[0m  [36/42], [94mLoss[0m : 2.08007
[1mStep[0m  [40/42], [94mLoss[0m : 2.27372

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04286
[1mStep[0m  [4/42], [94mLoss[0m : 1.95261
[1mStep[0m  [8/42], [94mLoss[0m : 2.06624
[1mStep[0m  [12/42], [94mLoss[0m : 2.01861
[1mStep[0m  [16/42], [94mLoss[0m : 2.07249
[1mStep[0m  [20/42], [94mLoss[0m : 2.01690
[1mStep[0m  [24/42], [94mLoss[0m : 2.07689
[1mStep[0m  [28/42], [94mLoss[0m : 2.04354
[1mStep[0m  [32/42], [94mLoss[0m : 1.89364
[1mStep[0m  [36/42], [94mLoss[0m : 1.91041
[1mStep[0m  [40/42], [94mLoss[0m : 2.03044

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.055, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91801
[1mStep[0m  [4/42], [94mLoss[0m : 2.04384
[1mStep[0m  [8/42], [94mLoss[0m : 1.85536
[1mStep[0m  [12/42], [94mLoss[0m : 2.02786
[1mStep[0m  [16/42], [94mLoss[0m : 1.87122
[1mStep[0m  [20/42], [94mLoss[0m : 2.06362
[1mStep[0m  [24/42], [94mLoss[0m : 2.10713
[1mStep[0m  [28/42], [94mLoss[0m : 1.99953
[1mStep[0m  [32/42], [94mLoss[0m : 2.03301
[1mStep[0m  [36/42], [94mLoss[0m : 2.08650
[1mStep[0m  [40/42], [94mLoss[0m : 1.87776

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04477
[1mStep[0m  [4/42], [94mLoss[0m : 1.99447
[1mStep[0m  [8/42], [94mLoss[0m : 1.87181
[1mStep[0m  [12/42], [94mLoss[0m : 1.96387
[1mStep[0m  [16/42], [94mLoss[0m : 1.99369
[1mStep[0m  [20/42], [94mLoss[0m : 1.91804
[1mStep[0m  [24/42], [94mLoss[0m : 1.94835
[1mStep[0m  [28/42], [94mLoss[0m : 1.96705
[1mStep[0m  [32/42], [94mLoss[0m : 2.02260
[1mStep[0m  [36/42], [94mLoss[0m : 2.02964
[1mStep[0m  [40/42], [94mLoss[0m : 1.98569

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11154
[1mStep[0m  [4/42], [94mLoss[0m : 1.80040
[1mStep[0m  [8/42], [94mLoss[0m : 1.97415
[1mStep[0m  [12/42], [94mLoss[0m : 1.95133
[1mStep[0m  [16/42], [94mLoss[0m : 2.16404
[1mStep[0m  [20/42], [94mLoss[0m : 1.97330
[1mStep[0m  [24/42], [94mLoss[0m : 1.84099
[1mStep[0m  [28/42], [94mLoss[0m : 2.05122
[1mStep[0m  [32/42], [94mLoss[0m : 1.86033
[1mStep[0m  [36/42], [94mLoss[0m : 1.99033
[1mStep[0m  [40/42], [94mLoss[0m : 1.94231

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76445
[1mStep[0m  [4/42], [94mLoss[0m : 1.90634
[1mStep[0m  [8/42], [94mLoss[0m : 1.91430
[1mStep[0m  [12/42], [94mLoss[0m : 1.87814
[1mStep[0m  [16/42], [94mLoss[0m : 2.07552
[1mStep[0m  [20/42], [94mLoss[0m : 1.90447
[1mStep[0m  [24/42], [94mLoss[0m : 1.82364
[1mStep[0m  [28/42], [94mLoss[0m : 1.85824
[1mStep[0m  [32/42], [94mLoss[0m : 2.04431
[1mStep[0m  [36/42], [94mLoss[0m : 1.86821
[1mStep[0m  [40/42], [94mLoss[0m : 1.80026

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90544
[1mStep[0m  [4/42], [94mLoss[0m : 1.79672
[1mStep[0m  [8/42], [94mLoss[0m : 1.71725
[1mStep[0m  [12/42], [94mLoss[0m : 1.93843
[1mStep[0m  [16/42], [94mLoss[0m : 1.77247
[1mStep[0m  [20/42], [94mLoss[0m : 1.95081
[1mStep[0m  [24/42], [94mLoss[0m : 2.03266
[1mStep[0m  [28/42], [94mLoss[0m : 1.99017
[1mStep[0m  [32/42], [94mLoss[0m : 1.79616
[1mStep[0m  [36/42], [94mLoss[0m : 1.93414
[1mStep[0m  [40/42], [94mLoss[0m : 1.89498

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74064
[1mStep[0m  [4/42], [94mLoss[0m : 1.70070
[1mStep[0m  [8/42], [94mLoss[0m : 1.72623
[1mStep[0m  [12/42], [94mLoss[0m : 1.89024
[1mStep[0m  [16/42], [94mLoss[0m : 1.87793
[1mStep[0m  [20/42], [94mLoss[0m : 1.84714
[1mStep[0m  [24/42], [94mLoss[0m : 1.92548
[1mStep[0m  [28/42], [94mLoss[0m : 1.75560
[1mStep[0m  [32/42], [94mLoss[0m : 1.87477
[1mStep[0m  [36/42], [94mLoss[0m : 1.75012
[1mStep[0m  [40/42], [94mLoss[0m : 2.01401

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75193
[1mStep[0m  [4/42], [94mLoss[0m : 1.66538
[1mStep[0m  [8/42], [94mLoss[0m : 1.67054
[1mStep[0m  [12/42], [94mLoss[0m : 1.77336
[1mStep[0m  [16/42], [94mLoss[0m : 1.82372
[1mStep[0m  [20/42], [94mLoss[0m : 1.76178
[1mStep[0m  [24/42], [94mLoss[0m : 1.81644
[1mStep[0m  [28/42], [94mLoss[0m : 1.89473
[1mStep[0m  [32/42], [94mLoss[0m : 1.76122
[1mStep[0m  [36/42], [94mLoss[0m : 1.70071
[1mStep[0m  [40/42], [94mLoss[0m : 1.71923

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72797
[1mStep[0m  [4/42], [94mLoss[0m : 1.69624
[1mStep[0m  [8/42], [94mLoss[0m : 1.78484
[1mStep[0m  [12/42], [94mLoss[0m : 1.59165
[1mStep[0m  [16/42], [94mLoss[0m : 1.80339
[1mStep[0m  [20/42], [94mLoss[0m : 1.84963
[1mStep[0m  [24/42], [94mLoss[0m : 1.95003
[1mStep[0m  [28/42], [94mLoss[0m : 1.79373
[1mStep[0m  [32/42], [94mLoss[0m : 1.80233
[1mStep[0m  [36/42], [94mLoss[0m : 1.62490
[1mStep[0m  [40/42], [94mLoss[0m : 1.81901

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68972
[1mStep[0m  [4/42], [94mLoss[0m : 1.75670
[1mStep[0m  [8/42], [94mLoss[0m : 1.74906
[1mStep[0m  [12/42], [94mLoss[0m : 1.69676
[1mStep[0m  [16/42], [94mLoss[0m : 1.74196
[1mStep[0m  [20/42], [94mLoss[0m : 1.85623
[1mStep[0m  [24/42], [94mLoss[0m : 1.66086
[1mStep[0m  [28/42], [94mLoss[0m : 1.69473
[1mStep[0m  [32/42], [94mLoss[0m : 1.65878
[1mStep[0m  [36/42], [94mLoss[0m : 1.75645
[1mStep[0m  [40/42], [94mLoss[0m : 1.74072

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54329
[1mStep[0m  [4/42], [94mLoss[0m : 1.48853
[1mStep[0m  [8/42], [94mLoss[0m : 1.48495
[1mStep[0m  [12/42], [94mLoss[0m : 1.75323
[1mStep[0m  [16/42], [94mLoss[0m : 1.72767
[1mStep[0m  [20/42], [94mLoss[0m : 1.65088
[1mStep[0m  [24/42], [94mLoss[0m : 1.43341
[1mStep[0m  [28/42], [94mLoss[0m : 1.79747
[1mStep[0m  [32/42], [94mLoss[0m : 1.72872
[1mStep[0m  [36/42], [94mLoss[0m : 1.73636
[1mStep[0m  [40/42], [94mLoss[0m : 1.77067

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50862
[1mStep[0m  [4/42], [94mLoss[0m : 1.65527
[1mStep[0m  [8/42], [94mLoss[0m : 1.69181
[1mStep[0m  [12/42], [94mLoss[0m : 1.55030
[1mStep[0m  [16/42], [94mLoss[0m : 1.72725
[1mStep[0m  [20/42], [94mLoss[0m : 1.60234
[1mStep[0m  [24/42], [94mLoss[0m : 1.58316
[1mStep[0m  [28/42], [94mLoss[0m : 1.60202
[1mStep[0m  [32/42], [94mLoss[0m : 1.63668
[1mStep[0m  [36/42], [94mLoss[0m : 1.60457
[1mStep[0m  [40/42], [94mLoss[0m : 1.63630

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72506
[1mStep[0m  [4/42], [94mLoss[0m : 1.59161
[1mStep[0m  [8/42], [94mLoss[0m : 1.53654
[1mStep[0m  [12/42], [94mLoss[0m : 1.51742
[1mStep[0m  [16/42], [94mLoss[0m : 1.56684
[1mStep[0m  [20/42], [94mLoss[0m : 1.57216
[1mStep[0m  [24/42], [94mLoss[0m : 1.53937
[1mStep[0m  [28/42], [94mLoss[0m : 1.66503
[1mStep[0m  [32/42], [94mLoss[0m : 1.49531
[1mStep[0m  [36/42], [94mLoss[0m : 1.64038
[1mStep[0m  [40/42], [94mLoss[0m : 1.56400

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.48370
[1mStep[0m  [4/42], [94mLoss[0m : 1.48709
[1mStep[0m  [8/42], [94mLoss[0m : 1.42407
[1mStep[0m  [12/42], [94mLoss[0m : 1.57201
[1mStep[0m  [16/42], [94mLoss[0m : 1.70944
[1mStep[0m  [20/42], [94mLoss[0m : 1.52708
[1mStep[0m  [24/42], [94mLoss[0m : 1.61885
[1mStep[0m  [28/42], [94mLoss[0m : 1.60648
[1mStep[0m  [32/42], [94mLoss[0m : 1.55280
[1mStep[0m  [36/42], [94mLoss[0m : 1.57318
[1mStep[0m  [40/42], [94mLoss[0m : 1.55858

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.580, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46822
[1mStep[0m  [4/42], [94mLoss[0m : 1.51057
[1mStep[0m  [8/42], [94mLoss[0m : 1.76267
[1mStep[0m  [12/42], [94mLoss[0m : 1.48907
[1mStep[0m  [16/42], [94mLoss[0m : 1.62865
[1mStep[0m  [20/42], [94mLoss[0m : 1.47498
[1mStep[0m  [24/42], [94mLoss[0m : 1.55791
[1mStep[0m  [28/42], [94mLoss[0m : 1.55282
[1mStep[0m  [32/42], [94mLoss[0m : 1.50345
[1mStep[0m  [36/42], [94mLoss[0m : 1.66589
[1mStep[0m  [40/42], [94mLoss[0m : 1.55709

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60388
[1mStep[0m  [4/42], [94mLoss[0m : 1.47133
[1mStep[0m  [8/42], [94mLoss[0m : 1.57334
[1mStep[0m  [12/42], [94mLoss[0m : 1.56663
[1mStep[0m  [16/42], [94mLoss[0m : 1.36586
[1mStep[0m  [20/42], [94mLoss[0m : 1.46372
[1mStep[0m  [24/42], [94mLoss[0m : 1.64589
[1mStep[0m  [28/42], [94mLoss[0m : 1.69067
[1mStep[0m  [32/42], [94mLoss[0m : 1.72898
[1mStep[0m  [36/42], [94mLoss[0m : 1.66000
[1mStep[0m  [40/42], [94mLoss[0m : 1.55652

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62669
[1mStep[0m  [4/42], [94mLoss[0m : 1.61805
[1mStep[0m  [8/42], [94mLoss[0m : 1.47121
[1mStep[0m  [12/42], [94mLoss[0m : 1.43126
[1mStep[0m  [16/42], [94mLoss[0m : 1.68859
[1mStep[0m  [20/42], [94mLoss[0m : 1.36964
[1mStep[0m  [24/42], [94mLoss[0m : 1.46325
[1mStep[0m  [28/42], [94mLoss[0m : 1.52338
[1mStep[0m  [32/42], [94mLoss[0m : 1.46818
[1mStep[0m  [36/42], [94mLoss[0m : 1.43580
[1mStep[0m  [40/42], [94mLoss[0m : 1.36772

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.524, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46835
[1mStep[0m  [4/42], [94mLoss[0m : 1.47784
[1mStep[0m  [8/42], [94mLoss[0m : 1.47207
[1mStep[0m  [12/42], [94mLoss[0m : 1.30132
[1mStep[0m  [16/42], [94mLoss[0m : 1.37520
[1mStep[0m  [20/42], [94mLoss[0m : 1.44209
[1mStep[0m  [24/42], [94mLoss[0m : 1.57806
[1mStep[0m  [28/42], [94mLoss[0m : 1.48648
[1mStep[0m  [32/42], [94mLoss[0m : 1.45932
[1mStep[0m  [36/42], [94mLoss[0m : 1.58840
[1mStep[0m  [40/42], [94mLoss[0m : 1.59363

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36204
[1mStep[0m  [4/42], [94mLoss[0m : 1.48361
[1mStep[0m  [8/42], [94mLoss[0m : 1.46721
[1mStep[0m  [12/42], [94mLoss[0m : 1.43199
[1mStep[0m  [16/42], [94mLoss[0m : 1.44760
[1mStep[0m  [20/42], [94mLoss[0m : 1.40341
[1mStep[0m  [24/42], [94mLoss[0m : 1.46356
[1mStep[0m  [28/42], [94mLoss[0m : 1.58566
[1mStep[0m  [32/42], [94mLoss[0m : 1.41984
[1mStep[0m  [36/42], [94mLoss[0m : 1.56108
[1mStep[0m  [40/42], [94mLoss[0m : 1.40890

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.480, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.465
====================================

Phase 2 - Evaluation MAE:  2.4649145092282976
MAE score P1      2.326215
MAE score P2      2.464915
loss              1.479792
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.18228
[1mStep[0m  [4/42], [94mLoss[0m : 9.90485
[1mStep[0m  [8/42], [94mLoss[0m : 9.31347
[1mStep[0m  [12/42], [94mLoss[0m : 8.70995
[1mStep[0m  [16/42], [94mLoss[0m : 7.92962
[1mStep[0m  [20/42], [94mLoss[0m : 7.02706
[1mStep[0m  [24/42], [94mLoss[0m : 6.81059
[1mStep[0m  [28/42], [94mLoss[0m : 6.05339
[1mStep[0m  [32/42], [94mLoss[0m : 5.57777
[1mStep[0m  [36/42], [94mLoss[0m : 5.00674
[1mStep[0m  [40/42], [94mLoss[0m : 4.25820

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.317, [92mTest[0m: 10.837, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.09408
[1mStep[0m  [4/42], [94mLoss[0m : 4.02670
[1mStep[0m  [8/42], [94mLoss[0m : 3.73572
[1mStep[0m  [12/42], [94mLoss[0m : 3.39706
[1mStep[0m  [16/42], [94mLoss[0m : 3.18622
[1mStep[0m  [20/42], [94mLoss[0m : 3.04925
[1mStep[0m  [24/42], [94mLoss[0m : 2.76562
[1mStep[0m  [28/42], [94mLoss[0m : 2.96203
[1mStep[0m  [32/42], [94mLoss[0m : 2.82729
[1mStep[0m  [36/42], [94mLoss[0m : 2.71603
[1mStep[0m  [40/42], [94mLoss[0m : 2.56632

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.185, [92mTest[0m: 6.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.93163
[1mStep[0m  [4/42], [94mLoss[0m : 2.80630
[1mStep[0m  [8/42], [94mLoss[0m : 2.92782
[1mStep[0m  [12/42], [94mLoss[0m : 2.69560
[1mStep[0m  [16/42], [94mLoss[0m : 2.74689
[1mStep[0m  [20/42], [94mLoss[0m : 2.70982
[1mStep[0m  [24/42], [94mLoss[0m : 2.62010
[1mStep[0m  [28/42], [94mLoss[0m : 2.59317
[1mStep[0m  [32/42], [94mLoss[0m : 2.49717
[1mStep[0m  [36/42], [94mLoss[0m : 2.78837
[1mStep[0m  [40/42], [94mLoss[0m : 2.59637

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.749, [92mTest[0m: 3.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64839
[1mStep[0m  [4/42], [94mLoss[0m : 2.60764
[1mStep[0m  [8/42], [94mLoss[0m : 2.93879
[1mStep[0m  [12/42], [94mLoss[0m : 2.60797
[1mStep[0m  [16/42], [94mLoss[0m : 2.71427
[1mStep[0m  [20/42], [94mLoss[0m : 2.57828
[1mStep[0m  [24/42], [94mLoss[0m : 2.59859
[1mStep[0m  [28/42], [94mLoss[0m : 2.63273
[1mStep[0m  [32/42], [94mLoss[0m : 2.42541
[1mStep[0m  [36/42], [94mLoss[0m : 2.66357
[1mStep[0m  [40/42], [94mLoss[0m : 2.61658

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.666, [92mTest[0m: 3.072, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70934
[1mStep[0m  [4/42], [94mLoss[0m : 2.68580
[1mStep[0m  [8/42], [94mLoss[0m : 2.40012
[1mStep[0m  [12/42], [94mLoss[0m : 2.69921
[1mStep[0m  [16/42], [94mLoss[0m : 2.54119
[1mStep[0m  [20/42], [94mLoss[0m : 2.60011
[1mStep[0m  [24/42], [94mLoss[0m : 2.46878
[1mStep[0m  [28/42], [94mLoss[0m : 2.54658
[1mStep[0m  [32/42], [94mLoss[0m : 2.62667
[1mStep[0m  [36/42], [94mLoss[0m : 2.84855
[1mStep[0m  [40/42], [94mLoss[0m : 2.47685

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.807, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48899
[1mStep[0m  [4/42], [94mLoss[0m : 2.53195
[1mStep[0m  [8/42], [94mLoss[0m : 2.59461
[1mStep[0m  [12/42], [94mLoss[0m : 2.54294
[1mStep[0m  [16/42], [94mLoss[0m : 2.64789
[1mStep[0m  [20/42], [94mLoss[0m : 2.41735
[1mStep[0m  [24/42], [94mLoss[0m : 2.84792
[1mStep[0m  [28/42], [94mLoss[0m : 2.74554
[1mStep[0m  [32/42], [94mLoss[0m : 2.57266
[1mStep[0m  [36/42], [94mLoss[0m : 2.77169
[1mStep[0m  [40/42], [94mLoss[0m : 2.69631

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.705, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84249
[1mStep[0m  [4/42], [94mLoss[0m : 2.50263
[1mStep[0m  [8/42], [94mLoss[0m : 2.57527
[1mStep[0m  [12/42], [94mLoss[0m : 2.53657
[1mStep[0m  [16/42], [94mLoss[0m : 2.62120
[1mStep[0m  [20/42], [94mLoss[0m : 2.52777
[1mStep[0m  [24/42], [94mLoss[0m : 2.74230
[1mStep[0m  [28/42], [94mLoss[0m : 2.68758
[1mStep[0m  [32/42], [94mLoss[0m : 2.53097
[1mStep[0m  [36/42], [94mLoss[0m : 2.62183
[1mStep[0m  [40/42], [94mLoss[0m : 2.56473

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.652, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78714
[1mStep[0m  [4/42], [94mLoss[0m : 2.71057
[1mStep[0m  [8/42], [94mLoss[0m : 2.57590
[1mStep[0m  [12/42], [94mLoss[0m : 2.53789
[1mStep[0m  [16/42], [94mLoss[0m : 2.49869
[1mStep[0m  [20/42], [94mLoss[0m : 2.49119
[1mStep[0m  [24/42], [94mLoss[0m : 2.43524
[1mStep[0m  [28/42], [94mLoss[0m : 2.63225
[1mStep[0m  [32/42], [94mLoss[0m : 2.46125
[1mStep[0m  [36/42], [94mLoss[0m : 2.64367
[1mStep[0m  [40/42], [94mLoss[0m : 2.66603

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.626, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56323
[1mStep[0m  [4/42], [94mLoss[0m : 2.49945
[1mStep[0m  [8/42], [94mLoss[0m : 2.52585
[1mStep[0m  [12/42], [94mLoss[0m : 2.61202
[1mStep[0m  [16/42], [94mLoss[0m : 2.50412
[1mStep[0m  [20/42], [94mLoss[0m : 2.50820
[1mStep[0m  [24/42], [94mLoss[0m : 2.52054
[1mStep[0m  [28/42], [94mLoss[0m : 2.63441
[1mStep[0m  [32/42], [94mLoss[0m : 2.59189
[1mStep[0m  [36/42], [94mLoss[0m : 2.42572
[1mStep[0m  [40/42], [94mLoss[0m : 2.59177

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48363
[1mStep[0m  [4/42], [94mLoss[0m : 2.37373
[1mStep[0m  [8/42], [94mLoss[0m : 2.67606
[1mStep[0m  [12/42], [94mLoss[0m : 2.54186
[1mStep[0m  [16/42], [94mLoss[0m : 2.52392
[1mStep[0m  [20/42], [94mLoss[0m : 2.72311
[1mStep[0m  [24/42], [94mLoss[0m : 2.72454
[1mStep[0m  [28/42], [94mLoss[0m : 2.44033
[1mStep[0m  [32/42], [94mLoss[0m : 2.44857
[1mStep[0m  [36/42], [94mLoss[0m : 2.60986
[1mStep[0m  [40/42], [94mLoss[0m : 2.46023

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60449
[1mStep[0m  [4/42], [94mLoss[0m : 2.59219
[1mStep[0m  [8/42], [94mLoss[0m : 2.65657
[1mStep[0m  [12/42], [94mLoss[0m : 2.73078
[1mStep[0m  [16/42], [94mLoss[0m : 2.49403
[1mStep[0m  [20/42], [94mLoss[0m : 2.54960
[1mStep[0m  [24/42], [94mLoss[0m : 2.77112
[1mStep[0m  [28/42], [94mLoss[0m : 2.53391
[1mStep[0m  [32/42], [94mLoss[0m : 2.52190
[1mStep[0m  [36/42], [94mLoss[0m : 2.48624
[1mStep[0m  [40/42], [94mLoss[0m : 2.70266

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73145
[1mStep[0m  [4/42], [94mLoss[0m : 2.74394
[1mStep[0m  [8/42], [94mLoss[0m : 2.38862
[1mStep[0m  [12/42], [94mLoss[0m : 2.36386
[1mStep[0m  [16/42], [94mLoss[0m : 2.55939
[1mStep[0m  [20/42], [94mLoss[0m : 2.61733
[1mStep[0m  [24/42], [94mLoss[0m : 2.54224
[1mStep[0m  [28/42], [94mLoss[0m : 2.51820
[1mStep[0m  [32/42], [94mLoss[0m : 2.39276
[1mStep[0m  [36/42], [94mLoss[0m : 2.76006
[1mStep[0m  [40/42], [94mLoss[0m : 2.70714

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.576, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65740
[1mStep[0m  [4/42], [94mLoss[0m : 2.52480
[1mStep[0m  [8/42], [94mLoss[0m : 2.42036
[1mStep[0m  [12/42], [94mLoss[0m : 2.53797
[1mStep[0m  [16/42], [94mLoss[0m : 2.54535
[1mStep[0m  [20/42], [94mLoss[0m : 2.46577
[1mStep[0m  [24/42], [94mLoss[0m : 2.68959
[1mStep[0m  [28/42], [94mLoss[0m : 2.70736
[1mStep[0m  [32/42], [94mLoss[0m : 2.55994
[1mStep[0m  [36/42], [94mLoss[0m : 2.53941
[1mStep[0m  [40/42], [94mLoss[0m : 2.46487

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59475
[1mStep[0m  [4/42], [94mLoss[0m : 2.56435
[1mStep[0m  [8/42], [94mLoss[0m : 2.72153
[1mStep[0m  [12/42], [94mLoss[0m : 2.68472
[1mStep[0m  [16/42], [94mLoss[0m : 2.59883
[1mStep[0m  [20/42], [94mLoss[0m : 2.41016
[1mStep[0m  [24/42], [94mLoss[0m : 2.55672
[1mStep[0m  [28/42], [94mLoss[0m : 2.20282
[1mStep[0m  [32/42], [94mLoss[0m : 2.45911
[1mStep[0m  [36/42], [94mLoss[0m : 2.68894
[1mStep[0m  [40/42], [94mLoss[0m : 2.53809

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46943
[1mStep[0m  [4/42], [94mLoss[0m : 2.54337
[1mStep[0m  [8/42], [94mLoss[0m : 2.73564
[1mStep[0m  [12/42], [94mLoss[0m : 2.49819
[1mStep[0m  [16/42], [94mLoss[0m : 2.57569
[1mStep[0m  [20/42], [94mLoss[0m : 2.34893
[1mStep[0m  [24/42], [94mLoss[0m : 2.43109
[1mStep[0m  [28/42], [94mLoss[0m : 2.41909
[1mStep[0m  [32/42], [94mLoss[0m : 2.66421
[1mStep[0m  [36/42], [94mLoss[0m : 2.36942
[1mStep[0m  [40/42], [94mLoss[0m : 2.62287

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52371
[1mStep[0m  [4/42], [94mLoss[0m : 2.54035
[1mStep[0m  [8/42], [94mLoss[0m : 2.38918
[1mStep[0m  [12/42], [94mLoss[0m : 2.43950
[1mStep[0m  [16/42], [94mLoss[0m : 2.59626
[1mStep[0m  [20/42], [94mLoss[0m : 2.45154
[1mStep[0m  [24/42], [94mLoss[0m : 2.44933
[1mStep[0m  [28/42], [94mLoss[0m : 2.32633
[1mStep[0m  [32/42], [94mLoss[0m : 2.66245
[1mStep[0m  [36/42], [94mLoss[0m : 2.65655
[1mStep[0m  [40/42], [94mLoss[0m : 2.54637

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59645
[1mStep[0m  [4/42], [94mLoss[0m : 2.63030
[1mStep[0m  [8/42], [94mLoss[0m : 2.34988
[1mStep[0m  [12/42], [94mLoss[0m : 2.52669
[1mStep[0m  [16/42], [94mLoss[0m : 2.43508
[1mStep[0m  [20/42], [94mLoss[0m : 2.38531
[1mStep[0m  [24/42], [94mLoss[0m : 2.48875
[1mStep[0m  [28/42], [94mLoss[0m : 2.60012
[1mStep[0m  [32/42], [94mLoss[0m : 2.69492
[1mStep[0m  [36/42], [94mLoss[0m : 2.73668
[1mStep[0m  [40/42], [94mLoss[0m : 2.59254

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59853
[1mStep[0m  [4/42], [94mLoss[0m : 2.45321
[1mStep[0m  [8/42], [94mLoss[0m : 2.59321
[1mStep[0m  [12/42], [94mLoss[0m : 2.27428
[1mStep[0m  [16/42], [94mLoss[0m : 2.30336
[1mStep[0m  [20/42], [94mLoss[0m : 2.51385
[1mStep[0m  [24/42], [94mLoss[0m : 2.86681
[1mStep[0m  [28/42], [94mLoss[0m : 2.69804
[1mStep[0m  [32/42], [94mLoss[0m : 2.45501
[1mStep[0m  [36/42], [94mLoss[0m : 2.26738
[1mStep[0m  [40/42], [94mLoss[0m : 2.38724

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45018
[1mStep[0m  [4/42], [94mLoss[0m : 2.44507
[1mStep[0m  [8/42], [94mLoss[0m : 2.45305
[1mStep[0m  [12/42], [94mLoss[0m : 2.46627
[1mStep[0m  [16/42], [94mLoss[0m : 2.59765
[1mStep[0m  [20/42], [94mLoss[0m : 2.35615
[1mStep[0m  [24/42], [94mLoss[0m : 2.38508
[1mStep[0m  [28/42], [94mLoss[0m : 2.59523
[1mStep[0m  [32/42], [94mLoss[0m : 2.61451
[1mStep[0m  [36/42], [94mLoss[0m : 2.70123
[1mStep[0m  [40/42], [94mLoss[0m : 2.73321

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68357
[1mStep[0m  [4/42], [94mLoss[0m : 2.46351
[1mStep[0m  [8/42], [94mLoss[0m : 2.54641
[1mStep[0m  [12/42], [94mLoss[0m : 2.37690
[1mStep[0m  [16/42], [94mLoss[0m : 2.56546
[1mStep[0m  [20/42], [94mLoss[0m : 2.49313
[1mStep[0m  [24/42], [94mLoss[0m : 2.59774
[1mStep[0m  [28/42], [94mLoss[0m : 2.30716
[1mStep[0m  [32/42], [94mLoss[0m : 2.55187
[1mStep[0m  [36/42], [94mLoss[0m : 2.43013
[1mStep[0m  [40/42], [94mLoss[0m : 2.45919

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53971
[1mStep[0m  [4/42], [94mLoss[0m : 2.53727
[1mStep[0m  [8/42], [94mLoss[0m : 2.26827
[1mStep[0m  [12/42], [94mLoss[0m : 2.27046
[1mStep[0m  [16/42], [94mLoss[0m : 2.68123
[1mStep[0m  [20/42], [94mLoss[0m : 2.46627
[1mStep[0m  [24/42], [94mLoss[0m : 2.45951
[1mStep[0m  [28/42], [94mLoss[0m : 2.56024
[1mStep[0m  [32/42], [94mLoss[0m : 2.59012
[1mStep[0m  [36/42], [94mLoss[0m : 2.39995
[1mStep[0m  [40/42], [94mLoss[0m : 2.40670

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41233
[1mStep[0m  [4/42], [94mLoss[0m : 2.74714
[1mStep[0m  [8/42], [94mLoss[0m : 2.54904
[1mStep[0m  [12/42], [94mLoss[0m : 2.47987
[1mStep[0m  [16/42], [94mLoss[0m : 2.48030
[1mStep[0m  [20/42], [94mLoss[0m : 2.30766
[1mStep[0m  [24/42], [94mLoss[0m : 2.63420
[1mStep[0m  [28/42], [94mLoss[0m : 2.45437
[1mStep[0m  [32/42], [94mLoss[0m : 2.34295
[1mStep[0m  [36/42], [94mLoss[0m : 2.63372
[1mStep[0m  [40/42], [94mLoss[0m : 2.66853

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38602
[1mStep[0m  [4/42], [94mLoss[0m : 2.49340
[1mStep[0m  [8/42], [94mLoss[0m : 2.61186
[1mStep[0m  [12/42], [94mLoss[0m : 2.57929
[1mStep[0m  [16/42], [94mLoss[0m : 2.45265
[1mStep[0m  [20/42], [94mLoss[0m : 2.57813
[1mStep[0m  [24/42], [94mLoss[0m : 2.55996
[1mStep[0m  [28/42], [94mLoss[0m : 2.38618
[1mStep[0m  [32/42], [94mLoss[0m : 2.39378
[1mStep[0m  [36/42], [94mLoss[0m : 2.52768
[1mStep[0m  [40/42], [94mLoss[0m : 2.48637

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70859
[1mStep[0m  [4/42], [94mLoss[0m : 2.49440
[1mStep[0m  [8/42], [94mLoss[0m : 2.43049
[1mStep[0m  [12/42], [94mLoss[0m : 2.42658
[1mStep[0m  [16/42], [94mLoss[0m : 2.53690
[1mStep[0m  [20/42], [94mLoss[0m : 2.42010
[1mStep[0m  [24/42], [94mLoss[0m : 2.40847
[1mStep[0m  [28/42], [94mLoss[0m : 2.31820
[1mStep[0m  [32/42], [94mLoss[0m : 2.61531
[1mStep[0m  [36/42], [94mLoss[0m : 2.49080
[1mStep[0m  [40/42], [94mLoss[0m : 2.57796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49435
[1mStep[0m  [4/42], [94mLoss[0m : 2.43381
[1mStep[0m  [8/42], [94mLoss[0m : 2.55918
[1mStep[0m  [12/42], [94mLoss[0m : 2.54005
[1mStep[0m  [16/42], [94mLoss[0m : 2.36959
[1mStep[0m  [20/42], [94mLoss[0m : 2.35946
[1mStep[0m  [24/42], [94mLoss[0m : 2.51997
[1mStep[0m  [28/42], [94mLoss[0m : 2.38200
[1mStep[0m  [32/42], [94mLoss[0m : 2.59958
[1mStep[0m  [36/42], [94mLoss[0m : 2.42898
[1mStep[0m  [40/42], [94mLoss[0m : 2.24111

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57969
[1mStep[0m  [4/42], [94mLoss[0m : 2.47859
[1mStep[0m  [8/42], [94mLoss[0m : 2.48422
[1mStep[0m  [12/42], [94mLoss[0m : 2.56263
[1mStep[0m  [16/42], [94mLoss[0m : 2.41155
[1mStep[0m  [20/42], [94mLoss[0m : 2.53654
[1mStep[0m  [24/42], [94mLoss[0m : 2.55139
[1mStep[0m  [28/42], [94mLoss[0m : 2.70953
[1mStep[0m  [32/42], [94mLoss[0m : 2.59699
[1mStep[0m  [36/42], [94mLoss[0m : 2.43657
[1mStep[0m  [40/42], [94mLoss[0m : 2.59498

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32410
[1mStep[0m  [4/42], [94mLoss[0m : 2.28606
[1mStep[0m  [8/42], [94mLoss[0m : 2.47241
[1mStep[0m  [12/42], [94mLoss[0m : 2.37322
[1mStep[0m  [16/42], [94mLoss[0m : 2.58937
[1mStep[0m  [20/42], [94mLoss[0m : 2.46860
[1mStep[0m  [24/42], [94mLoss[0m : 2.35085
[1mStep[0m  [28/42], [94mLoss[0m : 2.42087
[1mStep[0m  [32/42], [94mLoss[0m : 2.64368
[1mStep[0m  [36/42], [94mLoss[0m : 2.56221
[1mStep[0m  [40/42], [94mLoss[0m : 2.84556

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39888
[1mStep[0m  [4/42], [94mLoss[0m : 2.50432
[1mStep[0m  [8/42], [94mLoss[0m : 2.69543
[1mStep[0m  [12/42], [94mLoss[0m : 2.45324
[1mStep[0m  [16/42], [94mLoss[0m : 2.42221
[1mStep[0m  [20/42], [94mLoss[0m : 2.58527
[1mStep[0m  [24/42], [94mLoss[0m : 2.47891
[1mStep[0m  [28/42], [94mLoss[0m : 2.37937
[1mStep[0m  [32/42], [94mLoss[0m : 2.55769
[1mStep[0m  [36/42], [94mLoss[0m : 2.37957
[1mStep[0m  [40/42], [94mLoss[0m : 2.66241

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53827
[1mStep[0m  [4/42], [94mLoss[0m : 2.38704
[1mStep[0m  [8/42], [94mLoss[0m : 2.47579
[1mStep[0m  [12/42], [94mLoss[0m : 2.73209
[1mStep[0m  [16/42], [94mLoss[0m : 2.51890
[1mStep[0m  [20/42], [94mLoss[0m : 2.43145
[1mStep[0m  [24/42], [94mLoss[0m : 2.56827
[1mStep[0m  [28/42], [94mLoss[0m : 2.60842
[1mStep[0m  [32/42], [94mLoss[0m : 2.21300
[1mStep[0m  [36/42], [94mLoss[0m : 2.58949
[1mStep[0m  [40/42], [94mLoss[0m : 2.55957

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50357
[1mStep[0m  [4/42], [94mLoss[0m : 2.51453
[1mStep[0m  [8/42], [94mLoss[0m : 2.47452
[1mStep[0m  [12/42], [94mLoss[0m : 2.38636
[1mStep[0m  [16/42], [94mLoss[0m : 2.57881
[1mStep[0m  [20/42], [94mLoss[0m : 2.58480
[1mStep[0m  [24/42], [94mLoss[0m : 2.37552
[1mStep[0m  [28/42], [94mLoss[0m : 2.55978
[1mStep[0m  [32/42], [94mLoss[0m : 2.51840
[1mStep[0m  [36/42], [94mLoss[0m : 2.37464
[1mStep[0m  [40/42], [94mLoss[0m : 2.35115

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.375
====================================

Phase 1 - Evaluation MAE:  2.375029410634722
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.40149
[1mStep[0m  [4/42], [94mLoss[0m : 2.55541
[1mStep[0m  [8/42], [94mLoss[0m : 2.44544
[1mStep[0m  [12/42], [94mLoss[0m : 2.66143
[1mStep[0m  [16/42], [94mLoss[0m : 2.63794
[1mStep[0m  [20/42], [94mLoss[0m : 2.54350
[1mStep[0m  [24/42], [94mLoss[0m : 2.53768
[1mStep[0m  [28/42], [94mLoss[0m : 2.68399
[1mStep[0m  [32/42], [94mLoss[0m : 2.80038
[1mStep[0m  [36/42], [94mLoss[0m : 2.60940
[1mStep[0m  [40/42], [94mLoss[0m : 2.50464

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32419
[1mStep[0m  [4/42], [94mLoss[0m : 2.49074
[1mStep[0m  [8/42], [94mLoss[0m : 2.46768
[1mStep[0m  [12/42], [94mLoss[0m : 2.48496
[1mStep[0m  [16/42], [94mLoss[0m : 2.68895
[1mStep[0m  [20/42], [94mLoss[0m : 2.49737
[1mStep[0m  [24/42], [94mLoss[0m : 2.52217
[1mStep[0m  [28/42], [94mLoss[0m : 2.37029
[1mStep[0m  [32/42], [94mLoss[0m : 2.50446
[1mStep[0m  [36/42], [94mLoss[0m : 2.84169
[1mStep[0m  [40/42], [94mLoss[0m : 2.55328

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62072
[1mStep[0m  [4/42], [94mLoss[0m : 2.41986
[1mStep[0m  [8/42], [94mLoss[0m : 2.48935
[1mStep[0m  [12/42], [94mLoss[0m : 2.60990
[1mStep[0m  [16/42], [94mLoss[0m : 2.45738
[1mStep[0m  [20/42], [94mLoss[0m : 2.59233
[1mStep[0m  [24/42], [94mLoss[0m : 2.66237
[1mStep[0m  [28/42], [94mLoss[0m : 2.64335
[1mStep[0m  [32/42], [94mLoss[0m : 2.43881
[1mStep[0m  [36/42], [94mLoss[0m : 2.58883
[1mStep[0m  [40/42], [94mLoss[0m : 2.44382

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51664
[1mStep[0m  [4/42], [94mLoss[0m : 2.39387
[1mStep[0m  [8/42], [94mLoss[0m : 2.41686
[1mStep[0m  [12/42], [94mLoss[0m : 2.66070
[1mStep[0m  [16/42], [94mLoss[0m : 2.46687
[1mStep[0m  [20/42], [94mLoss[0m : 2.31476
[1mStep[0m  [24/42], [94mLoss[0m : 2.77455
[1mStep[0m  [28/42], [94mLoss[0m : 2.40135
[1mStep[0m  [32/42], [94mLoss[0m : 2.21627
[1mStep[0m  [36/42], [94mLoss[0m : 2.53841
[1mStep[0m  [40/42], [94mLoss[0m : 2.47794

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26221
[1mStep[0m  [4/42], [94mLoss[0m : 2.57279
[1mStep[0m  [8/42], [94mLoss[0m : 2.48115
[1mStep[0m  [12/42], [94mLoss[0m : 2.10063
[1mStep[0m  [16/42], [94mLoss[0m : 2.30260
[1mStep[0m  [20/42], [94mLoss[0m : 2.29002
[1mStep[0m  [24/42], [94mLoss[0m : 2.57158
[1mStep[0m  [28/42], [94mLoss[0m : 2.44121
[1mStep[0m  [32/42], [94mLoss[0m : 2.40870
[1mStep[0m  [36/42], [94mLoss[0m : 2.61625
[1mStep[0m  [40/42], [94mLoss[0m : 2.42132

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43506
[1mStep[0m  [4/42], [94mLoss[0m : 2.35208
[1mStep[0m  [8/42], [94mLoss[0m : 2.39725
[1mStep[0m  [12/42], [94mLoss[0m : 2.26249
[1mStep[0m  [16/42], [94mLoss[0m : 2.12028
[1mStep[0m  [20/42], [94mLoss[0m : 2.27551
[1mStep[0m  [24/42], [94mLoss[0m : 2.23778
[1mStep[0m  [28/42], [94mLoss[0m : 2.27822
[1mStep[0m  [32/42], [94mLoss[0m : 2.51832
[1mStep[0m  [36/42], [94mLoss[0m : 2.24713
[1mStep[0m  [40/42], [94mLoss[0m : 2.38816

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22063
[1mStep[0m  [4/42], [94mLoss[0m : 2.35643
[1mStep[0m  [8/42], [94mLoss[0m : 2.46253
[1mStep[0m  [12/42], [94mLoss[0m : 2.38168
[1mStep[0m  [16/42], [94mLoss[0m : 2.61351
[1mStep[0m  [20/42], [94mLoss[0m : 2.38746
[1mStep[0m  [24/42], [94mLoss[0m : 2.41104
[1mStep[0m  [28/42], [94mLoss[0m : 2.31904
[1mStep[0m  [32/42], [94mLoss[0m : 2.54363
[1mStep[0m  [36/42], [94mLoss[0m : 2.42091
[1mStep[0m  [40/42], [94mLoss[0m : 2.32807

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36380
[1mStep[0m  [4/42], [94mLoss[0m : 2.18839
[1mStep[0m  [8/42], [94mLoss[0m : 2.41983
[1mStep[0m  [12/42], [94mLoss[0m : 2.14686
[1mStep[0m  [16/42], [94mLoss[0m : 2.10770
[1mStep[0m  [20/42], [94mLoss[0m : 2.40726
[1mStep[0m  [24/42], [94mLoss[0m : 2.29892
[1mStep[0m  [28/42], [94mLoss[0m : 2.37589
[1mStep[0m  [32/42], [94mLoss[0m : 2.29836
[1mStep[0m  [36/42], [94mLoss[0m : 2.41892
[1mStep[0m  [40/42], [94mLoss[0m : 2.66926

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36542
[1mStep[0m  [4/42], [94mLoss[0m : 2.41696
[1mStep[0m  [8/42], [94mLoss[0m : 2.18232
[1mStep[0m  [12/42], [94mLoss[0m : 2.29773
[1mStep[0m  [16/42], [94mLoss[0m : 2.19412
[1mStep[0m  [20/42], [94mLoss[0m : 2.22020
[1mStep[0m  [24/42], [94mLoss[0m : 2.40360
[1mStep[0m  [28/42], [94mLoss[0m : 2.42554
[1mStep[0m  [32/42], [94mLoss[0m : 2.34463
[1mStep[0m  [36/42], [94mLoss[0m : 2.49509
[1mStep[0m  [40/42], [94mLoss[0m : 2.27118

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16335
[1mStep[0m  [4/42], [94mLoss[0m : 2.25162
[1mStep[0m  [8/42], [94mLoss[0m : 2.29894
[1mStep[0m  [12/42], [94mLoss[0m : 2.22307
[1mStep[0m  [16/42], [94mLoss[0m : 2.33681
[1mStep[0m  [20/42], [94mLoss[0m : 2.10859
[1mStep[0m  [24/42], [94mLoss[0m : 2.63892
[1mStep[0m  [28/42], [94mLoss[0m : 2.25892
[1mStep[0m  [32/42], [94mLoss[0m : 2.35480
[1mStep[0m  [36/42], [94mLoss[0m : 2.27679
[1mStep[0m  [40/42], [94mLoss[0m : 2.26393

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19498
[1mStep[0m  [4/42], [94mLoss[0m : 2.20967
[1mStep[0m  [8/42], [94mLoss[0m : 2.22974
[1mStep[0m  [12/42], [94mLoss[0m : 2.10262
[1mStep[0m  [16/42], [94mLoss[0m : 2.28230
[1mStep[0m  [20/42], [94mLoss[0m : 2.09730
[1mStep[0m  [24/42], [94mLoss[0m : 2.59581
[1mStep[0m  [28/42], [94mLoss[0m : 2.30368
[1mStep[0m  [32/42], [94mLoss[0m : 2.20444
[1mStep[0m  [36/42], [94mLoss[0m : 2.15539
[1mStep[0m  [40/42], [94mLoss[0m : 2.33636

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41360
[1mStep[0m  [4/42], [94mLoss[0m : 2.08691
[1mStep[0m  [8/42], [94mLoss[0m : 2.20572
[1mStep[0m  [12/42], [94mLoss[0m : 2.24413
[1mStep[0m  [16/42], [94mLoss[0m : 2.25191
[1mStep[0m  [20/42], [94mLoss[0m : 2.14215
[1mStep[0m  [24/42], [94mLoss[0m : 2.27843
[1mStep[0m  [28/42], [94mLoss[0m : 2.15850
[1mStep[0m  [32/42], [94mLoss[0m : 2.13106
[1mStep[0m  [36/42], [94mLoss[0m : 2.40708
[1mStep[0m  [40/42], [94mLoss[0m : 2.43897

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05298
[1mStep[0m  [4/42], [94mLoss[0m : 2.01835
[1mStep[0m  [8/42], [94mLoss[0m : 2.26326
[1mStep[0m  [12/42], [94mLoss[0m : 2.20938
[1mStep[0m  [16/42], [94mLoss[0m : 2.10999
[1mStep[0m  [20/42], [94mLoss[0m : 2.15147
[1mStep[0m  [24/42], [94mLoss[0m : 2.26263
[1mStep[0m  [28/42], [94mLoss[0m : 2.19834
[1mStep[0m  [32/42], [94mLoss[0m : 2.13115
[1mStep[0m  [36/42], [94mLoss[0m : 2.34399
[1mStep[0m  [40/42], [94mLoss[0m : 2.12370

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24189
[1mStep[0m  [4/42], [94mLoss[0m : 2.04373
[1mStep[0m  [8/42], [94mLoss[0m : 2.18426
[1mStep[0m  [12/42], [94mLoss[0m : 2.13451
[1mStep[0m  [16/42], [94mLoss[0m : 2.38004
[1mStep[0m  [20/42], [94mLoss[0m : 2.08220
[1mStep[0m  [24/42], [94mLoss[0m : 2.33211
[1mStep[0m  [28/42], [94mLoss[0m : 2.26265
[1mStep[0m  [32/42], [94mLoss[0m : 2.05957
[1mStep[0m  [36/42], [94mLoss[0m : 2.06728
[1mStep[0m  [40/42], [94mLoss[0m : 2.14643

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.518, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13291
[1mStep[0m  [4/42], [94mLoss[0m : 2.08361
[1mStep[0m  [8/42], [94mLoss[0m : 2.20300
[1mStep[0m  [12/42], [94mLoss[0m : 1.99174
[1mStep[0m  [16/42], [94mLoss[0m : 2.14632
[1mStep[0m  [20/42], [94mLoss[0m : 2.31645
[1mStep[0m  [24/42], [94mLoss[0m : 2.31562
[1mStep[0m  [28/42], [94mLoss[0m : 2.16050
[1mStep[0m  [32/42], [94mLoss[0m : 2.22827
[1mStep[0m  [36/42], [94mLoss[0m : 2.25027
[1mStep[0m  [40/42], [94mLoss[0m : 1.97250

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93440
[1mStep[0m  [4/42], [94mLoss[0m : 2.29776
[1mStep[0m  [8/42], [94mLoss[0m : 2.23924
[1mStep[0m  [12/42], [94mLoss[0m : 2.06533
[1mStep[0m  [16/42], [94mLoss[0m : 2.07340
[1mStep[0m  [20/42], [94mLoss[0m : 2.06211
[1mStep[0m  [24/42], [94mLoss[0m : 2.20069
[1mStep[0m  [28/42], [94mLoss[0m : 1.90205
[1mStep[0m  [32/42], [94mLoss[0m : 2.03728
[1mStep[0m  [36/42], [94mLoss[0m : 2.21989
[1mStep[0m  [40/42], [94mLoss[0m : 2.42308

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23599
[1mStep[0m  [4/42], [94mLoss[0m : 1.81452
[1mStep[0m  [8/42], [94mLoss[0m : 2.16192
[1mStep[0m  [12/42], [94mLoss[0m : 2.10263
[1mStep[0m  [16/42], [94mLoss[0m : 2.15115
[1mStep[0m  [20/42], [94mLoss[0m : 2.03766
[1mStep[0m  [24/42], [94mLoss[0m : 2.03036
[1mStep[0m  [28/42], [94mLoss[0m : 2.23613
[1mStep[0m  [32/42], [94mLoss[0m : 2.07746
[1mStep[0m  [36/42], [94mLoss[0m : 2.12329
[1mStep[0m  [40/42], [94mLoss[0m : 1.97324

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83488
[1mStep[0m  [4/42], [94mLoss[0m : 2.10978
[1mStep[0m  [8/42], [94mLoss[0m : 1.91471
[1mStep[0m  [12/42], [94mLoss[0m : 1.99340
[1mStep[0m  [16/42], [94mLoss[0m : 2.30925
[1mStep[0m  [20/42], [94mLoss[0m : 2.09012
[1mStep[0m  [24/42], [94mLoss[0m : 2.13496
[1mStep[0m  [28/42], [94mLoss[0m : 2.02683
[1mStep[0m  [32/42], [94mLoss[0m : 1.96510
[1mStep[0m  [36/42], [94mLoss[0m : 2.09914
[1mStep[0m  [40/42], [94mLoss[0m : 1.88737

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98643
[1mStep[0m  [4/42], [94mLoss[0m : 1.95851
[1mStep[0m  [8/42], [94mLoss[0m : 1.81927
[1mStep[0m  [12/42], [94mLoss[0m : 2.03404
[1mStep[0m  [16/42], [94mLoss[0m : 2.11876
[1mStep[0m  [20/42], [94mLoss[0m : 1.90073
[1mStep[0m  [24/42], [94mLoss[0m : 2.02310
[1mStep[0m  [28/42], [94mLoss[0m : 1.86809
[1mStep[0m  [32/42], [94mLoss[0m : 1.96730
[1mStep[0m  [36/42], [94mLoss[0m : 1.99327
[1mStep[0m  [40/42], [94mLoss[0m : 2.14198

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83197
[1mStep[0m  [4/42], [94mLoss[0m : 1.82781
[1mStep[0m  [8/42], [94mLoss[0m : 2.00271
[1mStep[0m  [12/42], [94mLoss[0m : 2.22187
[1mStep[0m  [16/42], [94mLoss[0m : 1.84086
[1mStep[0m  [20/42], [94mLoss[0m : 1.98012
[1mStep[0m  [24/42], [94mLoss[0m : 1.73419
[1mStep[0m  [28/42], [94mLoss[0m : 2.04418
[1mStep[0m  [32/42], [94mLoss[0m : 1.97029
[1mStep[0m  [36/42], [94mLoss[0m : 1.92084
[1mStep[0m  [40/42], [94mLoss[0m : 2.03373

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.511, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.80970
[1mStep[0m  [4/42], [94mLoss[0m : 1.95906
[1mStep[0m  [8/42], [94mLoss[0m : 1.82911
[1mStep[0m  [12/42], [94mLoss[0m : 1.83604
[1mStep[0m  [16/42], [94mLoss[0m : 1.89027
[1mStep[0m  [20/42], [94mLoss[0m : 2.05452
[1mStep[0m  [24/42], [94mLoss[0m : 2.00319
[1mStep[0m  [28/42], [94mLoss[0m : 1.99909
[1mStep[0m  [32/42], [94mLoss[0m : 2.09506
[1mStep[0m  [36/42], [94mLoss[0m : 2.15760
[1mStep[0m  [40/42], [94mLoss[0m : 1.88898

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83484
[1mStep[0m  [4/42], [94mLoss[0m : 1.77647
[1mStep[0m  [8/42], [94mLoss[0m : 1.91553
[1mStep[0m  [12/42], [94mLoss[0m : 1.86319
[1mStep[0m  [16/42], [94mLoss[0m : 2.00269
[1mStep[0m  [20/42], [94mLoss[0m : 2.08585
[1mStep[0m  [24/42], [94mLoss[0m : 1.95537
[1mStep[0m  [28/42], [94mLoss[0m : 1.99650
[1mStep[0m  [32/42], [94mLoss[0m : 1.92002
[1mStep[0m  [36/42], [94mLoss[0m : 1.96804
[1mStep[0m  [40/42], [94mLoss[0m : 1.97273

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72126
[1mStep[0m  [4/42], [94mLoss[0m : 1.84768
[1mStep[0m  [8/42], [94mLoss[0m : 1.92971
[1mStep[0m  [12/42], [94mLoss[0m : 1.88113
[1mStep[0m  [16/42], [94mLoss[0m : 1.80404
[1mStep[0m  [20/42], [94mLoss[0m : 1.74305
[1mStep[0m  [24/42], [94mLoss[0m : 1.68481
[1mStep[0m  [28/42], [94mLoss[0m : 1.83851
[1mStep[0m  [32/42], [94mLoss[0m : 2.00855
[1mStep[0m  [36/42], [94mLoss[0m : 1.89905
[1mStep[0m  [40/42], [94mLoss[0m : 1.85080

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.629, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02611
[1mStep[0m  [4/42], [94mLoss[0m : 1.83579
[1mStep[0m  [8/42], [94mLoss[0m : 1.67705
[1mStep[0m  [12/42], [94mLoss[0m : 1.94592
[1mStep[0m  [16/42], [94mLoss[0m : 1.75915
[1mStep[0m  [20/42], [94mLoss[0m : 1.89158
[1mStep[0m  [24/42], [94mLoss[0m : 1.79541
[1mStep[0m  [28/42], [94mLoss[0m : 1.80963
[1mStep[0m  [32/42], [94mLoss[0m : 1.77881
[1mStep[0m  [36/42], [94mLoss[0m : 1.98866
[1mStep[0m  [40/42], [94mLoss[0m : 2.00977

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.585, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00560
[1mStep[0m  [4/42], [94mLoss[0m : 1.79469
[1mStep[0m  [8/42], [94mLoss[0m : 1.83270
[1mStep[0m  [12/42], [94mLoss[0m : 1.89336
[1mStep[0m  [16/42], [94mLoss[0m : 1.74888
[1mStep[0m  [20/42], [94mLoss[0m : 1.81009
[1mStep[0m  [24/42], [94mLoss[0m : 1.84456
[1mStep[0m  [28/42], [94mLoss[0m : 1.95208
[1mStep[0m  [32/42], [94mLoss[0m : 1.82119
[1mStep[0m  [36/42], [94mLoss[0m : 1.93198
[1mStep[0m  [40/42], [94mLoss[0m : 1.70632

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.585, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83799
[1mStep[0m  [4/42], [94mLoss[0m : 1.73177
[1mStep[0m  [8/42], [94mLoss[0m : 1.63094
[1mStep[0m  [12/42], [94mLoss[0m : 1.84723
[1mStep[0m  [16/42], [94mLoss[0m : 1.93435
[1mStep[0m  [20/42], [94mLoss[0m : 1.88720
[1mStep[0m  [24/42], [94mLoss[0m : 1.81236
[1mStep[0m  [28/42], [94mLoss[0m : 1.86583
[1mStep[0m  [32/42], [94mLoss[0m : 1.83062
[1mStep[0m  [36/42], [94mLoss[0m : 2.03754
[1mStep[0m  [40/42], [94mLoss[0m : 1.76372

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.572, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81250
[1mStep[0m  [4/42], [94mLoss[0m : 1.87645
[1mStep[0m  [8/42], [94mLoss[0m : 1.73945
[1mStep[0m  [12/42], [94mLoss[0m : 1.78817
[1mStep[0m  [16/42], [94mLoss[0m : 1.73048
[1mStep[0m  [20/42], [94mLoss[0m : 1.68377
[1mStep[0m  [24/42], [94mLoss[0m : 1.87897
[1mStep[0m  [28/42], [94mLoss[0m : 1.81033
[1mStep[0m  [32/42], [94mLoss[0m : 1.73025
[1mStep[0m  [36/42], [94mLoss[0m : 1.99229
[1mStep[0m  [40/42], [94mLoss[0m : 1.75480

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65712
[1mStep[0m  [4/42], [94mLoss[0m : 1.74683
[1mStep[0m  [8/42], [94mLoss[0m : 1.79382
[1mStep[0m  [12/42], [94mLoss[0m : 1.79188
[1mStep[0m  [16/42], [94mLoss[0m : 1.79303
[1mStep[0m  [20/42], [94mLoss[0m : 1.77311
[1mStep[0m  [24/42], [94mLoss[0m : 1.69684
[1mStep[0m  [28/42], [94mLoss[0m : 1.91516
[1mStep[0m  [32/42], [94mLoss[0m : 1.89366
[1mStep[0m  [36/42], [94mLoss[0m : 1.73935
[1mStep[0m  [40/42], [94mLoss[0m : 1.71244

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71026
[1mStep[0m  [4/42], [94mLoss[0m : 1.79147
[1mStep[0m  [8/42], [94mLoss[0m : 1.83844
[1mStep[0m  [12/42], [94mLoss[0m : 1.77124
[1mStep[0m  [16/42], [94mLoss[0m : 1.79917
[1mStep[0m  [20/42], [94mLoss[0m : 1.88916
[1mStep[0m  [24/42], [94mLoss[0m : 1.88607
[1mStep[0m  [28/42], [94mLoss[0m : 1.86615
[1mStep[0m  [32/42], [94mLoss[0m : 1.86928
[1mStep[0m  [36/42], [94mLoss[0m : 1.66028
[1mStep[0m  [40/42], [94mLoss[0m : 1.74895

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88805
[1mStep[0m  [4/42], [94mLoss[0m : 1.91148
[1mStep[0m  [8/42], [94mLoss[0m : 1.66747
[1mStep[0m  [12/42], [94mLoss[0m : 1.73781
[1mStep[0m  [16/42], [94mLoss[0m : 1.55701
[1mStep[0m  [20/42], [94mLoss[0m : 1.76600
[1mStep[0m  [24/42], [94mLoss[0m : 1.84288
[1mStep[0m  [28/42], [94mLoss[0m : 1.78258
[1mStep[0m  [32/42], [94mLoss[0m : 1.67849
[1mStep[0m  [36/42], [94mLoss[0m : 1.60352
[1mStep[0m  [40/42], [94mLoss[0m : 1.56552

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.552, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.541
====================================

Phase 2 - Evaluation MAE:  2.5405705315726146
MAE score P1        2.375029
MAE score P2        2.540571
loss                1.738796
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay           0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.13750
[1mStep[0m  [4/42], [94mLoss[0m : 9.42482
[1mStep[0m  [8/42], [94mLoss[0m : 7.56975
[1mStep[0m  [12/42], [94mLoss[0m : 6.37086
[1mStep[0m  [16/42], [94mLoss[0m : 4.59073
[1mStep[0m  [20/42], [94mLoss[0m : 3.66445
[1mStep[0m  [24/42], [94mLoss[0m : 3.09382
[1mStep[0m  [28/42], [94mLoss[0m : 3.01378
[1mStep[0m  [32/42], [94mLoss[0m : 2.75197
[1mStep[0m  [36/42], [94mLoss[0m : 2.89405
[1mStep[0m  [40/42], [94mLoss[0m : 2.80216

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.996, [92mTest[0m: 11.064, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.86720
[1mStep[0m  [4/42], [94mLoss[0m : 2.72826
[1mStep[0m  [8/42], [94mLoss[0m : 2.80170
[1mStep[0m  [12/42], [94mLoss[0m : 2.65968
[1mStep[0m  [16/42], [94mLoss[0m : 2.60577
[1mStep[0m  [20/42], [94mLoss[0m : 2.71394
[1mStep[0m  [24/42], [94mLoss[0m : 2.58762
[1mStep[0m  [28/42], [94mLoss[0m : 2.70648
[1mStep[0m  [32/42], [94mLoss[0m : 2.43164
[1mStep[0m  [36/42], [94mLoss[0m : 2.60307
[1mStep[0m  [40/42], [94mLoss[0m : 2.69639

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.756, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58554
[1mStep[0m  [4/42], [94mLoss[0m : 2.63054
[1mStep[0m  [8/42], [94mLoss[0m : 2.50201
[1mStep[0m  [12/42], [94mLoss[0m : 2.44981
[1mStep[0m  [16/42], [94mLoss[0m : 2.50140
[1mStep[0m  [20/42], [94mLoss[0m : 2.67950
[1mStep[0m  [24/42], [94mLoss[0m : 2.47455
[1mStep[0m  [28/42], [94mLoss[0m : 2.59979
[1mStep[0m  [32/42], [94mLoss[0m : 2.66556
[1mStep[0m  [36/42], [94mLoss[0m : 2.66575
[1mStep[0m  [40/42], [94mLoss[0m : 2.40336

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55200
[1mStep[0m  [4/42], [94mLoss[0m : 2.49977
[1mStep[0m  [8/42], [94mLoss[0m : 2.64044
[1mStep[0m  [12/42], [94mLoss[0m : 2.67537
[1mStep[0m  [16/42], [94mLoss[0m : 2.54528
[1mStep[0m  [20/42], [94mLoss[0m : 2.51015
[1mStep[0m  [24/42], [94mLoss[0m : 2.79322
[1mStep[0m  [28/42], [94mLoss[0m : 2.60425
[1mStep[0m  [32/42], [94mLoss[0m : 2.35129
[1mStep[0m  [36/42], [94mLoss[0m : 2.51609
[1mStep[0m  [40/42], [94mLoss[0m : 2.61932

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56039
[1mStep[0m  [4/42], [94mLoss[0m : 2.70852
[1mStep[0m  [8/42], [94mLoss[0m : 2.46714
[1mStep[0m  [12/42], [94mLoss[0m : 2.48687
[1mStep[0m  [16/42], [94mLoss[0m : 2.60339
[1mStep[0m  [20/42], [94mLoss[0m : 2.33763
[1mStep[0m  [24/42], [94mLoss[0m : 2.36823
[1mStep[0m  [28/42], [94mLoss[0m : 2.27245
[1mStep[0m  [32/42], [94mLoss[0m : 2.40603
[1mStep[0m  [36/42], [94mLoss[0m : 2.45972
[1mStep[0m  [40/42], [94mLoss[0m : 2.62607

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56625
[1mStep[0m  [4/42], [94mLoss[0m : 2.48512
[1mStep[0m  [8/42], [94mLoss[0m : 2.55630
[1mStep[0m  [12/42], [94mLoss[0m : 2.57067
[1mStep[0m  [16/42], [94mLoss[0m : 2.56524
[1mStep[0m  [20/42], [94mLoss[0m : 2.43693
[1mStep[0m  [24/42], [94mLoss[0m : 2.33001
[1mStep[0m  [28/42], [94mLoss[0m : 2.30246
[1mStep[0m  [32/42], [94mLoss[0m : 2.50607
[1mStep[0m  [36/42], [94mLoss[0m : 2.44850
[1mStep[0m  [40/42], [94mLoss[0m : 2.49441

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66228
[1mStep[0m  [4/42], [94mLoss[0m : 2.36621
[1mStep[0m  [8/42], [94mLoss[0m : 2.51877
[1mStep[0m  [12/42], [94mLoss[0m : 2.64813
[1mStep[0m  [16/42], [94mLoss[0m : 2.69958
[1mStep[0m  [20/42], [94mLoss[0m : 2.40072
[1mStep[0m  [24/42], [94mLoss[0m : 2.31174
[1mStep[0m  [28/42], [94mLoss[0m : 2.49325
[1mStep[0m  [32/42], [94mLoss[0m : 2.46672
[1mStep[0m  [36/42], [94mLoss[0m : 2.68719
[1mStep[0m  [40/42], [94mLoss[0m : 2.48903

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66506
[1mStep[0m  [4/42], [94mLoss[0m : 2.46402
[1mStep[0m  [8/42], [94mLoss[0m : 2.58675
[1mStep[0m  [12/42], [94mLoss[0m : 2.55524
[1mStep[0m  [16/42], [94mLoss[0m : 2.50485
[1mStep[0m  [20/42], [94mLoss[0m : 2.28487
[1mStep[0m  [24/42], [94mLoss[0m : 2.53861
[1mStep[0m  [28/42], [94mLoss[0m : 2.34394
[1mStep[0m  [32/42], [94mLoss[0m : 2.63528
[1mStep[0m  [36/42], [94mLoss[0m : 2.63101
[1mStep[0m  [40/42], [94mLoss[0m : 2.30758

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61004
[1mStep[0m  [4/42], [94mLoss[0m : 2.37861
[1mStep[0m  [8/42], [94mLoss[0m : 2.58788
[1mStep[0m  [12/42], [94mLoss[0m : 2.51705
[1mStep[0m  [16/42], [94mLoss[0m : 2.42601
[1mStep[0m  [20/42], [94mLoss[0m : 2.33274
[1mStep[0m  [24/42], [94mLoss[0m : 2.59968
[1mStep[0m  [28/42], [94mLoss[0m : 2.66807
[1mStep[0m  [32/42], [94mLoss[0m : 2.57368
[1mStep[0m  [36/42], [94mLoss[0m : 2.57841
[1mStep[0m  [40/42], [94mLoss[0m : 2.48429

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42321
[1mStep[0m  [4/42], [94mLoss[0m : 2.66275
[1mStep[0m  [8/42], [94mLoss[0m : 2.66862
[1mStep[0m  [12/42], [94mLoss[0m : 2.62570
[1mStep[0m  [16/42], [94mLoss[0m : 2.53389
[1mStep[0m  [20/42], [94mLoss[0m : 2.37827
[1mStep[0m  [24/42], [94mLoss[0m : 2.37047
[1mStep[0m  [28/42], [94mLoss[0m : 2.41204
[1mStep[0m  [32/42], [94mLoss[0m : 2.36327
[1mStep[0m  [36/42], [94mLoss[0m : 2.59166
[1mStep[0m  [40/42], [94mLoss[0m : 2.61645

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46333
[1mStep[0m  [4/42], [94mLoss[0m : 2.58534
[1mStep[0m  [8/42], [94mLoss[0m : 2.49668
[1mStep[0m  [12/42], [94mLoss[0m : 2.33812
[1mStep[0m  [16/42], [94mLoss[0m : 2.57455
[1mStep[0m  [20/42], [94mLoss[0m : 2.39149
[1mStep[0m  [24/42], [94mLoss[0m : 2.59194
[1mStep[0m  [28/42], [94mLoss[0m : 2.34788
[1mStep[0m  [32/42], [94mLoss[0m : 2.52799
[1mStep[0m  [36/42], [94mLoss[0m : 2.46722
[1mStep[0m  [40/42], [94mLoss[0m : 2.45023

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60273
[1mStep[0m  [4/42], [94mLoss[0m : 2.42663
[1mStep[0m  [8/42], [94mLoss[0m : 2.37585
[1mStep[0m  [12/42], [94mLoss[0m : 2.26652
[1mStep[0m  [16/42], [94mLoss[0m : 2.61916
[1mStep[0m  [20/42], [94mLoss[0m : 2.73301
[1mStep[0m  [24/42], [94mLoss[0m : 2.54152
[1mStep[0m  [28/42], [94mLoss[0m : 2.74739
[1mStep[0m  [32/42], [94mLoss[0m : 2.60908
[1mStep[0m  [36/42], [94mLoss[0m : 2.57772
[1mStep[0m  [40/42], [94mLoss[0m : 2.56404

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26571
[1mStep[0m  [4/42], [94mLoss[0m : 2.49356
[1mStep[0m  [8/42], [94mLoss[0m : 2.55246
[1mStep[0m  [12/42], [94mLoss[0m : 2.57584
[1mStep[0m  [16/42], [94mLoss[0m : 2.68576
[1mStep[0m  [20/42], [94mLoss[0m : 2.44888
[1mStep[0m  [24/42], [94mLoss[0m : 2.33558
[1mStep[0m  [28/42], [94mLoss[0m : 2.58830
[1mStep[0m  [32/42], [94mLoss[0m : 2.40074
[1mStep[0m  [36/42], [94mLoss[0m : 2.58519
[1mStep[0m  [40/42], [94mLoss[0m : 2.44474

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36262
[1mStep[0m  [4/42], [94mLoss[0m : 2.56305
[1mStep[0m  [8/42], [94mLoss[0m : 2.24760
[1mStep[0m  [12/42], [94mLoss[0m : 2.64857
[1mStep[0m  [16/42], [94mLoss[0m : 2.56458
[1mStep[0m  [20/42], [94mLoss[0m : 2.59241
[1mStep[0m  [24/42], [94mLoss[0m : 2.28893
[1mStep[0m  [28/42], [94mLoss[0m : 2.63533
[1mStep[0m  [32/42], [94mLoss[0m : 2.35488
[1mStep[0m  [36/42], [94mLoss[0m : 2.47764
[1mStep[0m  [40/42], [94mLoss[0m : 2.40230

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49997
[1mStep[0m  [4/42], [94mLoss[0m : 2.34576
[1mStep[0m  [8/42], [94mLoss[0m : 2.46867
[1mStep[0m  [12/42], [94mLoss[0m : 2.45918
[1mStep[0m  [16/42], [94mLoss[0m : 2.28971
[1mStep[0m  [20/42], [94mLoss[0m : 2.26832
[1mStep[0m  [24/42], [94mLoss[0m : 2.60007
[1mStep[0m  [28/42], [94mLoss[0m : 2.34967
[1mStep[0m  [32/42], [94mLoss[0m : 2.43431
[1mStep[0m  [36/42], [94mLoss[0m : 2.28441
[1mStep[0m  [40/42], [94mLoss[0m : 2.38689

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65029
[1mStep[0m  [4/42], [94mLoss[0m : 2.40129
[1mStep[0m  [8/42], [94mLoss[0m : 2.34531
[1mStep[0m  [12/42], [94mLoss[0m : 2.42279
[1mStep[0m  [16/42], [94mLoss[0m : 2.46541
[1mStep[0m  [20/42], [94mLoss[0m : 2.52628
[1mStep[0m  [24/42], [94mLoss[0m : 2.32243
[1mStep[0m  [28/42], [94mLoss[0m : 2.65302
[1mStep[0m  [32/42], [94mLoss[0m : 2.46811
[1mStep[0m  [36/42], [94mLoss[0m : 2.40931
[1mStep[0m  [40/42], [94mLoss[0m : 2.49322

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40777
[1mStep[0m  [4/42], [94mLoss[0m : 2.47747
[1mStep[0m  [8/42], [94mLoss[0m : 2.43040
[1mStep[0m  [12/42], [94mLoss[0m : 2.45705
[1mStep[0m  [16/42], [94mLoss[0m : 2.33900
[1mStep[0m  [20/42], [94mLoss[0m : 2.46100
[1mStep[0m  [24/42], [94mLoss[0m : 2.34311
[1mStep[0m  [28/42], [94mLoss[0m : 2.38601
[1mStep[0m  [32/42], [94mLoss[0m : 2.39303
[1mStep[0m  [36/42], [94mLoss[0m : 2.47389
[1mStep[0m  [40/42], [94mLoss[0m : 2.34300

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36659
[1mStep[0m  [4/42], [94mLoss[0m : 2.45109
[1mStep[0m  [8/42], [94mLoss[0m : 2.52701
[1mStep[0m  [12/42], [94mLoss[0m : 2.60339
[1mStep[0m  [16/42], [94mLoss[0m : 2.46689
[1mStep[0m  [20/42], [94mLoss[0m : 2.58966
[1mStep[0m  [24/42], [94mLoss[0m : 2.35141
[1mStep[0m  [28/42], [94mLoss[0m : 2.14747
[1mStep[0m  [32/42], [94mLoss[0m : 2.53029
[1mStep[0m  [36/42], [94mLoss[0m : 2.61484
[1mStep[0m  [40/42], [94mLoss[0m : 2.41179

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53174
[1mStep[0m  [4/42], [94mLoss[0m : 2.33886
[1mStep[0m  [8/42], [94mLoss[0m : 2.52126
[1mStep[0m  [12/42], [94mLoss[0m : 2.48618
[1mStep[0m  [16/42], [94mLoss[0m : 2.42980
[1mStep[0m  [20/42], [94mLoss[0m : 2.64253
[1mStep[0m  [24/42], [94mLoss[0m : 2.33074
[1mStep[0m  [28/42], [94mLoss[0m : 2.51815
[1mStep[0m  [32/42], [94mLoss[0m : 2.44935
[1mStep[0m  [36/42], [94mLoss[0m : 2.58651
[1mStep[0m  [40/42], [94mLoss[0m : 2.18003

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56847
[1mStep[0m  [4/42], [94mLoss[0m : 2.35326
[1mStep[0m  [8/42], [94mLoss[0m : 2.46324
[1mStep[0m  [12/42], [94mLoss[0m : 2.36500
[1mStep[0m  [16/42], [94mLoss[0m : 2.53813
[1mStep[0m  [20/42], [94mLoss[0m : 2.48379
[1mStep[0m  [24/42], [94mLoss[0m : 2.34489
[1mStep[0m  [28/42], [94mLoss[0m : 2.70199
[1mStep[0m  [32/42], [94mLoss[0m : 2.37054
[1mStep[0m  [36/42], [94mLoss[0m : 2.23243
[1mStep[0m  [40/42], [94mLoss[0m : 2.23787

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31101
[1mStep[0m  [4/42], [94mLoss[0m : 2.42522
[1mStep[0m  [8/42], [94mLoss[0m : 2.49146
[1mStep[0m  [12/42], [94mLoss[0m : 2.36839
[1mStep[0m  [16/42], [94mLoss[0m : 2.43387
[1mStep[0m  [20/42], [94mLoss[0m : 2.35917
[1mStep[0m  [24/42], [94mLoss[0m : 2.45595
[1mStep[0m  [28/42], [94mLoss[0m : 2.45092
[1mStep[0m  [32/42], [94mLoss[0m : 2.53839
[1mStep[0m  [36/42], [94mLoss[0m : 2.35188
[1mStep[0m  [40/42], [94mLoss[0m : 2.53579

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36312
[1mStep[0m  [4/42], [94mLoss[0m : 2.55162
[1mStep[0m  [8/42], [94mLoss[0m : 2.35425
[1mStep[0m  [12/42], [94mLoss[0m : 2.51034
[1mStep[0m  [16/42], [94mLoss[0m : 2.43869
[1mStep[0m  [20/42], [94mLoss[0m : 2.54438
[1mStep[0m  [24/42], [94mLoss[0m : 2.39510
[1mStep[0m  [28/42], [94mLoss[0m : 2.44668
[1mStep[0m  [32/42], [94mLoss[0m : 2.49301
[1mStep[0m  [36/42], [94mLoss[0m : 2.40393
[1mStep[0m  [40/42], [94mLoss[0m : 2.58171

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66524
[1mStep[0m  [4/42], [94mLoss[0m : 2.34741
[1mStep[0m  [8/42], [94mLoss[0m : 2.47548
[1mStep[0m  [12/42], [94mLoss[0m : 2.60438
[1mStep[0m  [16/42], [94mLoss[0m : 2.46226
[1mStep[0m  [20/42], [94mLoss[0m : 2.83139
[1mStep[0m  [24/42], [94mLoss[0m : 2.43789
[1mStep[0m  [28/42], [94mLoss[0m : 2.37033
[1mStep[0m  [32/42], [94mLoss[0m : 2.53531
[1mStep[0m  [36/42], [94mLoss[0m : 2.51634
[1mStep[0m  [40/42], [94mLoss[0m : 2.28385

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17195
[1mStep[0m  [4/42], [94mLoss[0m : 2.43634
[1mStep[0m  [8/42], [94mLoss[0m : 2.47871
[1mStep[0m  [12/42], [94mLoss[0m : 2.48631
[1mStep[0m  [16/42], [94mLoss[0m : 2.52839
[1mStep[0m  [20/42], [94mLoss[0m : 2.49215
[1mStep[0m  [24/42], [94mLoss[0m : 2.63803
[1mStep[0m  [28/42], [94mLoss[0m : 2.51436
[1mStep[0m  [32/42], [94mLoss[0m : 2.67595
[1mStep[0m  [36/42], [94mLoss[0m : 2.56462
[1mStep[0m  [40/42], [94mLoss[0m : 2.44803

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22715
[1mStep[0m  [4/42], [94mLoss[0m : 2.42411
[1mStep[0m  [8/42], [94mLoss[0m : 2.68699
[1mStep[0m  [12/42], [94mLoss[0m : 2.43348
[1mStep[0m  [16/42], [94mLoss[0m : 2.31524
[1mStep[0m  [20/42], [94mLoss[0m : 2.41477
[1mStep[0m  [24/42], [94mLoss[0m : 2.61140
[1mStep[0m  [28/42], [94mLoss[0m : 2.67498
[1mStep[0m  [32/42], [94mLoss[0m : 2.36880
[1mStep[0m  [36/42], [94mLoss[0m : 2.38910
[1mStep[0m  [40/42], [94mLoss[0m : 2.49675

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55937
[1mStep[0m  [4/42], [94mLoss[0m : 2.75493
[1mStep[0m  [8/42], [94mLoss[0m : 2.52279
[1mStep[0m  [12/42], [94mLoss[0m : 2.67389
[1mStep[0m  [16/42], [94mLoss[0m : 2.73113
[1mStep[0m  [20/42], [94mLoss[0m : 2.50349
[1mStep[0m  [24/42], [94mLoss[0m : 2.42928
[1mStep[0m  [28/42], [94mLoss[0m : 2.37202
[1mStep[0m  [32/42], [94mLoss[0m : 2.51864
[1mStep[0m  [36/42], [94mLoss[0m : 2.56760
[1mStep[0m  [40/42], [94mLoss[0m : 2.57659

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45174
[1mStep[0m  [4/42], [94mLoss[0m : 2.63768
[1mStep[0m  [8/42], [94mLoss[0m : 2.41023
[1mStep[0m  [12/42], [94mLoss[0m : 2.53711
[1mStep[0m  [16/42], [94mLoss[0m : 2.51184
[1mStep[0m  [20/42], [94mLoss[0m : 2.39640
[1mStep[0m  [24/42], [94mLoss[0m : 2.30752
[1mStep[0m  [28/42], [94mLoss[0m : 2.46255
[1mStep[0m  [32/42], [94mLoss[0m : 2.41565
[1mStep[0m  [36/42], [94mLoss[0m : 2.43440
[1mStep[0m  [40/42], [94mLoss[0m : 2.43810

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37884
[1mStep[0m  [4/42], [94mLoss[0m : 2.24046
[1mStep[0m  [8/42], [94mLoss[0m : 2.60636
[1mStep[0m  [12/42], [94mLoss[0m : 2.47604
[1mStep[0m  [16/42], [94mLoss[0m : 2.28751
[1mStep[0m  [20/42], [94mLoss[0m : 2.50044
[1mStep[0m  [24/42], [94mLoss[0m : 2.27999
[1mStep[0m  [28/42], [94mLoss[0m : 2.50654
[1mStep[0m  [32/42], [94mLoss[0m : 2.46083
[1mStep[0m  [36/42], [94mLoss[0m : 2.67285
[1mStep[0m  [40/42], [94mLoss[0m : 2.43501

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42115
[1mStep[0m  [4/42], [94mLoss[0m : 2.72356
[1mStep[0m  [8/42], [94mLoss[0m : 2.42092
[1mStep[0m  [12/42], [94mLoss[0m : 2.40190
[1mStep[0m  [16/42], [94mLoss[0m : 2.40616
[1mStep[0m  [20/42], [94mLoss[0m : 2.33184
[1mStep[0m  [24/42], [94mLoss[0m : 2.44843
[1mStep[0m  [28/42], [94mLoss[0m : 2.33589
[1mStep[0m  [32/42], [94mLoss[0m : 2.31004
[1mStep[0m  [36/42], [94mLoss[0m : 2.72066
[1mStep[0m  [40/42], [94mLoss[0m : 2.46591

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58821
[1mStep[0m  [4/42], [94mLoss[0m : 2.21402
[1mStep[0m  [8/42], [94mLoss[0m : 2.40204
[1mStep[0m  [12/42], [94mLoss[0m : 2.38568
[1mStep[0m  [16/42], [94mLoss[0m : 2.41660
[1mStep[0m  [20/42], [94mLoss[0m : 2.43657
[1mStep[0m  [24/42], [94mLoss[0m : 2.53355
[1mStep[0m  [28/42], [94mLoss[0m : 2.49806
[1mStep[0m  [32/42], [94mLoss[0m : 2.42618
[1mStep[0m  [36/42], [94mLoss[0m : 2.45258
[1mStep[0m  [40/42], [94mLoss[0m : 2.30219

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3370443752833774
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.56637
[1mStep[0m  [4/42], [94mLoss[0m : 2.45737
[1mStep[0m  [8/42], [94mLoss[0m : 2.45456
[1mStep[0m  [12/42], [94mLoss[0m : 2.41422
[1mStep[0m  [16/42], [94mLoss[0m : 2.35148
[1mStep[0m  [20/42], [94mLoss[0m : 2.49262
[1mStep[0m  [24/42], [94mLoss[0m : 2.40086
[1mStep[0m  [28/42], [94mLoss[0m : 2.34591
[1mStep[0m  [32/42], [94mLoss[0m : 2.56752
[1mStep[0m  [36/42], [94mLoss[0m : 2.50859
[1mStep[0m  [40/42], [94mLoss[0m : 2.56338

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55941
[1mStep[0m  [4/42], [94mLoss[0m : 2.44234
[1mStep[0m  [8/42], [94mLoss[0m : 2.22461
[1mStep[0m  [12/42], [94mLoss[0m : 2.51989
[1mStep[0m  [16/42], [94mLoss[0m : 2.58348
[1mStep[0m  [20/42], [94mLoss[0m : 2.44241
[1mStep[0m  [24/42], [94mLoss[0m : 2.45963
[1mStep[0m  [28/42], [94mLoss[0m : 2.59064
[1mStep[0m  [32/42], [94mLoss[0m : 2.25201
[1mStep[0m  [36/42], [94mLoss[0m : 2.34435
[1mStep[0m  [40/42], [94mLoss[0m : 2.37828

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45452
[1mStep[0m  [4/42], [94mLoss[0m : 2.53284
[1mStep[0m  [8/42], [94mLoss[0m : 2.48973
[1mStep[0m  [12/42], [94mLoss[0m : 2.52527
[1mStep[0m  [16/42], [94mLoss[0m : 2.37625
[1mStep[0m  [20/42], [94mLoss[0m : 2.52012
[1mStep[0m  [24/42], [94mLoss[0m : 2.58624
[1mStep[0m  [28/42], [94mLoss[0m : 2.37486
[1mStep[0m  [32/42], [94mLoss[0m : 2.35982
[1mStep[0m  [36/42], [94mLoss[0m : 2.31583
[1mStep[0m  [40/42], [94mLoss[0m : 2.40432

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54970
[1mStep[0m  [4/42], [94mLoss[0m : 2.38820
[1mStep[0m  [8/42], [94mLoss[0m : 2.12680
[1mStep[0m  [12/42], [94mLoss[0m : 2.44688
[1mStep[0m  [16/42], [94mLoss[0m : 2.38331
[1mStep[0m  [20/42], [94mLoss[0m : 2.60353
[1mStep[0m  [24/42], [94mLoss[0m : 2.50464
[1mStep[0m  [28/42], [94mLoss[0m : 2.67007
[1mStep[0m  [32/42], [94mLoss[0m : 2.38092
[1mStep[0m  [36/42], [94mLoss[0m : 2.54320
[1mStep[0m  [40/42], [94mLoss[0m : 2.49981

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37540
[1mStep[0m  [4/42], [94mLoss[0m : 2.44412
[1mStep[0m  [8/42], [94mLoss[0m : 2.43048
[1mStep[0m  [12/42], [94mLoss[0m : 2.34479
[1mStep[0m  [16/42], [94mLoss[0m : 2.43532
[1mStep[0m  [20/42], [94mLoss[0m : 2.44362
[1mStep[0m  [24/42], [94mLoss[0m : 2.41409
[1mStep[0m  [28/42], [94mLoss[0m : 2.39082
[1mStep[0m  [32/42], [94mLoss[0m : 2.30773
[1mStep[0m  [36/42], [94mLoss[0m : 2.27297
[1mStep[0m  [40/42], [94mLoss[0m : 2.62358

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56232
[1mStep[0m  [4/42], [94mLoss[0m : 2.24887
[1mStep[0m  [8/42], [94mLoss[0m : 2.38129
[1mStep[0m  [12/42], [94mLoss[0m : 2.54320
[1mStep[0m  [16/42], [94mLoss[0m : 2.37395
[1mStep[0m  [20/42], [94mLoss[0m : 2.43628
[1mStep[0m  [24/42], [94mLoss[0m : 2.34827
[1mStep[0m  [28/42], [94mLoss[0m : 2.41928
[1mStep[0m  [32/42], [94mLoss[0m : 2.47608
[1mStep[0m  [36/42], [94mLoss[0m : 2.22826
[1mStep[0m  [40/42], [94mLoss[0m : 2.39740

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51755
[1mStep[0m  [4/42], [94mLoss[0m : 2.34449
[1mStep[0m  [8/42], [94mLoss[0m : 2.26061
[1mStep[0m  [12/42], [94mLoss[0m : 2.39645
[1mStep[0m  [16/42], [94mLoss[0m : 2.50106
[1mStep[0m  [20/42], [94mLoss[0m : 2.66494
[1mStep[0m  [24/42], [94mLoss[0m : 2.27331
[1mStep[0m  [28/42], [94mLoss[0m : 2.25926
[1mStep[0m  [32/42], [94mLoss[0m : 2.38872
[1mStep[0m  [36/42], [94mLoss[0m : 2.34314
[1mStep[0m  [40/42], [94mLoss[0m : 2.41648

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40908
[1mStep[0m  [4/42], [94mLoss[0m : 2.31204
[1mStep[0m  [8/42], [94mLoss[0m : 2.24155
[1mStep[0m  [12/42], [94mLoss[0m : 2.28463
[1mStep[0m  [16/42], [94mLoss[0m : 2.38904
[1mStep[0m  [20/42], [94mLoss[0m : 2.51244
[1mStep[0m  [24/42], [94mLoss[0m : 2.19098
[1mStep[0m  [28/42], [94mLoss[0m : 2.33693
[1mStep[0m  [32/42], [94mLoss[0m : 2.54802
[1mStep[0m  [36/42], [94mLoss[0m : 2.30807
[1mStep[0m  [40/42], [94mLoss[0m : 2.44237

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29720
[1mStep[0m  [4/42], [94mLoss[0m : 2.24667
[1mStep[0m  [8/42], [94mLoss[0m : 2.44659
[1mStep[0m  [12/42], [94mLoss[0m : 2.25404
[1mStep[0m  [16/42], [94mLoss[0m : 2.36332
[1mStep[0m  [20/42], [94mLoss[0m : 2.44071
[1mStep[0m  [24/42], [94mLoss[0m : 2.36102
[1mStep[0m  [28/42], [94mLoss[0m : 2.18775
[1mStep[0m  [32/42], [94mLoss[0m : 2.39149
[1mStep[0m  [36/42], [94mLoss[0m : 2.33968
[1mStep[0m  [40/42], [94mLoss[0m : 2.54580

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.583, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39530
[1mStep[0m  [4/42], [94mLoss[0m : 2.07300
[1mStep[0m  [8/42], [94mLoss[0m : 2.43784
[1mStep[0m  [12/42], [94mLoss[0m : 2.17457
[1mStep[0m  [16/42], [94mLoss[0m : 2.50135
[1mStep[0m  [20/42], [94mLoss[0m : 2.25507
[1mStep[0m  [24/42], [94mLoss[0m : 2.46169
[1mStep[0m  [28/42], [94mLoss[0m : 2.20959
[1mStep[0m  [32/42], [94mLoss[0m : 2.39646
[1mStep[0m  [36/42], [94mLoss[0m : 2.44215
[1mStep[0m  [40/42], [94mLoss[0m : 2.32094

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.594, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41687
[1mStep[0m  [4/42], [94mLoss[0m : 2.24810
[1mStep[0m  [8/42], [94mLoss[0m : 2.46700
[1mStep[0m  [12/42], [94mLoss[0m : 2.36248
[1mStep[0m  [16/42], [94mLoss[0m : 2.25352
[1mStep[0m  [20/42], [94mLoss[0m : 2.19169
[1mStep[0m  [24/42], [94mLoss[0m : 2.34576
[1mStep[0m  [28/42], [94mLoss[0m : 2.45420
[1mStep[0m  [32/42], [94mLoss[0m : 2.34956
[1mStep[0m  [36/42], [94mLoss[0m : 2.34906
[1mStep[0m  [40/42], [94mLoss[0m : 2.48660

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.590, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04724
[1mStep[0m  [4/42], [94mLoss[0m : 2.50192
[1mStep[0m  [8/42], [94mLoss[0m : 2.29358
[1mStep[0m  [12/42], [94mLoss[0m : 2.27146
[1mStep[0m  [16/42], [94mLoss[0m : 2.16816
[1mStep[0m  [20/42], [94mLoss[0m : 2.29960
[1mStep[0m  [24/42], [94mLoss[0m : 2.16336
[1mStep[0m  [28/42], [94mLoss[0m : 2.30467
[1mStep[0m  [32/42], [94mLoss[0m : 2.43085
[1mStep[0m  [36/42], [94mLoss[0m : 2.25179
[1mStep[0m  [40/42], [94mLoss[0m : 2.36798

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.640, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35304
[1mStep[0m  [4/42], [94mLoss[0m : 2.32996
[1mStep[0m  [8/42], [94mLoss[0m : 2.13668
[1mStep[0m  [12/42], [94mLoss[0m : 2.18879
[1mStep[0m  [16/42], [94mLoss[0m : 2.42501
[1mStep[0m  [20/42], [94mLoss[0m : 2.28049
[1mStep[0m  [24/42], [94mLoss[0m : 2.33814
[1mStep[0m  [28/42], [94mLoss[0m : 2.31775
[1mStep[0m  [32/42], [94mLoss[0m : 2.32122
[1mStep[0m  [36/42], [94mLoss[0m : 2.09686
[1mStep[0m  [40/42], [94mLoss[0m : 2.23353

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.643, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28229
[1mStep[0m  [4/42], [94mLoss[0m : 2.36049
[1mStep[0m  [8/42], [94mLoss[0m : 2.30830
[1mStep[0m  [12/42], [94mLoss[0m : 2.40722
[1mStep[0m  [16/42], [94mLoss[0m : 2.49937
[1mStep[0m  [20/42], [94mLoss[0m : 2.21970
[1mStep[0m  [24/42], [94mLoss[0m : 2.32987
[1mStep[0m  [28/42], [94mLoss[0m : 2.19800
[1mStep[0m  [32/42], [94mLoss[0m : 2.48953
[1mStep[0m  [36/42], [94mLoss[0m : 2.26635
[1mStep[0m  [40/42], [94mLoss[0m : 2.27929

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.575, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17447
[1mStep[0m  [4/42], [94mLoss[0m : 2.07487
[1mStep[0m  [8/42], [94mLoss[0m : 2.21328
[1mStep[0m  [12/42], [94mLoss[0m : 2.30852
[1mStep[0m  [16/42], [94mLoss[0m : 2.05572
[1mStep[0m  [20/42], [94mLoss[0m : 2.60004
[1mStep[0m  [24/42], [94mLoss[0m : 2.31515
[1mStep[0m  [28/42], [94mLoss[0m : 2.13593
[1mStep[0m  [32/42], [94mLoss[0m : 2.40918
[1mStep[0m  [36/42], [94mLoss[0m : 2.19379
[1mStep[0m  [40/42], [94mLoss[0m : 2.27677

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.236, [92mTest[0m: 2.640, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25432
[1mStep[0m  [4/42], [94mLoss[0m : 2.25008
[1mStep[0m  [8/42], [94mLoss[0m : 2.31019
[1mStep[0m  [12/42], [94mLoss[0m : 2.06613
[1mStep[0m  [16/42], [94mLoss[0m : 2.21421
[1mStep[0m  [20/42], [94mLoss[0m : 1.93002
[1mStep[0m  [24/42], [94mLoss[0m : 2.15614
[1mStep[0m  [28/42], [94mLoss[0m : 2.26603
[1mStep[0m  [32/42], [94mLoss[0m : 2.34249
[1mStep[0m  [36/42], [94mLoss[0m : 2.26511
[1mStep[0m  [40/42], [94mLoss[0m : 2.32478

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.671, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17184
[1mStep[0m  [4/42], [94mLoss[0m : 2.08914
[1mStep[0m  [8/42], [94mLoss[0m : 2.18312
[1mStep[0m  [12/42], [94mLoss[0m : 2.40060
[1mStep[0m  [16/42], [94mLoss[0m : 2.39375
[1mStep[0m  [20/42], [94mLoss[0m : 2.26225
[1mStep[0m  [24/42], [94mLoss[0m : 2.23222
[1mStep[0m  [28/42], [94mLoss[0m : 2.12340
[1mStep[0m  [32/42], [94mLoss[0m : 2.03154
[1mStep[0m  [36/42], [94mLoss[0m : 2.15531
[1mStep[0m  [40/42], [94mLoss[0m : 2.26878

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.621, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21293
[1mStep[0m  [4/42], [94mLoss[0m : 2.37971
[1mStep[0m  [8/42], [94mLoss[0m : 2.30933
[1mStep[0m  [12/42], [94mLoss[0m : 2.38373
[1mStep[0m  [16/42], [94mLoss[0m : 2.05166
[1mStep[0m  [20/42], [94mLoss[0m : 2.08421
[1mStep[0m  [24/42], [94mLoss[0m : 2.16501
[1mStep[0m  [28/42], [94mLoss[0m : 2.19165
[1mStep[0m  [32/42], [94mLoss[0m : 2.31516
[1mStep[0m  [36/42], [94mLoss[0m : 2.32039
[1mStep[0m  [40/42], [94mLoss[0m : 2.16538

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.648, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28307
[1mStep[0m  [4/42], [94mLoss[0m : 2.16834
[1mStep[0m  [8/42], [94mLoss[0m : 2.27162
[1mStep[0m  [12/42], [94mLoss[0m : 2.13161
[1mStep[0m  [16/42], [94mLoss[0m : 2.22083
[1mStep[0m  [20/42], [94mLoss[0m : 2.03954
[1mStep[0m  [24/42], [94mLoss[0m : 2.21205
[1mStep[0m  [28/42], [94mLoss[0m : 2.09679
[1mStep[0m  [32/42], [94mLoss[0m : 2.12274
[1mStep[0m  [36/42], [94mLoss[0m : 1.98374
[1mStep[0m  [40/42], [94mLoss[0m : 2.14961

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.624, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11825
[1mStep[0m  [4/42], [94mLoss[0m : 1.96881
[1mStep[0m  [8/42], [94mLoss[0m : 2.12038
[1mStep[0m  [12/42], [94mLoss[0m : 2.08892
[1mStep[0m  [16/42], [94mLoss[0m : 2.04732
[1mStep[0m  [20/42], [94mLoss[0m : 2.25679
[1mStep[0m  [24/42], [94mLoss[0m : 2.23202
[1mStep[0m  [28/42], [94mLoss[0m : 1.93030
[1mStep[0m  [32/42], [94mLoss[0m : 2.18365
[1mStep[0m  [36/42], [94mLoss[0m : 2.07000
[1mStep[0m  [40/42], [94mLoss[0m : 2.30806

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.664, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05557
[1mStep[0m  [4/42], [94mLoss[0m : 2.11208
[1mStep[0m  [8/42], [94mLoss[0m : 1.95612
[1mStep[0m  [12/42], [94mLoss[0m : 2.11513
[1mStep[0m  [16/42], [94mLoss[0m : 2.08013
[1mStep[0m  [20/42], [94mLoss[0m : 1.92690
[1mStep[0m  [24/42], [94mLoss[0m : 2.08435
[1mStep[0m  [28/42], [94mLoss[0m : 2.45617
[1mStep[0m  [32/42], [94mLoss[0m : 2.11916
[1mStep[0m  [36/42], [94mLoss[0m : 2.30156
[1mStep[0m  [40/42], [94mLoss[0m : 2.25892

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.591, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81570
[1mStep[0m  [4/42], [94mLoss[0m : 2.13504
[1mStep[0m  [8/42], [94mLoss[0m : 2.13158
[1mStep[0m  [12/42], [94mLoss[0m : 2.14158
[1mStep[0m  [16/42], [94mLoss[0m : 2.16328
[1mStep[0m  [20/42], [94mLoss[0m : 2.10674
[1mStep[0m  [24/42], [94mLoss[0m : 2.07467
[1mStep[0m  [28/42], [94mLoss[0m : 1.94757
[1mStep[0m  [32/42], [94mLoss[0m : 2.08151
[1mStep[0m  [36/42], [94mLoss[0m : 2.33909
[1mStep[0m  [40/42], [94mLoss[0m : 2.02459

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.619, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06211
[1mStep[0m  [4/42], [94mLoss[0m : 1.90642
[1mStep[0m  [8/42], [94mLoss[0m : 2.01833
[1mStep[0m  [12/42], [94mLoss[0m : 2.02258
[1mStep[0m  [16/42], [94mLoss[0m : 2.14933
[1mStep[0m  [20/42], [94mLoss[0m : 2.11544
[1mStep[0m  [24/42], [94mLoss[0m : 2.01761
[1mStep[0m  [28/42], [94mLoss[0m : 2.06828
[1mStep[0m  [32/42], [94mLoss[0m : 2.08714
[1mStep[0m  [36/42], [94mLoss[0m : 2.20637
[1mStep[0m  [40/42], [94mLoss[0m : 2.12061

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.559, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89079
[1mStep[0m  [4/42], [94mLoss[0m : 2.26724
[1mStep[0m  [8/42], [94mLoss[0m : 2.00196
[1mStep[0m  [12/42], [94mLoss[0m : 1.91830
[1mStep[0m  [16/42], [94mLoss[0m : 1.93340
[1mStep[0m  [20/42], [94mLoss[0m : 1.91678
[1mStep[0m  [24/42], [94mLoss[0m : 2.20242
[1mStep[0m  [28/42], [94mLoss[0m : 2.13898
[1mStep[0m  [32/42], [94mLoss[0m : 2.04402
[1mStep[0m  [36/42], [94mLoss[0m : 2.08763
[1mStep[0m  [40/42], [94mLoss[0m : 2.15598

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.76201
[1mStep[0m  [4/42], [94mLoss[0m : 2.10646
[1mStep[0m  [8/42], [94mLoss[0m : 1.91915
[1mStep[0m  [12/42], [94mLoss[0m : 1.80915
[1mStep[0m  [16/42], [94mLoss[0m : 2.09015
[1mStep[0m  [20/42], [94mLoss[0m : 2.06111
[1mStep[0m  [24/42], [94mLoss[0m : 1.77828
[1mStep[0m  [28/42], [94mLoss[0m : 1.95872
[1mStep[0m  [32/42], [94mLoss[0m : 2.10533
[1mStep[0m  [36/42], [94mLoss[0m : 2.09534
[1mStep[0m  [40/42], [94mLoss[0m : 2.07740

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03967
[1mStep[0m  [4/42], [94mLoss[0m : 2.18071
[1mStep[0m  [8/42], [94mLoss[0m : 1.77495
[1mStep[0m  [12/42], [94mLoss[0m : 1.90004
[1mStep[0m  [16/42], [94mLoss[0m : 1.85402
[1mStep[0m  [20/42], [94mLoss[0m : 2.08982
[1mStep[0m  [24/42], [94mLoss[0m : 1.90899
[1mStep[0m  [28/42], [94mLoss[0m : 2.14701
[1mStep[0m  [32/42], [94mLoss[0m : 2.16376
[1mStep[0m  [36/42], [94mLoss[0m : 1.93629
[1mStep[0m  [40/42], [94mLoss[0m : 1.85168

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83120
[1mStep[0m  [4/42], [94mLoss[0m : 1.98794
[1mStep[0m  [8/42], [94mLoss[0m : 1.96296
[1mStep[0m  [12/42], [94mLoss[0m : 2.06103
[1mStep[0m  [16/42], [94mLoss[0m : 2.08421
[1mStep[0m  [20/42], [94mLoss[0m : 2.00512
[1mStep[0m  [24/42], [94mLoss[0m : 2.06962
[1mStep[0m  [28/42], [94mLoss[0m : 1.93576
[1mStep[0m  [32/42], [94mLoss[0m : 1.97469
[1mStep[0m  [36/42], [94mLoss[0m : 2.09721
[1mStep[0m  [40/42], [94mLoss[0m : 1.85880

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.88461
[1mStep[0m  [4/42], [94mLoss[0m : 1.91964
[1mStep[0m  [8/42], [94mLoss[0m : 2.03378
[1mStep[0m  [12/42], [94mLoss[0m : 1.91155
[1mStep[0m  [16/42], [94mLoss[0m : 2.23019
[1mStep[0m  [20/42], [94mLoss[0m : 1.97026
[1mStep[0m  [24/42], [94mLoss[0m : 1.95443
[1mStep[0m  [28/42], [94mLoss[0m : 1.80445
[1mStep[0m  [32/42], [94mLoss[0m : 1.94679
[1mStep[0m  [36/42], [94mLoss[0m : 2.02214
[1mStep[0m  [40/42], [94mLoss[0m : 1.96412

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.549, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06025
[1mStep[0m  [4/42], [94mLoss[0m : 1.90392
[1mStep[0m  [8/42], [94mLoss[0m : 1.95829
[1mStep[0m  [12/42], [94mLoss[0m : 2.03476
[1mStep[0m  [16/42], [94mLoss[0m : 1.73994
[1mStep[0m  [20/42], [94mLoss[0m : 1.93991
[1mStep[0m  [24/42], [94mLoss[0m : 1.92745
[1mStep[0m  [28/42], [94mLoss[0m : 1.97668
[1mStep[0m  [32/42], [94mLoss[0m : 1.81015
[1mStep[0m  [36/42], [94mLoss[0m : 1.97329
[1mStep[0m  [40/42], [94mLoss[0m : 1.99115

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.501, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95752
[1mStep[0m  [4/42], [94mLoss[0m : 1.81621
[1mStep[0m  [8/42], [94mLoss[0m : 1.92097
[1mStep[0m  [12/42], [94mLoss[0m : 1.89209
[1mStep[0m  [16/42], [94mLoss[0m : 1.96918
[1mStep[0m  [20/42], [94mLoss[0m : 1.84140
[1mStep[0m  [24/42], [94mLoss[0m : 1.98446
[1mStep[0m  [28/42], [94mLoss[0m : 1.85267
[1mStep[0m  [32/42], [94mLoss[0m : 1.94762
[1mStep[0m  [36/42], [94mLoss[0m : 1.82034
[1mStep[0m  [40/42], [94mLoss[0m : 1.93557

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.579, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.475
====================================

Phase 2 - Evaluation MAE:  2.475439889090402
MAE score P1      2.337044
MAE score P2       2.47544
loss              1.912425
learning_rate      0.00505
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.47163
[1mStep[0m  [4/42], [94mLoss[0m : 11.09007
[1mStep[0m  [8/42], [94mLoss[0m : 10.59054
[1mStep[0m  [12/42], [94mLoss[0m : 10.58323
[1mStep[0m  [16/42], [94mLoss[0m : 10.00367
[1mStep[0m  [20/42], [94mLoss[0m : 9.96294
[1mStep[0m  [24/42], [94mLoss[0m : 9.26615
[1mStep[0m  [28/42], [94mLoss[0m : 8.90083
[1mStep[0m  [32/42], [94mLoss[0m : 8.75937
[1mStep[0m  [36/42], [94mLoss[0m : 8.70722
[1mStep[0m  [40/42], [94mLoss[0m : 8.21801

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.766, [92mTest[0m: 11.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.31966
[1mStep[0m  [4/42], [94mLoss[0m : 7.78957
[1mStep[0m  [8/42], [94mLoss[0m : 7.12123
[1mStep[0m  [12/42], [94mLoss[0m : 7.27428
[1mStep[0m  [16/42], [94mLoss[0m : 7.01259
[1mStep[0m  [20/42], [94mLoss[0m : 6.52200
[1mStep[0m  [24/42], [94mLoss[0m : 6.06558
[1mStep[0m  [28/42], [94mLoss[0m : 5.97423
[1mStep[0m  [32/42], [94mLoss[0m : 5.39766
[1mStep[0m  [36/42], [94mLoss[0m : 5.77936
[1mStep[0m  [40/42], [94mLoss[0m : 5.46846

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.578, [92mTest[0m: 8.105, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.94586
[1mStep[0m  [4/42], [94mLoss[0m : 4.69483
[1mStep[0m  [8/42], [94mLoss[0m : 4.72188
[1mStep[0m  [12/42], [94mLoss[0m : 4.10368
[1mStep[0m  [16/42], [94mLoss[0m : 4.23302
[1mStep[0m  [20/42], [94mLoss[0m : 4.12895
[1mStep[0m  [24/42], [94mLoss[0m : 3.58884
[1mStep[0m  [28/42], [94mLoss[0m : 3.78424
[1mStep[0m  [32/42], [94mLoss[0m : 3.50003
[1mStep[0m  [36/42], [94mLoss[0m : 3.57593
[1mStep[0m  [40/42], [94mLoss[0m : 3.35707

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.051, [92mTest[0m: 4.975, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.25860
[1mStep[0m  [4/42], [94mLoss[0m : 3.47332
[1mStep[0m  [8/42], [94mLoss[0m : 3.04647
[1mStep[0m  [12/42], [94mLoss[0m : 2.93416
[1mStep[0m  [16/42], [94mLoss[0m : 3.05672
[1mStep[0m  [20/42], [94mLoss[0m : 2.85579
[1mStep[0m  [24/42], [94mLoss[0m : 2.94837
[1mStep[0m  [28/42], [94mLoss[0m : 3.35182
[1mStep[0m  [32/42], [94mLoss[0m : 2.71095
[1mStep[0m  [36/42], [94mLoss[0m : 2.93454
[1mStep[0m  [40/42], [94mLoss[0m : 2.80151

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.020, [92mTest[0m: 3.198, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.22723
[1mStep[0m  [4/42], [94mLoss[0m : 2.54882
[1mStep[0m  [8/42], [94mLoss[0m : 2.60262
[1mStep[0m  [12/42], [94mLoss[0m : 2.74407
[1mStep[0m  [16/42], [94mLoss[0m : 2.96983
[1mStep[0m  [20/42], [94mLoss[0m : 2.61346
[1mStep[0m  [24/42], [94mLoss[0m : 2.84620
[1mStep[0m  [28/42], [94mLoss[0m : 2.61202
[1mStep[0m  [32/42], [94mLoss[0m : 2.79269
[1mStep[0m  [36/42], [94mLoss[0m : 2.80954
[1mStep[0m  [40/42], [94mLoss[0m : 2.84345

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.605, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42319
[1mStep[0m  [4/42], [94mLoss[0m : 2.46769
[1mStep[0m  [8/42], [94mLoss[0m : 2.71850
[1mStep[0m  [12/42], [94mLoss[0m : 2.75038
[1mStep[0m  [16/42], [94mLoss[0m : 2.58996
[1mStep[0m  [20/42], [94mLoss[0m : 2.69463
[1mStep[0m  [24/42], [94mLoss[0m : 2.58938
[1mStep[0m  [28/42], [94mLoss[0m : 2.58106
[1mStep[0m  [32/42], [94mLoss[0m : 2.77855
[1mStep[0m  [36/42], [94mLoss[0m : 2.57249
[1mStep[0m  [40/42], [94mLoss[0m : 2.75216

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26607
[1mStep[0m  [4/42], [94mLoss[0m : 2.55187
[1mStep[0m  [8/42], [94mLoss[0m : 2.49803
[1mStep[0m  [12/42], [94mLoss[0m : 2.49964
[1mStep[0m  [16/42], [94mLoss[0m : 2.56704
[1mStep[0m  [20/42], [94mLoss[0m : 2.51744
[1mStep[0m  [24/42], [94mLoss[0m : 2.72306
[1mStep[0m  [28/42], [94mLoss[0m : 2.50241
[1mStep[0m  [32/42], [94mLoss[0m : 2.53395
[1mStep[0m  [36/42], [94mLoss[0m : 2.60577
[1mStep[0m  [40/42], [94mLoss[0m : 2.56330

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56015
[1mStep[0m  [4/42], [94mLoss[0m : 2.58186
[1mStep[0m  [8/42], [94mLoss[0m : 2.61572
[1mStep[0m  [12/42], [94mLoss[0m : 2.26868
[1mStep[0m  [16/42], [94mLoss[0m : 2.66153
[1mStep[0m  [20/42], [94mLoss[0m : 2.56850
[1mStep[0m  [24/42], [94mLoss[0m : 2.68667
[1mStep[0m  [28/42], [94mLoss[0m : 2.71761
[1mStep[0m  [32/42], [94mLoss[0m : 2.55698
[1mStep[0m  [36/42], [94mLoss[0m : 2.69265
[1mStep[0m  [40/42], [94mLoss[0m : 2.47495

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47961
[1mStep[0m  [4/42], [94mLoss[0m : 2.72432
[1mStep[0m  [8/42], [94mLoss[0m : 2.80732
[1mStep[0m  [12/42], [94mLoss[0m : 2.56492
[1mStep[0m  [16/42], [94mLoss[0m : 2.32258
[1mStep[0m  [20/42], [94mLoss[0m : 2.35224
[1mStep[0m  [24/42], [94mLoss[0m : 2.51116
[1mStep[0m  [28/42], [94mLoss[0m : 2.69108
[1mStep[0m  [32/42], [94mLoss[0m : 2.48764
[1mStep[0m  [36/42], [94mLoss[0m : 2.56999
[1mStep[0m  [40/42], [94mLoss[0m : 2.58549

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44507
[1mStep[0m  [4/42], [94mLoss[0m : 2.84817
[1mStep[0m  [8/42], [94mLoss[0m : 2.52759
[1mStep[0m  [12/42], [94mLoss[0m : 2.51757
[1mStep[0m  [16/42], [94mLoss[0m : 2.47907
[1mStep[0m  [20/42], [94mLoss[0m : 2.39530
[1mStep[0m  [24/42], [94mLoss[0m : 2.49634
[1mStep[0m  [28/42], [94mLoss[0m : 2.48333
[1mStep[0m  [32/42], [94mLoss[0m : 2.43490
[1mStep[0m  [36/42], [94mLoss[0m : 2.44696
[1mStep[0m  [40/42], [94mLoss[0m : 2.50982

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70452
[1mStep[0m  [4/42], [94mLoss[0m : 2.43300
[1mStep[0m  [8/42], [94mLoss[0m : 2.59137
[1mStep[0m  [12/42], [94mLoss[0m : 2.54122
[1mStep[0m  [16/42], [94mLoss[0m : 2.70642
[1mStep[0m  [20/42], [94mLoss[0m : 2.65391
[1mStep[0m  [24/42], [94mLoss[0m : 2.41308
[1mStep[0m  [28/42], [94mLoss[0m : 2.80820
[1mStep[0m  [32/42], [94mLoss[0m : 2.58304
[1mStep[0m  [36/42], [94mLoss[0m : 2.40503
[1mStep[0m  [40/42], [94mLoss[0m : 2.34032

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47946
[1mStep[0m  [4/42], [94mLoss[0m : 2.63973
[1mStep[0m  [8/42], [94mLoss[0m : 2.65825
[1mStep[0m  [12/42], [94mLoss[0m : 2.51049
[1mStep[0m  [16/42], [94mLoss[0m : 2.45424
[1mStep[0m  [20/42], [94mLoss[0m : 2.57702
[1mStep[0m  [24/42], [94mLoss[0m : 2.27877
[1mStep[0m  [28/42], [94mLoss[0m : 2.67441
[1mStep[0m  [32/42], [94mLoss[0m : 2.67564
[1mStep[0m  [36/42], [94mLoss[0m : 2.51427
[1mStep[0m  [40/42], [94mLoss[0m : 2.66771

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51144
[1mStep[0m  [4/42], [94mLoss[0m : 2.48300
[1mStep[0m  [8/42], [94mLoss[0m : 2.61290
[1mStep[0m  [12/42], [94mLoss[0m : 2.44980
[1mStep[0m  [16/42], [94mLoss[0m : 2.36099
[1mStep[0m  [20/42], [94mLoss[0m : 2.45353
[1mStep[0m  [24/42], [94mLoss[0m : 2.59066
[1mStep[0m  [28/42], [94mLoss[0m : 2.54485
[1mStep[0m  [32/42], [94mLoss[0m : 2.56652
[1mStep[0m  [36/42], [94mLoss[0m : 2.47353
[1mStep[0m  [40/42], [94mLoss[0m : 2.66781

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47675
[1mStep[0m  [4/42], [94mLoss[0m : 2.33211
[1mStep[0m  [8/42], [94mLoss[0m : 2.43546
[1mStep[0m  [12/42], [94mLoss[0m : 2.55873
[1mStep[0m  [16/42], [94mLoss[0m : 2.57090
[1mStep[0m  [20/42], [94mLoss[0m : 2.53903
[1mStep[0m  [24/42], [94mLoss[0m : 2.34268
[1mStep[0m  [28/42], [94mLoss[0m : 2.52446
[1mStep[0m  [32/42], [94mLoss[0m : 2.56769
[1mStep[0m  [36/42], [94mLoss[0m : 2.29680
[1mStep[0m  [40/42], [94mLoss[0m : 2.54341

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38785
[1mStep[0m  [4/42], [94mLoss[0m : 2.43753
[1mStep[0m  [8/42], [94mLoss[0m : 2.25554
[1mStep[0m  [12/42], [94mLoss[0m : 2.28922
[1mStep[0m  [16/42], [94mLoss[0m : 2.33222
[1mStep[0m  [20/42], [94mLoss[0m : 2.81373
[1mStep[0m  [24/42], [94mLoss[0m : 2.60932
[1mStep[0m  [28/42], [94mLoss[0m : 2.63869
[1mStep[0m  [32/42], [94mLoss[0m : 2.49253
[1mStep[0m  [36/42], [94mLoss[0m : 2.43060
[1mStep[0m  [40/42], [94mLoss[0m : 2.46085

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48194
[1mStep[0m  [4/42], [94mLoss[0m : 2.43817
[1mStep[0m  [8/42], [94mLoss[0m : 2.66016
[1mStep[0m  [12/42], [94mLoss[0m : 2.59984
[1mStep[0m  [16/42], [94mLoss[0m : 2.55310
[1mStep[0m  [20/42], [94mLoss[0m : 2.46483
[1mStep[0m  [24/42], [94mLoss[0m : 2.49089
[1mStep[0m  [28/42], [94mLoss[0m : 2.50312
[1mStep[0m  [32/42], [94mLoss[0m : 2.61825
[1mStep[0m  [36/42], [94mLoss[0m : 2.49639
[1mStep[0m  [40/42], [94mLoss[0m : 2.52275

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36570
[1mStep[0m  [4/42], [94mLoss[0m : 2.46884
[1mStep[0m  [8/42], [94mLoss[0m : 2.18252
[1mStep[0m  [12/42], [94mLoss[0m : 2.68739
[1mStep[0m  [16/42], [94mLoss[0m : 2.54778
[1mStep[0m  [20/42], [94mLoss[0m : 2.72436
[1mStep[0m  [24/42], [94mLoss[0m : 2.82595
[1mStep[0m  [28/42], [94mLoss[0m : 2.46724
[1mStep[0m  [32/42], [94mLoss[0m : 2.66295
[1mStep[0m  [36/42], [94mLoss[0m : 2.71384
[1mStep[0m  [40/42], [94mLoss[0m : 2.52396

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79360
[1mStep[0m  [4/42], [94mLoss[0m : 2.60509
[1mStep[0m  [8/42], [94mLoss[0m : 2.53497
[1mStep[0m  [12/42], [94mLoss[0m : 2.47906
[1mStep[0m  [16/42], [94mLoss[0m : 2.58985
[1mStep[0m  [20/42], [94mLoss[0m : 2.50901
[1mStep[0m  [24/42], [94mLoss[0m : 2.56366
[1mStep[0m  [28/42], [94mLoss[0m : 2.52608
[1mStep[0m  [32/42], [94mLoss[0m : 2.55657
[1mStep[0m  [36/42], [94mLoss[0m : 2.62330
[1mStep[0m  [40/42], [94mLoss[0m : 2.58494

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50690
[1mStep[0m  [4/42], [94mLoss[0m : 2.51754
[1mStep[0m  [8/42], [94mLoss[0m : 2.53204
[1mStep[0m  [12/42], [94mLoss[0m : 2.37164
[1mStep[0m  [16/42], [94mLoss[0m : 2.26142
[1mStep[0m  [20/42], [94mLoss[0m : 2.27503
[1mStep[0m  [24/42], [94mLoss[0m : 2.65552
[1mStep[0m  [28/42], [94mLoss[0m : 2.47582
[1mStep[0m  [32/42], [94mLoss[0m : 2.45285
[1mStep[0m  [36/42], [94mLoss[0m : 2.48487
[1mStep[0m  [40/42], [94mLoss[0m : 2.59486

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26265
[1mStep[0m  [4/42], [94mLoss[0m : 2.59443
[1mStep[0m  [8/42], [94mLoss[0m : 2.37693
[1mStep[0m  [12/42], [94mLoss[0m : 2.55961
[1mStep[0m  [16/42], [94mLoss[0m : 2.52008
[1mStep[0m  [20/42], [94mLoss[0m : 2.46957
[1mStep[0m  [24/42], [94mLoss[0m : 2.46925
[1mStep[0m  [28/42], [94mLoss[0m : 2.87498
[1mStep[0m  [32/42], [94mLoss[0m : 2.61812
[1mStep[0m  [36/42], [94mLoss[0m : 2.81915
[1mStep[0m  [40/42], [94mLoss[0m : 2.60393

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56609
[1mStep[0m  [4/42], [94mLoss[0m : 2.47347
[1mStep[0m  [8/42], [94mLoss[0m : 2.84600
[1mStep[0m  [12/42], [94mLoss[0m : 2.52463
[1mStep[0m  [16/42], [94mLoss[0m : 2.77840
[1mStep[0m  [20/42], [94mLoss[0m : 2.51097
[1mStep[0m  [24/42], [94mLoss[0m : 2.40397
[1mStep[0m  [28/42], [94mLoss[0m : 2.37284
[1mStep[0m  [32/42], [94mLoss[0m : 2.45573
[1mStep[0m  [36/42], [94mLoss[0m : 2.49939
[1mStep[0m  [40/42], [94mLoss[0m : 2.59316

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37605
[1mStep[0m  [4/42], [94mLoss[0m : 2.41865
[1mStep[0m  [8/42], [94mLoss[0m : 2.28519
[1mStep[0m  [12/42], [94mLoss[0m : 2.63791
[1mStep[0m  [16/42], [94mLoss[0m : 2.72691
[1mStep[0m  [20/42], [94mLoss[0m : 2.40705
[1mStep[0m  [24/42], [94mLoss[0m : 2.44503
[1mStep[0m  [28/42], [94mLoss[0m : 2.65992
[1mStep[0m  [32/42], [94mLoss[0m : 2.35990
[1mStep[0m  [36/42], [94mLoss[0m : 2.50883
[1mStep[0m  [40/42], [94mLoss[0m : 2.47014

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54010
[1mStep[0m  [4/42], [94mLoss[0m : 2.67344
[1mStep[0m  [8/42], [94mLoss[0m : 2.54568
[1mStep[0m  [12/42], [94mLoss[0m : 2.55909
[1mStep[0m  [16/42], [94mLoss[0m : 2.35697
[1mStep[0m  [20/42], [94mLoss[0m : 2.49604
[1mStep[0m  [24/42], [94mLoss[0m : 2.42416
[1mStep[0m  [28/42], [94mLoss[0m : 2.55824
[1mStep[0m  [32/42], [94mLoss[0m : 2.51279
[1mStep[0m  [36/42], [94mLoss[0m : 2.64685
[1mStep[0m  [40/42], [94mLoss[0m : 2.59362

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49686
[1mStep[0m  [4/42], [94mLoss[0m : 2.45337
[1mStep[0m  [8/42], [94mLoss[0m : 2.40044
[1mStep[0m  [12/42], [94mLoss[0m : 2.53779
[1mStep[0m  [16/42], [94mLoss[0m : 2.39759
[1mStep[0m  [20/42], [94mLoss[0m : 2.53489
[1mStep[0m  [24/42], [94mLoss[0m : 2.50813
[1mStep[0m  [28/42], [94mLoss[0m : 2.60584
[1mStep[0m  [32/42], [94mLoss[0m : 2.45843
[1mStep[0m  [36/42], [94mLoss[0m : 2.53976
[1mStep[0m  [40/42], [94mLoss[0m : 2.40000

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42408
[1mStep[0m  [4/42], [94mLoss[0m : 2.62863
[1mStep[0m  [8/42], [94mLoss[0m : 2.44912
[1mStep[0m  [12/42], [94mLoss[0m : 2.59801
[1mStep[0m  [16/42], [94mLoss[0m : 2.37381
[1mStep[0m  [20/42], [94mLoss[0m : 2.71469
[1mStep[0m  [24/42], [94mLoss[0m : 2.73451
[1mStep[0m  [28/42], [94mLoss[0m : 2.61933
[1mStep[0m  [32/42], [94mLoss[0m : 2.43212
[1mStep[0m  [36/42], [94mLoss[0m : 2.57638
[1mStep[0m  [40/42], [94mLoss[0m : 2.43822

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56163
[1mStep[0m  [4/42], [94mLoss[0m : 2.74927
[1mStep[0m  [8/42], [94mLoss[0m : 2.75193
[1mStep[0m  [12/42], [94mLoss[0m : 2.48909
[1mStep[0m  [16/42], [94mLoss[0m : 2.58330
[1mStep[0m  [20/42], [94mLoss[0m : 2.38822
[1mStep[0m  [24/42], [94mLoss[0m : 2.34639
[1mStep[0m  [28/42], [94mLoss[0m : 2.56989
[1mStep[0m  [32/42], [94mLoss[0m : 2.46119
[1mStep[0m  [36/42], [94mLoss[0m : 2.61188
[1mStep[0m  [40/42], [94mLoss[0m : 2.42712

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76091
[1mStep[0m  [4/42], [94mLoss[0m : 2.52096
[1mStep[0m  [8/42], [94mLoss[0m : 2.40666
[1mStep[0m  [12/42], [94mLoss[0m : 2.62223
[1mStep[0m  [16/42], [94mLoss[0m : 2.63916
[1mStep[0m  [20/42], [94mLoss[0m : 2.44664
[1mStep[0m  [24/42], [94mLoss[0m : 2.71054
[1mStep[0m  [28/42], [94mLoss[0m : 2.35794
[1mStep[0m  [32/42], [94mLoss[0m : 2.74319
[1mStep[0m  [36/42], [94mLoss[0m : 2.58822
[1mStep[0m  [40/42], [94mLoss[0m : 2.52613

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36673
[1mStep[0m  [4/42], [94mLoss[0m : 2.37480
[1mStep[0m  [8/42], [94mLoss[0m : 2.39093
[1mStep[0m  [12/42], [94mLoss[0m : 2.34821
[1mStep[0m  [16/42], [94mLoss[0m : 2.53065
[1mStep[0m  [20/42], [94mLoss[0m : 2.30646
[1mStep[0m  [24/42], [94mLoss[0m : 2.26652
[1mStep[0m  [28/42], [94mLoss[0m : 2.59845
[1mStep[0m  [32/42], [94mLoss[0m : 2.47695
[1mStep[0m  [36/42], [94mLoss[0m : 2.54629
[1mStep[0m  [40/42], [94mLoss[0m : 2.65729

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61492
[1mStep[0m  [4/42], [94mLoss[0m : 2.58400
[1mStep[0m  [8/42], [94mLoss[0m : 2.49543
[1mStep[0m  [12/42], [94mLoss[0m : 2.58084
[1mStep[0m  [16/42], [94mLoss[0m : 2.47462
[1mStep[0m  [20/42], [94mLoss[0m : 2.52932
[1mStep[0m  [24/42], [94mLoss[0m : 2.56927
[1mStep[0m  [28/42], [94mLoss[0m : 2.51206
[1mStep[0m  [32/42], [94mLoss[0m : 2.29395
[1mStep[0m  [36/42], [94mLoss[0m : 2.43085
[1mStep[0m  [40/42], [94mLoss[0m : 2.56861

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45762
[1mStep[0m  [4/42], [94mLoss[0m : 2.45391
[1mStep[0m  [8/42], [94mLoss[0m : 2.63185
[1mStep[0m  [12/42], [94mLoss[0m : 2.44152
[1mStep[0m  [16/42], [94mLoss[0m : 2.53926
[1mStep[0m  [20/42], [94mLoss[0m : 2.32775
[1mStep[0m  [24/42], [94mLoss[0m : 2.36298
[1mStep[0m  [28/42], [94mLoss[0m : 2.45682
[1mStep[0m  [32/42], [94mLoss[0m : 2.40461
[1mStep[0m  [36/42], [94mLoss[0m : 2.42997
[1mStep[0m  [40/42], [94mLoss[0m : 2.39095

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3372979504721507
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.62097
[1mStep[0m  [4/42], [94mLoss[0m : 2.68203
[1mStep[0m  [8/42], [94mLoss[0m : 2.09679
[1mStep[0m  [12/42], [94mLoss[0m : 2.31823
[1mStep[0m  [16/42], [94mLoss[0m : 2.47991
[1mStep[0m  [20/42], [94mLoss[0m : 2.38235
[1mStep[0m  [24/42], [94mLoss[0m : 2.37165
[1mStep[0m  [28/42], [94mLoss[0m : 2.52802
[1mStep[0m  [32/42], [94mLoss[0m : 2.68412
[1mStep[0m  [36/42], [94mLoss[0m : 2.52414
[1mStep[0m  [40/42], [94mLoss[0m : 2.54731

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43514
[1mStep[0m  [4/42], [94mLoss[0m : 2.28845
[1mStep[0m  [8/42], [94mLoss[0m : 2.41320
[1mStep[0m  [12/42], [94mLoss[0m : 2.60506
[1mStep[0m  [16/42], [94mLoss[0m : 2.48945
[1mStep[0m  [20/42], [94mLoss[0m : 2.48134
[1mStep[0m  [24/42], [94mLoss[0m : 2.40475
[1mStep[0m  [28/42], [94mLoss[0m : 2.51902
[1mStep[0m  [32/42], [94mLoss[0m : 2.55544
[1mStep[0m  [36/42], [94mLoss[0m : 2.32484
[1mStep[0m  [40/42], [94mLoss[0m : 2.60003

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45304
[1mStep[0m  [4/42], [94mLoss[0m : 2.54139
[1mStep[0m  [8/42], [94mLoss[0m : 2.37587
[1mStep[0m  [12/42], [94mLoss[0m : 2.48313
[1mStep[0m  [16/42], [94mLoss[0m : 2.69357
[1mStep[0m  [20/42], [94mLoss[0m : 2.62065
[1mStep[0m  [24/42], [94mLoss[0m : 2.56470
[1mStep[0m  [28/42], [94mLoss[0m : 2.56221
[1mStep[0m  [32/42], [94mLoss[0m : 2.35059
[1mStep[0m  [36/42], [94mLoss[0m : 2.47575
[1mStep[0m  [40/42], [94mLoss[0m : 2.69601

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18135
[1mStep[0m  [4/42], [94mLoss[0m : 2.62441
[1mStep[0m  [8/42], [94mLoss[0m : 2.53308
[1mStep[0m  [12/42], [94mLoss[0m : 2.46330
[1mStep[0m  [16/42], [94mLoss[0m : 2.35725
[1mStep[0m  [20/42], [94mLoss[0m : 2.35479
[1mStep[0m  [24/42], [94mLoss[0m : 2.42208
[1mStep[0m  [28/42], [94mLoss[0m : 2.70718
[1mStep[0m  [32/42], [94mLoss[0m : 2.51055
[1mStep[0m  [36/42], [94mLoss[0m : 2.40663
[1mStep[0m  [40/42], [94mLoss[0m : 2.60792

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67740
[1mStep[0m  [4/42], [94mLoss[0m : 2.57017
[1mStep[0m  [8/42], [94mLoss[0m : 2.62199
[1mStep[0m  [12/42], [94mLoss[0m : 2.43633
[1mStep[0m  [16/42], [94mLoss[0m : 2.54778
[1mStep[0m  [20/42], [94mLoss[0m : 2.43204
[1mStep[0m  [24/42], [94mLoss[0m : 2.51627
[1mStep[0m  [28/42], [94mLoss[0m : 2.48049
[1mStep[0m  [32/42], [94mLoss[0m : 2.58642
[1mStep[0m  [36/42], [94mLoss[0m : 2.33753
[1mStep[0m  [40/42], [94mLoss[0m : 2.28176

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.625, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36521
[1mStep[0m  [4/42], [94mLoss[0m : 2.55240
[1mStep[0m  [8/42], [94mLoss[0m : 2.33511
[1mStep[0m  [12/42], [94mLoss[0m : 2.47681
[1mStep[0m  [16/42], [94mLoss[0m : 2.23209
[1mStep[0m  [20/42], [94mLoss[0m : 2.61499
[1mStep[0m  [24/42], [94mLoss[0m : 2.32148
[1mStep[0m  [28/42], [94mLoss[0m : 2.31983
[1mStep[0m  [32/42], [94mLoss[0m : 2.60117
[1mStep[0m  [36/42], [94mLoss[0m : 2.23603
[1mStep[0m  [40/42], [94mLoss[0m : 2.50133

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.637, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24445
[1mStep[0m  [4/42], [94mLoss[0m : 2.29742
[1mStep[0m  [8/42], [94mLoss[0m : 2.49831
[1mStep[0m  [12/42], [94mLoss[0m : 2.59677
[1mStep[0m  [16/42], [94mLoss[0m : 2.50497
[1mStep[0m  [20/42], [94mLoss[0m : 2.30758
[1mStep[0m  [24/42], [94mLoss[0m : 2.37240
[1mStep[0m  [28/42], [94mLoss[0m : 2.44005
[1mStep[0m  [32/42], [94mLoss[0m : 2.46353
[1mStep[0m  [36/42], [94mLoss[0m : 2.48643
[1mStep[0m  [40/42], [94mLoss[0m : 2.33516

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.608, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40880
[1mStep[0m  [4/42], [94mLoss[0m : 2.51967
[1mStep[0m  [8/42], [94mLoss[0m : 2.38420
[1mStep[0m  [12/42], [94mLoss[0m : 2.27608
[1mStep[0m  [16/42], [94mLoss[0m : 2.48485
[1mStep[0m  [20/42], [94mLoss[0m : 2.53746
[1mStep[0m  [24/42], [94mLoss[0m : 2.45248
[1mStep[0m  [28/42], [94mLoss[0m : 2.35733
[1mStep[0m  [32/42], [94mLoss[0m : 2.32253
[1mStep[0m  [36/42], [94mLoss[0m : 2.38221
[1mStep[0m  [40/42], [94mLoss[0m : 2.64972

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.621, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50590
[1mStep[0m  [4/42], [94mLoss[0m : 2.23993
[1mStep[0m  [8/42], [94mLoss[0m : 2.18372
[1mStep[0m  [12/42], [94mLoss[0m : 2.23166
[1mStep[0m  [16/42], [94mLoss[0m : 2.06805
[1mStep[0m  [20/42], [94mLoss[0m : 2.60540
[1mStep[0m  [24/42], [94mLoss[0m : 2.42383
[1mStep[0m  [28/42], [94mLoss[0m : 2.37152
[1mStep[0m  [32/42], [94mLoss[0m : 2.37341
[1mStep[0m  [36/42], [94mLoss[0m : 2.32548
[1mStep[0m  [40/42], [94mLoss[0m : 2.55432

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.630, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45879
[1mStep[0m  [4/42], [94mLoss[0m : 2.38651
[1mStep[0m  [8/42], [94mLoss[0m : 2.46550
[1mStep[0m  [12/42], [94mLoss[0m : 2.31624
[1mStep[0m  [16/42], [94mLoss[0m : 2.40531
[1mStep[0m  [20/42], [94mLoss[0m : 2.29116
[1mStep[0m  [24/42], [94mLoss[0m : 2.27050
[1mStep[0m  [28/42], [94mLoss[0m : 2.41758
[1mStep[0m  [32/42], [94mLoss[0m : 2.22780
[1mStep[0m  [36/42], [94mLoss[0m : 2.48766
[1mStep[0m  [40/42], [94mLoss[0m : 2.52070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.669, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46889
[1mStep[0m  [4/42], [94mLoss[0m : 2.45759
[1mStep[0m  [8/42], [94mLoss[0m : 2.25618
[1mStep[0m  [12/42], [94mLoss[0m : 2.39085
[1mStep[0m  [16/42], [94mLoss[0m : 2.37609
[1mStep[0m  [20/42], [94mLoss[0m : 2.14948
[1mStep[0m  [24/42], [94mLoss[0m : 2.32145
[1mStep[0m  [28/42], [94mLoss[0m : 2.47035
[1mStep[0m  [32/42], [94mLoss[0m : 2.44739
[1mStep[0m  [36/42], [94mLoss[0m : 2.48899
[1mStep[0m  [40/42], [94mLoss[0m : 2.45815

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.677, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46579
[1mStep[0m  [4/42], [94mLoss[0m : 2.32735
[1mStep[0m  [8/42], [94mLoss[0m : 2.33898
[1mStep[0m  [12/42], [94mLoss[0m : 2.23887
[1mStep[0m  [16/42], [94mLoss[0m : 2.31172
[1mStep[0m  [20/42], [94mLoss[0m : 2.18532
[1mStep[0m  [24/42], [94mLoss[0m : 2.37655
[1mStep[0m  [28/42], [94mLoss[0m : 2.32511
[1mStep[0m  [32/42], [94mLoss[0m : 2.37969
[1mStep[0m  [36/42], [94mLoss[0m : 2.57243
[1mStep[0m  [40/42], [94mLoss[0m : 2.28507

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.679, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45028
[1mStep[0m  [4/42], [94mLoss[0m : 2.26126
[1mStep[0m  [8/42], [94mLoss[0m : 2.27119
[1mStep[0m  [12/42], [94mLoss[0m : 2.47075
[1mStep[0m  [16/42], [94mLoss[0m : 2.30795
[1mStep[0m  [20/42], [94mLoss[0m : 2.51011
[1mStep[0m  [24/42], [94mLoss[0m : 2.25806
[1mStep[0m  [28/42], [94mLoss[0m : 2.44878
[1mStep[0m  [32/42], [94mLoss[0m : 2.28780
[1mStep[0m  [36/42], [94mLoss[0m : 2.24141
[1mStep[0m  [40/42], [94mLoss[0m : 2.33814

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.656, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36744
[1mStep[0m  [4/42], [94mLoss[0m : 2.23699
[1mStep[0m  [8/42], [94mLoss[0m : 2.29782
[1mStep[0m  [12/42], [94mLoss[0m : 2.13543
[1mStep[0m  [16/42], [94mLoss[0m : 2.22979
[1mStep[0m  [20/42], [94mLoss[0m : 2.49117
[1mStep[0m  [24/42], [94mLoss[0m : 2.39692
[1mStep[0m  [28/42], [94mLoss[0m : 2.16048
[1mStep[0m  [32/42], [94mLoss[0m : 2.24695
[1mStep[0m  [36/42], [94mLoss[0m : 2.21271
[1mStep[0m  [40/42], [94mLoss[0m : 2.36308

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.745, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37939
[1mStep[0m  [4/42], [94mLoss[0m : 2.37838
[1mStep[0m  [8/42], [94mLoss[0m : 2.37258
[1mStep[0m  [12/42], [94mLoss[0m : 2.40050
[1mStep[0m  [16/42], [94mLoss[0m : 2.22881
[1mStep[0m  [20/42], [94mLoss[0m : 2.25017
[1mStep[0m  [24/42], [94mLoss[0m : 2.28731
[1mStep[0m  [28/42], [94mLoss[0m : 2.43688
[1mStep[0m  [32/42], [94mLoss[0m : 1.98219
[1mStep[0m  [36/42], [94mLoss[0m : 2.31624
[1mStep[0m  [40/42], [94mLoss[0m : 2.37056

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.689, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24061
[1mStep[0m  [4/42], [94mLoss[0m : 2.28726
[1mStep[0m  [8/42], [94mLoss[0m : 2.21445
[1mStep[0m  [12/42], [94mLoss[0m : 2.47391
[1mStep[0m  [16/42], [94mLoss[0m : 2.02690
[1mStep[0m  [20/42], [94mLoss[0m : 2.16857
[1mStep[0m  [24/42], [94mLoss[0m : 2.37295
[1mStep[0m  [28/42], [94mLoss[0m : 2.25151
[1mStep[0m  [32/42], [94mLoss[0m : 2.46677
[1mStep[0m  [36/42], [94mLoss[0m : 2.14003
[1mStep[0m  [40/42], [94mLoss[0m : 2.20544

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.688, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29678
[1mStep[0m  [4/42], [94mLoss[0m : 2.22392
[1mStep[0m  [8/42], [94mLoss[0m : 2.11622
[1mStep[0m  [12/42], [94mLoss[0m : 2.42839
[1mStep[0m  [16/42], [94mLoss[0m : 2.30037
[1mStep[0m  [20/42], [94mLoss[0m : 2.14993
[1mStep[0m  [24/42], [94mLoss[0m : 2.18890
[1mStep[0m  [28/42], [94mLoss[0m : 2.26518
[1mStep[0m  [32/42], [94mLoss[0m : 2.39868
[1mStep[0m  [36/42], [94mLoss[0m : 2.37919
[1mStep[0m  [40/42], [94mLoss[0m : 2.34236

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.689, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12606
[1mStep[0m  [4/42], [94mLoss[0m : 2.35156
[1mStep[0m  [8/42], [94mLoss[0m : 2.22335
[1mStep[0m  [12/42], [94mLoss[0m : 2.43450
[1mStep[0m  [16/42], [94mLoss[0m : 2.24822
[1mStep[0m  [20/42], [94mLoss[0m : 2.17257
[1mStep[0m  [24/42], [94mLoss[0m : 2.11852
[1mStep[0m  [28/42], [94mLoss[0m : 2.08593
[1mStep[0m  [32/42], [94mLoss[0m : 2.32092
[1mStep[0m  [36/42], [94mLoss[0m : 2.14224
[1mStep[0m  [40/42], [94mLoss[0m : 2.31486

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.655, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.02270
[1mStep[0m  [4/42], [94mLoss[0m : 2.25224
[1mStep[0m  [8/42], [94mLoss[0m : 2.09339
[1mStep[0m  [12/42], [94mLoss[0m : 2.35683
[1mStep[0m  [16/42], [94mLoss[0m : 2.07575
[1mStep[0m  [20/42], [94mLoss[0m : 2.15598
[1mStep[0m  [24/42], [94mLoss[0m : 2.29171
[1mStep[0m  [28/42], [94mLoss[0m : 2.01237
[1mStep[0m  [32/42], [94mLoss[0m : 2.17227
[1mStep[0m  [36/42], [94mLoss[0m : 1.95662
[1mStep[0m  [40/42], [94mLoss[0m : 2.12034

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.642, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12833
[1mStep[0m  [4/42], [94mLoss[0m : 2.14088
[1mStep[0m  [8/42], [94mLoss[0m : 1.96052
[1mStep[0m  [12/42], [94mLoss[0m : 2.28220
[1mStep[0m  [16/42], [94mLoss[0m : 2.34377
[1mStep[0m  [20/42], [94mLoss[0m : 2.06247
[1mStep[0m  [24/42], [94mLoss[0m : 2.30787
[1mStep[0m  [28/42], [94mLoss[0m : 1.93220
[1mStep[0m  [32/42], [94mLoss[0m : 1.96840
[1mStep[0m  [36/42], [94mLoss[0m : 2.20903
[1mStep[0m  [40/42], [94mLoss[0m : 2.29853

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.648, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05637
[1mStep[0m  [4/42], [94mLoss[0m : 2.30155
[1mStep[0m  [8/42], [94mLoss[0m : 2.01782
[1mStep[0m  [12/42], [94mLoss[0m : 2.22532
[1mStep[0m  [16/42], [94mLoss[0m : 2.20054
[1mStep[0m  [20/42], [94mLoss[0m : 2.23629
[1mStep[0m  [24/42], [94mLoss[0m : 2.07255
[1mStep[0m  [28/42], [94mLoss[0m : 2.19873
[1mStep[0m  [32/42], [94mLoss[0m : 2.11764
[1mStep[0m  [36/42], [94mLoss[0m : 2.17882
[1mStep[0m  [40/42], [94mLoss[0m : 2.09938

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.586, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20543
[1mStep[0m  [4/42], [94mLoss[0m : 2.12343
[1mStep[0m  [8/42], [94mLoss[0m : 2.33741
[1mStep[0m  [12/42], [94mLoss[0m : 2.13619
[1mStep[0m  [16/42], [94mLoss[0m : 1.93180
[1mStep[0m  [20/42], [94mLoss[0m : 2.13633
[1mStep[0m  [24/42], [94mLoss[0m : 2.18580
[1mStep[0m  [28/42], [94mLoss[0m : 2.13805
[1mStep[0m  [32/42], [94mLoss[0m : 2.12864
[1mStep[0m  [36/42], [94mLoss[0m : 2.14380
[1mStep[0m  [40/42], [94mLoss[0m : 2.21184

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01886
[1mStep[0m  [4/42], [94mLoss[0m : 1.97664
[1mStep[0m  [8/42], [94mLoss[0m : 2.02166
[1mStep[0m  [12/42], [94mLoss[0m : 2.31783
[1mStep[0m  [16/42], [94mLoss[0m : 1.99966
[1mStep[0m  [20/42], [94mLoss[0m : 2.16107
[1mStep[0m  [24/42], [94mLoss[0m : 2.08516
[1mStep[0m  [28/42], [94mLoss[0m : 1.96811
[1mStep[0m  [32/42], [94mLoss[0m : 2.20287
[1mStep[0m  [36/42], [94mLoss[0m : 2.14866
[1mStep[0m  [40/42], [94mLoss[0m : 2.37233

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.623, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06552
[1mStep[0m  [4/42], [94mLoss[0m : 1.93228
[1mStep[0m  [8/42], [94mLoss[0m : 1.97632
[1mStep[0m  [12/42], [94mLoss[0m : 2.23543
[1mStep[0m  [16/42], [94mLoss[0m : 1.95349
[1mStep[0m  [20/42], [94mLoss[0m : 1.97018
[1mStep[0m  [24/42], [94mLoss[0m : 2.03639
[1mStep[0m  [28/42], [94mLoss[0m : 2.24940
[1mStep[0m  [32/42], [94mLoss[0m : 2.03714
[1mStep[0m  [36/42], [94mLoss[0m : 2.18281
[1mStep[0m  [40/42], [94mLoss[0m : 2.03146

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.674, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13775
[1mStep[0m  [4/42], [94mLoss[0m : 1.96818
[1mStep[0m  [8/42], [94mLoss[0m : 1.95289
[1mStep[0m  [12/42], [94mLoss[0m : 2.14884
[1mStep[0m  [16/42], [94mLoss[0m : 2.03924
[1mStep[0m  [20/42], [94mLoss[0m : 2.03163
[1mStep[0m  [24/42], [94mLoss[0m : 2.12861
[1mStep[0m  [28/42], [94mLoss[0m : 1.91115
[1mStep[0m  [32/42], [94mLoss[0m : 1.98784
[1mStep[0m  [36/42], [94mLoss[0m : 1.95357
[1mStep[0m  [40/42], [94mLoss[0m : 1.99866

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.627, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00018
[1mStep[0m  [4/42], [94mLoss[0m : 2.41052
[1mStep[0m  [8/42], [94mLoss[0m : 2.25921
[1mStep[0m  [12/42], [94mLoss[0m : 2.13817
[1mStep[0m  [16/42], [94mLoss[0m : 2.06954
[1mStep[0m  [20/42], [94mLoss[0m : 2.07448
[1mStep[0m  [24/42], [94mLoss[0m : 1.76521
[1mStep[0m  [28/42], [94mLoss[0m : 2.18205
[1mStep[0m  [32/42], [94mLoss[0m : 2.01495
[1mStep[0m  [36/42], [94mLoss[0m : 2.00307
[1mStep[0m  [40/42], [94mLoss[0m : 2.03201

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.059, [92mTest[0m: 2.543, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07324
[1mStep[0m  [4/42], [94mLoss[0m : 1.92729
[1mStep[0m  [8/42], [94mLoss[0m : 2.07008
[1mStep[0m  [12/42], [94mLoss[0m : 2.10288
[1mStep[0m  [16/42], [94mLoss[0m : 1.82799
[1mStep[0m  [20/42], [94mLoss[0m : 1.95182
[1mStep[0m  [24/42], [94mLoss[0m : 1.75843
[1mStep[0m  [28/42], [94mLoss[0m : 2.03221
[1mStep[0m  [32/42], [94mLoss[0m : 2.13199
[1mStep[0m  [36/42], [94mLoss[0m : 1.84736
[1mStep[0m  [40/42], [94mLoss[0m : 2.00214

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.579, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86611
[1mStep[0m  [4/42], [94mLoss[0m : 1.89825
[1mStep[0m  [8/42], [94mLoss[0m : 1.88458
[1mStep[0m  [12/42], [94mLoss[0m : 2.01644
[1mStep[0m  [16/42], [94mLoss[0m : 1.93743
[1mStep[0m  [20/42], [94mLoss[0m : 1.79317
[1mStep[0m  [24/42], [94mLoss[0m : 2.26689
[1mStep[0m  [28/42], [94mLoss[0m : 1.86263
[1mStep[0m  [32/42], [94mLoss[0m : 1.84114
[1mStep[0m  [36/42], [94mLoss[0m : 1.90326
[1mStep[0m  [40/42], [94mLoss[0m : 2.09753

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07215
[1mStep[0m  [4/42], [94mLoss[0m : 1.91688
[1mStep[0m  [8/42], [94mLoss[0m : 1.81265
[1mStep[0m  [12/42], [94mLoss[0m : 1.87642
[1mStep[0m  [16/42], [94mLoss[0m : 2.13841
[1mStep[0m  [20/42], [94mLoss[0m : 1.82381
[1mStep[0m  [24/42], [94mLoss[0m : 2.02367
[1mStep[0m  [28/42], [94mLoss[0m : 1.82798
[1mStep[0m  [32/42], [94mLoss[0m : 2.04788
[1mStep[0m  [36/42], [94mLoss[0m : 2.24668
[1mStep[0m  [40/42], [94mLoss[0m : 1.95484

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94262
[1mStep[0m  [4/42], [94mLoss[0m : 1.83646
[1mStep[0m  [8/42], [94mLoss[0m : 1.99167
[1mStep[0m  [12/42], [94mLoss[0m : 1.92163
[1mStep[0m  [16/42], [94mLoss[0m : 1.94940
[1mStep[0m  [20/42], [94mLoss[0m : 1.97273
[1mStep[0m  [24/42], [94mLoss[0m : 1.87657
[1mStep[0m  [28/42], [94mLoss[0m : 2.00410
[1mStep[0m  [32/42], [94mLoss[0m : 2.04599
[1mStep[0m  [36/42], [94mLoss[0m : 1.92918
[1mStep[0m  [40/42], [94mLoss[0m : 1.95622

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.527
====================================

Phase 2 - Evaluation MAE:  2.526851330484663
MAE score P1       2.337298
MAE score P2       2.526851
loss               1.960037
learning_rate       0.00505
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.43250
[1mStep[0m  [4/42], [94mLoss[0m : 8.74066
[1mStep[0m  [8/42], [94mLoss[0m : 5.32577
[1mStep[0m  [12/42], [94mLoss[0m : 3.14507
[1mStep[0m  [16/42], [94mLoss[0m : 3.25808
[1mStep[0m  [20/42], [94mLoss[0m : 3.77741
[1mStep[0m  [24/42], [94mLoss[0m : 3.00640
[1mStep[0m  [28/42], [94mLoss[0m : 2.89894
[1mStep[0m  [32/42], [94mLoss[0m : 2.72227
[1mStep[0m  [36/42], [94mLoss[0m : 2.53465
[1mStep[0m  [40/42], [94mLoss[0m : 2.61063

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.224, [92mTest[0m: 10.851, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82836
[1mStep[0m  [4/42], [94mLoss[0m : 2.59432
[1mStep[0m  [8/42], [94mLoss[0m : 2.72385
[1mStep[0m  [12/42], [94mLoss[0m : 2.54673
[1mStep[0m  [16/42], [94mLoss[0m : 2.51426
[1mStep[0m  [20/42], [94mLoss[0m : 2.91921
[1mStep[0m  [24/42], [94mLoss[0m : 2.70337
[1mStep[0m  [28/42], [94mLoss[0m : 2.42551
[1mStep[0m  [32/42], [94mLoss[0m : 2.39251
[1mStep[0m  [36/42], [94mLoss[0m : 2.31571
[1mStep[0m  [40/42], [94mLoss[0m : 2.56387

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.762, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73022
[1mStep[0m  [4/42], [94mLoss[0m : 2.52507
[1mStep[0m  [8/42], [94mLoss[0m : 2.57657
[1mStep[0m  [12/42], [94mLoss[0m : 2.40236
[1mStep[0m  [16/42], [94mLoss[0m : 2.52108
[1mStep[0m  [20/42], [94mLoss[0m : 2.26541
[1mStep[0m  [24/42], [94mLoss[0m : 2.44208
[1mStep[0m  [28/42], [94mLoss[0m : 2.52649
[1mStep[0m  [32/42], [94mLoss[0m : 2.35024
[1mStep[0m  [36/42], [94mLoss[0m : 2.42280
[1mStep[0m  [40/42], [94mLoss[0m : 2.40773

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43847
[1mStep[0m  [4/42], [94mLoss[0m : 2.51474
[1mStep[0m  [8/42], [94mLoss[0m : 2.54455
[1mStep[0m  [12/42], [94mLoss[0m : 2.55985
[1mStep[0m  [16/42], [94mLoss[0m : 2.49713
[1mStep[0m  [20/42], [94mLoss[0m : 2.55016
[1mStep[0m  [24/42], [94mLoss[0m : 2.56899
[1mStep[0m  [28/42], [94mLoss[0m : 2.16891
[1mStep[0m  [32/42], [94mLoss[0m : 2.31069
[1mStep[0m  [36/42], [94mLoss[0m : 2.47878
[1mStep[0m  [40/42], [94mLoss[0m : 2.31204

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40333
[1mStep[0m  [4/42], [94mLoss[0m : 2.39899
[1mStep[0m  [8/42], [94mLoss[0m : 2.24469
[1mStep[0m  [12/42], [94mLoss[0m : 2.56521
[1mStep[0m  [16/42], [94mLoss[0m : 2.38364
[1mStep[0m  [20/42], [94mLoss[0m : 2.53837
[1mStep[0m  [24/42], [94mLoss[0m : 2.46724
[1mStep[0m  [28/42], [94mLoss[0m : 2.47592
[1mStep[0m  [32/42], [94mLoss[0m : 2.41883
[1mStep[0m  [36/42], [94mLoss[0m : 2.54476
[1mStep[0m  [40/42], [94mLoss[0m : 2.42357

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05162
[1mStep[0m  [4/42], [94mLoss[0m : 2.49010
[1mStep[0m  [8/42], [94mLoss[0m : 2.54197
[1mStep[0m  [12/42], [94mLoss[0m : 2.35777
[1mStep[0m  [16/42], [94mLoss[0m : 2.30067
[1mStep[0m  [20/42], [94mLoss[0m : 2.58422
[1mStep[0m  [24/42], [94mLoss[0m : 2.36474
[1mStep[0m  [28/42], [94mLoss[0m : 2.47647
[1mStep[0m  [32/42], [94mLoss[0m : 2.27390
[1mStep[0m  [36/42], [94mLoss[0m : 2.36995
[1mStep[0m  [40/42], [94mLoss[0m : 2.30014

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51448
[1mStep[0m  [4/42], [94mLoss[0m : 2.37403
[1mStep[0m  [8/42], [94mLoss[0m : 2.32889
[1mStep[0m  [12/42], [94mLoss[0m : 2.33680
[1mStep[0m  [16/42], [94mLoss[0m : 2.25438
[1mStep[0m  [20/42], [94mLoss[0m : 2.50226
[1mStep[0m  [24/42], [94mLoss[0m : 2.25471
[1mStep[0m  [28/42], [94mLoss[0m : 2.47449
[1mStep[0m  [32/42], [94mLoss[0m : 2.32853
[1mStep[0m  [36/42], [94mLoss[0m : 2.30968
[1mStep[0m  [40/42], [94mLoss[0m : 2.22761

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35490
[1mStep[0m  [4/42], [94mLoss[0m : 2.41425
[1mStep[0m  [8/42], [94mLoss[0m : 2.50219
[1mStep[0m  [12/42], [94mLoss[0m : 2.35033
[1mStep[0m  [16/42], [94mLoss[0m : 2.36984
[1mStep[0m  [20/42], [94mLoss[0m : 2.48124
[1mStep[0m  [24/42], [94mLoss[0m : 2.09913
[1mStep[0m  [28/42], [94mLoss[0m : 2.41016
[1mStep[0m  [32/42], [94mLoss[0m : 2.26927
[1mStep[0m  [36/42], [94mLoss[0m : 2.46872
[1mStep[0m  [40/42], [94mLoss[0m : 2.33068

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28298
[1mStep[0m  [4/42], [94mLoss[0m : 2.26289
[1mStep[0m  [8/42], [94mLoss[0m : 2.47442
[1mStep[0m  [12/42], [94mLoss[0m : 2.16990
[1mStep[0m  [16/42], [94mLoss[0m : 2.46401
[1mStep[0m  [20/42], [94mLoss[0m : 2.47181
[1mStep[0m  [24/42], [94mLoss[0m : 2.48305
[1mStep[0m  [28/42], [94mLoss[0m : 2.41602
[1mStep[0m  [32/42], [94mLoss[0m : 2.29723
[1mStep[0m  [36/42], [94mLoss[0m : 2.32431
[1mStep[0m  [40/42], [94mLoss[0m : 2.46373

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21089
[1mStep[0m  [4/42], [94mLoss[0m : 2.61405
[1mStep[0m  [8/42], [94mLoss[0m : 2.22688
[1mStep[0m  [12/42], [94mLoss[0m : 2.27823
[1mStep[0m  [16/42], [94mLoss[0m : 2.26577
[1mStep[0m  [20/42], [94mLoss[0m : 2.03620
[1mStep[0m  [24/42], [94mLoss[0m : 2.29316
[1mStep[0m  [28/42], [94mLoss[0m : 2.67036
[1mStep[0m  [32/42], [94mLoss[0m : 2.58639
[1mStep[0m  [36/42], [94mLoss[0m : 2.34119
[1mStep[0m  [40/42], [94mLoss[0m : 2.39204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42174
[1mStep[0m  [4/42], [94mLoss[0m : 2.37139
[1mStep[0m  [8/42], [94mLoss[0m : 2.37717
[1mStep[0m  [12/42], [94mLoss[0m : 2.23076
[1mStep[0m  [16/42], [94mLoss[0m : 2.33180
[1mStep[0m  [20/42], [94mLoss[0m : 2.25059
[1mStep[0m  [24/42], [94mLoss[0m : 2.24225
[1mStep[0m  [28/42], [94mLoss[0m : 2.39693
[1mStep[0m  [32/42], [94mLoss[0m : 2.26401
[1mStep[0m  [36/42], [94mLoss[0m : 2.26292
[1mStep[0m  [40/42], [94mLoss[0m : 2.34892

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46468
[1mStep[0m  [4/42], [94mLoss[0m : 2.38601
[1mStep[0m  [8/42], [94mLoss[0m : 2.57119
[1mStep[0m  [12/42], [94mLoss[0m : 2.30093
[1mStep[0m  [16/42], [94mLoss[0m : 2.13088
[1mStep[0m  [20/42], [94mLoss[0m : 2.40403
[1mStep[0m  [24/42], [94mLoss[0m : 2.45155
[1mStep[0m  [28/42], [94mLoss[0m : 2.44143
[1mStep[0m  [32/42], [94mLoss[0m : 2.19371
[1mStep[0m  [36/42], [94mLoss[0m : 2.41268
[1mStep[0m  [40/42], [94mLoss[0m : 2.29965

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33364
[1mStep[0m  [4/42], [94mLoss[0m : 2.25679
[1mStep[0m  [8/42], [94mLoss[0m : 2.51492
[1mStep[0m  [12/42], [94mLoss[0m : 2.08816
[1mStep[0m  [16/42], [94mLoss[0m : 2.42942
[1mStep[0m  [20/42], [94mLoss[0m : 2.41803
[1mStep[0m  [24/42], [94mLoss[0m : 2.32723
[1mStep[0m  [28/42], [94mLoss[0m : 2.48830
[1mStep[0m  [32/42], [94mLoss[0m : 2.52192
[1mStep[0m  [36/42], [94mLoss[0m : 2.36800
[1mStep[0m  [40/42], [94mLoss[0m : 2.28142

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52859
[1mStep[0m  [4/42], [94mLoss[0m : 2.61200
[1mStep[0m  [8/42], [94mLoss[0m : 2.38266
[1mStep[0m  [12/42], [94mLoss[0m : 2.10687
[1mStep[0m  [16/42], [94mLoss[0m : 2.24698
[1mStep[0m  [20/42], [94mLoss[0m : 2.52449
[1mStep[0m  [24/42], [94mLoss[0m : 2.48109
[1mStep[0m  [28/42], [94mLoss[0m : 2.05569
[1mStep[0m  [32/42], [94mLoss[0m : 2.22693
[1mStep[0m  [36/42], [94mLoss[0m : 2.31971
[1mStep[0m  [40/42], [94mLoss[0m : 2.32560

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31583
[1mStep[0m  [4/42], [94mLoss[0m : 2.17885
[1mStep[0m  [8/42], [94mLoss[0m : 2.22196
[1mStep[0m  [12/42], [94mLoss[0m : 2.34150
[1mStep[0m  [16/42], [94mLoss[0m : 2.67463
[1mStep[0m  [20/42], [94mLoss[0m : 2.47132
[1mStep[0m  [24/42], [94mLoss[0m : 2.34910
[1mStep[0m  [28/42], [94mLoss[0m : 2.39934
[1mStep[0m  [32/42], [94mLoss[0m : 2.37987
[1mStep[0m  [36/42], [94mLoss[0m : 2.60810
[1mStep[0m  [40/42], [94mLoss[0m : 2.33112

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42564
[1mStep[0m  [4/42], [94mLoss[0m : 2.16647
[1mStep[0m  [8/42], [94mLoss[0m : 2.34425
[1mStep[0m  [12/42], [94mLoss[0m : 2.33106
[1mStep[0m  [16/42], [94mLoss[0m : 2.33404
[1mStep[0m  [20/42], [94mLoss[0m : 2.34477
[1mStep[0m  [24/42], [94mLoss[0m : 2.38833
[1mStep[0m  [28/42], [94mLoss[0m : 2.50272
[1mStep[0m  [32/42], [94mLoss[0m : 2.30055
[1mStep[0m  [36/42], [94mLoss[0m : 2.30012
[1mStep[0m  [40/42], [94mLoss[0m : 2.06242

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10689
[1mStep[0m  [4/42], [94mLoss[0m : 2.30037
[1mStep[0m  [8/42], [94mLoss[0m : 2.47058
[1mStep[0m  [12/42], [94mLoss[0m : 2.26711
[1mStep[0m  [16/42], [94mLoss[0m : 2.23049
[1mStep[0m  [20/42], [94mLoss[0m : 2.22660
[1mStep[0m  [24/42], [94mLoss[0m : 2.21860
[1mStep[0m  [28/42], [94mLoss[0m : 2.36531
[1mStep[0m  [32/42], [94mLoss[0m : 2.17842
[1mStep[0m  [36/42], [94mLoss[0m : 2.08125
[1mStep[0m  [40/42], [94mLoss[0m : 2.50020

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24387
[1mStep[0m  [4/42], [94mLoss[0m : 2.40030
[1mStep[0m  [8/42], [94mLoss[0m : 2.18864
[1mStep[0m  [12/42], [94mLoss[0m : 2.43028
[1mStep[0m  [16/42], [94mLoss[0m : 2.28963
[1mStep[0m  [20/42], [94mLoss[0m : 2.12854
[1mStep[0m  [24/42], [94mLoss[0m : 2.38359
[1mStep[0m  [28/42], [94mLoss[0m : 2.30651
[1mStep[0m  [32/42], [94mLoss[0m : 2.26719
[1mStep[0m  [36/42], [94mLoss[0m : 2.29317
[1mStep[0m  [40/42], [94mLoss[0m : 2.27918

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.315, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37734
[1mStep[0m  [4/42], [94mLoss[0m : 2.35128
[1mStep[0m  [8/42], [94mLoss[0m : 2.51246
[1mStep[0m  [12/42], [94mLoss[0m : 2.14372
[1mStep[0m  [16/42], [94mLoss[0m : 2.41317
[1mStep[0m  [20/42], [94mLoss[0m : 2.44175
[1mStep[0m  [24/42], [94mLoss[0m : 2.43408
[1mStep[0m  [28/42], [94mLoss[0m : 2.38415
[1mStep[0m  [32/42], [94mLoss[0m : 2.44112
[1mStep[0m  [36/42], [94mLoss[0m : 2.28733
[1mStep[0m  [40/42], [94mLoss[0m : 2.39566

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20935
[1mStep[0m  [4/42], [94mLoss[0m : 2.19710
[1mStep[0m  [8/42], [94mLoss[0m : 2.42971
[1mStep[0m  [12/42], [94mLoss[0m : 2.27438
[1mStep[0m  [16/42], [94mLoss[0m : 2.20246
[1mStep[0m  [20/42], [94mLoss[0m : 2.39826
[1mStep[0m  [24/42], [94mLoss[0m : 2.29426
[1mStep[0m  [28/42], [94mLoss[0m : 2.39723
[1mStep[0m  [32/42], [94mLoss[0m : 2.13509
[1mStep[0m  [36/42], [94mLoss[0m : 2.20955
[1mStep[0m  [40/42], [94mLoss[0m : 2.37125

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15986
[1mStep[0m  [4/42], [94mLoss[0m : 2.03484
[1mStep[0m  [8/42], [94mLoss[0m : 2.35149
[1mStep[0m  [12/42], [94mLoss[0m : 2.49587
[1mStep[0m  [16/42], [94mLoss[0m : 2.34445
[1mStep[0m  [20/42], [94mLoss[0m : 2.30718
[1mStep[0m  [24/42], [94mLoss[0m : 2.02611
[1mStep[0m  [28/42], [94mLoss[0m : 2.34632
[1mStep[0m  [32/42], [94mLoss[0m : 2.41229
[1mStep[0m  [36/42], [94mLoss[0m : 2.50587
[1mStep[0m  [40/42], [94mLoss[0m : 2.22272

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.371, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20873
[1mStep[0m  [4/42], [94mLoss[0m : 2.28784
[1mStep[0m  [8/42], [94mLoss[0m : 2.33238
[1mStep[0m  [12/42], [94mLoss[0m : 2.17074
[1mStep[0m  [16/42], [94mLoss[0m : 2.31976
[1mStep[0m  [20/42], [94mLoss[0m : 2.41217
[1mStep[0m  [24/42], [94mLoss[0m : 2.08277
[1mStep[0m  [28/42], [94mLoss[0m : 2.36495
[1mStep[0m  [32/42], [94mLoss[0m : 2.45054
[1mStep[0m  [36/42], [94mLoss[0m : 2.31498
[1mStep[0m  [40/42], [94mLoss[0m : 2.50412

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43215
[1mStep[0m  [4/42], [94mLoss[0m : 2.11776
[1mStep[0m  [8/42], [94mLoss[0m : 2.20490
[1mStep[0m  [12/42], [94mLoss[0m : 2.14767
[1mStep[0m  [16/42], [94mLoss[0m : 2.27623
[1mStep[0m  [20/42], [94mLoss[0m : 2.24227
[1mStep[0m  [24/42], [94mLoss[0m : 2.44361
[1mStep[0m  [28/42], [94mLoss[0m : 2.20859
[1mStep[0m  [32/42], [94mLoss[0m : 2.35023
[1mStep[0m  [36/42], [94mLoss[0m : 2.32199
[1mStep[0m  [40/42], [94mLoss[0m : 2.40797

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36413
[1mStep[0m  [4/42], [94mLoss[0m : 2.23267
[1mStep[0m  [8/42], [94mLoss[0m : 2.39438
[1mStep[0m  [12/42], [94mLoss[0m : 2.51853
[1mStep[0m  [16/42], [94mLoss[0m : 2.29254
[1mStep[0m  [20/42], [94mLoss[0m : 2.14968
[1mStep[0m  [24/42], [94mLoss[0m : 2.21038
[1mStep[0m  [28/42], [94mLoss[0m : 2.47575
[1mStep[0m  [32/42], [94mLoss[0m : 2.38134
[1mStep[0m  [36/42], [94mLoss[0m : 2.34983
[1mStep[0m  [40/42], [94mLoss[0m : 2.36500

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.305, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11253
[1mStep[0m  [4/42], [94mLoss[0m : 1.93952
[1mStep[0m  [8/42], [94mLoss[0m : 2.44111
[1mStep[0m  [12/42], [94mLoss[0m : 2.21971
[1mStep[0m  [16/42], [94mLoss[0m : 2.31402
[1mStep[0m  [20/42], [94mLoss[0m : 2.07550
[1mStep[0m  [24/42], [94mLoss[0m : 2.08473
[1mStep[0m  [28/42], [94mLoss[0m : 2.34036
[1mStep[0m  [32/42], [94mLoss[0m : 2.20218
[1mStep[0m  [36/42], [94mLoss[0m : 2.15178
[1mStep[0m  [40/42], [94mLoss[0m : 2.36215

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.317, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49986
[1mStep[0m  [4/42], [94mLoss[0m : 2.26127
[1mStep[0m  [8/42], [94mLoss[0m : 2.11858
[1mStep[0m  [12/42], [94mLoss[0m : 2.37556
[1mStep[0m  [16/42], [94mLoss[0m : 2.26197
[1mStep[0m  [20/42], [94mLoss[0m : 2.19855
[1mStep[0m  [24/42], [94mLoss[0m : 2.23446
[1mStep[0m  [28/42], [94mLoss[0m : 2.38382
[1mStep[0m  [32/42], [94mLoss[0m : 2.16387
[1mStep[0m  [36/42], [94mLoss[0m : 2.19604
[1mStep[0m  [40/42], [94mLoss[0m : 2.31024

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.306, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90737
[1mStep[0m  [4/42], [94mLoss[0m : 2.39513
[1mStep[0m  [8/42], [94mLoss[0m : 2.28204
[1mStep[0m  [12/42], [94mLoss[0m : 2.13857
[1mStep[0m  [16/42], [94mLoss[0m : 2.22687
[1mStep[0m  [20/42], [94mLoss[0m : 2.15401
[1mStep[0m  [24/42], [94mLoss[0m : 2.11194
[1mStep[0m  [28/42], [94mLoss[0m : 2.15991
[1mStep[0m  [32/42], [94mLoss[0m : 2.23864
[1mStep[0m  [36/42], [94mLoss[0m : 2.18327
[1mStep[0m  [40/42], [94mLoss[0m : 2.37019

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18669
[1mStep[0m  [4/42], [94mLoss[0m : 2.29566
[1mStep[0m  [8/42], [94mLoss[0m : 2.17614
[1mStep[0m  [12/42], [94mLoss[0m : 2.18788
[1mStep[0m  [16/42], [94mLoss[0m : 2.18557
[1mStep[0m  [20/42], [94mLoss[0m : 2.20153
[1mStep[0m  [24/42], [94mLoss[0m : 2.24381
[1mStep[0m  [28/42], [94mLoss[0m : 2.26810
[1mStep[0m  [32/42], [94mLoss[0m : 2.11272
[1mStep[0m  [36/42], [94mLoss[0m : 2.23822
[1mStep[0m  [40/42], [94mLoss[0m : 2.25524

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.309, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14859
[1mStep[0m  [4/42], [94mLoss[0m : 2.24334
[1mStep[0m  [8/42], [94mLoss[0m : 2.28477
[1mStep[0m  [12/42], [94mLoss[0m : 2.22311
[1mStep[0m  [16/42], [94mLoss[0m : 2.50171
[1mStep[0m  [20/42], [94mLoss[0m : 2.20249
[1mStep[0m  [24/42], [94mLoss[0m : 2.23543
[1mStep[0m  [28/42], [94mLoss[0m : 2.25449
[1mStep[0m  [32/42], [94mLoss[0m : 2.17698
[1mStep[0m  [36/42], [94mLoss[0m : 2.38909
[1mStep[0m  [40/42], [94mLoss[0m : 2.36853

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.261, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25379
[1mStep[0m  [4/42], [94mLoss[0m : 2.31210
[1mStep[0m  [8/42], [94mLoss[0m : 2.18606
[1mStep[0m  [12/42], [94mLoss[0m : 2.56975
[1mStep[0m  [16/42], [94mLoss[0m : 2.16829
[1mStep[0m  [20/42], [94mLoss[0m : 2.41430
[1mStep[0m  [24/42], [94mLoss[0m : 2.36370
[1mStep[0m  [28/42], [94mLoss[0m : 2.27165
[1mStep[0m  [32/42], [94mLoss[0m : 2.44299
[1mStep[0m  [36/42], [94mLoss[0m : 2.26630
[1mStep[0m  [40/42], [94mLoss[0m : 2.31650

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.293, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.305
====================================

Phase 1 - Evaluation MAE:  2.304790564945766
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.20283
[1mStep[0m  [4/42], [94mLoss[0m : 2.26408
[1mStep[0m  [8/42], [94mLoss[0m : 2.19682
[1mStep[0m  [12/42], [94mLoss[0m : 2.17146
[1mStep[0m  [16/42], [94mLoss[0m : 2.35645
[1mStep[0m  [20/42], [94mLoss[0m : 2.53701
[1mStep[0m  [24/42], [94mLoss[0m : 2.45347
[1mStep[0m  [28/42], [94mLoss[0m : 2.76230
[1mStep[0m  [32/42], [94mLoss[0m : 2.25334
[1mStep[0m  [36/42], [94mLoss[0m : 2.26453
[1mStep[0m  [40/42], [94mLoss[0m : 2.47404

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.302, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24243
[1mStep[0m  [4/42], [94mLoss[0m : 2.24240
[1mStep[0m  [8/42], [94mLoss[0m : 2.53388
[1mStep[0m  [12/42], [94mLoss[0m : 2.55046
[1mStep[0m  [16/42], [94mLoss[0m : 2.21574
[1mStep[0m  [20/42], [94mLoss[0m : 2.24869
[1mStep[0m  [24/42], [94mLoss[0m : 2.48459
[1mStep[0m  [28/42], [94mLoss[0m : 2.38172
[1mStep[0m  [32/42], [94mLoss[0m : 2.18615
[1mStep[0m  [36/42], [94mLoss[0m : 2.19271
[1mStep[0m  [40/42], [94mLoss[0m : 2.34897

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15787
[1mStep[0m  [4/42], [94mLoss[0m : 2.02162
[1mStep[0m  [8/42], [94mLoss[0m : 2.16754
[1mStep[0m  [12/42], [94mLoss[0m : 2.17218
[1mStep[0m  [16/42], [94mLoss[0m : 2.24235
[1mStep[0m  [20/42], [94mLoss[0m : 2.14437
[1mStep[0m  [24/42], [94mLoss[0m : 2.17749
[1mStep[0m  [28/42], [94mLoss[0m : 2.14363
[1mStep[0m  [32/42], [94mLoss[0m : 2.30606
[1mStep[0m  [36/42], [94mLoss[0m : 2.12226
[1mStep[0m  [40/42], [94mLoss[0m : 2.15819

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01741
[1mStep[0m  [4/42], [94mLoss[0m : 1.94009
[1mStep[0m  [8/42], [94mLoss[0m : 2.04506
[1mStep[0m  [12/42], [94mLoss[0m : 2.02313
[1mStep[0m  [16/42], [94mLoss[0m : 1.95305
[1mStep[0m  [20/42], [94mLoss[0m : 2.10575
[1mStep[0m  [24/42], [94mLoss[0m : 2.05233
[1mStep[0m  [28/42], [94mLoss[0m : 2.18314
[1mStep[0m  [32/42], [94mLoss[0m : 2.19557
[1mStep[0m  [36/42], [94mLoss[0m : 2.25258
[1mStep[0m  [40/42], [94mLoss[0m : 2.14128

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73459
[1mStep[0m  [4/42], [94mLoss[0m : 1.99567
[1mStep[0m  [8/42], [94mLoss[0m : 1.90073
[1mStep[0m  [12/42], [94mLoss[0m : 2.16155
[1mStep[0m  [16/42], [94mLoss[0m : 1.76451
[1mStep[0m  [20/42], [94mLoss[0m : 1.85653
[1mStep[0m  [24/42], [94mLoss[0m : 1.94418
[1mStep[0m  [28/42], [94mLoss[0m : 1.96038
[1mStep[0m  [32/42], [94mLoss[0m : 1.94462
[1mStep[0m  [36/42], [94mLoss[0m : 2.16690
[1mStep[0m  [40/42], [94mLoss[0m : 2.13496

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74220
[1mStep[0m  [4/42], [94mLoss[0m : 1.79293
[1mStep[0m  [8/42], [94mLoss[0m : 1.84362
[1mStep[0m  [12/42], [94mLoss[0m : 1.72499
[1mStep[0m  [16/42], [94mLoss[0m : 1.96202
[1mStep[0m  [20/42], [94mLoss[0m : 1.96277
[1mStep[0m  [24/42], [94mLoss[0m : 1.94089
[1mStep[0m  [28/42], [94mLoss[0m : 1.96060
[1mStep[0m  [32/42], [94mLoss[0m : 1.95224
[1mStep[0m  [36/42], [94mLoss[0m : 2.07804
[1mStep[0m  [40/42], [94mLoss[0m : 1.92520

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70837
[1mStep[0m  [4/42], [94mLoss[0m : 1.72377
[1mStep[0m  [8/42], [94mLoss[0m : 1.82299
[1mStep[0m  [12/42], [94mLoss[0m : 1.81624
[1mStep[0m  [16/42], [94mLoss[0m : 1.92191
[1mStep[0m  [20/42], [94mLoss[0m : 1.85409
[1mStep[0m  [24/42], [94mLoss[0m : 1.81429
[1mStep[0m  [28/42], [94mLoss[0m : 1.70076
[1mStep[0m  [32/42], [94mLoss[0m : 1.87199
[1mStep[0m  [36/42], [94mLoss[0m : 1.83025
[1mStep[0m  [40/42], [94mLoss[0m : 1.95289

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66107
[1mStep[0m  [4/42], [94mLoss[0m : 1.82013
[1mStep[0m  [8/42], [94mLoss[0m : 1.77958
[1mStep[0m  [12/42], [94mLoss[0m : 1.60778
[1mStep[0m  [16/42], [94mLoss[0m : 1.66817
[1mStep[0m  [20/42], [94mLoss[0m : 1.70082
[1mStep[0m  [24/42], [94mLoss[0m : 1.61997
[1mStep[0m  [28/42], [94mLoss[0m : 1.89171
[1mStep[0m  [32/42], [94mLoss[0m : 1.72931
[1mStep[0m  [36/42], [94mLoss[0m : 1.61382
[1mStep[0m  [40/42], [94mLoss[0m : 1.97381

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63319
[1mStep[0m  [4/42], [94mLoss[0m : 1.73617
[1mStep[0m  [8/42], [94mLoss[0m : 1.45417
[1mStep[0m  [12/42], [94mLoss[0m : 1.77409
[1mStep[0m  [16/42], [94mLoss[0m : 1.73328
[1mStep[0m  [20/42], [94mLoss[0m : 1.74049
[1mStep[0m  [24/42], [94mLoss[0m : 1.93043
[1mStep[0m  [28/42], [94mLoss[0m : 1.74926
[1mStep[0m  [32/42], [94mLoss[0m : 1.75726
[1mStep[0m  [36/42], [94mLoss[0m : 1.58340
[1mStep[0m  [40/42], [94mLoss[0m : 1.65484

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.47513
[1mStep[0m  [4/42], [94mLoss[0m : 1.58418
[1mStep[0m  [8/42], [94mLoss[0m : 1.63274
[1mStep[0m  [12/42], [94mLoss[0m : 1.64815
[1mStep[0m  [16/42], [94mLoss[0m : 1.69749
[1mStep[0m  [20/42], [94mLoss[0m : 1.64183
[1mStep[0m  [24/42], [94mLoss[0m : 1.79013
[1mStep[0m  [28/42], [94mLoss[0m : 1.77929
[1mStep[0m  [32/42], [94mLoss[0m : 1.64330
[1mStep[0m  [36/42], [94mLoss[0m : 1.61872
[1mStep[0m  [40/42], [94mLoss[0m : 1.65835

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69314
[1mStep[0m  [4/42], [94mLoss[0m : 1.52032
[1mStep[0m  [8/42], [94mLoss[0m : 1.58027
[1mStep[0m  [12/42], [94mLoss[0m : 1.64260
[1mStep[0m  [16/42], [94mLoss[0m : 1.61491
[1mStep[0m  [20/42], [94mLoss[0m : 1.45159
[1mStep[0m  [24/42], [94mLoss[0m : 1.61893
[1mStep[0m  [28/42], [94mLoss[0m : 1.51925
[1mStep[0m  [32/42], [94mLoss[0m : 1.56741
[1mStep[0m  [36/42], [94mLoss[0m : 1.55537
[1mStep[0m  [40/42], [94mLoss[0m : 1.65698

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.603, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42701
[1mStep[0m  [4/42], [94mLoss[0m : 1.54613
[1mStep[0m  [8/42], [94mLoss[0m : 1.46419
[1mStep[0m  [12/42], [94mLoss[0m : 1.41207
[1mStep[0m  [16/42], [94mLoss[0m : 1.43894
[1mStep[0m  [20/42], [94mLoss[0m : 1.65984
[1mStep[0m  [24/42], [94mLoss[0m : 1.41886
[1mStep[0m  [28/42], [94mLoss[0m : 1.57794
[1mStep[0m  [32/42], [94mLoss[0m : 1.60213
[1mStep[0m  [36/42], [94mLoss[0m : 1.74039
[1mStep[0m  [40/42], [94mLoss[0m : 1.53834

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.567, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42828
[1mStep[0m  [4/42], [94mLoss[0m : 1.37320
[1mStep[0m  [8/42], [94mLoss[0m : 1.49064
[1mStep[0m  [12/42], [94mLoss[0m : 1.48062
[1mStep[0m  [16/42], [94mLoss[0m : 1.69161
[1mStep[0m  [20/42], [94mLoss[0m : 1.59208
[1mStep[0m  [24/42], [94mLoss[0m : 1.43977
[1mStep[0m  [28/42], [94mLoss[0m : 1.49661
[1mStep[0m  [32/42], [94mLoss[0m : 1.54349
[1mStep[0m  [36/42], [94mLoss[0m : 1.52352
[1mStep[0m  [40/42], [94mLoss[0m : 1.60158

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.554, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.32025
[1mStep[0m  [4/42], [94mLoss[0m : 1.55737
[1mStep[0m  [8/42], [94mLoss[0m : 1.49768
[1mStep[0m  [12/42], [94mLoss[0m : 1.62622
[1mStep[0m  [16/42], [94mLoss[0m : 1.35205
[1mStep[0m  [20/42], [94mLoss[0m : 1.53505
[1mStep[0m  [24/42], [94mLoss[0m : 1.50984
[1mStep[0m  [28/42], [94mLoss[0m : 1.48653
[1mStep[0m  [32/42], [94mLoss[0m : 1.63733
[1mStep[0m  [36/42], [94mLoss[0m : 1.60963
[1mStep[0m  [40/42], [94mLoss[0m : 1.64745

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.504, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.34734
[1mStep[0m  [4/42], [94mLoss[0m : 1.44566
[1mStep[0m  [8/42], [94mLoss[0m : 1.47568
[1mStep[0m  [12/42], [94mLoss[0m : 1.28797
[1mStep[0m  [16/42], [94mLoss[0m : 1.51515
[1mStep[0m  [20/42], [94mLoss[0m : 1.48257
[1mStep[0m  [24/42], [94mLoss[0m : 1.54102
[1mStep[0m  [28/42], [94mLoss[0m : 1.55735
[1mStep[0m  [32/42], [94mLoss[0m : 1.42069
[1mStep[0m  [36/42], [94mLoss[0m : 1.32436
[1mStep[0m  [40/42], [94mLoss[0m : 1.35024

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35859
[1mStep[0m  [4/42], [94mLoss[0m : 1.43400
[1mStep[0m  [8/42], [94mLoss[0m : 1.37894
[1mStep[0m  [12/42], [94mLoss[0m : 1.37005
[1mStep[0m  [16/42], [94mLoss[0m : 1.64255
[1mStep[0m  [20/42], [94mLoss[0m : 1.33800
[1mStep[0m  [24/42], [94mLoss[0m : 1.55242
[1mStep[0m  [28/42], [94mLoss[0m : 1.35936
[1mStep[0m  [32/42], [94mLoss[0m : 1.45333
[1mStep[0m  [36/42], [94mLoss[0m : 1.48068
[1mStep[0m  [40/42], [94mLoss[0m : 1.50201

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.35948
[1mStep[0m  [4/42], [94mLoss[0m : 1.45576
[1mStep[0m  [8/42], [94mLoss[0m : 1.53690
[1mStep[0m  [12/42], [94mLoss[0m : 1.47925
[1mStep[0m  [16/42], [94mLoss[0m : 1.31505
[1mStep[0m  [20/42], [94mLoss[0m : 1.38403
[1mStep[0m  [24/42], [94mLoss[0m : 1.45582
[1mStep[0m  [28/42], [94mLoss[0m : 1.50344
[1mStep[0m  [32/42], [94mLoss[0m : 1.47421
[1mStep[0m  [36/42], [94mLoss[0m : 1.29298
[1mStep[0m  [40/42], [94mLoss[0m : 1.47417

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.427, [92mTest[0m: 2.526, [96mlr[0m: 0.005050000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 16 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 2 - Evaluation MAE:  2.487261618886675
MAE score P1      2.304791
MAE score P2      2.487262
loss              1.427029
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.18640
[1mStep[0m  [8/84], [94mLoss[0m : 10.47283
[1mStep[0m  [16/84], [94mLoss[0m : 9.64244
[1mStep[0m  [24/84], [94mLoss[0m : 8.82319
[1mStep[0m  [32/84], [94mLoss[0m : 8.76174
[1mStep[0m  [40/84], [94mLoss[0m : 7.87912
[1mStep[0m  [48/84], [94mLoss[0m : 6.94755
[1mStep[0m  [56/84], [94mLoss[0m : 6.24645
[1mStep[0m  [64/84], [94mLoss[0m : 5.01265
[1mStep[0m  [72/84], [94mLoss[0m : 4.67110
[1mStep[0m  [80/84], [94mLoss[0m : 3.83260

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.448, [92mTest[0m: 10.857, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.77574
[1mStep[0m  [8/84], [94mLoss[0m : 3.41536
[1mStep[0m  [16/84], [94mLoss[0m : 3.23843
[1mStep[0m  [24/84], [94mLoss[0m : 3.26335
[1mStep[0m  [32/84], [94mLoss[0m : 2.77792
[1mStep[0m  [40/84], [94mLoss[0m : 2.88891
[1mStep[0m  [48/84], [94mLoss[0m : 2.72326
[1mStep[0m  [56/84], [94mLoss[0m : 2.63487
[1mStep[0m  [64/84], [94mLoss[0m : 2.60813
[1mStep[0m  [72/84], [94mLoss[0m : 2.91330
[1mStep[0m  [80/84], [94mLoss[0m : 3.20742

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.091, [92mTest[0m: 5.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.08294
[1mStep[0m  [8/84], [94mLoss[0m : 2.74121
[1mStep[0m  [16/84], [94mLoss[0m : 2.50249
[1mStep[0m  [24/84], [94mLoss[0m : 2.83740
[1mStep[0m  [32/84], [94mLoss[0m : 2.69938
[1mStep[0m  [40/84], [94mLoss[0m : 2.53019
[1mStep[0m  [48/84], [94mLoss[0m : 2.99120
[1mStep[0m  [56/84], [94mLoss[0m : 2.83487
[1mStep[0m  [64/84], [94mLoss[0m : 3.01340
[1mStep[0m  [72/84], [94mLoss[0m : 2.79139
[1mStep[0m  [80/84], [94mLoss[0m : 2.70117

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.814, [92mTest[0m: 2.827, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.91134
[1mStep[0m  [8/84], [94mLoss[0m : 2.53958
[1mStep[0m  [16/84], [94mLoss[0m : 3.05148
[1mStep[0m  [24/84], [94mLoss[0m : 2.66104
[1mStep[0m  [32/84], [94mLoss[0m : 2.75629
[1mStep[0m  [40/84], [94mLoss[0m : 2.75476
[1mStep[0m  [48/84], [94mLoss[0m : 2.79318
[1mStep[0m  [56/84], [94mLoss[0m : 2.71311
[1mStep[0m  [64/84], [94mLoss[0m : 3.02822
[1mStep[0m  [72/84], [94mLoss[0m : 2.78086
[1mStep[0m  [80/84], [94mLoss[0m : 2.76409

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.777, [92mTest[0m: 2.554, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67465
[1mStep[0m  [8/84], [94mLoss[0m : 2.52681
[1mStep[0m  [16/84], [94mLoss[0m : 2.83625
[1mStep[0m  [24/84], [94mLoss[0m : 2.19064
[1mStep[0m  [32/84], [94mLoss[0m : 3.17458
[1mStep[0m  [40/84], [94mLoss[0m : 2.70952
[1mStep[0m  [48/84], [94mLoss[0m : 2.73106
[1mStep[0m  [56/84], [94mLoss[0m : 2.92114
[1mStep[0m  [64/84], [94mLoss[0m : 2.60688
[1mStep[0m  [72/84], [94mLoss[0m : 3.10353
[1mStep[0m  [80/84], [94mLoss[0m : 2.73888

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82400
[1mStep[0m  [8/84], [94mLoss[0m : 2.63268
[1mStep[0m  [16/84], [94mLoss[0m : 2.60241
[1mStep[0m  [24/84], [94mLoss[0m : 2.87543
[1mStep[0m  [32/84], [94mLoss[0m : 2.77584
[1mStep[0m  [40/84], [94mLoss[0m : 3.37300
[1mStep[0m  [48/84], [94mLoss[0m : 2.92223
[1mStep[0m  [56/84], [94mLoss[0m : 2.69462
[1mStep[0m  [64/84], [94mLoss[0m : 2.94897
[1mStep[0m  [72/84], [94mLoss[0m : 2.78409
[1mStep[0m  [80/84], [94mLoss[0m : 2.60891

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.509, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37651
[1mStep[0m  [8/84], [94mLoss[0m : 2.93705
[1mStep[0m  [16/84], [94mLoss[0m : 2.06439
[1mStep[0m  [24/84], [94mLoss[0m : 2.58769
[1mStep[0m  [32/84], [94mLoss[0m : 2.65181
[1mStep[0m  [40/84], [94mLoss[0m : 2.72209
[1mStep[0m  [48/84], [94mLoss[0m : 2.71995
[1mStep[0m  [56/84], [94mLoss[0m : 2.57720
[1mStep[0m  [64/84], [94mLoss[0m : 2.84552
[1mStep[0m  [72/84], [94mLoss[0m : 2.77640
[1mStep[0m  [80/84], [94mLoss[0m : 2.49979

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71804
[1mStep[0m  [8/84], [94mLoss[0m : 2.80944
[1mStep[0m  [16/84], [94mLoss[0m : 2.57310
[1mStep[0m  [24/84], [94mLoss[0m : 2.68893
[1mStep[0m  [32/84], [94mLoss[0m : 2.91081
[1mStep[0m  [40/84], [94mLoss[0m : 2.58528
[1mStep[0m  [48/84], [94mLoss[0m : 2.87917
[1mStep[0m  [56/84], [94mLoss[0m : 2.64271
[1mStep[0m  [64/84], [94mLoss[0m : 3.05713
[1mStep[0m  [72/84], [94mLoss[0m : 2.80601
[1mStep[0m  [80/84], [94mLoss[0m : 2.44710

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45091
[1mStep[0m  [8/84], [94mLoss[0m : 2.63727
[1mStep[0m  [16/84], [94mLoss[0m : 3.24547
[1mStep[0m  [24/84], [94mLoss[0m : 2.77059
[1mStep[0m  [32/84], [94mLoss[0m : 2.47632
[1mStep[0m  [40/84], [94mLoss[0m : 2.48646
[1mStep[0m  [48/84], [94mLoss[0m : 2.45911
[1mStep[0m  [56/84], [94mLoss[0m : 2.54306
[1mStep[0m  [64/84], [94mLoss[0m : 2.40993
[1mStep[0m  [72/84], [94mLoss[0m : 2.33571
[1mStep[0m  [80/84], [94mLoss[0m : 2.57961

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61848
[1mStep[0m  [8/84], [94mLoss[0m : 2.68700
[1mStep[0m  [16/84], [94mLoss[0m : 2.71224
[1mStep[0m  [24/84], [94mLoss[0m : 2.54676
[1mStep[0m  [32/84], [94mLoss[0m : 2.47542
[1mStep[0m  [40/84], [94mLoss[0m : 2.52943
[1mStep[0m  [48/84], [94mLoss[0m : 2.69234
[1mStep[0m  [56/84], [94mLoss[0m : 2.51248
[1mStep[0m  [64/84], [94mLoss[0m : 2.95200
[1mStep[0m  [72/84], [94mLoss[0m : 2.60424
[1mStep[0m  [80/84], [94mLoss[0m : 2.35715

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66091
[1mStep[0m  [8/84], [94mLoss[0m : 2.34600
[1mStep[0m  [16/84], [94mLoss[0m : 2.55594
[1mStep[0m  [24/84], [94mLoss[0m : 2.54617
[1mStep[0m  [32/84], [94mLoss[0m : 2.59314
[1mStep[0m  [40/84], [94mLoss[0m : 2.71627
[1mStep[0m  [48/84], [94mLoss[0m : 2.28887
[1mStep[0m  [56/84], [94mLoss[0m : 2.61564
[1mStep[0m  [64/84], [94mLoss[0m : 2.68008
[1mStep[0m  [72/84], [94mLoss[0m : 2.72276
[1mStep[0m  [80/84], [94mLoss[0m : 2.68856

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43704
[1mStep[0m  [8/84], [94mLoss[0m : 2.49727
[1mStep[0m  [16/84], [94mLoss[0m : 2.81409
[1mStep[0m  [24/84], [94mLoss[0m : 2.46952
[1mStep[0m  [32/84], [94mLoss[0m : 2.34591
[1mStep[0m  [40/84], [94mLoss[0m : 2.71518
[1mStep[0m  [48/84], [94mLoss[0m : 2.65904
[1mStep[0m  [56/84], [94mLoss[0m : 2.54906
[1mStep[0m  [64/84], [94mLoss[0m : 2.73826
[1mStep[0m  [72/84], [94mLoss[0m : 2.60769
[1mStep[0m  [80/84], [94mLoss[0m : 2.42954

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67767
[1mStep[0m  [8/84], [94mLoss[0m : 2.80074
[1mStep[0m  [16/84], [94mLoss[0m : 2.54857
[1mStep[0m  [24/84], [94mLoss[0m : 2.56861
[1mStep[0m  [32/84], [94mLoss[0m : 2.32474
[1mStep[0m  [40/84], [94mLoss[0m : 2.45112
[1mStep[0m  [48/84], [94mLoss[0m : 2.56747
[1mStep[0m  [56/84], [94mLoss[0m : 2.47828
[1mStep[0m  [64/84], [94mLoss[0m : 2.55975
[1mStep[0m  [72/84], [94mLoss[0m : 2.97018
[1mStep[0m  [80/84], [94mLoss[0m : 2.60471

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71782
[1mStep[0m  [8/84], [94mLoss[0m : 2.84652
[1mStep[0m  [16/84], [94mLoss[0m : 2.77041
[1mStep[0m  [24/84], [94mLoss[0m : 2.47173
[1mStep[0m  [32/84], [94mLoss[0m : 2.70719
[1mStep[0m  [40/84], [94mLoss[0m : 2.66416
[1mStep[0m  [48/84], [94mLoss[0m : 2.58256
[1mStep[0m  [56/84], [94mLoss[0m : 2.91183
[1mStep[0m  [64/84], [94mLoss[0m : 2.36442
[1mStep[0m  [72/84], [94mLoss[0m : 2.74915
[1mStep[0m  [80/84], [94mLoss[0m : 2.50819

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56954
[1mStep[0m  [8/84], [94mLoss[0m : 2.34977
[1mStep[0m  [16/84], [94mLoss[0m : 2.53995
[1mStep[0m  [24/84], [94mLoss[0m : 2.72334
[1mStep[0m  [32/84], [94mLoss[0m : 2.45915
[1mStep[0m  [40/84], [94mLoss[0m : 2.50280
[1mStep[0m  [48/84], [94mLoss[0m : 2.86913
[1mStep[0m  [56/84], [94mLoss[0m : 2.46323
[1mStep[0m  [64/84], [94mLoss[0m : 2.81851
[1mStep[0m  [72/84], [94mLoss[0m : 2.68070
[1mStep[0m  [80/84], [94mLoss[0m : 2.78816

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31698
[1mStep[0m  [8/84], [94mLoss[0m : 2.51982
[1mStep[0m  [16/84], [94mLoss[0m : 2.93308
[1mStep[0m  [24/84], [94mLoss[0m : 2.64839
[1mStep[0m  [32/84], [94mLoss[0m : 2.54900
[1mStep[0m  [40/84], [94mLoss[0m : 2.19052
[1mStep[0m  [48/84], [94mLoss[0m : 2.61556
[1mStep[0m  [56/84], [94mLoss[0m : 2.55085
[1mStep[0m  [64/84], [94mLoss[0m : 2.39784
[1mStep[0m  [72/84], [94mLoss[0m : 2.48783
[1mStep[0m  [80/84], [94mLoss[0m : 2.68859

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31960
[1mStep[0m  [8/84], [94mLoss[0m : 2.21290
[1mStep[0m  [16/84], [94mLoss[0m : 2.68066
[1mStep[0m  [24/84], [94mLoss[0m : 2.59117
[1mStep[0m  [32/84], [94mLoss[0m : 2.56195
[1mStep[0m  [40/84], [94mLoss[0m : 2.50738
[1mStep[0m  [48/84], [94mLoss[0m : 2.34666
[1mStep[0m  [56/84], [94mLoss[0m : 2.67932
[1mStep[0m  [64/84], [94mLoss[0m : 2.72172
[1mStep[0m  [72/84], [94mLoss[0m : 2.62888
[1mStep[0m  [80/84], [94mLoss[0m : 2.40359

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89984
[1mStep[0m  [8/84], [94mLoss[0m : 2.34725
[1mStep[0m  [16/84], [94mLoss[0m : 2.59644
[1mStep[0m  [24/84], [94mLoss[0m : 2.61068
[1mStep[0m  [32/84], [94mLoss[0m : 2.32113
[1mStep[0m  [40/84], [94mLoss[0m : 2.65156
[1mStep[0m  [48/84], [94mLoss[0m : 2.28643
[1mStep[0m  [56/84], [94mLoss[0m : 2.62362
[1mStep[0m  [64/84], [94mLoss[0m : 2.59316
[1mStep[0m  [72/84], [94mLoss[0m : 2.78198
[1mStep[0m  [80/84], [94mLoss[0m : 2.42604

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64825
[1mStep[0m  [8/84], [94mLoss[0m : 2.39985
[1mStep[0m  [16/84], [94mLoss[0m : 2.76593
[1mStep[0m  [24/84], [94mLoss[0m : 2.70835
[1mStep[0m  [32/84], [94mLoss[0m : 2.55358
[1mStep[0m  [40/84], [94mLoss[0m : 2.59029
[1mStep[0m  [48/84], [94mLoss[0m : 2.82924
[1mStep[0m  [56/84], [94mLoss[0m : 2.54003
[1mStep[0m  [64/84], [94mLoss[0m : 2.41973
[1mStep[0m  [72/84], [94mLoss[0m : 2.53120
[1mStep[0m  [80/84], [94mLoss[0m : 2.70857

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37219
[1mStep[0m  [8/84], [94mLoss[0m : 2.65766
[1mStep[0m  [16/84], [94mLoss[0m : 2.57193
[1mStep[0m  [24/84], [94mLoss[0m : 3.02448
[1mStep[0m  [32/84], [94mLoss[0m : 2.40889
[1mStep[0m  [40/84], [94mLoss[0m : 2.52695
[1mStep[0m  [48/84], [94mLoss[0m : 2.61288
[1mStep[0m  [56/84], [94mLoss[0m : 2.14943
[1mStep[0m  [64/84], [94mLoss[0m : 2.60332
[1mStep[0m  [72/84], [94mLoss[0m : 2.44247
[1mStep[0m  [80/84], [94mLoss[0m : 2.54381

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35706
[1mStep[0m  [8/84], [94mLoss[0m : 2.77393
[1mStep[0m  [16/84], [94mLoss[0m : 2.52971
[1mStep[0m  [24/84], [94mLoss[0m : 2.73936
[1mStep[0m  [32/84], [94mLoss[0m : 2.38617
[1mStep[0m  [40/84], [94mLoss[0m : 2.46368
[1mStep[0m  [48/84], [94mLoss[0m : 2.48807
[1mStep[0m  [56/84], [94mLoss[0m : 2.49536
[1mStep[0m  [64/84], [94mLoss[0m : 2.07037
[1mStep[0m  [72/84], [94mLoss[0m : 2.55296
[1mStep[0m  [80/84], [94mLoss[0m : 2.60712

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68986
[1mStep[0m  [8/84], [94mLoss[0m : 2.22191
[1mStep[0m  [16/84], [94mLoss[0m : 2.25276
[1mStep[0m  [24/84], [94mLoss[0m : 2.22217
[1mStep[0m  [32/84], [94mLoss[0m : 2.55046
[1mStep[0m  [40/84], [94mLoss[0m : 2.62324
[1mStep[0m  [48/84], [94mLoss[0m : 2.31219
[1mStep[0m  [56/84], [94mLoss[0m : 2.53026
[1mStep[0m  [64/84], [94mLoss[0m : 2.67526
[1mStep[0m  [72/84], [94mLoss[0m : 2.60223
[1mStep[0m  [80/84], [94mLoss[0m : 2.51960

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22296
[1mStep[0m  [8/84], [94mLoss[0m : 2.38086
[1mStep[0m  [16/84], [94mLoss[0m : 2.53224
[1mStep[0m  [24/84], [94mLoss[0m : 2.32817
[1mStep[0m  [32/84], [94mLoss[0m : 2.43295
[1mStep[0m  [40/84], [94mLoss[0m : 2.62354
[1mStep[0m  [48/84], [94mLoss[0m : 2.26175
[1mStep[0m  [56/84], [94mLoss[0m : 2.67689
[1mStep[0m  [64/84], [94mLoss[0m : 2.62647
[1mStep[0m  [72/84], [94mLoss[0m : 2.58001
[1mStep[0m  [80/84], [94mLoss[0m : 2.89879

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11890
[1mStep[0m  [8/84], [94mLoss[0m : 2.45877
[1mStep[0m  [16/84], [94mLoss[0m : 2.74351
[1mStep[0m  [24/84], [94mLoss[0m : 2.21117
[1mStep[0m  [32/84], [94mLoss[0m : 2.73585
[1mStep[0m  [40/84], [94mLoss[0m : 2.61943
[1mStep[0m  [48/84], [94mLoss[0m : 2.54493
[1mStep[0m  [56/84], [94mLoss[0m : 2.49455
[1mStep[0m  [64/84], [94mLoss[0m : 2.38295
[1mStep[0m  [72/84], [94mLoss[0m : 2.41372
[1mStep[0m  [80/84], [94mLoss[0m : 2.38893

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35140
[1mStep[0m  [8/84], [94mLoss[0m : 2.33141
[1mStep[0m  [16/84], [94mLoss[0m : 2.57657
[1mStep[0m  [24/84], [94mLoss[0m : 2.33450
[1mStep[0m  [32/84], [94mLoss[0m : 2.65985
[1mStep[0m  [40/84], [94mLoss[0m : 2.48995
[1mStep[0m  [48/84], [94mLoss[0m : 2.94715
[1mStep[0m  [56/84], [94mLoss[0m : 2.62308
[1mStep[0m  [64/84], [94mLoss[0m : 2.60380
[1mStep[0m  [72/84], [94mLoss[0m : 2.36778
[1mStep[0m  [80/84], [94mLoss[0m : 2.42354

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57423
[1mStep[0m  [8/84], [94mLoss[0m : 2.42228
[1mStep[0m  [16/84], [94mLoss[0m : 2.63355
[1mStep[0m  [24/84], [94mLoss[0m : 2.60488
[1mStep[0m  [32/84], [94mLoss[0m : 2.29810
[1mStep[0m  [40/84], [94mLoss[0m : 2.09507
[1mStep[0m  [48/84], [94mLoss[0m : 2.51472
[1mStep[0m  [56/84], [94mLoss[0m : 2.75130
[1mStep[0m  [64/84], [94mLoss[0m : 2.46470
[1mStep[0m  [72/84], [94mLoss[0m : 2.39915
[1mStep[0m  [80/84], [94mLoss[0m : 2.39467

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33371
[1mStep[0m  [8/84], [94mLoss[0m : 2.47706
[1mStep[0m  [16/84], [94mLoss[0m : 2.44646
[1mStep[0m  [24/84], [94mLoss[0m : 2.48828
[1mStep[0m  [32/84], [94mLoss[0m : 2.16763
[1mStep[0m  [40/84], [94mLoss[0m : 2.48063
[1mStep[0m  [48/84], [94mLoss[0m : 2.53316
[1mStep[0m  [56/84], [94mLoss[0m : 2.30459
[1mStep[0m  [64/84], [94mLoss[0m : 2.53641
[1mStep[0m  [72/84], [94mLoss[0m : 2.53092
[1mStep[0m  [80/84], [94mLoss[0m : 2.54685

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67437
[1mStep[0m  [8/84], [94mLoss[0m : 2.32232
[1mStep[0m  [16/84], [94mLoss[0m : 2.64592
[1mStep[0m  [24/84], [94mLoss[0m : 2.40862
[1mStep[0m  [32/84], [94mLoss[0m : 2.55808
[1mStep[0m  [40/84], [94mLoss[0m : 2.40524
[1mStep[0m  [48/84], [94mLoss[0m : 2.38640
[1mStep[0m  [56/84], [94mLoss[0m : 2.59121
[1mStep[0m  [64/84], [94mLoss[0m : 2.25757
[1mStep[0m  [72/84], [94mLoss[0m : 2.27239
[1mStep[0m  [80/84], [94mLoss[0m : 2.32446

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74871
[1mStep[0m  [8/84], [94mLoss[0m : 2.79909
[1mStep[0m  [16/84], [94mLoss[0m : 2.48849
[1mStep[0m  [24/84], [94mLoss[0m : 2.15613
[1mStep[0m  [32/84], [94mLoss[0m : 2.41777
[1mStep[0m  [40/84], [94mLoss[0m : 2.63293
[1mStep[0m  [48/84], [94mLoss[0m : 2.60870
[1mStep[0m  [56/84], [94mLoss[0m : 2.63563
[1mStep[0m  [64/84], [94mLoss[0m : 2.39863
[1mStep[0m  [72/84], [94mLoss[0m : 2.61374
[1mStep[0m  [80/84], [94mLoss[0m : 2.39242

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61399
[1mStep[0m  [8/84], [94mLoss[0m : 2.52382
[1mStep[0m  [16/84], [94mLoss[0m : 2.42837
[1mStep[0m  [24/84], [94mLoss[0m : 2.47032
[1mStep[0m  [32/84], [94mLoss[0m : 2.55317
[1mStep[0m  [40/84], [94mLoss[0m : 2.37210
[1mStep[0m  [48/84], [94mLoss[0m : 2.27296
[1mStep[0m  [56/84], [94mLoss[0m : 2.53665
[1mStep[0m  [64/84], [94mLoss[0m : 2.46745
[1mStep[0m  [72/84], [94mLoss[0m : 2.42770
[1mStep[0m  [80/84], [94mLoss[0m : 2.29882

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.333123121942793
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.11778
[1mStep[0m  [8/84], [94mLoss[0m : 2.64258
[1mStep[0m  [16/84], [94mLoss[0m : 2.55939
[1mStep[0m  [24/84], [94mLoss[0m : 2.74052
[1mStep[0m  [32/84], [94mLoss[0m : 2.43641
[1mStep[0m  [40/84], [94mLoss[0m : 2.41003
[1mStep[0m  [48/84], [94mLoss[0m : 2.39556
[1mStep[0m  [56/84], [94mLoss[0m : 2.56855
[1mStep[0m  [64/84], [94mLoss[0m : 2.56246
[1mStep[0m  [72/84], [94mLoss[0m : 2.36824
[1mStep[0m  [80/84], [94mLoss[0m : 2.52201

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51462
[1mStep[0m  [8/84], [94mLoss[0m : 2.54104
[1mStep[0m  [16/84], [94mLoss[0m : 2.38225
[1mStep[0m  [24/84], [94mLoss[0m : 2.33360
[1mStep[0m  [32/84], [94mLoss[0m : 2.34550
[1mStep[0m  [40/84], [94mLoss[0m : 2.72494
[1mStep[0m  [48/84], [94mLoss[0m : 2.49411
[1mStep[0m  [56/84], [94mLoss[0m : 2.76215
[1mStep[0m  [64/84], [94mLoss[0m : 2.49765
[1mStep[0m  [72/84], [94mLoss[0m : 2.96475
[1mStep[0m  [80/84], [94mLoss[0m : 2.49707

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.542, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63767
[1mStep[0m  [8/84], [94mLoss[0m : 2.64188
[1mStep[0m  [16/84], [94mLoss[0m : 2.61101
[1mStep[0m  [24/84], [94mLoss[0m : 2.82475
[1mStep[0m  [32/84], [94mLoss[0m : 2.30576
[1mStep[0m  [40/84], [94mLoss[0m : 2.38776
[1mStep[0m  [48/84], [94mLoss[0m : 2.51038
[1mStep[0m  [56/84], [94mLoss[0m : 2.33117
[1mStep[0m  [64/84], [94mLoss[0m : 2.35032
[1mStep[0m  [72/84], [94mLoss[0m : 2.54514
[1mStep[0m  [80/84], [94mLoss[0m : 2.54047

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.751, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21799
[1mStep[0m  [8/84], [94mLoss[0m : 2.67755
[1mStep[0m  [16/84], [94mLoss[0m : 2.79121
[1mStep[0m  [24/84], [94mLoss[0m : 2.54321
[1mStep[0m  [32/84], [94mLoss[0m : 2.25672
[1mStep[0m  [40/84], [94mLoss[0m : 2.72744
[1mStep[0m  [48/84], [94mLoss[0m : 2.41819
[1mStep[0m  [56/84], [94mLoss[0m : 2.31464
[1mStep[0m  [64/84], [94mLoss[0m : 2.11092
[1mStep[0m  [72/84], [94mLoss[0m : 2.61408
[1mStep[0m  [80/84], [94mLoss[0m : 2.51736

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.635, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49751
[1mStep[0m  [8/84], [94mLoss[0m : 2.67665
[1mStep[0m  [16/84], [94mLoss[0m : 2.26604
[1mStep[0m  [24/84], [94mLoss[0m : 2.34443
[1mStep[0m  [32/84], [94mLoss[0m : 2.22219
[1mStep[0m  [40/84], [94mLoss[0m : 2.30406
[1mStep[0m  [48/84], [94mLoss[0m : 2.35879
[1mStep[0m  [56/84], [94mLoss[0m : 2.29981
[1mStep[0m  [64/84], [94mLoss[0m : 2.39182
[1mStep[0m  [72/84], [94mLoss[0m : 2.51086
[1mStep[0m  [80/84], [94mLoss[0m : 2.26986

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24869
[1mStep[0m  [8/84], [94mLoss[0m : 2.14164
[1mStep[0m  [16/84], [94mLoss[0m : 2.19148
[1mStep[0m  [24/84], [94mLoss[0m : 2.35253
[1mStep[0m  [32/84], [94mLoss[0m : 2.36174
[1mStep[0m  [40/84], [94mLoss[0m : 2.21327
[1mStep[0m  [48/84], [94mLoss[0m : 2.45477
[1mStep[0m  [56/84], [94mLoss[0m : 2.23248
[1mStep[0m  [64/84], [94mLoss[0m : 2.02620
[1mStep[0m  [72/84], [94mLoss[0m : 2.39666
[1mStep[0m  [80/84], [94mLoss[0m : 2.04967

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43882
[1mStep[0m  [8/84], [94mLoss[0m : 2.05368
[1mStep[0m  [16/84], [94mLoss[0m : 2.35671
[1mStep[0m  [24/84], [94mLoss[0m : 2.55256
[1mStep[0m  [32/84], [94mLoss[0m : 2.26737
[1mStep[0m  [40/84], [94mLoss[0m : 1.88960
[1mStep[0m  [48/84], [94mLoss[0m : 2.31764
[1mStep[0m  [56/84], [94mLoss[0m : 2.26777
[1mStep[0m  [64/84], [94mLoss[0m : 2.30011
[1mStep[0m  [72/84], [94mLoss[0m : 2.12854
[1mStep[0m  [80/84], [94mLoss[0m : 2.55796

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.568, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26149
[1mStep[0m  [8/84], [94mLoss[0m : 2.38463
[1mStep[0m  [16/84], [94mLoss[0m : 2.34195
[1mStep[0m  [24/84], [94mLoss[0m : 2.02636
[1mStep[0m  [32/84], [94mLoss[0m : 2.09685
[1mStep[0m  [40/84], [94mLoss[0m : 2.11477
[1mStep[0m  [48/84], [94mLoss[0m : 2.44201
[1mStep[0m  [56/84], [94mLoss[0m : 2.11223
[1mStep[0m  [64/84], [94mLoss[0m : 2.52007
[1mStep[0m  [72/84], [94mLoss[0m : 2.03614
[1mStep[0m  [80/84], [94mLoss[0m : 2.14674

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69556
[1mStep[0m  [8/84], [94mLoss[0m : 1.99680
[1mStep[0m  [16/84], [94mLoss[0m : 2.17341
[1mStep[0m  [24/84], [94mLoss[0m : 2.27043
[1mStep[0m  [32/84], [94mLoss[0m : 2.29125
[1mStep[0m  [40/84], [94mLoss[0m : 2.05021
[1mStep[0m  [48/84], [94mLoss[0m : 2.19471
[1mStep[0m  [56/84], [94mLoss[0m : 2.32169
[1mStep[0m  [64/84], [94mLoss[0m : 2.17232
[1mStep[0m  [72/84], [94mLoss[0m : 2.44918
[1mStep[0m  [80/84], [94mLoss[0m : 2.01149

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88827
[1mStep[0m  [8/84], [94mLoss[0m : 2.52459
[1mStep[0m  [16/84], [94mLoss[0m : 2.12668
[1mStep[0m  [24/84], [94mLoss[0m : 2.10834
[1mStep[0m  [32/84], [94mLoss[0m : 1.96309
[1mStep[0m  [40/84], [94mLoss[0m : 2.28469
[1mStep[0m  [48/84], [94mLoss[0m : 1.86046
[1mStep[0m  [56/84], [94mLoss[0m : 2.39434
[1mStep[0m  [64/84], [94mLoss[0m : 2.11386
[1mStep[0m  [72/84], [94mLoss[0m : 2.42966
[1mStep[0m  [80/84], [94mLoss[0m : 2.11182

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78513
[1mStep[0m  [8/84], [94mLoss[0m : 2.17280
[1mStep[0m  [16/84], [94mLoss[0m : 2.10977
[1mStep[0m  [24/84], [94mLoss[0m : 2.47000
[1mStep[0m  [32/84], [94mLoss[0m : 2.25433
[1mStep[0m  [40/84], [94mLoss[0m : 2.13964
[1mStep[0m  [48/84], [94mLoss[0m : 2.33338
[1mStep[0m  [56/84], [94mLoss[0m : 2.25970
[1mStep[0m  [64/84], [94mLoss[0m : 2.19173
[1mStep[0m  [72/84], [94mLoss[0m : 2.10014
[1mStep[0m  [80/84], [94mLoss[0m : 2.21001

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09462
[1mStep[0m  [8/84], [94mLoss[0m : 1.88318
[1mStep[0m  [16/84], [94mLoss[0m : 1.99894
[1mStep[0m  [24/84], [94mLoss[0m : 2.15864
[1mStep[0m  [32/84], [94mLoss[0m : 2.01074
[1mStep[0m  [40/84], [94mLoss[0m : 2.28881
[1mStep[0m  [48/84], [94mLoss[0m : 2.08482
[1mStep[0m  [56/84], [94mLoss[0m : 1.86195
[1mStep[0m  [64/84], [94mLoss[0m : 2.08345
[1mStep[0m  [72/84], [94mLoss[0m : 2.07263
[1mStep[0m  [80/84], [94mLoss[0m : 1.96955

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98918
[1mStep[0m  [8/84], [94mLoss[0m : 2.25671
[1mStep[0m  [16/84], [94mLoss[0m : 1.89885
[1mStep[0m  [24/84], [94mLoss[0m : 1.95823
[1mStep[0m  [32/84], [94mLoss[0m : 2.10844
[1mStep[0m  [40/84], [94mLoss[0m : 2.07110
[1mStep[0m  [48/84], [94mLoss[0m : 2.09528
[1mStep[0m  [56/84], [94mLoss[0m : 2.02620
[1mStep[0m  [64/84], [94mLoss[0m : 2.06070
[1mStep[0m  [72/84], [94mLoss[0m : 1.96741
[1mStep[0m  [80/84], [94mLoss[0m : 2.07388

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07059
[1mStep[0m  [8/84], [94mLoss[0m : 1.71266
[1mStep[0m  [16/84], [94mLoss[0m : 1.81812
[1mStep[0m  [24/84], [94mLoss[0m : 1.95880
[1mStep[0m  [32/84], [94mLoss[0m : 2.09295
[1mStep[0m  [40/84], [94mLoss[0m : 2.06366
[1mStep[0m  [48/84], [94mLoss[0m : 2.02954
[1mStep[0m  [56/84], [94mLoss[0m : 2.02291
[1mStep[0m  [64/84], [94mLoss[0m : 1.98343
[1mStep[0m  [72/84], [94mLoss[0m : 1.88374
[1mStep[0m  [80/84], [94mLoss[0m : 1.90839

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77259
[1mStep[0m  [8/84], [94mLoss[0m : 1.98487
[1mStep[0m  [16/84], [94mLoss[0m : 1.76407
[1mStep[0m  [24/84], [94mLoss[0m : 1.87074
[1mStep[0m  [32/84], [94mLoss[0m : 1.93807
[1mStep[0m  [40/84], [94mLoss[0m : 2.16868
[1mStep[0m  [48/84], [94mLoss[0m : 2.05541
[1mStep[0m  [56/84], [94mLoss[0m : 1.93348
[1mStep[0m  [64/84], [94mLoss[0m : 1.92846
[1mStep[0m  [72/84], [94mLoss[0m : 2.35188
[1mStep[0m  [80/84], [94mLoss[0m : 2.31542

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16214
[1mStep[0m  [8/84], [94mLoss[0m : 1.65115
[1mStep[0m  [16/84], [94mLoss[0m : 1.98659
[1mStep[0m  [24/84], [94mLoss[0m : 1.72086
[1mStep[0m  [32/84], [94mLoss[0m : 1.78539
[1mStep[0m  [40/84], [94mLoss[0m : 1.98617
[1mStep[0m  [48/84], [94mLoss[0m : 1.78789
[1mStep[0m  [56/84], [94mLoss[0m : 1.84765
[1mStep[0m  [64/84], [94mLoss[0m : 2.11132
[1mStep[0m  [72/84], [94mLoss[0m : 2.15442
[1mStep[0m  [80/84], [94mLoss[0m : 1.95166

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.941, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72565
[1mStep[0m  [8/84], [94mLoss[0m : 1.64683
[1mStep[0m  [16/84], [94mLoss[0m : 2.28664
[1mStep[0m  [24/84], [94mLoss[0m : 1.76200
[1mStep[0m  [32/84], [94mLoss[0m : 1.74620
[1mStep[0m  [40/84], [94mLoss[0m : 1.93323
[1mStep[0m  [48/84], [94mLoss[0m : 2.03374
[1mStep[0m  [56/84], [94mLoss[0m : 1.94005
[1mStep[0m  [64/84], [94mLoss[0m : 1.90426
[1mStep[0m  [72/84], [94mLoss[0m : 2.01523
[1mStep[0m  [80/84], [94mLoss[0m : 2.08562

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04829
[1mStep[0m  [8/84], [94mLoss[0m : 1.84806
[1mStep[0m  [16/84], [94mLoss[0m : 1.85317
[1mStep[0m  [24/84], [94mLoss[0m : 1.96745
[1mStep[0m  [32/84], [94mLoss[0m : 2.08469
[1mStep[0m  [40/84], [94mLoss[0m : 2.24039
[1mStep[0m  [48/84], [94mLoss[0m : 1.98600
[1mStep[0m  [56/84], [94mLoss[0m : 1.99201
[1mStep[0m  [64/84], [94mLoss[0m : 1.80037
[1mStep[0m  [72/84], [94mLoss[0m : 1.89112
[1mStep[0m  [80/84], [94mLoss[0m : 1.77718

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93152
[1mStep[0m  [8/84], [94mLoss[0m : 1.55680
[1mStep[0m  [16/84], [94mLoss[0m : 1.79465
[1mStep[0m  [24/84], [94mLoss[0m : 1.80734
[1mStep[0m  [32/84], [94mLoss[0m : 2.07736
[1mStep[0m  [40/84], [94mLoss[0m : 1.96405
[1mStep[0m  [48/84], [94mLoss[0m : 1.93499
[1mStep[0m  [56/84], [94mLoss[0m : 1.75734
[1mStep[0m  [64/84], [94mLoss[0m : 2.06782
[1mStep[0m  [72/84], [94mLoss[0m : 1.72875
[1mStep[0m  [80/84], [94mLoss[0m : 1.66209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.495, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77373
[1mStep[0m  [8/84], [94mLoss[0m : 1.85161
[1mStep[0m  [16/84], [94mLoss[0m : 1.84726
[1mStep[0m  [24/84], [94mLoss[0m : 1.97933
[1mStep[0m  [32/84], [94mLoss[0m : 1.67151
[1mStep[0m  [40/84], [94mLoss[0m : 1.71948
[1mStep[0m  [48/84], [94mLoss[0m : 1.96049
[1mStep[0m  [56/84], [94mLoss[0m : 1.94019
[1mStep[0m  [64/84], [94mLoss[0m : 1.70471
[1mStep[0m  [72/84], [94mLoss[0m : 1.79941
[1mStep[0m  [80/84], [94mLoss[0m : 1.52939

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85689
[1mStep[0m  [8/84], [94mLoss[0m : 1.73376
[1mStep[0m  [16/84], [94mLoss[0m : 1.56902
[1mStep[0m  [24/84], [94mLoss[0m : 1.92422
[1mStep[0m  [32/84], [94mLoss[0m : 1.68076
[1mStep[0m  [40/84], [94mLoss[0m : 1.69723
[1mStep[0m  [48/84], [94mLoss[0m : 1.93409
[1mStep[0m  [56/84], [94mLoss[0m : 1.97543
[1mStep[0m  [64/84], [94mLoss[0m : 2.11748
[1mStep[0m  [72/84], [94mLoss[0m : 1.69524
[1mStep[0m  [80/84], [94mLoss[0m : 1.94908

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63112
[1mStep[0m  [8/84], [94mLoss[0m : 1.83461
[1mStep[0m  [16/84], [94mLoss[0m : 1.84534
[1mStep[0m  [24/84], [94mLoss[0m : 1.62047
[1mStep[0m  [32/84], [94mLoss[0m : 1.66146
[1mStep[0m  [40/84], [94mLoss[0m : 1.59901
[1mStep[0m  [48/84], [94mLoss[0m : 1.66296
[1mStep[0m  [56/84], [94mLoss[0m : 1.99127
[1mStep[0m  [64/84], [94mLoss[0m : 1.73362
[1mStep[0m  [72/84], [94mLoss[0m : 1.76074
[1mStep[0m  [80/84], [94mLoss[0m : 1.72862

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.747, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58287
[1mStep[0m  [8/84], [94mLoss[0m : 1.52914
[1mStep[0m  [16/84], [94mLoss[0m : 1.79428
[1mStep[0m  [24/84], [94mLoss[0m : 1.56652
[1mStep[0m  [32/84], [94mLoss[0m : 1.78415
[1mStep[0m  [40/84], [94mLoss[0m : 1.88874
[1mStep[0m  [48/84], [94mLoss[0m : 1.72444
[1mStep[0m  [56/84], [94mLoss[0m : 1.68492
[1mStep[0m  [64/84], [94mLoss[0m : 1.85578
[1mStep[0m  [72/84], [94mLoss[0m : 1.80840
[1mStep[0m  [80/84], [94mLoss[0m : 1.57825

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.729, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97028
[1mStep[0m  [8/84], [94mLoss[0m : 1.66605
[1mStep[0m  [16/84], [94mLoss[0m : 1.75233
[1mStep[0m  [24/84], [94mLoss[0m : 1.86867
[1mStep[0m  [32/84], [94mLoss[0m : 1.62666
[1mStep[0m  [40/84], [94mLoss[0m : 1.81411
[1mStep[0m  [48/84], [94mLoss[0m : 1.66872
[1mStep[0m  [56/84], [94mLoss[0m : 1.64397
[1mStep[0m  [64/84], [94mLoss[0m : 1.89307
[1mStep[0m  [72/84], [94mLoss[0m : 1.84870
[1mStep[0m  [80/84], [94mLoss[0m : 1.72782

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51436
[1mStep[0m  [8/84], [94mLoss[0m : 1.69700
[1mStep[0m  [16/84], [94mLoss[0m : 1.86755
[1mStep[0m  [24/84], [94mLoss[0m : 1.70425
[1mStep[0m  [32/84], [94mLoss[0m : 1.50497
[1mStep[0m  [40/84], [94mLoss[0m : 1.68287
[1mStep[0m  [48/84], [94mLoss[0m : 1.77034
[1mStep[0m  [56/84], [94mLoss[0m : 1.86932
[1mStep[0m  [64/84], [94mLoss[0m : 1.66940
[1mStep[0m  [72/84], [94mLoss[0m : 1.84884
[1mStep[0m  [80/84], [94mLoss[0m : 1.73802

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.524, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62842
[1mStep[0m  [8/84], [94mLoss[0m : 1.76110
[1mStep[0m  [16/84], [94mLoss[0m : 1.59492
[1mStep[0m  [24/84], [94mLoss[0m : 1.58733
[1mStep[0m  [32/84], [94mLoss[0m : 1.54862
[1mStep[0m  [40/84], [94mLoss[0m : 1.58958
[1mStep[0m  [48/84], [94mLoss[0m : 1.90264
[1mStep[0m  [56/84], [94mLoss[0m : 1.67862
[1mStep[0m  [64/84], [94mLoss[0m : 1.86087
[1mStep[0m  [72/84], [94mLoss[0m : 1.77174
[1mStep[0m  [80/84], [94mLoss[0m : 1.57126

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85710
[1mStep[0m  [8/84], [94mLoss[0m : 1.59276
[1mStep[0m  [16/84], [94mLoss[0m : 1.66263
[1mStep[0m  [24/84], [94mLoss[0m : 1.58623
[1mStep[0m  [32/84], [94mLoss[0m : 1.67197
[1mStep[0m  [40/84], [94mLoss[0m : 1.59552
[1mStep[0m  [48/84], [94mLoss[0m : 1.73489
[1mStep[0m  [56/84], [94mLoss[0m : 1.50646
[1mStep[0m  [64/84], [94mLoss[0m : 1.70929
[1mStep[0m  [72/84], [94mLoss[0m : 1.74912
[1mStep[0m  [80/84], [94mLoss[0m : 1.86952

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41303
[1mStep[0m  [8/84], [94mLoss[0m : 1.64792
[1mStep[0m  [16/84], [94mLoss[0m : 1.76384
[1mStep[0m  [24/84], [94mLoss[0m : 1.59708
[1mStep[0m  [32/84], [94mLoss[0m : 1.56913
[1mStep[0m  [40/84], [94mLoss[0m : 1.40486
[1mStep[0m  [48/84], [94mLoss[0m : 1.71237
[1mStep[0m  [56/84], [94mLoss[0m : 1.52574
[1mStep[0m  [64/84], [94mLoss[0m : 1.83397
[1mStep[0m  [72/84], [94mLoss[0m : 1.87303
[1mStep[0m  [80/84], [94mLoss[0m : 1.55442

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71348
[1mStep[0m  [8/84], [94mLoss[0m : 1.49926
[1mStep[0m  [16/84], [94mLoss[0m : 1.62793
[1mStep[0m  [24/84], [94mLoss[0m : 1.74573
[1mStep[0m  [32/84], [94mLoss[0m : 1.57630
[1mStep[0m  [40/84], [94mLoss[0m : 1.49173
[1mStep[0m  [48/84], [94mLoss[0m : 1.52094
[1mStep[0m  [56/84], [94mLoss[0m : 1.69867
[1mStep[0m  [64/84], [94mLoss[0m : 1.45300
[1mStep[0m  [72/84], [94mLoss[0m : 1.69604
[1mStep[0m  [80/84], [94mLoss[0m : 1.51992

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.552, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42965
[1mStep[0m  [8/84], [94mLoss[0m : 1.71719
[1mStep[0m  [16/84], [94mLoss[0m : 1.52802
[1mStep[0m  [24/84], [94mLoss[0m : 1.56683
[1mStep[0m  [32/84], [94mLoss[0m : 1.54496
[1mStep[0m  [40/84], [94mLoss[0m : 1.47747
[1mStep[0m  [48/84], [94mLoss[0m : 1.68377
[1mStep[0m  [56/84], [94mLoss[0m : 1.53806
[1mStep[0m  [64/84], [94mLoss[0m : 1.57582
[1mStep[0m  [72/84], [94mLoss[0m : 1.64508
[1mStep[0m  [80/84], [94mLoss[0m : 1.47025

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.517
====================================

Phase 2 - Evaluation MAE:  2.517331429890224
MAE score P1       2.333123
MAE score P2       2.517331
loss               1.588804
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.08206
[1mStep[0m  [4/42], [94mLoss[0m : 10.78953
[1mStep[0m  [8/42], [94mLoss[0m : 10.19508
[1mStep[0m  [12/42], [94mLoss[0m : 9.66186
[1mStep[0m  [16/42], [94mLoss[0m : 8.82448
[1mStep[0m  [20/42], [94mLoss[0m : 8.67573
[1mStep[0m  [24/42], [94mLoss[0m : 7.73004
[1mStep[0m  [28/42], [94mLoss[0m : 7.47486
[1mStep[0m  [32/42], [94mLoss[0m : 6.79892
[1mStep[0m  [36/42], [94mLoss[0m : 6.36760
[1mStep[0m  [40/42], [94mLoss[0m : 6.07171

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.401, [92mTest[0m: 11.126, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.85376
[1mStep[0m  [4/42], [94mLoss[0m : 5.31052
[1mStep[0m  [8/42], [94mLoss[0m : 4.26296
[1mStep[0m  [12/42], [94mLoss[0m : 4.28963
[1mStep[0m  [16/42], [94mLoss[0m : 3.57989
[1mStep[0m  [20/42], [94mLoss[0m : 3.76993
[1mStep[0m  [24/42], [94mLoss[0m : 3.44573
[1mStep[0m  [28/42], [94mLoss[0m : 3.41303
[1mStep[0m  [32/42], [94mLoss[0m : 3.10893
[1mStep[0m  [36/42], [94mLoss[0m : 2.91006
[1mStep[0m  [40/42], [94mLoss[0m : 2.70772

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.836, [92mTest[0m: 5.495, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89302
[1mStep[0m  [4/42], [94mLoss[0m : 3.08515
[1mStep[0m  [8/42], [94mLoss[0m : 2.79262
[1mStep[0m  [12/42], [94mLoss[0m : 2.64964
[1mStep[0m  [16/42], [94mLoss[0m : 2.70737
[1mStep[0m  [20/42], [94mLoss[0m : 2.77429
[1mStep[0m  [24/42], [94mLoss[0m : 2.62848
[1mStep[0m  [28/42], [94mLoss[0m : 2.60575
[1mStep[0m  [32/42], [94mLoss[0m : 2.47472
[1mStep[0m  [36/42], [94mLoss[0m : 2.48427
[1mStep[0m  [40/42], [94mLoss[0m : 2.56776

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.733, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57849
[1mStep[0m  [4/42], [94mLoss[0m : 2.77297
[1mStep[0m  [8/42], [94mLoss[0m : 2.49019
[1mStep[0m  [12/42], [94mLoss[0m : 2.39194
[1mStep[0m  [16/42], [94mLoss[0m : 2.52340
[1mStep[0m  [20/42], [94mLoss[0m : 2.49994
[1mStep[0m  [24/42], [94mLoss[0m : 2.61729
[1mStep[0m  [28/42], [94mLoss[0m : 2.83432
[1mStep[0m  [32/42], [94mLoss[0m : 2.51464
[1mStep[0m  [36/42], [94mLoss[0m : 2.30215
[1mStep[0m  [40/42], [94mLoss[0m : 2.57164

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41563
[1mStep[0m  [4/42], [94mLoss[0m : 2.70799
[1mStep[0m  [8/42], [94mLoss[0m : 2.46537
[1mStep[0m  [12/42], [94mLoss[0m : 2.39516
[1mStep[0m  [16/42], [94mLoss[0m : 2.86577
[1mStep[0m  [20/42], [94mLoss[0m : 2.50719
[1mStep[0m  [24/42], [94mLoss[0m : 2.45957
[1mStep[0m  [28/42], [94mLoss[0m : 2.46401
[1mStep[0m  [32/42], [94mLoss[0m : 2.34888
[1mStep[0m  [36/42], [94mLoss[0m : 2.39652
[1mStep[0m  [40/42], [94mLoss[0m : 2.73699

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52901
[1mStep[0m  [4/42], [94mLoss[0m : 2.62210
[1mStep[0m  [8/42], [94mLoss[0m : 2.60731
[1mStep[0m  [12/42], [94mLoss[0m : 2.70555
[1mStep[0m  [16/42], [94mLoss[0m : 2.66996
[1mStep[0m  [20/42], [94mLoss[0m : 2.37862
[1mStep[0m  [24/42], [94mLoss[0m : 2.56520
[1mStep[0m  [28/42], [94mLoss[0m : 2.59530
[1mStep[0m  [32/42], [94mLoss[0m : 2.68279
[1mStep[0m  [36/42], [94mLoss[0m : 2.68367
[1mStep[0m  [40/42], [94mLoss[0m : 2.66640

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63809
[1mStep[0m  [4/42], [94mLoss[0m : 2.59713
[1mStep[0m  [8/42], [94mLoss[0m : 2.73395
[1mStep[0m  [12/42], [94mLoss[0m : 2.74755
[1mStep[0m  [16/42], [94mLoss[0m : 2.61418
[1mStep[0m  [20/42], [94mLoss[0m : 2.33914
[1mStep[0m  [24/42], [94mLoss[0m : 2.36659
[1mStep[0m  [28/42], [94mLoss[0m : 2.58148
[1mStep[0m  [32/42], [94mLoss[0m : 2.53611
[1mStep[0m  [36/42], [94mLoss[0m : 2.38710
[1mStep[0m  [40/42], [94mLoss[0m : 2.44106

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64840
[1mStep[0m  [4/42], [94mLoss[0m : 2.39037
[1mStep[0m  [8/42], [94mLoss[0m : 2.07754
[1mStep[0m  [12/42], [94mLoss[0m : 2.65346
[1mStep[0m  [16/42], [94mLoss[0m : 2.63328
[1mStep[0m  [20/42], [94mLoss[0m : 2.59586
[1mStep[0m  [24/42], [94mLoss[0m : 2.41172
[1mStep[0m  [28/42], [94mLoss[0m : 2.68764
[1mStep[0m  [32/42], [94mLoss[0m : 2.47693
[1mStep[0m  [36/42], [94mLoss[0m : 2.50628
[1mStep[0m  [40/42], [94mLoss[0m : 2.43973

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25802
[1mStep[0m  [4/42], [94mLoss[0m : 2.68217
[1mStep[0m  [8/42], [94mLoss[0m : 2.35956
[1mStep[0m  [12/42], [94mLoss[0m : 2.55078
[1mStep[0m  [16/42], [94mLoss[0m : 2.29887
[1mStep[0m  [20/42], [94mLoss[0m : 2.46025
[1mStep[0m  [24/42], [94mLoss[0m : 2.54261
[1mStep[0m  [28/42], [94mLoss[0m : 2.52360
[1mStep[0m  [32/42], [94mLoss[0m : 2.41654
[1mStep[0m  [36/42], [94mLoss[0m : 2.42462
[1mStep[0m  [40/42], [94mLoss[0m : 2.53720

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47443
[1mStep[0m  [4/42], [94mLoss[0m : 2.27762
[1mStep[0m  [8/42], [94mLoss[0m : 2.42243
[1mStep[0m  [12/42], [94mLoss[0m : 2.72075
[1mStep[0m  [16/42], [94mLoss[0m : 2.47480
[1mStep[0m  [20/42], [94mLoss[0m : 2.52196
[1mStep[0m  [24/42], [94mLoss[0m : 2.47922
[1mStep[0m  [28/42], [94mLoss[0m : 2.58616
[1mStep[0m  [32/42], [94mLoss[0m : 2.81373
[1mStep[0m  [36/42], [94mLoss[0m : 2.31962
[1mStep[0m  [40/42], [94mLoss[0m : 2.45544

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41173
[1mStep[0m  [4/42], [94mLoss[0m : 2.60295
[1mStep[0m  [8/42], [94mLoss[0m : 2.51517
[1mStep[0m  [12/42], [94mLoss[0m : 2.59245
[1mStep[0m  [16/42], [94mLoss[0m : 2.42910
[1mStep[0m  [20/42], [94mLoss[0m : 2.45077
[1mStep[0m  [24/42], [94mLoss[0m : 2.43102
[1mStep[0m  [28/42], [94mLoss[0m : 2.49171
[1mStep[0m  [32/42], [94mLoss[0m : 2.57531
[1mStep[0m  [36/42], [94mLoss[0m : 2.67892
[1mStep[0m  [40/42], [94mLoss[0m : 2.44492

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39292
[1mStep[0m  [4/42], [94mLoss[0m : 2.31476
[1mStep[0m  [8/42], [94mLoss[0m : 2.45963
[1mStep[0m  [12/42], [94mLoss[0m : 2.49034
[1mStep[0m  [16/42], [94mLoss[0m : 2.52403
[1mStep[0m  [20/42], [94mLoss[0m : 2.56595
[1mStep[0m  [24/42], [94mLoss[0m : 2.57804
[1mStep[0m  [28/42], [94mLoss[0m : 2.68065
[1mStep[0m  [32/42], [94mLoss[0m : 2.47808
[1mStep[0m  [36/42], [94mLoss[0m : 2.58016
[1mStep[0m  [40/42], [94mLoss[0m : 2.45488

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34789
[1mStep[0m  [4/42], [94mLoss[0m : 2.52020
[1mStep[0m  [8/42], [94mLoss[0m : 2.46024
[1mStep[0m  [12/42], [94mLoss[0m : 2.34350
[1mStep[0m  [16/42], [94mLoss[0m : 2.38745
[1mStep[0m  [20/42], [94mLoss[0m : 2.48692
[1mStep[0m  [24/42], [94mLoss[0m : 2.84951
[1mStep[0m  [28/42], [94mLoss[0m : 2.75436
[1mStep[0m  [32/42], [94mLoss[0m : 2.59473
[1mStep[0m  [36/42], [94mLoss[0m : 2.32184
[1mStep[0m  [40/42], [94mLoss[0m : 2.49145

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51995
[1mStep[0m  [4/42], [94mLoss[0m : 2.57677
[1mStep[0m  [8/42], [94mLoss[0m : 2.43637
[1mStep[0m  [12/42], [94mLoss[0m : 2.57881
[1mStep[0m  [16/42], [94mLoss[0m : 2.44390
[1mStep[0m  [20/42], [94mLoss[0m : 2.38434
[1mStep[0m  [24/42], [94mLoss[0m : 2.54190
[1mStep[0m  [28/42], [94mLoss[0m : 2.57446
[1mStep[0m  [32/42], [94mLoss[0m : 2.32430
[1mStep[0m  [36/42], [94mLoss[0m : 2.68454
[1mStep[0m  [40/42], [94mLoss[0m : 2.59758

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40573
[1mStep[0m  [4/42], [94mLoss[0m : 2.35460
[1mStep[0m  [8/42], [94mLoss[0m : 2.49778
[1mStep[0m  [12/42], [94mLoss[0m : 2.58497
[1mStep[0m  [16/42], [94mLoss[0m : 2.45418
[1mStep[0m  [20/42], [94mLoss[0m : 2.34876
[1mStep[0m  [24/42], [94mLoss[0m : 2.50358
[1mStep[0m  [28/42], [94mLoss[0m : 2.61542
[1mStep[0m  [32/42], [94mLoss[0m : 2.65787
[1mStep[0m  [36/42], [94mLoss[0m : 2.62097
[1mStep[0m  [40/42], [94mLoss[0m : 2.61012

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41202
[1mStep[0m  [4/42], [94mLoss[0m : 2.44179
[1mStep[0m  [8/42], [94mLoss[0m : 2.66414
[1mStep[0m  [12/42], [94mLoss[0m : 2.43617
[1mStep[0m  [16/42], [94mLoss[0m : 2.33778
[1mStep[0m  [20/42], [94mLoss[0m : 2.71474
[1mStep[0m  [24/42], [94mLoss[0m : 2.52390
[1mStep[0m  [28/42], [94mLoss[0m : 2.46216
[1mStep[0m  [32/42], [94mLoss[0m : 2.25345
[1mStep[0m  [36/42], [94mLoss[0m : 2.44654
[1mStep[0m  [40/42], [94mLoss[0m : 2.37131

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40636
[1mStep[0m  [4/42], [94mLoss[0m : 2.56693
[1mStep[0m  [8/42], [94mLoss[0m : 2.58875
[1mStep[0m  [12/42], [94mLoss[0m : 2.80535
[1mStep[0m  [16/42], [94mLoss[0m : 2.75914
[1mStep[0m  [20/42], [94mLoss[0m : 2.59030
[1mStep[0m  [24/42], [94mLoss[0m : 2.41352
[1mStep[0m  [28/42], [94mLoss[0m : 2.30885
[1mStep[0m  [32/42], [94mLoss[0m : 2.45284
[1mStep[0m  [36/42], [94mLoss[0m : 2.46120
[1mStep[0m  [40/42], [94mLoss[0m : 2.63228

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64986
[1mStep[0m  [4/42], [94mLoss[0m : 2.56066
[1mStep[0m  [8/42], [94mLoss[0m : 2.68241
[1mStep[0m  [12/42], [94mLoss[0m : 2.73860
[1mStep[0m  [16/42], [94mLoss[0m : 2.44985
[1mStep[0m  [20/42], [94mLoss[0m : 2.48645
[1mStep[0m  [24/42], [94mLoss[0m : 2.54438
[1mStep[0m  [28/42], [94mLoss[0m : 2.55020
[1mStep[0m  [32/42], [94mLoss[0m : 2.52491
[1mStep[0m  [36/42], [94mLoss[0m : 2.41860
[1mStep[0m  [40/42], [94mLoss[0m : 2.36250

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51040
[1mStep[0m  [4/42], [94mLoss[0m : 2.49726
[1mStep[0m  [8/42], [94mLoss[0m : 2.37294
[1mStep[0m  [12/42], [94mLoss[0m : 2.65303
[1mStep[0m  [16/42], [94mLoss[0m : 2.21951
[1mStep[0m  [20/42], [94mLoss[0m : 2.55009
[1mStep[0m  [24/42], [94mLoss[0m : 2.51009
[1mStep[0m  [28/42], [94mLoss[0m : 2.60491
[1mStep[0m  [32/42], [94mLoss[0m : 2.30164
[1mStep[0m  [36/42], [94mLoss[0m : 2.65995
[1mStep[0m  [40/42], [94mLoss[0m : 2.60705

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29623
[1mStep[0m  [4/42], [94mLoss[0m : 2.51154
[1mStep[0m  [8/42], [94mLoss[0m : 2.36326
[1mStep[0m  [12/42], [94mLoss[0m : 2.35886
[1mStep[0m  [16/42], [94mLoss[0m : 2.45482
[1mStep[0m  [20/42], [94mLoss[0m : 2.53956
[1mStep[0m  [24/42], [94mLoss[0m : 2.44400
[1mStep[0m  [28/42], [94mLoss[0m : 2.19195
[1mStep[0m  [32/42], [94mLoss[0m : 2.48119
[1mStep[0m  [36/42], [94mLoss[0m : 2.61866
[1mStep[0m  [40/42], [94mLoss[0m : 2.51678

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57851
[1mStep[0m  [4/42], [94mLoss[0m : 2.47255
[1mStep[0m  [8/42], [94mLoss[0m : 2.48030
[1mStep[0m  [12/42], [94mLoss[0m : 2.39255
[1mStep[0m  [16/42], [94mLoss[0m : 2.61286
[1mStep[0m  [20/42], [94mLoss[0m : 2.36657
[1mStep[0m  [24/42], [94mLoss[0m : 2.49452
[1mStep[0m  [28/42], [94mLoss[0m : 2.23704
[1mStep[0m  [32/42], [94mLoss[0m : 2.53540
[1mStep[0m  [36/42], [94mLoss[0m : 2.53501
[1mStep[0m  [40/42], [94mLoss[0m : 2.51397

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29866
[1mStep[0m  [4/42], [94mLoss[0m : 2.59004
[1mStep[0m  [8/42], [94mLoss[0m : 2.48167
[1mStep[0m  [12/42], [94mLoss[0m : 2.25829
[1mStep[0m  [16/42], [94mLoss[0m : 2.81387
[1mStep[0m  [20/42], [94mLoss[0m : 2.72059
[1mStep[0m  [24/42], [94mLoss[0m : 2.50277
[1mStep[0m  [28/42], [94mLoss[0m : 2.54954
[1mStep[0m  [32/42], [94mLoss[0m : 2.44885
[1mStep[0m  [36/42], [94mLoss[0m : 2.70734
[1mStep[0m  [40/42], [94mLoss[0m : 2.34404

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35147
[1mStep[0m  [4/42], [94mLoss[0m : 2.56680
[1mStep[0m  [8/42], [94mLoss[0m : 2.31965
[1mStep[0m  [12/42], [94mLoss[0m : 2.36580
[1mStep[0m  [16/42], [94mLoss[0m : 2.61314
[1mStep[0m  [20/42], [94mLoss[0m : 2.45805
[1mStep[0m  [24/42], [94mLoss[0m : 2.20123
[1mStep[0m  [28/42], [94mLoss[0m : 2.67919
[1mStep[0m  [32/42], [94mLoss[0m : 2.43493
[1mStep[0m  [36/42], [94mLoss[0m : 2.35703
[1mStep[0m  [40/42], [94mLoss[0m : 2.45836

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53762
[1mStep[0m  [4/42], [94mLoss[0m : 2.66141
[1mStep[0m  [8/42], [94mLoss[0m : 2.31935
[1mStep[0m  [12/42], [94mLoss[0m : 2.61243
[1mStep[0m  [16/42], [94mLoss[0m : 2.60300
[1mStep[0m  [20/42], [94mLoss[0m : 2.40468
[1mStep[0m  [24/42], [94mLoss[0m : 2.47221
[1mStep[0m  [28/42], [94mLoss[0m : 2.54491
[1mStep[0m  [32/42], [94mLoss[0m : 2.62426
[1mStep[0m  [36/42], [94mLoss[0m : 2.34594
[1mStep[0m  [40/42], [94mLoss[0m : 2.69728

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40717
[1mStep[0m  [4/42], [94mLoss[0m : 2.45570
[1mStep[0m  [8/42], [94mLoss[0m : 2.49093
[1mStep[0m  [12/42], [94mLoss[0m : 2.47426
[1mStep[0m  [16/42], [94mLoss[0m : 2.67714
[1mStep[0m  [20/42], [94mLoss[0m : 2.30001
[1mStep[0m  [24/42], [94mLoss[0m : 2.60411
[1mStep[0m  [28/42], [94mLoss[0m : 2.40706
[1mStep[0m  [32/42], [94mLoss[0m : 2.65096
[1mStep[0m  [36/42], [94mLoss[0m : 2.45110
[1mStep[0m  [40/42], [94mLoss[0m : 2.79920

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45956
[1mStep[0m  [4/42], [94mLoss[0m : 2.61477
[1mStep[0m  [8/42], [94mLoss[0m : 2.55417
[1mStep[0m  [12/42], [94mLoss[0m : 2.44186
[1mStep[0m  [16/42], [94mLoss[0m : 2.38071
[1mStep[0m  [20/42], [94mLoss[0m : 2.41993
[1mStep[0m  [24/42], [94mLoss[0m : 2.44608
[1mStep[0m  [28/42], [94mLoss[0m : 2.34021
[1mStep[0m  [32/42], [94mLoss[0m : 2.51667
[1mStep[0m  [36/42], [94mLoss[0m : 2.24867
[1mStep[0m  [40/42], [94mLoss[0m : 2.43862

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35407
[1mStep[0m  [4/42], [94mLoss[0m : 2.55688
[1mStep[0m  [8/42], [94mLoss[0m : 2.60144
[1mStep[0m  [12/42], [94mLoss[0m : 2.38039
[1mStep[0m  [16/42], [94mLoss[0m : 2.32880
[1mStep[0m  [20/42], [94mLoss[0m : 2.47712
[1mStep[0m  [24/42], [94mLoss[0m : 2.49341
[1mStep[0m  [28/42], [94mLoss[0m : 2.35285
[1mStep[0m  [32/42], [94mLoss[0m : 2.35889
[1mStep[0m  [36/42], [94mLoss[0m : 2.55840
[1mStep[0m  [40/42], [94mLoss[0m : 2.82270

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38600
[1mStep[0m  [4/42], [94mLoss[0m : 2.21707
[1mStep[0m  [8/42], [94mLoss[0m : 2.58973
[1mStep[0m  [12/42], [94mLoss[0m : 2.29174
[1mStep[0m  [16/42], [94mLoss[0m : 2.62963
[1mStep[0m  [20/42], [94mLoss[0m : 2.40975
[1mStep[0m  [24/42], [94mLoss[0m : 2.56646
[1mStep[0m  [28/42], [94mLoss[0m : 2.56807
[1mStep[0m  [32/42], [94mLoss[0m : 2.50438
[1mStep[0m  [36/42], [94mLoss[0m : 2.53916
[1mStep[0m  [40/42], [94mLoss[0m : 2.44078

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49402
[1mStep[0m  [4/42], [94mLoss[0m : 2.50749
[1mStep[0m  [8/42], [94mLoss[0m : 2.47106
[1mStep[0m  [12/42], [94mLoss[0m : 2.39739
[1mStep[0m  [16/42], [94mLoss[0m : 2.34000
[1mStep[0m  [20/42], [94mLoss[0m : 2.46364
[1mStep[0m  [24/42], [94mLoss[0m : 2.49478
[1mStep[0m  [28/42], [94mLoss[0m : 2.47343
[1mStep[0m  [32/42], [94mLoss[0m : 2.55797
[1mStep[0m  [36/42], [94mLoss[0m : 2.30292
[1mStep[0m  [40/42], [94mLoss[0m : 2.29848

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38412
[1mStep[0m  [4/42], [94mLoss[0m : 2.19753
[1mStep[0m  [8/42], [94mLoss[0m : 2.57193
[1mStep[0m  [12/42], [94mLoss[0m : 2.29467
[1mStep[0m  [16/42], [94mLoss[0m : 2.61583
[1mStep[0m  [20/42], [94mLoss[0m : 2.34299
[1mStep[0m  [24/42], [94mLoss[0m : 2.33119
[1mStep[0m  [28/42], [94mLoss[0m : 2.43682
[1mStep[0m  [32/42], [94mLoss[0m : 2.46239
[1mStep[0m  [36/42], [94mLoss[0m : 2.37334
[1mStep[0m  [40/42], [94mLoss[0m : 2.57684

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.322753276143755
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.40583
[1mStep[0m  [4/42], [94mLoss[0m : 2.46976
[1mStep[0m  [8/42], [94mLoss[0m : 2.33153
[1mStep[0m  [12/42], [94mLoss[0m : 2.54924
[1mStep[0m  [16/42], [94mLoss[0m : 2.41170
[1mStep[0m  [20/42], [94mLoss[0m : 2.74121
[1mStep[0m  [24/42], [94mLoss[0m : 2.55698
[1mStep[0m  [28/42], [94mLoss[0m : 2.44193
[1mStep[0m  [32/42], [94mLoss[0m : 2.47861
[1mStep[0m  [36/42], [94mLoss[0m : 2.59826
[1mStep[0m  [40/42], [94mLoss[0m : 2.55859

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60508
[1mStep[0m  [4/42], [94mLoss[0m : 2.41879
[1mStep[0m  [8/42], [94mLoss[0m : 2.59407
[1mStep[0m  [12/42], [94mLoss[0m : 2.42308
[1mStep[0m  [16/42], [94mLoss[0m : 2.64990
[1mStep[0m  [20/42], [94mLoss[0m : 2.47333
[1mStep[0m  [24/42], [94mLoss[0m : 2.35014
[1mStep[0m  [28/42], [94mLoss[0m : 2.48833
[1mStep[0m  [32/42], [94mLoss[0m : 2.50007
[1mStep[0m  [36/42], [94mLoss[0m : 2.48379
[1mStep[0m  [40/42], [94mLoss[0m : 2.45796

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48114
[1mStep[0m  [4/42], [94mLoss[0m : 2.20739
[1mStep[0m  [8/42], [94mLoss[0m : 2.26999
[1mStep[0m  [12/42], [94mLoss[0m : 2.49021
[1mStep[0m  [16/42], [94mLoss[0m : 2.40220
[1mStep[0m  [20/42], [94mLoss[0m : 2.52043
[1mStep[0m  [24/42], [94mLoss[0m : 2.75955
[1mStep[0m  [28/42], [94mLoss[0m : 2.59998
[1mStep[0m  [32/42], [94mLoss[0m : 2.28797
[1mStep[0m  [36/42], [94mLoss[0m : 2.43762
[1mStep[0m  [40/42], [94mLoss[0m : 2.24350

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28505
[1mStep[0m  [4/42], [94mLoss[0m : 2.46753
[1mStep[0m  [8/42], [94mLoss[0m : 2.29766
[1mStep[0m  [12/42], [94mLoss[0m : 2.76337
[1mStep[0m  [16/42], [94mLoss[0m : 2.24485
[1mStep[0m  [20/42], [94mLoss[0m : 2.30407
[1mStep[0m  [24/42], [94mLoss[0m : 2.37055
[1mStep[0m  [28/42], [94mLoss[0m : 2.54975
[1mStep[0m  [32/42], [94mLoss[0m : 2.50320
[1mStep[0m  [36/42], [94mLoss[0m : 2.29983
[1mStep[0m  [40/42], [94mLoss[0m : 2.51519

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.311, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39676
[1mStep[0m  [4/42], [94mLoss[0m : 2.50483
[1mStep[0m  [8/42], [94mLoss[0m : 2.44246
[1mStep[0m  [12/42], [94mLoss[0m : 2.30094
[1mStep[0m  [16/42], [94mLoss[0m : 2.44199
[1mStep[0m  [20/42], [94mLoss[0m : 2.18020
[1mStep[0m  [24/42], [94mLoss[0m : 2.12712
[1mStep[0m  [28/42], [94mLoss[0m : 2.49479
[1mStep[0m  [32/42], [94mLoss[0m : 2.26035
[1mStep[0m  [36/42], [94mLoss[0m : 2.31745
[1mStep[0m  [40/42], [94mLoss[0m : 2.36590

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.310, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41925
[1mStep[0m  [4/42], [94mLoss[0m : 2.33837
[1mStep[0m  [8/42], [94mLoss[0m : 2.19931
[1mStep[0m  [12/42], [94mLoss[0m : 2.31803
[1mStep[0m  [16/42], [94mLoss[0m : 2.56511
[1mStep[0m  [20/42], [94mLoss[0m : 2.26982
[1mStep[0m  [24/42], [94mLoss[0m : 2.48005
[1mStep[0m  [28/42], [94mLoss[0m : 2.45688
[1mStep[0m  [32/42], [94mLoss[0m : 2.43760
[1mStep[0m  [36/42], [94mLoss[0m : 2.39939
[1mStep[0m  [40/42], [94mLoss[0m : 2.26496

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20114
[1mStep[0m  [4/42], [94mLoss[0m : 2.30327
[1mStep[0m  [8/42], [94mLoss[0m : 2.09498
[1mStep[0m  [12/42], [94mLoss[0m : 2.29606
[1mStep[0m  [16/42], [94mLoss[0m : 2.36054
[1mStep[0m  [20/42], [94mLoss[0m : 2.27146
[1mStep[0m  [24/42], [94mLoss[0m : 2.29399
[1mStep[0m  [28/42], [94mLoss[0m : 2.51419
[1mStep[0m  [32/42], [94mLoss[0m : 2.27058
[1mStep[0m  [36/42], [94mLoss[0m : 2.29019
[1mStep[0m  [40/42], [94mLoss[0m : 2.40336

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32357
[1mStep[0m  [4/42], [94mLoss[0m : 2.32386
[1mStep[0m  [8/42], [94mLoss[0m : 2.38217
[1mStep[0m  [12/42], [94mLoss[0m : 2.29418
[1mStep[0m  [16/42], [94mLoss[0m : 2.33679
[1mStep[0m  [20/42], [94mLoss[0m : 2.17911
[1mStep[0m  [24/42], [94mLoss[0m : 2.39979
[1mStep[0m  [28/42], [94mLoss[0m : 2.26651
[1mStep[0m  [32/42], [94mLoss[0m : 2.03201
[1mStep[0m  [36/42], [94mLoss[0m : 2.12601
[1mStep[0m  [40/42], [94mLoss[0m : 2.12713

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34764
[1mStep[0m  [4/42], [94mLoss[0m : 2.33686
[1mStep[0m  [8/42], [94mLoss[0m : 2.28378
[1mStep[0m  [12/42], [94mLoss[0m : 2.15804
[1mStep[0m  [16/42], [94mLoss[0m : 2.36449
[1mStep[0m  [20/42], [94mLoss[0m : 2.32192
[1mStep[0m  [24/42], [94mLoss[0m : 2.23489
[1mStep[0m  [28/42], [94mLoss[0m : 2.33752
[1mStep[0m  [32/42], [94mLoss[0m : 2.03021
[1mStep[0m  [36/42], [94mLoss[0m : 2.23264
[1mStep[0m  [40/42], [94mLoss[0m : 2.27174

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43621
[1mStep[0m  [4/42], [94mLoss[0m : 2.40969
[1mStep[0m  [8/42], [94mLoss[0m : 2.14023
[1mStep[0m  [12/42], [94mLoss[0m : 2.29657
[1mStep[0m  [16/42], [94mLoss[0m : 2.24699
[1mStep[0m  [20/42], [94mLoss[0m : 2.31043
[1mStep[0m  [24/42], [94mLoss[0m : 2.10556
[1mStep[0m  [28/42], [94mLoss[0m : 2.24371
[1mStep[0m  [32/42], [94mLoss[0m : 2.38569
[1mStep[0m  [36/42], [94mLoss[0m : 2.18885
[1mStep[0m  [40/42], [94mLoss[0m : 2.32422

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00043
[1mStep[0m  [4/42], [94mLoss[0m : 2.23186
[1mStep[0m  [8/42], [94mLoss[0m : 2.37837
[1mStep[0m  [12/42], [94mLoss[0m : 2.18197
[1mStep[0m  [16/42], [94mLoss[0m : 2.29420
[1mStep[0m  [20/42], [94mLoss[0m : 2.05784
[1mStep[0m  [24/42], [94mLoss[0m : 2.14055
[1mStep[0m  [28/42], [94mLoss[0m : 2.25082
[1mStep[0m  [32/42], [94mLoss[0m : 2.25434
[1mStep[0m  [36/42], [94mLoss[0m : 2.16482
[1mStep[0m  [40/42], [94mLoss[0m : 2.43841

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13765
[1mStep[0m  [4/42], [94mLoss[0m : 2.01750
[1mStep[0m  [8/42], [94mLoss[0m : 2.05852
[1mStep[0m  [12/42], [94mLoss[0m : 2.14378
[1mStep[0m  [16/42], [94mLoss[0m : 2.42983
[1mStep[0m  [20/42], [94mLoss[0m : 2.03564
[1mStep[0m  [24/42], [94mLoss[0m : 2.16445
[1mStep[0m  [28/42], [94mLoss[0m : 2.37647
[1mStep[0m  [32/42], [94mLoss[0m : 2.19580
[1mStep[0m  [36/42], [94mLoss[0m : 1.96756
[1mStep[0m  [40/42], [94mLoss[0m : 2.12941

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03633
[1mStep[0m  [4/42], [94mLoss[0m : 2.10134
[1mStep[0m  [8/42], [94mLoss[0m : 2.14434
[1mStep[0m  [12/42], [94mLoss[0m : 1.90112
[1mStep[0m  [16/42], [94mLoss[0m : 2.27652
[1mStep[0m  [20/42], [94mLoss[0m : 2.05128
[1mStep[0m  [24/42], [94mLoss[0m : 2.04429
[1mStep[0m  [28/42], [94mLoss[0m : 2.10354
[1mStep[0m  [32/42], [94mLoss[0m : 2.31724
[1mStep[0m  [36/42], [94mLoss[0m : 2.07254
[1mStep[0m  [40/42], [94mLoss[0m : 2.26234

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14091
[1mStep[0m  [4/42], [94mLoss[0m : 2.11595
[1mStep[0m  [8/42], [94mLoss[0m : 2.04500
[1mStep[0m  [12/42], [94mLoss[0m : 2.03334
[1mStep[0m  [16/42], [94mLoss[0m : 2.21478
[1mStep[0m  [20/42], [94mLoss[0m : 1.98022
[1mStep[0m  [24/42], [94mLoss[0m : 2.09105
[1mStep[0m  [28/42], [94mLoss[0m : 2.00319
[1mStep[0m  [32/42], [94mLoss[0m : 2.03477
[1mStep[0m  [36/42], [94mLoss[0m : 2.08766
[1mStep[0m  [40/42], [94mLoss[0m : 1.99731

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.07998
[1mStep[0m  [4/42], [94mLoss[0m : 1.98849
[1mStep[0m  [8/42], [94mLoss[0m : 1.91205
[1mStep[0m  [12/42], [94mLoss[0m : 2.12839
[1mStep[0m  [16/42], [94mLoss[0m : 2.13422
[1mStep[0m  [20/42], [94mLoss[0m : 2.03608
[1mStep[0m  [24/42], [94mLoss[0m : 2.15996
[1mStep[0m  [28/42], [94mLoss[0m : 2.14755
[1mStep[0m  [32/42], [94mLoss[0m : 1.76512
[1mStep[0m  [36/42], [94mLoss[0m : 2.20553
[1mStep[0m  [40/42], [94mLoss[0m : 2.42435

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98593
[1mStep[0m  [4/42], [94mLoss[0m : 1.97159
[1mStep[0m  [8/42], [94mLoss[0m : 2.19280
[1mStep[0m  [12/42], [94mLoss[0m : 2.09942
[1mStep[0m  [16/42], [94mLoss[0m : 2.00318
[1mStep[0m  [20/42], [94mLoss[0m : 1.84843
[1mStep[0m  [24/42], [94mLoss[0m : 2.08095
[1mStep[0m  [28/42], [94mLoss[0m : 2.08034
[1mStep[0m  [32/42], [94mLoss[0m : 1.98740
[1mStep[0m  [36/42], [94mLoss[0m : 2.07318
[1mStep[0m  [40/42], [94mLoss[0m : 1.98330

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94018
[1mStep[0m  [4/42], [94mLoss[0m : 1.89176
[1mStep[0m  [8/42], [94mLoss[0m : 1.82844
[1mStep[0m  [12/42], [94mLoss[0m : 1.95670
[1mStep[0m  [16/42], [94mLoss[0m : 2.16292
[1mStep[0m  [20/42], [94mLoss[0m : 2.04469
[1mStep[0m  [24/42], [94mLoss[0m : 2.02813
[1mStep[0m  [28/42], [94mLoss[0m : 2.02711
[1mStep[0m  [32/42], [94mLoss[0m : 1.97774
[1mStep[0m  [36/42], [94mLoss[0m : 1.96127
[1mStep[0m  [40/42], [94mLoss[0m : 2.06166

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00449
[1mStep[0m  [4/42], [94mLoss[0m : 1.75166
[1mStep[0m  [8/42], [94mLoss[0m : 1.93042
[1mStep[0m  [12/42], [94mLoss[0m : 1.85331
[1mStep[0m  [16/42], [94mLoss[0m : 1.93129
[1mStep[0m  [20/42], [94mLoss[0m : 1.86013
[1mStep[0m  [24/42], [94mLoss[0m : 2.01680
[1mStep[0m  [28/42], [94mLoss[0m : 2.03282
[1mStep[0m  [32/42], [94mLoss[0m : 1.85981
[1mStep[0m  [36/42], [94mLoss[0m : 1.95673
[1mStep[0m  [40/42], [94mLoss[0m : 1.99105

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00872
[1mStep[0m  [4/42], [94mLoss[0m : 1.84180
[1mStep[0m  [8/42], [94mLoss[0m : 1.81968
[1mStep[0m  [12/42], [94mLoss[0m : 1.82726
[1mStep[0m  [16/42], [94mLoss[0m : 1.98306
[1mStep[0m  [20/42], [94mLoss[0m : 1.95494
[1mStep[0m  [24/42], [94mLoss[0m : 1.92765
[1mStep[0m  [28/42], [94mLoss[0m : 1.79865
[1mStep[0m  [32/42], [94mLoss[0m : 1.95926
[1mStep[0m  [36/42], [94mLoss[0m : 2.19148
[1mStep[0m  [40/42], [94mLoss[0m : 1.95246

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83590
[1mStep[0m  [4/42], [94mLoss[0m : 1.94730
[1mStep[0m  [8/42], [94mLoss[0m : 1.74413
[1mStep[0m  [12/42], [94mLoss[0m : 1.64641
[1mStep[0m  [16/42], [94mLoss[0m : 1.95077
[1mStep[0m  [20/42], [94mLoss[0m : 2.14196
[1mStep[0m  [24/42], [94mLoss[0m : 1.94174
[1mStep[0m  [28/42], [94mLoss[0m : 1.80053
[1mStep[0m  [32/42], [94mLoss[0m : 1.90436
[1mStep[0m  [36/42], [94mLoss[0m : 1.88174
[1mStep[0m  [40/42], [94mLoss[0m : 1.80804

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96292
[1mStep[0m  [4/42], [94mLoss[0m : 1.97814
[1mStep[0m  [8/42], [94mLoss[0m : 1.83303
[1mStep[0m  [12/42], [94mLoss[0m : 2.00327
[1mStep[0m  [16/42], [94mLoss[0m : 1.74432
[1mStep[0m  [20/42], [94mLoss[0m : 1.77670
[1mStep[0m  [24/42], [94mLoss[0m : 1.88571
[1mStep[0m  [28/42], [94mLoss[0m : 1.80292
[1mStep[0m  [32/42], [94mLoss[0m : 1.86983
[1mStep[0m  [36/42], [94mLoss[0m : 1.79341
[1mStep[0m  [40/42], [94mLoss[0m : 1.99890

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.854, [92mTest[0m: 2.545, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65881
[1mStep[0m  [4/42], [94mLoss[0m : 1.93774
[1mStep[0m  [8/42], [94mLoss[0m : 1.79708
[1mStep[0m  [12/42], [94mLoss[0m : 1.81756
[1mStep[0m  [16/42], [94mLoss[0m : 1.68446
[1mStep[0m  [20/42], [94mLoss[0m : 1.95342
[1mStep[0m  [24/42], [94mLoss[0m : 1.69960
[1mStep[0m  [28/42], [94mLoss[0m : 1.80523
[1mStep[0m  [32/42], [94mLoss[0m : 1.63596
[1mStep[0m  [36/42], [94mLoss[0m : 1.70997
[1mStep[0m  [40/42], [94mLoss[0m : 2.01124

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69029
[1mStep[0m  [4/42], [94mLoss[0m : 1.84634
[1mStep[0m  [8/42], [94mLoss[0m : 1.72407
[1mStep[0m  [12/42], [94mLoss[0m : 2.01105
[1mStep[0m  [16/42], [94mLoss[0m : 1.92774
[1mStep[0m  [20/42], [94mLoss[0m : 1.73077
[1mStep[0m  [24/42], [94mLoss[0m : 1.78857
[1mStep[0m  [28/42], [94mLoss[0m : 1.85269
[1mStep[0m  [32/42], [94mLoss[0m : 1.59630
[1mStep[0m  [36/42], [94mLoss[0m : 1.70469
[1mStep[0m  [40/42], [94mLoss[0m : 1.85034

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95275
[1mStep[0m  [4/42], [94mLoss[0m : 1.69148
[1mStep[0m  [8/42], [94mLoss[0m : 1.70209
[1mStep[0m  [12/42], [94mLoss[0m : 1.74781
[1mStep[0m  [16/42], [94mLoss[0m : 1.80362
[1mStep[0m  [20/42], [94mLoss[0m : 1.96559
[1mStep[0m  [24/42], [94mLoss[0m : 1.78066
[1mStep[0m  [28/42], [94mLoss[0m : 1.75452
[1mStep[0m  [32/42], [94mLoss[0m : 1.80725
[1mStep[0m  [36/42], [94mLoss[0m : 1.79747
[1mStep[0m  [40/42], [94mLoss[0m : 1.70456

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61329
[1mStep[0m  [4/42], [94mLoss[0m : 1.63106
[1mStep[0m  [8/42], [94mLoss[0m : 1.75325
[1mStep[0m  [12/42], [94mLoss[0m : 1.95383
[1mStep[0m  [16/42], [94mLoss[0m : 1.48274
[1mStep[0m  [20/42], [94mLoss[0m : 1.48102
[1mStep[0m  [24/42], [94mLoss[0m : 1.63932
[1mStep[0m  [28/42], [94mLoss[0m : 1.94165
[1mStep[0m  [32/42], [94mLoss[0m : 1.79367
[1mStep[0m  [36/42], [94mLoss[0m : 1.73992
[1mStep[0m  [40/42], [94mLoss[0m : 1.80292

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63791
[1mStep[0m  [4/42], [94mLoss[0m : 1.61364
[1mStep[0m  [8/42], [94mLoss[0m : 1.84772
[1mStep[0m  [12/42], [94mLoss[0m : 1.56606
[1mStep[0m  [16/42], [94mLoss[0m : 1.67179
[1mStep[0m  [20/42], [94mLoss[0m : 1.59952
[1mStep[0m  [24/42], [94mLoss[0m : 1.82892
[1mStep[0m  [28/42], [94mLoss[0m : 1.77052
[1mStep[0m  [32/42], [94mLoss[0m : 1.68365
[1mStep[0m  [36/42], [94mLoss[0m : 1.71871
[1mStep[0m  [40/42], [94mLoss[0m : 1.71217

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.74561
[1mStep[0m  [4/42], [94mLoss[0m : 1.66832
[1mStep[0m  [8/42], [94mLoss[0m : 1.72716
[1mStep[0m  [12/42], [94mLoss[0m : 1.70099
[1mStep[0m  [16/42], [94mLoss[0m : 1.61192
[1mStep[0m  [20/42], [94mLoss[0m : 1.59475
[1mStep[0m  [24/42], [94mLoss[0m : 1.64599
[1mStep[0m  [28/42], [94mLoss[0m : 1.85231
[1mStep[0m  [32/42], [94mLoss[0m : 1.66280
[1mStep[0m  [36/42], [94mLoss[0m : 1.58400
[1mStep[0m  [40/42], [94mLoss[0m : 1.69359

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.69049
[1mStep[0m  [4/42], [94mLoss[0m : 1.76680
[1mStep[0m  [8/42], [94mLoss[0m : 1.54349
[1mStep[0m  [12/42], [94mLoss[0m : 1.69633
[1mStep[0m  [16/42], [94mLoss[0m : 1.58057
[1mStep[0m  [20/42], [94mLoss[0m : 1.58363
[1mStep[0m  [24/42], [94mLoss[0m : 1.80140
[1mStep[0m  [28/42], [94mLoss[0m : 1.51173
[1mStep[0m  [32/42], [94mLoss[0m : 1.64721
[1mStep[0m  [36/42], [94mLoss[0m : 1.73290
[1mStep[0m  [40/42], [94mLoss[0m : 1.73664

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66188
[1mStep[0m  [4/42], [94mLoss[0m : 1.53929
[1mStep[0m  [8/42], [94mLoss[0m : 1.53267
[1mStep[0m  [12/42], [94mLoss[0m : 1.46375
[1mStep[0m  [16/42], [94mLoss[0m : 1.51365
[1mStep[0m  [20/42], [94mLoss[0m : 1.56710
[1mStep[0m  [24/42], [94mLoss[0m : 1.64008
[1mStep[0m  [28/42], [94mLoss[0m : 1.57987
[1mStep[0m  [32/42], [94mLoss[0m : 1.60327
[1mStep[0m  [36/42], [94mLoss[0m : 1.52605
[1mStep[0m  [40/42], [94mLoss[0m : 1.74285

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56138
[1mStep[0m  [4/42], [94mLoss[0m : 1.68518
[1mStep[0m  [8/42], [94mLoss[0m : 1.58685
[1mStep[0m  [12/42], [94mLoss[0m : 1.68530
[1mStep[0m  [16/42], [94mLoss[0m : 1.63018
[1mStep[0m  [20/42], [94mLoss[0m : 1.66962
[1mStep[0m  [24/42], [94mLoss[0m : 1.56823
[1mStep[0m  [28/42], [94mLoss[0m : 1.72015
[1mStep[0m  [32/42], [94mLoss[0m : 1.67787
[1mStep[0m  [36/42], [94mLoss[0m : 1.70994
[1mStep[0m  [40/42], [94mLoss[0m : 1.86219

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.580, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.4950001750673567
MAE score P1       2.322753
MAE score P2          2.495
loss               1.622681
learning_rate       0.00505
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.75051
[1mStep[0m  [4/42], [94mLoss[0m : 10.12032
[1mStep[0m  [8/42], [94mLoss[0m : 9.28846
[1mStep[0m  [12/42], [94mLoss[0m : 9.24853
[1mStep[0m  [16/42], [94mLoss[0m : 8.69460
[1mStep[0m  [20/42], [94mLoss[0m : 7.75941
[1mStep[0m  [24/42], [94mLoss[0m : 7.20350
[1mStep[0m  [28/42], [94mLoss[0m : 6.79019
[1mStep[0m  [32/42], [94mLoss[0m : 6.13156
[1mStep[0m  [36/42], [94mLoss[0m : 5.23010
[1mStep[0m  [40/42], [94mLoss[0m : 5.03080

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.793, [92mTest[0m: 10.773, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.06485
[1mStep[0m  [4/42], [94mLoss[0m : 4.44454
[1mStep[0m  [8/42], [94mLoss[0m : 4.22112
[1mStep[0m  [12/42], [94mLoss[0m : 4.13240
[1mStep[0m  [16/42], [94mLoss[0m : 3.99499
[1mStep[0m  [20/42], [94mLoss[0m : 3.47054
[1mStep[0m  [24/42], [94mLoss[0m : 3.11545
[1mStep[0m  [28/42], [94mLoss[0m : 3.34143
[1mStep[0m  [32/42], [94mLoss[0m : 2.92783
[1mStep[0m  [36/42], [94mLoss[0m : 2.72816
[1mStep[0m  [40/42], [94mLoss[0m : 2.94157

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.538, [92mTest[0m: 4.797, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92554
[1mStep[0m  [4/42], [94mLoss[0m : 2.75864
[1mStep[0m  [8/42], [94mLoss[0m : 3.04010
[1mStep[0m  [12/42], [94mLoss[0m : 3.01452
[1mStep[0m  [16/42], [94mLoss[0m : 2.69660
[1mStep[0m  [20/42], [94mLoss[0m : 2.39522
[1mStep[0m  [24/42], [94mLoss[0m : 2.47535
[1mStep[0m  [28/42], [94mLoss[0m : 2.57222
[1mStep[0m  [32/42], [94mLoss[0m : 2.49015
[1mStep[0m  [36/42], [94mLoss[0m : 2.74911
[1mStep[0m  [40/42], [94mLoss[0m : 2.78596

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.718, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67952
[1mStep[0m  [4/42], [94mLoss[0m : 2.52171
[1mStep[0m  [8/42], [94mLoss[0m : 2.67397
[1mStep[0m  [12/42], [94mLoss[0m : 2.45554
[1mStep[0m  [16/42], [94mLoss[0m : 2.61260
[1mStep[0m  [20/42], [94mLoss[0m : 2.49525
[1mStep[0m  [24/42], [94mLoss[0m : 2.59164
[1mStep[0m  [28/42], [94mLoss[0m : 2.46369
[1mStep[0m  [32/42], [94mLoss[0m : 2.61304
[1mStep[0m  [36/42], [94mLoss[0m : 2.50905
[1mStep[0m  [40/42], [94mLoss[0m : 2.60447

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67513
[1mStep[0m  [4/42], [94mLoss[0m : 2.78765
[1mStep[0m  [8/42], [94mLoss[0m : 2.73986
[1mStep[0m  [12/42], [94mLoss[0m : 2.71910
[1mStep[0m  [16/42], [94mLoss[0m : 2.62873
[1mStep[0m  [20/42], [94mLoss[0m : 2.51300
[1mStep[0m  [24/42], [94mLoss[0m : 2.55377
[1mStep[0m  [28/42], [94mLoss[0m : 2.52349
[1mStep[0m  [32/42], [94mLoss[0m : 2.40463
[1mStep[0m  [36/42], [94mLoss[0m : 2.56589
[1mStep[0m  [40/42], [94mLoss[0m : 2.78783

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57319
[1mStep[0m  [4/42], [94mLoss[0m : 2.55755
[1mStep[0m  [8/42], [94mLoss[0m : 2.59028
[1mStep[0m  [12/42], [94mLoss[0m : 2.65788
[1mStep[0m  [16/42], [94mLoss[0m : 2.41452
[1mStep[0m  [20/42], [94mLoss[0m : 2.53091
[1mStep[0m  [24/42], [94mLoss[0m : 2.52690
[1mStep[0m  [28/42], [94mLoss[0m : 2.25715
[1mStep[0m  [32/42], [94mLoss[0m : 2.49949
[1mStep[0m  [36/42], [94mLoss[0m : 2.78427
[1mStep[0m  [40/42], [94mLoss[0m : 2.48887

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53730
[1mStep[0m  [4/42], [94mLoss[0m : 2.62564
[1mStep[0m  [8/42], [94mLoss[0m : 2.54379
[1mStep[0m  [12/42], [94mLoss[0m : 2.49939
[1mStep[0m  [16/42], [94mLoss[0m : 2.57125
[1mStep[0m  [20/42], [94mLoss[0m : 2.63662
[1mStep[0m  [24/42], [94mLoss[0m : 2.46391
[1mStep[0m  [28/42], [94mLoss[0m : 2.76909
[1mStep[0m  [32/42], [94mLoss[0m : 2.56532
[1mStep[0m  [36/42], [94mLoss[0m : 2.49174
[1mStep[0m  [40/42], [94mLoss[0m : 2.54229

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45337
[1mStep[0m  [4/42], [94mLoss[0m : 2.65246
[1mStep[0m  [8/42], [94mLoss[0m : 2.49924
[1mStep[0m  [12/42], [94mLoss[0m : 2.60145
[1mStep[0m  [16/42], [94mLoss[0m : 2.52142
[1mStep[0m  [20/42], [94mLoss[0m : 2.60464
[1mStep[0m  [24/42], [94mLoss[0m : 2.50580
[1mStep[0m  [28/42], [94mLoss[0m : 2.66009
[1mStep[0m  [32/42], [94mLoss[0m : 2.52676
[1mStep[0m  [36/42], [94mLoss[0m : 2.59093
[1mStep[0m  [40/42], [94mLoss[0m : 2.66428

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55382
[1mStep[0m  [4/42], [94mLoss[0m : 2.46477
[1mStep[0m  [8/42], [94mLoss[0m : 2.57649
[1mStep[0m  [12/42], [94mLoss[0m : 2.55332
[1mStep[0m  [16/42], [94mLoss[0m : 2.54813
[1mStep[0m  [20/42], [94mLoss[0m : 2.55792
[1mStep[0m  [24/42], [94mLoss[0m : 2.60104
[1mStep[0m  [28/42], [94mLoss[0m : 2.61009
[1mStep[0m  [32/42], [94mLoss[0m : 2.59341
[1mStep[0m  [36/42], [94mLoss[0m : 2.48354
[1mStep[0m  [40/42], [94mLoss[0m : 2.58451

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50525
[1mStep[0m  [4/42], [94mLoss[0m : 2.51697
[1mStep[0m  [8/42], [94mLoss[0m : 2.60531
[1mStep[0m  [12/42], [94mLoss[0m : 2.65225
[1mStep[0m  [16/42], [94mLoss[0m : 2.37681
[1mStep[0m  [20/42], [94mLoss[0m : 2.55673
[1mStep[0m  [24/42], [94mLoss[0m : 2.44943
[1mStep[0m  [28/42], [94mLoss[0m : 2.54953
[1mStep[0m  [32/42], [94mLoss[0m : 2.56511
[1mStep[0m  [36/42], [94mLoss[0m : 2.56555
[1mStep[0m  [40/42], [94mLoss[0m : 2.74981

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74262
[1mStep[0m  [4/42], [94mLoss[0m : 2.86321
[1mStep[0m  [8/42], [94mLoss[0m : 2.44905
[1mStep[0m  [12/42], [94mLoss[0m : 2.54698
[1mStep[0m  [16/42], [94mLoss[0m : 2.30787
[1mStep[0m  [20/42], [94mLoss[0m : 2.59406
[1mStep[0m  [24/42], [94mLoss[0m : 2.54386
[1mStep[0m  [28/42], [94mLoss[0m : 2.33718
[1mStep[0m  [32/42], [94mLoss[0m : 2.63943
[1mStep[0m  [36/42], [94mLoss[0m : 2.38187
[1mStep[0m  [40/42], [94mLoss[0m : 2.41804

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50222
[1mStep[0m  [4/42], [94mLoss[0m : 2.51499
[1mStep[0m  [8/42], [94mLoss[0m : 2.50206
[1mStep[0m  [12/42], [94mLoss[0m : 2.51915
[1mStep[0m  [16/42], [94mLoss[0m : 2.63182
[1mStep[0m  [20/42], [94mLoss[0m : 2.46384
[1mStep[0m  [24/42], [94mLoss[0m : 2.51864
[1mStep[0m  [28/42], [94mLoss[0m : 2.50313
[1mStep[0m  [32/42], [94mLoss[0m : 2.36358
[1mStep[0m  [36/42], [94mLoss[0m : 2.46253
[1mStep[0m  [40/42], [94mLoss[0m : 2.49036

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47225
[1mStep[0m  [4/42], [94mLoss[0m : 2.48779
[1mStep[0m  [8/42], [94mLoss[0m : 2.40381
[1mStep[0m  [12/42], [94mLoss[0m : 2.66678
[1mStep[0m  [16/42], [94mLoss[0m : 2.85305
[1mStep[0m  [20/42], [94mLoss[0m : 2.37740
[1mStep[0m  [24/42], [94mLoss[0m : 2.45685
[1mStep[0m  [28/42], [94mLoss[0m : 2.28392
[1mStep[0m  [32/42], [94mLoss[0m : 2.58632
[1mStep[0m  [36/42], [94mLoss[0m : 2.50754
[1mStep[0m  [40/42], [94mLoss[0m : 2.32127

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53432
[1mStep[0m  [4/42], [94mLoss[0m : 2.52119
[1mStep[0m  [8/42], [94mLoss[0m : 2.34180
[1mStep[0m  [12/42], [94mLoss[0m : 2.42289
[1mStep[0m  [16/42], [94mLoss[0m : 2.62457
[1mStep[0m  [20/42], [94mLoss[0m : 2.57852
[1mStep[0m  [24/42], [94mLoss[0m : 2.57966
[1mStep[0m  [28/42], [94mLoss[0m : 2.79096
[1mStep[0m  [32/42], [94mLoss[0m : 2.42701
[1mStep[0m  [36/42], [94mLoss[0m : 2.59729
[1mStep[0m  [40/42], [94mLoss[0m : 2.41898

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49633
[1mStep[0m  [4/42], [94mLoss[0m : 2.58744
[1mStep[0m  [8/42], [94mLoss[0m : 2.46640
[1mStep[0m  [12/42], [94mLoss[0m : 2.50039
[1mStep[0m  [16/42], [94mLoss[0m : 2.28446
[1mStep[0m  [20/42], [94mLoss[0m : 2.23367
[1mStep[0m  [24/42], [94mLoss[0m : 2.40403
[1mStep[0m  [28/42], [94mLoss[0m : 2.50079
[1mStep[0m  [32/42], [94mLoss[0m : 2.60282
[1mStep[0m  [36/42], [94mLoss[0m : 2.54400
[1mStep[0m  [40/42], [94mLoss[0m : 2.32398

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44531
[1mStep[0m  [4/42], [94mLoss[0m : 2.38283
[1mStep[0m  [8/42], [94mLoss[0m : 2.30408
[1mStep[0m  [12/42], [94mLoss[0m : 2.50581
[1mStep[0m  [16/42], [94mLoss[0m : 2.46137
[1mStep[0m  [20/42], [94mLoss[0m : 2.47270
[1mStep[0m  [24/42], [94mLoss[0m : 2.56314
[1mStep[0m  [28/42], [94mLoss[0m : 2.59335
[1mStep[0m  [32/42], [94mLoss[0m : 2.41296
[1mStep[0m  [36/42], [94mLoss[0m : 2.32040
[1mStep[0m  [40/42], [94mLoss[0m : 2.36780

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64103
[1mStep[0m  [4/42], [94mLoss[0m : 2.60920
[1mStep[0m  [8/42], [94mLoss[0m : 2.49246
[1mStep[0m  [12/42], [94mLoss[0m : 2.58663
[1mStep[0m  [16/42], [94mLoss[0m : 2.35287
[1mStep[0m  [20/42], [94mLoss[0m : 2.62377
[1mStep[0m  [24/42], [94mLoss[0m : 2.57015
[1mStep[0m  [28/42], [94mLoss[0m : 2.30756
[1mStep[0m  [32/42], [94mLoss[0m : 2.33365
[1mStep[0m  [36/42], [94mLoss[0m : 2.55638
[1mStep[0m  [40/42], [94mLoss[0m : 2.43443

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57516
[1mStep[0m  [4/42], [94mLoss[0m : 2.50564
[1mStep[0m  [8/42], [94mLoss[0m : 2.56106
[1mStep[0m  [12/42], [94mLoss[0m : 2.50539
[1mStep[0m  [16/42], [94mLoss[0m : 2.40775
[1mStep[0m  [20/42], [94mLoss[0m : 2.48490
[1mStep[0m  [24/42], [94mLoss[0m : 2.47077
[1mStep[0m  [28/42], [94mLoss[0m : 2.31304
[1mStep[0m  [32/42], [94mLoss[0m : 2.66242
[1mStep[0m  [36/42], [94mLoss[0m : 2.49769
[1mStep[0m  [40/42], [94mLoss[0m : 2.69307

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50453
[1mStep[0m  [4/42], [94mLoss[0m : 2.46683
[1mStep[0m  [8/42], [94mLoss[0m : 2.65961
[1mStep[0m  [12/42], [94mLoss[0m : 2.54818
[1mStep[0m  [16/42], [94mLoss[0m : 2.52543
[1mStep[0m  [20/42], [94mLoss[0m : 2.42140
[1mStep[0m  [24/42], [94mLoss[0m : 2.37888
[1mStep[0m  [28/42], [94mLoss[0m : 2.54149
[1mStep[0m  [32/42], [94mLoss[0m : 2.59982
[1mStep[0m  [36/42], [94mLoss[0m : 2.45382
[1mStep[0m  [40/42], [94mLoss[0m : 2.54134

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61131
[1mStep[0m  [4/42], [94mLoss[0m : 2.85286
[1mStep[0m  [8/42], [94mLoss[0m : 2.35240
[1mStep[0m  [12/42], [94mLoss[0m : 2.48716
[1mStep[0m  [16/42], [94mLoss[0m : 2.63737
[1mStep[0m  [20/42], [94mLoss[0m : 2.38778
[1mStep[0m  [24/42], [94mLoss[0m : 2.63973
[1mStep[0m  [28/42], [94mLoss[0m : 2.40789
[1mStep[0m  [32/42], [94mLoss[0m : 2.44454
[1mStep[0m  [36/42], [94mLoss[0m : 2.53322
[1mStep[0m  [40/42], [94mLoss[0m : 2.46615

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63912
[1mStep[0m  [4/42], [94mLoss[0m : 2.79512
[1mStep[0m  [8/42], [94mLoss[0m : 2.46498
[1mStep[0m  [12/42], [94mLoss[0m : 2.50230
[1mStep[0m  [16/42], [94mLoss[0m : 2.48341
[1mStep[0m  [20/42], [94mLoss[0m : 2.58556
[1mStep[0m  [24/42], [94mLoss[0m : 2.30394
[1mStep[0m  [28/42], [94mLoss[0m : 2.59114
[1mStep[0m  [32/42], [94mLoss[0m : 2.38312
[1mStep[0m  [36/42], [94mLoss[0m : 2.52066
[1mStep[0m  [40/42], [94mLoss[0m : 2.44594

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46788
[1mStep[0m  [4/42], [94mLoss[0m : 2.28247
[1mStep[0m  [8/42], [94mLoss[0m : 2.46403
[1mStep[0m  [12/42], [94mLoss[0m : 2.42757
[1mStep[0m  [16/42], [94mLoss[0m : 2.60629
[1mStep[0m  [20/42], [94mLoss[0m : 2.48249
[1mStep[0m  [24/42], [94mLoss[0m : 2.31712
[1mStep[0m  [28/42], [94mLoss[0m : 2.64908
[1mStep[0m  [32/42], [94mLoss[0m : 2.47777
[1mStep[0m  [36/42], [94mLoss[0m : 2.53780
[1mStep[0m  [40/42], [94mLoss[0m : 2.42260

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56118
[1mStep[0m  [4/42], [94mLoss[0m : 2.60494
[1mStep[0m  [8/42], [94mLoss[0m : 2.64996
[1mStep[0m  [12/42], [94mLoss[0m : 2.51986
[1mStep[0m  [16/42], [94mLoss[0m : 2.57515
[1mStep[0m  [20/42], [94mLoss[0m : 2.42702
[1mStep[0m  [24/42], [94mLoss[0m : 2.38118
[1mStep[0m  [28/42], [94mLoss[0m : 2.61041
[1mStep[0m  [32/42], [94mLoss[0m : 2.27651
[1mStep[0m  [36/42], [94mLoss[0m : 2.50109
[1mStep[0m  [40/42], [94mLoss[0m : 2.35656

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65299
[1mStep[0m  [4/42], [94mLoss[0m : 2.58194
[1mStep[0m  [8/42], [94mLoss[0m : 2.56835
[1mStep[0m  [12/42], [94mLoss[0m : 2.55835
[1mStep[0m  [16/42], [94mLoss[0m : 2.21118
[1mStep[0m  [20/42], [94mLoss[0m : 2.44956
[1mStep[0m  [24/42], [94mLoss[0m : 2.61230
[1mStep[0m  [28/42], [94mLoss[0m : 2.54112
[1mStep[0m  [32/42], [94mLoss[0m : 2.37463
[1mStep[0m  [36/42], [94mLoss[0m : 2.41466
[1mStep[0m  [40/42], [94mLoss[0m : 2.60610

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45001
[1mStep[0m  [4/42], [94mLoss[0m : 2.39596
[1mStep[0m  [8/42], [94mLoss[0m : 2.55243
[1mStep[0m  [12/42], [94mLoss[0m : 2.44437
[1mStep[0m  [16/42], [94mLoss[0m : 2.32488
[1mStep[0m  [20/42], [94mLoss[0m : 2.48032
[1mStep[0m  [24/42], [94mLoss[0m : 2.33564
[1mStep[0m  [28/42], [94mLoss[0m : 2.41667
[1mStep[0m  [32/42], [94mLoss[0m : 2.53950
[1mStep[0m  [36/42], [94mLoss[0m : 2.67166
[1mStep[0m  [40/42], [94mLoss[0m : 2.46079

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48620
[1mStep[0m  [4/42], [94mLoss[0m : 2.63669
[1mStep[0m  [8/42], [94mLoss[0m : 2.40241
[1mStep[0m  [12/42], [94mLoss[0m : 2.48122
[1mStep[0m  [16/42], [94mLoss[0m : 2.49367
[1mStep[0m  [20/42], [94mLoss[0m : 2.35896
[1mStep[0m  [24/42], [94mLoss[0m : 2.41541
[1mStep[0m  [28/42], [94mLoss[0m : 2.58160
[1mStep[0m  [32/42], [94mLoss[0m : 2.72471
[1mStep[0m  [36/42], [94mLoss[0m : 2.48197
[1mStep[0m  [40/42], [94mLoss[0m : 2.69833

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49648
[1mStep[0m  [4/42], [94mLoss[0m : 2.58660
[1mStep[0m  [8/42], [94mLoss[0m : 2.45166
[1mStep[0m  [12/42], [94mLoss[0m : 2.44695
[1mStep[0m  [16/42], [94mLoss[0m : 2.42496
[1mStep[0m  [20/42], [94mLoss[0m : 2.35187
[1mStep[0m  [24/42], [94mLoss[0m : 2.45602
[1mStep[0m  [28/42], [94mLoss[0m : 2.52307
[1mStep[0m  [32/42], [94mLoss[0m : 2.60175
[1mStep[0m  [36/42], [94mLoss[0m : 2.19816
[1mStep[0m  [40/42], [94mLoss[0m : 2.63758

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30911
[1mStep[0m  [4/42], [94mLoss[0m : 2.54310
[1mStep[0m  [8/42], [94mLoss[0m : 2.41277
[1mStep[0m  [12/42], [94mLoss[0m : 2.49702
[1mStep[0m  [16/42], [94mLoss[0m : 2.41089
[1mStep[0m  [20/42], [94mLoss[0m : 2.76019
[1mStep[0m  [24/42], [94mLoss[0m : 2.40982
[1mStep[0m  [28/42], [94mLoss[0m : 2.42025
[1mStep[0m  [32/42], [94mLoss[0m : 2.49521
[1mStep[0m  [36/42], [94mLoss[0m : 2.50032
[1mStep[0m  [40/42], [94mLoss[0m : 2.58875

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49735
[1mStep[0m  [4/42], [94mLoss[0m : 2.76468
[1mStep[0m  [8/42], [94mLoss[0m : 2.31514
[1mStep[0m  [12/42], [94mLoss[0m : 2.35426
[1mStep[0m  [16/42], [94mLoss[0m : 2.56282
[1mStep[0m  [20/42], [94mLoss[0m : 2.43360
[1mStep[0m  [24/42], [94mLoss[0m : 2.68534
[1mStep[0m  [28/42], [94mLoss[0m : 2.60696
[1mStep[0m  [32/42], [94mLoss[0m : 2.55157
[1mStep[0m  [36/42], [94mLoss[0m : 2.67746
[1mStep[0m  [40/42], [94mLoss[0m : 2.53193

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35343
[1mStep[0m  [4/42], [94mLoss[0m : 2.53671
[1mStep[0m  [8/42], [94mLoss[0m : 2.37555
[1mStep[0m  [12/42], [94mLoss[0m : 2.53081
[1mStep[0m  [16/42], [94mLoss[0m : 2.53438
[1mStep[0m  [20/42], [94mLoss[0m : 2.54068
[1mStep[0m  [24/42], [94mLoss[0m : 2.40748
[1mStep[0m  [28/42], [94mLoss[0m : 2.69258
[1mStep[0m  [32/42], [94mLoss[0m : 2.28721
[1mStep[0m  [36/42], [94mLoss[0m : 2.40256
[1mStep[0m  [40/42], [94mLoss[0m : 2.55213

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.329
====================================

Phase 1 - Evaluation MAE:  2.3294505902699063
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.43273
[1mStep[0m  [4/42], [94mLoss[0m : 2.53706
[1mStep[0m  [8/42], [94mLoss[0m : 2.55348
[1mStep[0m  [12/42], [94mLoss[0m : 2.35579
[1mStep[0m  [16/42], [94mLoss[0m : 2.46081
[1mStep[0m  [20/42], [94mLoss[0m : 2.31868
[1mStep[0m  [24/42], [94mLoss[0m : 2.50471
[1mStep[0m  [28/42], [94mLoss[0m : 2.42593
[1mStep[0m  [32/42], [94mLoss[0m : 2.47439
[1mStep[0m  [36/42], [94mLoss[0m : 2.57860
[1mStep[0m  [40/42], [94mLoss[0m : 2.53417

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27484
[1mStep[0m  [4/42], [94mLoss[0m : 2.47151
[1mStep[0m  [8/42], [94mLoss[0m : 2.57499
[1mStep[0m  [12/42], [94mLoss[0m : 2.50644
[1mStep[0m  [16/42], [94mLoss[0m : 2.51477
[1mStep[0m  [20/42], [94mLoss[0m : 2.67890
[1mStep[0m  [24/42], [94mLoss[0m : 2.31948
[1mStep[0m  [28/42], [94mLoss[0m : 2.17588
[1mStep[0m  [32/42], [94mLoss[0m : 2.46759
[1mStep[0m  [36/42], [94mLoss[0m : 2.43464
[1mStep[0m  [40/42], [94mLoss[0m : 2.55627

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44849
[1mStep[0m  [4/42], [94mLoss[0m : 2.51382
[1mStep[0m  [8/42], [94mLoss[0m : 2.36851
[1mStep[0m  [12/42], [94mLoss[0m : 2.44090
[1mStep[0m  [16/42], [94mLoss[0m : 2.39991
[1mStep[0m  [20/42], [94mLoss[0m : 2.43072
[1mStep[0m  [24/42], [94mLoss[0m : 2.43824
[1mStep[0m  [28/42], [94mLoss[0m : 2.57676
[1mStep[0m  [32/42], [94mLoss[0m : 2.31681
[1mStep[0m  [36/42], [94mLoss[0m : 2.52618
[1mStep[0m  [40/42], [94mLoss[0m : 2.58098

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29631
[1mStep[0m  [4/42], [94mLoss[0m : 2.22734
[1mStep[0m  [8/42], [94mLoss[0m : 2.35656
[1mStep[0m  [12/42], [94mLoss[0m : 2.32193
[1mStep[0m  [16/42], [94mLoss[0m : 2.61178
[1mStep[0m  [20/42], [94mLoss[0m : 2.37333
[1mStep[0m  [24/42], [94mLoss[0m : 2.38402
[1mStep[0m  [28/42], [94mLoss[0m : 2.34543
[1mStep[0m  [32/42], [94mLoss[0m : 2.57722
[1mStep[0m  [36/42], [94mLoss[0m : 2.55553
[1mStep[0m  [40/42], [94mLoss[0m : 2.48388

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63524
[1mStep[0m  [4/42], [94mLoss[0m : 2.47817
[1mStep[0m  [8/42], [94mLoss[0m : 2.12643
[1mStep[0m  [12/42], [94mLoss[0m : 2.40200
[1mStep[0m  [16/42], [94mLoss[0m : 2.39731
[1mStep[0m  [20/42], [94mLoss[0m : 2.41730
[1mStep[0m  [24/42], [94mLoss[0m : 2.26942
[1mStep[0m  [28/42], [94mLoss[0m : 2.49251
[1mStep[0m  [32/42], [94mLoss[0m : 2.51072
[1mStep[0m  [36/42], [94mLoss[0m : 2.18570
[1mStep[0m  [40/42], [94mLoss[0m : 2.45013

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56586
[1mStep[0m  [4/42], [94mLoss[0m : 2.55555
[1mStep[0m  [8/42], [94mLoss[0m : 2.41950
[1mStep[0m  [12/42], [94mLoss[0m : 2.44489
[1mStep[0m  [16/42], [94mLoss[0m : 2.25102
[1mStep[0m  [20/42], [94mLoss[0m : 2.46365
[1mStep[0m  [24/42], [94mLoss[0m : 2.22880
[1mStep[0m  [28/42], [94mLoss[0m : 2.32639
[1mStep[0m  [32/42], [94mLoss[0m : 2.56838
[1mStep[0m  [36/42], [94mLoss[0m : 2.37905
[1mStep[0m  [40/42], [94mLoss[0m : 2.30014

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53557
[1mStep[0m  [4/42], [94mLoss[0m : 2.37931
[1mStep[0m  [8/42], [94mLoss[0m : 2.34202
[1mStep[0m  [12/42], [94mLoss[0m : 2.39154
[1mStep[0m  [16/42], [94mLoss[0m : 2.36336
[1mStep[0m  [20/42], [94mLoss[0m : 2.30955
[1mStep[0m  [24/42], [94mLoss[0m : 2.78574
[1mStep[0m  [28/42], [94mLoss[0m : 2.29360
[1mStep[0m  [32/42], [94mLoss[0m : 2.30938
[1mStep[0m  [36/42], [94mLoss[0m : 2.26303
[1mStep[0m  [40/42], [94mLoss[0m : 2.31102

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27633
[1mStep[0m  [4/42], [94mLoss[0m : 2.39743
[1mStep[0m  [8/42], [94mLoss[0m : 2.61654
[1mStep[0m  [12/42], [94mLoss[0m : 2.25103
[1mStep[0m  [16/42], [94mLoss[0m : 2.61641
[1mStep[0m  [20/42], [94mLoss[0m : 2.42411
[1mStep[0m  [24/42], [94mLoss[0m : 2.35234
[1mStep[0m  [28/42], [94mLoss[0m : 2.22080
[1mStep[0m  [32/42], [94mLoss[0m : 2.37220
[1mStep[0m  [36/42], [94mLoss[0m : 2.27463
[1mStep[0m  [40/42], [94mLoss[0m : 2.39123

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37218
[1mStep[0m  [4/42], [94mLoss[0m : 2.42230
[1mStep[0m  [8/42], [94mLoss[0m : 2.29780
[1mStep[0m  [12/42], [94mLoss[0m : 2.42566
[1mStep[0m  [16/42], [94mLoss[0m : 2.39829
[1mStep[0m  [20/42], [94mLoss[0m : 2.30110
[1mStep[0m  [24/42], [94mLoss[0m : 2.21549
[1mStep[0m  [28/42], [94mLoss[0m : 2.45166
[1mStep[0m  [32/42], [94mLoss[0m : 2.24401
[1mStep[0m  [36/42], [94mLoss[0m : 2.43294
[1mStep[0m  [40/42], [94mLoss[0m : 2.42303

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39829
[1mStep[0m  [4/42], [94mLoss[0m : 2.42263
[1mStep[0m  [8/42], [94mLoss[0m : 2.27403
[1mStep[0m  [12/42], [94mLoss[0m : 2.48461
[1mStep[0m  [16/42], [94mLoss[0m : 2.74602
[1mStep[0m  [20/42], [94mLoss[0m : 2.30402
[1mStep[0m  [24/42], [94mLoss[0m : 2.44599
[1mStep[0m  [28/42], [94mLoss[0m : 2.24758
[1mStep[0m  [32/42], [94mLoss[0m : 2.57567
[1mStep[0m  [36/42], [94mLoss[0m : 2.41392
[1mStep[0m  [40/42], [94mLoss[0m : 2.31275

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19033
[1mStep[0m  [4/42], [94mLoss[0m : 2.27407
[1mStep[0m  [8/42], [94mLoss[0m : 2.38670
[1mStep[0m  [12/42], [94mLoss[0m : 2.10188
[1mStep[0m  [16/42], [94mLoss[0m : 2.37896
[1mStep[0m  [20/42], [94mLoss[0m : 2.46924
[1mStep[0m  [24/42], [94mLoss[0m : 2.38280
[1mStep[0m  [28/42], [94mLoss[0m : 2.24993
[1mStep[0m  [32/42], [94mLoss[0m : 2.60298
[1mStep[0m  [36/42], [94mLoss[0m : 2.42643
[1mStep[0m  [40/42], [94mLoss[0m : 2.44168

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25193
[1mStep[0m  [4/42], [94mLoss[0m : 2.32930
[1mStep[0m  [8/42], [94mLoss[0m : 2.30709
[1mStep[0m  [12/42], [94mLoss[0m : 2.31493
[1mStep[0m  [16/42], [94mLoss[0m : 2.26742
[1mStep[0m  [20/42], [94mLoss[0m : 2.06605
[1mStep[0m  [24/42], [94mLoss[0m : 2.46166
[1mStep[0m  [28/42], [94mLoss[0m : 2.49155
[1mStep[0m  [32/42], [94mLoss[0m : 2.47721
[1mStep[0m  [36/42], [94mLoss[0m : 2.34573
[1mStep[0m  [40/42], [94mLoss[0m : 2.67213

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21577
[1mStep[0m  [4/42], [94mLoss[0m : 2.31861
[1mStep[0m  [8/42], [94mLoss[0m : 2.61785
[1mStep[0m  [12/42], [94mLoss[0m : 2.14142
[1mStep[0m  [16/42], [94mLoss[0m : 2.24644
[1mStep[0m  [20/42], [94mLoss[0m : 2.39160
[1mStep[0m  [24/42], [94mLoss[0m : 2.16520
[1mStep[0m  [28/42], [94mLoss[0m : 2.33912
[1mStep[0m  [32/42], [94mLoss[0m : 2.27277
[1mStep[0m  [36/42], [94mLoss[0m : 2.37558
[1mStep[0m  [40/42], [94mLoss[0m : 2.15698

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30414
[1mStep[0m  [4/42], [94mLoss[0m : 2.34201
[1mStep[0m  [8/42], [94mLoss[0m : 2.42618
[1mStep[0m  [12/42], [94mLoss[0m : 2.45165
[1mStep[0m  [16/42], [94mLoss[0m : 2.38419
[1mStep[0m  [20/42], [94mLoss[0m : 2.38178
[1mStep[0m  [24/42], [94mLoss[0m : 2.22228
[1mStep[0m  [28/42], [94mLoss[0m : 2.29628
[1mStep[0m  [32/42], [94mLoss[0m : 2.28827
[1mStep[0m  [36/42], [94mLoss[0m : 2.15532
[1mStep[0m  [40/42], [94mLoss[0m : 2.29108

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21113
[1mStep[0m  [4/42], [94mLoss[0m : 2.42635
[1mStep[0m  [8/42], [94mLoss[0m : 2.22965
[1mStep[0m  [12/42], [94mLoss[0m : 2.29550
[1mStep[0m  [16/42], [94mLoss[0m : 2.27858
[1mStep[0m  [20/42], [94mLoss[0m : 2.46104
[1mStep[0m  [24/42], [94mLoss[0m : 2.26433
[1mStep[0m  [28/42], [94mLoss[0m : 2.13133
[1mStep[0m  [32/42], [94mLoss[0m : 2.25668
[1mStep[0m  [36/42], [94mLoss[0m : 2.26640
[1mStep[0m  [40/42], [94mLoss[0m : 2.23801

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24142
[1mStep[0m  [4/42], [94mLoss[0m : 2.39765
[1mStep[0m  [8/42], [94mLoss[0m : 2.28136
[1mStep[0m  [12/42], [94mLoss[0m : 2.37692
[1mStep[0m  [16/42], [94mLoss[0m : 2.20300
[1mStep[0m  [20/42], [94mLoss[0m : 2.21761
[1mStep[0m  [24/42], [94mLoss[0m : 2.31306
[1mStep[0m  [28/42], [94mLoss[0m : 2.23359
[1mStep[0m  [32/42], [94mLoss[0m : 2.28083
[1mStep[0m  [36/42], [94mLoss[0m : 2.18211
[1mStep[0m  [40/42], [94mLoss[0m : 2.22155

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05271
[1mStep[0m  [4/42], [94mLoss[0m : 2.17144
[1mStep[0m  [8/42], [94mLoss[0m : 2.33999
[1mStep[0m  [12/42], [94mLoss[0m : 2.08509
[1mStep[0m  [16/42], [94mLoss[0m : 2.36587
[1mStep[0m  [20/42], [94mLoss[0m : 2.01206
[1mStep[0m  [24/42], [94mLoss[0m : 2.14007
[1mStep[0m  [28/42], [94mLoss[0m : 2.17179
[1mStep[0m  [32/42], [94mLoss[0m : 2.16795
[1mStep[0m  [36/42], [94mLoss[0m : 2.23908
[1mStep[0m  [40/42], [94mLoss[0m : 2.31385

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25178
[1mStep[0m  [4/42], [94mLoss[0m : 2.37344
[1mStep[0m  [8/42], [94mLoss[0m : 2.35637
[1mStep[0m  [12/42], [94mLoss[0m : 2.44056
[1mStep[0m  [16/42], [94mLoss[0m : 2.27050
[1mStep[0m  [20/42], [94mLoss[0m : 2.30692
[1mStep[0m  [24/42], [94mLoss[0m : 2.23591
[1mStep[0m  [28/42], [94mLoss[0m : 2.15963
[1mStep[0m  [32/42], [94mLoss[0m : 2.16532
[1mStep[0m  [36/42], [94mLoss[0m : 2.19880
[1mStep[0m  [40/42], [94mLoss[0m : 2.43929

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05616
[1mStep[0m  [4/42], [94mLoss[0m : 2.20183
[1mStep[0m  [8/42], [94mLoss[0m : 2.15624
[1mStep[0m  [12/42], [94mLoss[0m : 2.20259
[1mStep[0m  [16/42], [94mLoss[0m : 2.12330
[1mStep[0m  [20/42], [94mLoss[0m : 2.15649
[1mStep[0m  [24/42], [94mLoss[0m : 2.18104
[1mStep[0m  [28/42], [94mLoss[0m : 2.19999
[1mStep[0m  [32/42], [94mLoss[0m : 2.27934
[1mStep[0m  [36/42], [94mLoss[0m : 2.11684
[1mStep[0m  [40/42], [94mLoss[0m : 2.16748

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21263
[1mStep[0m  [4/42], [94mLoss[0m : 2.17932
[1mStep[0m  [8/42], [94mLoss[0m : 2.22661
[1mStep[0m  [12/42], [94mLoss[0m : 1.89065
[1mStep[0m  [16/42], [94mLoss[0m : 2.27391
[1mStep[0m  [20/42], [94mLoss[0m : 1.99787
[1mStep[0m  [24/42], [94mLoss[0m : 2.02182
[1mStep[0m  [28/42], [94mLoss[0m : 2.31402
[1mStep[0m  [32/42], [94mLoss[0m : 2.00851
[1mStep[0m  [36/42], [94mLoss[0m : 2.02747
[1mStep[0m  [40/42], [94mLoss[0m : 2.24337

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13855
[1mStep[0m  [4/42], [94mLoss[0m : 1.93412
[1mStep[0m  [8/42], [94mLoss[0m : 2.10941
[1mStep[0m  [12/42], [94mLoss[0m : 1.89001
[1mStep[0m  [16/42], [94mLoss[0m : 2.06607
[1mStep[0m  [20/42], [94mLoss[0m : 2.12677
[1mStep[0m  [24/42], [94mLoss[0m : 2.07630
[1mStep[0m  [28/42], [94mLoss[0m : 2.05489
[1mStep[0m  [32/42], [94mLoss[0m : 2.17925
[1mStep[0m  [36/42], [94mLoss[0m : 2.20780
[1mStep[0m  [40/42], [94mLoss[0m : 2.25640

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09199
[1mStep[0m  [4/42], [94mLoss[0m : 2.18492
[1mStep[0m  [8/42], [94mLoss[0m : 2.18804
[1mStep[0m  [12/42], [94mLoss[0m : 2.31545
[1mStep[0m  [16/42], [94mLoss[0m : 2.02447
[1mStep[0m  [20/42], [94mLoss[0m : 2.14874
[1mStep[0m  [24/42], [94mLoss[0m : 2.28403
[1mStep[0m  [28/42], [94mLoss[0m : 2.08829
[1mStep[0m  [32/42], [94mLoss[0m : 2.09219
[1mStep[0m  [36/42], [94mLoss[0m : 1.97918
[1mStep[0m  [40/42], [94mLoss[0m : 2.29163

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.445, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21551
[1mStep[0m  [4/42], [94mLoss[0m : 2.04220
[1mStep[0m  [8/42], [94mLoss[0m : 1.96860
[1mStep[0m  [12/42], [94mLoss[0m : 1.96916
[1mStep[0m  [16/42], [94mLoss[0m : 2.18469
[1mStep[0m  [20/42], [94mLoss[0m : 2.20310
[1mStep[0m  [24/42], [94mLoss[0m : 2.03675
[1mStep[0m  [28/42], [94mLoss[0m : 2.03074
[1mStep[0m  [32/42], [94mLoss[0m : 2.13061
[1mStep[0m  [36/42], [94mLoss[0m : 1.93098
[1mStep[0m  [40/42], [94mLoss[0m : 2.03339

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.058, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99465
[1mStep[0m  [4/42], [94mLoss[0m : 2.04611
[1mStep[0m  [8/42], [94mLoss[0m : 2.10047
[1mStep[0m  [12/42], [94mLoss[0m : 2.08072
[1mStep[0m  [16/42], [94mLoss[0m : 1.89542
[1mStep[0m  [20/42], [94mLoss[0m : 2.00790
[1mStep[0m  [24/42], [94mLoss[0m : 2.11786
[1mStep[0m  [28/42], [94mLoss[0m : 1.93222
[1mStep[0m  [32/42], [94mLoss[0m : 2.02906
[1mStep[0m  [36/42], [94mLoss[0m : 2.17987
[1mStep[0m  [40/42], [94mLoss[0m : 2.09512

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.493, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97662
[1mStep[0m  [4/42], [94mLoss[0m : 1.78002
[1mStep[0m  [8/42], [94mLoss[0m : 1.85207
[1mStep[0m  [12/42], [94mLoss[0m : 1.94093
[1mStep[0m  [16/42], [94mLoss[0m : 2.12060
[1mStep[0m  [20/42], [94mLoss[0m : 2.17294
[1mStep[0m  [24/42], [94mLoss[0m : 1.95894
[1mStep[0m  [28/42], [94mLoss[0m : 2.24991
[1mStep[0m  [32/42], [94mLoss[0m : 1.99656
[1mStep[0m  [36/42], [94mLoss[0m : 1.85550
[1mStep[0m  [40/42], [94mLoss[0m : 2.35453

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.82967
[1mStep[0m  [4/42], [94mLoss[0m : 1.91275
[1mStep[0m  [8/42], [94mLoss[0m : 1.97331
[1mStep[0m  [12/42], [94mLoss[0m : 1.92426
[1mStep[0m  [16/42], [94mLoss[0m : 1.88555
[1mStep[0m  [20/42], [94mLoss[0m : 1.93777
[1mStep[0m  [24/42], [94mLoss[0m : 2.27571
[1mStep[0m  [28/42], [94mLoss[0m : 1.89947
[1mStep[0m  [32/42], [94mLoss[0m : 1.85072
[1mStep[0m  [36/42], [94mLoss[0m : 1.92812
[1mStep[0m  [40/42], [94mLoss[0m : 1.81683

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12926
[1mStep[0m  [4/42], [94mLoss[0m : 2.04051
[1mStep[0m  [8/42], [94mLoss[0m : 1.90410
[1mStep[0m  [12/42], [94mLoss[0m : 1.97626
[1mStep[0m  [16/42], [94mLoss[0m : 1.88243
[1mStep[0m  [20/42], [94mLoss[0m : 1.98890
[1mStep[0m  [24/42], [94mLoss[0m : 1.97269
[1mStep[0m  [28/42], [94mLoss[0m : 1.87270
[1mStep[0m  [32/42], [94mLoss[0m : 2.13488
[1mStep[0m  [36/42], [94mLoss[0m : 1.92279
[1mStep[0m  [40/42], [94mLoss[0m : 1.88608

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87648
[1mStep[0m  [4/42], [94mLoss[0m : 2.01338
[1mStep[0m  [8/42], [94mLoss[0m : 2.09750
[1mStep[0m  [12/42], [94mLoss[0m : 1.93255
[1mStep[0m  [16/42], [94mLoss[0m : 2.22457
[1mStep[0m  [20/42], [94mLoss[0m : 1.95218
[1mStep[0m  [24/42], [94mLoss[0m : 1.93864
[1mStep[0m  [28/42], [94mLoss[0m : 1.87853
[1mStep[0m  [32/42], [94mLoss[0m : 1.97241
[1mStep[0m  [36/42], [94mLoss[0m : 1.95303
[1mStep[0m  [40/42], [94mLoss[0m : 1.86186

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92887
[1mStep[0m  [4/42], [94mLoss[0m : 2.04941
[1mStep[0m  [8/42], [94mLoss[0m : 1.89065
[1mStep[0m  [12/42], [94mLoss[0m : 2.04468
[1mStep[0m  [16/42], [94mLoss[0m : 1.72193
[1mStep[0m  [20/42], [94mLoss[0m : 1.82631
[1mStep[0m  [24/42], [94mLoss[0m : 1.97623
[1mStep[0m  [28/42], [94mLoss[0m : 1.81466
[1mStep[0m  [32/42], [94mLoss[0m : 1.77697
[1mStep[0m  [36/42], [94mLoss[0m : 1.96762
[1mStep[0m  [40/42], [94mLoss[0m : 2.06086

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06965
[1mStep[0m  [4/42], [94mLoss[0m : 1.82004
[1mStep[0m  [8/42], [94mLoss[0m : 1.72436
[1mStep[0m  [12/42], [94mLoss[0m : 1.72056
[1mStep[0m  [16/42], [94mLoss[0m : 1.86278
[1mStep[0m  [20/42], [94mLoss[0m : 2.00892
[1mStep[0m  [24/42], [94mLoss[0m : 1.84572
[1mStep[0m  [28/42], [94mLoss[0m : 2.12554
[1mStep[0m  [32/42], [94mLoss[0m : 2.00438
[1mStep[0m  [36/42], [94mLoss[0m : 1.79081
[1mStep[0m  [40/42], [94mLoss[0m : 1.90365

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.429
====================================

Phase 2 - Evaluation MAE:  2.429424524307251
MAE score P1      2.329451
MAE score P2      2.429425
loss              1.898207
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.02812
[1mStep[0m  [8/84], [94mLoss[0m : 8.47603
[1mStep[0m  [16/84], [94mLoss[0m : 7.27308
[1mStep[0m  [24/84], [94mLoss[0m : 4.23205
[1mStep[0m  [32/84], [94mLoss[0m : 3.92401
[1mStep[0m  [40/84], [94mLoss[0m : 3.43165
[1mStep[0m  [48/84], [94mLoss[0m : 2.91077
[1mStep[0m  [56/84], [94mLoss[0m : 3.04392
[1mStep[0m  [64/84], [94mLoss[0m : 2.54850
[1mStep[0m  [72/84], [94mLoss[0m : 2.88148
[1mStep[0m  [80/84], [94mLoss[0m : 2.21502

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.396, [92mTest[0m: 10.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59394
[1mStep[0m  [8/84], [94mLoss[0m : 2.77259
[1mStep[0m  [16/84], [94mLoss[0m : 2.41671
[1mStep[0m  [24/84], [94mLoss[0m : 2.66431
[1mStep[0m  [32/84], [94mLoss[0m : 2.80840
[1mStep[0m  [40/84], [94mLoss[0m : 2.50922
[1mStep[0m  [48/84], [94mLoss[0m : 2.75485
[1mStep[0m  [56/84], [94mLoss[0m : 2.45917
[1mStep[0m  [64/84], [94mLoss[0m : 2.53912
[1mStep[0m  [72/84], [94mLoss[0m : 2.73545
[1mStep[0m  [80/84], [94mLoss[0m : 2.53590

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.523, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98656
[1mStep[0m  [8/84], [94mLoss[0m : 2.43170
[1mStep[0m  [16/84], [94mLoss[0m : 2.69599
[1mStep[0m  [24/84], [94mLoss[0m : 2.56251
[1mStep[0m  [32/84], [94mLoss[0m : 2.69408
[1mStep[0m  [40/84], [94mLoss[0m : 2.70880
[1mStep[0m  [48/84], [94mLoss[0m : 2.46767
[1mStep[0m  [56/84], [94mLoss[0m : 2.63923
[1mStep[0m  [64/84], [94mLoss[0m : 2.70204
[1mStep[0m  [72/84], [94mLoss[0m : 2.71054
[1mStep[0m  [80/84], [94mLoss[0m : 2.94544

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84068
[1mStep[0m  [8/84], [94mLoss[0m : 2.68928
[1mStep[0m  [16/84], [94mLoss[0m : 2.78917
[1mStep[0m  [24/84], [94mLoss[0m : 2.59345
[1mStep[0m  [32/84], [94mLoss[0m : 2.36753
[1mStep[0m  [40/84], [94mLoss[0m : 2.61197
[1mStep[0m  [48/84], [94mLoss[0m : 2.76384
[1mStep[0m  [56/84], [94mLoss[0m : 2.45426
[1mStep[0m  [64/84], [94mLoss[0m : 2.72290
[1mStep[0m  [72/84], [94mLoss[0m : 2.40346
[1mStep[0m  [80/84], [94mLoss[0m : 2.38158

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76080
[1mStep[0m  [8/84], [94mLoss[0m : 2.72593
[1mStep[0m  [16/84], [94mLoss[0m : 2.63261
[1mStep[0m  [24/84], [94mLoss[0m : 2.78364
[1mStep[0m  [32/84], [94mLoss[0m : 2.38808
[1mStep[0m  [40/84], [94mLoss[0m : 2.71119
[1mStep[0m  [48/84], [94mLoss[0m : 2.59201
[1mStep[0m  [56/84], [94mLoss[0m : 2.77614
[1mStep[0m  [64/84], [94mLoss[0m : 2.60223
[1mStep[0m  [72/84], [94mLoss[0m : 2.52669
[1mStep[0m  [80/84], [94mLoss[0m : 2.58435

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39381
[1mStep[0m  [8/84], [94mLoss[0m : 2.66718
[1mStep[0m  [16/84], [94mLoss[0m : 2.47294
[1mStep[0m  [24/84], [94mLoss[0m : 2.55353
[1mStep[0m  [32/84], [94mLoss[0m : 2.42411
[1mStep[0m  [40/84], [94mLoss[0m : 2.62286
[1mStep[0m  [48/84], [94mLoss[0m : 2.35099
[1mStep[0m  [56/84], [94mLoss[0m : 2.60451
[1mStep[0m  [64/84], [94mLoss[0m : 2.70534
[1mStep[0m  [72/84], [94mLoss[0m : 2.82676
[1mStep[0m  [80/84], [94mLoss[0m : 2.56418

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.03623
[1mStep[0m  [8/84], [94mLoss[0m : 2.32673
[1mStep[0m  [16/84], [94mLoss[0m : 2.50157
[1mStep[0m  [24/84], [94mLoss[0m : 2.77313
[1mStep[0m  [32/84], [94mLoss[0m : 2.41950
[1mStep[0m  [40/84], [94mLoss[0m : 2.46041
[1mStep[0m  [48/84], [94mLoss[0m : 2.56471
[1mStep[0m  [56/84], [94mLoss[0m : 2.29567
[1mStep[0m  [64/84], [94mLoss[0m : 2.35965
[1mStep[0m  [72/84], [94mLoss[0m : 2.24025
[1mStep[0m  [80/84], [94mLoss[0m : 2.25471

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54039
[1mStep[0m  [8/84], [94mLoss[0m : 2.78194
[1mStep[0m  [16/84], [94mLoss[0m : 2.47077
[1mStep[0m  [24/84], [94mLoss[0m : 2.83600
[1mStep[0m  [32/84], [94mLoss[0m : 2.57855
[1mStep[0m  [40/84], [94mLoss[0m : 2.74293
[1mStep[0m  [48/84], [94mLoss[0m : 2.59650
[1mStep[0m  [56/84], [94mLoss[0m : 2.50395
[1mStep[0m  [64/84], [94mLoss[0m : 2.43426
[1mStep[0m  [72/84], [94mLoss[0m : 2.60141
[1mStep[0m  [80/84], [94mLoss[0m : 2.50506

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48523
[1mStep[0m  [8/84], [94mLoss[0m : 2.68421
[1mStep[0m  [16/84], [94mLoss[0m : 2.67077
[1mStep[0m  [24/84], [94mLoss[0m : 2.40525
[1mStep[0m  [32/84], [94mLoss[0m : 2.67227
[1mStep[0m  [40/84], [94mLoss[0m : 2.63809
[1mStep[0m  [48/84], [94mLoss[0m : 2.97413
[1mStep[0m  [56/84], [94mLoss[0m : 2.49026
[1mStep[0m  [64/84], [94mLoss[0m : 2.67344
[1mStep[0m  [72/84], [94mLoss[0m : 2.59332
[1mStep[0m  [80/84], [94mLoss[0m : 2.29287

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58070
[1mStep[0m  [8/84], [94mLoss[0m : 3.07395
[1mStep[0m  [16/84], [94mLoss[0m : 2.49013
[1mStep[0m  [24/84], [94mLoss[0m : 2.46116
[1mStep[0m  [32/84], [94mLoss[0m : 2.74132
[1mStep[0m  [40/84], [94mLoss[0m : 2.63153
[1mStep[0m  [48/84], [94mLoss[0m : 2.54333
[1mStep[0m  [56/84], [94mLoss[0m : 2.81547
[1mStep[0m  [64/84], [94mLoss[0m : 2.36254
[1mStep[0m  [72/84], [94mLoss[0m : 2.68132
[1mStep[0m  [80/84], [94mLoss[0m : 2.54332

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68390
[1mStep[0m  [8/84], [94mLoss[0m : 2.38091
[1mStep[0m  [16/84], [94mLoss[0m : 2.40116
[1mStep[0m  [24/84], [94mLoss[0m : 2.29530
[1mStep[0m  [32/84], [94mLoss[0m : 2.50584
[1mStep[0m  [40/84], [94mLoss[0m : 2.38756
[1mStep[0m  [48/84], [94mLoss[0m : 2.53481
[1mStep[0m  [56/84], [94mLoss[0m : 2.51922
[1mStep[0m  [64/84], [94mLoss[0m : 2.34702
[1mStep[0m  [72/84], [94mLoss[0m : 2.56018
[1mStep[0m  [80/84], [94mLoss[0m : 2.72329

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47295
[1mStep[0m  [8/84], [94mLoss[0m : 2.41467
[1mStep[0m  [16/84], [94mLoss[0m : 2.25961
[1mStep[0m  [24/84], [94mLoss[0m : 2.62690
[1mStep[0m  [32/84], [94mLoss[0m : 2.21767
[1mStep[0m  [40/84], [94mLoss[0m : 2.55231
[1mStep[0m  [48/84], [94mLoss[0m : 2.43406
[1mStep[0m  [56/84], [94mLoss[0m : 2.58983
[1mStep[0m  [64/84], [94mLoss[0m : 2.71358
[1mStep[0m  [72/84], [94mLoss[0m : 2.51747
[1mStep[0m  [80/84], [94mLoss[0m : 2.48093

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52135
[1mStep[0m  [8/84], [94mLoss[0m : 2.51365
[1mStep[0m  [16/84], [94mLoss[0m : 2.59825
[1mStep[0m  [24/84], [94mLoss[0m : 2.51093
[1mStep[0m  [32/84], [94mLoss[0m : 2.72204
[1mStep[0m  [40/84], [94mLoss[0m : 2.39015
[1mStep[0m  [48/84], [94mLoss[0m : 2.32312
[1mStep[0m  [56/84], [94mLoss[0m : 2.69212
[1mStep[0m  [64/84], [94mLoss[0m : 2.38024
[1mStep[0m  [72/84], [94mLoss[0m : 2.73816
[1mStep[0m  [80/84], [94mLoss[0m : 2.31254

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38168
[1mStep[0m  [8/84], [94mLoss[0m : 2.38232
[1mStep[0m  [16/84], [94mLoss[0m : 2.64122
[1mStep[0m  [24/84], [94mLoss[0m : 2.34595
[1mStep[0m  [32/84], [94mLoss[0m : 2.83545
[1mStep[0m  [40/84], [94mLoss[0m : 2.63653
[1mStep[0m  [48/84], [94mLoss[0m : 2.62969
[1mStep[0m  [56/84], [94mLoss[0m : 2.77935
[1mStep[0m  [64/84], [94mLoss[0m : 2.85119
[1mStep[0m  [72/84], [94mLoss[0m : 2.48997
[1mStep[0m  [80/84], [94mLoss[0m : 2.44034

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54837
[1mStep[0m  [8/84], [94mLoss[0m : 2.40925
[1mStep[0m  [16/84], [94mLoss[0m : 2.73888
[1mStep[0m  [24/84], [94mLoss[0m : 2.50082
[1mStep[0m  [32/84], [94mLoss[0m : 2.59196
[1mStep[0m  [40/84], [94mLoss[0m : 2.50443
[1mStep[0m  [48/84], [94mLoss[0m : 2.43170
[1mStep[0m  [56/84], [94mLoss[0m : 2.63575
[1mStep[0m  [64/84], [94mLoss[0m : 2.31764
[1mStep[0m  [72/84], [94mLoss[0m : 2.49624
[1mStep[0m  [80/84], [94mLoss[0m : 2.80116

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60558
[1mStep[0m  [8/84], [94mLoss[0m : 2.61250
[1mStep[0m  [16/84], [94mLoss[0m : 2.48058
[1mStep[0m  [24/84], [94mLoss[0m : 2.72687
[1mStep[0m  [32/84], [94mLoss[0m : 2.61840
[1mStep[0m  [40/84], [94mLoss[0m : 2.62637
[1mStep[0m  [48/84], [94mLoss[0m : 2.46420
[1mStep[0m  [56/84], [94mLoss[0m : 2.43665
[1mStep[0m  [64/84], [94mLoss[0m : 2.75785
[1mStep[0m  [72/84], [94mLoss[0m : 2.45778
[1mStep[0m  [80/84], [94mLoss[0m : 2.48561

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76993
[1mStep[0m  [8/84], [94mLoss[0m : 2.40342
[1mStep[0m  [16/84], [94mLoss[0m : 2.57994
[1mStep[0m  [24/84], [94mLoss[0m : 2.36201
[1mStep[0m  [32/84], [94mLoss[0m : 2.54093
[1mStep[0m  [40/84], [94mLoss[0m : 2.62882
[1mStep[0m  [48/84], [94mLoss[0m : 2.35280
[1mStep[0m  [56/84], [94mLoss[0m : 2.43199
[1mStep[0m  [64/84], [94mLoss[0m : 2.62682
[1mStep[0m  [72/84], [94mLoss[0m : 2.63516
[1mStep[0m  [80/84], [94mLoss[0m : 2.32215

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30451
[1mStep[0m  [8/84], [94mLoss[0m : 2.66441
[1mStep[0m  [16/84], [94mLoss[0m : 2.58850
[1mStep[0m  [24/84], [94mLoss[0m : 2.53430
[1mStep[0m  [32/84], [94mLoss[0m : 2.76803
[1mStep[0m  [40/84], [94mLoss[0m : 2.40145
[1mStep[0m  [48/84], [94mLoss[0m : 2.48944
[1mStep[0m  [56/84], [94mLoss[0m : 2.29419
[1mStep[0m  [64/84], [94mLoss[0m : 2.25169
[1mStep[0m  [72/84], [94mLoss[0m : 2.38503
[1mStep[0m  [80/84], [94mLoss[0m : 2.49484

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76295
[1mStep[0m  [8/84], [94mLoss[0m : 2.32045
[1mStep[0m  [16/84], [94mLoss[0m : 2.32009
[1mStep[0m  [24/84], [94mLoss[0m : 2.68207
[1mStep[0m  [32/84], [94mLoss[0m : 2.57059
[1mStep[0m  [40/84], [94mLoss[0m : 2.49307
[1mStep[0m  [48/84], [94mLoss[0m : 2.37924
[1mStep[0m  [56/84], [94mLoss[0m : 2.80847
[1mStep[0m  [64/84], [94mLoss[0m : 2.47063
[1mStep[0m  [72/84], [94mLoss[0m : 2.45494
[1mStep[0m  [80/84], [94mLoss[0m : 2.30527

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36283
[1mStep[0m  [8/84], [94mLoss[0m : 2.53409
[1mStep[0m  [16/84], [94mLoss[0m : 2.55531
[1mStep[0m  [24/84], [94mLoss[0m : 2.53861
[1mStep[0m  [32/84], [94mLoss[0m : 2.10506
[1mStep[0m  [40/84], [94mLoss[0m : 2.75647
[1mStep[0m  [48/84], [94mLoss[0m : 2.42291
[1mStep[0m  [56/84], [94mLoss[0m : 2.24174
[1mStep[0m  [64/84], [94mLoss[0m : 2.61557
[1mStep[0m  [72/84], [94mLoss[0m : 2.61356
[1mStep[0m  [80/84], [94mLoss[0m : 2.64246

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33494
[1mStep[0m  [8/84], [94mLoss[0m : 2.53539
[1mStep[0m  [16/84], [94mLoss[0m : 2.55901
[1mStep[0m  [24/84], [94mLoss[0m : 2.39789
[1mStep[0m  [32/84], [94mLoss[0m : 2.65959
[1mStep[0m  [40/84], [94mLoss[0m : 2.51964
[1mStep[0m  [48/84], [94mLoss[0m : 2.52939
[1mStep[0m  [56/84], [94mLoss[0m : 2.40243
[1mStep[0m  [64/84], [94mLoss[0m : 2.70918
[1mStep[0m  [72/84], [94mLoss[0m : 2.60663
[1mStep[0m  [80/84], [94mLoss[0m : 2.38973

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55428
[1mStep[0m  [8/84], [94mLoss[0m : 2.70007
[1mStep[0m  [16/84], [94mLoss[0m : 2.43596
[1mStep[0m  [24/84], [94mLoss[0m : 2.35441
[1mStep[0m  [32/84], [94mLoss[0m : 2.67082
[1mStep[0m  [40/84], [94mLoss[0m : 2.44430
[1mStep[0m  [48/84], [94mLoss[0m : 2.33404
[1mStep[0m  [56/84], [94mLoss[0m : 2.67458
[1mStep[0m  [64/84], [94mLoss[0m : 2.62882
[1mStep[0m  [72/84], [94mLoss[0m : 2.69503
[1mStep[0m  [80/84], [94mLoss[0m : 2.58877

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62853
[1mStep[0m  [8/84], [94mLoss[0m : 2.12867
[1mStep[0m  [16/84], [94mLoss[0m : 2.40187
[1mStep[0m  [24/84], [94mLoss[0m : 2.26589
[1mStep[0m  [32/84], [94mLoss[0m : 2.47162
[1mStep[0m  [40/84], [94mLoss[0m : 2.76323
[1mStep[0m  [48/84], [94mLoss[0m : 2.45059
[1mStep[0m  [56/84], [94mLoss[0m : 2.36501
[1mStep[0m  [64/84], [94mLoss[0m : 3.02890
[1mStep[0m  [72/84], [94mLoss[0m : 2.73277
[1mStep[0m  [80/84], [94mLoss[0m : 2.84971

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46338
[1mStep[0m  [8/84], [94mLoss[0m : 2.32706
[1mStep[0m  [16/84], [94mLoss[0m : 2.56574
[1mStep[0m  [24/84], [94mLoss[0m : 2.56659
[1mStep[0m  [32/84], [94mLoss[0m : 2.61336
[1mStep[0m  [40/84], [94mLoss[0m : 2.25852
[1mStep[0m  [48/84], [94mLoss[0m : 2.60959
[1mStep[0m  [56/84], [94mLoss[0m : 2.35969
[1mStep[0m  [64/84], [94mLoss[0m : 2.61169
[1mStep[0m  [72/84], [94mLoss[0m : 1.97287
[1mStep[0m  [80/84], [94mLoss[0m : 2.22636

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37876
[1mStep[0m  [8/84], [94mLoss[0m : 2.42091
[1mStep[0m  [16/84], [94mLoss[0m : 2.30536
[1mStep[0m  [24/84], [94mLoss[0m : 2.53381
[1mStep[0m  [32/84], [94mLoss[0m : 2.55706
[1mStep[0m  [40/84], [94mLoss[0m : 2.40977
[1mStep[0m  [48/84], [94mLoss[0m : 2.83352
[1mStep[0m  [56/84], [94mLoss[0m : 2.39135
[1mStep[0m  [64/84], [94mLoss[0m : 2.71883
[1mStep[0m  [72/84], [94mLoss[0m : 2.50092
[1mStep[0m  [80/84], [94mLoss[0m : 2.21177

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64720
[1mStep[0m  [8/84], [94mLoss[0m : 2.40804
[1mStep[0m  [16/84], [94mLoss[0m : 2.29761
[1mStep[0m  [24/84], [94mLoss[0m : 2.45233
[1mStep[0m  [32/84], [94mLoss[0m : 2.15253
[1mStep[0m  [40/84], [94mLoss[0m : 2.63989
[1mStep[0m  [48/84], [94mLoss[0m : 2.48454
[1mStep[0m  [56/84], [94mLoss[0m : 2.67695
[1mStep[0m  [64/84], [94mLoss[0m : 2.58938
[1mStep[0m  [72/84], [94mLoss[0m : 2.44874
[1mStep[0m  [80/84], [94mLoss[0m : 2.45164

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66406
[1mStep[0m  [8/84], [94mLoss[0m : 2.22553
[1mStep[0m  [16/84], [94mLoss[0m : 2.84622
[1mStep[0m  [24/84], [94mLoss[0m : 2.46083
[1mStep[0m  [32/84], [94mLoss[0m : 2.32281
[1mStep[0m  [40/84], [94mLoss[0m : 2.57856
[1mStep[0m  [48/84], [94mLoss[0m : 2.68956
[1mStep[0m  [56/84], [94mLoss[0m : 2.38693
[1mStep[0m  [64/84], [94mLoss[0m : 2.54321
[1mStep[0m  [72/84], [94mLoss[0m : 2.37036
[1mStep[0m  [80/84], [94mLoss[0m : 2.59842

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70875
[1mStep[0m  [8/84], [94mLoss[0m : 2.30662
[1mStep[0m  [16/84], [94mLoss[0m : 2.28015
[1mStep[0m  [24/84], [94mLoss[0m : 2.35162
[1mStep[0m  [32/84], [94mLoss[0m : 2.30684
[1mStep[0m  [40/84], [94mLoss[0m : 2.57761
[1mStep[0m  [48/84], [94mLoss[0m : 2.47955
[1mStep[0m  [56/84], [94mLoss[0m : 2.42983
[1mStep[0m  [64/84], [94mLoss[0m : 2.45707
[1mStep[0m  [72/84], [94mLoss[0m : 2.50784
[1mStep[0m  [80/84], [94mLoss[0m : 2.41519

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80621
[1mStep[0m  [8/84], [94mLoss[0m : 2.50807
[1mStep[0m  [16/84], [94mLoss[0m : 2.33324
[1mStep[0m  [24/84], [94mLoss[0m : 2.36775
[1mStep[0m  [32/84], [94mLoss[0m : 2.76135
[1mStep[0m  [40/84], [94mLoss[0m : 2.51398
[1mStep[0m  [48/84], [94mLoss[0m : 2.18831
[1mStep[0m  [56/84], [94mLoss[0m : 2.63838
[1mStep[0m  [64/84], [94mLoss[0m : 2.50043
[1mStep[0m  [72/84], [94mLoss[0m : 2.77205
[1mStep[0m  [80/84], [94mLoss[0m : 2.71718

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35114
[1mStep[0m  [8/84], [94mLoss[0m : 2.48518
[1mStep[0m  [16/84], [94mLoss[0m : 2.60455
[1mStep[0m  [24/84], [94mLoss[0m : 2.65157
[1mStep[0m  [32/84], [94mLoss[0m : 2.55432
[1mStep[0m  [40/84], [94mLoss[0m : 2.43066
[1mStep[0m  [48/84], [94mLoss[0m : 2.56930
[1mStep[0m  [56/84], [94mLoss[0m : 2.58635
[1mStep[0m  [64/84], [94mLoss[0m : 2.46294
[1mStep[0m  [72/84], [94mLoss[0m : 2.68605
[1mStep[0m  [80/84], [94mLoss[0m : 2.27586

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.325334153005055
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.25697
[1mStep[0m  [8/84], [94mLoss[0m : 2.62894
[1mStep[0m  [16/84], [94mLoss[0m : 2.64709
[1mStep[0m  [24/84], [94mLoss[0m : 2.34132
[1mStep[0m  [32/84], [94mLoss[0m : 2.57145
[1mStep[0m  [40/84], [94mLoss[0m : 2.94827
[1mStep[0m  [48/84], [94mLoss[0m : 2.39938
[1mStep[0m  [56/84], [94mLoss[0m : 2.29489
[1mStep[0m  [64/84], [94mLoss[0m : 2.65242
[1mStep[0m  [72/84], [94mLoss[0m : 2.78056
[1mStep[0m  [80/84], [94mLoss[0m : 2.61351

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18354
[1mStep[0m  [8/84], [94mLoss[0m : 2.16709
[1mStep[0m  [16/84], [94mLoss[0m : 2.12685
[1mStep[0m  [24/84], [94mLoss[0m : 2.19879
[1mStep[0m  [32/84], [94mLoss[0m : 2.69007
[1mStep[0m  [40/84], [94mLoss[0m : 2.52049
[1mStep[0m  [48/84], [94mLoss[0m : 2.24048
[1mStep[0m  [56/84], [94mLoss[0m : 2.23265
[1mStep[0m  [64/84], [94mLoss[0m : 2.58307
[1mStep[0m  [72/84], [94mLoss[0m : 2.36368
[1mStep[0m  [80/84], [94mLoss[0m : 2.69313

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78041
[1mStep[0m  [8/84], [94mLoss[0m : 2.39945
[1mStep[0m  [16/84], [94mLoss[0m : 2.52583
[1mStep[0m  [24/84], [94mLoss[0m : 2.50768
[1mStep[0m  [32/84], [94mLoss[0m : 2.49781
[1mStep[0m  [40/84], [94mLoss[0m : 2.70750
[1mStep[0m  [48/84], [94mLoss[0m : 1.94748
[1mStep[0m  [56/84], [94mLoss[0m : 2.31878
[1mStep[0m  [64/84], [94mLoss[0m : 2.44436
[1mStep[0m  [72/84], [94mLoss[0m : 2.50388
[1mStep[0m  [80/84], [94mLoss[0m : 2.21326

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50106
[1mStep[0m  [8/84], [94mLoss[0m : 2.51981
[1mStep[0m  [16/84], [94mLoss[0m : 2.79042
[1mStep[0m  [24/84], [94mLoss[0m : 2.34179
[1mStep[0m  [32/84], [94mLoss[0m : 2.49463
[1mStep[0m  [40/84], [94mLoss[0m : 2.32737
[1mStep[0m  [48/84], [94mLoss[0m : 2.26634
[1mStep[0m  [56/84], [94mLoss[0m : 2.29536
[1mStep[0m  [64/84], [94mLoss[0m : 2.46463
[1mStep[0m  [72/84], [94mLoss[0m : 2.43753
[1mStep[0m  [80/84], [94mLoss[0m : 2.49125

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19900
[1mStep[0m  [8/84], [94mLoss[0m : 2.33807
[1mStep[0m  [16/84], [94mLoss[0m : 2.26778
[1mStep[0m  [24/84], [94mLoss[0m : 2.40751
[1mStep[0m  [32/84], [94mLoss[0m : 2.58673
[1mStep[0m  [40/84], [94mLoss[0m : 2.41878
[1mStep[0m  [48/84], [94mLoss[0m : 2.43972
[1mStep[0m  [56/84], [94mLoss[0m : 2.47517
[1mStep[0m  [64/84], [94mLoss[0m : 2.47114
[1mStep[0m  [72/84], [94mLoss[0m : 2.74009
[1mStep[0m  [80/84], [94mLoss[0m : 2.29045

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35709
[1mStep[0m  [8/84], [94mLoss[0m : 2.28578
[1mStep[0m  [16/84], [94mLoss[0m : 2.26892
[1mStep[0m  [24/84], [94mLoss[0m : 2.17744
[1mStep[0m  [32/84], [94mLoss[0m : 2.33446
[1mStep[0m  [40/84], [94mLoss[0m : 2.18046
[1mStep[0m  [48/84], [94mLoss[0m : 2.11720
[1mStep[0m  [56/84], [94mLoss[0m : 2.09289
[1mStep[0m  [64/84], [94mLoss[0m : 2.27669
[1mStep[0m  [72/84], [94mLoss[0m : 2.36224
[1mStep[0m  [80/84], [94mLoss[0m : 2.41209

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09636
[1mStep[0m  [8/84], [94mLoss[0m : 2.58035
[1mStep[0m  [16/84], [94mLoss[0m : 2.28192
[1mStep[0m  [24/84], [94mLoss[0m : 2.22774
[1mStep[0m  [32/84], [94mLoss[0m : 2.32001
[1mStep[0m  [40/84], [94mLoss[0m : 2.53906
[1mStep[0m  [48/84], [94mLoss[0m : 2.16963
[1mStep[0m  [56/84], [94mLoss[0m : 2.15658
[1mStep[0m  [64/84], [94mLoss[0m : 2.14907
[1mStep[0m  [72/84], [94mLoss[0m : 2.39537
[1mStep[0m  [80/84], [94mLoss[0m : 2.50815

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21063
[1mStep[0m  [8/84], [94mLoss[0m : 2.05978
[1mStep[0m  [16/84], [94mLoss[0m : 2.35767
[1mStep[0m  [24/84], [94mLoss[0m : 2.22853
[1mStep[0m  [32/84], [94mLoss[0m : 2.44127
[1mStep[0m  [40/84], [94mLoss[0m : 2.25834
[1mStep[0m  [48/84], [94mLoss[0m : 2.20992
[1mStep[0m  [56/84], [94mLoss[0m : 2.49940
[1mStep[0m  [64/84], [94mLoss[0m : 2.32845
[1mStep[0m  [72/84], [94mLoss[0m : 2.11087
[1mStep[0m  [80/84], [94mLoss[0m : 2.14587

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32710
[1mStep[0m  [8/84], [94mLoss[0m : 2.02350
[1mStep[0m  [16/84], [94mLoss[0m : 2.62441
[1mStep[0m  [24/84], [94mLoss[0m : 2.13880
[1mStep[0m  [32/84], [94mLoss[0m : 2.13005
[1mStep[0m  [40/84], [94mLoss[0m : 2.29649
[1mStep[0m  [48/84], [94mLoss[0m : 2.20202
[1mStep[0m  [56/84], [94mLoss[0m : 2.07027
[1mStep[0m  [64/84], [94mLoss[0m : 2.47010
[1mStep[0m  [72/84], [94mLoss[0m : 2.15995
[1mStep[0m  [80/84], [94mLoss[0m : 2.39115

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36394
[1mStep[0m  [8/84], [94mLoss[0m : 2.25551
[1mStep[0m  [16/84], [94mLoss[0m : 2.03297
[1mStep[0m  [24/84], [94mLoss[0m : 2.04094
[1mStep[0m  [32/84], [94mLoss[0m : 2.21767
[1mStep[0m  [40/84], [94mLoss[0m : 2.17477
[1mStep[0m  [48/84], [94mLoss[0m : 2.18638
[1mStep[0m  [56/84], [94mLoss[0m : 1.96499
[1mStep[0m  [64/84], [94mLoss[0m : 2.14688
[1mStep[0m  [72/84], [94mLoss[0m : 2.22029
[1mStep[0m  [80/84], [94mLoss[0m : 2.50270

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07492
[1mStep[0m  [8/84], [94mLoss[0m : 2.23331
[1mStep[0m  [16/84], [94mLoss[0m : 1.97295
[1mStep[0m  [24/84], [94mLoss[0m : 2.16793
[1mStep[0m  [32/84], [94mLoss[0m : 2.39468
[1mStep[0m  [40/84], [94mLoss[0m : 2.08194
[1mStep[0m  [48/84], [94mLoss[0m : 2.23904
[1mStep[0m  [56/84], [94mLoss[0m : 2.33908
[1mStep[0m  [64/84], [94mLoss[0m : 2.05819
[1mStep[0m  [72/84], [94mLoss[0m : 2.30819
[1mStep[0m  [80/84], [94mLoss[0m : 2.17681

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02551
[1mStep[0m  [8/84], [94mLoss[0m : 2.12150
[1mStep[0m  [16/84], [94mLoss[0m : 2.17875
[1mStep[0m  [24/84], [94mLoss[0m : 1.93390
[1mStep[0m  [32/84], [94mLoss[0m : 2.11065
[1mStep[0m  [40/84], [94mLoss[0m : 2.21244
[1mStep[0m  [48/84], [94mLoss[0m : 2.33378
[1mStep[0m  [56/84], [94mLoss[0m : 2.09961
[1mStep[0m  [64/84], [94mLoss[0m : 1.93310
[1mStep[0m  [72/84], [94mLoss[0m : 2.39774
[1mStep[0m  [80/84], [94mLoss[0m : 2.18009

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.104, [92mTest[0m: 2.569, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69901
[1mStep[0m  [8/84], [94mLoss[0m : 2.19299
[1mStep[0m  [16/84], [94mLoss[0m : 2.00897
[1mStep[0m  [24/84], [94mLoss[0m : 2.07435
[1mStep[0m  [32/84], [94mLoss[0m : 2.11606
[1mStep[0m  [40/84], [94mLoss[0m : 1.98263
[1mStep[0m  [48/84], [94mLoss[0m : 2.13305
[1mStep[0m  [56/84], [94mLoss[0m : 2.17407
[1mStep[0m  [64/84], [94mLoss[0m : 2.21236
[1mStep[0m  [72/84], [94mLoss[0m : 2.27535
[1mStep[0m  [80/84], [94mLoss[0m : 1.80402

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02465
[1mStep[0m  [8/84], [94mLoss[0m : 2.14947
[1mStep[0m  [16/84], [94mLoss[0m : 2.09620
[1mStep[0m  [24/84], [94mLoss[0m : 2.10625
[1mStep[0m  [32/84], [94mLoss[0m : 2.00919
[1mStep[0m  [40/84], [94mLoss[0m : 1.99994
[1mStep[0m  [48/84], [94mLoss[0m : 1.92718
[1mStep[0m  [56/84], [94mLoss[0m : 2.12273
[1mStep[0m  [64/84], [94mLoss[0m : 2.05595
[1mStep[0m  [72/84], [94mLoss[0m : 1.85832
[1mStep[0m  [80/84], [94mLoss[0m : 1.87855

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90361
[1mStep[0m  [8/84], [94mLoss[0m : 1.90329
[1mStep[0m  [16/84], [94mLoss[0m : 2.10408
[1mStep[0m  [24/84], [94mLoss[0m : 1.87838
[1mStep[0m  [32/84], [94mLoss[0m : 1.78048
[1mStep[0m  [40/84], [94mLoss[0m : 2.16395
[1mStep[0m  [48/84], [94mLoss[0m : 1.95996
[1mStep[0m  [56/84], [94mLoss[0m : 1.71833
[1mStep[0m  [64/84], [94mLoss[0m : 1.97235
[1mStep[0m  [72/84], [94mLoss[0m : 1.92728
[1mStep[0m  [80/84], [94mLoss[0m : 2.12339

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16576
[1mStep[0m  [8/84], [94mLoss[0m : 1.83455
[1mStep[0m  [16/84], [94mLoss[0m : 1.83983
[1mStep[0m  [24/84], [94mLoss[0m : 2.04797
[1mStep[0m  [32/84], [94mLoss[0m : 1.89587
[1mStep[0m  [40/84], [94mLoss[0m : 1.80121
[1mStep[0m  [48/84], [94mLoss[0m : 1.83270
[1mStep[0m  [56/84], [94mLoss[0m : 1.76006
[1mStep[0m  [64/84], [94mLoss[0m : 2.10960
[1mStep[0m  [72/84], [94mLoss[0m : 2.18367
[1mStep[0m  [80/84], [94mLoss[0m : 2.09371

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93275
[1mStep[0m  [8/84], [94mLoss[0m : 1.81538
[1mStep[0m  [16/84], [94mLoss[0m : 2.00031
[1mStep[0m  [24/84], [94mLoss[0m : 2.02662
[1mStep[0m  [32/84], [94mLoss[0m : 1.97461
[1mStep[0m  [40/84], [94mLoss[0m : 1.67709
[1mStep[0m  [48/84], [94mLoss[0m : 2.32504
[1mStep[0m  [56/84], [94mLoss[0m : 2.02204
[1mStep[0m  [64/84], [94mLoss[0m : 1.99812
[1mStep[0m  [72/84], [94mLoss[0m : 2.28442
[1mStep[0m  [80/84], [94mLoss[0m : 1.90292

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68506
[1mStep[0m  [8/84], [94mLoss[0m : 2.04636
[1mStep[0m  [16/84], [94mLoss[0m : 1.71073
[1mStep[0m  [24/84], [94mLoss[0m : 1.91583
[1mStep[0m  [32/84], [94mLoss[0m : 1.89453
[1mStep[0m  [40/84], [94mLoss[0m : 2.00105
[1mStep[0m  [48/84], [94mLoss[0m : 2.23297
[1mStep[0m  [56/84], [94mLoss[0m : 1.93598
[1mStep[0m  [64/84], [94mLoss[0m : 1.87392
[1mStep[0m  [72/84], [94mLoss[0m : 1.98593
[1mStep[0m  [80/84], [94mLoss[0m : 1.76634

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71897
[1mStep[0m  [8/84], [94mLoss[0m : 1.95727
[1mStep[0m  [16/84], [94mLoss[0m : 1.89911
[1mStep[0m  [24/84], [94mLoss[0m : 1.82400
[1mStep[0m  [32/84], [94mLoss[0m : 1.51930
[1mStep[0m  [40/84], [94mLoss[0m : 1.63438
[1mStep[0m  [48/84], [94mLoss[0m : 1.83793
[1mStep[0m  [56/84], [94mLoss[0m : 1.91904
[1mStep[0m  [64/84], [94mLoss[0m : 1.81860
[1mStep[0m  [72/84], [94mLoss[0m : 1.73339
[1mStep[0m  [80/84], [94mLoss[0m : 2.13436

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63400
[1mStep[0m  [8/84], [94mLoss[0m : 1.87014
[1mStep[0m  [16/84], [94mLoss[0m : 1.83013
[1mStep[0m  [24/84], [94mLoss[0m : 1.86836
[1mStep[0m  [32/84], [94mLoss[0m : 1.71821
[1mStep[0m  [40/84], [94mLoss[0m : 1.99373
[1mStep[0m  [48/84], [94mLoss[0m : 1.89282
[1mStep[0m  [56/84], [94mLoss[0m : 1.73665
[1mStep[0m  [64/84], [94mLoss[0m : 1.88311
[1mStep[0m  [72/84], [94mLoss[0m : 1.99467
[1mStep[0m  [80/84], [94mLoss[0m : 1.89833

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90519
[1mStep[0m  [8/84], [94mLoss[0m : 2.09070
[1mStep[0m  [16/84], [94mLoss[0m : 1.76803
[1mStep[0m  [24/84], [94mLoss[0m : 1.62319
[1mStep[0m  [32/84], [94mLoss[0m : 1.64040
[1mStep[0m  [40/84], [94mLoss[0m : 1.80540
[1mStep[0m  [48/84], [94mLoss[0m : 1.77456
[1mStep[0m  [56/84], [94mLoss[0m : 1.81553
[1mStep[0m  [64/84], [94mLoss[0m : 1.88432
[1mStep[0m  [72/84], [94mLoss[0m : 1.68742
[1mStep[0m  [80/84], [94mLoss[0m : 1.79202

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.517, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67280
[1mStep[0m  [8/84], [94mLoss[0m : 1.73133
[1mStep[0m  [16/84], [94mLoss[0m : 1.76419
[1mStep[0m  [24/84], [94mLoss[0m : 1.80790
[1mStep[0m  [32/84], [94mLoss[0m : 1.77029
[1mStep[0m  [40/84], [94mLoss[0m : 1.66700
[1mStep[0m  [48/84], [94mLoss[0m : 1.79945
[1mStep[0m  [56/84], [94mLoss[0m : 1.79715
[1mStep[0m  [64/84], [94mLoss[0m : 2.03607
[1mStep[0m  [72/84], [94mLoss[0m : 1.85887
[1mStep[0m  [80/84], [94mLoss[0m : 1.74403

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80315
[1mStep[0m  [8/84], [94mLoss[0m : 1.76856
[1mStep[0m  [16/84], [94mLoss[0m : 1.68984
[1mStep[0m  [24/84], [94mLoss[0m : 1.67114
[1mStep[0m  [32/84], [94mLoss[0m : 1.90881
[1mStep[0m  [40/84], [94mLoss[0m : 1.67389
[1mStep[0m  [48/84], [94mLoss[0m : 1.87115
[1mStep[0m  [56/84], [94mLoss[0m : 1.96309
[1mStep[0m  [64/84], [94mLoss[0m : 2.12932
[1mStep[0m  [72/84], [94mLoss[0m : 1.73386
[1mStep[0m  [80/84], [94mLoss[0m : 1.61208

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.765, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94534
[1mStep[0m  [8/84], [94mLoss[0m : 1.63742
[1mStep[0m  [16/84], [94mLoss[0m : 1.94631
[1mStep[0m  [24/84], [94mLoss[0m : 1.66255
[1mStep[0m  [32/84], [94mLoss[0m : 1.63327
[1mStep[0m  [40/84], [94mLoss[0m : 1.71422
[1mStep[0m  [48/84], [94mLoss[0m : 1.50005
[1mStep[0m  [56/84], [94mLoss[0m : 1.70655
[1mStep[0m  [64/84], [94mLoss[0m : 1.61978
[1mStep[0m  [72/84], [94mLoss[0m : 1.65265
[1mStep[0m  [80/84], [94mLoss[0m : 1.72243

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65079
[1mStep[0m  [8/84], [94mLoss[0m : 1.80942
[1mStep[0m  [16/84], [94mLoss[0m : 1.62182
[1mStep[0m  [24/84], [94mLoss[0m : 1.53372
[1mStep[0m  [32/84], [94mLoss[0m : 1.74068
[1mStep[0m  [40/84], [94mLoss[0m : 1.63025
[1mStep[0m  [48/84], [94mLoss[0m : 1.63369
[1mStep[0m  [56/84], [94mLoss[0m : 1.80302
[1mStep[0m  [64/84], [94mLoss[0m : 1.61398
[1mStep[0m  [72/84], [94mLoss[0m : 1.58944
[1mStep[0m  [80/84], [94mLoss[0m : 1.58653

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.545, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57564
[1mStep[0m  [8/84], [94mLoss[0m : 1.42780
[1mStep[0m  [16/84], [94mLoss[0m : 1.53145
[1mStep[0m  [24/84], [94mLoss[0m : 1.59479
[1mStep[0m  [32/84], [94mLoss[0m : 1.80205
[1mStep[0m  [40/84], [94mLoss[0m : 1.77873
[1mStep[0m  [48/84], [94mLoss[0m : 1.88184
[1mStep[0m  [56/84], [94mLoss[0m : 1.67626
[1mStep[0m  [64/84], [94mLoss[0m : 1.53808
[1mStep[0m  [72/84], [94mLoss[0m : 1.76037
[1mStep[0m  [80/84], [94mLoss[0m : 1.53684

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58284
[1mStep[0m  [8/84], [94mLoss[0m : 1.67822
[1mStep[0m  [16/84], [94mLoss[0m : 1.69025
[1mStep[0m  [24/84], [94mLoss[0m : 1.53581
[1mStep[0m  [32/84], [94mLoss[0m : 1.55605
[1mStep[0m  [40/84], [94mLoss[0m : 1.57746
[1mStep[0m  [48/84], [94mLoss[0m : 1.64559
[1mStep[0m  [56/84], [94mLoss[0m : 1.86808
[1mStep[0m  [64/84], [94mLoss[0m : 1.56523
[1mStep[0m  [72/84], [94mLoss[0m : 1.54483
[1mStep[0m  [80/84], [94mLoss[0m : 1.67529

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61086
[1mStep[0m  [8/84], [94mLoss[0m : 1.68371
[1mStep[0m  [16/84], [94mLoss[0m : 1.68809
[1mStep[0m  [24/84], [94mLoss[0m : 1.59392
[1mStep[0m  [32/84], [94mLoss[0m : 1.88334
[1mStep[0m  [40/84], [94mLoss[0m : 1.65005
[1mStep[0m  [48/84], [94mLoss[0m : 1.50939
[1mStep[0m  [56/84], [94mLoss[0m : 1.74653
[1mStep[0m  [64/84], [94mLoss[0m : 1.48183
[1mStep[0m  [72/84], [94mLoss[0m : 1.68644
[1mStep[0m  [80/84], [94mLoss[0m : 1.75925

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56810
[1mStep[0m  [8/84], [94mLoss[0m : 1.59526
[1mStep[0m  [16/84], [94mLoss[0m : 1.50363
[1mStep[0m  [24/84], [94mLoss[0m : 1.75199
[1mStep[0m  [32/84], [94mLoss[0m : 1.51338
[1mStep[0m  [40/84], [94mLoss[0m : 1.59143
[1mStep[0m  [48/84], [94mLoss[0m : 1.69315
[1mStep[0m  [56/84], [94mLoss[0m : 1.62507
[1mStep[0m  [64/84], [94mLoss[0m : 1.56649
[1mStep[0m  [72/84], [94mLoss[0m : 1.68538
[1mStep[0m  [80/84], [94mLoss[0m : 1.53106

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67440
[1mStep[0m  [8/84], [94mLoss[0m : 1.69194
[1mStep[0m  [16/84], [94mLoss[0m : 1.62380
[1mStep[0m  [24/84], [94mLoss[0m : 1.49184
[1mStep[0m  [32/84], [94mLoss[0m : 1.64016
[1mStep[0m  [40/84], [94mLoss[0m : 1.71602
[1mStep[0m  [48/84], [94mLoss[0m : 1.47403
[1mStep[0m  [56/84], [94mLoss[0m : 1.59382
[1mStep[0m  [64/84], [94mLoss[0m : 1.62856
[1mStep[0m  [72/84], [94mLoss[0m : 1.58911
[1mStep[0m  [80/84], [94mLoss[0m : 1.40431

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.574
====================================

Phase 2 - Evaluation MAE:  2.5740975056375777
MAE score P1        2.325334
MAE score P2        2.574098
loss                1.639129
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.5
weight_decay            0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.95311
[1mStep[0m  [8/84], [94mLoss[0m : 10.28654
[1mStep[0m  [16/84], [94mLoss[0m : 9.08459
[1mStep[0m  [24/84], [94mLoss[0m : 7.83752
[1mStep[0m  [32/84], [94mLoss[0m : 7.09762
[1mStep[0m  [40/84], [94mLoss[0m : 5.52925
[1mStep[0m  [48/84], [94mLoss[0m : 4.77135
[1mStep[0m  [56/84], [94mLoss[0m : 3.58111
[1mStep[0m  [64/84], [94mLoss[0m : 3.43891
[1mStep[0m  [72/84], [94mLoss[0m : 3.58158
[1mStep[0m  [80/84], [94mLoss[0m : 3.23022

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.110, [92mTest[0m: 10.982, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.01102
[1mStep[0m  [8/84], [94mLoss[0m : 2.71647
[1mStep[0m  [16/84], [94mLoss[0m : 2.69134
[1mStep[0m  [24/84], [94mLoss[0m : 2.57751
[1mStep[0m  [32/84], [94mLoss[0m : 2.71864
[1mStep[0m  [40/84], [94mLoss[0m : 2.80782
[1mStep[0m  [48/84], [94mLoss[0m : 2.84309
[1mStep[0m  [56/84], [94mLoss[0m : 2.59574
[1mStep[0m  [64/84], [94mLoss[0m : 2.88106
[1mStep[0m  [72/84], [94mLoss[0m : 2.76205
[1mStep[0m  [80/84], [94mLoss[0m : 3.05518

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.926, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46250
[1mStep[0m  [8/84], [94mLoss[0m : 2.74824
[1mStep[0m  [16/84], [94mLoss[0m : 2.75736
[1mStep[0m  [24/84], [94mLoss[0m : 2.46830
[1mStep[0m  [32/84], [94mLoss[0m : 2.90351
[1mStep[0m  [40/84], [94mLoss[0m : 2.73668
[1mStep[0m  [48/84], [94mLoss[0m : 2.68620
[1mStep[0m  [56/84], [94mLoss[0m : 2.38900
[1mStep[0m  [64/84], [94mLoss[0m : 2.81706
[1mStep[0m  [72/84], [94mLoss[0m : 2.88492
[1mStep[0m  [80/84], [94mLoss[0m : 2.91074

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72815
[1mStep[0m  [8/84], [94mLoss[0m : 2.59177
[1mStep[0m  [16/84], [94mLoss[0m : 2.40636
[1mStep[0m  [24/84], [94mLoss[0m : 2.87731
[1mStep[0m  [32/84], [94mLoss[0m : 2.79997
[1mStep[0m  [40/84], [94mLoss[0m : 2.94426
[1mStep[0m  [48/84], [94mLoss[0m : 2.91573
[1mStep[0m  [56/84], [94mLoss[0m : 2.90717
[1mStep[0m  [64/84], [94mLoss[0m : 2.87569
[1mStep[0m  [72/84], [94mLoss[0m : 2.96665
[1mStep[0m  [80/84], [94mLoss[0m : 2.42325

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72660
[1mStep[0m  [8/84], [94mLoss[0m : 2.69654
[1mStep[0m  [16/84], [94mLoss[0m : 3.08359
[1mStep[0m  [24/84], [94mLoss[0m : 2.54824
[1mStep[0m  [32/84], [94mLoss[0m : 2.59687
[1mStep[0m  [40/84], [94mLoss[0m : 2.61101
[1mStep[0m  [48/84], [94mLoss[0m : 2.87906
[1mStep[0m  [56/84], [94mLoss[0m : 2.39547
[1mStep[0m  [64/84], [94mLoss[0m : 2.69433
[1mStep[0m  [72/84], [94mLoss[0m : 2.44134
[1mStep[0m  [80/84], [94mLoss[0m : 2.40510

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83430
[1mStep[0m  [8/84], [94mLoss[0m : 2.87549
[1mStep[0m  [16/84], [94mLoss[0m : 2.47732
[1mStep[0m  [24/84], [94mLoss[0m : 2.89180
[1mStep[0m  [32/84], [94mLoss[0m : 2.94666
[1mStep[0m  [40/84], [94mLoss[0m : 2.54566
[1mStep[0m  [48/84], [94mLoss[0m : 2.68051
[1mStep[0m  [56/84], [94mLoss[0m : 2.46989
[1mStep[0m  [64/84], [94mLoss[0m : 2.57378
[1mStep[0m  [72/84], [94mLoss[0m : 2.89865
[1mStep[0m  [80/84], [94mLoss[0m : 2.56534

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96059
[1mStep[0m  [8/84], [94mLoss[0m : 2.59467
[1mStep[0m  [16/84], [94mLoss[0m : 3.04706
[1mStep[0m  [24/84], [94mLoss[0m : 2.64272
[1mStep[0m  [32/84], [94mLoss[0m : 2.80484
[1mStep[0m  [40/84], [94mLoss[0m : 3.01540
[1mStep[0m  [48/84], [94mLoss[0m : 2.20036
[1mStep[0m  [56/84], [94mLoss[0m : 2.62208
[1mStep[0m  [64/84], [94mLoss[0m : 2.75593
[1mStep[0m  [72/84], [94mLoss[0m : 2.66897
[1mStep[0m  [80/84], [94mLoss[0m : 2.84020

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80399
[1mStep[0m  [8/84], [94mLoss[0m : 2.32621
[1mStep[0m  [16/84], [94mLoss[0m : 2.66107
[1mStep[0m  [24/84], [94mLoss[0m : 2.47949
[1mStep[0m  [32/84], [94mLoss[0m : 2.74638
[1mStep[0m  [40/84], [94mLoss[0m : 2.58763
[1mStep[0m  [48/84], [94mLoss[0m : 2.62295
[1mStep[0m  [56/84], [94mLoss[0m : 2.57424
[1mStep[0m  [64/84], [94mLoss[0m : 2.67087
[1mStep[0m  [72/84], [94mLoss[0m : 2.57313
[1mStep[0m  [80/84], [94mLoss[0m : 2.63057

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81880
[1mStep[0m  [8/84], [94mLoss[0m : 3.16459
[1mStep[0m  [16/84], [94mLoss[0m : 2.52551
[1mStep[0m  [24/84], [94mLoss[0m : 2.78236
[1mStep[0m  [32/84], [94mLoss[0m : 2.40612
[1mStep[0m  [40/84], [94mLoss[0m : 2.55174
[1mStep[0m  [48/84], [94mLoss[0m : 2.40973
[1mStep[0m  [56/84], [94mLoss[0m : 2.60100
[1mStep[0m  [64/84], [94mLoss[0m : 2.61564
[1mStep[0m  [72/84], [94mLoss[0m : 2.39659
[1mStep[0m  [80/84], [94mLoss[0m : 2.70420

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33476
[1mStep[0m  [8/84], [94mLoss[0m : 2.32645
[1mStep[0m  [16/84], [94mLoss[0m : 2.79574
[1mStep[0m  [24/84], [94mLoss[0m : 2.75105
[1mStep[0m  [32/84], [94mLoss[0m : 2.63360
[1mStep[0m  [40/84], [94mLoss[0m : 2.42700
[1mStep[0m  [48/84], [94mLoss[0m : 2.46343
[1mStep[0m  [56/84], [94mLoss[0m : 2.83947
[1mStep[0m  [64/84], [94mLoss[0m : 2.99574
[1mStep[0m  [72/84], [94mLoss[0m : 2.43952
[1mStep[0m  [80/84], [94mLoss[0m : 2.56041

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46516
[1mStep[0m  [8/84], [94mLoss[0m : 2.88662
[1mStep[0m  [16/84], [94mLoss[0m : 2.65986
[1mStep[0m  [24/84], [94mLoss[0m : 2.31684
[1mStep[0m  [32/84], [94mLoss[0m : 2.52565
[1mStep[0m  [40/84], [94mLoss[0m : 2.44829
[1mStep[0m  [48/84], [94mLoss[0m : 2.62795
[1mStep[0m  [56/84], [94mLoss[0m : 2.83794
[1mStep[0m  [64/84], [94mLoss[0m : 2.50297
[1mStep[0m  [72/84], [94mLoss[0m : 2.34109
[1mStep[0m  [80/84], [94mLoss[0m : 2.78206

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.619, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51223
[1mStep[0m  [8/84], [94mLoss[0m : 2.68893
[1mStep[0m  [16/84], [94mLoss[0m : 2.34026
[1mStep[0m  [24/84], [94mLoss[0m : 2.81515
[1mStep[0m  [32/84], [94mLoss[0m : 2.79795
[1mStep[0m  [40/84], [94mLoss[0m : 2.75964
[1mStep[0m  [48/84], [94mLoss[0m : 2.84793
[1mStep[0m  [56/84], [94mLoss[0m : 2.76969
[1mStep[0m  [64/84], [94mLoss[0m : 2.54533
[1mStep[0m  [72/84], [94mLoss[0m : 2.79179
[1mStep[0m  [80/84], [94mLoss[0m : 2.80934

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95908
[1mStep[0m  [8/84], [94mLoss[0m : 2.67546
[1mStep[0m  [16/84], [94mLoss[0m : 2.34686
[1mStep[0m  [24/84], [94mLoss[0m : 2.31658
[1mStep[0m  [32/84], [94mLoss[0m : 2.52428
[1mStep[0m  [40/84], [94mLoss[0m : 2.51883
[1mStep[0m  [48/84], [94mLoss[0m : 2.66557
[1mStep[0m  [56/84], [94mLoss[0m : 2.39288
[1mStep[0m  [64/84], [94mLoss[0m : 2.31900
[1mStep[0m  [72/84], [94mLoss[0m : 2.47066
[1mStep[0m  [80/84], [94mLoss[0m : 2.82171

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23138
[1mStep[0m  [8/84], [94mLoss[0m : 2.62182
[1mStep[0m  [16/84], [94mLoss[0m : 2.59361
[1mStep[0m  [24/84], [94mLoss[0m : 2.40565
[1mStep[0m  [32/84], [94mLoss[0m : 2.51947
[1mStep[0m  [40/84], [94mLoss[0m : 2.54404
[1mStep[0m  [48/84], [94mLoss[0m : 2.78427
[1mStep[0m  [56/84], [94mLoss[0m : 2.42575
[1mStep[0m  [64/84], [94mLoss[0m : 2.66405
[1mStep[0m  [72/84], [94mLoss[0m : 2.73796
[1mStep[0m  [80/84], [94mLoss[0m : 2.28254

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73332
[1mStep[0m  [8/84], [94mLoss[0m : 2.90084
[1mStep[0m  [16/84], [94mLoss[0m : 2.49585
[1mStep[0m  [24/84], [94mLoss[0m : 2.54455
[1mStep[0m  [32/84], [94mLoss[0m : 2.79222
[1mStep[0m  [40/84], [94mLoss[0m : 2.81167
[1mStep[0m  [48/84], [94mLoss[0m : 2.77310
[1mStep[0m  [56/84], [94mLoss[0m : 2.39141
[1mStep[0m  [64/84], [94mLoss[0m : 2.73382
[1mStep[0m  [72/84], [94mLoss[0m : 2.59271
[1mStep[0m  [80/84], [94mLoss[0m : 2.52531

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34035
[1mStep[0m  [8/84], [94mLoss[0m : 2.53147
[1mStep[0m  [16/84], [94mLoss[0m : 2.65054
[1mStep[0m  [24/84], [94mLoss[0m : 2.65222
[1mStep[0m  [32/84], [94mLoss[0m : 2.46354
[1mStep[0m  [40/84], [94mLoss[0m : 2.59001
[1mStep[0m  [48/84], [94mLoss[0m : 2.70245
[1mStep[0m  [56/84], [94mLoss[0m : 2.59675
[1mStep[0m  [64/84], [94mLoss[0m : 2.49475
[1mStep[0m  [72/84], [94mLoss[0m : 2.54542
[1mStep[0m  [80/84], [94mLoss[0m : 2.67216

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77777
[1mStep[0m  [8/84], [94mLoss[0m : 2.54042
[1mStep[0m  [16/84], [94mLoss[0m : 2.79745
[1mStep[0m  [24/84], [94mLoss[0m : 2.52251
[1mStep[0m  [32/84], [94mLoss[0m : 2.39197
[1mStep[0m  [40/84], [94mLoss[0m : 2.76189
[1mStep[0m  [48/84], [94mLoss[0m : 2.50379
[1mStep[0m  [56/84], [94mLoss[0m : 2.12201
[1mStep[0m  [64/84], [94mLoss[0m : 2.75462
[1mStep[0m  [72/84], [94mLoss[0m : 2.68461
[1mStep[0m  [80/84], [94mLoss[0m : 3.12613

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35113
[1mStep[0m  [8/84], [94mLoss[0m : 2.61314
[1mStep[0m  [16/84], [94mLoss[0m : 2.82554
[1mStep[0m  [24/84], [94mLoss[0m : 2.64042
[1mStep[0m  [32/84], [94mLoss[0m : 2.43081
[1mStep[0m  [40/84], [94mLoss[0m : 2.72840
[1mStep[0m  [48/84], [94mLoss[0m : 2.51611
[1mStep[0m  [56/84], [94mLoss[0m : 2.68433
[1mStep[0m  [64/84], [94mLoss[0m : 2.84351
[1mStep[0m  [72/84], [94mLoss[0m : 2.92566
[1mStep[0m  [80/84], [94mLoss[0m : 2.62355

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97870
[1mStep[0m  [8/84], [94mLoss[0m : 2.73580
[1mStep[0m  [16/84], [94mLoss[0m : 2.57320
[1mStep[0m  [24/84], [94mLoss[0m : 2.46543
[1mStep[0m  [32/84], [94mLoss[0m : 3.00223
[1mStep[0m  [40/84], [94mLoss[0m : 2.56827
[1mStep[0m  [48/84], [94mLoss[0m : 2.70319
[1mStep[0m  [56/84], [94mLoss[0m : 2.70766
[1mStep[0m  [64/84], [94mLoss[0m : 2.59944
[1mStep[0m  [72/84], [94mLoss[0m : 2.40971
[1mStep[0m  [80/84], [94mLoss[0m : 2.42209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55913
[1mStep[0m  [8/84], [94mLoss[0m : 2.42039
[1mStep[0m  [16/84], [94mLoss[0m : 2.75696
[1mStep[0m  [24/84], [94mLoss[0m : 2.79797
[1mStep[0m  [32/84], [94mLoss[0m : 2.12579
[1mStep[0m  [40/84], [94mLoss[0m : 2.95826
[1mStep[0m  [48/84], [94mLoss[0m : 2.53018
[1mStep[0m  [56/84], [94mLoss[0m : 2.24057
[1mStep[0m  [64/84], [94mLoss[0m : 2.72149
[1mStep[0m  [72/84], [94mLoss[0m : 2.39585
[1mStep[0m  [80/84], [94mLoss[0m : 2.48033

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72425
[1mStep[0m  [8/84], [94mLoss[0m : 2.48873
[1mStep[0m  [16/84], [94mLoss[0m : 2.16415
[1mStep[0m  [24/84], [94mLoss[0m : 2.60817
[1mStep[0m  [32/84], [94mLoss[0m : 2.64531
[1mStep[0m  [40/84], [94mLoss[0m : 2.45668
[1mStep[0m  [48/84], [94mLoss[0m : 2.59840
[1mStep[0m  [56/84], [94mLoss[0m : 2.38774
[1mStep[0m  [64/84], [94mLoss[0m : 2.66793
[1mStep[0m  [72/84], [94mLoss[0m : 2.37289
[1mStep[0m  [80/84], [94mLoss[0m : 2.74949

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35237
[1mStep[0m  [8/84], [94mLoss[0m : 2.65193
[1mStep[0m  [16/84], [94mLoss[0m : 2.79301
[1mStep[0m  [24/84], [94mLoss[0m : 2.75749
[1mStep[0m  [32/84], [94mLoss[0m : 2.45052
[1mStep[0m  [40/84], [94mLoss[0m : 2.86030
[1mStep[0m  [48/84], [94mLoss[0m : 2.62962
[1mStep[0m  [56/84], [94mLoss[0m : 2.77125
[1mStep[0m  [64/84], [94mLoss[0m : 2.60393
[1mStep[0m  [72/84], [94mLoss[0m : 2.56784
[1mStep[0m  [80/84], [94mLoss[0m : 2.58719

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65447
[1mStep[0m  [8/84], [94mLoss[0m : 2.28938
[1mStep[0m  [16/84], [94mLoss[0m : 2.92209
[1mStep[0m  [24/84], [94mLoss[0m : 2.60251
[1mStep[0m  [32/84], [94mLoss[0m : 2.62974
[1mStep[0m  [40/84], [94mLoss[0m : 2.50331
[1mStep[0m  [48/84], [94mLoss[0m : 2.74092
[1mStep[0m  [56/84], [94mLoss[0m : 2.78079
[1mStep[0m  [64/84], [94mLoss[0m : 2.49937
[1mStep[0m  [72/84], [94mLoss[0m : 2.48261
[1mStep[0m  [80/84], [94mLoss[0m : 3.02588

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38235
[1mStep[0m  [8/84], [94mLoss[0m : 2.39398
[1mStep[0m  [16/84], [94mLoss[0m : 2.61780
[1mStep[0m  [24/84], [94mLoss[0m : 2.73131
[1mStep[0m  [32/84], [94mLoss[0m : 2.72535
[1mStep[0m  [40/84], [94mLoss[0m : 2.50601
[1mStep[0m  [48/84], [94mLoss[0m : 2.18732
[1mStep[0m  [56/84], [94mLoss[0m : 2.64937
[1mStep[0m  [64/84], [94mLoss[0m : 2.72240
[1mStep[0m  [72/84], [94mLoss[0m : 2.47311
[1mStep[0m  [80/84], [94mLoss[0m : 2.81857

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59472
[1mStep[0m  [8/84], [94mLoss[0m : 2.44037
[1mStep[0m  [16/84], [94mLoss[0m : 2.78571
[1mStep[0m  [24/84], [94mLoss[0m : 2.91039
[1mStep[0m  [32/84], [94mLoss[0m : 3.05884
[1mStep[0m  [40/84], [94mLoss[0m : 2.65215
[1mStep[0m  [48/84], [94mLoss[0m : 2.78004
[1mStep[0m  [56/84], [94mLoss[0m : 2.41189
[1mStep[0m  [64/84], [94mLoss[0m : 2.38382
[1mStep[0m  [72/84], [94mLoss[0m : 2.80540
[1mStep[0m  [80/84], [94mLoss[0m : 2.23389

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62105
[1mStep[0m  [8/84], [94mLoss[0m : 2.67536
[1mStep[0m  [16/84], [94mLoss[0m : 2.74975
[1mStep[0m  [24/84], [94mLoss[0m : 2.50667
[1mStep[0m  [32/84], [94mLoss[0m : 2.74005
[1mStep[0m  [40/84], [94mLoss[0m : 2.57747
[1mStep[0m  [48/84], [94mLoss[0m : 2.54419
[1mStep[0m  [56/84], [94mLoss[0m : 2.40434
[1mStep[0m  [64/84], [94mLoss[0m : 2.69203
[1mStep[0m  [72/84], [94mLoss[0m : 2.46308
[1mStep[0m  [80/84], [94mLoss[0m : 2.67432

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60809
[1mStep[0m  [8/84], [94mLoss[0m : 2.78165
[1mStep[0m  [16/84], [94mLoss[0m : 2.59843
[1mStep[0m  [24/84], [94mLoss[0m : 2.88544
[1mStep[0m  [32/84], [94mLoss[0m : 2.39295
[1mStep[0m  [40/84], [94mLoss[0m : 2.44671
[1mStep[0m  [48/84], [94mLoss[0m : 2.32889
[1mStep[0m  [56/84], [94mLoss[0m : 2.22868
[1mStep[0m  [64/84], [94mLoss[0m : 2.37412
[1mStep[0m  [72/84], [94mLoss[0m : 2.87407
[1mStep[0m  [80/84], [94mLoss[0m : 2.37119

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67172
[1mStep[0m  [8/84], [94mLoss[0m : 2.37936
[1mStep[0m  [16/84], [94mLoss[0m : 2.69630
[1mStep[0m  [24/84], [94mLoss[0m : 2.72193
[1mStep[0m  [32/84], [94mLoss[0m : 2.57101
[1mStep[0m  [40/84], [94mLoss[0m : 2.73402
[1mStep[0m  [48/84], [94mLoss[0m : 2.54165
[1mStep[0m  [56/84], [94mLoss[0m : 2.03902
[1mStep[0m  [64/84], [94mLoss[0m : 2.82529
[1mStep[0m  [72/84], [94mLoss[0m : 2.61788
[1mStep[0m  [80/84], [94mLoss[0m : 2.74083

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52496
[1mStep[0m  [8/84], [94mLoss[0m : 2.30656
[1mStep[0m  [16/84], [94mLoss[0m : 2.51327
[1mStep[0m  [24/84], [94mLoss[0m : 2.55873
[1mStep[0m  [32/84], [94mLoss[0m : 2.23604
[1mStep[0m  [40/84], [94mLoss[0m : 2.62195
[1mStep[0m  [48/84], [94mLoss[0m : 2.94066
[1mStep[0m  [56/84], [94mLoss[0m : 2.62707
[1mStep[0m  [64/84], [94mLoss[0m : 2.02636
[1mStep[0m  [72/84], [94mLoss[0m : 2.51562
[1mStep[0m  [80/84], [94mLoss[0m : 2.78019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80124
[1mStep[0m  [8/84], [94mLoss[0m : 2.52876
[1mStep[0m  [16/84], [94mLoss[0m : 2.76365
[1mStep[0m  [24/84], [94mLoss[0m : 2.59674
[1mStep[0m  [32/84], [94mLoss[0m : 2.62951
[1mStep[0m  [40/84], [94mLoss[0m : 2.56167
[1mStep[0m  [48/84], [94mLoss[0m : 2.33043
[1mStep[0m  [56/84], [94mLoss[0m : 2.71627
[1mStep[0m  [64/84], [94mLoss[0m : 2.48643
[1mStep[0m  [72/84], [94mLoss[0m : 2.52414
[1mStep[0m  [80/84], [94mLoss[0m : 2.39365

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3317534582955495
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.51500
[1mStep[0m  [8/84], [94mLoss[0m : 2.74660
[1mStep[0m  [16/84], [94mLoss[0m : 2.30963
[1mStep[0m  [24/84], [94mLoss[0m : 2.07406
[1mStep[0m  [32/84], [94mLoss[0m : 2.40087
[1mStep[0m  [40/84], [94mLoss[0m : 2.59111
[1mStep[0m  [48/84], [94mLoss[0m : 2.56686
[1mStep[0m  [56/84], [94mLoss[0m : 2.72593
[1mStep[0m  [64/84], [94mLoss[0m : 2.46665
[1mStep[0m  [72/84], [94mLoss[0m : 2.37752
[1mStep[0m  [80/84], [94mLoss[0m : 2.63028

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51111
[1mStep[0m  [8/84], [94mLoss[0m : 2.69871
[1mStep[0m  [16/84], [94mLoss[0m : 2.34829
[1mStep[0m  [24/84], [94mLoss[0m : 2.50604
[1mStep[0m  [32/84], [94mLoss[0m : 2.52084
[1mStep[0m  [40/84], [94mLoss[0m : 2.69585
[1mStep[0m  [48/84], [94mLoss[0m : 2.27866
[1mStep[0m  [56/84], [94mLoss[0m : 2.59392
[1mStep[0m  [64/84], [94mLoss[0m : 2.77260
[1mStep[0m  [72/84], [94mLoss[0m : 2.58747
[1mStep[0m  [80/84], [94mLoss[0m : 2.42062

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61477
[1mStep[0m  [8/84], [94mLoss[0m : 2.60920
[1mStep[0m  [16/84], [94mLoss[0m : 2.48487
[1mStep[0m  [24/84], [94mLoss[0m : 2.50650
[1mStep[0m  [32/84], [94mLoss[0m : 2.39495
[1mStep[0m  [40/84], [94mLoss[0m : 2.80773
[1mStep[0m  [48/84], [94mLoss[0m : 2.19591
[1mStep[0m  [56/84], [94mLoss[0m : 2.53717
[1mStep[0m  [64/84], [94mLoss[0m : 2.65915
[1mStep[0m  [72/84], [94mLoss[0m : 2.63331
[1mStep[0m  [80/84], [94mLoss[0m : 2.30056

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54965
[1mStep[0m  [8/84], [94mLoss[0m : 2.53794
[1mStep[0m  [16/84], [94mLoss[0m : 2.56763
[1mStep[0m  [24/84], [94mLoss[0m : 2.28512
[1mStep[0m  [32/84], [94mLoss[0m : 2.58495
[1mStep[0m  [40/84], [94mLoss[0m : 2.59492
[1mStep[0m  [48/84], [94mLoss[0m : 2.26692
[1mStep[0m  [56/84], [94mLoss[0m : 2.44555
[1mStep[0m  [64/84], [94mLoss[0m : 2.46007
[1mStep[0m  [72/84], [94mLoss[0m : 2.28665
[1mStep[0m  [80/84], [94mLoss[0m : 2.75205

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79644
[1mStep[0m  [8/84], [94mLoss[0m : 2.31974
[1mStep[0m  [16/84], [94mLoss[0m : 2.32506
[1mStep[0m  [24/84], [94mLoss[0m : 2.45442
[1mStep[0m  [32/84], [94mLoss[0m : 2.81828
[1mStep[0m  [40/84], [94mLoss[0m : 2.12546
[1mStep[0m  [48/84], [94mLoss[0m : 2.43657
[1mStep[0m  [56/84], [94mLoss[0m : 2.66464
[1mStep[0m  [64/84], [94mLoss[0m : 2.35961
[1mStep[0m  [72/84], [94mLoss[0m : 2.62505
[1mStep[0m  [80/84], [94mLoss[0m : 2.34144

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34873
[1mStep[0m  [8/84], [94mLoss[0m : 2.35903
[1mStep[0m  [16/84], [94mLoss[0m : 2.21731
[1mStep[0m  [24/84], [94mLoss[0m : 2.10219
[1mStep[0m  [32/84], [94mLoss[0m : 2.27933
[1mStep[0m  [40/84], [94mLoss[0m : 2.28271
[1mStep[0m  [48/84], [94mLoss[0m : 2.37307
[1mStep[0m  [56/84], [94mLoss[0m : 2.48292
[1mStep[0m  [64/84], [94mLoss[0m : 2.40603
[1mStep[0m  [72/84], [94mLoss[0m : 2.41659
[1mStep[0m  [80/84], [94mLoss[0m : 2.36834

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33745
[1mStep[0m  [8/84], [94mLoss[0m : 2.56822
[1mStep[0m  [16/84], [94mLoss[0m : 2.15196
[1mStep[0m  [24/84], [94mLoss[0m : 2.41176
[1mStep[0m  [32/84], [94mLoss[0m : 2.70733
[1mStep[0m  [40/84], [94mLoss[0m : 2.40166
[1mStep[0m  [48/84], [94mLoss[0m : 2.31211
[1mStep[0m  [56/84], [94mLoss[0m : 2.69308
[1mStep[0m  [64/84], [94mLoss[0m : 2.40570
[1mStep[0m  [72/84], [94mLoss[0m : 2.39201
[1mStep[0m  [80/84], [94mLoss[0m : 2.08535

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25741
[1mStep[0m  [8/84], [94mLoss[0m : 2.15298
[1mStep[0m  [16/84], [94mLoss[0m : 2.25997
[1mStep[0m  [24/84], [94mLoss[0m : 2.40078
[1mStep[0m  [32/84], [94mLoss[0m : 2.45888
[1mStep[0m  [40/84], [94mLoss[0m : 1.96081
[1mStep[0m  [48/84], [94mLoss[0m : 2.42426
[1mStep[0m  [56/84], [94mLoss[0m : 2.51043
[1mStep[0m  [64/84], [94mLoss[0m : 2.32828
[1mStep[0m  [72/84], [94mLoss[0m : 2.29528
[1mStep[0m  [80/84], [94mLoss[0m : 2.15709

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23842
[1mStep[0m  [8/84], [94mLoss[0m : 2.36009
[1mStep[0m  [16/84], [94mLoss[0m : 1.95127
[1mStep[0m  [24/84], [94mLoss[0m : 2.26870
[1mStep[0m  [32/84], [94mLoss[0m : 2.65137
[1mStep[0m  [40/84], [94mLoss[0m : 2.49193
[1mStep[0m  [48/84], [94mLoss[0m : 2.26488
[1mStep[0m  [56/84], [94mLoss[0m : 2.39563
[1mStep[0m  [64/84], [94mLoss[0m : 2.55505
[1mStep[0m  [72/84], [94mLoss[0m : 2.39464
[1mStep[0m  [80/84], [94mLoss[0m : 2.53354

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03384
[1mStep[0m  [8/84], [94mLoss[0m : 2.08815
[1mStep[0m  [16/84], [94mLoss[0m : 2.09937
[1mStep[0m  [24/84], [94mLoss[0m : 2.12308
[1mStep[0m  [32/84], [94mLoss[0m : 2.56746
[1mStep[0m  [40/84], [94mLoss[0m : 2.39321
[1mStep[0m  [48/84], [94mLoss[0m : 2.27516
[1mStep[0m  [56/84], [94mLoss[0m : 2.38622
[1mStep[0m  [64/84], [94mLoss[0m : 2.54436
[1mStep[0m  [72/84], [94mLoss[0m : 2.19457
[1mStep[0m  [80/84], [94mLoss[0m : 2.22762

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30319
[1mStep[0m  [8/84], [94mLoss[0m : 1.84050
[1mStep[0m  [16/84], [94mLoss[0m : 1.96661
[1mStep[0m  [24/84], [94mLoss[0m : 2.40165
[1mStep[0m  [32/84], [94mLoss[0m : 2.41427
[1mStep[0m  [40/84], [94mLoss[0m : 2.33177
[1mStep[0m  [48/84], [94mLoss[0m : 2.26278
[1mStep[0m  [56/84], [94mLoss[0m : 2.37370
[1mStep[0m  [64/84], [94mLoss[0m : 1.87858
[1mStep[0m  [72/84], [94mLoss[0m : 2.07520
[1mStep[0m  [80/84], [94mLoss[0m : 2.45125

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10585
[1mStep[0m  [8/84], [94mLoss[0m : 1.78495
[1mStep[0m  [16/84], [94mLoss[0m : 2.09089
[1mStep[0m  [24/84], [94mLoss[0m : 2.50773
[1mStep[0m  [32/84], [94mLoss[0m : 2.37630
[1mStep[0m  [40/84], [94mLoss[0m : 2.32021
[1mStep[0m  [48/84], [94mLoss[0m : 2.14775
[1mStep[0m  [56/84], [94mLoss[0m : 2.20610
[1mStep[0m  [64/84], [94mLoss[0m : 2.07947
[1mStep[0m  [72/84], [94mLoss[0m : 2.61313
[1mStep[0m  [80/84], [94mLoss[0m : 2.31914

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.197, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11869
[1mStep[0m  [8/84], [94mLoss[0m : 2.20958
[1mStep[0m  [16/84], [94mLoss[0m : 2.51956
[1mStep[0m  [24/84], [94mLoss[0m : 2.15085
[1mStep[0m  [32/84], [94mLoss[0m : 2.34298
[1mStep[0m  [40/84], [94mLoss[0m : 2.27557
[1mStep[0m  [48/84], [94mLoss[0m : 2.16908
[1mStep[0m  [56/84], [94mLoss[0m : 2.06154
[1mStep[0m  [64/84], [94mLoss[0m : 2.31358
[1mStep[0m  [72/84], [94mLoss[0m : 1.89197
[1mStep[0m  [80/84], [94mLoss[0m : 2.26282

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18224
[1mStep[0m  [8/84], [94mLoss[0m : 1.95071
[1mStep[0m  [16/84], [94mLoss[0m : 2.32498
[1mStep[0m  [24/84], [94mLoss[0m : 2.26607
[1mStep[0m  [32/84], [94mLoss[0m : 2.11714
[1mStep[0m  [40/84], [94mLoss[0m : 2.02396
[1mStep[0m  [48/84], [94mLoss[0m : 2.14276
[1mStep[0m  [56/84], [94mLoss[0m : 2.19558
[1mStep[0m  [64/84], [94mLoss[0m : 2.17796
[1mStep[0m  [72/84], [94mLoss[0m : 2.04988
[1mStep[0m  [80/84], [94mLoss[0m : 1.90558

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.566, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77041
[1mStep[0m  [8/84], [94mLoss[0m : 2.10894
[1mStep[0m  [16/84], [94mLoss[0m : 2.10475
[1mStep[0m  [24/84], [94mLoss[0m : 2.17136
[1mStep[0m  [32/84], [94mLoss[0m : 2.07095
[1mStep[0m  [40/84], [94mLoss[0m : 2.22960
[1mStep[0m  [48/84], [94mLoss[0m : 2.05756
[1mStep[0m  [56/84], [94mLoss[0m : 1.99683
[1mStep[0m  [64/84], [94mLoss[0m : 1.82980
[1mStep[0m  [72/84], [94mLoss[0m : 1.94607
[1mStep[0m  [80/84], [94mLoss[0m : 2.03398

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84906
[1mStep[0m  [8/84], [94mLoss[0m : 2.40170
[1mStep[0m  [16/84], [94mLoss[0m : 2.29615
[1mStep[0m  [24/84], [94mLoss[0m : 2.06288
[1mStep[0m  [32/84], [94mLoss[0m : 1.91752
[1mStep[0m  [40/84], [94mLoss[0m : 2.20827
[1mStep[0m  [48/84], [94mLoss[0m : 2.26633
[1mStep[0m  [56/84], [94mLoss[0m : 2.23875
[1mStep[0m  [64/84], [94mLoss[0m : 1.79730
[1mStep[0m  [72/84], [94mLoss[0m : 2.11095
[1mStep[0m  [80/84], [94mLoss[0m : 2.03621

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16929
[1mStep[0m  [8/84], [94mLoss[0m : 1.84824
[1mStep[0m  [16/84], [94mLoss[0m : 1.99786
[1mStep[0m  [24/84], [94mLoss[0m : 2.10257
[1mStep[0m  [32/84], [94mLoss[0m : 1.86685
[1mStep[0m  [40/84], [94mLoss[0m : 2.15559
[1mStep[0m  [48/84], [94mLoss[0m : 1.97218
[1mStep[0m  [56/84], [94mLoss[0m : 1.82684
[1mStep[0m  [64/84], [94mLoss[0m : 1.95762
[1mStep[0m  [72/84], [94mLoss[0m : 1.99865
[1mStep[0m  [80/84], [94mLoss[0m : 1.98543

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97523
[1mStep[0m  [8/84], [94mLoss[0m : 2.17147
[1mStep[0m  [16/84], [94mLoss[0m : 1.96576
[1mStep[0m  [24/84], [94mLoss[0m : 1.97083
[1mStep[0m  [32/84], [94mLoss[0m : 2.20854
[1mStep[0m  [40/84], [94mLoss[0m : 2.02909
[1mStep[0m  [48/84], [94mLoss[0m : 1.97063
[1mStep[0m  [56/84], [94mLoss[0m : 1.86649
[1mStep[0m  [64/84], [94mLoss[0m : 2.29436
[1mStep[0m  [72/84], [94mLoss[0m : 2.09263
[1mStep[0m  [80/84], [94mLoss[0m : 1.99107

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07474
[1mStep[0m  [8/84], [94mLoss[0m : 1.71854
[1mStep[0m  [16/84], [94mLoss[0m : 1.71119
[1mStep[0m  [24/84], [94mLoss[0m : 2.04097
[1mStep[0m  [32/84], [94mLoss[0m : 2.17237
[1mStep[0m  [40/84], [94mLoss[0m : 2.13256
[1mStep[0m  [48/84], [94mLoss[0m : 2.04665
[1mStep[0m  [56/84], [94mLoss[0m : 1.95084
[1mStep[0m  [64/84], [94mLoss[0m : 1.94765
[1mStep[0m  [72/84], [94mLoss[0m : 1.79639
[1mStep[0m  [80/84], [94mLoss[0m : 2.13895

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94753
[1mStep[0m  [8/84], [94mLoss[0m : 2.12868
[1mStep[0m  [16/84], [94mLoss[0m : 1.64447
[1mStep[0m  [24/84], [94mLoss[0m : 2.19798
[1mStep[0m  [32/84], [94mLoss[0m : 1.84509
[1mStep[0m  [40/84], [94mLoss[0m : 2.01810
[1mStep[0m  [48/84], [94mLoss[0m : 2.11450
[1mStep[0m  [56/84], [94mLoss[0m : 1.89541
[1mStep[0m  [64/84], [94mLoss[0m : 1.77194
[1mStep[0m  [72/84], [94mLoss[0m : 2.18937
[1mStep[0m  [80/84], [94mLoss[0m : 2.11948

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.551, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69835
[1mStep[0m  [8/84], [94mLoss[0m : 1.76430
[1mStep[0m  [16/84], [94mLoss[0m : 1.94675
[1mStep[0m  [24/84], [94mLoss[0m : 2.02573
[1mStep[0m  [32/84], [94mLoss[0m : 1.87369
[1mStep[0m  [40/84], [94mLoss[0m : 2.13025
[1mStep[0m  [48/84], [94mLoss[0m : 1.83861
[1mStep[0m  [56/84], [94mLoss[0m : 2.00079
[1mStep[0m  [64/84], [94mLoss[0m : 1.94021
[1mStep[0m  [72/84], [94mLoss[0m : 2.15706
[1mStep[0m  [80/84], [94mLoss[0m : 1.81206

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92598
[1mStep[0m  [8/84], [94mLoss[0m : 1.80227
[1mStep[0m  [16/84], [94mLoss[0m : 1.50978
[1mStep[0m  [24/84], [94mLoss[0m : 1.80808
[1mStep[0m  [32/84], [94mLoss[0m : 1.89288
[1mStep[0m  [40/84], [94mLoss[0m : 1.85993
[1mStep[0m  [48/84], [94mLoss[0m : 2.11597
[1mStep[0m  [56/84], [94mLoss[0m : 1.83142
[1mStep[0m  [64/84], [94mLoss[0m : 2.15327
[1mStep[0m  [72/84], [94mLoss[0m : 1.95097
[1mStep[0m  [80/84], [94mLoss[0m : 2.07591

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77864
[1mStep[0m  [8/84], [94mLoss[0m : 1.82872
[1mStep[0m  [16/84], [94mLoss[0m : 1.81476
[1mStep[0m  [24/84], [94mLoss[0m : 1.78370
[1mStep[0m  [32/84], [94mLoss[0m : 1.80171
[1mStep[0m  [40/84], [94mLoss[0m : 1.74296
[1mStep[0m  [48/84], [94mLoss[0m : 1.76190
[1mStep[0m  [56/84], [94mLoss[0m : 1.91479
[1mStep[0m  [64/84], [94mLoss[0m : 1.76095
[1mStep[0m  [72/84], [94mLoss[0m : 1.68973
[1mStep[0m  [80/84], [94mLoss[0m : 1.90354

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01247
[1mStep[0m  [8/84], [94mLoss[0m : 1.79104
[1mStep[0m  [16/84], [94mLoss[0m : 1.75212
[1mStep[0m  [24/84], [94mLoss[0m : 1.77337
[1mStep[0m  [32/84], [94mLoss[0m : 2.12127
[1mStep[0m  [40/84], [94mLoss[0m : 1.72804
[1mStep[0m  [48/84], [94mLoss[0m : 2.05604
[1mStep[0m  [56/84], [94mLoss[0m : 1.69362
[1mStep[0m  [64/84], [94mLoss[0m : 1.72307
[1mStep[0m  [72/84], [94mLoss[0m : 1.81540
[1mStep[0m  [80/84], [94mLoss[0m : 1.85434

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.848, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79207
[1mStep[0m  [8/84], [94mLoss[0m : 1.79851
[1mStep[0m  [16/84], [94mLoss[0m : 1.70279
[1mStep[0m  [24/84], [94mLoss[0m : 1.89786
[1mStep[0m  [32/84], [94mLoss[0m : 1.77988
[1mStep[0m  [40/84], [94mLoss[0m : 1.93082
[1mStep[0m  [48/84], [94mLoss[0m : 2.04510
[1mStep[0m  [56/84], [94mLoss[0m : 1.89280
[1mStep[0m  [64/84], [94mLoss[0m : 1.45076
[1mStep[0m  [72/84], [94mLoss[0m : 1.92008
[1mStep[0m  [80/84], [94mLoss[0m : 1.97443

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00864
[1mStep[0m  [8/84], [94mLoss[0m : 1.58688
[1mStep[0m  [16/84], [94mLoss[0m : 2.03086
[1mStep[0m  [24/84], [94mLoss[0m : 2.13029
[1mStep[0m  [32/84], [94mLoss[0m : 1.69392
[1mStep[0m  [40/84], [94mLoss[0m : 1.86386
[1mStep[0m  [48/84], [94mLoss[0m : 1.67998
[1mStep[0m  [56/84], [94mLoss[0m : 1.79626
[1mStep[0m  [64/84], [94mLoss[0m : 1.67728
[1mStep[0m  [72/84], [94mLoss[0m : 1.74522
[1mStep[0m  [80/84], [94mLoss[0m : 1.70969

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.600, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87461
[1mStep[0m  [8/84], [94mLoss[0m : 1.85066
[1mStep[0m  [16/84], [94mLoss[0m : 1.86824
[1mStep[0m  [24/84], [94mLoss[0m : 2.04271
[1mStep[0m  [32/84], [94mLoss[0m : 1.72801
[1mStep[0m  [40/84], [94mLoss[0m : 1.59816
[1mStep[0m  [48/84], [94mLoss[0m : 2.00759
[1mStep[0m  [56/84], [94mLoss[0m : 1.78902
[1mStep[0m  [64/84], [94mLoss[0m : 1.72212
[1mStep[0m  [72/84], [94mLoss[0m : 1.69802
[1mStep[0m  [80/84], [94mLoss[0m : 1.88690

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76615
[1mStep[0m  [8/84], [94mLoss[0m : 1.77848
[1mStep[0m  [16/84], [94mLoss[0m : 1.93668
[1mStep[0m  [24/84], [94mLoss[0m : 1.51307
[1mStep[0m  [32/84], [94mLoss[0m : 1.81573
[1mStep[0m  [40/84], [94mLoss[0m : 1.76739
[1mStep[0m  [48/84], [94mLoss[0m : 1.80728
[1mStep[0m  [56/84], [94mLoss[0m : 1.77990
[1mStep[0m  [64/84], [94mLoss[0m : 1.67898
[1mStep[0m  [72/84], [94mLoss[0m : 1.77746
[1mStep[0m  [80/84], [94mLoss[0m : 1.87791

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88070
[1mStep[0m  [8/84], [94mLoss[0m : 2.03469
[1mStep[0m  [16/84], [94mLoss[0m : 1.57434
[1mStep[0m  [24/84], [94mLoss[0m : 1.61489
[1mStep[0m  [32/84], [94mLoss[0m : 1.73449
[1mStep[0m  [40/84], [94mLoss[0m : 2.04064
[1mStep[0m  [48/84], [94mLoss[0m : 1.83742
[1mStep[0m  [56/84], [94mLoss[0m : 1.88177
[1mStep[0m  [64/84], [94mLoss[0m : 1.55547
[1mStep[0m  [72/84], [94mLoss[0m : 1.72410
[1mStep[0m  [80/84], [94mLoss[0m : 1.72092

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73513
[1mStep[0m  [8/84], [94mLoss[0m : 1.97484
[1mStep[0m  [16/84], [94mLoss[0m : 1.51138
[1mStep[0m  [24/84], [94mLoss[0m : 1.74444
[1mStep[0m  [32/84], [94mLoss[0m : 2.06559
[1mStep[0m  [40/84], [94mLoss[0m : 1.61054
[1mStep[0m  [48/84], [94mLoss[0m : 1.89476
[1mStep[0m  [56/84], [94mLoss[0m : 1.80687
[1mStep[0m  [64/84], [94mLoss[0m : 2.07715
[1mStep[0m  [72/84], [94mLoss[0m : 2.07082
[1mStep[0m  [80/84], [94mLoss[0m : 1.93313

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.565
====================================

Phase 2 - Evaluation MAE:  2.5651930400303433
MAE score P1       2.331753
MAE score P2       2.565193
loss               1.750395
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.64833
[1mStep[0m  [4/42], [94mLoss[0m : 10.34892
[1mStep[0m  [8/42], [94mLoss[0m : 9.92317
[1mStep[0m  [12/42], [94mLoss[0m : 9.60893
[1mStep[0m  [16/42], [94mLoss[0m : 9.08541
[1mStep[0m  [20/42], [94mLoss[0m : 8.66184
[1mStep[0m  [24/42], [94mLoss[0m : 8.35023
[1mStep[0m  [28/42], [94mLoss[0m : 8.19089
[1mStep[0m  [32/42], [94mLoss[0m : 7.94696
[1mStep[0m  [36/42], [94mLoss[0m : 7.25993
[1mStep[0m  [40/42], [94mLoss[0m : 7.01205

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.763, [92mTest[0m: 10.865, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.62235
[1mStep[0m  [4/42], [94mLoss[0m : 6.99218
[1mStep[0m  [8/42], [94mLoss[0m : 5.95165
[1mStep[0m  [12/42], [94mLoss[0m : 5.96285
[1mStep[0m  [16/42], [94mLoss[0m : 5.38934
[1mStep[0m  [20/42], [94mLoss[0m : 5.45676
[1mStep[0m  [24/42], [94mLoss[0m : 4.69226
[1mStep[0m  [28/42], [94mLoss[0m : 4.32004
[1mStep[0m  [32/42], [94mLoss[0m : 4.39759
[1mStep[0m  [36/42], [94mLoss[0m : 4.08599
[1mStep[0m  [40/42], [94mLoss[0m : 4.13768

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.222, [92mTest[0m: 8.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.81068
[1mStep[0m  [4/42], [94mLoss[0m : 3.34356
[1mStep[0m  [8/42], [94mLoss[0m : 3.65189
[1mStep[0m  [12/42], [94mLoss[0m : 3.66236
[1mStep[0m  [16/42], [94mLoss[0m : 3.13085
[1mStep[0m  [20/42], [94mLoss[0m : 3.21363
[1mStep[0m  [24/42], [94mLoss[0m : 3.05863
[1mStep[0m  [28/42], [94mLoss[0m : 2.99305
[1mStep[0m  [32/42], [94mLoss[0m : 2.82293
[1mStep[0m  [36/42], [94mLoss[0m : 3.10090
[1mStep[0m  [40/42], [94mLoss[0m : 2.97159

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.228, [92mTest[0m: 5.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71604
[1mStep[0m  [4/42], [94mLoss[0m : 2.76068
[1mStep[0m  [8/42], [94mLoss[0m : 3.00009
[1mStep[0m  [12/42], [94mLoss[0m : 2.65857
[1mStep[0m  [16/42], [94mLoss[0m : 3.07397
[1mStep[0m  [20/42], [94mLoss[0m : 2.60649
[1mStep[0m  [24/42], [94mLoss[0m : 3.17804
[1mStep[0m  [28/42], [94mLoss[0m : 2.73850
[1mStep[0m  [32/42], [94mLoss[0m : 2.56685
[1mStep[0m  [36/42], [94mLoss[0m : 2.69720
[1mStep[0m  [40/42], [94mLoss[0m : 2.50064

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.784, [92mTest[0m: 3.873, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66303
[1mStep[0m  [4/42], [94mLoss[0m : 2.72534
[1mStep[0m  [8/42], [94mLoss[0m : 2.66277
[1mStep[0m  [12/42], [94mLoss[0m : 2.65107
[1mStep[0m  [16/42], [94mLoss[0m : 2.79582
[1mStep[0m  [20/42], [94mLoss[0m : 2.87506
[1mStep[0m  [24/42], [94mLoss[0m : 2.60757
[1mStep[0m  [28/42], [94mLoss[0m : 2.73435
[1mStep[0m  [32/42], [94mLoss[0m : 2.59236
[1mStep[0m  [36/42], [94mLoss[0m : 2.51234
[1mStep[0m  [40/42], [94mLoss[0m : 2.74444

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.724, [92mTest[0m: 3.240, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.85314
[1mStep[0m  [4/42], [94mLoss[0m : 2.23407
[1mStep[0m  [8/42], [94mLoss[0m : 2.93087
[1mStep[0m  [12/42], [94mLoss[0m : 2.64490
[1mStep[0m  [16/42], [94mLoss[0m : 2.62343
[1mStep[0m  [20/42], [94mLoss[0m : 2.62181
[1mStep[0m  [24/42], [94mLoss[0m : 2.87777
[1mStep[0m  [28/42], [94mLoss[0m : 2.38613
[1mStep[0m  [32/42], [94mLoss[0m : 2.63617
[1mStep[0m  [36/42], [94mLoss[0m : 2.78656
[1mStep[0m  [40/42], [94mLoss[0m : 2.67304

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.671, [92mTest[0m: 3.024, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50985
[1mStep[0m  [4/42], [94mLoss[0m : 2.74524
[1mStep[0m  [8/42], [94mLoss[0m : 2.76465
[1mStep[0m  [12/42], [94mLoss[0m : 2.62434
[1mStep[0m  [16/42], [94mLoss[0m : 2.73836
[1mStep[0m  [20/42], [94mLoss[0m : 2.65466
[1mStep[0m  [24/42], [94mLoss[0m : 2.45600
[1mStep[0m  [28/42], [94mLoss[0m : 2.64638
[1mStep[0m  [32/42], [94mLoss[0m : 2.63235
[1mStep[0m  [36/42], [94mLoss[0m : 2.54546
[1mStep[0m  [40/42], [94mLoss[0m : 2.52747

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.880, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63621
[1mStep[0m  [4/42], [94mLoss[0m : 2.51464
[1mStep[0m  [8/42], [94mLoss[0m : 2.75114
[1mStep[0m  [12/42], [94mLoss[0m : 2.61410
[1mStep[0m  [16/42], [94mLoss[0m : 2.41435
[1mStep[0m  [20/42], [94mLoss[0m : 2.80689
[1mStep[0m  [24/42], [94mLoss[0m : 2.70051
[1mStep[0m  [28/42], [94mLoss[0m : 2.70174
[1mStep[0m  [32/42], [94mLoss[0m : 2.54542
[1mStep[0m  [36/42], [94mLoss[0m : 2.71547
[1mStep[0m  [40/42], [94mLoss[0m : 2.72952

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.787, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56867
[1mStep[0m  [4/42], [94mLoss[0m : 2.78940
[1mStep[0m  [8/42], [94mLoss[0m : 2.56943
[1mStep[0m  [12/42], [94mLoss[0m : 2.84229
[1mStep[0m  [16/42], [94mLoss[0m : 2.68683
[1mStep[0m  [20/42], [94mLoss[0m : 2.79641
[1mStep[0m  [24/42], [94mLoss[0m : 3.02815
[1mStep[0m  [28/42], [94mLoss[0m : 2.57844
[1mStep[0m  [32/42], [94mLoss[0m : 2.83273
[1mStep[0m  [36/42], [94mLoss[0m : 2.60682
[1mStep[0m  [40/42], [94mLoss[0m : 2.51881

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.742, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72364
[1mStep[0m  [4/42], [94mLoss[0m : 2.69105
[1mStep[0m  [8/42], [94mLoss[0m : 2.46111
[1mStep[0m  [12/42], [94mLoss[0m : 2.63190
[1mStep[0m  [16/42], [94mLoss[0m : 2.39708
[1mStep[0m  [20/42], [94mLoss[0m : 2.55487
[1mStep[0m  [24/42], [94mLoss[0m : 2.42261
[1mStep[0m  [28/42], [94mLoss[0m : 2.55699
[1mStep[0m  [32/42], [94mLoss[0m : 2.83106
[1mStep[0m  [36/42], [94mLoss[0m : 2.60866
[1mStep[0m  [40/42], [94mLoss[0m : 2.68188

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.721, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52418
[1mStep[0m  [4/42], [94mLoss[0m : 2.72531
[1mStep[0m  [8/42], [94mLoss[0m : 2.66641
[1mStep[0m  [12/42], [94mLoss[0m : 2.55046
[1mStep[0m  [16/42], [94mLoss[0m : 2.78787
[1mStep[0m  [20/42], [94mLoss[0m : 2.68936
[1mStep[0m  [24/42], [94mLoss[0m : 2.50304
[1mStep[0m  [28/42], [94mLoss[0m : 2.55874
[1mStep[0m  [32/42], [94mLoss[0m : 2.67388
[1mStep[0m  [36/42], [94mLoss[0m : 2.55405
[1mStep[0m  [40/42], [94mLoss[0m : 2.76594

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.641, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62719
[1mStep[0m  [4/42], [94mLoss[0m : 2.65535
[1mStep[0m  [8/42], [94mLoss[0m : 2.60161
[1mStep[0m  [12/42], [94mLoss[0m : 2.74161
[1mStep[0m  [16/42], [94mLoss[0m : 2.59032
[1mStep[0m  [20/42], [94mLoss[0m : 2.45948
[1mStep[0m  [24/42], [94mLoss[0m : 2.41671
[1mStep[0m  [28/42], [94mLoss[0m : 2.66687
[1mStep[0m  [32/42], [94mLoss[0m : 2.34530
[1mStep[0m  [36/42], [94mLoss[0m : 2.51442
[1mStep[0m  [40/42], [94mLoss[0m : 2.65016

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.678, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58150
[1mStep[0m  [4/42], [94mLoss[0m : 2.49651
[1mStep[0m  [8/42], [94mLoss[0m : 2.70760
[1mStep[0m  [12/42], [94mLoss[0m : 2.71065
[1mStep[0m  [16/42], [94mLoss[0m : 2.44433
[1mStep[0m  [20/42], [94mLoss[0m : 2.73273
[1mStep[0m  [24/42], [94mLoss[0m : 2.50816
[1mStep[0m  [28/42], [94mLoss[0m : 2.40408
[1mStep[0m  [32/42], [94mLoss[0m : 2.84779
[1mStep[0m  [36/42], [94mLoss[0m : 2.48313
[1mStep[0m  [40/42], [94mLoss[0m : 2.49050

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.685, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46183
[1mStep[0m  [4/42], [94mLoss[0m : 2.49362
[1mStep[0m  [8/42], [94mLoss[0m : 2.82342
[1mStep[0m  [12/42], [94mLoss[0m : 2.72894
[1mStep[0m  [16/42], [94mLoss[0m : 2.62797
[1mStep[0m  [20/42], [94mLoss[0m : 2.49997
[1mStep[0m  [24/42], [94mLoss[0m : 2.51116
[1mStep[0m  [28/42], [94mLoss[0m : 3.03225
[1mStep[0m  [32/42], [94mLoss[0m : 2.69246
[1mStep[0m  [36/42], [94mLoss[0m : 2.64767
[1mStep[0m  [40/42], [94mLoss[0m : 2.48073

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.608, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47588
[1mStep[0m  [4/42], [94mLoss[0m : 2.57845
[1mStep[0m  [8/42], [94mLoss[0m : 2.64269
[1mStep[0m  [12/42], [94mLoss[0m : 2.72780
[1mStep[0m  [16/42], [94mLoss[0m : 2.57551
[1mStep[0m  [20/42], [94mLoss[0m : 2.62722
[1mStep[0m  [24/42], [94mLoss[0m : 2.43533
[1mStep[0m  [28/42], [94mLoss[0m : 2.74596
[1mStep[0m  [32/42], [94mLoss[0m : 2.66705
[1mStep[0m  [36/42], [94mLoss[0m : 2.58502
[1mStep[0m  [40/42], [94mLoss[0m : 2.71760

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.575, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55762
[1mStep[0m  [4/42], [94mLoss[0m : 2.51418
[1mStep[0m  [8/42], [94mLoss[0m : 2.50027
[1mStep[0m  [12/42], [94mLoss[0m : 2.61897
[1mStep[0m  [16/42], [94mLoss[0m : 2.65446
[1mStep[0m  [20/42], [94mLoss[0m : 2.53813
[1mStep[0m  [24/42], [94mLoss[0m : 2.59782
[1mStep[0m  [28/42], [94mLoss[0m : 2.39139
[1mStep[0m  [32/42], [94mLoss[0m : 2.68914
[1mStep[0m  [36/42], [94mLoss[0m : 2.61753
[1mStep[0m  [40/42], [94mLoss[0m : 2.63220

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.586, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78316
[1mStep[0m  [4/42], [94mLoss[0m : 2.44865
[1mStep[0m  [8/42], [94mLoss[0m : 2.84704
[1mStep[0m  [12/42], [94mLoss[0m : 2.56351
[1mStep[0m  [16/42], [94mLoss[0m : 2.41062
[1mStep[0m  [20/42], [94mLoss[0m : 2.69715
[1mStep[0m  [24/42], [94mLoss[0m : 2.43637
[1mStep[0m  [28/42], [94mLoss[0m : 2.48439
[1mStep[0m  [32/42], [94mLoss[0m : 2.62447
[1mStep[0m  [36/42], [94mLoss[0m : 2.60496
[1mStep[0m  [40/42], [94mLoss[0m : 2.64496

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.558, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72778
[1mStep[0m  [4/42], [94mLoss[0m : 2.63377
[1mStep[0m  [8/42], [94mLoss[0m : 2.67691
[1mStep[0m  [12/42], [94mLoss[0m : 2.59313
[1mStep[0m  [16/42], [94mLoss[0m : 2.58026
[1mStep[0m  [20/42], [94mLoss[0m : 2.66213
[1mStep[0m  [24/42], [94mLoss[0m : 2.59701
[1mStep[0m  [28/42], [94mLoss[0m : 2.62207
[1mStep[0m  [32/42], [94mLoss[0m : 2.55576
[1mStep[0m  [36/42], [94mLoss[0m : 2.45750
[1mStep[0m  [40/42], [94mLoss[0m : 2.62289

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.546, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40609
[1mStep[0m  [4/42], [94mLoss[0m : 2.43853
[1mStep[0m  [8/42], [94mLoss[0m : 2.67568
[1mStep[0m  [12/42], [94mLoss[0m : 2.57169
[1mStep[0m  [16/42], [94mLoss[0m : 2.68062
[1mStep[0m  [20/42], [94mLoss[0m : 2.51980
[1mStep[0m  [24/42], [94mLoss[0m : 2.48023
[1mStep[0m  [28/42], [94mLoss[0m : 2.59544
[1mStep[0m  [32/42], [94mLoss[0m : 2.59089
[1mStep[0m  [36/42], [94mLoss[0m : 2.71019
[1mStep[0m  [40/42], [94mLoss[0m : 2.34727

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.508, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60455
[1mStep[0m  [4/42], [94mLoss[0m : 2.36026
[1mStep[0m  [8/42], [94mLoss[0m : 2.71655
[1mStep[0m  [12/42], [94mLoss[0m : 2.58142
[1mStep[0m  [16/42], [94mLoss[0m : 2.71544
[1mStep[0m  [20/42], [94mLoss[0m : 2.33297
[1mStep[0m  [24/42], [94mLoss[0m : 2.57642
[1mStep[0m  [28/42], [94mLoss[0m : 2.82593
[1mStep[0m  [32/42], [94mLoss[0m : 2.45610
[1mStep[0m  [36/42], [94mLoss[0m : 2.38157
[1mStep[0m  [40/42], [94mLoss[0m : 2.74143

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54544
[1mStep[0m  [4/42], [94mLoss[0m : 2.44937
[1mStep[0m  [8/42], [94mLoss[0m : 2.50475
[1mStep[0m  [12/42], [94mLoss[0m : 2.63174
[1mStep[0m  [16/42], [94mLoss[0m : 2.41233
[1mStep[0m  [20/42], [94mLoss[0m : 2.56140
[1mStep[0m  [24/42], [94mLoss[0m : 2.58779
[1mStep[0m  [28/42], [94mLoss[0m : 2.36956
[1mStep[0m  [32/42], [94mLoss[0m : 2.49415
[1mStep[0m  [36/42], [94mLoss[0m : 2.60404
[1mStep[0m  [40/42], [94mLoss[0m : 2.34801

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38776
[1mStep[0m  [4/42], [94mLoss[0m : 2.40256
[1mStep[0m  [8/42], [94mLoss[0m : 2.61828
[1mStep[0m  [12/42], [94mLoss[0m : 2.37939
[1mStep[0m  [16/42], [94mLoss[0m : 2.48489
[1mStep[0m  [20/42], [94mLoss[0m : 2.46590
[1mStep[0m  [24/42], [94mLoss[0m : 2.60368
[1mStep[0m  [28/42], [94mLoss[0m : 2.41138
[1mStep[0m  [32/42], [94mLoss[0m : 2.50491
[1mStep[0m  [36/42], [94mLoss[0m : 2.50049
[1mStep[0m  [40/42], [94mLoss[0m : 2.63752

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55828
[1mStep[0m  [4/42], [94mLoss[0m : 2.53503
[1mStep[0m  [8/42], [94mLoss[0m : 2.63027
[1mStep[0m  [12/42], [94mLoss[0m : 2.56627
[1mStep[0m  [16/42], [94mLoss[0m : 2.43129
[1mStep[0m  [20/42], [94mLoss[0m : 2.43502
[1mStep[0m  [24/42], [94mLoss[0m : 2.68393
[1mStep[0m  [28/42], [94mLoss[0m : 2.45848
[1mStep[0m  [32/42], [94mLoss[0m : 2.39265
[1mStep[0m  [36/42], [94mLoss[0m : 2.36095
[1mStep[0m  [40/42], [94mLoss[0m : 2.57395

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62141
[1mStep[0m  [4/42], [94mLoss[0m : 2.53833
[1mStep[0m  [8/42], [94mLoss[0m : 2.39804
[1mStep[0m  [12/42], [94mLoss[0m : 2.58856
[1mStep[0m  [16/42], [94mLoss[0m : 2.59917
[1mStep[0m  [20/42], [94mLoss[0m : 2.70248
[1mStep[0m  [24/42], [94mLoss[0m : 2.56249
[1mStep[0m  [28/42], [94mLoss[0m : 2.46578
[1mStep[0m  [32/42], [94mLoss[0m : 2.47279
[1mStep[0m  [36/42], [94mLoss[0m : 2.46933
[1mStep[0m  [40/42], [94mLoss[0m : 2.69833

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46074
[1mStep[0m  [4/42], [94mLoss[0m : 2.71687
[1mStep[0m  [8/42], [94mLoss[0m : 2.72942
[1mStep[0m  [12/42], [94mLoss[0m : 2.53139
[1mStep[0m  [16/42], [94mLoss[0m : 2.46623
[1mStep[0m  [20/42], [94mLoss[0m : 2.56140
[1mStep[0m  [24/42], [94mLoss[0m : 2.56779
[1mStep[0m  [28/42], [94mLoss[0m : 2.52239
[1mStep[0m  [32/42], [94mLoss[0m : 2.50594
[1mStep[0m  [36/42], [94mLoss[0m : 2.46772
[1mStep[0m  [40/42], [94mLoss[0m : 2.04639

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47473
[1mStep[0m  [4/42], [94mLoss[0m : 2.67611
[1mStep[0m  [8/42], [94mLoss[0m : 2.62336
[1mStep[0m  [12/42], [94mLoss[0m : 2.59531
[1mStep[0m  [16/42], [94mLoss[0m : 2.45597
[1mStep[0m  [20/42], [94mLoss[0m : 2.71973
[1mStep[0m  [24/42], [94mLoss[0m : 2.74608
[1mStep[0m  [28/42], [94mLoss[0m : 2.48570
[1mStep[0m  [32/42], [94mLoss[0m : 2.41021
[1mStep[0m  [36/42], [94mLoss[0m : 2.60568
[1mStep[0m  [40/42], [94mLoss[0m : 2.39693

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72654
[1mStep[0m  [4/42], [94mLoss[0m : 2.32411
[1mStep[0m  [8/42], [94mLoss[0m : 2.41466
[1mStep[0m  [12/42], [94mLoss[0m : 2.46423
[1mStep[0m  [16/42], [94mLoss[0m : 2.53966
[1mStep[0m  [20/42], [94mLoss[0m : 2.39736
[1mStep[0m  [24/42], [94mLoss[0m : 2.55895
[1mStep[0m  [28/42], [94mLoss[0m : 2.55763
[1mStep[0m  [32/42], [94mLoss[0m : 2.56015
[1mStep[0m  [36/42], [94mLoss[0m : 2.50098
[1mStep[0m  [40/42], [94mLoss[0m : 2.46849

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48041
[1mStep[0m  [4/42], [94mLoss[0m : 2.34216
[1mStep[0m  [8/42], [94mLoss[0m : 2.39162
[1mStep[0m  [12/42], [94mLoss[0m : 2.69607
[1mStep[0m  [16/42], [94mLoss[0m : 2.48707
[1mStep[0m  [20/42], [94mLoss[0m : 2.56208
[1mStep[0m  [24/42], [94mLoss[0m : 2.43764
[1mStep[0m  [28/42], [94mLoss[0m : 2.69334
[1mStep[0m  [32/42], [94mLoss[0m : 2.45505
[1mStep[0m  [36/42], [94mLoss[0m : 2.33482
[1mStep[0m  [40/42], [94mLoss[0m : 2.45702

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44480
[1mStep[0m  [4/42], [94mLoss[0m : 2.37009
[1mStep[0m  [8/42], [94mLoss[0m : 2.59517
[1mStep[0m  [12/42], [94mLoss[0m : 2.56511
[1mStep[0m  [16/42], [94mLoss[0m : 2.41800
[1mStep[0m  [20/42], [94mLoss[0m : 2.54944
[1mStep[0m  [24/42], [94mLoss[0m : 2.84720
[1mStep[0m  [28/42], [94mLoss[0m : 2.42707
[1mStep[0m  [32/42], [94mLoss[0m : 2.69835
[1mStep[0m  [36/42], [94mLoss[0m : 2.41627
[1mStep[0m  [40/42], [94mLoss[0m : 2.43667

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67343
[1mStep[0m  [4/42], [94mLoss[0m : 2.47480
[1mStep[0m  [8/42], [94mLoss[0m : 2.34353
[1mStep[0m  [12/42], [94mLoss[0m : 2.53480
[1mStep[0m  [16/42], [94mLoss[0m : 2.17356
[1mStep[0m  [20/42], [94mLoss[0m : 2.37841
[1mStep[0m  [24/42], [94mLoss[0m : 2.56126
[1mStep[0m  [28/42], [94mLoss[0m : 2.45174
[1mStep[0m  [32/42], [94mLoss[0m : 2.60208
[1mStep[0m  [36/42], [94mLoss[0m : 2.55153
[1mStep[0m  [40/42], [94mLoss[0m : 2.49449

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.444
====================================

Phase 1 - Evaluation MAE:  2.44390036378588
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.69551
[1mStep[0m  [4/42], [94mLoss[0m : 2.30327
[1mStep[0m  [8/42], [94mLoss[0m : 2.58249
[1mStep[0m  [12/42], [94mLoss[0m : 2.51680
[1mStep[0m  [16/42], [94mLoss[0m : 2.80354
[1mStep[0m  [20/42], [94mLoss[0m : 2.49765
[1mStep[0m  [24/42], [94mLoss[0m : 2.45864
[1mStep[0m  [28/42], [94mLoss[0m : 2.57552
[1mStep[0m  [32/42], [94mLoss[0m : 2.46849
[1mStep[0m  [36/42], [94mLoss[0m : 2.42242
[1mStep[0m  [40/42], [94mLoss[0m : 2.71146

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43660
[1mStep[0m  [4/42], [94mLoss[0m : 2.48548
[1mStep[0m  [8/42], [94mLoss[0m : 2.51628
[1mStep[0m  [12/42], [94mLoss[0m : 2.49849
[1mStep[0m  [16/42], [94mLoss[0m : 2.50129
[1mStep[0m  [20/42], [94mLoss[0m : 2.48209
[1mStep[0m  [24/42], [94mLoss[0m : 2.58764
[1mStep[0m  [28/42], [94mLoss[0m : 2.33846
[1mStep[0m  [32/42], [94mLoss[0m : 2.61190
[1mStep[0m  [36/42], [94mLoss[0m : 2.64649
[1mStep[0m  [40/42], [94mLoss[0m : 2.80581

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65074
[1mStep[0m  [4/42], [94mLoss[0m : 2.48132
[1mStep[0m  [8/42], [94mLoss[0m : 2.98682
[1mStep[0m  [12/42], [94mLoss[0m : 2.61045
[1mStep[0m  [16/42], [94mLoss[0m : 2.57804
[1mStep[0m  [20/42], [94mLoss[0m : 2.41698
[1mStep[0m  [24/42], [94mLoss[0m : 2.39123
[1mStep[0m  [28/42], [94mLoss[0m : 2.45778
[1mStep[0m  [32/42], [94mLoss[0m : 2.62344
[1mStep[0m  [36/42], [94mLoss[0m : 2.53229
[1mStep[0m  [40/42], [94mLoss[0m : 2.46117

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34290
[1mStep[0m  [4/42], [94mLoss[0m : 2.65456
[1mStep[0m  [8/42], [94mLoss[0m : 2.58103
[1mStep[0m  [12/42], [94mLoss[0m : 2.55652
[1mStep[0m  [16/42], [94mLoss[0m : 2.23849
[1mStep[0m  [20/42], [94mLoss[0m : 2.31659
[1mStep[0m  [24/42], [94mLoss[0m : 2.70601
[1mStep[0m  [28/42], [94mLoss[0m : 2.58911
[1mStep[0m  [32/42], [94mLoss[0m : 2.48715
[1mStep[0m  [36/42], [94mLoss[0m : 2.84623
[1mStep[0m  [40/42], [94mLoss[0m : 2.65482

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43563
[1mStep[0m  [4/42], [94mLoss[0m : 2.45871
[1mStep[0m  [8/42], [94mLoss[0m : 2.47176
[1mStep[0m  [12/42], [94mLoss[0m : 2.34589
[1mStep[0m  [16/42], [94mLoss[0m : 2.65790
[1mStep[0m  [20/42], [94mLoss[0m : 2.53420
[1mStep[0m  [24/42], [94mLoss[0m : 2.70529
[1mStep[0m  [28/42], [94mLoss[0m : 2.30444
[1mStep[0m  [32/42], [94mLoss[0m : 2.56199
[1mStep[0m  [36/42], [94mLoss[0m : 2.64197
[1mStep[0m  [40/42], [94mLoss[0m : 2.26200

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38124
[1mStep[0m  [4/42], [94mLoss[0m : 2.58845
[1mStep[0m  [8/42], [94mLoss[0m : 2.53846
[1mStep[0m  [12/42], [94mLoss[0m : 2.43723
[1mStep[0m  [16/42], [94mLoss[0m : 2.52199
[1mStep[0m  [20/42], [94mLoss[0m : 2.34406
[1mStep[0m  [24/42], [94mLoss[0m : 2.41536
[1mStep[0m  [28/42], [94mLoss[0m : 2.39820
[1mStep[0m  [32/42], [94mLoss[0m : 2.73426
[1mStep[0m  [36/42], [94mLoss[0m : 2.62925
[1mStep[0m  [40/42], [94mLoss[0m : 2.34295

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33109
[1mStep[0m  [4/42], [94mLoss[0m : 2.48789
[1mStep[0m  [8/42], [94mLoss[0m : 2.33736
[1mStep[0m  [12/42], [94mLoss[0m : 2.43942
[1mStep[0m  [16/42], [94mLoss[0m : 2.62414
[1mStep[0m  [20/42], [94mLoss[0m : 2.67902
[1mStep[0m  [24/42], [94mLoss[0m : 2.69350
[1mStep[0m  [28/42], [94mLoss[0m : 2.56190
[1mStep[0m  [32/42], [94mLoss[0m : 2.43527
[1mStep[0m  [36/42], [94mLoss[0m : 2.46885
[1mStep[0m  [40/42], [94mLoss[0m : 2.58505

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30648
[1mStep[0m  [4/42], [94mLoss[0m : 2.69481
[1mStep[0m  [8/42], [94mLoss[0m : 2.32242
[1mStep[0m  [12/42], [94mLoss[0m : 2.40356
[1mStep[0m  [16/42], [94mLoss[0m : 2.45516
[1mStep[0m  [20/42], [94mLoss[0m : 2.28842
[1mStep[0m  [24/42], [94mLoss[0m : 2.62093
[1mStep[0m  [28/42], [94mLoss[0m : 2.64380
[1mStep[0m  [32/42], [94mLoss[0m : 2.55348
[1mStep[0m  [36/42], [94mLoss[0m : 2.22957
[1mStep[0m  [40/42], [94mLoss[0m : 2.42130

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56416
[1mStep[0m  [4/42], [94mLoss[0m : 2.26272
[1mStep[0m  [8/42], [94mLoss[0m : 2.50372
[1mStep[0m  [12/42], [94mLoss[0m : 2.63057
[1mStep[0m  [16/42], [94mLoss[0m : 2.36897
[1mStep[0m  [20/42], [94mLoss[0m : 2.45268
[1mStep[0m  [24/42], [94mLoss[0m : 2.58211
[1mStep[0m  [28/42], [94mLoss[0m : 2.50099
[1mStep[0m  [32/42], [94mLoss[0m : 2.62462
[1mStep[0m  [36/42], [94mLoss[0m : 2.64527
[1mStep[0m  [40/42], [94mLoss[0m : 2.47430

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39745
[1mStep[0m  [4/42], [94mLoss[0m : 2.40970
[1mStep[0m  [8/42], [94mLoss[0m : 2.43455
[1mStep[0m  [12/42], [94mLoss[0m : 2.67464
[1mStep[0m  [16/42], [94mLoss[0m : 2.43473
[1mStep[0m  [20/42], [94mLoss[0m : 2.31749
[1mStep[0m  [24/42], [94mLoss[0m : 2.49667
[1mStep[0m  [28/42], [94mLoss[0m : 2.25690
[1mStep[0m  [32/42], [94mLoss[0m : 2.55880
[1mStep[0m  [36/42], [94mLoss[0m : 2.35800
[1mStep[0m  [40/42], [94mLoss[0m : 2.32908

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34700
[1mStep[0m  [4/42], [94mLoss[0m : 2.42685
[1mStep[0m  [8/42], [94mLoss[0m : 2.59477
[1mStep[0m  [12/42], [94mLoss[0m : 2.32538
[1mStep[0m  [16/42], [94mLoss[0m : 2.08605
[1mStep[0m  [20/42], [94mLoss[0m : 2.35224
[1mStep[0m  [24/42], [94mLoss[0m : 2.36877
[1mStep[0m  [28/42], [94mLoss[0m : 2.48787
[1mStep[0m  [32/42], [94mLoss[0m : 2.31705
[1mStep[0m  [36/42], [94mLoss[0m : 2.31778
[1mStep[0m  [40/42], [94mLoss[0m : 2.33148

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61158
[1mStep[0m  [4/42], [94mLoss[0m : 2.43895
[1mStep[0m  [8/42], [94mLoss[0m : 2.41715
[1mStep[0m  [12/42], [94mLoss[0m : 2.37229
[1mStep[0m  [16/42], [94mLoss[0m : 2.44478
[1mStep[0m  [20/42], [94mLoss[0m : 2.12153
[1mStep[0m  [24/42], [94mLoss[0m : 2.38288
[1mStep[0m  [28/42], [94mLoss[0m : 2.59585
[1mStep[0m  [32/42], [94mLoss[0m : 2.43764
[1mStep[0m  [36/42], [94mLoss[0m : 2.43391
[1mStep[0m  [40/42], [94mLoss[0m : 2.36019

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25692
[1mStep[0m  [4/42], [94mLoss[0m : 2.34808
[1mStep[0m  [8/42], [94mLoss[0m : 2.32022
[1mStep[0m  [12/42], [94mLoss[0m : 2.42345
[1mStep[0m  [16/42], [94mLoss[0m : 2.48667
[1mStep[0m  [20/42], [94mLoss[0m : 2.37474
[1mStep[0m  [24/42], [94mLoss[0m : 2.13115
[1mStep[0m  [28/42], [94mLoss[0m : 2.26071
[1mStep[0m  [32/42], [94mLoss[0m : 2.54159
[1mStep[0m  [36/42], [94mLoss[0m : 2.44917
[1mStep[0m  [40/42], [94mLoss[0m : 2.21738

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13796
[1mStep[0m  [4/42], [94mLoss[0m : 2.34597
[1mStep[0m  [8/42], [94mLoss[0m : 2.33696
[1mStep[0m  [12/42], [94mLoss[0m : 2.34229
[1mStep[0m  [16/42], [94mLoss[0m : 2.57301
[1mStep[0m  [20/42], [94mLoss[0m : 2.20760
[1mStep[0m  [24/42], [94mLoss[0m : 2.20663
[1mStep[0m  [28/42], [94mLoss[0m : 2.58629
[1mStep[0m  [32/42], [94mLoss[0m : 2.52878
[1mStep[0m  [36/42], [94mLoss[0m : 2.31474
[1mStep[0m  [40/42], [94mLoss[0m : 2.37140

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.548, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.16430
[1mStep[0m  [4/42], [94mLoss[0m : 2.22693
[1mStep[0m  [8/42], [94mLoss[0m : 2.25964
[1mStep[0m  [12/42], [94mLoss[0m : 2.30335
[1mStep[0m  [16/42], [94mLoss[0m : 2.35954
[1mStep[0m  [20/42], [94mLoss[0m : 2.37094
[1mStep[0m  [24/42], [94mLoss[0m : 2.32836
[1mStep[0m  [28/42], [94mLoss[0m : 2.35396
[1mStep[0m  [32/42], [94mLoss[0m : 2.08789
[1mStep[0m  [36/42], [94mLoss[0m : 2.42663
[1mStep[0m  [40/42], [94mLoss[0m : 2.56075

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29153
[1mStep[0m  [4/42], [94mLoss[0m : 2.24872
[1mStep[0m  [8/42], [94mLoss[0m : 2.44332
[1mStep[0m  [12/42], [94mLoss[0m : 2.50909
[1mStep[0m  [16/42], [94mLoss[0m : 2.10729
[1mStep[0m  [20/42], [94mLoss[0m : 2.32355
[1mStep[0m  [24/42], [94mLoss[0m : 2.32434
[1mStep[0m  [28/42], [94mLoss[0m : 2.19227
[1mStep[0m  [32/42], [94mLoss[0m : 2.21812
[1mStep[0m  [36/42], [94mLoss[0m : 2.32721
[1mStep[0m  [40/42], [94mLoss[0m : 2.34141

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.553, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24779
[1mStep[0m  [4/42], [94mLoss[0m : 2.29040
[1mStep[0m  [8/42], [94mLoss[0m : 2.23574
[1mStep[0m  [12/42], [94mLoss[0m : 2.08940
[1mStep[0m  [16/42], [94mLoss[0m : 2.33411
[1mStep[0m  [20/42], [94mLoss[0m : 2.16353
[1mStep[0m  [24/42], [94mLoss[0m : 2.33189
[1mStep[0m  [28/42], [94mLoss[0m : 2.14870
[1mStep[0m  [32/42], [94mLoss[0m : 2.31415
[1mStep[0m  [36/42], [94mLoss[0m : 2.38322
[1mStep[0m  [40/42], [94mLoss[0m : 2.18685

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20984
[1mStep[0m  [4/42], [94mLoss[0m : 2.22243
[1mStep[0m  [8/42], [94mLoss[0m : 2.24660
[1mStep[0m  [12/42], [94mLoss[0m : 2.33637
[1mStep[0m  [16/42], [94mLoss[0m : 2.29459
[1mStep[0m  [20/42], [94mLoss[0m : 2.59549
[1mStep[0m  [24/42], [94mLoss[0m : 2.15361
[1mStep[0m  [28/42], [94mLoss[0m : 2.20247
[1mStep[0m  [32/42], [94mLoss[0m : 2.17399
[1mStep[0m  [36/42], [94mLoss[0m : 2.42998
[1mStep[0m  [40/42], [94mLoss[0m : 2.34881

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.567, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31723
[1mStep[0m  [4/42], [94mLoss[0m : 2.31443
[1mStep[0m  [8/42], [94mLoss[0m : 2.14173
[1mStep[0m  [12/42], [94mLoss[0m : 2.17076
[1mStep[0m  [16/42], [94mLoss[0m : 2.34887
[1mStep[0m  [20/42], [94mLoss[0m : 2.06732
[1mStep[0m  [24/42], [94mLoss[0m : 2.02992
[1mStep[0m  [28/42], [94mLoss[0m : 2.21864
[1mStep[0m  [32/42], [94mLoss[0m : 2.45458
[1mStep[0m  [36/42], [94mLoss[0m : 2.18196
[1mStep[0m  [40/42], [94mLoss[0m : 2.31708

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.583, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36166
[1mStep[0m  [4/42], [94mLoss[0m : 2.26360
[1mStep[0m  [8/42], [94mLoss[0m : 2.26796
[1mStep[0m  [12/42], [94mLoss[0m : 2.28500
[1mStep[0m  [16/42], [94mLoss[0m : 2.34651
[1mStep[0m  [20/42], [94mLoss[0m : 2.39585
[1mStep[0m  [24/42], [94mLoss[0m : 2.28541
[1mStep[0m  [28/42], [94mLoss[0m : 2.05210
[1mStep[0m  [32/42], [94mLoss[0m : 2.07220
[1mStep[0m  [36/42], [94mLoss[0m : 2.13959
[1mStep[0m  [40/42], [94mLoss[0m : 2.16452

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37235
[1mStep[0m  [4/42], [94mLoss[0m : 2.08440
[1mStep[0m  [8/42], [94mLoss[0m : 1.99051
[1mStep[0m  [12/42], [94mLoss[0m : 2.19844
[1mStep[0m  [16/42], [94mLoss[0m : 2.26132
[1mStep[0m  [20/42], [94mLoss[0m : 2.21521
[1mStep[0m  [24/42], [94mLoss[0m : 2.01974
[1mStep[0m  [28/42], [94mLoss[0m : 2.11527
[1mStep[0m  [32/42], [94mLoss[0m : 2.27758
[1mStep[0m  [36/42], [94mLoss[0m : 2.27177
[1mStep[0m  [40/42], [94mLoss[0m : 2.17773

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.573, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03878
[1mStep[0m  [4/42], [94mLoss[0m : 1.95604
[1mStep[0m  [8/42], [94mLoss[0m : 2.25919
[1mStep[0m  [12/42], [94mLoss[0m : 2.11647
[1mStep[0m  [16/42], [94mLoss[0m : 2.13185
[1mStep[0m  [20/42], [94mLoss[0m : 1.94995
[1mStep[0m  [24/42], [94mLoss[0m : 2.26390
[1mStep[0m  [28/42], [94mLoss[0m : 2.22735
[1mStep[0m  [32/42], [94mLoss[0m : 2.03564
[1mStep[0m  [36/42], [94mLoss[0m : 2.06359
[1mStep[0m  [40/42], [94mLoss[0m : 1.99428

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.554, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10871
[1mStep[0m  [4/42], [94mLoss[0m : 2.18761
[1mStep[0m  [8/42], [94mLoss[0m : 2.24813
[1mStep[0m  [12/42], [94mLoss[0m : 1.91155
[1mStep[0m  [16/42], [94mLoss[0m : 2.18456
[1mStep[0m  [20/42], [94mLoss[0m : 2.09472
[1mStep[0m  [24/42], [94mLoss[0m : 2.42383
[1mStep[0m  [28/42], [94mLoss[0m : 2.35572
[1mStep[0m  [32/42], [94mLoss[0m : 2.21987
[1mStep[0m  [36/42], [94mLoss[0m : 2.28421
[1mStep[0m  [40/42], [94mLoss[0m : 2.04501

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.165, [92mTest[0m: 2.589, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95730
[1mStep[0m  [4/42], [94mLoss[0m : 2.28053
[1mStep[0m  [8/42], [94mLoss[0m : 2.16265
[1mStep[0m  [12/42], [94mLoss[0m : 2.08042
[1mStep[0m  [16/42], [94mLoss[0m : 2.09942
[1mStep[0m  [20/42], [94mLoss[0m : 2.18690
[1mStep[0m  [24/42], [94mLoss[0m : 2.27549
[1mStep[0m  [28/42], [94mLoss[0m : 2.01191
[1mStep[0m  [32/42], [94mLoss[0m : 2.16467
[1mStep[0m  [36/42], [94mLoss[0m : 2.17610
[1mStep[0m  [40/42], [94mLoss[0m : 1.95951

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.604, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10626
[1mStep[0m  [4/42], [94mLoss[0m : 2.01093
[1mStep[0m  [8/42], [94mLoss[0m : 2.14820
[1mStep[0m  [12/42], [94mLoss[0m : 2.24656
[1mStep[0m  [16/42], [94mLoss[0m : 2.08684
[1mStep[0m  [20/42], [94mLoss[0m : 2.05265
[1mStep[0m  [24/42], [94mLoss[0m : 2.14214
[1mStep[0m  [28/42], [94mLoss[0m : 2.10035
[1mStep[0m  [32/42], [94mLoss[0m : 2.12599
[1mStep[0m  [36/42], [94mLoss[0m : 2.22155
[1mStep[0m  [40/42], [94mLoss[0m : 2.14127

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.569, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03541
[1mStep[0m  [4/42], [94mLoss[0m : 2.09117
[1mStep[0m  [8/42], [94mLoss[0m : 2.06830
[1mStep[0m  [12/42], [94mLoss[0m : 2.24336
[1mStep[0m  [16/42], [94mLoss[0m : 2.07291
[1mStep[0m  [20/42], [94mLoss[0m : 2.05159
[1mStep[0m  [24/42], [94mLoss[0m : 2.10004
[1mStep[0m  [28/42], [94mLoss[0m : 2.30322
[1mStep[0m  [32/42], [94mLoss[0m : 2.14655
[1mStep[0m  [36/42], [94mLoss[0m : 2.24274
[1mStep[0m  [40/42], [94mLoss[0m : 1.96380

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.589, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99970
[1mStep[0m  [4/42], [94mLoss[0m : 2.06326
[1mStep[0m  [8/42], [94mLoss[0m : 2.20144
[1mStep[0m  [12/42], [94mLoss[0m : 2.16830
[1mStep[0m  [16/42], [94mLoss[0m : 2.11705
[1mStep[0m  [20/42], [94mLoss[0m : 2.28323
[1mStep[0m  [24/42], [94mLoss[0m : 2.17008
[1mStep[0m  [28/42], [94mLoss[0m : 2.06885
[1mStep[0m  [32/42], [94mLoss[0m : 2.01780
[1mStep[0m  [36/42], [94mLoss[0m : 2.10751
[1mStep[0m  [40/42], [94mLoss[0m : 2.07953

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06282
[1mStep[0m  [4/42], [94mLoss[0m : 2.02322
[1mStep[0m  [8/42], [94mLoss[0m : 2.03210
[1mStep[0m  [12/42], [94mLoss[0m : 2.01863
[1mStep[0m  [16/42], [94mLoss[0m : 1.89409
[1mStep[0m  [20/42], [94mLoss[0m : 2.14537
[1mStep[0m  [24/42], [94mLoss[0m : 2.20814
[1mStep[0m  [28/42], [94mLoss[0m : 2.09760
[1mStep[0m  [32/42], [94mLoss[0m : 2.19366
[1mStep[0m  [36/42], [94mLoss[0m : 2.10598
[1mStep[0m  [40/42], [94mLoss[0m : 2.20587

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.059, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78593
[1mStep[0m  [4/42], [94mLoss[0m : 1.94601
[1mStep[0m  [8/42], [94mLoss[0m : 2.01939
[1mStep[0m  [12/42], [94mLoss[0m : 2.10571
[1mStep[0m  [16/42], [94mLoss[0m : 1.83252
[1mStep[0m  [20/42], [94mLoss[0m : 2.02913
[1mStep[0m  [24/42], [94mLoss[0m : 2.08242
[1mStep[0m  [28/42], [94mLoss[0m : 1.85236
[1mStep[0m  [32/42], [94mLoss[0m : 2.03457
[1mStep[0m  [36/42], [94mLoss[0m : 2.18696
[1mStep[0m  [40/42], [94mLoss[0m : 1.79562

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00632
[1mStep[0m  [4/42], [94mLoss[0m : 1.93341
[1mStep[0m  [8/42], [94mLoss[0m : 1.88453
[1mStep[0m  [12/42], [94mLoss[0m : 1.95856
[1mStep[0m  [16/42], [94mLoss[0m : 1.93855
[1mStep[0m  [20/42], [94mLoss[0m : 1.99792
[1mStep[0m  [24/42], [94mLoss[0m : 2.00884
[1mStep[0m  [28/42], [94mLoss[0m : 1.88210
[1mStep[0m  [32/42], [94mLoss[0m : 1.96105
[1mStep[0m  [36/42], [94mLoss[0m : 1.91411
[1mStep[0m  [40/42], [94mLoss[0m : 1.90512

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.649, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.579
====================================

Phase 2 - Evaluation MAE:  2.5794354677200317
MAE score P1          2.4439
MAE score P2        2.579435
loss                1.991197
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay           0.001
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.43032
[1mStep[0m  [8/84], [94mLoss[0m : 9.73514
[1mStep[0m  [16/84], [94mLoss[0m : 5.69046
[1mStep[0m  [24/84], [94mLoss[0m : 2.89740
[1mStep[0m  [32/84], [94mLoss[0m : 3.05324
[1mStep[0m  [40/84], [94mLoss[0m : 2.49178
[1mStep[0m  [48/84], [94mLoss[0m : 2.44905
[1mStep[0m  [56/84], [94mLoss[0m : 2.66431
[1mStep[0m  [64/84], [94mLoss[0m : 2.31910
[1mStep[0m  [72/84], [94mLoss[0m : 2.32686
[1mStep[0m  [80/84], [94mLoss[0m : 2.53357

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.987, [92mTest[0m: 11.205, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37289
[1mStep[0m  [8/84], [94mLoss[0m : 2.54607
[1mStep[0m  [16/84], [94mLoss[0m : 2.49095
[1mStep[0m  [24/84], [94mLoss[0m : 2.43938
[1mStep[0m  [32/84], [94mLoss[0m : 2.27419
[1mStep[0m  [40/84], [94mLoss[0m : 2.78553
[1mStep[0m  [48/84], [94mLoss[0m : 2.42343
[1mStep[0m  [56/84], [94mLoss[0m : 2.56888
[1mStep[0m  [64/84], [94mLoss[0m : 2.22647
[1mStep[0m  [72/84], [94mLoss[0m : 2.17583
[1mStep[0m  [80/84], [94mLoss[0m : 2.41419

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47012
[1mStep[0m  [8/84], [94mLoss[0m : 2.59164
[1mStep[0m  [16/84], [94mLoss[0m : 2.40986
[1mStep[0m  [24/84], [94mLoss[0m : 2.30625
[1mStep[0m  [32/84], [94mLoss[0m : 2.59499
[1mStep[0m  [40/84], [94mLoss[0m : 2.69354
[1mStep[0m  [48/84], [94mLoss[0m : 2.34341
[1mStep[0m  [56/84], [94mLoss[0m : 2.67019
[1mStep[0m  [64/84], [94mLoss[0m : 2.86288
[1mStep[0m  [72/84], [94mLoss[0m : 2.46827
[1mStep[0m  [80/84], [94mLoss[0m : 2.53303

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41883
[1mStep[0m  [8/84], [94mLoss[0m : 2.81739
[1mStep[0m  [16/84], [94mLoss[0m : 2.64043
[1mStep[0m  [24/84], [94mLoss[0m : 2.76614
[1mStep[0m  [32/84], [94mLoss[0m : 2.39041
[1mStep[0m  [40/84], [94mLoss[0m : 2.55057
[1mStep[0m  [48/84], [94mLoss[0m : 2.18949
[1mStep[0m  [56/84], [94mLoss[0m : 2.37223
[1mStep[0m  [64/84], [94mLoss[0m : 2.70794
[1mStep[0m  [72/84], [94mLoss[0m : 2.42730
[1mStep[0m  [80/84], [94mLoss[0m : 2.31523

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49332
[1mStep[0m  [8/84], [94mLoss[0m : 2.53378
[1mStep[0m  [16/84], [94mLoss[0m : 2.52175
[1mStep[0m  [24/84], [94mLoss[0m : 2.38258
[1mStep[0m  [32/84], [94mLoss[0m : 2.35574
[1mStep[0m  [40/84], [94mLoss[0m : 2.48696
[1mStep[0m  [48/84], [94mLoss[0m : 2.71494
[1mStep[0m  [56/84], [94mLoss[0m : 2.35487
[1mStep[0m  [64/84], [94mLoss[0m : 2.57399
[1mStep[0m  [72/84], [94mLoss[0m : 2.56449
[1mStep[0m  [80/84], [94mLoss[0m : 2.33336

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36447
[1mStep[0m  [8/84], [94mLoss[0m : 2.50045
[1mStep[0m  [16/84], [94mLoss[0m : 2.60922
[1mStep[0m  [24/84], [94mLoss[0m : 2.56715
[1mStep[0m  [32/84], [94mLoss[0m : 2.23030
[1mStep[0m  [40/84], [94mLoss[0m : 2.21435
[1mStep[0m  [48/84], [94mLoss[0m : 2.79240
[1mStep[0m  [56/84], [94mLoss[0m : 2.43199
[1mStep[0m  [64/84], [94mLoss[0m : 2.66270
[1mStep[0m  [72/84], [94mLoss[0m : 2.31393
[1mStep[0m  [80/84], [94mLoss[0m : 2.54114

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27629
[1mStep[0m  [8/84], [94mLoss[0m : 2.39634
[1mStep[0m  [16/84], [94mLoss[0m : 2.41450
[1mStep[0m  [24/84], [94mLoss[0m : 2.54635
[1mStep[0m  [32/84], [94mLoss[0m : 2.50909
[1mStep[0m  [40/84], [94mLoss[0m : 2.52559
[1mStep[0m  [48/84], [94mLoss[0m : 2.41031
[1mStep[0m  [56/84], [94mLoss[0m : 2.40040
[1mStep[0m  [64/84], [94mLoss[0m : 2.74906
[1mStep[0m  [72/84], [94mLoss[0m : 2.32837
[1mStep[0m  [80/84], [94mLoss[0m : 2.26771

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59582
[1mStep[0m  [8/84], [94mLoss[0m : 2.27218
[1mStep[0m  [16/84], [94mLoss[0m : 2.55464
[1mStep[0m  [24/84], [94mLoss[0m : 2.26670
[1mStep[0m  [32/84], [94mLoss[0m : 2.65005
[1mStep[0m  [40/84], [94mLoss[0m : 2.72399
[1mStep[0m  [48/84], [94mLoss[0m : 2.55109
[1mStep[0m  [56/84], [94mLoss[0m : 2.57399
[1mStep[0m  [64/84], [94mLoss[0m : 2.19869
[1mStep[0m  [72/84], [94mLoss[0m : 2.39651
[1mStep[0m  [80/84], [94mLoss[0m : 2.52439

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14249
[1mStep[0m  [8/84], [94mLoss[0m : 2.53146
[1mStep[0m  [16/84], [94mLoss[0m : 2.50144
[1mStep[0m  [24/84], [94mLoss[0m : 2.56861
[1mStep[0m  [32/84], [94mLoss[0m : 2.52954
[1mStep[0m  [40/84], [94mLoss[0m : 2.45389
[1mStep[0m  [48/84], [94mLoss[0m : 2.45620
[1mStep[0m  [56/84], [94mLoss[0m : 2.43887
[1mStep[0m  [64/84], [94mLoss[0m : 2.66513
[1mStep[0m  [72/84], [94mLoss[0m : 2.34412
[1mStep[0m  [80/84], [94mLoss[0m : 2.35205

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48923
[1mStep[0m  [8/84], [94mLoss[0m : 2.30547
[1mStep[0m  [16/84], [94mLoss[0m : 2.32720
[1mStep[0m  [24/84], [94mLoss[0m : 2.51382
[1mStep[0m  [32/84], [94mLoss[0m : 2.64246
[1mStep[0m  [40/84], [94mLoss[0m : 2.75505
[1mStep[0m  [48/84], [94mLoss[0m : 2.71324
[1mStep[0m  [56/84], [94mLoss[0m : 2.65186
[1mStep[0m  [64/84], [94mLoss[0m : 2.22717
[1mStep[0m  [72/84], [94mLoss[0m : 2.44019
[1mStep[0m  [80/84], [94mLoss[0m : 2.52398

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32933
[1mStep[0m  [8/84], [94mLoss[0m : 2.40418
[1mStep[0m  [16/84], [94mLoss[0m : 2.39631
[1mStep[0m  [24/84], [94mLoss[0m : 2.36601
[1mStep[0m  [32/84], [94mLoss[0m : 2.54061
[1mStep[0m  [40/84], [94mLoss[0m : 2.53168
[1mStep[0m  [48/84], [94mLoss[0m : 2.51661
[1mStep[0m  [56/84], [94mLoss[0m : 2.53599
[1mStep[0m  [64/84], [94mLoss[0m : 2.41871
[1mStep[0m  [72/84], [94mLoss[0m : 2.33043
[1mStep[0m  [80/84], [94mLoss[0m : 2.30082

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40258
[1mStep[0m  [8/84], [94mLoss[0m : 2.61153
[1mStep[0m  [16/84], [94mLoss[0m : 2.44924
[1mStep[0m  [24/84], [94mLoss[0m : 2.33567
[1mStep[0m  [32/84], [94mLoss[0m : 2.27021
[1mStep[0m  [40/84], [94mLoss[0m : 2.48958
[1mStep[0m  [48/84], [94mLoss[0m : 2.34718
[1mStep[0m  [56/84], [94mLoss[0m : 2.58677
[1mStep[0m  [64/84], [94mLoss[0m : 2.58103
[1mStep[0m  [72/84], [94mLoss[0m : 2.39290
[1mStep[0m  [80/84], [94mLoss[0m : 2.43411

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81039
[1mStep[0m  [8/84], [94mLoss[0m : 2.62417
[1mStep[0m  [16/84], [94mLoss[0m : 2.55176
[1mStep[0m  [24/84], [94mLoss[0m : 2.39566
[1mStep[0m  [32/84], [94mLoss[0m : 2.17191
[1mStep[0m  [40/84], [94mLoss[0m : 2.53155
[1mStep[0m  [48/84], [94mLoss[0m : 2.24231
[1mStep[0m  [56/84], [94mLoss[0m : 2.41980
[1mStep[0m  [64/84], [94mLoss[0m : 2.33506
[1mStep[0m  [72/84], [94mLoss[0m : 2.38175
[1mStep[0m  [80/84], [94mLoss[0m : 2.08682

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35710
[1mStep[0m  [8/84], [94mLoss[0m : 2.46276
[1mStep[0m  [16/84], [94mLoss[0m : 2.49682
[1mStep[0m  [24/84], [94mLoss[0m : 2.52780
[1mStep[0m  [32/84], [94mLoss[0m : 2.36647
[1mStep[0m  [40/84], [94mLoss[0m : 2.37403
[1mStep[0m  [48/84], [94mLoss[0m : 2.59317
[1mStep[0m  [56/84], [94mLoss[0m : 2.40156
[1mStep[0m  [64/84], [94mLoss[0m : 2.38063
[1mStep[0m  [72/84], [94mLoss[0m : 2.04884
[1mStep[0m  [80/84], [94mLoss[0m : 2.67783

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68463
[1mStep[0m  [8/84], [94mLoss[0m : 2.63203
[1mStep[0m  [16/84], [94mLoss[0m : 2.43425
[1mStep[0m  [24/84], [94mLoss[0m : 2.41824
[1mStep[0m  [32/84], [94mLoss[0m : 2.24568
[1mStep[0m  [40/84], [94mLoss[0m : 2.23210
[1mStep[0m  [48/84], [94mLoss[0m : 2.60686
[1mStep[0m  [56/84], [94mLoss[0m : 2.41586
[1mStep[0m  [64/84], [94mLoss[0m : 2.65974
[1mStep[0m  [72/84], [94mLoss[0m : 2.43979
[1mStep[0m  [80/84], [94mLoss[0m : 2.45754

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12613
[1mStep[0m  [8/84], [94mLoss[0m : 2.42896
[1mStep[0m  [16/84], [94mLoss[0m : 2.59355
[1mStep[0m  [24/84], [94mLoss[0m : 2.50724
[1mStep[0m  [32/84], [94mLoss[0m : 2.23272
[1mStep[0m  [40/84], [94mLoss[0m : 2.53488
[1mStep[0m  [48/84], [94mLoss[0m : 2.51767
[1mStep[0m  [56/84], [94mLoss[0m : 2.40478
[1mStep[0m  [64/84], [94mLoss[0m : 2.75499
[1mStep[0m  [72/84], [94mLoss[0m : 2.32694
[1mStep[0m  [80/84], [94mLoss[0m : 2.37107

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50206
[1mStep[0m  [8/84], [94mLoss[0m : 2.51432
[1mStep[0m  [16/84], [94mLoss[0m : 2.63735
[1mStep[0m  [24/84], [94mLoss[0m : 2.18952
[1mStep[0m  [32/84], [94mLoss[0m : 2.48712
[1mStep[0m  [40/84], [94mLoss[0m : 2.68160
[1mStep[0m  [48/84], [94mLoss[0m : 2.48744
[1mStep[0m  [56/84], [94mLoss[0m : 2.62721
[1mStep[0m  [64/84], [94mLoss[0m : 2.21713
[1mStep[0m  [72/84], [94mLoss[0m : 2.38752
[1mStep[0m  [80/84], [94mLoss[0m : 2.63220

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57461
[1mStep[0m  [8/84], [94mLoss[0m : 2.37790
[1mStep[0m  [16/84], [94mLoss[0m : 2.84367
[1mStep[0m  [24/84], [94mLoss[0m : 2.36708
[1mStep[0m  [32/84], [94mLoss[0m : 2.36227
[1mStep[0m  [40/84], [94mLoss[0m : 2.41178
[1mStep[0m  [48/84], [94mLoss[0m : 2.63564
[1mStep[0m  [56/84], [94mLoss[0m : 2.49316
[1mStep[0m  [64/84], [94mLoss[0m : 2.60582
[1mStep[0m  [72/84], [94mLoss[0m : 2.41268
[1mStep[0m  [80/84], [94mLoss[0m : 2.30774

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50004
[1mStep[0m  [8/84], [94mLoss[0m : 2.49196
[1mStep[0m  [16/84], [94mLoss[0m : 2.59965
[1mStep[0m  [24/84], [94mLoss[0m : 2.57793
[1mStep[0m  [32/84], [94mLoss[0m : 2.80907
[1mStep[0m  [40/84], [94mLoss[0m : 2.52693
[1mStep[0m  [48/84], [94mLoss[0m : 2.52996
[1mStep[0m  [56/84], [94mLoss[0m : 2.53549
[1mStep[0m  [64/84], [94mLoss[0m : 2.75810
[1mStep[0m  [72/84], [94mLoss[0m : 2.27630
[1mStep[0m  [80/84], [94mLoss[0m : 2.41775

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43017
[1mStep[0m  [8/84], [94mLoss[0m : 2.40198
[1mStep[0m  [16/84], [94mLoss[0m : 2.46914
[1mStep[0m  [24/84], [94mLoss[0m : 2.47832
[1mStep[0m  [32/84], [94mLoss[0m : 2.28248
[1mStep[0m  [40/84], [94mLoss[0m : 2.67770
[1mStep[0m  [48/84], [94mLoss[0m : 2.54509
[1mStep[0m  [56/84], [94mLoss[0m : 2.58070
[1mStep[0m  [64/84], [94mLoss[0m : 2.49031
[1mStep[0m  [72/84], [94mLoss[0m : 2.28875
[1mStep[0m  [80/84], [94mLoss[0m : 2.49180

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60865
[1mStep[0m  [8/84], [94mLoss[0m : 2.19685
[1mStep[0m  [16/84], [94mLoss[0m : 2.76085
[1mStep[0m  [24/84], [94mLoss[0m : 2.31451
[1mStep[0m  [32/84], [94mLoss[0m : 3.00194
[1mStep[0m  [40/84], [94mLoss[0m : 2.89722
[1mStep[0m  [48/84], [94mLoss[0m : 2.18643
[1mStep[0m  [56/84], [94mLoss[0m : 2.46713
[1mStep[0m  [64/84], [94mLoss[0m : 2.65319
[1mStep[0m  [72/84], [94mLoss[0m : 2.71498
[1mStep[0m  [80/84], [94mLoss[0m : 2.43875

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59714
[1mStep[0m  [8/84], [94mLoss[0m : 2.57176
[1mStep[0m  [16/84], [94mLoss[0m : 2.33362
[1mStep[0m  [24/84], [94mLoss[0m : 2.51390
[1mStep[0m  [32/84], [94mLoss[0m : 2.37247
[1mStep[0m  [40/84], [94mLoss[0m : 2.69531
[1mStep[0m  [48/84], [94mLoss[0m : 2.37830
[1mStep[0m  [56/84], [94mLoss[0m : 2.71537
[1mStep[0m  [64/84], [94mLoss[0m : 2.78425
[1mStep[0m  [72/84], [94mLoss[0m : 2.16972
[1mStep[0m  [80/84], [94mLoss[0m : 2.42727

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53897
[1mStep[0m  [8/84], [94mLoss[0m : 2.13255
[1mStep[0m  [16/84], [94mLoss[0m : 2.83043
[1mStep[0m  [24/84], [94mLoss[0m : 2.56983
[1mStep[0m  [32/84], [94mLoss[0m : 2.33490
[1mStep[0m  [40/84], [94mLoss[0m : 2.42440
[1mStep[0m  [48/84], [94mLoss[0m : 2.47301
[1mStep[0m  [56/84], [94mLoss[0m : 2.53794
[1mStep[0m  [64/84], [94mLoss[0m : 2.29743
[1mStep[0m  [72/84], [94mLoss[0m : 2.42674
[1mStep[0m  [80/84], [94mLoss[0m : 2.73523

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11360
[1mStep[0m  [8/84], [94mLoss[0m : 2.58298
[1mStep[0m  [16/84], [94mLoss[0m : 2.67554
[1mStep[0m  [24/84], [94mLoss[0m : 2.39438
[1mStep[0m  [32/84], [94mLoss[0m : 2.46564
[1mStep[0m  [40/84], [94mLoss[0m : 2.32007
[1mStep[0m  [48/84], [94mLoss[0m : 2.38018
[1mStep[0m  [56/84], [94mLoss[0m : 2.56771
[1mStep[0m  [64/84], [94mLoss[0m : 2.52746
[1mStep[0m  [72/84], [94mLoss[0m : 2.40508
[1mStep[0m  [80/84], [94mLoss[0m : 2.32281

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48366
[1mStep[0m  [8/84], [94mLoss[0m : 2.35888
[1mStep[0m  [16/84], [94mLoss[0m : 2.25784
[1mStep[0m  [24/84], [94mLoss[0m : 2.58787
[1mStep[0m  [32/84], [94mLoss[0m : 2.41949
[1mStep[0m  [40/84], [94mLoss[0m : 2.70055
[1mStep[0m  [48/84], [94mLoss[0m : 2.40734
[1mStep[0m  [56/84], [94mLoss[0m : 2.71705
[1mStep[0m  [64/84], [94mLoss[0m : 2.67507
[1mStep[0m  [72/84], [94mLoss[0m : 2.27672
[1mStep[0m  [80/84], [94mLoss[0m : 2.14572

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10943
[1mStep[0m  [8/84], [94mLoss[0m : 2.22944
[1mStep[0m  [16/84], [94mLoss[0m : 2.67473
[1mStep[0m  [24/84], [94mLoss[0m : 2.52285
[1mStep[0m  [32/84], [94mLoss[0m : 2.37599
[1mStep[0m  [40/84], [94mLoss[0m : 2.24287
[1mStep[0m  [48/84], [94mLoss[0m : 2.61160
[1mStep[0m  [56/84], [94mLoss[0m : 2.27374
[1mStep[0m  [64/84], [94mLoss[0m : 2.64576
[1mStep[0m  [72/84], [94mLoss[0m : 2.43201
[1mStep[0m  [80/84], [94mLoss[0m : 2.44769

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.355, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62256
[1mStep[0m  [8/84], [94mLoss[0m : 2.45756
[1mStep[0m  [16/84], [94mLoss[0m : 2.42170
[1mStep[0m  [24/84], [94mLoss[0m : 2.48442
[1mStep[0m  [32/84], [94mLoss[0m : 2.34176
[1mStep[0m  [40/84], [94mLoss[0m : 2.39931
[1mStep[0m  [48/84], [94mLoss[0m : 2.59053
[1mStep[0m  [56/84], [94mLoss[0m : 2.51818
[1mStep[0m  [64/84], [94mLoss[0m : 2.60758
[1mStep[0m  [72/84], [94mLoss[0m : 2.53744
[1mStep[0m  [80/84], [94mLoss[0m : 2.22162

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61127
[1mStep[0m  [8/84], [94mLoss[0m : 2.40429
[1mStep[0m  [16/84], [94mLoss[0m : 2.16738
[1mStep[0m  [24/84], [94mLoss[0m : 2.52493
[1mStep[0m  [32/84], [94mLoss[0m : 2.58560
[1mStep[0m  [40/84], [94mLoss[0m : 2.42228
[1mStep[0m  [48/84], [94mLoss[0m : 2.35059
[1mStep[0m  [56/84], [94mLoss[0m : 2.19100
[1mStep[0m  [64/84], [94mLoss[0m : 2.22112
[1mStep[0m  [72/84], [94mLoss[0m : 2.60106
[1mStep[0m  [80/84], [94mLoss[0m : 2.62133

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32843
[1mStep[0m  [8/84], [94mLoss[0m : 2.48207
[1mStep[0m  [16/84], [94mLoss[0m : 2.19510
[1mStep[0m  [24/84], [94mLoss[0m : 2.30447
[1mStep[0m  [32/84], [94mLoss[0m : 2.80043
[1mStep[0m  [40/84], [94mLoss[0m : 2.45135
[1mStep[0m  [48/84], [94mLoss[0m : 2.39797
[1mStep[0m  [56/84], [94mLoss[0m : 2.54238
[1mStep[0m  [64/84], [94mLoss[0m : 2.65672
[1mStep[0m  [72/84], [94mLoss[0m : 2.30285
[1mStep[0m  [80/84], [94mLoss[0m : 2.76184

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.347, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55894
[1mStep[0m  [8/84], [94mLoss[0m : 2.17033
[1mStep[0m  [16/84], [94mLoss[0m : 2.42637
[1mStep[0m  [24/84], [94mLoss[0m : 2.46302
[1mStep[0m  [32/84], [94mLoss[0m : 2.46269
[1mStep[0m  [40/84], [94mLoss[0m : 2.46728
[1mStep[0m  [48/84], [94mLoss[0m : 2.96364
[1mStep[0m  [56/84], [94mLoss[0m : 2.32193
[1mStep[0m  [64/84], [94mLoss[0m : 2.45991
[1mStep[0m  [72/84], [94mLoss[0m : 2.41910
[1mStep[0m  [80/84], [94mLoss[0m : 2.35942

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.349
====================================

Phase 1 - Evaluation MAE:  2.3485820634024486
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.52077
[1mStep[0m  [8/84], [94mLoss[0m : 3.05117
[1mStep[0m  [16/84], [94mLoss[0m : 2.21931
[1mStep[0m  [24/84], [94mLoss[0m : 2.59760
[1mStep[0m  [32/84], [94mLoss[0m : 2.28474
[1mStep[0m  [40/84], [94mLoss[0m : 2.72970
[1mStep[0m  [48/84], [94mLoss[0m : 2.20234
[1mStep[0m  [56/84], [94mLoss[0m : 2.53541
[1mStep[0m  [64/84], [94mLoss[0m : 2.66611
[1mStep[0m  [72/84], [94mLoss[0m : 2.75144
[1mStep[0m  [80/84], [94mLoss[0m : 2.36024

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52471
[1mStep[0m  [8/84], [94mLoss[0m : 2.35387
[1mStep[0m  [16/84], [94mLoss[0m : 2.40224
[1mStep[0m  [24/84], [94mLoss[0m : 2.44191
[1mStep[0m  [32/84], [94mLoss[0m : 2.33935
[1mStep[0m  [40/84], [94mLoss[0m : 2.33647
[1mStep[0m  [48/84], [94mLoss[0m : 2.53263
[1mStep[0m  [56/84], [94mLoss[0m : 2.22988
[1mStep[0m  [64/84], [94mLoss[0m : 2.31390
[1mStep[0m  [72/84], [94mLoss[0m : 2.51810
[1mStep[0m  [80/84], [94mLoss[0m : 2.56423

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33918
[1mStep[0m  [8/84], [94mLoss[0m : 2.29541
[1mStep[0m  [16/84], [94mLoss[0m : 2.46333
[1mStep[0m  [24/84], [94mLoss[0m : 2.28734
[1mStep[0m  [32/84], [94mLoss[0m : 2.66624
[1mStep[0m  [40/84], [94mLoss[0m : 2.53118
[1mStep[0m  [48/84], [94mLoss[0m : 2.38309
[1mStep[0m  [56/84], [94mLoss[0m : 2.44814
[1mStep[0m  [64/84], [94mLoss[0m : 2.38260
[1mStep[0m  [72/84], [94mLoss[0m : 2.42919
[1mStep[0m  [80/84], [94mLoss[0m : 2.38363

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18215
[1mStep[0m  [8/84], [94mLoss[0m : 2.09606
[1mStep[0m  [16/84], [94mLoss[0m : 2.18047
[1mStep[0m  [24/84], [94mLoss[0m : 1.96285
[1mStep[0m  [32/84], [94mLoss[0m : 1.95420
[1mStep[0m  [40/84], [94mLoss[0m : 2.24571
[1mStep[0m  [48/84], [94mLoss[0m : 2.37950
[1mStep[0m  [56/84], [94mLoss[0m : 2.13325
[1mStep[0m  [64/84], [94mLoss[0m : 2.29342
[1mStep[0m  [72/84], [94mLoss[0m : 2.23003
[1mStep[0m  [80/84], [94mLoss[0m : 2.35146

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13604
[1mStep[0m  [8/84], [94mLoss[0m : 1.90572
[1mStep[0m  [16/84], [94mLoss[0m : 2.31074
[1mStep[0m  [24/84], [94mLoss[0m : 2.15444
[1mStep[0m  [32/84], [94mLoss[0m : 1.97466
[1mStep[0m  [40/84], [94mLoss[0m : 2.04086
[1mStep[0m  [48/84], [94mLoss[0m : 2.30957
[1mStep[0m  [56/84], [94mLoss[0m : 2.59251
[1mStep[0m  [64/84], [94mLoss[0m : 2.29300
[1mStep[0m  [72/84], [94mLoss[0m : 2.62058
[1mStep[0m  [80/84], [94mLoss[0m : 2.19679

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.227, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44980
[1mStep[0m  [8/84], [94mLoss[0m : 2.08541
[1mStep[0m  [16/84], [94mLoss[0m : 2.59193
[1mStep[0m  [24/84], [94mLoss[0m : 2.31002
[1mStep[0m  [32/84], [94mLoss[0m : 2.12556
[1mStep[0m  [40/84], [94mLoss[0m : 1.97801
[1mStep[0m  [48/84], [94mLoss[0m : 2.23342
[1mStep[0m  [56/84], [94mLoss[0m : 2.27763
[1mStep[0m  [64/84], [94mLoss[0m : 2.38366
[1mStep[0m  [72/84], [94mLoss[0m : 2.25821
[1mStep[0m  [80/84], [94mLoss[0m : 2.29831

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07642
[1mStep[0m  [8/84], [94mLoss[0m : 2.10919
[1mStep[0m  [16/84], [94mLoss[0m : 2.19839
[1mStep[0m  [24/84], [94mLoss[0m : 2.30749
[1mStep[0m  [32/84], [94mLoss[0m : 2.33555
[1mStep[0m  [40/84], [94mLoss[0m : 1.90613
[1mStep[0m  [48/84], [94mLoss[0m : 1.92056
[1mStep[0m  [56/84], [94mLoss[0m : 2.19237
[1mStep[0m  [64/84], [94mLoss[0m : 2.19458
[1mStep[0m  [72/84], [94mLoss[0m : 2.07431
[1mStep[0m  [80/84], [94mLoss[0m : 2.35024

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00907
[1mStep[0m  [8/84], [94mLoss[0m : 2.06774
[1mStep[0m  [16/84], [94mLoss[0m : 2.29767
[1mStep[0m  [24/84], [94mLoss[0m : 2.11284
[1mStep[0m  [32/84], [94mLoss[0m : 2.16931
[1mStep[0m  [40/84], [94mLoss[0m : 1.92364
[1mStep[0m  [48/84], [94mLoss[0m : 2.05886
[1mStep[0m  [56/84], [94mLoss[0m : 2.05316
[1mStep[0m  [64/84], [94mLoss[0m : 1.99592
[1mStep[0m  [72/84], [94mLoss[0m : 2.02092
[1mStep[0m  [80/84], [94mLoss[0m : 2.10603

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.110, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14233
[1mStep[0m  [8/84], [94mLoss[0m : 2.25587
[1mStep[0m  [16/84], [94mLoss[0m : 2.02287
[1mStep[0m  [24/84], [94mLoss[0m : 1.98510
[1mStep[0m  [32/84], [94mLoss[0m : 1.95872
[1mStep[0m  [40/84], [94mLoss[0m : 1.94250
[1mStep[0m  [48/84], [94mLoss[0m : 2.08622
[1mStep[0m  [56/84], [94mLoss[0m : 2.22284
[1mStep[0m  [64/84], [94mLoss[0m : 2.10935
[1mStep[0m  [72/84], [94mLoss[0m : 2.40909
[1mStep[0m  [80/84], [94mLoss[0m : 1.98024

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.077, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.94104
[1mStep[0m  [8/84], [94mLoss[0m : 1.73293
[1mStep[0m  [16/84], [94mLoss[0m : 2.50841
[1mStep[0m  [24/84], [94mLoss[0m : 2.02231
[1mStep[0m  [32/84], [94mLoss[0m : 2.22669
[1mStep[0m  [40/84], [94mLoss[0m : 2.04348
[1mStep[0m  [48/84], [94mLoss[0m : 2.07249
[1mStep[0m  [56/84], [94mLoss[0m : 1.90324
[1mStep[0m  [64/84], [94mLoss[0m : 2.25827
[1mStep[0m  [72/84], [94mLoss[0m : 1.87538
[1mStep[0m  [80/84], [94mLoss[0m : 2.00833

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33893
[1mStep[0m  [8/84], [94mLoss[0m : 2.25351
[1mStep[0m  [16/84], [94mLoss[0m : 1.97296
[1mStep[0m  [24/84], [94mLoss[0m : 1.92031
[1mStep[0m  [32/84], [94mLoss[0m : 2.13069
[1mStep[0m  [40/84], [94mLoss[0m : 2.00204
[1mStep[0m  [48/84], [94mLoss[0m : 1.82034
[1mStep[0m  [56/84], [94mLoss[0m : 2.00680
[1mStep[0m  [64/84], [94mLoss[0m : 1.99609
[1mStep[0m  [72/84], [94mLoss[0m : 2.35693
[1mStep[0m  [80/84], [94mLoss[0m : 2.28363

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05209
[1mStep[0m  [8/84], [94mLoss[0m : 1.98677
[1mStep[0m  [16/84], [94mLoss[0m : 1.94563
[1mStep[0m  [24/84], [94mLoss[0m : 2.02831
[1mStep[0m  [32/84], [94mLoss[0m : 1.87400
[1mStep[0m  [40/84], [94mLoss[0m : 2.32343
[1mStep[0m  [48/84], [94mLoss[0m : 1.94898
[1mStep[0m  [56/84], [94mLoss[0m : 1.94045
[1mStep[0m  [64/84], [94mLoss[0m : 2.13801
[1mStep[0m  [72/84], [94mLoss[0m : 1.76201
[1mStep[0m  [80/84], [94mLoss[0m : 2.01091

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13615
[1mStep[0m  [8/84], [94mLoss[0m : 1.77287
[1mStep[0m  [16/84], [94mLoss[0m : 1.77604
[1mStep[0m  [24/84], [94mLoss[0m : 2.20579
[1mStep[0m  [32/84], [94mLoss[0m : 1.95178
[1mStep[0m  [40/84], [94mLoss[0m : 2.03617
[1mStep[0m  [48/84], [94mLoss[0m : 2.05612
[1mStep[0m  [56/84], [94mLoss[0m : 1.69570
[1mStep[0m  [64/84], [94mLoss[0m : 1.82992
[1mStep[0m  [72/84], [94mLoss[0m : 2.11855
[1mStep[0m  [80/84], [94mLoss[0m : 1.81954

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84661
[1mStep[0m  [8/84], [94mLoss[0m : 2.05700
[1mStep[0m  [16/84], [94mLoss[0m : 1.84993
[1mStep[0m  [24/84], [94mLoss[0m : 2.11277
[1mStep[0m  [32/84], [94mLoss[0m : 2.34630
[1mStep[0m  [40/84], [94mLoss[0m : 1.83746
[1mStep[0m  [48/84], [94mLoss[0m : 1.76064
[1mStep[0m  [56/84], [94mLoss[0m : 2.07729
[1mStep[0m  [64/84], [94mLoss[0m : 2.29443
[1mStep[0m  [72/84], [94mLoss[0m : 2.02089
[1mStep[0m  [80/84], [94mLoss[0m : 2.09238

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95550
[1mStep[0m  [8/84], [94mLoss[0m : 2.00343
[1mStep[0m  [16/84], [94mLoss[0m : 1.72334
[1mStep[0m  [24/84], [94mLoss[0m : 2.02052
[1mStep[0m  [32/84], [94mLoss[0m : 1.86090
[1mStep[0m  [40/84], [94mLoss[0m : 2.04383
[1mStep[0m  [48/84], [94mLoss[0m : 1.81529
[1mStep[0m  [56/84], [94mLoss[0m : 1.74138
[1mStep[0m  [64/84], [94mLoss[0m : 2.34677
[1mStep[0m  [72/84], [94mLoss[0m : 2.00849
[1mStep[0m  [80/84], [94mLoss[0m : 1.84634

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79699
[1mStep[0m  [8/84], [94mLoss[0m : 1.93018
[1mStep[0m  [16/84], [94mLoss[0m : 1.81157
[1mStep[0m  [24/84], [94mLoss[0m : 1.96937
[1mStep[0m  [32/84], [94mLoss[0m : 1.74891
[1mStep[0m  [40/84], [94mLoss[0m : 1.73613
[1mStep[0m  [48/84], [94mLoss[0m : 2.05959
[1mStep[0m  [56/84], [94mLoss[0m : 2.26038
[1mStep[0m  [64/84], [94mLoss[0m : 1.75844
[1mStep[0m  [72/84], [94mLoss[0m : 2.25222
[1mStep[0m  [80/84], [94mLoss[0m : 1.60576

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99565
[1mStep[0m  [8/84], [94mLoss[0m : 1.90565
[1mStep[0m  [16/84], [94mLoss[0m : 1.69802
[1mStep[0m  [24/84], [94mLoss[0m : 1.78100
[1mStep[0m  [32/84], [94mLoss[0m : 1.73372
[1mStep[0m  [40/84], [94mLoss[0m : 1.70267
[1mStep[0m  [48/84], [94mLoss[0m : 1.91519
[1mStep[0m  [56/84], [94mLoss[0m : 2.01637
[1mStep[0m  [64/84], [94mLoss[0m : 1.89413
[1mStep[0m  [72/84], [94mLoss[0m : 1.78402
[1mStep[0m  [80/84], [94mLoss[0m : 1.92841

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54504
[1mStep[0m  [8/84], [94mLoss[0m : 1.98768
[1mStep[0m  [16/84], [94mLoss[0m : 1.90799
[1mStep[0m  [24/84], [94mLoss[0m : 1.72048
[1mStep[0m  [32/84], [94mLoss[0m : 1.74905
[1mStep[0m  [40/84], [94mLoss[0m : 1.78790
[1mStep[0m  [48/84], [94mLoss[0m : 1.76407
[1mStep[0m  [56/84], [94mLoss[0m : 2.05096
[1mStep[0m  [64/84], [94mLoss[0m : 1.93516
[1mStep[0m  [72/84], [94mLoss[0m : 1.79436
[1mStep[0m  [80/84], [94mLoss[0m : 2.01191

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69467
[1mStep[0m  [8/84], [94mLoss[0m : 1.50977
[1mStep[0m  [16/84], [94mLoss[0m : 1.76519
[1mStep[0m  [24/84], [94mLoss[0m : 1.85526
[1mStep[0m  [32/84], [94mLoss[0m : 1.80040
[1mStep[0m  [40/84], [94mLoss[0m : 2.01715
[1mStep[0m  [48/84], [94mLoss[0m : 1.86002
[1mStep[0m  [56/84], [94mLoss[0m : 1.77922
[1mStep[0m  [64/84], [94mLoss[0m : 1.82243
[1mStep[0m  [72/84], [94mLoss[0m : 1.94570
[1mStep[0m  [80/84], [94mLoss[0m : 2.04940

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.852, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64030
[1mStep[0m  [8/84], [94mLoss[0m : 1.68122
[1mStep[0m  [16/84], [94mLoss[0m : 2.11764
[1mStep[0m  [24/84], [94mLoss[0m : 1.87139
[1mStep[0m  [32/84], [94mLoss[0m : 1.74784
[1mStep[0m  [40/84], [94mLoss[0m : 1.59338
[1mStep[0m  [48/84], [94mLoss[0m : 1.99709
[1mStep[0m  [56/84], [94mLoss[0m : 1.84252
[1mStep[0m  [64/84], [94mLoss[0m : 1.87081
[1mStep[0m  [72/84], [94mLoss[0m : 1.85546
[1mStep[0m  [80/84], [94mLoss[0m : 1.98147

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.881, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97128
[1mStep[0m  [8/84], [94mLoss[0m : 1.73688
[1mStep[0m  [16/84], [94mLoss[0m : 1.88940
[1mStep[0m  [24/84], [94mLoss[0m : 2.04078
[1mStep[0m  [32/84], [94mLoss[0m : 1.74646
[1mStep[0m  [40/84], [94mLoss[0m : 1.92997
[1mStep[0m  [48/84], [94mLoss[0m : 2.01963
[1mStep[0m  [56/84], [94mLoss[0m : 1.84599
[1mStep[0m  [64/84], [94mLoss[0m : 1.63188
[1mStep[0m  [72/84], [94mLoss[0m : 1.76929
[1mStep[0m  [80/84], [94mLoss[0m : 1.89095

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78600
[1mStep[0m  [8/84], [94mLoss[0m : 2.00651
[1mStep[0m  [16/84], [94mLoss[0m : 1.87686
[1mStep[0m  [24/84], [94mLoss[0m : 1.95023
[1mStep[0m  [32/84], [94mLoss[0m : 2.02808
[1mStep[0m  [40/84], [94mLoss[0m : 1.96567
[1mStep[0m  [48/84], [94mLoss[0m : 1.66823
[1mStep[0m  [56/84], [94mLoss[0m : 1.60752
[1mStep[0m  [64/84], [94mLoss[0m : 1.75529
[1mStep[0m  [72/84], [94mLoss[0m : 1.76240
[1mStep[0m  [80/84], [94mLoss[0m : 1.80855

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72136
[1mStep[0m  [8/84], [94mLoss[0m : 1.95416
[1mStep[0m  [16/84], [94mLoss[0m : 1.82442
[1mStep[0m  [24/84], [94mLoss[0m : 1.69470
[1mStep[0m  [32/84], [94mLoss[0m : 1.81526
[1mStep[0m  [40/84], [94mLoss[0m : 1.81222
[1mStep[0m  [48/84], [94mLoss[0m : 1.80375
[1mStep[0m  [56/84], [94mLoss[0m : 1.87736
[1mStep[0m  [64/84], [94mLoss[0m : 1.67253
[1mStep[0m  [72/84], [94mLoss[0m : 1.86645
[1mStep[0m  [80/84], [94mLoss[0m : 1.89841

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98317
[1mStep[0m  [8/84], [94mLoss[0m : 1.90646
[1mStep[0m  [16/84], [94mLoss[0m : 1.65576
[1mStep[0m  [24/84], [94mLoss[0m : 1.55933
[1mStep[0m  [32/84], [94mLoss[0m : 1.82653
[1mStep[0m  [40/84], [94mLoss[0m : 1.67370
[1mStep[0m  [48/84], [94mLoss[0m : 1.74030
[1mStep[0m  [56/84], [94mLoss[0m : 1.63354
[1mStep[0m  [64/84], [94mLoss[0m : 1.84045
[1mStep[0m  [72/84], [94mLoss[0m : 1.99305
[1mStep[0m  [80/84], [94mLoss[0m : 1.72926

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71303
[1mStep[0m  [8/84], [94mLoss[0m : 1.63463
[1mStep[0m  [16/84], [94mLoss[0m : 1.73058
[1mStep[0m  [24/84], [94mLoss[0m : 1.73344
[1mStep[0m  [32/84], [94mLoss[0m : 1.56015
[1mStep[0m  [40/84], [94mLoss[0m : 1.77164
[1mStep[0m  [48/84], [94mLoss[0m : 1.56150
[1mStep[0m  [56/84], [94mLoss[0m : 1.56951
[1mStep[0m  [64/84], [94mLoss[0m : 2.01997
[1mStep[0m  [72/84], [94mLoss[0m : 1.98437
[1mStep[0m  [80/84], [94mLoss[0m : 2.04521

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55548
[1mStep[0m  [8/84], [94mLoss[0m : 1.62759
[1mStep[0m  [16/84], [94mLoss[0m : 1.44355
[1mStep[0m  [24/84], [94mLoss[0m : 1.58399
[1mStep[0m  [32/84], [94mLoss[0m : 1.61482
[1mStep[0m  [40/84], [94mLoss[0m : 1.64863
[1mStep[0m  [48/84], [94mLoss[0m : 1.97197
[1mStep[0m  [56/84], [94mLoss[0m : 1.58757
[1mStep[0m  [64/84], [94mLoss[0m : 1.47231
[1mStep[0m  [72/84], [94mLoss[0m : 1.62862
[1mStep[0m  [80/84], [94mLoss[0m : 1.75692

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72131
[1mStep[0m  [8/84], [94mLoss[0m : 1.65975
[1mStep[0m  [16/84], [94mLoss[0m : 1.87592
[1mStep[0m  [24/84], [94mLoss[0m : 1.85458
[1mStep[0m  [32/84], [94mLoss[0m : 1.53715
[1mStep[0m  [40/84], [94mLoss[0m : 1.88144
[1mStep[0m  [48/84], [94mLoss[0m : 1.94119
[1mStep[0m  [56/84], [94mLoss[0m : 1.72117
[1mStep[0m  [64/84], [94mLoss[0m : 1.76794
[1mStep[0m  [72/84], [94mLoss[0m : 1.88765
[1mStep[0m  [80/84], [94mLoss[0m : 1.73717

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82124
[1mStep[0m  [8/84], [94mLoss[0m : 1.70996
[1mStep[0m  [16/84], [94mLoss[0m : 1.87715
[1mStep[0m  [24/84], [94mLoss[0m : 2.04505
[1mStep[0m  [32/84], [94mLoss[0m : 1.41961
[1mStep[0m  [40/84], [94mLoss[0m : 1.97943
[1mStep[0m  [48/84], [94mLoss[0m : 1.63317
[1mStep[0m  [56/84], [94mLoss[0m : 1.87330
[1mStep[0m  [64/84], [94mLoss[0m : 1.85569
[1mStep[0m  [72/84], [94mLoss[0m : 1.98343
[1mStep[0m  [80/84], [94mLoss[0m : 1.68373

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53126
[1mStep[0m  [8/84], [94mLoss[0m : 1.77158
[1mStep[0m  [16/84], [94mLoss[0m : 1.66434
[1mStep[0m  [24/84], [94mLoss[0m : 1.80581
[1mStep[0m  [32/84], [94mLoss[0m : 1.49697
[1mStep[0m  [40/84], [94mLoss[0m : 1.91826
[1mStep[0m  [48/84], [94mLoss[0m : 1.76123
[1mStep[0m  [56/84], [94mLoss[0m : 1.84411
[1mStep[0m  [64/84], [94mLoss[0m : 1.70411
[1mStep[0m  [72/84], [94mLoss[0m : 1.81439
[1mStep[0m  [80/84], [94mLoss[0m : 2.07580

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58188
[1mStep[0m  [8/84], [94mLoss[0m : 1.67276
[1mStep[0m  [16/84], [94mLoss[0m : 1.71422
[1mStep[0m  [24/84], [94mLoss[0m : 1.62388
[1mStep[0m  [32/84], [94mLoss[0m : 1.63800
[1mStep[0m  [40/84], [94mLoss[0m : 1.71132
[1mStep[0m  [48/84], [94mLoss[0m : 1.97897
[1mStep[0m  [56/84], [94mLoss[0m : 1.76741
[1mStep[0m  [64/84], [94mLoss[0m : 1.91728
[1mStep[0m  [72/84], [94mLoss[0m : 1.88588
[1mStep[0m  [80/84], [94mLoss[0m : 1.62497

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.497
====================================

Phase 2 - Evaluation MAE:  2.4968875646591187
MAE score P1       2.348582
MAE score P2       2.496888
loss               1.740015
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.21489
[1mStep[0m  [4/42], [94mLoss[0m : 10.84709
[1mStep[0m  [8/42], [94mLoss[0m : 10.40381
[1mStep[0m  [12/42], [94mLoss[0m : 10.61498
[1mStep[0m  [16/42], [94mLoss[0m : 9.89965
[1mStep[0m  [20/42], [94mLoss[0m : 9.83394
[1mStep[0m  [24/42], [94mLoss[0m : 9.58971
[1mStep[0m  [28/42], [94mLoss[0m : 9.26790
[1mStep[0m  [32/42], [94mLoss[0m : 8.97041
[1mStep[0m  [36/42], [94mLoss[0m : 8.41246
[1mStep[0m  [40/42], [94mLoss[0m : 8.24315

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.762, [92mTest[0m: 10.866, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.77033
[1mStep[0m  [4/42], [94mLoss[0m : 7.20943
[1mStep[0m  [8/42], [94mLoss[0m : 7.40245
[1mStep[0m  [12/42], [94mLoss[0m : 6.42795
[1mStep[0m  [16/42], [94mLoss[0m : 5.61048
[1mStep[0m  [20/42], [94mLoss[0m : 5.82141
[1mStep[0m  [24/42], [94mLoss[0m : 5.67894
[1mStep[0m  [28/42], [94mLoss[0m : 5.09915
[1mStep[0m  [32/42], [94mLoss[0m : 5.29295
[1mStep[0m  [36/42], [94mLoss[0m : 4.64716
[1mStep[0m  [40/42], [94mLoss[0m : 3.98617

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.981, [92mTest[0m: 8.233, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.26289
[1mStep[0m  [4/42], [94mLoss[0m : 3.83642
[1mStep[0m  [8/42], [94mLoss[0m : 3.53994
[1mStep[0m  [12/42], [94mLoss[0m : 3.07367
[1mStep[0m  [16/42], [94mLoss[0m : 2.96252
[1mStep[0m  [20/42], [94mLoss[0m : 3.17281
[1mStep[0m  [24/42], [94mLoss[0m : 3.18126
[1mStep[0m  [28/42], [94mLoss[0m : 2.62973
[1mStep[0m  [32/42], [94mLoss[0m : 2.88014
[1mStep[0m  [36/42], [94mLoss[0m : 2.71436
[1mStep[0m  [40/42], [94mLoss[0m : 2.53199

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.186, [92mTest[0m: 3.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62572
[1mStep[0m  [4/42], [94mLoss[0m : 2.80812
[1mStep[0m  [8/42], [94mLoss[0m : 2.62312
[1mStep[0m  [12/42], [94mLoss[0m : 2.53748
[1mStep[0m  [16/42], [94mLoss[0m : 2.62369
[1mStep[0m  [20/42], [94mLoss[0m : 2.68920
[1mStep[0m  [24/42], [94mLoss[0m : 2.52936
[1mStep[0m  [28/42], [94mLoss[0m : 2.77929
[1mStep[0m  [32/42], [94mLoss[0m : 2.79385
[1mStep[0m  [36/42], [94mLoss[0m : 2.50875
[1mStep[0m  [40/42], [94mLoss[0m : 2.77313

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64021
[1mStep[0m  [4/42], [94mLoss[0m : 2.73240
[1mStep[0m  [8/42], [94mLoss[0m : 2.44259
[1mStep[0m  [12/42], [94mLoss[0m : 2.71020
[1mStep[0m  [16/42], [94mLoss[0m : 2.35166
[1mStep[0m  [20/42], [94mLoss[0m : 2.43590
[1mStep[0m  [24/42], [94mLoss[0m : 2.31977
[1mStep[0m  [28/42], [94mLoss[0m : 2.59350
[1mStep[0m  [32/42], [94mLoss[0m : 2.72131
[1mStep[0m  [36/42], [94mLoss[0m : 2.57687
[1mStep[0m  [40/42], [94mLoss[0m : 2.39708

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49350
[1mStep[0m  [4/42], [94mLoss[0m : 2.73288
[1mStep[0m  [8/42], [94mLoss[0m : 2.58777
[1mStep[0m  [12/42], [94mLoss[0m : 2.65786
[1mStep[0m  [16/42], [94mLoss[0m : 2.44313
[1mStep[0m  [20/42], [94mLoss[0m : 2.57374
[1mStep[0m  [24/42], [94mLoss[0m : 2.52419
[1mStep[0m  [28/42], [94mLoss[0m : 2.32921
[1mStep[0m  [32/42], [94mLoss[0m : 2.56559
[1mStep[0m  [36/42], [94mLoss[0m : 2.61701
[1mStep[0m  [40/42], [94mLoss[0m : 2.72348

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76829
[1mStep[0m  [4/42], [94mLoss[0m : 2.36020
[1mStep[0m  [8/42], [94mLoss[0m : 2.76116
[1mStep[0m  [12/42], [94mLoss[0m : 2.37172
[1mStep[0m  [16/42], [94mLoss[0m : 2.36697
[1mStep[0m  [20/42], [94mLoss[0m : 2.52389
[1mStep[0m  [24/42], [94mLoss[0m : 2.73342
[1mStep[0m  [28/42], [94mLoss[0m : 2.62883
[1mStep[0m  [32/42], [94mLoss[0m : 2.42188
[1mStep[0m  [36/42], [94mLoss[0m : 2.51117
[1mStep[0m  [40/42], [94mLoss[0m : 2.35770

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37522
[1mStep[0m  [4/42], [94mLoss[0m : 2.41242
[1mStep[0m  [8/42], [94mLoss[0m : 2.56515
[1mStep[0m  [12/42], [94mLoss[0m : 2.38856
[1mStep[0m  [16/42], [94mLoss[0m : 2.55421
[1mStep[0m  [20/42], [94mLoss[0m : 2.59810
[1mStep[0m  [24/42], [94mLoss[0m : 2.48027
[1mStep[0m  [28/42], [94mLoss[0m : 2.60734
[1mStep[0m  [32/42], [94mLoss[0m : 2.58155
[1mStep[0m  [36/42], [94mLoss[0m : 2.48280
[1mStep[0m  [40/42], [94mLoss[0m : 2.50123

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46367
[1mStep[0m  [4/42], [94mLoss[0m : 2.59705
[1mStep[0m  [8/42], [94mLoss[0m : 2.58363
[1mStep[0m  [12/42], [94mLoss[0m : 2.45005
[1mStep[0m  [16/42], [94mLoss[0m : 2.29061
[1mStep[0m  [20/42], [94mLoss[0m : 2.34113
[1mStep[0m  [24/42], [94mLoss[0m : 2.24996
[1mStep[0m  [28/42], [94mLoss[0m : 2.41629
[1mStep[0m  [32/42], [94mLoss[0m : 2.57752
[1mStep[0m  [36/42], [94mLoss[0m : 2.63955
[1mStep[0m  [40/42], [94mLoss[0m : 2.51151

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43463
[1mStep[0m  [4/42], [94mLoss[0m : 2.44164
[1mStep[0m  [8/42], [94mLoss[0m : 2.43878
[1mStep[0m  [12/42], [94mLoss[0m : 2.37171
[1mStep[0m  [16/42], [94mLoss[0m : 2.49056
[1mStep[0m  [20/42], [94mLoss[0m : 2.44132
[1mStep[0m  [24/42], [94mLoss[0m : 2.44738
[1mStep[0m  [28/42], [94mLoss[0m : 2.38556
[1mStep[0m  [32/42], [94mLoss[0m : 2.40293
[1mStep[0m  [36/42], [94mLoss[0m : 2.38771
[1mStep[0m  [40/42], [94mLoss[0m : 2.45965

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59783
[1mStep[0m  [4/42], [94mLoss[0m : 2.24290
[1mStep[0m  [8/42], [94mLoss[0m : 2.44585
[1mStep[0m  [12/42], [94mLoss[0m : 2.51328
[1mStep[0m  [16/42], [94mLoss[0m : 2.75085
[1mStep[0m  [20/42], [94mLoss[0m : 2.27958
[1mStep[0m  [24/42], [94mLoss[0m : 2.73306
[1mStep[0m  [28/42], [94mLoss[0m : 2.35585
[1mStep[0m  [32/42], [94mLoss[0m : 2.34230
[1mStep[0m  [36/42], [94mLoss[0m : 2.25834
[1mStep[0m  [40/42], [94mLoss[0m : 2.72055

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58431
[1mStep[0m  [4/42], [94mLoss[0m : 2.42838
[1mStep[0m  [8/42], [94mLoss[0m : 2.25835
[1mStep[0m  [12/42], [94mLoss[0m : 2.41999
[1mStep[0m  [16/42], [94mLoss[0m : 2.51314
[1mStep[0m  [20/42], [94mLoss[0m : 2.47450
[1mStep[0m  [24/42], [94mLoss[0m : 2.26495
[1mStep[0m  [28/42], [94mLoss[0m : 2.49105
[1mStep[0m  [32/42], [94mLoss[0m : 2.47863
[1mStep[0m  [36/42], [94mLoss[0m : 2.39093
[1mStep[0m  [40/42], [94mLoss[0m : 2.42983

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58485
[1mStep[0m  [4/42], [94mLoss[0m : 2.52758
[1mStep[0m  [8/42], [94mLoss[0m : 2.26830
[1mStep[0m  [12/42], [94mLoss[0m : 2.45429
[1mStep[0m  [16/42], [94mLoss[0m : 2.48089
[1mStep[0m  [20/42], [94mLoss[0m : 2.53736
[1mStep[0m  [24/42], [94mLoss[0m : 2.51312
[1mStep[0m  [28/42], [94mLoss[0m : 2.56498
[1mStep[0m  [32/42], [94mLoss[0m : 2.56144
[1mStep[0m  [36/42], [94mLoss[0m : 2.33772
[1mStep[0m  [40/42], [94mLoss[0m : 2.51453

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21725
[1mStep[0m  [4/42], [94mLoss[0m : 2.33869
[1mStep[0m  [8/42], [94mLoss[0m : 2.53162
[1mStep[0m  [12/42], [94mLoss[0m : 2.69203
[1mStep[0m  [16/42], [94mLoss[0m : 2.36610
[1mStep[0m  [20/42], [94mLoss[0m : 2.23164
[1mStep[0m  [24/42], [94mLoss[0m : 2.57823
[1mStep[0m  [28/42], [94mLoss[0m : 2.35692
[1mStep[0m  [32/42], [94mLoss[0m : 2.37565
[1mStep[0m  [36/42], [94mLoss[0m : 2.47044
[1mStep[0m  [40/42], [94mLoss[0m : 2.36078

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57207
[1mStep[0m  [4/42], [94mLoss[0m : 2.46235
[1mStep[0m  [8/42], [94mLoss[0m : 2.36380
[1mStep[0m  [12/42], [94mLoss[0m : 2.34611
[1mStep[0m  [16/42], [94mLoss[0m : 2.30094
[1mStep[0m  [20/42], [94mLoss[0m : 2.46343
[1mStep[0m  [24/42], [94mLoss[0m : 2.41714
[1mStep[0m  [28/42], [94mLoss[0m : 2.28343
[1mStep[0m  [32/42], [94mLoss[0m : 2.50162
[1mStep[0m  [36/42], [94mLoss[0m : 2.63826
[1mStep[0m  [40/42], [94mLoss[0m : 2.53535

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58409
[1mStep[0m  [4/42], [94mLoss[0m : 2.33596
[1mStep[0m  [8/42], [94mLoss[0m : 2.51484
[1mStep[0m  [12/42], [94mLoss[0m : 2.29262
[1mStep[0m  [16/42], [94mLoss[0m : 2.44026
[1mStep[0m  [20/42], [94mLoss[0m : 2.50258
[1mStep[0m  [24/42], [94mLoss[0m : 2.20198
[1mStep[0m  [28/42], [94mLoss[0m : 2.38317
[1mStep[0m  [32/42], [94mLoss[0m : 2.33021
[1mStep[0m  [36/42], [94mLoss[0m : 2.48526
[1mStep[0m  [40/42], [94mLoss[0m : 2.64294

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50102
[1mStep[0m  [4/42], [94mLoss[0m : 2.35984
[1mStep[0m  [8/42], [94mLoss[0m : 2.37913
[1mStep[0m  [12/42], [94mLoss[0m : 2.26592
[1mStep[0m  [16/42], [94mLoss[0m : 2.56094
[1mStep[0m  [20/42], [94mLoss[0m : 2.47278
[1mStep[0m  [24/42], [94mLoss[0m : 2.55326
[1mStep[0m  [28/42], [94mLoss[0m : 2.64571
[1mStep[0m  [32/42], [94mLoss[0m : 2.37449
[1mStep[0m  [36/42], [94mLoss[0m : 2.41487
[1mStep[0m  [40/42], [94mLoss[0m : 2.54442

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54605
[1mStep[0m  [4/42], [94mLoss[0m : 2.48162
[1mStep[0m  [8/42], [94mLoss[0m : 2.47580
[1mStep[0m  [12/42], [94mLoss[0m : 2.46053
[1mStep[0m  [16/42], [94mLoss[0m : 2.41998
[1mStep[0m  [20/42], [94mLoss[0m : 2.26922
[1mStep[0m  [24/42], [94mLoss[0m : 2.41552
[1mStep[0m  [28/42], [94mLoss[0m : 2.56438
[1mStep[0m  [32/42], [94mLoss[0m : 2.35664
[1mStep[0m  [36/42], [94mLoss[0m : 2.21776
[1mStep[0m  [40/42], [94mLoss[0m : 2.48918

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53496
[1mStep[0m  [4/42], [94mLoss[0m : 2.42969
[1mStep[0m  [8/42], [94mLoss[0m : 2.39160
[1mStep[0m  [12/42], [94mLoss[0m : 2.08149
[1mStep[0m  [16/42], [94mLoss[0m : 2.55924
[1mStep[0m  [20/42], [94mLoss[0m : 2.34592
[1mStep[0m  [24/42], [94mLoss[0m : 2.48369
[1mStep[0m  [28/42], [94mLoss[0m : 2.37356
[1mStep[0m  [32/42], [94mLoss[0m : 2.51179
[1mStep[0m  [36/42], [94mLoss[0m : 2.34120
[1mStep[0m  [40/42], [94mLoss[0m : 2.42490

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49401
[1mStep[0m  [4/42], [94mLoss[0m : 2.30673
[1mStep[0m  [8/42], [94mLoss[0m : 2.46421
[1mStep[0m  [12/42], [94mLoss[0m : 2.48872
[1mStep[0m  [16/42], [94mLoss[0m : 2.40236
[1mStep[0m  [20/42], [94mLoss[0m : 2.47550
[1mStep[0m  [24/42], [94mLoss[0m : 2.48245
[1mStep[0m  [28/42], [94mLoss[0m : 2.35158
[1mStep[0m  [32/42], [94mLoss[0m : 2.37247
[1mStep[0m  [36/42], [94mLoss[0m : 2.52923
[1mStep[0m  [40/42], [94mLoss[0m : 2.33633

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37445
[1mStep[0m  [4/42], [94mLoss[0m : 2.62967
[1mStep[0m  [8/42], [94mLoss[0m : 2.35556
[1mStep[0m  [12/42], [94mLoss[0m : 2.56914
[1mStep[0m  [16/42], [94mLoss[0m : 2.22281
[1mStep[0m  [20/42], [94mLoss[0m : 2.15048
[1mStep[0m  [24/42], [94mLoss[0m : 2.65227
[1mStep[0m  [28/42], [94mLoss[0m : 2.46945
[1mStep[0m  [32/42], [94mLoss[0m : 2.62165
[1mStep[0m  [36/42], [94mLoss[0m : 2.36018
[1mStep[0m  [40/42], [94mLoss[0m : 2.51232

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26839
[1mStep[0m  [4/42], [94mLoss[0m : 2.47713
[1mStep[0m  [8/42], [94mLoss[0m : 2.49601
[1mStep[0m  [12/42], [94mLoss[0m : 2.51771
[1mStep[0m  [16/42], [94mLoss[0m : 2.32930
[1mStep[0m  [20/42], [94mLoss[0m : 2.38062
[1mStep[0m  [24/42], [94mLoss[0m : 2.49065
[1mStep[0m  [28/42], [94mLoss[0m : 2.51060
[1mStep[0m  [32/42], [94mLoss[0m : 2.31301
[1mStep[0m  [36/42], [94mLoss[0m : 2.35255
[1mStep[0m  [40/42], [94mLoss[0m : 2.48794

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38084
[1mStep[0m  [4/42], [94mLoss[0m : 2.40717
[1mStep[0m  [8/42], [94mLoss[0m : 2.42143
[1mStep[0m  [12/42], [94mLoss[0m : 2.41648
[1mStep[0m  [16/42], [94mLoss[0m : 2.25845
[1mStep[0m  [20/42], [94mLoss[0m : 2.30861
[1mStep[0m  [24/42], [94mLoss[0m : 2.53078
[1mStep[0m  [28/42], [94mLoss[0m : 2.34146
[1mStep[0m  [32/42], [94mLoss[0m : 2.37514
[1mStep[0m  [36/42], [94mLoss[0m : 2.29429
[1mStep[0m  [40/42], [94mLoss[0m : 2.59014

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40591
[1mStep[0m  [4/42], [94mLoss[0m : 2.39230
[1mStep[0m  [8/42], [94mLoss[0m : 2.18639
[1mStep[0m  [12/42], [94mLoss[0m : 2.42170
[1mStep[0m  [16/42], [94mLoss[0m : 2.23612
[1mStep[0m  [20/42], [94mLoss[0m : 2.38820
[1mStep[0m  [24/42], [94mLoss[0m : 2.39353
[1mStep[0m  [28/42], [94mLoss[0m : 2.25793
[1mStep[0m  [32/42], [94mLoss[0m : 2.37607
[1mStep[0m  [36/42], [94mLoss[0m : 2.68799
[1mStep[0m  [40/42], [94mLoss[0m : 2.35026

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27318
[1mStep[0m  [4/42], [94mLoss[0m : 2.47933
[1mStep[0m  [8/42], [94mLoss[0m : 2.33477
[1mStep[0m  [12/42], [94mLoss[0m : 2.50600
[1mStep[0m  [16/42], [94mLoss[0m : 2.43086
[1mStep[0m  [20/42], [94mLoss[0m : 2.55258
[1mStep[0m  [24/42], [94mLoss[0m : 2.45720
[1mStep[0m  [28/42], [94mLoss[0m : 2.33025
[1mStep[0m  [32/42], [94mLoss[0m : 2.39949
[1mStep[0m  [36/42], [94mLoss[0m : 2.50099
[1mStep[0m  [40/42], [94mLoss[0m : 2.35909

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57656
[1mStep[0m  [4/42], [94mLoss[0m : 2.40761
[1mStep[0m  [8/42], [94mLoss[0m : 2.32348
[1mStep[0m  [12/42], [94mLoss[0m : 2.48352
[1mStep[0m  [16/42], [94mLoss[0m : 2.40039
[1mStep[0m  [20/42], [94mLoss[0m : 2.19584
[1mStep[0m  [24/42], [94mLoss[0m : 2.43523
[1mStep[0m  [28/42], [94mLoss[0m : 2.43673
[1mStep[0m  [32/42], [94mLoss[0m : 2.26175
[1mStep[0m  [36/42], [94mLoss[0m : 2.49862
[1mStep[0m  [40/42], [94mLoss[0m : 2.47168

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26667
[1mStep[0m  [4/42], [94mLoss[0m : 2.56094
[1mStep[0m  [8/42], [94mLoss[0m : 2.38208
[1mStep[0m  [12/42], [94mLoss[0m : 2.30646
[1mStep[0m  [16/42], [94mLoss[0m : 2.50447
[1mStep[0m  [20/42], [94mLoss[0m : 2.30489
[1mStep[0m  [24/42], [94mLoss[0m : 2.36587
[1mStep[0m  [28/42], [94mLoss[0m : 2.33651
[1mStep[0m  [32/42], [94mLoss[0m : 2.14649
[1mStep[0m  [36/42], [94mLoss[0m : 2.54710
[1mStep[0m  [40/42], [94mLoss[0m : 2.39611

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47617
[1mStep[0m  [4/42], [94mLoss[0m : 2.32787
[1mStep[0m  [8/42], [94mLoss[0m : 2.26114
[1mStep[0m  [12/42], [94mLoss[0m : 2.69698
[1mStep[0m  [16/42], [94mLoss[0m : 2.44356
[1mStep[0m  [20/42], [94mLoss[0m : 2.15695
[1mStep[0m  [24/42], [94mLoss[0m : 2.32135
[1mStep[0m  [28/42], [94mLoss[0m : 2.26819
[1mStep[0m  [32/42], [94mLoss[0m : 2.15261
[1mStep[0m  [36/42], [94mLoss[0m : 2.29518
[1mStep[0m  [40/42], [94mLoss[0m : 2.51837

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57964
[1mStep[0m  [4/42], [94mLoss[0m : 2.53870
[1mStep[0m  [8/42], [94mLoss[0m : 2.19452
[1mStep[0m  [12/42], [94mLoss[0m : 2.28242
[1mStep[0m  [16/42], [94mLoss[0m : 2.19127
[1mStep[0m  [20/42], [94mLoss[0m : 2.51759
[1mStep[0m  [24/42], [94mLoss[0m : 2.42247
[1mStep[0m  [28/42], [94mLoss[0m : 2.29184
[1mStep[0m  [32/42], [94mLoss[0m : 2.33669
[1mStep[0m  [36/42], [94mLoss[0m : 2.28455
[1mStep[0m  [40/42], [94mLoss[0m : 2.61054

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52559
[1mStep[0m  [4/42], [94mLoss[0m : 2.31963
[1mStep[0m  [8/42], [94mLoss[0m : 2.37896
[1mStep[0m  [12/42], [94mLoss[0m : 2.29448
[1mStep[0m  [16/42], [94mLoss[0m : 2.53387
[1mStep[0m  [20/42], [94mLoss[0m : 2.39148
[1mStep[0m  [24/42], [94mLoss[0m : 2.41346
[1mStep[0m  [28/42], [94mLoss[0m : 2.53067
[1mStep[0m  [32/42], [94mLoss[0m : 2.35664
[1mStep[0m  [36/42], [94mLoss[0m : 2.36358
[1mStep[0m  [40/42], [94mLoss[0m : 2.60278

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.3328452791486467
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.29766
[1mStep[0m  [4/42], [94mLoss[0m : 2.55684
[1mStep[0m  [8/42], [94mLoss[0m : 2.53862
[1mStep[0m  [12/42], [94mLoss[0m : 2.49691
[1mStep[0m  [16/42], [94mLoss[0m : 2.30631
[1mStep[0m  [20/42], [94mLoss[0m : 2.40250
[1mStep[0m  [24/42], [94mLoss[0m : 2.70075
[1mStep[0m  [28/42], [94mLoss[0m : 2.61242
[1mStep[0m  [32/42], [94mLoss[0m : 2.55145
[1mStep[0m  [36/42], [94mLoss[0m : 2.44294
[1mStep[0m  [40/42], [94mLoss[0m : 2.43704

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45020
[1mStep[0m  [4/42], [94mLoss[0m : 2.32824
[1mStep[0m  [8/42], [94mLoss[0m : 1.99700
[1mStep[0m  [12/42], [94mLoss[0m : 2.15445
[1mStep[0m  [16/42], [94mLoss[0m : 2.21795
[1mStep[0m  [20/42], [94mLoss[0m : 2.14067
[1mStep[0m  [24/42], [94mLoss[0m : 2.43902
[1mStep[0m  [28/42], [94mLoss[0m : 2.40841
[1mStep[0m  [32/42], [94mLoss[0m : 2.38415
[1mStep[0m  [36/42], [94mLoss[0m : 2.55636
[1mStep[0m  [40/42], [94mLoss[0m : 2.28526

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23574
[1mStep[0m  [4/42], [94mLoss[0m : 2.10536
[1mStep[0m  [8/42], [94mLoss[0m : 2.29156
[1mStep[0m  [12/42], [94mLoss[0m : 2.05189
[1mStep[0m  [16/42], [94mLoss[0m : 2.48589
[1mStep[0m  [20/42], [94mLoss[0m : 2.23277
[1mStep[0m  [24/42], [94mLoss[0m : 2.49403
[1mStep[0m  [28/42], [94mLoss[0m : 2.16747
[1mStep[0m  [32/42], [94mLoss[0m : 2.21420
[1mStep[0m  [36/42], [94mLoss[0m : 2.11622
[1mStep[0m  [40/42], [94mLoss[0m : 2.02393

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23550
[1mStep[0m  [4/42], [94mLoss[0m : 2.16405
[1mStep[0m  [8/42], [94mLoss[0m : 2.02338
[1mStep[0m  [12/42], [94mLoss[0m : 2.12176
[1mStep[0m  [16/42], [94mLoss[0m : 1.96326
[1mStep[0m  [20/42], [94mLoss[0m : 2.06743
[1mStep[0m  [24/42], [94mLoss[0m : 2.13510
[1mStep[0m  [28/42], [94mLoss[0m : 2.24718
[1mStep[0m  [32/42], [94mLoss[0m : 2.17937
[1mStep[0m  [36/42], [94mLoss[0m : 2.10215
[1mStep[0m  [40/42], [94mLoss[0m : 2.32652

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.160, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23221
[1mStep[0m  [4/42], [94mLoss[0m : 2.08244
[1mStep[0m  [8/42], [94mLoss[0m : 2.18749
[1mStep[0m  [12/42], [94mLoss[0m : 1.98510
[1mStep[0m  [16/42], [94mLoss[0m : 2.02733
[1mStep[0m  [20/42], [94mLoss[0m : 2.06259
[1mStep[0m  [24/42], [94mLoss[0m : 2.09937
[1mStep[0m  [28/42], [94mLoss[0m : 2.09991
[1mStep[0m  [32/42], [94mLoss[0m : 2.06353
[1mStep[0m  [36/42], [94mLoss[0m : 2.26848
[1mStep[0m  [40/42], [94mLoss[0m : 2.01057

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27088
[1mStep[0m  [4/42], [94mLoss[0m : 2.07106
[1mStep[0m  [8/42], [94mLoss[0m : 2.01621
[1mStep[0m  [12/42], [94mLoss[0m : 2.06285
[1mStep[0m  [16/42], [94mLoss[0m : 1.77434
[1mStep[0m  [20/42], [94mLoss[0m : 1.99294
[1mStep[0m  [24/42], [94mLoss[0m : 1.86835
[1mStep[0m  [28/42], [94mLoss[0m : 2.00680
[1mStep[0m  [32/42], [94mLoss[0m : 2.07569
[1mStep[0m  [36/42], [94mLoss[0m : 1.97853
[1mStep[0m  [40/42], [94mLoss[0m : 2.00974

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99384
[1mStep[0m  [4/42], [94mLoss[0m : 1.89235
[1mStep[0m  [8/42], [94mLoss[0m : 1.94967
[1mStep[0m  [12/42], [94mLoss[0m : 1.90645
[1mStep[0m  [16/42], [94mLoss[0m : 2.03344
[1mStep[0m  [20/42], [94mLoss[0m : 1.88533
[1mStep[0m  [24/42], [94mLoss[0m : 1.93344
[1mStep[0m  [28/42], [94mLoss[0m : 1.91209
[1mStep[0m  [32/42], [94mLoss[0m : 2.01947
[1mStep[0m  [36/42], [94mLoss[0m : 2.00352
[1mStep[0m  [40/42], [94mLoss[0m : 2.02020

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66106
[1mStep[0m  [4/42], [94mLoss[0m : 1.76159
[1mStep[0m  [8/42], [94mLoss[0m : 2.00207
[1mStep[0m  [12/42], [94mLoss[0m : 1.91203
[1mStep[0m  [16/42], [94mLoss[0m : 1.74152
[1mStep[0m  [20/42], [94mLoss[0m : 2.02520
[1mStep[0m  [24/42], [94mLoss[0m : 1.91932
[1mStep[0m  [28/42], [94mLoss[0m : 1.97069
[1mStep[0m  [32/42], [94mLoss[0m : 2.14885
[1mStep[0m  [36/42], [94mLoss[0m : 2.01534
[1mStep[0m  [40/42], [94mLoss[0m : 1.78637

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01737
[1mStep[0m  [4/42], [94mLoss[0m : 1.85008
[1mStep[0m  [8/42], [94mLoss[0m : 1.65565
[1mStep[0m  [12/42], [94mLoss[0m : 1.79753
[1mStep[0m  [16/42], [94mLoss[0m : 1.85840
[1mStep[0m  [20/42], [94mLoss[0m : 1.72830
[1mStep[0m  [24/42], [94mLoss[0m : 1.81379
[1mStep[0m  [28/42], [94mLoss[0m : 1.88541
[1mStep[0m  [32/42], [94mLoss[0m : 1.73837
[1mStep[0m  [36/42], [94mLoss[0m : 1.87049
[1mStep[0m  [40/42], [94mLoss[0m : 1.97098

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71546
[1mStep[0m  [4/42], [94mLoss[0m : 1.81035
[1mStep[0m  [8/42], [94mLoss[0m : 1.85264
[1mStep[0m  [12/42], [94mLoss[0m : 1.86654
[1mStep[0m  [16/42], [94mLoss[0m : 1.60948
[1mStep[0m  [20/42], [94mLoss[0m : 2.05369
[1mStep[0m  [24/42], [94mLoss[0m : 1.76996
[1mStep[0m  [28/42], [94mLoss[0m : 1.77654
[1mStep[0m  [32/42], [94mLoss[0m : 1.67464
[1mStep[0m  [36/42], [94mLoss[0m : 2.18631
[1mStep[0m  [40/42], [94mLoss[0m : 1.73628

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.75352
[1mStep[0m  [4/42], [94mLoss[0m : 1.63172
[1mStep[0m  [8/42], [94mLoss[0m : 1.80304
[1mStep[0m  [12/42], [94mLoss[0m : 1.99495
[1mStep[0m  [16/42], [94mLoss[0m : 1.78060
[1mStep[0m  [20/42], [94mLoss[0m : 1.74707
[1mStep[0m  [24/42], [94mLoss[0m : 1.83287
[1mStep[0m  [28/42], [94mLoss[0m : 1.65020
[1mStep[0m  [32/42], [94mLoss[0m : 1.91261
[1mStep[0m  [36/42], [94mLoss[0m : 1.58781
[1mStep[0m  [40/42], [94mLoss[0m : 1.74942

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.60203
[1mStep[0m  [4/42], [94mLoss[0m : 1.77367
[1mStep[0m  [8/42], [94mLoss[0m : 1.64776
[1mStep[0m  [12/42], [94mLoss[0m : 1.61642
[1mStep[0m  [16/42], [94mLoss[0m : 1.63600
[1mStep[0m  [20/42], [94mLoss[0m : 1.78193
[1mStep[0m  [24/42], [94mLoss[0m : 1.77471
[1mStep[0m  [28/42], [94mLoss[0m : 1.82799
[1mStep[0m  [32/42], [94mLoss[0m : 1.83846
[1mStep[0m  [36/42], [94mLoss[0m : 1.80416
[1mStep[0m  [40/42], [94mLoss[0m : 1.65156

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.690, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.38702
[1mStep[0m  [4/42], [94mLoss[0m : 1.54267
[1mStep[0m  [8/42], [94mLoss[0m : 1.67625
[1mStep[0m  [12/42], [94mLoss[0m : 1.68149
[1mStep[0m  [16/42], [94mLoss[0m : 1.64726
[1mStep[0m  [20/42], [94mLoss[0m : 1.63659
[1mStep[0m  [24/42], [94mLoss[0m : 1.60296
[1mStep[0m  [28/42], [94mLoss[0m : 1.73914
[1mStep[0m  [32/42], [94mLoss[0m : 1.68857
[1mStep[0m  [36/42], [94mLoss[0m : 1.50420
[1mStep[0m  [40/42], [94mLoss[0m : 1.68019

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49435
[1mStep[0m  [4/42], [94mLoss[0m : 1.55617
[1mStep[0m  [8/42], [94mLoss[0m : 1.77469
[1mStep[0m  [12/42], [94mLoss[0m : 1.56054
[1mStep[0m  [16/42], [94mLoss[0m : 1.66749
[1mStep[0m  [20/42], [94mLoss[0m : 1.56038
[1mStep[0m  [24/42], [94mLoss[0m : 1.51014
[1mStep[0m  [28/42], [94mLoss[0m : 1.57826
[1mStep[0m  [32/42], [94mLoss[0m : 1.73720
[1mStep[0m  [36/42], [94mLoss[0m : 1.70762
[1mStep[0m  [40/42], [94mLoss[0m : 1.72047

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66448
[1mStep[0m  [4/42], [94mLoss[0m : 1.43480
[1mStep[0m  [8/42], [94mLoss[0m : 1.44446
[1mStep[0m  [12/42], [94mLoss[0m : 1.46462
[1mStep[0m  [16/42], [94mLoss[0m : 1.59850
[1mStep[0m  [20/42], [94mLoss[0m : 1.67013
[1mStep[0m  [24/42], [94mLoss[0m : 1.52584
[1mStep[0m  [28/42], [94mLoss[0m : 1.59942
[1mStep[0m  [32/42], [94mLoss[0m : 1.55188
[1mStep[0m  [36/42], [94mLoss[0m : 1.56699
[1mStep[0m  [40/42], [94mLoss[0m : 1.40878

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.559, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45590
[1mStep[0m  [4/42], [94mLoss[0m : 1.37603
[1mStep[0m  [8/42], [94mLoss[0m : 1.56444
[1mStep[0m  [12/42], [94mLoss[0m : 1.50852
[1mStep[0m  [16/42], [94mLoss[0m : 1.56417
[1mStep[0m  [20/42], [94mLoss[0m : 1.56894
[1mStep[0m  [24/42], [94mLoss[0m : 1.57894
[1mStep[0m  [28/42], [94mLoss[0m : 1.49089
[1mStep[0m  [32/42], [94mLoss[0m : 1.62198
[1mStep[0m  [36/42], [94mLoss[0m : 1.60869
[1mStep[0m  [40/42], [94mLoss[0m : 1.59144

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54113
[1mStep[0m  [4/42], [94mLoss[0m : 1.47587
[1mStep[0m  [8/42], [94mLoss[0m : 1.48297
[1mStep[0m  [12/42], [94mLoss[0m : 1.42537
[1mStep[0m  [16/42], [94mLoss[0m : 1.46468
[1mStep[0m  [20/42], [94mLoss[0m : 1.45210
[1mStep[0m  [24/42], [94mLoss[0m : 1.45594
[1mStep[0m  [28/42], [94mLoss[0m : 1.50553
[1mStep[0m  [32/42], [94mLoss[0m : 1.50607
[1mStep[0m  [36/42], [94mLoss[0m : 1.51460
[1mStep[0m  [40/42], [94mLoss[0m : 1.40556

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42432
[1mStep[0m  [4/42], [94mLoss[0m : 1.51259
[1mStep[0m  [8/42], [94mLoss[0m : 1.36819
[1mStep[0m  [12/42], [94mLoss[0m : 1.38679
[1mStep[0m  [16/42], [94mLoss[0m : 1.42616
[1mStep[0m  [20/42], [94mLoss[0m : 1.48617
[1mStep[0m  [24/42], [94mLoss[0m : 1.48869
[1mStep[0m  [28/42], [94mLoss[0m : 1.41529
[1mStep[0m  [32/42], [94mLoss[0m : 1.55136
[1mStep[0m  [36/42], [94mLoss[0m : 1.52192
[1mStep[0m  [40/42], [94mLoss[0m : 1.48391

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.456, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36192
[1mStep[0m  [4/42], [94mLoss[0m : 1.37107
[1mStep[0m  [8/42], [94mLoss[0m : 1.46262
[1mStep[0m  [12/42], [94mLoss[0m : 1.41145
[1mStep[0m  [16/42], [94mLoss[0m : 1.44849
[1mStep[0m  [20/42], [94mLoss[0m : 1.46873
[1mStep[0m  [24/42], [94mLoss[0m : 1.47548
[1mStep[0m  [28/42], [94mLoss[0m : 1.40226
[1mStep[0m  [32/42], [94mLoss[0m : 1.37578
[1mStep[0m  [36/42], [94mLoss[0m : 1.40566
[1mStep[0m  [40/42], [94mLoss[0m : 1.45994

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.438, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.36376
[1mStep[0m  [4/42], [94mLoss[0m : 1.46657
[1mStep[0m  [8/42], [94mLoss[0m : 1.43712
[1mStep[0m  [12/42], [94mLoss[0m : 1.32166
[1mStep[0m  [16/42], [94mLoss[0m : 1.32357
[1mStep[0m  [20/42], [94mLoss[0m : 1.38000
[1mStep[0m  [24/42], [94mLoss[0m : 1.49168
[1mStep[0m  [28/42], [94mLoss[0m : 1.50573
[1mStep[0m  [32/42], [94mLoss[0m : 1.50820
[1mStep[0m  [36/42], [94mLoss[0m : 1.54792
[1mStep[0m  [40/42], [94mLoss[0m : 1.46813

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 19 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.524
====================================

Phase 2 - Evaluation MAE:  2.5236633334841048
MAE score P1      2.332845
MAE score P2      2.523663
loss                1.4231
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 17, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.96802
[1mStep[0m  [4/42], [94mLoss[0m : 10.97709
[1mStep[0m  [8/42], [94mLoss[0m : 10.67540
[1mStep[0m  [12/42], [94mLoss[0m : 10.88460
[1mStep[0m  [16/42], [94mLoss[0m : 10.87119
[1mStep[0m  [20/42], [94mLoss[0m : 10.95518
[1mStep[0m  [24/42], [94mLoss[0m : 10.97547
[1mStep[0m  [28/42], [94mLoss[0m : 10.33055
[1mStep[0m  [32/42], [94mLoss[0m : 10.80992
[1mStep[0m  [36/42], [94mLoss[0m : 10.65364
[1mStep[0m  [40/42], [94mLoss[0m : 10.15890

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.706, [92mTest[0m: 10.915, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.56893
[1mStep[0m  [4/42], [94mLoss[0m : 10.69256
[1mStep[0m  [8/42], [94mLoss[0m : 10.28064
[1mStep[0m  [12/42], [94mLoss[0m : 10.21738
[1mStep[0m  [16/42], [94mLoss[0m : 10.39074
[1mStep[0m  [20/42], [94mLoss[0m : 10.50783
[1mStep[0m  [24/42], [94mLoss[0m : 9.85761
[1mStep[0m  [28/42], [94mLoss[0m : 10.29021
[1mStep[0m  [32/42], [94mLoss[0m : 10.27312
[1mStep[0m  [36/42], [94mLoss[0m : 10.32026
[1mStep[0m  [40/42], [94mLoss[0m : 10.18032

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.303, [92mTest[0m: 10.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.68096
[1mStep[0m  [4/42], [94mLoss[0m : 10.27094
[1mStep[0m  [8/42], [94mLoss[0m : 9.49411
[1mStep[0m  [12/42], [94mLoss[0m : 9.80702
[1mStep[0m  [16/42], [94mLoss[0m : 9.86162
[1mStep[0m  [20/42], [94mLoss[0m : 9.72683
[1mStep[0m  [24/42], [94mLoss[0m : 9.91713
[1mStep[0m  [28/42], [94mLoss[0m : 9.73106
[1mStep[0m  [32/42], [94mLoss[0m : 9.21730
[1mStep[0m  [36/42], [94mLoss[0m : 9.78141
[1mStep[0m  [40/42], [94mLoss[0m : 10.07599

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.844, [92mTest[0m: 9.762, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.58022
[1mStep[0m  [4/42], [94mLoss[0m : 9.47787
[1mStep[0m  [8/42], [94mLoss[0m : 9.29827
[1mStep[0m  [12/42], [94mLoss[0m : 9.52659
[1mStep[0m  [16/42], [94mLoss[0m : 9.17021
[1mStep[0m  [20/42], [94mLoss[0m : 9.80808
[1mStep[0m  [24/42], [94mLoss[0m : 9.32585
[1mStep[0m  [28/42], [94mLoss[0m : 9.54914
[1mStep[0m  [32/42], [94mLoss[0m : 9.07469
[1mStep[0m  [36/42], [94mLoss[0m : 9.02681
[1mStep[0m  [40/42], [94mLoss[0m : 8.90037

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.315, [92mTest[0m: 9.218, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.02073
[1mStep[0m  [4/42], [94mLoss[0m : 8.90490
[1mStep[0m  [8/42], [94mLoss[0m : 8.77344
[1mStep[0m  [12/42], [94mLoss[0m : 8.80582
[1mStep[0m  [16/42], [94mLoss[0m : 9.40879
[1mStep[0m  [20/42], [94mLoss[0m : 8.76238
[1mStep[0m  [24/42], [94mLoss[0m : 8.61445
[1mStep[0m  [28/42], [94mLoss[0m : 8.38424
[1mStep[0m  [32/42], [94mLoss[0m : 8.61018
[1mStep[0m  [36/42], [94mLoss[0m : 8.62698
[1mStep[0m  [40/42], [94mLoss[0m : 8.49362

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.739, [92mTest[0m: 8.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34663
[1mStep[0m  [4/42], [94mLoss[0m : 8.14080
[1mStep[0m  [8/42], [94mLoss[0m : 8.01631
[1mStep[0m  [12/42], [94mLoss[0m : 8.27401
[1mStep[0m  [16/42], [94mLoss[0m : 8.51302
[1mStep[0m  [20/42], [94mLoss[0m : 8.24964
[1mStep[0m  [24/42], [94mLoss[0m : 8.23865
[1mStep[0m  [28/42], [94mLoss[0m : 8.15358
[1mStep[0m  [32/42], [94mLoss[0m : 8.01756
[1mStep[0m  [36/42], [94mLoss[0m : 8.18866
[1mStep[0m  [40/42], [94mLoss[0m : 7.51972

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.149, [92mTest[0m: 7.790, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.91910
[1mStep[0m  [4/42], [94mLoss[0m : 7.73461
[1mStep[0m  [8/42], [94mLoss[0m : 7.93065
[1mStep[0m  [12/42], [94mLoss[0m : 7.75509
[1mStep[0m  [16/42], [94mLoss[0m : 7.56580
[1mStep[0m  [20/42], [94mLoss[0m : 7.63513
[1mStep[0m  [24/42], [94mLoss[0m : 7.74325
[1mStep[0m  [28/42], [94mLoss[0m : 7.19811
[1mStep[0m  [32/42], [94mLoss[0m : 8.01219
[1mStep[0m  [36/42], [94mLoss[0m : 7.11958
[1mStep[0m  [40/42], [94mLoss[0m : 7.42376

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.604, [92mTest[0m: 7.068, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.23891
[1mStep[0m  [4/42], [94mLoss[0m : 7.47481
[1mStep[0m  [8/42], [94mLoss[0m : 7.16038
[1mStep[0m  [12/42], [94mLoss[0m : 7.10440
[1mStep[0m  [16/42], [94mLoss[0m : 7.07391
[1mStep[0m  [20/42], [94mLoss[0m : 7.01802
[1mStep[0m  [24/42], [94mLoss[0m : 7.25007
[1mStep[0m  [28/42], [94mLoss[0m : 7.38718
[1mStep[0m  [32/42], [94mLoss[0m : 7.31240
[1mStep[0m  [36/42], [94mLoss[0m : 6.74109
[1mStep[0m  [40/42], [94mLoss[0m : 6.74032

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.105, [92mTest[0m: 6.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.61981
[1mStep[0m  [4/42], [94mLoss[0m : 6.89826
[1mStep[0m  [8/42], [94mLoss[0m : 6.45566
[1mStep[0m  [12/42], [94mLoss[0m : 6.91293
[1mStep[0m  [16/42], [94mLoss[0m : 7.00168
[1mStep[0m  [20/42], [94mLoss[0m : 6.84668
[1mStep[0m  [24/42], [94mLoss[0m : 6.60318
[1mStep[0m  [28/42], [94mLoss[0m : 6.48689
[1mStep[0m  [32/42], [94mLoss[0m : 6.59924
[1mStep[0m  [36/42], [94mLoss[0m : 6.73599
[1mStep[0m  [40/42], [94mLoss[0m : 6.73895

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.678, [92mTest[0m: 5.962, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.39674
[1mStep[0m  [4/42], [94mLoss[0m : 6.48174
[1mStep[0m  [8/42], [94mLoss[0m : 6.36891
[1mStep[0m  [12/42], [94mLoss[0m : 6.00857
[1mStep[0m  [16/42], [94mLoss[0m : 6.02345
[1mStep[0m  [20/42], [94mLoss[0m : 6.22973
[1mStep[0m  [24/42], [94mLoss[0m : 5.96229
[1mStep[0m  [28/42], [94mLoss[0m : 6.16485
[1mStep[0m  [32/42], [94mLoss[0m : 6.05839
[1mStep[0m  [36/42], [94mLoss[0m : 6.00259
[1mStep[0m  [40/42], [94mLoss[0m : 5.66385

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.216, [92mTest[0m: 5.591, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.10775
[1mStep[0m  [4/42], [94mLoss[0m : 6.09017
[1mStep[0m  [8/42], [94mLoss[0m : 6.11112
[1mStep[0m  [12/42], [94mLoss[0m : 5.89930
[1mStep[0m  [16/42], [94mLoss[0m : 5.80661
[1mStep[0m  [20/42], [94mLoss[0m : 5.50499
[1mStep[0m  [24/42], [94mLoss[0m : 5.53271
[1mStep[0m  [28/42], [94mLoss[0m : 5.97964
[1mStep[0m  [32/42], [94mLoss[0m : 5.75845
[1mStep[0m  [36/42], [94mLoss[0m : 6.04116
[1mStep[0m  [40/42], [94mLoss[0m : 5.72072

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.743, [92mTest[0m: 5.049, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.65421
[1mStep[0m  [4/42], [94mLoss[0m : 5.62601
[1mStep[0m  [8/42], [94mLoss[0m : 5.63757
[1mStep[0m  [12/42], [94mLoss[0m : 5.60486
[1mStep[0m  [16/42], [94mLoss[0m : 5.37503
[1mStep[0m  [20/42], [94mLoss[0m : 5.17435
[1mStep[0m  [24/42], [94mLoss[0m : 5.11885
[1mStep[0m  [28/42], [94mLoss[0m : 5.44689
[1mStep[0m  [32/42], [94mLoss[0m : 4.77567
[1mStep[0m  [36/42], [94mLoss[0m : 5.06782
[1mStep[0m  [40/42], [94mLoss[0m : 4.71483

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.263, [92mTest[0m: 4.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.48865
[1mStep[0m  [4/42], [94mLoss[0m : 5.26550
[1mStep[0m  [8/42], [94mLoss[0m : 4.73245
[1mStep[0m  [12/42], [94mLoss[0m : 5.04685
[1mStep[0m  [16/42], [94mLoss[0m : 4.66720
[1mStep[0m  [20/42], [94mLoss[0m : 5.25176
[1mStep[0m  [24/42], [94mLoss[0m : 5.00474
[1mStep[0m  [28/42], [94mLoss[0m : 4.44404
[1mStep[0m  [32/42], [94mLoss[0m : 4.40559
[1mStep[0m  [36/42], [94mLoss[0m : 4.11489
[1mStep[0m  [40/42], [94mLoss[0m : 4.36259

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.718, [92mTest[0m: 4.012, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.25205
[1mStep[0m  [4/42], [94mLoss[0m : 4.40265
[1mStep[0m  [8/42], [94mLoss[0m : 4.32989
[1mStep[0m  [12/42], [94mLoss[0m : 4.54959
[1mStep[0m  [16/42], [94mLoss[0m : 4.42465
[1mStep[0m  [20/42], [94mLoss[0m : 3.95516
[1mStep[0m  [24/42], [94mLoss[0m : 4.05455
[1mStep[0m  [28/42], [94mLoss[0m : 4.14503
[1mStep[0m  [32/42], [94mLoss[0m : 4.00104
[1mStep[0m  [36/42], [94mLoss[0m : 4.10875
[1mStep[0m  [40/42], [94mLoss[0m : 3.75333

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.175, [92mTest[0m: 3.621, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.05884
[1mStep[0m  [4/42], [94mLoss[0m : 3.97428
[1mStep[0m  [8/42], [94mLoss[0m : 3.58018
[1mStep[0m  [12/42], [94mLoss[0m : 3.60619
[1mStep[0m  [16/42], [94mLoss[0m : 3.91803
[1mStep[0m  [20/42], [94mLoss[0m : 3.53088
[1mStep[0m  [24/42], [94mLoss[0m : 3.97681
[1mStep[0m  [28/42], [94mLoss[0m : 3.61041
[1mStep[0m  [32/42], [94mLoss[0m : 3.45000
[1mStep[0m  [36/42], [94mLoss[0m : 3.44225
[1mStep[0m  [40/42], [94mLoss[0m : 3.60532

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.640, [92mTest[0m: 3.151, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.37416
[1mStep[0m  [4/42], [94mLoss[0m : 3.45337
[1mStep[0m  [8/42], [94mLoss[0m : 2.82633
[1mStep[0m  [12/42], [94mLoss[0m : 3.45080
[1mStep[0m  [16/42], [94mLoss[0m : 3.30138
[1mStep[0m  [20/42], [94mLoss[0m : 3.33316
[1mStep[0m  [24/42], [94mLoss[0m : 2.95814
[1mStep[0m  [28/42], [94mLoss[0m : 3.07328
[1mStep[0m  [32/42], [94mLoss[0m : 3.17882
[1mStep[0m  [36/42], [94mLoss[0m : 3.22280
[1mStep[0m  [40/42], [94mLoss[0m : 2.89601

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.168, [92mTest[0m: 2.826, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82911
[1mStep[0m  [4/42], [94mLoss[0m : 2.75605
[1mStep[0m  [8/42], [94mLoss[0m : 2.77007
[1mStep[0m  [12/42], [94mLoss[0m : 2.85271
[1mStep[0m  [16/42], [94mLoss[0m : 2.54863
[1mStep[0m  [20/42], [94mLoss[0m : 2.76361
[1mStep[0m  [24/42], [94mLoss[0m : 2.70053
[1mStep[0m  [28/42], [94mLoss[0m : 2.88278
[1mStep[0m  [32/42], [94mLoss[0m : 2.71132
[1mStep[0m  [36/42], [94mLoss[0m : 2.70669
[1mStep[0m  [40/42], [94mLoss[0m : 2.94654

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.859, [92mTest[0m: 2.543, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.96216
[1mStep[0m  [4/42], [94mLoss[0m : 2.73391
[1mStep[0m  [8/42], [94mLoss[0m : 2.71638
[1mStep[0m  [12/42], [94mLoss[0m : 2.47220
[1mStep[0m  [16/42], [94mLoss[0m : 2.56079
[1mStep[0m  [20/42], [94mLoss[0m : 2.82993
[1mStep[0m  [24/42], [94mLoss[0m : 2.48279
[1mStep[0m  [28/42], [94mLoss[0m : 2.80902
[1mStep[0m  [32/42], [94mLoss[0m : 2.61292
[1mStep[0m  [36/42], [94mLoss[0m : 2.72345
[1mStep[0m  [40/42], [94mLoss[0m : 2.56069

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51439
[1mStep[0m  [4/42], [94mLoss[0m : 2.29589
[1mStep[0m  [8/42], [94mLoss[0m : 2.65327
[1mStep[0m  [12/42], [94mLoss[0m : 2.72985
[1mStep[0m  [16/42], [94mLoss[0m : 2.53446
[1mStep[0m  [20/42], [94mLoss[0m : 2.64270
[1mStep[0m  [24/42], [94mLoss[0m : 2.33571
[1mStep[0m  [28/42], [94mLoss[0m : 2.43196
[1mStep[0m  [32/42], [94mLoss[0m : 2.56356
[1mStep[0m  [36/42], [94mLoss[0m : 2.38006
[1mStep[0m  [40/42], [94mLoss[0m : 2.59233

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35072
[1mStep[0m  [4/42], [94mLoss[0m : 2.78085
[1mStep[0m  [8/42], [94mLoss[0m : 2.55977
[1mStep[0m  [12/42], [94mLoss[0m : 2.78044
[1mStep[0m  [16/42], [94mLoss[0m : 2.70404
[1mStep[0m  [20/42], [94mLoss[0m : 2.64258
[1mStep[0m  [24/42], [94mLoss[0m : 2.64499
[1mStep[0m  [28/42], [94mLoss[0m : 2.54472
[1mStep[0m  [32/42], [94mLoss[0m : 2.76037
[1mStep[0m  [36/42], [94mLoss[0m : 2.52165
[1mStep[0m  [40/42], [94mLoss[0m : 2.49415

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63981
[1mStep[0m  [4/42], [94mLoss[0m : 2.50960
[1mStep[0m  [8/42], [94mLoss[0m : 2.67268
[1mStep[0m  [12/42], [94mLoss[0m : 2.46172
[1mStep[0m  [16/42], [94mLoss[0m : 2.70118
[1mStep[0m  [20/42], [94mLoss[0m : 2.38854
[1mStep[0m  [24/42], [94mLoss[0m : 2.67486
[1mStep[0m  [28/42], [94mLoss[0m : 2.26368
[1mStep[0m  [32/42], [94mLoss[0m : 2.52002
[1mStep[0m  [36/42], [94mLoss[0m : 2.48607
[1mStep[0m  [40/42], [94mLoss[0m : 2.56661

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.389, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41955
[1mStep[0m  [4/42], [94mLoss[0m : 2.66720
[1mStep[0m  [8/42], [94mLoss[0m : 2.52172
[1mStep[0m  [12/42], [94mLoss[0m : 2.65669
[1mStep[0m  [16/42], [94mLoss[0m : 2.71317
[1mStep[0m  [20/42], [94mLoss[0m : 2.55965
[1mStep[0m  [24/42], [94mLoss[0m : 2.62664
[1mStep[0m  [28/42], [94mLoss[0m : 2.55791
[1mStep[0m  [32/42], [94mLoss[0m : 2.45511
[1mStep[0m  [36/42], [94mLoss[0m : 2.55568
[1mStep[0m  [40/42], [94mLoss[0m : 2.66266

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69596
[1mStep[0m  [4/42], [94mLoss[0m : 2.53994
[1mStep[0m  [8/42], [94mLoss[0m : 2.44249
[1mStep[0m  [12/42], [94mLoss[0m : 2.66216
[1mStep[0m  [16/42], [94mLoss[0m : 2.55150
[1mStep[0m  [20/42], [94mLoss[0m : 2.57051
[1mStep[0m  [24/42], [94mLoss[0m : 2.49172
[1mStep[0m  [28/42], [94mLoss[0m : 2.60472
[1mStep[0m  [32/42], [94mLoss[0m : 2.67202
[1mStep[0m  [36/42], [94mLoss[0m : 2.65065
[1mStep[0m  [40/42], [94mLoss[0m : 2.64066

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47344
[1mStep[0m  [4/42], [94mLoss[0m : 2.48486
[1mStep[0m  [8/42], [94mLoss[0m : 2.47400
[1mStep[0m  [12/42], [94mLoss[0m : 2.41943
[1mStep[0m  [16/42], [94mLoss[0m : 2.31186
[1mStep[0m  [20/42], [94mLoss[0m : 2.70824
[1mStep[0m  [24/42], [94mLoss[0m : 2.78342
[1mStep[0m  [28/42], [94mLoss[0m : 2.50244
[1mStep[0m  [32/42], [94mLoss[0m : 2.71109
[1mStep[0m  [36/42], [94mLoss[0m : 2.45314
[1mStep[0m  [40/42], [94mLoss[0m : 2.43299

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75322
[1mStep[0m  [4/42], [94mLoss[0m : 2.48086
[1mStep[0m  [8/42], [94mLoss[0m : 2.29559
[1mStep[0m  [12/42], [94mLoss[0m : 2.44662
[1mStep[0m  [16/42], [94mLoss[0m : 2.37247
[1mStep[0m  [20/42], [94mLoss[0m : 2.58517
[1mStep[0m  [24/42], [94mLoss[0m : 2.65511
[1mStep[0m  [28/42], [94mLoss[0m : 2.45415
[1mStep[0m  [32/42], [94mLoss[0m : 2.70410
[1mStep[0m  [36/42], [94mLoss[0m : 2.32836
[1mStep[0m  [40/42], [94mLoss[0m : 2.62672

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.390, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48317
[1mStep[0m  [4/42], [94mLoss[0m : 2.47655
[1mStep[0m  [8/42], [94mLoss[0m : 2.66875
[1mStep[0m  [12/42], [94mLoss[0m : 2.37863
[1mStep[0m  [16/42], [94mLoss[0m : 2.26662
[1mStep[0m  [20/42], [94mLoss[0m : 2.95580
[1mStep[0m  [24/42], [94mLoss[0m : 2.38142
[1mStep[0m  [28/42], [94mLoss[0m : 2.58633
[1mStep[0m  [32/42], [94mLoss[0m : 2.50233
[1mStep[0m  [36/42], [94mLoss[0m : 2.52405
[1mStep[0m  [40/42], [94mLoss[0m : 2.43937

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38888
[1mStep[0m  [4/42], [94mLoss[0m : 2.54698
[1mStep[0m  [8/42], [94mLoss[0m : 2.55420
[1mStep[0m  [12/42], [94mLoss[0m : 2.52458
[1mStep[0m  [16/42], [94mLoss[0m : 2.87852
[1mStep[0m  [20/42], [94mLoss[0m : 2.35587
[1mStep[0m  [24/42], [94mLoss[0m : 2.57921
[1mStep[0m  [28/42], [94mLoss[0m : 2.55596
[1mStep[0m  [32/42], [94mLoss[0m : 2.40123
[1mStep[0m  [36/42], [94mLoss[0m : 2.50788
[1mStep[0m  [40/42], [94mLoss[0m : 2.69186

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61879
[1mStep[0m  [4/42], [94mLoss[0m : 2.52626
[1mStep[0m  [8/42], [94mLoss[0m : 2.79689
[1mStep[0m  [12/42], [94mLoss[0m : 2.33006
[1mStep[0m  [16/42], [94mLoss[0m : 2.42818
[1mStep[0m  [20/42], [94mLoss[0m : 2.38176
[1mStep[0m  [24/42], [94mLoss[0m : 2.53440
[1mStep[0m  [28/42], [94mLoss[0m : 2.40105
[1mStep[0m  [32/42], [94mLoss[0m : 2.51796
[1mStep[0m  [36/42], [94mLoss[0m : 2.22142
[1mStep[0m  [40/42], [94mLoss[0m : 2.58607

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57757
[1mStep[0m  [4/42], [94mLoss[0m : 2.57623
[1mStep[0m  [8/42], [94mLoss[0m : 2.46266
[1mStep[0m  [12/42], [94mLoss[0m : 2.35679
[1mStep[0m  [16/42], [94mLoss[0m : 2.44175
[1mStep[0m  [20/42], [94mLoss[0m : 2.67802
[1mStep[0m  [24/42], [94mLoss[0m : 2.72763
[1mStep[0m  [28/42], [94mLoss[0m : 2.64154
[1mStep[0m  [32/42], [94mLoss[0m : 2.34455
[1mStep[0m  [36/42], [94mLoss[0m : 2.32817
[1mStep[0m  [40/42], [94mLoss[0m : 2.56187

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35578
[1mStep[0m  [4/42], [94mLoss[0m : 2.49688
[1mStep[0m  [8/42], [94mLoss[0m : 2.59090
[1mStep[0m  [12/42], [94mLoss[0m : 2.62873
[1mStep[0m  [16/42], [94mLoss[0m : 2.40985
[1mStep[0m  [20/42], [94mLoss[0m : 2.45442
[1mStep[0m  [24/42], [94mLoss[0m : 2.77706
[1mStep[0m  [28/42], [94mLoss[0m : 2.45781
[1mStep[0m  [32/42], [94mLoss[0m : 2.37975
[1mStep[0m  [36/42], [94mLoss[0m : 2.76945
[1mStep[0m  [40/42], [94mLoss[0m : 2.43427

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.395
====================================

Phase 1 - Evaluation MAE:  2.3948393719536916
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.70776
[1mStep[0m  [4/42], [94mLoss[0m : 2.59329
[1mStep[0m  [8/42], [94mLoss[0m : 2.59166
[1mStep[0m  [12/42], [94mLoss[0m : 2.56671
[1mStep[0m  [16/42], [94mLoss[0m : 2.41881
[1mStep[0m  [20/42], [94mLoss[0m : 2.42731
[1mStep[0m  [24/42], [94mLoss[0m : 2.73686
[1mStep[0m  [28/42], [94mLoss[0m : 2.53590
[1mStep[0m  [32/42], [94mLoss[0m : 2.55833
[1mStep[0m  [36/42], [94mLoss[0m : 2.52577
[1mStep[0m  [40/42], [94mLoss[0m : 2.57777

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63217
[1mStep[0m  [4/42], [94mLoss[0m : 2.63173
[1mStep[0m  [8/42], [94mLoss[0m : 2.52226
[1mStep[0m  [12/42], [94mLoss[0m : 2.45635
[1mStep[0m  [16/42], [94mLoss[0m : 2.45452
[1mStep[0m  [20/42], [94mLoss[0m : 2.42278
[1mStep[0m  [24/42], [94mLoss[0m : 2.64872
[1mStep[0m  [28/42], [94mLoss[0m : 2.60260
[1mStep[0m  [32/42], [94mLoss[0m : 2.74738
[1mStep[0m  [36/42], [94mLoss[0m : 2.37188
[1mStep[0m  [40/42], [94mLoss[0m : 2.44131

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27663
[1mStep[0m  [4/42], [94mLoss[0m : 2.38118
[1mStep[0m  [8/42], [94mLoss[0m : 2.59822
[1mStep[0m  [12/42], [94mLoss[0m : 2.61368
[1mStep[0m  [16/42], [94mLoss[0m : 2.59948
[1mStep[0m  [20/42], [94mLoss[0m : 2.22082
[1mStep[0m  [24/42], [94mLoss[0m : 2.41917
[1mStep[0m  [28/42], [94mLoss[0m : 2.25714
[1mStep[0m  [32/42], [94mLoss[0m : 2.41806
[1mStep[0m  [36/42], [94mLoss[0m : 2.31498
[1mStep[0m  [40/42], [94mLoss[0m : 2.39723

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46118
[1mStep[0m  [4/42], [94mLoss[0m : 2.43404
[1mStep[0m  [8/42], [94mLoss[0m : 2.39719
[1mStep[0m  [12/42], [94mLoss[0m : 2.55953
[1mStep[0m  [16/42], [94mLoss[0m : 2.52570
[1mStep[0m  [20/42], [94mLoss[0m : 2.35343
[1mStep[0m  [24/42], [94mLoss[0m : 2.31621
[1mStep[0m  [28/42], [94mLoss[0m : 2.54332
[1mStep[0m  [32/42], [94mLoss[0m : 2.54605
[1mStep[0m  [36/42], [94mLoss[0m : 2.38340
[1mStep[0m  [40/42], [94mLoss[0m : 2.28033

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.531, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53515
[1mStep[0m  [4/42], [94mLoss[0m : 2.39559
[1mStep[0m  [8/42], [94mLoss[0m : 2.33206
[1mStep[0m  [12/42], [94mLoss[0m : 2.47711
[1mStep[0m  [16/42], [94mLoss[0m : 2.57753
[1mStep[0m  [20/42], [94mLoss[0m : 2.75864
[1mStep[0m  [24/42], [94mLoss[0m : 2.45959
[1mStep[0m  [28/42], [94mLoss[0m : 2.64698
[1mStep[0m  [32/42], [94mLoss[0m : 2.29883
[1mStep[0m  [36/42], [94mLoss[0m : 2.31920
[1mStep[0m  [40/42], [94mLoss[0m : 2.51084

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.629, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42378
[1mStep[0m  [4/42], [94mLoss[0m : 2.29467
[1mStep[0m  [8/42], [94mLoss[0m : 2.40925
[1mStep[0m  [12/42], [94mLoss[0m : 2.34901
[1mStep[0m  [16/42], [94mLoss[0m : 2.26866
[1mStep[0m  [20/42], [94mLoss[0m : 2.51881
[1mStep[0m  [24/42], [94mLoss[0m : 2.39847
[1mStep[0m  [28/42], [94mLoss[0m : 2.31780
[1mStep[0m  [32/42], [94mLoss[0m : 2.58852
[1mStep[0m  [36/42], [94mLoss[0m : 2.58405
[1mStep[0m  [40/42], [94mLoss[0m : 2.42470

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.556, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34812
[1mStep[0m  [4/42], [94mLoss[0m : 2.40370
[1mStep[0m  [8/42], [94mLoss[0m : 2.43923
[1mStep[0m  [12/42], [94mLoss[0m : 2.49575
[1mStep[0m  [16/42], [94mLoss[0m : 2.22480
[1mStep[0m  [20/42], [94mLoss[0m : 2.41494
[1mStep[0m  [24/42], [94mLoss[0m : 2.57212
[1mStep[0m  [28/42], [94mLoss[0m : 2.26573
[1mStep[0m  [32/42], [94mLoss[0m : 2.30227
[1mStep[0m  [36/42], [94mLoss[0m : 2.33987
[1mStep[0m  [40/42], [94mLoss[0m : 2.57990

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.689, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19836
[1mStep[0m  [4/42], [94mLoss[0m : 2.28166
[1mStep[0m  [8/42], [94mLoss[0m : 2.46636
[1mStep[0m  [12/42], [94mLoss[0m : 2.25284
[1mStep[0m  [16/42], [94mLoss[0m : 2.37464
[1mStep[0m  [20/42], [94mLoss[0m : 2.44537
[1mStep[0m  [24/42], [94mLoss[0m : 2.25103
[1mStep[0m  [28/42], [94mLoss[0m : 2.35357
[1mStep[0m  [32/42], [94mLoss[0m : 2.59356
[1mStep[0m  [36/42], [94mLoss[0m : 2.31467
[1mStep[0m  [40/42], [94mLoss[0m : 2.50771

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.580, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48030
[1mStep[0m  [4/42], [94mLoss[0m : 2.38792
[1mStep[0m  [8/42], [94mLoss[0m : 2.50483
[1mStep[0m  [12/42], [94mLoss[0m : 2.36635
[1mStep[0m  [16/42], [94mLoss[0m : 2.25988
[1mStep[0m  [20/42], [94mLoss[0m : 2.62733
[1mStep[0m  [24/42], [94mLoss[0m : 2.23081
[1mStep[0m  [28/42], [94mLoss[0m : 2.36822
[1mStep[0m  [32/42], [94mLoss[0m : 2.25537
[1mStep[0m  [36/42], [94mLoss[0m : 2.44216
[1mStep[0m  [40/42], [94mLoss[0m : 2.34073

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.625, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25985
[1mStep[0m  [4/42], [94mLoss[0m : 2.29911
[1mStep[0m  [8/42], [94mLoss[0m : 2.23819
[1mStep[0m  [12/42], [94mLoss[0m : 2.23181
[1mStep[0m  [16/42], [94mLoss[0m : 2.38068
[1mStep[0m  [20/42], [94mLoss[0m : 2.41641
[1mStep[0m  [24/42], [94mLoss[0m : 2.42067
[1mStep[0m  [28/42], [94mLoss[0m : 2.43174
[1mStep[0m  [32/42], [94mLoss[0m : 2.47993
[1mStep[0m  [36/42], [94mLoss[0m : 2.33504
[1mStep[0m  [40/42], [94mLoss[0m : 2.22114

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.745, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41929
[1mStep[0m  [4/42], [94mLoss[0m : 2.13625
[1mStep[0m  [8/42], [94mLoss[0m : 2.32770
[1mStep[0m  [12/42], [94mLoss[0m : 2.21102
[1mStep[0m  [16/42], [94mLoss[0m : 2.06385
[1mStep[0m  [20/42], [94mLoss[0m : 2.26258
[1mStep[0m  [24/42], [94mLoss[0m : 2.30434
[1mStep[0m  [28/42], [94mLoss[0m : 2.29484
[1mStep[0m  [32/42], [94mLoss[0m : 2.34119
[1mStep[0m  [36/42], [94mLoss[0m : 2.32525
[1mStep[0m  [40/42], [94mLoss[0m : 2.45745

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.747, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00598
[1mStep[0m  [4/42], [94mLoss[0m : 2.37184
[1mStep[0m  [8/42], [94mLoss[0m : 2.35291
[1mStep[0m  [12/42], [94mLoss[0m : 2.18789
[1mStep[0m  [16/42], [94mLoss[0m : 2.20745
[1mStep[0m  [20/42], [94mLoss[0m : 2.41372
[1mStep[0m  [24/42], [94mLoss[0m : 2.14464
[1mStep[0m  [28/42], [94mLoss[0m : 2.14890
[1mStep[0m  [32/42], [94mLoss[0m : 2.19701
[1mStep[0m  [36/42], [94mLoss[0m : 2.28194
[1mStep[0m  [40/42], [94mLoss[0m : 2.26147

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.707, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93884
[1mStep[0m  [4/42], [94mLoss[0m : 2.25469
[1mStep[0m  [8/42], [94mLoss[0m : 2.13059
[1mStep[0m  [12/42], [94mLoss[0m : 2.25042
[1mStep[0m  [16/42], [94mLoss[0m : 2.29408
[1mStep[0m  [20/42], [94mLoss[0m : 2.13307
[1mStep[0m  [24/42], [94mLoss[0m : 2.11901
[1mStep[0m  [28/42], [94mLoss[0m : 2.41151
[1mStep[0m  [32/42], [94mLoss[0m : 2.20403
[1mStep[0m  [36/42], [94mLoss[0m : 2.19364
[1mStep[0m  [40/42], [94mLoss[0m : 2.33222

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.842, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12913
[1mStep[0m  [4/42], [94mLoss[0m : 2.21569
[1mStep[0m  [8/42], [94mLoss[0m : 1.96250
[1mStep[0m  [12/42], [94mLoss[0m : 2.17234
[1mStep[0m  [16/42], [94mLoss[0m : 1.78635
[1mStep[0m  [20/42], [94mLoss[0m : 2.05911
[1mStep[0m  [24/42], [94mLoss[0m : 2.29444
[1mStep[0m  [28/42], [94mLoss[0m : 2.19778
[1mStep[0m  [32/42], [94mLoss[0m : 2.15958
[1mStep[0m  [36/42], [94mLoss[0m : 2.16247
[1mStep[0m  [40/42], [94mLoss[0m : 2.12503

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.203, [92mTest[0m: 2.674, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08652
[1mStep[0m  [4/42], [94mLoss[0m : 1.96042
[1mStep[0m  [8/42], [94mLoss[0m : 2.23616
[1mStep[0m  [12/42], [94mLoss[0m : 2.06685
[1mStep[0m  [16/42], [94mLoss[0m : 2.05669
[1mStep[0m  [20/42], [94mLoss[0m : 2.09484
[1mStep[0m  [24/42], [94mLoss[0m : 2.24281
[1mStep[0m  [28/42], [94mLoss[0m : 2.02668
[1mStep[0m  [32/42], [94mLoss[0m : 2.04573
[1mStep[0m  [36/42], [94mLoss[0m : 2.18920
[1mStep[0m  [40/42], [94mLoss[0m : 2.45217

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.699, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40101
[1mStep[0m  [4/42], [94mLoss[0m : 2.07881
[1mStep[0m  [8/42], [94mLoss[0m : 2.25415
[1mStep[0m  [12/42], [94mLoss[0m : 2.29491
[1mStep[0m  [16/42], [94mLoss[0m : 2.27319
[1mStep[0m  [20/42], [94mLoss[0m : 2.22310
[1mStep[0m  [24/42], [94mLoss[0m : 2.25954
[1mStep[0m  [28/42], [94mLoss[0m : 2.23071
[1mStep[0m  [32/42], [94mLoss[0m : 1.95978
[1mStep[0m  [36/42], [94mLoss[0m : 2.14204
[1mStep[0m  [40/42], [94mLoss[0m : 2.22983

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.715, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25761
[1mStep[0m  [4/42], [94mLoss[0m : 1.94829
[1mStep[0m  [8/42], [94mLoss[0m : 2.25603
[1mStep[0m  [12/42], [94mLoss[0m : 1.99328
[1mStep[0m  [16/42], [94mLoss[0m : 2.03083
[1mStep[0m  [20/42], [94mLoss[0m : 1.93503
[1mStep[0m  [24/42], [94mLoss[0m : 2.12911
[1mStep[0m  [28/42], [94mLoss[0m : 1.97697
[1mStep[0m  [32/42], [94mLoss[0m : 1.91803
[1mStep[0m  [36/42], [94mLoss[0m : 2.14117
[1mStep[0m  [40/42], [94mLoss[0m : 2.10549

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.576, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21042
[1mStep[0m  [4/42], [94mLoss[0m : 1.93667
[1mStep[0m  [8/42], [94mLoss[0m : 2.23097
[1mStep[0m  [12/42], [94mLoss[0m : 2.09727
[1mStep[0m  [16/42], [94mLoss[0m : 2.06698
[1mStep[0m  [20/42], [94mLoss[0m : 2.04034
[1mStep[0m  [24/42], [94mLoss[0m : 2.08620
[1mStep[0m  [28/42], [94mLoss[0m : 2.22474
[1mStep[0m  [32/42], [94mLoss[0m : 2.26726
[1mStep[0m  [36/42], [94mLoss[0m : 2.22875
[1mStep[0m  [40/42], [94mLoss[0m : 1.91458

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.666, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28085
[1mStep[0m  [4/42], [94mLoss[0m : 1.88189
[1mStep[0m  [8/42], [94mLoss[0m : 1.93523
[1mStep[0m  [12/42], [94mLoss[0m : 2.03752
[1mStep[0m  [16/42], [94mLoss[0m : 2.12258
[1mStep[0m  [20/42], [94mLoss[0m : 2.06478
[1mStep[0m  [24/42], [94mLoss[0m : 2.01233
[1mStep[0m  [28/42], [94mLoss[0m : 1.99028
[1mStep[0m  [32/42], [94mLoss[0m : 2.05698
[1mStep[0m  [36/42], [94mLoss[0m : 2.12948
[1mStep[0m  [40/42], [94mLoss[0m : 2.05105

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.618, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.99623
[1mStep[0m  [4/42], [94mLoss[0m : 1.97776
[1mStep[0m  [8/42], [94mLoss[0m : 1.83263
[1mStep[0m  [12/42], [94mLoss[0m : 1.97215
[1mStep[0m  [16/42], [94mLoss[0m : 1.95626
[1mStep[0m  [20/42], [94mLoss[0m : 1.94387
[1mStep[0m  [24/42], [94mLoss[0m : 2.10287
[1mStep[0m  [28/42], [94mLoss[0m : 1.99934
[1mStep[0m  [32/42], [94mLoss[0m : 2.30130
[1mStep[0m  [36/42], [94mLoss[0m : 1.97458
[1mStep[0m  [40/42], [94mLoss[0m : 1.99959

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.655, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.09748
[1mStep[0m  [4/42], [94mLoss[0m : 2.03855
[1mStep[0m  [8/42], [94mLoss[0m : 1.94812
[1mStep[0m  [12/42], [94mLoss[0m : 2.16664
[1mStep[0m  [16/42], [94mLoss[0m : 1.99377
[1mStep[0m  [20/42], [94mLoss[0m : 2.05806
[1mStep[0m  [24/42], [94mLoss[0m : 1.99381
[1mStep[0m  [28/42], [94mLoss[0m : 2.11460
[1mStep[0m  [32/42], [94mLoss[0m : 1.96878
[1mStep[0m  [36/42], [94mLoss[0m : 2.17416
[1mStep[0m  [40/42], [94mLoss[0m : 1.83928

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.583, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00893
[1mStep[0m  [4/42], [94mLoss[0m : 1.93381
[1mStep[0m  [8/42], [94mLoss[0m : 2.07560
[1mStep[0m  [12/42], [94mLoss[0m : 2.02972
[1mStep[0m  [16/42], [94mLoss[0m : 2.17186
[1mStep[0m  [20/42], [94mLoss[0m : 1.94662
[1mStep[0m  [24/42], [94mLoss[0m : 1.89629
[1mStep[0m  [28/42], [94mLoss[0m : 1.96665
[1mStep[0m  [32/42], [94mLoss[0m : 2.07450
[1mStep[0m  [36/42], [94mLoss[0m : 1.99624
[1mStep[0m  [40/42], [94mLoss[0m : 1.97534

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97632
[1mStep[0m  [4/42], [94mLoss[0m : 1.73957
[1mStep[0m  [8/42], [94mLoss[0m : 1.86467
[1mStep[0m  [12/42], [94mLoss[0m : 2.21046
[1mStep[0m  [16/42], [94mLoss[0m : 1.78376
[1mStep[0m  [20/42], [94mLoss[0m : 1.90034
[1mStep[0m  [24/42], [94mLoss[0m : 2.10741
[1mStep[0m  [28/42], [94mLoss[0m : 1.92247
[1mStep[0m  [32/42], [94mLoss[0m : 1.98680
[1mStep[0m  [36/42], [94mLoss[0m : 1.99684
[1mStep[0m  [40/42], [94mLoss[0m : 2.04487

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.658, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77738
[1mStep[0m  [4/42], [94mLoss[0m : 1.85925
[1mStep[0m  [8/42], [94mLoss[0m : 1.85248
[1mStep[0m  [12/42], [94mLoss[0m : 1.79218
[1mStep[0m  [16/42], [94mLoss[0m : 1.99338
[1mStep[0m  [20/42], [94mLoss[0m : 1.87537
[1mStep[0m  [24/42], [94mLoss[0m : 1.99754
[1mStep[0m  [28/42], [94mLoss[0m : 2.20367
[1mStep[0m  [32/42], [94mLoss[0m : 1.74156
[1mStep[0m  [36/42], [94mLoss[0m : 1.98289
[1mStep[0m  [40/42], [94mLoss[0m : 1.89834

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08870
[1mStep[0m  [4/42], [94mLoss[0m : 1.99478
[1mStep[0m  [8/42], [94mLoss[0m : 1.83244
[1mStep[0m  [12/42], [94mLoss[0m : 2.02790
[1mStep[0m  [16/42], [94mLoss[0m : 1.96374
[1mStep[0m  [20/42], [94mLoss[0m : 1.89678
[1mStep[0m  [24/42], [94mLoss[0m : 1.80697
[1mStep[0m  [28/42], [94mLoss[0m : 1.90447
[1mStep[0m  [32/42], [94mLoss[0m : 2.02626
[1mStep[0m  [36/42], [94mLoss[0m : 2.05442
[1mStep[0m  [40/42], [94mLoss[0m : 1.90863

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.617, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.91194
[1mStep[0m  [4/42], [94mLoss[0m : 1.83132
[1mStep[0m  [8/42], [94mLoss[0m : 2.03374
[1mStep[0m  [12/42], [94mLoss[0m : 1.83144
[1mStep[0m  [16/42], [94mLoss[0m : 1.68908
[1mStep[0m  [20/42], [94mLoss[0m : 2.02464
[1mStep[0m  [24/42], [94mLoss[0m : 1.79328
[1mStep[0m  [28/42], [94mLoss[0m : 1.68556
[1mStep[0m  [32/42], [94mLoss[0m : 1.88154
[1mStep[0m  [36/42], [94mLoss[0m : 1.91427
[1mStep[0m  [40/42], [94mLoss[0m : 2.04745

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.647, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.78210
[1mStep[0m  [4/42], [94mLoss[0m : 1.87071
[1mStep[0m  [8/42], [94mLoss[0m : 2.10884
[1mStep[0m  [12/42], [94mLoss[0m : 1.92440
[1mStep[0m  [16/42], [94mLoss[0m : 1.86556
[1mStep[0m  [20/42], [94mLoss[0m : 1.82277
[1mStep[0m  [24/42], [94mLoss[0m : 1.71267
[1mStep[0m  [28/42], [94mLoss[0m : 1.90866
[1mStep[0m  [32/42], [94mLoss[0m : 1.89173
[1mStep[0m  [36/42], [94mLoss[0m : 1.86998
[1mStep[0m  [40/42], [94mLoss[0m : 2.01607

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.585, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68128
[1mStep[0m  [4/42], [94mLoss[0m : 1.78101
[1mStep[0m  [8/42], [94mLoss[0m : 1.84891
[1mStep[0m  [12/42], [94mLoss[0m : 1.85101
[1mStep[0m  [16/42], [94mLoss[0m : 1.89275
[1mStep[0m  [20/42], [94mLoss[0m : 1.70389
[1mStep[0m  [24/42], [94mLoss[0m : 1.75884
[1mStep[0m  [28/42], [94mLoss[0m : 1.83853
[1mStep[0m  [32/42], [94mLoss[0m : 1.85542
[1mStep[0m  [36/42], [94mLoss[0m : 1.80392
[1mStep[0m  [40/42], [94mLoss[0m : 1.89388

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.839, [92mTest[0m: 2.655, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.81378
[1mStep[0m  [4/42], [94mLoss[0m : 1.85161
[1mStep[0m  [8/42], [94mLoss[0m : 1.93043
[1mStep[0m  [12/42], [94mLoss[0m : 1.71412
[1mStep[0m  [16/42], [94mLoss[0m : 1.82340
[1mStep[0m  [20/42], [94mLoss[0m : 1.58956
[1mStep[0m  [24/42], [94mLoss[0m : 1.60685
[1mStep[0m  [28/42], [94mLoss[0m : 1.60809
[1mStep[0m  [32/42], [94mLoss[0m : 1.98877
[1mStep[0m  [36/42], [94mLoss[0m : 1.92288
[1mStep[0m  [40/42], [94mLoss[0m : 2.04432

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.692, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.70402
[1mStep[0m  [4/42], [94mLoss[0m : 1.91574
[1mStep[0m  [8/42], [94mLoss[0m : 1.90678
[1mStep[0m  [12/42], [94mLoss[0m : 1.72498
[1mStep[0m  [16/42], [94mLoss[0m : 1.75789
[1mStep[0m  [20/42], [94mLoss[0m : 1.76894
[1mStep[0m  [24/42], [94mLoss[0m : 1.78991
[1mStep[0m  [28/42], [94mLoss[0m : 1.70907
[1mStep[0m  [32/42], [94mLoss[0m : 1.70620
[1mStep[0m  [36/42], [94mLoss[0m : 1.60918
[1mStep[0m  [40/42], [94mLoss[0m : 1.84165

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.606, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.550
====================================

Phase 2 - Evaluation MAE:  2.5500517742974416
MAE score P1      2.394839
MAE score P2      2.550052
loss              1.798198
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 18, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.88890
[1mStep[0m  [4/42], [94mLoss[0m : 10.43554
[1mStep[0m  [8/42], [94mLoss[0m : 9.80033
[1mStep[0m  [12/42], [94mLoss[0m : 9.70507
[1mStep[0m  [16/42], [94mLoss[0m : 9.36764
[1mStep[0m  [20/42], [94mLoss[0m : 8.55434
[1mStep[0m  [24/42], [94mLoss[0m : 8.84862
[1mStep[0m  [28/42], [94mLoss[0m : 8.34583
[1mStep[0m  [32/42], [94mLoss[0m : 8.13896
[1mStep[0m  [36/42], [94mLoss[0m : 7.98273
[1mStep[0m  [40/42], [94mLoss[0m : 7.49802

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.038, [92mTest[0m: 10.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.47072
[1mStep[0m  [4/42], [94mLoss[0m : 6.99232
[1mStep[0m  [8/42], [94mLoss[0m : 6.88467
[1mStep[0m  [12/42], [94mLoss[0m : 6.44574
[1mStep[0m  [16/42], [94mLoss[0m : 6.29904
[1mStep[0m  [20/42], [94mLoss[0m : 5.93051
[1mStep[0m  [24/42], [94mLoss[0m : 5.72480
[1mStep[0m  [28/42], [94mLoss[0m : 5.11282
[1mStep[0m  [32/42], [94mLoss[0m : 5.51354
[1mStep[0m  [36/42], [94mLoss[0m : 4.71345
[1mStep[0m  [40/42], [94mLoss[0m : 4.29490

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.896, [92mTest[0m: 7.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.52372
[1mStep[0m  [4/42], [94mLoss[0m : 4.44711
[1mStep[0m  [8/42], [94mLoss[0m : 4.36874
[1mStep[0m  [12/42], [94mLoss[0m : 3.89036
[1mStep[0m  [16/42], [94mLoss[0m : 3.81492
[1mStep[0m  [20/42], [94mLoss[0m : 3.63790
[1mStep[0m  [24/42], [94mLoss[0m : 3.69520
[1mStep[0m  [28/42], [94mLoss[0m : 3.31097
[1mStep[0m  [32/42], [94mLoss[0m : 3.72397
[1mStep[0m  [36/42], [94mLoss[0m : 3.20729
[1mStep[0m  [40/42], [94mLoss[0m : 3.08037

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.780, [92mTest[0m: 4.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.40334
[1mStep[0m  [4/42], [94mLoss[0m : 3.17950
[1mStep[0m  [8/42], [94mLoss[0m : 2.96414
[1mStep[0m  [12/42], [94mLoss[0m : 2.96538
[1mStep[0m  [16/42], [94mLoss[0m : 2.95804
[1mStep[0m  [20/42], [94mLoss[0m : 2.91013
[1mStep[0m  [24/42], [94mLoss[0m : 3.27931
[1mStep[0m  [28/42], [94mLoss[0m : 2.78891
[1mStep[0m  [32/42], [94mLoss[0m : 2.98493
[1mStep[0m  [36/42], [94mLoss[0m : 2.76519
[1mStep[0m  [40/42], [94mLoss[0m : 2.80227

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.980, [92mTest[0m: 3.069, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84142
[1mStep[0m  [4/42], [94mLoss[0m : 2.56579
[1mStep[0m  [8/42], [94mLoss[0m : 2.85186
[1mStep[0m  [12/42], [94mLoss[0m : 2.66670
[1mStep[0m  [16/42], [94mLoss[0m : 2.95570
[1mStep[0m  [20/42], [94mLoss[0m : 3.14683
[1mStep[0m  [24/42], [94mLoss[0m : 2.58800
[1mStep[0m  [28/42], [94mLoss[0m : 2.67115
[1mStep[0m  [32/42], [94mLoss[0m : 2.66585
[1mStep[0m  [36/42], [94mLoss[0m : 2.68399
[1mStep[0m  [40/42], [94mLoss[0m : 2.95865

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.584, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78504
[1mStep[0m  [4/42], [94mLoss[0m : 2.73356
[1mStep[0m  [8/42], [94mLoss[0m : 2.61690
[1mStep[0m  [12/42], [94mLoss[0m : 2.64382
[1mStep[0m  [16/42], [94mLoss[0m : 2.74746
[1mStep[0m  [20/42], [94mLoss[0m : 2.86886
[1mStep[0m  [24/42], [94mLoss[0m : 2.73536
[1mStep[0m  [28/42], [94mLoss[0m : 2.67974
[1mStep[0m  [32/42], [94mLoss[0m : 2.75748
[1mStep[0m  [36/42], [94mLoss[0m : 2.79065
[1mStep[0m  [40/42], [94mLoss[0m : 2.75864

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56008
[1mStep[0m  [4/42], [94mLoss[0m : 2.67410
[1mStep[0m  [8/42], [94mLoss[0m : 2.81374
[1mStep[0m  [12/42], [94mLoss[0m : 2.57995
[1mStep[0m  [16/42], [94mLoss[0m : 2.60939
[1mStep[0m  [20/42], [94mLoss[0m : 2.65162
[1mStep[0m  [24/42], [94mLoss[0m : 2.62367
[1mStep[0m  [28/42], [94mLoss[0m : 2.32044
[1mStep[0m  [32/42], [94mLoss[0m : 2.61335
[1mStep[0m  [36/42], [94mLoss[0m : 2.80734
[1mStep[0m  [40/42], [94mLoss[0m : 2.61117

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41378
[1mStep[0m  [4/42], [94mLoss[0m : 2.71244
[1mStep[0m  [8/42], [94mLoss[0m : 2.54733
[1mStep[0m  [12/42], [94mLoss[0m : 2.54637
[1mStep[0m  [16/42], [94mLoss[0m : 2.69862
[1mStep[0m  [20/42], [94mLoss[0m : 2.70446
[1mStep[0m  [24/42], [94mLoss[0m : 2.74623
[1mStep[0m  [28/42], [94mLoss[0m : 2.60668
[1mStep[0m  [32/42], [94mLoss[0m : 2.57249
[1mStep[0m  [36/42], [94mLoss[0m : 2.51262
[1mStep[0m  [40/42], [94mLoss[0m : 2.41094

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69515
[1mStep[0m  [4/42], [94mLoss[0m : 2.68957
[1mStep[0m  [8/42], [94mLoss[0m : 2.58457
[1mStep[0m  [12/42], [94mLoss[0m : 2.62202
[1mStep[0m  [16/42], [94mLoss[0m : 2.71431
[1mStep[0m  [20/42], [94mLoss[0m : 2.68375
[1mStep[0m  [24/42], [94mLoss[0m : 2.55099
[1mStep[0m  [28/42], [94mLoss[0m : 2.37603
[1mStep[0m  [32/42], [94mLoss[0m : 2.65857
[1mStep[0m  [36/42], [94mLoss[0m : 2.64301
[1mStep[0m  [40/42], [94mLoss[0m : 2.64617

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81509
[1mStep[0m  [4/42], [94mLoss[0m : 2.35873
[1mStep[0m  [8/42], [94mLoss[0m : 2.65950
[1mStep[0m  [12/42], [94mLoss[0m : 2.71162
[1mStep[0m  [16/42], [94mLoss[0m : 2.46094
[1mStep[0m  [20/42], [94mLoss[0m : 2.60138
[1mStep[0m  [24/42], [94mLoss[0m : 2.57939
[1mStep[0m  [28/42], [94mLoss[0m : 2.55906
[1mStep[0m  [32/42], [94mLoss[0m : 2.66379
[1mStep[0m  [36/42], [94mLoss[0m : 2.67860
[1mStep[0m  [40/42], [94mLoss[0m : 2.62671

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62173
[1mStep[0m  [4/42], [94mLoss[0m : 2.66836
[1mStep[0m  [8/42], [94mLoss[0m : 2.67285
[1mStep[0m  [12/42], [94mLoss[0m : 2.60828
[1mStep[0m  [16/42], [94mLoss[0m : 2.73388
[1mStep[0m  [20/42], [94mLoss[0m : 2.66572
[1mStep[0m  [24/42], [94mLoss[0m : 2.52120
[1mStep[0m  [28/42], [94mLoss[0m : 2.63560
[1mStep[0m  [32/42], [94mLoss[0m : 2.77165
[1mStep[0m  [36/42], [94mLoss[0m : 2.61663
[1mStep[0m  [40/42], [94mLoss[0m : 2.47631

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73564
[1mStep[0m  [4/42], [94mLoss[0m : 2.67624
[1mStep[0m  [8/42], [94mLoss[0m : 2.74838
[1mStep[0m  [12/42], [94mLoss[0m : 2.56771
[1mStep[0m  [16/42], [94mLoss[0m : 2.57717
[1mStep[0m  [20/42], [94mLoss[0m : 2.76191
[1mStep[0m  [24/42], [94mLoss[0m : 2.62240
[1mStep[0m  [28/42], [94mLoss[0m : 2.50607
[1mStep[0m  [32/42], [94mLoss[0m : 2.44797
[1mStep[0m  [36/42], [94mLoss[0m : 2.71707
[1mStep[0m  [40/42], [94mLoss[0m : 2.67834

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56179
[1mStep[0m  [4/42], [94mLoss[0m : 2.50964
[1mStep[0m  [8/42], [94mLoss[0m : 2.72578
[1mStep[0m  [12/42], [94mLoss[0m : 2.61092
[1mStep[0m  [16/42], [94mLoss[0m : 2.57001
[1mStep[0m  [20/42], [94mLoss[0m : 2.61388
[1mStep[0m  [24/42], [94mLoss[0m : 2.62166
[1mStep[0m  [28/42], [94mLoss[0m : 2.73694
[1mStep[0m  [32/42], [94mLoss[0m : 2.38447
[1mStep[0m  [36/42], [94mLoss[0m : 2.55463
[1mStep[0m  [40/42], [94mLoss[0m : 2.48442

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53451
[1mStep[0m  [4/42], [94mLoss[0m : 2.63541
[1mStep[0m  [8/42], [94mLoss[0m : 2.62706
[1mStep[0m  [12/42], [94mLoss[0m : 2.52066
[1mStep[0m  [16/42], [94mLoss[0m : 2.44274
[1mStep[0m  [20/42], [94mLoss[0m : 2.46844
[1mStep[0m  [24/42], [94mLoss[0m : 2.64224
[1mStep[0m  [28/42], [94mLoss[0m : 2.63642
[1mStep[0m  [32/42], [94mLoss[0m : 2.57814
[1mStep[0m  [36/42], [94mLoss[0m : 2.62661
[1mStep[0m  [40/42], [94mLoss[0m : 2.67305

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37216
[1mStep[0m  [4/42], [94mLoss[0m : 2.49426
[1mStep[0m  [8/42], [94mLoss[0m : 2.67471
[1mStep[0m  [12/42], [94mLoss[0m : 2.63680
[1mStep[0m  [16/42], [94mLoss[0m : 2.54998
[1mStep[0m  [20/42], [94mLoss[0m : 2.42781
[1mStep[0m  [24/42], [94mLoss[0m : 2.68018
[1mStep[0m  [28/42], [94mLoss[0m : 2.62889
[1mStep[0m  [32/42], [94mLoss[0m : 2.37709
[1mStep[0m  [36/42], [94mLoss[0m : 2.40585
[1mStep[0m  [40/42], [94mLoss[0m : 2.47541

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.78882
[1mStep[0m  [4/42], [94mLoss[0m : 2.62957
[1mStep[0m  [8/42], [94mLoss[0m : 2.51474
[1mStep[0m  [12/42], [94mLoss[0m : 2.72171
[1mStep[0m  [16/42], [94mLoss[0m : 2.51275
[1mStep[0m  [20/42], [94mLoss[0m : 2.46392
[1mStep[0m  [24/42], [94mLoss[0m : 2.64710
[1mStep[0m  [28/42], [94mLoss[0m : 2.43229
[1mStep[0m  [32/42], [94mLoss[0m : 2.52861
[1mStep[0m  [36/42], [94mLoss[0m : 2.51746
[1mStep[0m  [40/42], [94mLoss[0m : 2.45682

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63837
[1mStep[0m  [4/42], [94mLoss[0m : 2.59005
[1mStep[0m  [8/42], [94mLoss[0m : 2.71543
[1mStep[0m  [12/42], [94mLoss[0m : 2.70696
[1mStep[0m  [16/42], [94mLoss[0m : 2.71985
[1mStep[0m  [20/42], [94mLoss[0m : 2.65421
[1mStep[0m  [24/42], [94mLoss[0m : 2.62555
[1mStep[0m  [28/42], [94mLoss[0m : 2.54695
[1mStep[0m  [32/42], [94mLoss[0m : 2.39626
[1mStep[0m  [36/42], [94mLoss[0m : 2.47501
[1mStep[0m  [40/42], [94mLoss[0m : 2.40254

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43437
[1mStep[0m  [4/42], [94mLoss[0m : 2.56838
[1mStep[0m  [8/42], [94mLoss[0m : 2.58631
[1mStep[0m  [12/42], [94mLoss[0m : 2.50005
[1mStep[0m  [16/42], [94mLoss[0m : 2.72146
[1mStep[0m  [20/42], [94mLoss[0m : 2.50101
[1mStep[0m  [24/42], [94mLoss[0m : 2.44908
[1mStep[0m  [28/42], [94mLoss[0m : 2.63438
[1mStep[0m  [32/42], [94mLoss[0m : 2.67523
[1mStep[0m  [36/42], [94mLoss[0m : 2.53423
[1mStep[0m  [40/42], [94mLoss[0m : 2.48070

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31273
[1mStep[0m  [4/42], [94mLoss[0m : 2.68737
[1mStep[0m  [8/42], [94mLoss[0m : 2.46982
[1mStep[0m  [12/42], [94mLoss[0m : 2.71185
[1mStep[0m  [16/42], [94mLoss[0m : 2.52611
[1mStep[0m  [20/42], [94mLoss[0m : 2.61450
[1mStep[0m  [24/42], [94mLoss[0m : 2.72833
[1mStep[0m  [28/42], [94mLoss[0m : 2.84761
[1mStep[0m  [32/42], [94mLoss[0m : 2.47038
[1mStep[0m  [36/42], [94mLoss[0m : 2.69841
[1mStep[0m  [40/42], [94mLoss[0m : 2.47065

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48774
[1mStep[0m  [4/42], [94mLoss[0m : 2.30387
[1mStep[0m  [8/42], [94mLoss[0m : 2.42169
[1mStep[0m  [12/42], [94mLoss[0m : 2.69464
[1mStep[0m  [16/42], [94mLoss[0m : 2.48768
[1mStep[0m  [20/42], [94mLoss[0m : 2.62417
[1mStep[0m  [24/42], [94mLoss[0m : 2.77507
[1mStep[0m  [28/42], [94mLoss[0m : 2.63601
[1mStep[0m  [32/42], [94mLoss[0m : 2.65299
[1mStep[0m  [36/42], [94mLoss[0m : 2.49942
[1mStep[0m  [40/42], [94mLoss[0m : 2.69278

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62058
[1mStep[0m  [4/42], [94mLoss[0m : 2.53274
[1mStep[0m  [8/42], [94mLoss[0m : 2.47308
[1mStep[0m  [12/42], [94mLoss[0m : 2.60514
[1mStep[0m  [16/42], [94mLoss[0m : 2.38837
[1mStep[0m  [20/42], [94mLoss[0m : 2.63578
[1mStep[0m  [24/42], [94mLoss[0m : 2.47732
[1mStep[0m  [28/42], [94mLoss[0m : 2.53457
[1mStep[0m  [32/42], [94mLoss[0m : 2.72727
[1mStep[0m  [36/42], [94mLoss[0m : 2.70546
[1mStep[0m  [40/42], [94mLoss[0m : 2.47811

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.357, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45557
[1mStep[0m  [4/42], [94mLoss[0m : 2.60220
[1mStep[0m  [8/42], [94mLoss[0m : 2.59357
[1mStep[0m  [12/42], [94mLoss[0m : 2.47647
[1mStep[0m  [16/42], [94mLoss[0m : 2.51735
[1mStep[0m  [20/42], [94mLoss[0m : 2.47875
[1mStep[0m  [24/42], [94mLoss[0m : 2.47513
[1mStep[0m  [28/42], [94mLoss[0m : 2.84296
[1mStep[0m  [32/42], [94mLoss[0m : 2.51730
[1mStep[0m  [36/42], [94mLoss[0m : 2.51539
[1mStep[0m  [40/42], [94mLoss[0m : 2.75467

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.359, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33244
[1mStep[0m  [4/42], [94mLoss[0m : 2.70291
[1mStep[0m  [8/42], [94mLoss[0m : 2.55933
[1mStep[0m  [12/42], [94mLoss[0m : 2.51697
[1mStep[0m  [16/42], [94mLoss[0m : 2.74483
[1mStep[0m  [20/42], [94mLoss[0m : 2.44772
[1mStep[0m  [24/42], [94mLoss[0m : 2.48595
[1mStep[0m  [28/42], [94mLoss[0m : 2.45966
[1mStep[0m  [32/42], [94mLoss[0m : 2.68646
[1mStep[0m  [36/42], [94mLoss[0m : 2.50872
[1mStep[0m  [40/42], [94mLoss[0m : 2.67817

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70657
[1mStep[0m  [4/42], [94mLoss[0m : 2.49077
[1mStep[0m  [8/42], [94mLoss[0m : 2.49203
[1mStep[0m  [12/42], [94mLoss[0m : 2.61698
[1mStep[0m  [16/42], [94mLoss[0m : 2.49386
[1mStep[0m  [20/42], [94mLoss[0m : 2.60069
[1mStep[0m  [24/42], [94mLoss[0m : 2.60017
[1mStep[0m  [28/42], [94mLoss[0m : 2.34625
[1mStep[0m  [32/42], [94mLoss[0m : 2.39770
[1mStep[0m  [36/42], [94mLoss[0m : 2.93299
[1mStep[0m  [40/42], [94mLoss[0m : 2.89069

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46086
[1mStep[0m  [4/42], [94mLoss[0m : 2.65374
[1mStep[0m  [8/42], [94mLoss[0m : 2.75529
[1mStep[0m  [12/42], [94mLoss[0m : 2.58417
[1mStep[0m  [16/42], [94mLoss[0m : 2.52497
[1mStep[0m  [20/42], [94mLoss[0m : 2.70839
[1mStep[0m  [24/42], [94mLoss[0m : 2.40857
[1mStep[0m  [28/42], [94mLoss[0m : 2.39420
[1mStep[0m  [32/42], [94mLoss[0m : 2.23352
[1mStep[0m  [36/42], [94mLoss[0m : 2.33922
[1mStep[0m  [40/42], [94mLoss[0m : 2.67245

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.355, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38872
[1mStep[0m  [4/42], [94mLoss[0m : 2.77886
[1mStep[0m  [8/42], [94mLoss[0m : 2.43598
[1mStep[0m  [12/42], [94mLoss[0m : 2.40673
[1mStep[0m  [16/42], [94mLoss[0m : 2.81280
[1mStep[0m  [20/42], [94mLoss[0m : 2.52025
[1mStep[0m  [24/42], [94mLoss[0m : 2.68577
[1mStep[0m  [28/42], [94mLoss[0m : 2.60226
[1mStep[0m  [32/42], [94mLoss[0m : 2.39287
[1mStep[0m  [36/42], [94mLoss[0m : 2.83541
[1mStep[0m  [40/42], [94mLoss[0m : 2.33855

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50643
[1mStep[0m  [4/42], [94mLoss[0m : 2.82359
[1mStep[0m  [8/42], [94mLoss[0m : 2.64580
[1mStep[0m  [12/42], [94mLoss[0m : 2.71668
[1mStep[0m  [16/42], [94mLoss[0m : 2.53240
[1mStep[0m  [20/42], [94mLoss[0m : 2.81899
[1mStep[0m  [24/42], [94mLoss[0m : 2.58972
[1mStep[0m  [28/42], [94mLoss[0m : 2.32502
[1mStep[0m  [32/42], [94mLoss[0m : 2.48672
[1mStep[0m  [36/42], [94mLoss[0m : 2.59950
[1mStep[0m  [40/42], [94mLoss[0m : 2.50115

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.352, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45072
[1mStep[0m  [4/42], [94mLoss[0m : 2.43493
[1mStep[0m  [8/42], [94mLoss[0m : 2.58276
[1mStep[0m  [12/42], [94mLoss[0m : 2.58817
[1mStep[0m  [16/42], [94mLoss[0m : 2.55794
[1mStep[0m  [20/42], [94mLoss[0m : 2.47801
[1mStep[0m  [24/42], [94mLoss[0m : 2.52398
[1mStep[0m  [28/42], [94mLoss[0m : 2.57543
[1mStep[0m  [32/42], [94mLoss[0m : 2.71828
[1mStep[0m  [36/42], [94mLoss[0m : 2.51680
[1mStep[0m  [40/42], [94mLoss[0m : 2.44084

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.357, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52800
[1mStep[0m  [4/42], [94mLoss[0m : 2.63429
[1mStep[0m  [8/42], [94mLoss[0m : 2.53789
[1mStep[0m  [12/42], [94mLoss[0m : 2.56064
[1mStep[0m  [16/42], [94mLoss[0m : 2.50038
[1mStep[0m  [20/42], [94mLoss[0m : 2.37944
[1mStep[0m  [24/42], [94mLoss[0m : 2.72390
[1mStep[0m  [28/42], [94mLoss[0m : 2.49424
[1mStep[0m  [32/42], [94mLoss[0m : 2.59137
[1mStep[0m  [36/42], [94mLoss[0m : 2.66907
[1mStep[0m  [40/42], [94mLoss[0m : 2.31135

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43616
[1mStep[0m  [4/42], [94mLoss[0m : 2.47070
[1mStep[0m  [8/42], [94mLoss[0m : 2.53510
[1mStep[0m  [12/42], [94mLoss[0m : 2.71343
[1mStep[0m  [16/42], [94mLoss[0m : 2.35818
[1mStep[0m  [20/42], [94mLoss[0m : 2.68710
[1mStep[0m  [24/42], [94mLoss[0m : 2.44541
[1mStep[0m  [28/42], [94mLoss[0m : 2.78997
[1mStep[0m  [32/42], [94mLoss[0m : 2.59811
[1mStep[0m  [36/42], [94mLoss[0m : 2.39965
[1mStep[0m  [40/42], [94mLoss[0m : 2.44954

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.352
====================================

Phase 1 - Evaluation MAE:  2.3517372608184814
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.58840
[1mStep[0m  [4/42], [94mLoss[0m : 2.50271
[1mStep[0m  [8/42], [94mLoss[0m : 2.47009
[1mStep[0m  [12/42], [94mLoss[0m : 2.53807
[1mStep[0m  [16/42], [94mLoss[0m : 2.51669
[1mStep[0m  [20/42], [94mLoss[0m : 2.56536
[1mStep[0m  [24/42], [94mLoss[0m : 2.47879
[1mStep[0m  [28/42], [94mLoss[0m : 2.63038
[1mStep[0m  [32/42], [94mLoss[0m : 2.66913
[1mStep[0m  [36/42], [94mLoss[0m : 2.41432
[1mStep[0m  [40/42], [94mLoss[0m : 2.41593

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41693
[1mStep[0m  [4/42], [94mLoss[0m : 2.56312
[1mStep[0m  [8/42], [94mLoss[0m : 2.60902
[1mStep[0m  [12/42], [94mLoss[0m : 2.61181
[1mStep[0m  [16/42], [94mLoss[0m : 2.40317
[1mStep[0m  [20/42], [94mLoss[0m : 2.64463
[1mStep[0m  [24/42], [94mLoss[0m : 2.57881
[1mStep[0m  [28/42], [94mLoss[0m : 2.45566
[1mStep[0m  [32/42], [94mLoss[0m : 2.75097
[1mStep[0m  [36/42], [94mLoss[0m : 2.49584
[1mStep[0m  [40/42], [94mLoss[0m : 2.36115

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69802
[1mStep[0m  [4/42], [94mLoss[0m : 2.31021
[1mStep[0m  [8/42], [94mLoss[0m : 2.67021
[1mStep[0m  [12/42], [94mLoss[0m : 2.55225
[1mStep[0m  [16/42], [94mLoss[0m : 2.67239
[1mStep[0m  [20/42], [94mLoss[0m : 2.43098
[1mStep[0m  [24/42], [94mLoss[0m : 2.56658
[1mStep[0m  [28/42], [94mLoss[0m : 2.42925
[1mStep[0m  [32/42], [94mLoss[0m : 2.72970
[1mStep[0m  [36/42], [94mLoss[0m : 2.62081
[1mStep[0m  [40/42], [94mLoss[0m : 2.51426

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54450
[1mStep[0m  [4/42], [94mLoss[0m : 2.65500
[1mStep[0m  [8/42], [94mLoss[0m : 2.50345
[1mStep[0m  [12/42], [94mLoss[0m : 2.47421
[1mStep[0m  [16/42], [94mLoss[0m : 2.38414
[1mStep[0m  [20/42], [94mLoss[0m : 2.56898
[1mStep[0m  [24/42], [94mLoss[0m : 2.62680
[1mStep[0m  [28/42], [94mLoss[0m : 2.55772
[1mStep[0m  [32/42], [94mLoss[0m : 2.47888
[1mStep[0m  [36/42], [94mLoss[0m : 2.21970
[1mStep[0m  [40/42], [94mLoss[0m : 2.42153

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49645
[1mStep[0m  [4/42], [94mLoss[0m : 2.36997
[1mStep[0m  [8/42], [94mLoss[0m : 2.63165
[1mStep[0m  [12/42], [94mLoss[0m : 2.51944
[1mStep[0m  [16/42], [94mLoss[0m : 2.41427
[1mStep[0m  [20/42], [94mLoss[0m : 2.49845
[1mStep[0m  [24/42], [94mLoss[0m : 2.51776
[1mStep[0m  [28/42], [94mLoss[0m : 2.70448
[1mStep[0m  [32/42], [94mLoss[0m : 2.50861
[1mStep[0m  [36/42], [94mLoss[0m : 2.63690
[1mStep[0m  [40/42], [94mLoss[0m : 2.38219

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.568, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32268
[1mStep[0m  [4/42], [94mLoss[0m : 2.61822
[1mStep[0m  [8/42], [94mLoss[0m : 2.72463
[1mStep[0m  [12/42], [94mLoss[0m : 2.51385
[1mStep[0m  [16/42], [94mLoss[0m : 2.61572
[1mStep[0m  [20/42], [94mLoss[0m : 2.41524
[1mStep[0m  [24/42], [94mLoss[0m : 2.60646
[1mStep[0m  [28/42], [94mLoss[0m : 2.45807
[1mStep[0m  [32/42], [94mLoss[0m : 2.55508
[1mStep[0m  [36/42], [94mLoss[0m : 2.61770
[1mStep[0m  [40/42], [94mLoss[0m : 2.52271

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.566, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36875
[1mStep[0m  [4/42], [94mLoss[0m : 2.48810
[1mStep[0m  [8/42], [94mLoss[0m : 2.41481
[1mStep[0m  [12/42], [94mLoss[0m : 2.51924
[1mStep[0m  [16/42], [94mLoss[0m : 2.50602
[1mStep[0m  [20/42], [94mLoss[0m : 2.45982
[1mStep[0m  [24/42], [94mLoss[0m : 2.55256
[1mStep[0m  [28/42], [94mLoss[0m : 2.49086
[1mStep[0m  [32/42], [94mLoss[0m : 2.60787
[1mStep[0m  [36/42], [94mLoss[0m : 2.55024
[1mStep[0m  [40/42], [94mLoss[0m : 2.45134

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.697, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42337
[1mStep[0m  [4/42], [94mLoss[0m : 2.42112
[1mStep[0m  [8/42], [94mLoss[0m : 2.48626
[1mStep[0m  [12/42], [94mLoss[0m : 2.46952
[1mStep[0m  [16/42], [94mLoss[0m : 2.38577
[1mStep[0m  [20/42], [94mLoss[0m : 2.39712
[1mStep[0m  [24/42], [94mLoss[0m : 2.32963
[1mStep[0m  [28/42], [94mLoss[0m : 2.39646
[1mStep[0m  [32/42], [94mLoss[0m : 2.40874
[1mStep[0m  [36/42], [94mLoss[0m : 2.43523
[1mStep[0m  [40/42], [94mLoss[0m : 2.44230

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.569, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54611
[1mStep[0m  [4/42], [94mLoss[0m : 2.35982
[1mStep[0m  [8/42], [94mLoss[0m : 2.32694
[1mStep[0m  [12/42], [94mLoss[0m : 2.49946
[1mStep[0m  [16/42], [94mLoss[0m : 2.14140
[1mStep[0m  [20/42], [94mLoss[0m : 2.40253
[1mStep[0m  [24/42], [94mLoss[0m : 2.45334
[1mStep[0m  [28/42], [94mLoss[0m : 2.42966
[1mStep[0m  [32/42], [94mLoss[0m : 2.75050
[1mStep[0m  [36/42], [94mLoss[0m : 2.38087
[1mStep[0m  [40/42], [94mLoss[0m : 2.28113

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.643, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42448
[1mStep[0m  [4/42], [94mLoss[0m : 2.57465
[1mStep[0m  [8/42], [94mLoss[0m : 2.65908
[1mStep[0m  [12/42], [94mLoss[0m : 2.28668
[1mStep[0m  [16/42], [94mLoss[0m : 2.56594
[1mStep[0m  [20/42], [94mLoss[0m : 2.36449
[1mStep[0m  [24/42], [94mLoss[0m : 2.46452
[1mStep[0m  [28/42], [94mLoss[0m : 2.41127
[1mStep[0m  [32/42], [94mLoss[0m : 2.49808
[1mStep[0m  [36/42], [94mLoss[0m : 2.59302
[1mStep[0m  [40/42], [94mLoss[0m : 2.53529

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.702, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39770
[1mStep[0m  [4/42], [94mLoss[0m : 2.38612
[1mStep[0m  [8/42], [94mLoss[0m : 2.44631
[1mStep[0m  [12/42], [94mLoss[0m : 2.48133
[1mStep[0m  [16/42], [94mLoss[0m : 2.41227
[1mStep[0m  [20/42], [94mLoss[0m : 2.32481
[1mStep[0m  [24/42], [94mLoss[0m : 2.46396
[1mStep[0m  [28/42], [94mLoss[0m : 2.63500
[1mStep[0m  [32/42], [94mLoss[0m : 2.48262
[1mStep[0m  [36/42], [94mLoss[0m : 2.23949
[1mStep[0m  [40/42], [94mLoss[0m : 2.37884

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.625, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27254
[1mStep[0m  [4/42], [94mLoss[0m : 2.32355
[1mStep[0m  [8/42], [94mLoss[0m : 2.48336
[1mStep[0m  [12/42], [94mLoss[0m : 2.52842
[1mStep[0m  [16/42], [94mLoss[0m : 2.53852
[1mStep[0m  [20/42], [94mLoss[0m : 2.46388
[1mStep[0m  [24/42], [94mLoss[0m : 2.55136
[1mStep[0m  [28/42], [94mLoss[0m : 2.44229
[1mStep[0m  [32/42], [94mLoss[0m : 2.53111
[1mStep[0m  [36/42], [94mLoss[0m : 2.43044
[1mStep[0m  [40/42], [94mLoss[0m : 2.52315

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.679, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39155
[1mStep[0m  [4/42], [94mLoss[0m : 2.32117
[1mStep[0m  [8/42], [94mLoss[0m : 2.43391
[1mStep[0m  [12/42], [94mLoss[0m : 2.30110
[1mStep[0m  [16/42], [94mLoss[0m : 2.64572
[1mStep[0m  [20/42], [94mLoss[0m : 2.49041
[1mStep[0m  [24/42], [94mLoss[0m : 2.26343
[1mStep[0m  [28/42], [94mLoss[0m : 2.34049
[1mStep[0m  [32/42], [94mLoss[0m : 2.34511
[1mStep[0m  [36/42], [94mLoss[0m : 2.41073
[1mStep[0m  [40/42], [94mLoss[0m : 2.47262

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.703, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28253
[1mStep[0m  [4/42], [94mLoss[0m : 2.46631
[1mStep[0m  [8/42], [94mLoss[0m : 2.23850
[1mStep[0m  [12/42], [94mLoss[0m : 2.43367
[1mStep[0m  [16/42], [94mLoss[0m : 2.39327
[1mStep[0m  [20/42], [94mLoss[0m : 2.26962
[1mStep[0m  [24/42], [94mLoss[0m : 2.30569
[1mStep[0m  [28/42], [94mLoss[0m : 2.30792
[1mStep[0m  [32/42], [94mLoss[0m : 2.33202
[1mStep[0m  [36/42], [94mLoss[0m : 2.41924
[1mStep[0m  [40/42], [94mLoss[0m : 2.29446

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.672, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41171
[1mStep[0m  [4/42], [94mLoss[0m : 2.66610
[1mStep[0m  [8/42], [94mLoss[0m : 2.24889
[1mStep[0m  [12/42], [94mLoss[0m : 2.59278
[1mStep[0m  [16/42], [94mLoss[0m : 2.71840
[1mStep[0m  [20/42], [94mLoss[0m : 2.23163
[1mStep[0m  [24/42], [94mLoss[0m : 2.51360
[1mStep[0m  [28/42], [94mLoss[0m : 2.33856
[1mStep[0m  [32/42], [94mLoss[0m : 2.45288
[1mStep[0m  [36/42], [94mLoss[0m : 2.38412
[1mStep[0m  [40/42], [94mLoss[0m : 2.30087

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.648, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64155
[1mStep[0m  [4/42], [94mLoss[0m : 2.42689
[1mStep[0m  [8/42], [94mLoss[0m : 2.31176
[1mStep[0m  [12/42], [94mLoss[0m : 2.54035
[1mStep[0m  [16/42], [94mLoss[0m : 2.45801
[1mStep[0m  [20/42], [94mLoss[0m : 2.36463
[1mStep[0m  [24/42], [94mLoss[0m : 2.20358
[1mStep[0m  [28/42], [94mLoss[0m : 2.36758
[1mStep[0m  [32/42], [94mLoss[0m : 2.07501
[1mStep[0m  [36/42], [94mLoss[0m : 2.37963
[1mStep[0m  [40/42], [94mLoss[0m : 2.50774

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.628, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45758
[1mStep[0m  [4/42], [94mLoss[0m : 2.25725
[1mStep[0m  [8/42], [94mLoss[0m : 2.48413
[1mStep[0m  [12/42], [94mLoss[0m : 2.54183
[1mStep[0m  [16/42], [94mLoss[0m : 2.50965
[1mStep[0m  [20/42], [94mLoss[0m : 2.50933
[1mStep[0m  [24/42], [94mLoss[0m : 2.42380
[1mStep[0m  [28/42], [94mLoss[0m : 2.06875
[1mStep[0m  [32/42], [94mLoss[0m : 2.24416
[1mStep[0m  [36/42], [94mLoss[0m : 2.56374
[1mStep[0m  [40/42], [94mLoss[0m : 2.28752

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.686, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36610
[1mStep[0m  [4/42], [94mLoss[0m : 2.42841
[1mStep[0m  [8/42], [94mLoss[0m : 2.34615
[1mStep[0m  [12/42], [94mLoss[0m : 2.38014
[1mStep[0m  [16/42], [94mLoss[0m : 2.45942
[1mStep[0m  [20/42], [94mLoss[0m : 2.39649
[1mStep[0m  [24/42], [94mLoss[0m : 2.27328
[1mStep[0m  [28/42], [94mLoss[0m : 2.40366
[1mStep[0m  [32/42], [94mLoss[0m : 2.07592
[1mStep[0m  [36/42], [94mLoss[0m : 2.09558
[1mStep[0m  [40/42], [94mLoss[0m : 2.38603

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.713, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50988
[1mStep[0m  [4/42], [94mLoss[0m : 2.20574
[1mStep[0m  [8/42], [94mLoss[0m : 2.25937
[1mStep[0m  [12/42], [94mLoss[0m : 2.34750
[1mStep[0m  [16/42], [94mLoss[0m : 2.27442
[1mStep[0m  [20/42], [94mLoss[0m : 2.50063
[1mStep[0m  [24/42], [94mLoss[0m : 2.25600
[1mStep[0m  [28/42], [94mLoss[0m : 2.53960
[1mStep[0m  [32/42], [94mLoss[0m : 2.53381
[1mStep[0m  [36/42], [94mLoss[0m : 2.30565
[1mStep[0m  [40/42], [94mLoss[0m : 2.32561

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.645, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27143
[1mStep[0m  [4/42], [94mLoss[0m : 2.25093
[1mStep[0m  [8/42], [94mLoss[0m : 2.31086
[1mStep[0m  [12/42], [94mLoss[0m : 2.38406
[1mStep[0m  [16/42], [94mLoss[0m : 2.43897
[1mStep[0m  [20/42], [94mLoss[0m : 2.25541
[1mStep[0m  [24/42], [94mLoss[0m : 2.51279
[1mStep[0m  [28/42], [94mLoss[0m : 2.61512
[1mStep[0m  [32/42], [94mLoss[0m : 2.45719
[1mStep[0m  [36/42], [94mLoss[0m : 2.14867
[1mStep[0m  [40/42], [94mLoss[0m : 2.32380

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.669, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45825
[1mStep[0m  [4/42], [94mLoss[0m : 2.15313
[1mStep[0m  [8/42], [94mLoss[0m : 2.26657
[1mStep[0m  [12/42], [94mLoss[0m : 2.18737
[1mStep[0m  [16/42], [94mLoss[0m : 2.07093
[1mStep[0m  [20/42], [94mLoss[0m : 2.11386
[1mStep[0m  [24/42], [94mLoss[0m : 2.30002
[1mStep[0m  [28/42], [94mLoss[0m : 2.43584
[1mStep[0m  [32/42], [94mLoss[0m : 2.48662
[1mStep[0m  [36/42], [94mLoss[0m : 2.35738
[1mStep[0m  [40/42], [94mLoss[0m : 2.32117

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.709, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19108
[1mStep[0m  [4/42], [94mLoss[0m : 2.18280
[1mStep[0m  [8/42], [94mLoss[0m : 2.37520
[1mStep[0m  [12/42], [94mLoss[0m : 2.08346
[1mStep[0m  [16/42], [94mLoss[0m : 2.28329
[1mStep[0m  [20/42], [94mLoss[0m : 2.42939
[1mStep[0m  [24/42], [94mLoss[0m : 2.22253
[1mStep[0m  [28/42], [94mLoss[0m : 2.16113
[1mStep[0m  [32/42], [94mLoss[0m : 2.22208
[1mStep[0m  [36/42], [94mLoss[0m : 2.33212
[1mStep[0m  [40/42], [94mLoss[0m : 2.25509

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.630, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40658
[1mStep[0m  [4/42], [94mLoss[0m : 2.30514
[1mStep[0m  [8/42], [94mLoss[0m : 2.26428
[1mStep[0m  [12/42], [94mLoss[0m : 2.04991
[1mStep[0m  [16/42], [94mLoss[0m : 2.20771
[1mStep[0m  [20/42], [94mLoss[0m : 2.28944
[1mStep[0m  [24/42], [94mLoss[0m : 2.16243
[1mStep[0m  [28/42], [94mLoss[0m : 2.29076
[1mStep[0m  [32/42], [94mLoss[0m : 2.09491
[1mStep[0m  [36/42], [94mLoss[0m : 2.30402
[1mStep[0m  [40/42], [94mLoss[0m : 2.22826

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.732, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95422
[1mStep[0m  [4/42], [94mLoss[0m : 1.99962
[1mStep[0m  [8/42], [94mLoss[0m : 2.06471
[1mStep[0m  [12/42], [94mLoss[0m : 2.25317
[1mStep[0m  [16/42], [94mLoss[0m : 2.19376
[1mStep[0m  [20/42], [94mLoss[0m : 2.34273
[1mStep[0m  [24/42], [94mLoss[0m : 2.06984
[1mStep[0m  [28/42], [94mLoss[0m : 2.04545
[1mStep[0m  [32/42], [94mLoss[0m : 2.15694
[1mStep[0m  [36/42], [94mLoss[0m : 2.02863
[1mStep[0m  [40/42], [94mLoss[0m : 2.13426

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.752, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24862
[1mStep[0m  [4/42], [94mLoss[0m : 2.23565
[1mStep[0m  [8/42], [94mLoss[0m : 2.11620
[1mStep[0m  [12/42], [94mLoss[0m : 2.34583
[1mStep[0m  [16/42], [94mLoss[0m : 2.17704
[1mStep[0m  [20/42], [94mLoss[0m : 2.19761
[1mStep[0m  [24/42], [94mLoss[0m : 2.24927
[1mStep[0m  [28/42], [94mLoss[0m : 2.14339
[1mStep[0m  [32/42], [94mLoss[0m : 2.07863
[1mStep[0m  [36/42], [94mLoss[0m : 2.26501
[1mStep[0m  [40/42], [94mLoss[0m : 2.47946

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.221, [92mTest[0m: 2.780, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14720
[1mStep[0m  [4/42], [94mLoss[0m : 2.38864
[1mStep[0m  [8/42], [94mLoss[0m : 2.02466
[1mStep[0m  [12/42], [94mLoss[0m : 2.08256
[1mStep[0m  [16/42], [94mLoss[0m : 2.16681
[1mStep[0m  [20/42], [94mLoss[0m : 2.18176
[1mStep[0m  [24/42], [94mLoss[0m : 2.13346
[1mStep[0m  [28/42], [94mLoss[0m : 2.05450
[1mStep[0m  [32/42], [94mLoss[0m : 2.38521
[1mStep[0m  [36/42], [94mLoss[0m : 2.34207
[1mStep[0m  [40/42], [94mLoss[0m : 2.13454

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.204, [92mTest[0m: 2.704, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13162
[1mStep[0m  [4/42], [94mLoss[0m : 2.39269
[1mStep[0m  [8/42], [94mLoss[0m : 2.20801
[1mStep[0m  [12/42], [94mLoss[0m : 1.97550
[1mStep[0m  [16/42], [94mLoss[0m : 1.97727
[1mStep[0m  [20/42], [94mLoss[0m : 1.93263
[1mStep[0m  [24/42], [94mLoss[0m : 2.12907
[1mStep[0m  [28/42], [94mLoss[0m : 2.41934
[1mStep[0m  [32/42], [94mLoss[0m : 2.15222
[1mStep[0m  [36/42], [94mLoss[0m : 2.23474
[1mStep[0m  [40/42], [94mLoss[0m : 2.16979

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.762, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.05613
[1mStep[0m  [4/42], [94mLoss[0m : 2.16204
[1mStep[0m  [8/42], [94mLoss[0m : 2.13294
[1mStep[0m  [12/42], [94mLoss[0m : 2.16326
[1mStep[0m  [16/42], [94mLoss[0m : 2.13494
[1mStep[0m  [20/42], [94mLoss[0m : 2.14793
[1mStep[0m  [24/42], [94mLoss[0m : 2.20193
[1mStep[0m  [28/42], [94mLoss[0m : 2.18671
[1mStep[0m  [32/42], [94mLoss[0m : 2.02216
[1mStep[0m  [36/42], [94mLoss[0m : 2.10833
[1mStep[0m  [40/42], [94mLoss[0m : 2.05790

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.733, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17518
[1mStep[0m  [4/42], [94mLoss[0m : 2.17733
[1mStep[0m  [8/42], [94mLoss[0m : 1.98175
[1mStep[0m  [12/42], [94mLoss[0m : 2.20982
[1mStep[0m  [16/42], [94mLoss[0m : 2.01694
[1mStep[0m  [20/42], [94mLoss[0m : 2.03011
[1mStep[0m  [24/42], [94mLoss[0m : 2.11805
[1mStep[0m  [28/42], [94mLoss[0m : 2.19087
[1mStep[0m  [32/42], [94mLoss[0m : 2.16922
[1mStep[0m  [36/42], [94mLoss[0m : 2.04382
[1mStep[0m  [40/42], [94mLoss[0m : 2.11238

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.647, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00761
[1mStep[0m  [4/42], [94mLoss[0m : 1.84744
[1mStep[0m  [8/42], [94mLoss[0m : 2.32164
[1mStep[0m  [12/42], [94mLoss[0m : 1.97829
[1mStep[0m  [16/42], [94mLoss[0m : 2.17382
[1mStep[0m  [20/42], [94mLoss[0m : 2.05636
[1mStep[0m  [24/42], [94mLoss[0m : 2.13783
[1mStep[0m  [28/42], [94mLoss[0m : 2.28328
[1mStep[0m  [32/42], [94mLoss[0m : 2.10528
[1mStep[0m  [36/42], [94mLoss[0m : 2.22968
[1mStep[0m  [40/42], [94mLoss[0m : 2.07348

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.643, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.627
====================================

Phase 2 - Evaluation MAE:  2.6271933146885464
MAE score P1       2.351737
MAE score P2       2.627193
loss               2.094259
learning_rate       0.00505
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 19, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.40134
[1mStep[0m  [8/84], [94mLoss[0m : 10.52571
[1mStep[0m  [16/84], [94mLoss[0m : 8.12740
[1mStep[0m  [24/84], [94mLoss[0m : 4.85996
[1mStep[0m  [32/84], [94mLoss[0m : 3.06663
[1mStep[0m  [40/84], [94mLoss[0m : 3.04719
[1mStep[0m  [48/84], [94mLoss[0m : 2.94216
[1mStep[0m  [56/84], [94mLoss[0m : 2.61392
[1mStep[0m  [64/84], [94mLoss[0m : 2.99443
[1mStep[0m  [72/84], [94mLoss[0m : 2.72720
[1mStep[0m  [80/84], [94mLoss[0m : 2.79925

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.707, [92mTest[0m: 11.064, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.12934
[1mStep[0m  [8/84], [94mLoss[0m : 2.86094
[1mStep[0m  [16/84], [94mLoss[0m : 2.73887
[1mStep[0m  [24/84], [94mLoss[0m : 2.76747
[1mStep[0m  [32/84], [94mLoss[0m : 2.65321
[1mStep[0m  [40/84], [94mLoss[0m : 2.80124
[1mStep[0m  [48/84], [94mLoss[0m : 2.53957
[1mStep[0m  [56/84], [94mLoss[0m : 3.02987
[1mStep[0m  [64/84], [94mLoss[0m : 2.83252
[1mStep[0m  [72/84], [94mLoss[0m : 2.64674
[1mStep[0m  [80/84], [94mLoss[0m : 2.49571

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.766, [92mTest[0m: 2.571, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64129
[1mStep[0m  [8/84], [94mLoss[0m : 3.04102
[1mStep[0m  [16/84], [94mLoss[0m : 2.24317
[1mStep[0m  [24/84], [94mLoss[0m : 2.60637
[1mStep[0m  [32/84], [94mLoss[0m : 2.39865
[1mStep[0m  [40/84], [94mLoss[0m : 2.88539
[1mStep[0m  [48/84], [94mLoss[0m : 2.39307
[1mStep[0m  [56/84], [94mLoss[0m : 2.35432
[1mStep[0m  [64/84], [94mLoss[0m : 3.04068
[1mStep[0m  [72/84], [94mLoss[0m : 2.48911
[1mStep[0m  [80/84], [94mLoss[0m : 2.62467

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72982
[1mStep[0m  [8/84], [94mLoss[0m : 2.51614
[1mStep[0m  [16/84], [94mLoss[0m : 2.54019
[1mStep[0m  [24/84], [94mLoss[0m : 2.57708
[1mStep[0m  [32/84], [94mLoss[0m : 2.73538
[1mStep[0m  [40/84], [94mLoss[0m : 2.42892
[1mStep[0m  [48/84], [94mLoss[0m : 2.63787
[1mStep[0m  [56/84], [94mLoss[0m : 2.65694
[1mStep[0m  [64/84], [94mLoss[0m : 2.51995
[1mStep[0m  [72/84], [94mLoss[0m : 2.65975
[1mStep[0m  [80/84], [94mLoss[0m : 2.60471

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85172
[1mStep[0m  [8/84], [94mLoss[0m : 2.41984
[1mStep[0m  [16/84], [94mLoss[0m : 2.65693
[1mStep[0m  [24/84], [94mLoss[0m : 2.55300
[1mStep[0m  [32/84], [94mLoss[0m : 2.75093
[1mStep[0m  [40/84], [94mLoss[0m : 2.42757
[1mStep[0m  [48/84], [94mLoss[0m : 2.40076
[1mStep[0m  [56/84], [94mLoss[0m : 2.18876
[1mStep[0m  [64/84], [94mLoss[0m : 2.24867
[1mStep[0m  [72/84], [94mLoss[0m : 2.84376
[1mStep[0m  [80/84], [94mLoss[0m : 2.78394

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15924
[1mStep[0m  [8/84], [94mLoss[0m : 2.59138
[1mStep[0m  [16/84], [94mLoss[0m : 2.43214
[1mStep[0m  [24/84], [94mLoss[0m : 2.34198
[1mStep[0m  [32/84], [94mLoss[0m : 2.50614
[1mStep[0m  [40/84], [94mLoss[0m : 2.40460
[1mStep[0m  [48/84], [94mLoss[0m : 2.63298
[1mStep[0m  [56/84], [94mLoss[0m : 2.76330
[1mStep[0m  [64/84], [94mLoss[0m : 2.51708
[1mStep[0m  [72/84], [94mLoss[0m : 2.73809
[1mStep[0m  [80/84], [94mLoss[0m : 2.65327

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41198
[1mStep[0m  [8/84], [94mLoss[0m : 2.52742
[1mStep[0m  [16/84], [94mLoss[0m : 1.98066
[1mStep[0m  [24/84], [94mLoss[0m : 2.88648
[1mStep[0m  [32/84], [94mLoss[0m : 2.80737
[1mStep[0m  [40/84], [94mLoss[0m : 2.49538
[1mStep[0m  [48/84], [94mLoss[0m : 2.56430
[1mStep[0m  [56/84], [94mLoss[0m : 2.79861
[1mStep[0m  [64/84], [94mLoss[0m : 2.83381
[1mStep[0m  [72/84], [94mLoss[0m : 2.38077
[1mStep[0m  [80/84], [94mLoss[0m : 2.41115

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18503
[1mStep[0m  [8/84], [94mLoss[0m : 2.54665
[1mStep[0m  [16/84], [94mLoss[0m : 2.62594
[1mStep[0m  [24/84], [94mLoss[0m : 2.39204
[1mStep[0m  [32/84], [94mLoss[0m : 2.30562
[1mStep[0m  [40/84], [94mLoss[0m : 2.70908
[1mStep[0m  [48/84], [94mLoss[0m : 2.43015
[1mStep[0m  [56/84], [94mLoss[0m : 2.63385
[1mStep[0m  [64/84], [94mLoss[0m : 2.44382
[1mStep[0m  [72/84], [94mLoss[0m : 2.69921
[1mStep[0m  [80/84], [94mLoss[0m : 2.54034

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.317, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27371
[1mStep[0m  [8/84], [94mLoss[0m : 2.40846
[1mStep[0m  [16/84], [94mLoss[0m : 2.44036
[1mStep[0m  [24/84], [94mLoss[0m : 2.37812
[1mStep[0m  [32/84], [94mLoss[0m : 2.48690
[1mStep[0m  [40/84], [94mLoss[0m : 2.31355
[1mStep[0m  [48/84], [94mLoss[0m : 2.48221
[1mStep[0m  [56/84], [94mLoss[0m : 2.47962
[1mStep[0m  [64/84], [94mLoss[0m : 2.68818
[1mStep[0m  [72/84], [94mLoss[0m : 2.50030
[1mStep[0m  [80/84], [94mLoss[0m : 2.60398

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60231
[1mStep[0m  [8/84], [94mLoss[0m : 2.44462
[1mStep[0m  [16/84], [94mLoss[0m : 2.20347
[1mStep[0m  [24/84], [94mLoss[0m : 2.29306
[1mStep[0m  [32/84], [94mLoss[0m : 2.50205
[1mStep[0m  [40/84], [94mLoss[0m : 2.28803
[1mStep[0m  [48/84], [94mLoss[0m : 2.61690
[1mStep[0m  [56/84], [94mLoss[0m : 2.38738
[1mStep[0m  [64/84], [94mLoss[0m : 2.18961
[1mStep[0m  [72/84], [94mLoss[0m : 2.23921
[1mStep[0m  [80/84], [94mLoss[0m : 2.58351

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33650
[1mStep[0m  [8/84], [94mLoss[0m : 2.12514
[1mStep[0m  [16/84], [94mLoss[0m : 2.23877
[1mStep[0m  [24/84], [94mLoss[0m : 2.44196
[1mStep[0m  [32/84], [94mLoss[0m : 2.24274
[1mStep[0m  [40/84], [94mLoss[0m : 2.54559
[1mStep[0m  [48/84], [94mLoss[0m : 2.56551
[1mStep[0m  [56/84], [94mLoss[0m : 2.38468
[1mStep[0m  [64/84], [94mLoss[0m : 2.38188
[1mStep[0m  [72/84], [94mLoss[0m : 2.50075
[1mStep[0m  [80/84], [94mLoss[0m : 2.56623

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41231
[1mStep[0m  [8/84], [94mLoss[0m : 2.55297
[1mStep[0m  [16/84], [94mLoss[0m : 2.62440
[1mStep[0m  [24/84], [94mLoss[0m : 2.30934
[1mStep[0m  [32/84], [94mLoss[0m : 2.60109
[1mStep[0m  [40/84], [94mLoss[0m : 2.53683
[1mStep[0m  [48/84], [94mLoss[0m : 2.38029
[1mStep[0m  [56/84], [94mLoss[0m : 2.31564
[1mStep[0m  [64/84], [94mLoss[0m : 2.38922
[1mStep[0m  [72/84], [94mLoss[0m : 2.13898
[1mStep[0m  [80/84], [94mLoss[0m : 2.22765

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54429
[1mStep[0m  [8/84], [94mLoss[0m : 2.21913
[1mStep[0m  [16/84], [94mLoss[0m : 2.40369
[1mStep[0m  [24/84], [94mLoss[0m : 2.55998
[1mStep[0m  [32/84], [94mLoss[0m : 2.65365
[1mStep[0m  [40/84], [94mLoss[0m : 2.69437
[1mStep[0m  [48/84], [94mLoss[0m : 2.22207
[1mStep[0m  [56/84], [94mLoss[0m : 2.61931
[1mStep[0m  [64/84], [94mLoss[0m : 2.63559
[1mStep[0m  [72/84], [94mLoss[0m : 2.54924
[1mStep[0m  [80/84], [94mLoss[0m : 2.26020

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38618
[1mStep[0m  [8/84], [94mLoss[0m : 2.48305
[1mStep[0m  [16/84], [94mLoss[0m : 2.35818
[1mStep[0m  [24/84], [94mLoss[0m : 2.70662
[1mStep[0m  [32/84], [94mLoss[0m : 2.54470
[1mStep[0m  [40/84], [94mLoss[0m : 2.45850
[1mStep[0m  [48/84], [94mLoss[0m : 2.56898
[1mStep[0m  [56/84], [94mLoss[0m : 2.60988
[1mStep[0m  [64/84], [94mLoss[0m : 2.34518
[1mStep[0m  [72/84], [94mLoss[0m : 2.25016
[1mStep[0m  [80/84], [94mLoss[0m : 2.91394

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36260
[1mStep[0m  [8/84], [94mLoss[0m : 2.92040
[1mStep[0m  [16/84], [94mLoss[0m : 2.43141
[1mStep[0m  [24/84], [94mLoss[0m : 2.36785
[1mStep[0m  [32/84], [94mLoss[0m : 2.33091
[1mStep[0m  [40/84], [94mLoss[0m : 2.36600
[1mStep[0m  [48/84], [94mLoss[0m : 2.39056
[1mStep[0m  [56/84], [94mLoss[0m : 2.59681
[1mStep[0m  [64/84], [94mLoss[0m : 2.47096
[1mStep[0m  [72/84], [94mLoss[0m : 2.31486
[1mStep[0m  [80/84], [94mLoss[0m : 2.49269

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.306, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51541
[1mStep[0m  [8/84], [94mLoss[0m : 2.57588
[1mStep[0m  [16/84], [94mLoss[0m : 2.46266
[1mStep[0m  [24/84], [94mLoss[0m : 2.06429
[1mStep[0m  [32/84], [94mLoss[0m : 2.50223
[1mStep[0m  [40/84], [94mLoss[0m : 2.62165
[1mStep[0m  [48/84], [94mLoss[0m : 2.42535
[1mStep[0m  [56/84], [94mLoss[0m : 2.31038
[1mStep[0m  [64/84], [94mLoss[0m : 2.24646
[1mStep[0m  [72/84], [94mLoss[0m : 2.47946
[1mStep[0m  [80/84], [94mLoss[0m : 2.47647

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.307, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27353
[1mStep[0m  [8/84], [94mLoss[0m : 2.13803
[1mStep[0m  [16/84], [94mLoss[0m : 2.53037
[1mStep[0m  [24/84], [94mLoss[0m : 2.53464
[1mStep[0m  [32/84], [94mLoss[0m : 2.85702
[1mStep[0m  [40/84], [94mLoss[0m : 2.11794
[1mStep[0m  [48/84], [94mLoss[0m : 2.21091
[1mStep[0m  [56/84], [94mLoss[0m : 2.42062
[1mStep[0m  [64/84], [94mLoss[0m : 2.52214
[1mStep[0m  [72/84], [94mLoss[0m : 2.44001
[1mStep[0m  [80/84], [94mLoss[0m : 2.51606

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.305, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47839
[1mStep[0m  [8/84], [94mLoss[0m : 2.42290
[1mStep[0m  [16/84], [94mLoss[0m : 2.34411
[1mStep[0m  [24/84], [94mLoss[0m : 2.55652
[1mStep[0m  [32/84], [94mLoss[0m : 2.30618
[1mStep[0m  [40/84], [94mLoss[0m : 2.32949
[1mStep[0m  [48/84], [94mLoss[0m : 2.47145
[1mStep[0m  [56/84], [94mLoss[0m : 2.56682
[1mStep[0m  [64/84], [94mLoss[0m : 2.71594
[1mStep[0m  [72/84], [94mLoss[0m : 2.22072
[1mStep[0m  [80/84], [94mLoss[0m : 2.39117

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37307
[1mStep[0m  [8/84], [94mLoss[0m : 2.63443
[1mStep[0m  [16/84], [94mLoss[0m : 2.56032
[1mStep[0m  [24/84], [94mLoss[0m : 2.41470
[1mStep[0m  [32/84], [94mLoss[0m : 2.54764
[1mStep[0m  [40/84], [94mLoss[0m : 2.69680
[1mStep[0m  [48/84], [94mLoss[0m : 2.09556
[1mStep[0m  [56/84], [94mLoss[0m : 2.67671
[1mStep[0m  [64/84], [94mLoss[0m : 2.15351
[1mStep[0m  [72/84], [94mLoss[0m : 2.22246
[1mStep[0m  [80/84], [94mLoss[0m : 2.57174

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58907
[1mStep[0m  [8/84], [94mLoss[0m : 2.61947
[1mStep[0m  [16/84], [94mLoss[0m : 2.46335
[1mStep[0m  [24/84], [94mLoss[0m : 2.17539
[1mStep[0m  [32/84], [94mLoss[0m : 2.55812
[1mStep[0m  [40/84], [94mLoss[0m : 2.63031
[1mStep[0m  [48/84], [94mLoss[0m : 2.50942
[1mStep[0m  [56/84], [94mLoss[0m : 2.33031
[1mStep[0m  [64/84], [94mLoss[0m : 2.74755
[1mStep[0m  [72/84], [94mLoss[0m : 2.32341
[1mStep[0m  [80/84], [94mLoss[0m : 2.34371

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51382
[1mStep[0m  [8/84], [94mLoss[0m : 2.39389
[1mStep[0m  [16/84], [94mLoss[0m : 2.30711
[1mStep[0m  [24/84], [94mLoss[0m : 2.32338
[1mStep[0m  [32/84], [94mLoss[0m : 2.21821
[1mStep[0m  [40/84], [94mLoss[0m : 2.55300
[1mStep[0m  [48/84], [94mLoss[0m : 2.38658
[1mStep[0m  [56/84], [94mLoss[0m : 2.72662
[1mStep[0m  [64/84], [94mLoss[0m : 2.34516
[1mStep[0m  [72/84], [94mLoss[0m : 2.38842
[1mStep[0m  [80/84], [94mLoss[0m : 2.55332

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13608
[1mStep[0m  [8/84], [94mLoss[0m : 2.09261
[1mStep[0m  [16/84], [94mLoss[0m : 2.51678
[1mStep[0m  [24/84], [94mLoss[0m : 2.62339
[1mStep[0m  [32/84], [94mLoss[0m : 2.36508
[1mStep[0m  [40/84], [94mLoss[0m : 2.45688
[1mStep[0m  [48/84], [94mLoss[0m : 2.43311
[1mStep[0m  [56/84], [94mLoss[0m : 2.42355
[1mStep[0m  [64/84], [94mLoss[0m : 2.41986
[1mStep[0m  [72/84], [94mLoss[0m : 2.24546
[1mStep[0m  [80/84], [94mLoss[0m : 2.02590

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67748
[1mStep[0m  [8/84], [94mLoss[0m : 2.48715
[1mStep[0m  [16/84], [94mLoss[0m : 2.47059
[1mStep[0m  [24/84], [94mLoss[0m : 2.25810
[1mStep[0m  [32/84], [94mLoss[0m : 2.38667
[1mStep[0m  [40/84], [94mLoss[0m : 2.58899
[1mStep[0m  [48/84], [94mLoss[0m : 2.11244
[1mStep[0m  [56/84], [94mLoss[0m : 2.26230
[1mStep[0m  [64/84], [94mLoss[0m : 2.50936
[1mStep[0m  [72/84], [94mLoss[0m : 2.35015
[1mStep[0m  [80/84], [94mLoss[0m : 2.27741

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.296, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48526
[1mStep[0m  [8/84], [94mLoss[0m : 2.75631
[1mStep[0m  [16/84], [94mLoss[0m : 2.28555
[1mStep[0m  [24/84], [94mLoss[0m : 2.04906
[1mStep[0m  [32/84], [94mLoss[0m : 2.27726
[1mStep[0m  [40/84], [94mLoss[0m : 1.98570
[1mStep[0m  [48/84], [94mLoss[0m : 2.27564
[1mStep[0m  [56/84], [94mLoss[0m : 2.40193
[1mStep[0m  [64/84], [94mLoss[0m : 2.62195
[1mStep[0m  [72/84], [94mLoss[0m : 2.43293
[1mStep[0m  [80/84], [94mLoss[0m : 2.36807

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.303, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36533
[1mStep[0m  [8/84], [94mLoss[0m : 2.52986
[1mStep[0m  [16/84], [94mLoss[0m : 2.42719
[1mStep[0m  [24/84], [94mLoss[0m : 2.30130
[1mStep[0m  [32/84], [94mLoss[0m : 2.57067
[1mStep[0m  [40/84], [94mLoss[0m : 2.40288
[1mStep[0m  [48/84], [94mLoss[0m : 2.38912
[1mStep[0m  [56/84], [94mLoss[0m : 2.25793
[1mStep[0m  [64/84], [94mLoss[0m : 2.52057
[1mStep[0m  [72/84], [94mLoss[0m : 2.25391
[1mStep[0m  [80/84], [94mLoss[0m : 2.49181

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.296, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24333
[1mStep[0m  [8/84], [94mLoss[0m : 1.96246
[1mStep[0m  [16/84], [94mLoss[0m : 2.45405
[1mStep[0m  [24/84], [94mLoss[0m : 2.14072
[1mStep[0m  [32/84], [94mLoss[0m : 2.34137
[1mStep[0m  [40/84], [94mLoss[0m : 2.28341
[1mStep[0m  [48/84], [94mLoss[0m : 2.30242
[1mStep[0m  [56/84], [94mLoss[0m : 2.50440
[1mStep[0m  [64/84], [94mLoss[0m : 2.82455
[1mStep[0m  [72/84], [94mLoss[0m : 2.21690
[1mStep[0m  [80/84], [94mLoss[0m : 2.29742

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.312, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56188
[1mStep[0m  [8/84], [94mLoss[0m : 2.44825
[1mStep[0m  [16/84], [94mLoss[0m : 2.15940
[1mStep[0m  [24/84], [94mLoss[0m : 2.93877
[1mStep[0m  [32/84], [94mLoss[0m : 2.42788
[1mStep[0m  [40/84], [94mLoss[0m : 2.02139
[1mStep[0m  [48/84], [94mLoss[0m : 2.42116
[1mStep[0m  [56/84], [94mLoss[0m : 2.58357
[1mStep[0m  [64/84], [94mLoss[0m : 2.34972
[1mStep[0m  [72/84], [94mLoss[0m : 2.20395
[1mStep[0m  [80/84], [94mLoss[0m : 2.50759

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.309, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15405
[1mStep[0m  [8/84], [94mLoss[0m : 2.33098
[1mStep[0m  [16/84], [94mLoss[0m : 2.42141
[1mStep[0m  [24/84], [94mLoss[0m : 2.29460
[1mStep[0m  [32/84], [94mLoss[0m : 2.35501
[1mStep[0m  [40/84], [94mLoss[0m : 2.32968
[1mStep[0m  [48/84], [94mLoss[0m : 2.41321
[1mStep[0m  [56/84], [94mLoss[0m : 2.22247
[1mStep[0m  [64/84], [94mLoss[0m : 2.54023
[1mStep[0m  [72/84], [94mLoss[0m : 2.50739
[1mStep[0m  [80/84], [94mLoss[0m : 2.23422

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.307, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35434
[1mStep[0m  [8/84], [94mLoss[0m : 2.28832
[1mStep[0m  [16/84], [94mLoss[0m : 2.12403
[1mStep[0m  [24/84], [94mLoss[0m : 2.44464
[1mStep[0m  [32/84], [94mLoss[0m : 2.55287
[1mStep[0m  [40/84], [94mLoss[0m : 2.25571
[1mStep[0m  [48/84], [94mLoss[0m : 2.44689
[1mStep[0m  [56/84], [94mLoss[0m : 2.22353
[1mStep[0m  [64/84], [94mLoss[0m : 2.45831
[1mStep[0m  [72/84], [94mLoss[0m : 2.84491
[1mStep[0m  [80/84], [94mLoss[0m : 2.50429

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.312, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31656
[1mStep[0m  [8/84], [94mLoss[0m : 2.21521
[1mStep[0m  [16/84], [94mLoss[0m : 2.11287
[1mStep[0m  [24/84], [94mLoss[0m : 2.36358
[1mStep[0m  [32/84], [94mLoss[0m : 2.45276
[1mStep[0m  [40/84], [94mLoss[0m : 2.50926
[1mStep[0m  [48/84], [94mLoss[0m : 2.47241
[1mStep[0m  [56/84], [94mLoss[0m : 2.18264
[1mStep[0m  [64/84], [94mLoss[0m : 2.18440
[1mStep[0m  [72/84], [94mLoss[0m : 2.47887
[1mStep[0m  [80/84], [94mLoss[0m : 2.60555

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.297, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.296
====================================

Phase 1 - Evaluation MAE:  2.2964504616601125
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.24622
[1mStep[0m  [8/84], [94mLoss[0m : 2.28347
[1mStep[0m  [16/84], [94mLoss[0m : 2.63524
[1mStep[0m  [24/84], [94mLoss[0m : 2.34786
[1mStep[0m  [32/84], [94mLoss[0m : 2.71225
[1mStep[0m  [40/84], [94mLoss[0m : 2.54919
[1mStep[0m  [48/84], [94mLoss[0m : 2.53653
[1mStep[0m  [56/84], [94mLoss[0m : 2.60842
[1mStep[0m  [64/84], [94mLoss[0m : 2.62366
[1mStep[0m  [72/84], [94mLoss[0m : 2.65772
[1mStep[0m  [80/84], [94mLoss[0m : 2.74180

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.296, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41260
[1mStep[0m  [8/84], [94mLoss[0m : 2.60273
[1mStep[0m  [16/84], [94mLoss[0m : 2.24085
[1mStep[0m  [24/84], [94mLoss[0m : 2.72779
[1mStep[0m  [32/84], [94mLoss[0m : 2.40442
[1mStep[0m  [40/84], [94mLoss[0m : 2.27780
[1mStep[0m  [48/84], [94mLoss[0m : 2.86074
[1mStep[0m  [56/84], [94mLoss[0m : 2.47051
[1mStep[0m  [64/84], [94mLoss[0m : 2.21756
[1mStep[0m  [72/84], [94mLoss[0m : 2.38133
[1mStep[0m  [80/84], [94mLoss[0m : 2.19388

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43207
[1mStep[0m  [8/84], [94mLoss[0m : 2.15951
[1mStep[0m  [16/84], [94mLoss[0m : 1.99043
[1mStep[0m  [24/84], [94mLoss[0m : 2.07487
[1mStep[0m  [32/84], [94mLoss[0m : 2.39090
[1mStep[0m  [40/84], [94mLoss[0m : 2.09364
[1mStep[0m  [48/84], [94mLoss[0m : 2.19845
[1mStep[0m  [56/84], [94mLoss[0m : 2.20003
[1mStep[0m  [64/84], [94mLoss[0m : 2.45320
[1mStep[0m  [72/84], [94mLoss[0m : 2.64124
[1mStep[0m  [80/84], [94mLoss[0m : 2.18655

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28071
[1mStep[0m  [8/84], [94mLoss[0m : 2.28367
[1mStep[0m  [16/84], [94mLoss[0m : 2.11026
[1mStep[0m  [24/84], [94mLoss[0m : 2.57646
[1mStep[0m  [32/84], [94mLoss[0m : 1.88403
[1mStep[0m  [40/84], [94mLoss[0m : 2.19586
[1mStep[0m  [48/84], [94mLoss[0m : 2.38371
[1mStep[0m  [56/84], [94mLoss[0m : 2.12688
[1mStep[0m  [64/84], [94mLoss[0m : 2.15595
[1mStep[0m  [72/84], [94mLoss[0m : 2.33812
[1mStep[0m  [80/84], [94mLoss[0m : 2.04586

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03313
[1mStep[0m  [8/84], [94mLoss[0m : 2.26013
[1mStep[0m  [16/84], [94mLoss[0m : 2.12280
[1mStep[0m  [24/84], [94mLoss[0m : 2.16698
[1mStep[0m  [32/84], [94mLoss[0m : 2.41449
[1mStep[0m  [40/84], [94mLoss[0m : 2.16438
[1mStep[0m  [48/84], [94mLoss[0m : 2.19245
[1mStep[0m  [56/84], [94mLoss[0m : 2.13313
[1mStep[0m  [64/84], [94mLoss[0m : 2.06944
[1mStep[0m  [72/84], [94mLoss[0m : 2.16956
[1mStep[0m  [80/84], [94mLoss[0m : 2.32579

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02115
[1mStep[0m  [8/84], [94mLoss[0m : 1.90343
[1mStep[0m  [16/84], [94mLoss[0m : 2.25775
[1mStep[0m  [24/84], [94mLoss[0m : 2.21713
[1mStep[0m  [32/84], [94mLoss[0m : 2.36187
[1mStep[0m  [40/84], [94mLoss[0m : 2.29814
[1mStep[0m  [48/84], [94mLoss[0m : 2.04445
[1mStep[0m  [56/84], [94mLoss[0m : 2.17689
[1mStep[0m  [64/84], [94mLoss[0m : 2.17704
[1mStep[0m  [72/84], [94mLoss[0m : 2.15896
[1mStep[0m  [80/84], [94mLoss[0m : 1.92779

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02022
[1mStep[0m  [8/84], [94mLoss[0m : 2.01882
[1mStep[0m  [16/84], [94mLoss[0m : 1.92509
[1mStep[0m  [24/84], [94mLoss[0m : 2.38242
[1mStep[0m  [32/84], [94mLoss[0m : 2.13927
[1mStep[0m  [40/84], [94mLoss[0m : 2.27460
[1mStep[0m  [48/84], [94mLoss[0m : 2.11136
[1mStep[0m  [56/84], [94mLoss[0m : 2.11613
[1mStep[0m  [64/84], [94mLoss[0m : 2.31620
[1mStep[0m  [72/84], [94mLoss[0m : 2.04177
[1mStep[0m  [80/84], [94mLoss[0m : 2.15697

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38543
[1mStep[0m  [8/84], [94mLoss[0m : 1.99869
[1mStep[0m  [16/84], [94mLoss[0m : 1.64826
[1mStep[0m  [24/84], [94mLoss[0m : 1.72892
[1mStep[0m  [32/84], [94mLoss[0m : 1.87161
[1mStep[0m  [40/84], [94mLoss[0m : 1.90709
[1mStep[0m  [48/84], [94mLoss[0m : 1.73782
[1mStep[0m  [56/84], [94mLoss[0m : 1.88082
[1mStep[0m  [64/84], [94mLoss[0m : 1.89005
[1mStep[0m  [72/84], [94mLoss[0m : 2.35433
[1mStep[0m  [80/84], [94mLoss[0m : 2.01097

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01133
[1mStep[0m  [8/84], [94mLoss[0m : 2.02623
[1mStep[0m  [16/84], [94mLoss[0m : 2.22779
[1mStep[0m  [24/84], [94mLoss[0m : 2.04922
[1mStep[0m  [32/84], [94mLoss[0m : 1.72777
[1mStep[0m  [40/84], [94mLoss[0m : 2.32613
[1mStep[0m  [48/84], [94mLoss[0m : 2.39478
[1mStep[0m  [56/84], [94mLoss[0m : 1.81827
[1mStep[0m  [64/84], [94mLoss[0m : 1.70286
[1mStep[0m  [72/84], [94mLoss[0m : 1.94827
[1mStep[0m  [80/84], [94mLoss[0m : 1.97881

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.982, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71863
[1mStep[0m  [8/84], [94mLoss[0m : 1.92650
[1mStep[0m  [16/84], [94mLoss[0m : 1.89309
[1mStep[0m  [24/84], [94mLoss[0m : 2.04357
[1mStep[0m  [32/84], [94mLoss[0m : 1.93078
[1mStep[0m  [40/84], [94mLoss[0m : 1.85470
[1mStep[0m  [48/84], [94mLoss[0m : 1.94318
[1mStep[0m  [56/84], [94mLoss[0m : 1.86539
[1mStep[0m  [64/84], [94mLoss[0m : 1.93394
[1mStep[0m  [72/84], [94mLoss[0m : 1.95386
[1mStep[0m  [80/84], [94mLoss[0m : 1.84092

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96269
[1mStep[0m  [8/84], [94mLoss[0m : 1.97117
[1mStep[0m  [16/84], [94mLoss[0m : 2.01535
[1mStep[0m  [24/84], [94mLoss[0m : 1.88542
[1mStep[0m  [32/84], [94mLoss[0m : 2.01125
[1mStep[0m  [40/84], [94mLoss[0m : 2.11080
[1mStep[0m  [48/84], [94mLoss[0m : 1.70772
[1mStep[0m  [56/84], [94mLoss[0m : 1.75528
[1mStep[0m  [64/84], [94mLoss[0m : 1.79659
[1mStep[0m  [72/84], [94mLoss[0m : 2.05200
[1mStep[0m  [80/84], [94mLoss[0m : 2.00992

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80715
[1mStep[0m  [8/84], [94mLoss[0m : 1.71480
[1mStep[0m  [16/84], [94mLoss[0m : 1.90477
[1mStep[0m  [24/84], [94mLoss[0m : 1.88971
[1mStep[0m  [32/84], [94mLoss[0m : 1.86686
[1mStep[0m  [40/84], [94mLoss[0m : 1.92505
[1mStep[0m  [48/84], [94mLoss[0m : 2.20971
[1mStep[0m  [56/84], [94mLoss[0m : 1.84218
[1mStep[0m  [64/84], [94mLoss[0m : 1.97705
[1mStep[0m  [72/84], [94mLoss[0m : 1.89832
[1mStep[0m  [80/84], [94mLoss[0m : 1.97640

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74277
[1mStep[0m  [8/84], [94mLoss[0m : 1.92486
[1mStep[0m  [16/84], [94mLoss[0m : 1.78785
[1mStep[0m  [24/84], [94mLoss[0m : 2.10560
[1mStep[0m  [32/84], [94mLoss[0m : 1.97669
[1mStep[0m  [40/84], [94mLoss[0m : 1.73528
[1mStep[0m  [48/84], [94mLoss[0m : 1.70807
[1mStep[0m  [56/84], [94mLoss[0m : 1.76542
[1mStep[0m  [64/84], [94mLoss[0m : 1.69212
[1mStep[0m  [72/84], [94mLoss[0m : 2.06007
[1mStep[0m  [80/84], [94mLoss[0m : 1.93955

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.864, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80361
[1mStep[0m  [8/84], [94mLoss[0m : 1.72366
[1mStep[0m  [16/84], [94mLoss[0m : 1.90212
[1mStep[0m  [24/84], [94mLoss[0m : 1.71842
[1mStep[0m  [32/84], [94mLoss[0m : 1.94033
[1mStep[0m  [40/84], [94mLoss[0m : 2.07523
[1mStep[0m  [48/84], [94mLoss[0m : 1.69032
[1mStep[0m  [56/84], [94mLoss[0m : 1.58415
[1mStep[0m  [64/84], [94mLoss[0m : 1.68243
[1mStep[0m  [72/84], [94mLoss[0m : 2.08772
[1mStep[0m  [80/84], [94mLoss[0m : 2.23224

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03512
[1mStep[0m  [8/84], [94mLoss[0m : 1.53312
[1mStep[0m  [16/84], [94mLoss[0m : 1.67488
[1mStep[0m  [24/84], [94mLoss[0m : 1.66985
[1mStep[0m  [32/84], [94mLoss[0m : 1.63629
[1mStep[0m  [40/84], [94mLoss[0m : 1.81983
[1mStep[0m  [48/84], [94mLoss[0m : 2.03616
[1mStep[0m  [56/84], [94mLoss[0m : 1.76007
[1mStep[0m  [64/84], [94mLoss[0m : 1.87554
[1mStep[0m  [72/84], [94mLoss[0m : 2.05264
[1mStep[0m  [80/84], [94mLoss[0m : 2.11560

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73274
[1mStep[0m  [8/84], [94mLoss[0m : 1.63023
[1mStep[0m  [16/84], [94mLoss[0m : 1.79778
[1mStep[0m  [24/84], [94mLoss[0m : 1.78656
[1mStep[0m  [32/84], [94mLoss[0m : 1.85265
[1mStep[0m  [40/84], [94mLoss[0m : 2.16748
[1mStep[0m  [48/84], [94mLoss[0m : 1.80191
[1mStep[0m  [56/84], [94mLoss[0m : 1.99693
[1mStep[0m  [64/84], [94mLoss[0m : 1.69999
[1mStep[0m  [72/84], [94mLoss[0m : 1.72951
[1mStep[0m  [80/84], [94mLoss[0m : 1.79892

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65143
[1mStep[0m  [8/84], [94mLoss[0m : 1.75697
[1mStep[0m  [16/84], [94mLoss[0m : 1.89528
[1mStep[0m  [24/84], [94mLoss[0m : 1.69586
[1mStep[0m  [32/84], [94mLoss[0m : 1.48352
[1mStep[0m  [40/84], [94mLoss[0m : 1.58769
[1mStep[0m  [48/84], [94mLoss[0m : 1.88295
[1mStep[0m  [56/84], [94mLoss[0m : 1.85964
[1mStep[0m  [64/84], [94mLoss[0m : 1.90337
[1mStep[0m  [72/84], [94mLoss[0m : 1.86999
[1mStep[0m  [80/84], [94mLoss[0m : 1.84136

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79542
[1mStep[0m  [8/84], [94mLoss[0m : 1.63403
[1mStep[0m  [16/84], [94mLoss[0m : 1.68132
[1mStep[0m  [24/84], [94mLoss[0m : 1.65521
[1mStep[0m  [32/84], [94mLoss[0m : 1.84494
[1mStep[0m  [40/84], [94mLoss[0m : 1.90931
[1mStep[0m  [48/84], [94mLoss[0m : 1.78124
[1mStep[0m  [56/84], [94mLoss[0m : 1.94047
[1mStep[0m  [64/84], [94mLoss[0m : 1.74417
[1mStep[0m  [72/84], [94mLoss[0m : 2.00575
[1mStep[0m  [80/84], [94mLoss[0m : 1.92938

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72149
[1mStep[0m  [8/84], [94mLoss[0m : 1.62190
[1mStep[0m  [16/84], [94mLoss[0m : 1.77046
[1mStep[0m  [24/84], [94mLoss[0m : 2.04173
[1mStep[0m  [32/84], [94mLoss[0m : 1.78456
[1mStep[0m  [40/84], [94mLoss[0m : 1.78915
[1mStep[0m  [48/84], [94mLoss[0m : 1.66806
[1mStep[0m  [56/84], [94mLoss[0m : 1.88897
[1mStep[0m  [64/84], [94mLoss[0m : 1.79656
[1mStep[0m  [72/84], [94mLoss[0m : 1.76935
[1mStep[0m  [80/84], [94mLoss[0m : 1.54409

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68034
[1mStep[0m  [8/84], [94mLoss[0m : 1.44659
[1mStep[0m  [16/84], [94mLoss[0m : 1.70847
[1mStep[0m  [24/84], [94mLoss[0m : 1.68042
[1mStep[0m  [32/84], [94mLoss[0m : 1.66264
[1mStep[0m  [40/84], [94mLoss[0m : 1.71193
[1mStep[0m  [48/84], [94mLoss[0m : 1.67293
[1mStep[0m  [56/84], [94mLoss[0m : 1.68748
[1mStep[0m  [64/84], [94mLoss[0m : 1.98399
[1mStep[0m  [72/84], [94mLoss[0m : 1.85044
[1mStep[0m  [80/84], [94mLoss[0m : 1.86265

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.98950
[1mStep[0m  [8/84], [94mLoss[0m : 1.61272
[1mStep[0m  [16/84], [94mLoss[0m : 1.72741
[1mStep[0m  [24/84], [94mLoss[0m : 1.72572
[1mStep[0m  [32/84], [94mLoss[0m : 1.72092
[1mStep[0m  [40/84], [94mLoss[0m : 1.64755
[1mStep[0m  [48/84], [94mLoss[0m : 1.79100
[1mStep[0m  [56/84], [94mLoss[0m : 1.68546
[1mStep[0m  [64/84], [94mLoss[0m : 1.65605
[1mStep[0m  [72/84], [94mLoss[0m : 1.70816
[1mStep[0m  [80/84], [94mLoss[0m : 1.55101

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95290
[1mStep[0m  [8/84], [94mLoss[0m : 1.51988
[1mStep[0m  [16/84], [94mLoss[0m : 1.64295
[1mStep[0m  [24/84], [94mLoss[0m : 1.55751
[1mStep[0m  [32/84], [94mLoss[0m : 1.79976
[1mStep[0m  [40/84], [94mLoss[0m : 1.60989
[1mStep[0m  [48/84], [94mLoss[0m : 1.71842
[1mStep[0m  [56/84], [94mLoss[0m : 1.59108
[1mStep[0m  [64/84], [94mLoss[0m : 1.73088
[1mStep[0m  [72/84], [94mLoss[0m : 1.84855
[1mStep[0m  [80/84], [94mLoss[0m : 1.90474

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62555
[1mStep[0m  [8/84], [94mLoss[0m : 1.48557
[1mStep[0m  [16/84], [94mLoss[0m : 1.66204
[1mStep[0m  [24/84], [94mLoss[0m : 1.43432
[1mStep[0m  [32/84], [94mLoss[0m : 1.45217
[1mStep[0m  [40/84], [94mLoss[0m : 1.85155
[1mStep[0m  [48/84], [94mLoss[0m : 1.80930
[1mStep[0m  [56/84], [94mLoss[0m : 2.07376
[1mStep[0m  [64/84], [94mLoss[0m : 1.72752
[1mStep[0m  [72/84], [94mLoss[0m : 1.87431
[1mStep[0m  [80/84], [94mLoss[0m : 1.68526

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62184
[1mStep[0m  [8/84], [94mLoss[0m : 1.65753
[1mStep[0m  [16/84], [94mLoss[0m : 1.49371
[1mStep[0m  [24/84], [94mLoss[0m : 1.70055
[1mStep[0m  [32/84], [94mLoss[0m : 1.63652
[1mStep[0m  [40/84], [94mLoss[0m : 1.75983
[1mStep[0m  [48/84], [94mLoss[0m : 1.73477
[1mStep[0m  [56/84], [94mLoss[0m : 1.53044
[1mStep[0m  [64/84], [94mLoss[0m : 1.80494
[1mStep[0m  [72/84], [94mLoss[0m : 1.66768
[1mStep[0m  [80/84], [94mLoss[0m : 1.71829

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.504, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64402
[1mStep[0m  [8/84], [94mLoss[0m : 1.41284
[1mStep[0m  [16/84], [94mLoss[0m : 1.50884
[1mStep[0m  [24/84], [94mLoss[0m : 1.43089
[1mStep[0m  [32/84], [94mLoss[0m : 1.56979
[1mStep[0m  [40/84], [94mLoss[0m : 1.66134
[1mStep[0m  [48/84], [94mLoss[0m : 1.57945
[1mStep[0m  [56/84], [94mLoss[0m : 1.94439
[1mStep[0m  [64/84], [94mLoss[0m : 1.63987
[1mStep[0m  [72/84], [94mLoss[0m : 1.72445
[1mStep[0m  [80/84], [94mLoss[0m : 2.00072

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81008
[1mStep[0m  [8/84], [94mLoss[0m : 1.53229
[1mStep[0m  [16/84], [94mLoss[0m : 1.66321
[1mStep[0m  [24/84], [94mLoss[0m : 1.65472
[1mStep[0m  [32/84], [94mLoss[0m : 1.59396
[1mStep[0m  [40/84], [94mLoss[0m : 1.74611
[1mStep[0m  [48/84], [94mLoss[0m : 1.63023
[1mStep[0m  [56/84], [94mLoss[0m : 1.65779
[1mStep[0m  [64/84], [94mLoss[0m : 1.58801
[1mStep[0m  [72/84], [94mLoss[0m : 1.69182
[1mStep[0m  [80/84], [94mLoss[0m : 1.73867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.42904
[1mStep[0m  [8/84], [94mLoss[0m : 1.79738
[1mStep[0m  [16/84], [94mLoss[0m : 1.77586
[1mStep[0m  [24/84], [94mLoss[0m : 1.52445
[1mStep[0m  [32/84], [94mLoss[0m : 1.46994
[1mStep[0m  [40/84], [94mLoss[0m : 1.63579
[1mStep[0m  [48/84], [94mLoss[0m : 1.46285
[1mStep[0m  [56/84], [94mLoss[0m : 1.47468
[1mStep[0m  [64/84], [94mLoss[0m : 1.76150
[1mStep[0m  [72/84], [94mLoss[0m : 1.69994
[1mStep[0m  [80/84], [94mLoss[0m : 2.04808

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52745
[1mStep[0m  [8/84], [94mLoss[0m : 1.69226
[1mStep[0m  [16/84], [94mLoss[0m : 1.50464
[1mStep[0m  [24/84], [94mLoss[0m : 1.57923
[1mStep[0m  [32/84], [94mLoss[0m : 1.69349
[1mStep[0m  [40/84], [94mLoss[0m : 1.56514
[1mStep[0m  [48/84], [94mLoss[0m : 1.50088
[1mStep[0m  [56/84], [94mLoss[0m : 1.74836
[1mStep[0m  [64/84], [94mLoss[0m : 2.13321
[1mStep[0m  [72/84], [94mLoss[0m : 1.48938
[1mStep[0m  [80/84], [94mLoss[0m : 1.77857

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.522, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65365
[1mStep[0m  [8/84], [94mLoss[0m : 1.35978
[1mStep[0m  [16/84], [94mLoss[0m : 1.62516
[1mStep[0m  [24/84], [94mLoss[0m : 1.56691
[1mStep[0m  [32/84], [94mLoss[0m : 1.57158
[1mStep[0m  [40/84], [94mLoss[0m : 1.52979
[1mStep[0m  [48/84], [94mLoss[0m : 1.85819
[1mStep[0m  [56/84], [94mLoss[0m : 1.58176
[1mStep[0m  [64/84], [94mLoss[0m : 1.77810
[1mStep[0m  [72/84], [94mLoss[0m : 1.80836
[1mStep[0m  [80/84], [94mLoss[0m : 1.73233

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.49038
[1mStep[0m  [8/84], [94mLoss[0m : 1.58777
[1mStep[0m  [16/84], [94mLoss[0m : 1.63413
[1mStep[0m  [24/84], [94mLoss[0m : 1.60174
[1mStep[0m  [32/84], [94mLoss[0m : 1.65354
[1mStep[0m  [40/84], [94mLoss[0m : 1.44090
[1mStep[0m  [48/84], [94mLoss[0m : 1.70333
[1mStep[0m  [56/84], [94mLoss[0m : 1.63695
[1mStep[0m  [64/84], [94mLoss[0m : 1.44393
[1mStep[0m  [72/84], [94mLoss[0m : 1.68345
[1mStep[0m  [80/84], [94mLoss[0m : 1.81274

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.615, [92mTest[0m: 2.610, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.519
====================================

Phase 2 - Evaluation MAE:  2.5189705150468007
MAE score P1        2.29645
MAE score P2       2.518971
loss               1.591472
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 20, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.20274
[1mStep[0m  [4/42], [94mLoss[0m : 10.36767
[1mStep[0m  [8/42], [94mLoss[0m : 9.04447
[1mStep[0m  [12/42], [94mLoss[0m : 6.76131
[1mStep[0m  [16/42], [94mLoss[0m : 4.67680
[1mStep[0m  [20/42], [94mLoss[0m : 3.23035
[1mStep[0m  [24/42], [94mLoss[0m : 2.87573
[1mStep[0m  [28/42], [94mLoss[0m : 3.17597
[1mStep[0m  [32/42], [94mLoss[0m : 3.25151
[1mStep[0m  [36/42], [94mLoss[0m : 2.85528
[1mStep[0m  [40/42], [94mLoss[0m : 2.60843

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.252, [92mTest[0m: 10.891, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.88622
[1mStep[0m  [4/42], [94mLoss[0m : 2.65536
[1mStep[0m  [8/42], [94mLoss[0m : 2.52739
[1mStep[0m  [12/42], [94mLoss[0m : 2.48531
[1mStep[0m  [16/42], [94mLoss[0m : 2.91575
[1mStep[0m  [20/42], [94mLoss[0m : 2.67524
[1mStep[0m  [24/42], [94mLoss[0m : 2.39937
[1mStep[0m  [28/42], [94mLoss[0m : 2.81877
[1mStep[0m  [32/42], [94mLoss[0m : 2.54880
[1mStep[0m  [36/42], [94mLoss[0m : 2.57792
[1mStep[0m  [40/42], [94mLoss[0m : 2.47647

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.686, [92mTest[0m: 3.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54157
[1mStep[0m  [4/42], [94mLoss[0m : 2.65839
[1mStep[0m  [8/42], [94mLoss[0m : 2.56428
[1mStep[0m  [12/42], [94mLoss[0m : 2.51866
[1mStep[0m  [16/42], [94mLoss[0m : 2.50939
[1mStep[0m  [20/42], [94mLoss[0m : 2.52383
[1mStep[0m  [24/42], [94mLoss[0m : 2.59884
[1mStep[0m  [28/42], [94mLoss[0m : 2.74773
[1mStep[0m  [32/42], [94mLoss[0m : 2.46533
[1mStep[0m  [36/42], [94mLoss[0m : 2.70364
[1mStep[0m  [40/42], [94mLoss[0m : 2.71064

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.680, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72189
[1mStep[0m  [4/42], [94mLoss[0m : 2.56182
[1mStep[0m  [8/42], [94mLoss[0m : 2.60111
[1mStep[0m  [12/42], [94mLoss[0m : 2.44778
[1mStep[0m  [16/42], [94mLoss[0m : 2.51723
[1mStep[0m  [20/42], [94mLoss[0m : 2.73877
[1mStep[0m  [24/42], [94mLoss[0m : 2.57904
[1mStep[0m  [28/42], [94mLoss[0m : 2.59969
[1mStep[0m  [32/42], [94mLoss[0m : 2.44776
[1mStep[0m  [36/42], [94mLoss[0m : 2.65134
[1mStep[0m  [40/42], [94mLoss[0m : 2.58832

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53470
[1mStep[0m  [4/42], [94mLoss[0m : 2.52418
[1mStep[0m  [8/42], [94mLoss[0m : 2.74441
[1mStep[0m  [12/42], [94mLoss[0m : 2.63784
[1mStep[0m  [16/42], [94mLoss[0m : 2.73878
[1mStep[0m  [20/42], [94mLoss[0m : 2.38996
[1mStep[0m  [24/42], [94mLoss[0m : 2.75428
[1mStep[0m  [28/42], [94mLoss[0m : 2.41118
[1mStep[0m  [32/42], [94mLoss[0m : 2.47096
[1mStep[0m  [36/42], [94mLoss[0m : 2.40682
[1mStep[0m  [40/42], [94mLoss[0m : 2.59073

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61999
[1mStep[0m  [4/42], [94mLoss[0m : 2.50403
[1mStep[0m  [8/42], [94mLoss[0m : 2.62479
[1mStep[0m  [12/42], [94mLoss[0m : 2.49209
[1mStep[0m  [16/42], [94mLoss[0m : 2.52852
[1mStep[0m  [20/42], [94mLoss[0m : 2.37400
[1mStep[0m  [24/42], [94mLoss[0m : 2.45836
[1mStep[0m  [28/42], [94mLoss[0m : 2.57562
[1mStep[0m  [32/42], [94mLoss[0m : 2.63322
[1mStep[0m  [36/42], [94mLoss[0m : 2.47572
[1mStep[0m  [40/42], [94mLoss[0m : 2.61240

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45515
[1mStep[0m  [4/42], [94mLoss[0m : 2.46245
[1mStep[0m  [8/42], [94mLoss[0m : 2.23821
[1mStep[0m  [12/42], [94mLoss[0m : 2.49433
[1mStep[0m  [16/42], [94mLoss[0m : 2.38271
[1mStep[0m  [20/42], [94mLoss[0m : 2.57894
[1mStep[0m  [24/42], [94mLoss[0m : 2.56869
[1mStep[0m  [28/42], [94mLoss[0m : 2.48069
[1mStep[0m  [32/42], [94mLoss[0m : 2.50757
[1mStep[0m  [36/42], [94mLoss[0m : 2.47475
[1mStep[0m  [40/42], [94mLoss[0m : 2.46775

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37942
[1mStep[0m  [4/42], [94mLoss[0m : 2.26443
[1mStep[0m  [8/42], [94mLoss[0m : 2.50442
[1mStep[0m  [12/42], [94mLoss[0m : 2.51378
[1mStep[0m  [16/42], [94mLoss[0m : 2.41744
[1mStep[0m  [20/42], [94mLoss[0m : 2.55662
[1mStep[0m  [24/42], [94mLoss[0m : 2.29604
[1mStep[0m  [28/42], [94mLoss[0m : 2.61789
[1mStep[0m  [32/42], [94mLoss[0m : 2.62729
[1mStep[0m  [36/42], [94mLoss[0m : 2.64625
[1mStep[0m  [40/42], [94mLoss[0m : 2.48256

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58551
[1mStep[0m  [4/42], [94mLoss[0m : 2.28195
[1mStep[0m  [8/42], [94mLoss[0m : 2.50427
[1mStep[0m  [12/42], [94mLoss[0m : 2.40629
[1mStep[0m  [16/42], [94mLoss[0m : 2.63089
[1mStep[0m  [20/42], [94mLoss[0m : 2.42239
[1mStep[0m  [24/42], [94mLoss[0m : 2.85408
[1mStep[0m  [28/42], [94mLoss[0m : 2.45299
[1mStep[0m  [32/42], [94mLoss[0m : 2.31680
[1mStep[0m  [36/42], [94mLoss[0m : 2.50597
[1mStep[0m  [40/42], [94mLoss[0m : 2.41234

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40483
[1mStep[0m  [4/42], [94mLoss[0m : 2.21493
[1mStep[0m  [8/42], [94mLoss[0m : 2.49522
[1mStep[0m  [12/42], [94mLoss[0m : 2.38245
[1mStep[0m  [16/42], [94mLoss[0m : 2.48102
[1mStep[0m  [20/42], [94mLoss[0m : 2.58626
[1mStep[0m  [24/42], [94mLoss[0m : 2.42472
[1mStep[0m  [28/42], [94mLoss[0m : 2.35706
[1mStep[0m  [32/42], [94mLoss[0m : 2.55920
[1mStep[0m  [36/42], [94mLoss[0m : 2.42617
[1mStep[0m  [40/42], [94mLoss[0m : 2.68480

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61594
[1mStep[0m  [4/42], [94mLoss[0m : 2.28929
[1mStep[0m  [8/42], [94mLoss[0m : 2.40100
[1mStep[0m  [12/42], [94mLoss[0m : 2.40795
[1mStep[0m  [16/42], [94mLoss[0m : 2.47817
[1mStep[0m  [20/42], [94mLoss[0m : 2.23441
[1mStep[0m  [24/42], [94mLoss[0m : 2.33246
[1mStep[0m  [28/42], [94mLoss[0m : 2.22343
[1mStep[0m  [32/42], [94mLoss[0m : 2.44898
[1mStep[0m  [36/42], [94mLoss[0m : 2.31965
[1mStep[0m  [40/42], [94mLoss[0m : 2.62175

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33116
[1mStep[0m  [4/42], [94mLoss[0m : 2.48642
[1mStep[0m  [8/42], [94mLoss[0m : 2.43394
[1mStep[0m  [12/42], [94mLoss[0m : 2.28594
[1mStep[0m  [16/42], [94mLoss[0m : 2.46922
[1mStep[0m  [20/42], [94mLoss[0m : 2.38489
[1mStep[0m  [24/42], [94mLoss[0m : 2.54708
[1mStep[0m  [28/42], [94mLoss[0m : 2.42842
[1mStep[0m  [32/42], [94mLoss[0m : 2.39876
[1mStep[0m  [36/42], [94mLoss[0m : 2.31209
[1mStep[0m  [40/42], [94mLoss[0m : 2.59887

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45966
[1mStep[0m  [4/42], [94mLoss[0m : 2.49829
[1mStep[0m  [8/42], [94mLoss[0m : 2.35580
[1mStep[0m  [12/42], [94mLoss[0m : 2.29277
[1mStep[0m  [16/42], [94mLoss[0m : 2.39867
[1mStep[0m  [20/42], [94mLoss[0m : 2.60517
[1mStep[0m  [24/42], [94mLoss[0m : 2.46385
[1mStep[0m  [28/42], [94mLoss[0m : 2.51819
[1mStep[0m  [32/42], [94mLoss[0m : 2.37709
[1mStep[0m  [36/42], [94mLoss[0m : 2.35240
[1mStep[0m  [40/42], [94mLoss[0m : 2.23702

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30268
[1mStep[0m  [4/42], [94mLoss[0m : 2.45712
[1mStep[0m  [8/42], [94mLoss[0m : 2.41702
[1mStep[0m  [12/42], [94mLoss[0m : 2.48892
[1mStep[0m  [16/42], [94mLoss[0m : 2.37457
[1mStep[0m  [20/42], [94mLoss[0m : 2.41892
[1mStep[0m  [24/42], [94mLoss[0m : 2.50972
[1mStep[0m  [28/42], [94mLoss[0m : 2.36959
[1mStep[0m  [32/42], [94mLoss[0m : 2.42585
[1mStep[0m  [36/42], [94mLoss[0m : 2.56821
[1mStep[0m  [40/42], [94mLoss[0m : 2.54322

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46910
[1mStep[0m  [4/42], [94mLoss[0m : 2.38687
[1mStep[0m  [8/42], [94mLoss[0m : 2.37608
[1mStep[0m  [12/42], [94mLoss[0m : 2.57395
[1mStep[0m  [16/42], [94mLoss[0m : 2.28777
[1mStep[0m  [20/42], [94mLoss[0m : 2.48557
[1mStep[0m  [24/42], [94mLoss[0m : 2.35339
[1mStep[0m  [28/42], [94mLoss[0m : 2.24277
[1mStep[0m  [32/42], [94mLoss[0m : 2.29808
[1mStep[0m  [36/42], [94mLoss[0m : 2.66137
[1mStep[0m  [40/42], [94mLoss[0m : 2.56572

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31714
[1mStep[0m  [4/42], [94mLoss[0m : 2.37106
[1mStep[0m  [8/42], [94mLoss[0m : 2.38336
[1mStep[0m  [12/42], [94mLoss[0m : 2.30573
[1mStep[0m  [16/42], [94mLoss[0m : 2.27008
[1mStep[0m  [20/42], [94mLoss[0m : 2.53326
[1mStep[0m  [24/42], [94mLoss[0m : 2.34769
[1mStep[0m  [28/42], [94mLoss[0m : 2.44454
[1mStep[0m  [32/42], [94mLoss[0m : 2.39900
[1mStep[0m  [36/42], [94mLoss[0m : 2.37965
[1mStep[0m  [40/42], [94mLoss[0m : 2.49682

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51510
[1mStep[0m  [4/42], [94mLoss[0m : 2.53588
[1mStep[0m  [8/42], [94mLoss[0m : 2.28301
[1mStep[0m  [12/42], [94mLoss[0m : 2.21527
[1mStep[0m  [16/42], [94mLoss[0m : 2.32054
[1mStep[0m  [20/42], [94mLoss[0m : 2.57016
[1mStep[0m  [24/42], [94mLoss[0m : 2.07721
[1mStep[0m  [28/42], [94mLoss[0m : 2.38525
[1mStep[0m  [32/42], [94mLoss[0m : 2.27633
[1mStep[0m  [36/42], [94mLoss[0m : 2.53011
[1mStep[0m  [40/42], [94mLoss[0m : 2.56955

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28100
[1mStep[0m  [4/42], [94mLoss[0m : 2.29880
[1mStep[0m  [8/42], [94mLoss[0m : 2.37692
[1mStep[0m  [12/42], [94mLoss[0m : 2.36646
[1mStep[0m  [16/42], [94mLoss[0m : 2.39634
[1mStep[0m  [20/42], [94mLoss[0m : 2.43804
[1mStep[0m  [24/42], [94mLoss[0m : 2.34348
[1mStep[0m  [28/42], [94mLoss[0m : 2.24437
[1mStep[0m  [32/42], [94mLoss[0m : 2.47306
[1mStep[0m  [36/42], [94mLoss[0m : 2.51365
[1mStep[0m  [40/42], [94mLoss[0m : 2.46472

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40990
[1mStep[0m  [4/42], [94mLoss[0m : 2.33156
[1mStep[0m  [8/42], [94mLoss[0m : 2.54641
[1mStep[0m  [12/42], [94mLoss[0m : 2.49410
[1mStep[0m  [16/42], [94mLoss[0m : 2.57161
[1mStep[0m  [20/42], [94mLoss[0m : 2.24088
[1mStep[0m  [24/42], [94mLoss[0m : 2.28317
[1mStep[0m  [28/42], [94mLoss[0m : 2.28296
[1mStep[0m  [32/42], [94mLoss[0m : 2.32092
[1mStep[0m  [36/42], [94mLoss[0m : 2.45792
[1mStep[0m  [40/42], [94mLoss[0m : 2.40499

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31992
[1mStep[0m  [4/42], [94mLoss[0m : 2.33474
[1mStep[0m  [8/42], [94mLoss[0m : 2.40099
[1mStep[0m  [12/42], [94mLoss[0m : 2.35262
[1mStep[0m  [16/42], [94mLoss[0m : 2.32357
[1mStep[0m  [20/42], [94mLoss[0m : 2.54891
[1mStep[0m  [24/42], [94mLoss[0m : 2.44314
[1mStep[0m  [28/42], [94mLoss[0m : 2.43323
[1mStep[0m  [32/42], [94mLoss[0m : 2.50453
[1mStep[0m  [36/42], [94mLoss[0m : 2.31963
[1mStep[0m  [40/42], [94mLoss[0m : 2.39061

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44693
[1mStep[0m  [4/42], [94mLoss[0m : 2.27543
[1mStep[0m  [8/42], [94mLoss[0m : 2.31503
[1mStep[0m  [12/42], [94mLoss[0m : 2.57992
[1mStep[0m  [16/42], [94mLoss[0m : 2.43793
[1mStep[0m  [20/42], [94mLoss[0m : 2.22124
[1mStep[0m  [24/42], [94mLoss[0m : 2.39289
[1mStep[0m  [28/42], [94mLoss[0m : 2.47312
[1mStep[0m  [32/42], [94mLoss[0m : 2.41197
[1mStep[0m  [36/42], [94mLoss[0m : 2.32083
[1mStep[0m  [40/42], [94mLoss[0m : 2.35229

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36488
[1mStep[0m  [4/42], [94mLoss[0m : 2.46387
[1mStep[0m  [8/42], [94mLoss[0m : 2.52505
[1mStep[0m  [12/42], [94mLoss[0m : 2.41004
[1mStep[0m  [16/42], [94mLoss[0m : 2.48705
[1mStep[0m  [20/42], [94mLoss[0m : 2.33350
[1mStep[0m  [24/42], [94mLoss[0m : 2.56154
[1mStep[0m  [28/42], [94mLoss[0m : 2.39879
[1mStep[0m  [32/42], [94mLoss[0m : 2.38078
[1mStep[0m  [36/42], [94mLoss[0m : 2.24216
[1mStep[0m  [40/42], [94mLoss[0m : 2.27986

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39919
[1mStep[0m  [4/42], [94mLoss[0m : 2.51479
[1mStep[0m  [8/42], [94mLoss[0m : 2.36821
[1mStep[0m  [12/42], [94mLoss[0m : 2.44912
[1mStep[0m  [16/42], [94mLoss[0m : 2.28978
[1mStep[0m  [20/42], [94mLoss[0m : 2.50716
[1mStep[0m  [24/42], [94mLoss[0m : 2.40681
[1mStep[0m  [28/42], [94mLoss[0m : 2.45530
[1mStep[0m  [32/42], [94mLoss[0m : 2.43905
[1mStep[0m  [36/42], [94mLoss[0m : 2.30004
[1mStep[0m  [40/42], [94mLoss[0m : 2.44162

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38676
[1mStep[0m  [4/42], [94mLoss[0m : 2.43766
[1mStep[0m  [8/42], [94mLoss[0m : 2.37400
[1mStep[0m  [12/42], [94mLoss[0m : 2.29965
[1mStep[0m  [16/42], [94mLoss[0m : 2.38530
[1mStep[0m  [20/42], [94mLoss[0m : 2.32893
[1mStep[0m  [24/42], [94mLoss[0m : 2.42722
[1mStep[0m  [28/42], [94mLoss[0m : 2.38610
[1mStep[0m  [32/42], [94mLoss[0m : 2.45594
[1mStep[0m  [36/42], [94mLoss[0m : 2.20255
[1mStep[0m  [40/42], [94mLoss[0m : 2.22644

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36281
[1mStep[0m  [4/42], [94mLoss[0m : 2.59570
[1mStep[0m  [8/42], [94mLoss[0m : 2.22942
[1mStep[0m  [12/42], [94mLoss[0m : 2.26657
[1mStep[0m  [16/42], [94mLoss[0m : 2.25816
[1mStep[0m  [20/42], [94mLoss[0m : 2.41654
[1mStep[0m  [24/42], [94mLoss[0m : 2.48399
[1mStep[0m  [28/42], [94mLoss[0m : 2.37323
[1mStep[0m  [32/42], [94mLoss[0m : 2.20276
[1mStep[0m  [36/42], [94mLoss[0m : 2.22954
[1mStep[0m  [40/42], [94mLoss[0m : 2.16554

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.314, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38779
[1mStep[0m  [4/42], [94mLoss[0m : 2.19435
[1mStep[0m  [8/42], [94mLoss[0m : 2.24543
[1mStep[0m  [12/42], [94mLoss[0m : 2.23254
[1mStep[0m  [16/42], [94mLoss[0m : 2.40593
[1mStep[0m  [20/42], [94mLoss[0m : 2.40515
[1mStep[0m  [24/42], [94mLoss[0m : 2.40324
[1mStep[0m  [28/42], [94mLoss[0m : 2.25670
[1mStep[0m  [32/42], [94mLoss[0m : 2.54697
[1mStep[0m  [36/42], [94mLoss[0m : 2.27795
[1mStep[0m  [40/42], [94mLoss[0m : 2.37733

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49967
[1mStep[0m  [4/42], [94mLoss[0m : 2.32350
[1mStep[0m  [8/42], [94mLoss[0m : 2.28905
[1mStep[0m  [12/42], [94mLoss[0m : 2.40684
[1mStep[0m  [16/42], [94mLoss[0m : 2.56444
[1mStep[0m  [20/42], [94mLoss[0m : 2.37002
[1mStep[0m  [24/42], [94mLoss[0m : 2.21344
[1mStep[0m  [28/42], [94mLoss[0m : 2.39935
[1mStep[0m  [32/42], [94mLoss[0m : 2.25588
[1mStep[0m  [36/42], [94mLoss[0m : 2.39863
[1mStep[0m  [40/42], [94mLoss[0m : 2.20058

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.315, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48456
[1mStep[0m  [4/42], [94mLoss[0m : 2.61323
[1mStep[0m  [8/42], [94mLoss[0m : 2.50245
[1mStep[0m  [12/42], [94mLoss[0m : 2.33946
[1mStep[0m  [16/42], [94mLoss[0m : 2.37107
[1mStep[0m  [20/42], [94mLoss[0m : 2.49758
[1mStep[0m  [24/42], [94mLoss[0m : 2.31346
[1mStep[0m  [28/42], [94mLoss[0m : 2.50299
[1mStep[0m  [32/42], [94mLoss[0m : 2.36746
[1mStep[0m  [36/42], [94mLoss[0m : 2.12184
[1mStep[0m  [40/42], [94mLoss[0m : 2.13862

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31189
[1mStep[0m  [4/42], [94mLoss[0m : 2.29578
[1mStep[0m  [8/42], [94mLoss[0m : 2.49812
[1mStep[0m  [12/42], [94mLoss[0m : 2.39277
[1mStep[0m  [16/42], [94mLoss[0m : 2.44050
[1mStep[0m  [20/42], [94mLoss[0m : 2.37967
[1mStep[0m  [24/42], [94mLoss[0m : 2.19455
[1mStep[0m  [28/42], [94mLoss[0m : 2.28148
[1mStep[0m  [32/42], [94mLoss[0m : 2.36207
[1mStep[0m  [36/42], [94mLoss[0m : 2.15750
[1mStep[0m  [40/42], [94mLoss[0m : 2.46305

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.338, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22915
[1mStep[0m  [4/42], [94mLoss[0m : 2.19002
[1mStep[0m  [8/42], [94mLoss[0m : 2.41283
[1mStep[0m  [12/42], [94mLoss[0m : 2.46687
[1mStep[0m  [16/42], [94mLoss[0m : 2.36360
[1mStep[0m  [20/42], [94mLoss[0m : 2.25493
[1mStep[0m  [24/42], [94mLoss[0m : 2.37027
[1mStep[0m  [28/42], [94mLoss[0m : 2.23220
[1mStep[0m  [32/42], [94mLoss[0m : 2.25497
[1mStep[0m  [36/42], [94mLoss[0m : 2.45943
[1mStep[0m  [40/42], [94mLoss[0m : 2.19830

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.334
====================================

Phase 1 - Evaluation MAE:  2.3335116931370328
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.38714
[1mStep[0m  [4/42], [94mLoss[0m : 2.63609
[1mStep[0m  [8/42], [94mLoss[0m : 2.29820
[1mStep[0m  [12/42], [94mLoss[0m : 2.50014
[1mStep[0m  [16/42], [94mLoss[0m : 2.40071
[1mStep[0m  [20/42], [94mLoss[0m : 2.67625
[1mStep[0m  [24/42], [94mLoss[0m : 2.65705
[1mStep[0m  [28/42], [94mLoss[0m : 2.64184
[1mStep[0m  [32/42], [94mLoss[0m : 2.61004
[1mStep[0m  [36/42], [94mLoss[0m : 2.58375
[1mStep[0m  [40/42], [94mLoss[0m : 2.67826

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35800
[1mStep[0m  [4/42], [94mLoss[0m : 2.47650
[1mStep[0m  [8/42], [94mLoss[0m : 2.45415
[1mStep[0m  [12/42], [94mLoss[0m : 2.14920
[1mStep[0m  [16/42], [94mLoss[0m : 2.36206
[1mStep[0m  [20/42], [94mLoss[0m : 2.45689
[1mStep[0m  [24/42], [94mLoss[0m : 2.45336
[1mStep[0m  [28/42], [94mLoss[0m : 2.50520
[1mStep[0m  [32/42], [94mLoss[0m : 2.21925
[1mStep[0m  [36/42], [94mLoss[0m : 2.34889
[1mStep[0m  [40/42], [94mLoss[0m : 2.33795

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37095
[1mStep[0m  [4/42], [94mLoss[0m : 2.21921
[1mStep[0m  [8/42], [94mLoss[0m : 2.34717
[1mStep[0m  [12/42], [94mLoss[0m : 2.48289
[1mStep[0m  [16/42], [94mLoss[0m : 2.49192
[1mStep[0m  [20/42], [94mLoss[0m : 2.30622
[1mStep[0m  [24/42], [94mLoss[0m : 2.18944
[1mStep[0m  [28/42], [94mLoss[0m : 2.18157
[1mStep[0m  [32/42], [94mLoss[0m : 2.28585
[1mStep[0m  [36/42], [94mLoss[0m : 2.35119
[1mStep[0m  [40/42], [94mLoss[0m : 2.15172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21961
[1mStep[0m  [4/42], [94mLoss[0m : 2.10772
[1mStep[0m  [8/42], [94mLoss[0m : 2.06233
[1mStep[0m  [12/42], [94mLoss[0m : 2.21003
[1mStep[0m  [16/42], [94mLoss[0m : 2.22767
[1mStep[0m  [20/42], [94mLoss[0m : 2.17834
[1mStep[0m  [24/42], [94mLoss[0m : 2.33492
[1mStep[0m  [28/42], [94mLoss[0m : 2.08988
[1mStep[0m  [32/42], [94mLoss[0m : 2.18236
[1mStep[0m  [36/42], [94mLoss[0m : 2.32636
[1mStep[0m  [40/42], [94mLoss[0m : 2.23549

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94882
[1mStep[0m  [4/42], [94mLoss[0m : 2.01145
[1mStep[0m  [8/42], [94mLoss[0m : 2.15511
[1mStep[0m  [12/42], [94mLoss[0m : 2.05101
[1mStep[0m  [16/42], [94mLoss[0m : 2.09564
[1mStep[0m  [20/42], [94mLoss[0m : 1.99541
[1mStep[0m  [24/42], [94mLoss[0m : 2.24901
[1mStep[0m  [28/42], [94mLoss[0m : 2.00303
[1mStep[0m  [32/42], [94mLoss[0m : 2.30186
[1mStep[0m  [36/42], [94mLoss[0m : 2.15455
[1mStep[0m  [40/42], [94mLoss[0m : 2.15051

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26103
[1mStep[0m  [4/42], [94mLoss[0m : 2.09590
[1mStep[0m  [8/42], [94mLoss[0m : 2.34966
[1mStep[0m  [12/42], [94mLoss[0m : 2.13182
[1mStep[0m  [16/42], [94mLoss[0m : 2.13063
[1mStep[0m  [20/42], [94mLoss[0m : 2.11406
[1mStep[0m  [24/42], [94mLoss[0m : 2.02478
[1mStep[0m  [28/42], [94mLoss[0m : 2.12640
[1mStep[0m  [32/42], [94mLoss[0m : 2.32041
[1mStep[0m  [36/42], [94mLoss[0m : 2.26476
[1mStep[0m  [40/42], [94mLoss[0m : 2.25841

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.113, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94562
[1mStep[0m  [4/42], [94mLoss[0m : 1.89055
[1mStep[0m  [8/42], [94mLoss[0m : 2.03002
[1mStep[0m  [12/42], [94mLoss[0m : 2.11915
[1mStep[0m  [16/42], [94mLoss[0m : 1.95832
[1mStep[0m  [20/42], [94mLoss[0m : 2.00740
[1mStep[0m  [24/42], [94mLoss[0m : 2.03172
[1mStep[0m  [28/42], [94mLoss[0m : 2.12125
[1mStep[0m  [32/42], [94mLoss[0m : 1.92901
[1mStep[0m  [36/42], [94mLoss[0m : 2.14907
[1mStep[0m  [40/42], [94mLoss[0m : 1.95316

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.96390
[1mStep[0m  [4/42], [94mLoss[0m : 2.07790
[1mStep[0m  [8/42], [94mLoss[0m : 2.11667
[1mStep[0m  [12/42], [94mLoss[0m : 1.90370
[1mStep[0m  [16/42], [94mLoss[0m : 1.90469
[1mStep[0m  [20/42], [94mLoss[0m : 2.04546
[1mStep[0m  [24/42], [94mLoss[0m : 2.06821
[1mStep[0m  [28/42], [94mLoss[0m : 1.83482
[1mStep[0m  [32/42], [94mLoss[0m : 1.91331
[1mStep[0m  [36/42], [94mLoss[0m : 2.04570
[1mStep[0m  [40/42], [94mLoss[0m : 1.90584

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98387
[1mStep[0m  [4/42], [94mLoss[0m : 1.72274
[1mStep[0m  [8/42], [94mLoss[0m : 2.06809
[1mStep[0m  [12/42], [94mLoss[0m : 1.97100
[1mStep[0m  [16/42], [94mLoss[0m : 1.92970
[1mStep[0m  [20/42], [94mLoss[0m : 1.83354
[1mStep[0m  [24/42], [94mLoss[0m : 1.88938
[1mStep[0m  [28/42], [94mLoss[0m : 1.78354
[1mStep[0m  [32/42], [94mLoss[0m : 2.07475
[1mStep[0m  [36/42], [94mLoss[0m : 2.00822
[1mStep[0m  [40/42], [94mLoss[0m : 2.05402

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.94382
[1mStep[0m  [4/42], [94mLoss[0m : 1.65593
[1mStep[0m  [8/42], [94mLoss[0m : 1.85628
[1mStep[0m  [12/42], [94mLoss[0m : 1.96522
[1mStep[0m  [16/42], [94mLoss[0m : 1.94447
[1mStep[0m  [20/42], [94mLoss[0m : 1.84014
[1mStep[0m  [24/42], [94mLoss[0m : 1.90191
[1mStep[0m  [28/42], [94mLoss[0m : 1.71230
[1mStep[0m  [32/42], [94mLoss[0m : 1.83877
[1mStep[0m  [36/42], [94mLoss[0m : 1.97310
[1mStep[0m  [40/42], [94mLoss[0m : 1.87622

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.01084
[1mStep[0m  [4/42], [94mLoss[0m : 1.72829
[1mStep[0m  [8/42], [94mLoss[0m : 1.87662
[1mStep[0m  [12/42], [94mLoss[0m : 1.74907
[1mStep[0m  [16/42], [94mLoss[0m : 1.77968
[1mStep[0m  [20/42], [94mLoss[0m : 1.75028
[1mStep[0m  [24/42], [94mLoss[0m : 1.68518
[1mStep[0m  [28/42], [94mLoss[0m : 1.74776
[1mStep[0m  [32/42], [94mLoss[0m : 1.71511
[1mStep[0m  [36/42], [94mLoss[0m : 1.90611
[1mStep[0m  [40/42], [94mLoss[0m : 1.82255

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.508, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87820
[1mStep[0m  [4/42], [94mLoss[0m : 1.82002
[1mStep[0m  [8/42], [94mLoss[0m : 1.77984
[1mStep[0m  [12/42], [94mLoss[0m : 1.68341
[1mStep[0m  [16/42], [94mLoss[0m : 1.86457
[1mStep[0m  [20/42], [94mLoss[0m : 1.81869
[1mStep[0m  [24/42], [94mLoss[0m : 1.89077
[1mStep[0m  [28/42], [94mLoss[0m : 1.81926
[1mStep[0m  [32/42], [94mLoss[0m : 1.86713
[1mStep[0m  [36/42], [94mLoss[0m : 1.72677
[1mStep[0m  [40/42], [94mLoss[0m : 1.84400

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.73390
[1mStep[0m  [4/42], [94mLoss[0m : 1.65103
[1mStep[0m  [8/42], [94mLoss[0m : 1.72860
[1mStep[0m  [12/42], [94mLoss[0m : 1.72858
[1mStep[0m  [16/42], [94mLoss[0m : 1.75774
[1mStep[0m  [20/42], [94mLoss[0m : 1.66930
[1mStep[0m  [24/42], [94mLoss[0m : 1.56064
[1mStep[0m  [28/42], [94mLoss[0m : 1.72114
[1mStep[0m  [32/42], [94mLoss[0m : 1.73142
[1mStep[0m  [36/42], [94mLoss[0m : 1.64096
[1mStep[0m  [40/42], [94mLoss[0m : 1.86327

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.517, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67386
[1mStep[0m  [4/42], [94mLoss[0m : 1.58625
[1mStep[0m  [8/42], [94mLoss[0m : 1.57482
[1mStep[0m  [12/42], [94mLoss[0m : 1.65055
[1mStep[0m  [16/42], [94mLoss[0m : 1.64974
[1mStep[0m  [20/42], [94mLoss[0m : 1.84110
[1mStep[0m  [24/42], [94mLoss[0m : 1.63004
[1mStep[0m  [28/42], [94mLoss[0m : 1.72140
[1mStep[0m  [32/42], [94mLoss[0m : 1.57545
[1mStep[0m  [36/42], [94mLoss[0m : 1.67012
[1mStep[0m  [40/42], [94mLoss[0m : 1.63844

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.62979
[1mStep[0m  [4/42], [94mLoss[0m : 1.66751
[1mStep[0m  [8/42], [94mLoss[0m : 1.75758
[1mStep[0m  [12/42], [94mLoss[0m : 1.57284
[1mStep[0m  [16/42], [94mLoss[0m : 1.51764
[1mStep[0m  [20/42], [94mLoss[0m : 1.70792
[1mStep[0m  [24/42], [94mLoss[0m : 1.81259
[1mStep[0m  [28/42], [94mLoss[0m : 1.68678
[1mStep[0m  [32/42], [94mLoss[0m : 1.63670
[1mStep[0m  [36/42], [94mLoss[0m : 1.73806
[1mStep[0m  [40/42], [94mLoss[0m : 1.63030

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50637
[1mStep[0m  [4/42], [94mLoss[0m : 1.65923
[1mStep[0m  [8/42], [94mLoss[0m : 1.78598
[1mStep[0m  [12/42], [94mLoss[0m : 1.54062
[1mStep[0m  [16/42], [94mLoss[0m : 1.54943
[1mStep[0m  [20/42], [94mLoss[0m : 1.63024
[1mStep[0m  [24/42], [94mLoss[0m : 1.47839
[1mStep[0m  [28/42], [94mLoss[0m : 1.68975
[1mStep[0m  [32/42], [94mLoss[0m : 1.71589
[1mStep[0m  [36/42], [94mLoss[0m : 1.69466
[1mStep[0m  [40/42], [94mLoss[0m : 1.53822

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.59137
[1mStep[0m  [4/42], [94mLoss[0m : 1.51100
[1mStep[0m  [8/42], [94mLoss[0m : 1.65960
[1mStep[0m  [12/42], [94mLoss[0m : 1.64286
[1mStep[0m  [16/42], [94mLoss[0m : 1.54700
[1mStep[0m  [20/42], [94mLoss[0m : 1.41821
[1mStep[0m  [24/42], [94mLoss[0m : 1.52913
[1mStep[0m  [28/42], [94mLoss[0m : 1.40358
[1mStep[0m  [32/42], [94mLoss[0m : 1.43261
[1mStep[0m  [36/42], [94mLoss[0m : 1.66881
[1mStep[0m  [40/42], [94mLoss[0m : 1.59021

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.557, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41744
[1mStep[0m  [4/42], [94mLoss[0m : 1.68132
[1mStep[0m  [8/42], [94mLoss[0m : 1.31154
[1mStep[0m  [12/42], [94mLoss[0m : 1.54799
[1mStep[0m  [16/42], [94mLoss[0m : 1.40763
[1mStep[0m  [20/42], [94mLoss[0m : 1.49294
[1mStep[0m  [24/42], [94mLoss[0m : 1.64614
[1mStep[0m  [28/42], [94mLoss[0m : 1.55953
[1mStep[0m  [32/42], [94mLoss[0m : 1.61285
[1mStep[0m  [36/42], [94mLoss[0m : 1.67009
[1mStep[0m  [40/42], [94mLoss[0m : 1.52219

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56553
[1mStep[0m  [4/42], [94mLoss[0m : 1.52204
[1mStep[0m  [8/42], [94mLoss[0m : 1.52778
[1mStep[0m  [12/42], [94mLoss[0m : 1.51052
[1mStep[0m  [16/42], [94mLoss[0m : 1.46567
[1mStep[0m  [20/42], [94mLoss[0m : 1.57362
[1mStep[0m  [24/42], [94mLoss[0m : 1.51548
[1mStep[0m  [28/42], [94mLoss[0m : 1.46676
[1mStep[0m  [32/42], [94mLoss[0m : 1.49403
[1mStep[0m  [36/42], [94mLoss[0m : 1.65785
[1mStep[0m  [40/42], [94mLoss[0m : 1.43985

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.45312
[1mStep[0m  [4/42], [94mLoss[0m : 1.53499
[1mStep[0m  [8/42], [94mLoss[0m : 1.42744
[1mStep[0m  [12/42], [94mLoss[0m : 1.43868
[1mStep[0m  [16/42], [94mLoss[0m : 1.48248
[1mStep[0m  [20/42], [94mLoss[0m : 1.37886
[1mStep[0m  [24/42], [94mLoss[0m : 1.44298
[1mStep[0m  [28/42], [94mLoss[0m : 1.44620
[1mStep[0m  [32/42], [94mLoss[0m : 1.49538
[1mStep[0m  [36/42], [94mLoss[0m : 1.34564
[1mStep[0m  [40/42], [94mLoss[0m : 1.59995

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.490, [92mTest[0m: 2.623, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.37499
[1mStep[0m  [4/42], [94mLoss[0m : 1.41838
[1mStep[0m  [8/42], [94mLoss[0m : 1.39722
[1mStep[0m  [12/42], [94mLoss[0m : 1.50961
[1mStep[0m  [16/42], [94mLoss[0m : 1.52090
[1mStep[0m  [20/42], [94mLoss[0m : 1.53186
[1mStep[0m  [24/42], [94mLoss[0m : 1.45738
[1mStep[0m  [28/42], [94mLoss[0m : 1.43344
[1mStep[0m  [32/42], [94mLoss[0m : 1.39686
[1mStep[0m  [36/42], [94mLoss[0m : 1.44115
[1mStep[0m  [40/42], [94mLoss[0m : 1.55517

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43858
[1mStep[0m  [4/42], [94mLoss[0m : 1.42649
[1mStep[0m  [8/42], [94mLoss[0m : 1.54721
[1mStep[0m  [12/42], [94mLoss[0m : 1.58928
[1mStep[0m  [16/42], [94mLoss[0m : 1.43531
[1mStep[0m  [20/42], [94mLoss[0m : 1.48877
[1mStep[0m  [24/42], [94mLoss[0m : 1.33688
[1mStep[0m  [28/42], [94mLoss[0m : 1.40553
[1mStep[0m  [32/42], [94mLoss[0m : 1.45929
[1mStep[0m  [36/42], [94mLoss[0m : 1.32959
[1mStep[0m  [40/42], [94mLoss[0m : 1.38437

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.528, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.49032
[1mStep[0m  [4/42], [94mLoss[0m : 1.45641
[1mStep[0m  [8/42], [94mLoss[0m : 1.46294
[1mStep[0m  [12/42], [94mLoss[0m : 1.43788
[1mStep[0m  [16/42], [94mLoss[0m : 1.46512
[1mStep[0m  [20/42], [94mLoss[0m : 1.38612
[1mStep[0m  [24/42], [94mLoss[0m : 1.42381
[1mStep[0m  [28/42], [94mLoss[0m : 1.23566
[1mStep[0m  [32/42], [94mLoss[0m : 1.42383
[1mStep[0m  [36/42], [94mLoss[0m : 1.46730
[1mStep[0m  [40/42], [94mLoss[0m : 1.51813

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.413, [92mTest[0m: 2.575, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.28898
[1mStep[0m  [4/42], [94mLoss[0m : 1.40730
[1mStep[0m  [8/42], [94mLoss[0m : 1.34282
[1mStep[0m  [12/42], [94mLoss[0m : 1.32879
[1mStep[0m  [16/42], [94mLoss[0m : 1.29027
[1mStep[0m  [20/42], [94mLoss[0m : 1.42333
[1mStep[0m  [24/42], [94mLoss[0m : 1.48232
[1mStep[0m  [28/42], [94mLoss[0m : 1.33905
[1mStep[0m  [32/42], [94mLoss[0m : 1.26738
[1mStep[0m  [36/42], [94mLoss[0m : 1.24436
[1mStep[0m  [40/42], [94mLoss[0m : 1.56500

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.368, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.449
====================================

Phase 2 - Evaluation MAE:  2.4488438878740584
MAE score P1        2.333512
MAE score P2        2.448844
loss                1.368352
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 21, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.34215
[1mStep[0m  [8/84], [94mLoss[0m : 10.28097
[1mStep[0m  [16/84], [94mLoss[0m : 9.72686
[1mStep[0m  [24/84], [94mLoss[0m : 9.95360
[1mStep[0m  [32/84], [94mLoss[0m : 8.66809
[1mStep[0m  [40/84], [94mLoss[0m : 7.60584
[1mStep[0m  [48/84], [94mLoss[0m : 7.53105
[1mStep[0m  [56/84], [94mLoss[0m : 7.11424
[1mStep[0m  [64/84], [94mLoss[0m : 5.86563
[1mStep[0m  [72/84], [94mLoss[0m : 5.45250
[1mStep[0m  [80/84], [94mLoss[0m : 5.28330

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.988, [92mTest[0m: 11.137, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.85663
[1mStep[0m  [8/84], [94mLoss[0m : 4.35874
[1mStep[0m  [16/84], [94mLoss[0m : 4.26974
[1mStep[0m  [24/84], [94mLoss[0m : 3.19686
[1mStep[0m  [32/84], [94mLoss[0m : 3.24328
[1mStep[0m  [40/84], [94mLoss[0m : 3.93643
[1mStep[0m  [48/84], [94mLoss[0m : 2.98178
[1mStep[0m  [56/84], [94mLoss[0m : 2.68829
[1mStep[0m  [64/84], [94mLoss[0m : 3.00223
[1mStep[0m  [72/84], [94mLoss[0m : 3.03959
[1mStep[0m  [80/84], [94mLoss[0m : 2.84841

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.547, [92mTest[0m: 4.882, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76253
[1mStep[0m  [8/84], [94mLoss[0m : 2.67940
[1mStep[0m  [16/84], [94mLoss[0m : 2.61254
[1mStep[0m  [24/84], [94mLoss[0m : 3.02466
[1mStep[0m  [32/84], [94mLoss[0m : 2.66454
[1mStep[0m  [40/84], [94mLoss[0m : 2.66531
[1mStep[0m  [48/84], [94mLoss[0m : 2.35733
[1mStep[0m  [56/84], [94mLoss[0m : 2.64480
[1mStep[0m  [64/84], [94mLoss[0m : 3.26376
[1mStep[0m  [72/84], [94mLoss[0m : 2.69951
[1mStep[0m  [80/84], [94mLoss[0m : 2.79828

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.656, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58640
[1mStep[0m  [8/84], [94mLoss[0m : 2.47396
[1mStep[0m  [16/84], [94mLoss[0m : 2.69693
[1mStep[0m  [24/84], [94mLoss[0m : 3.01316
[1mStep[0m  [32/84], [94mLoss[0m : 2.72100
[1mStep[0m  [40/84], [94mLoss[0m : 2.72093
[1mStep[0m  [48/84], [94mLoss[0m : 2.26269
[1mStep[0m  [56/84], [94mLoss[0m : 2.40941
[1mStep[0m  [64/84], [94mLoss[0m : 2.42809
[1mStep[0m  [72/84], [94mLoss[0m : 2.75901
[1mStep[0m  [80/84], [94mLoss[0m : 2.80775

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73528
[1mStep[0m  [8/84], [94mLoss[0m : 2.44777
[1mStep[0m  [16/84], [94mLoss[0m : 2.53299
[1mStep[0m  [24/84], [94mLoss[0m : 2.70226
[1mStep[0m  [32/84], [94mLoss[0m : 2.78263
[1mStep[0m  [40/84], [94mLoss[0m : 2.38223
[1mStep[0m  [48/84], [94mLoss[0m : 2.56478
[1mStep[0m  [56/84], [94mLoss[0m : 2.48466
[1mStep[0m  [64/84], [94mLoss[0m : 2.64278
[1mStep[0m  [72/84], [94mLoss[0m : 2.56234
[1mStep[0m  [80/84], [94mLoss[0m : 2.46668

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74708
[1mStep[0m  [8/84], [94mLoss[0m : 2.22425
[1mStep[0m  [16/84], [94mLoss[0m : 2.52197
[1mStep[0m  [24/84], [94mLoss[0m : 2.56450
[1mStep[0m  [32/84], [94mLoss[0m : 2.69416
[1mStep[0m  [40/84], [94mLoss[0m : 2.82895
[1mStep[0m  [48/84], [94mLoss[0m : 2.59381
[1mStep[0m  [56/84], [94mLoss[0m : 2.54187
[1mStep[0m  [64/84], [94mLoss[0m : 2.90129
[1mStep[0m  [72/84], [94mLoss[0m : 2.57275
[1mStep[0m  [80/84], [94mLoss[0m : 2.73972

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39113
[1mStep[0m  [8/84], [94mLoss[0m : 2.70122
[1mStep[0m  [16/84], [94mLoss[0m : 2.59627
[1mStep[0m  [24/84], [94mLoss[0m : 2.86714
[1mStep[0m  [32/84], [94mLoss[0m : 2.27759
[1mStep[0m  [40/84], [94mLoss[0m : 2.66705
[1mStep[0m  [48/84], [94mLoss[0m : 2.72695
[1mStep[0m  [56/84], [94mLoss[0m : 2.72552
[1mStep[0m  [64/84], [94mLoss[0m : 2.63731
[1mStep[0m  [72/84], [94mLoss[0m : 2.30897
[1mStep[0m  [80/84], [94mLoss[0m : 2.67569

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96072
[1mStep[0m  [8/84], [94mLoss[0m : 2.81927
[1mStep[0m  [16/84], [94mLoss[0m : 2.34060
[1mStep[0m  [24/84], [94mLoss[0m : 2.29761
[1mStep[0m  [32/84], [94mLoss[0m : 2.77697
[1mStep[0m  [40/84], [94mLoss[0m : 2.36339
[1mStep[0m  [48/84], [94mLoss[0m : 2.59886
[1mStep[0m  [56/84], [94mLoss[0m : 2.56185
[1mStep[0m  [64/84], [94mLoss[0m : 2.77209
[1mStep[0m  [72/84], [94mLoss[0m : 2.58865
[1mStep[0m  [80/84], [94mLoss[0m : 2.46771

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80318
[1mStep[0m  [8/84], [94mLoss[0m : 2.74540
[1mStep[0m  [16/84], [94mLoss[0m : 2.46744
[1mStep[0m  [24/84], [94mLoss[0m : 2.41163
[1mStep[0m  [32/84], [94mLoss[0m : 2.60601
[1mStep[0m  [40/84], [94mLoss[0m : 2.55745
[1mStep[0m  [48/84], [94mLoss[0m : 2.48161
[1mStep[0m  [56/84], [94mLoss[0m : 2.45859
[1mStep[0m  [64/84], [94mLoss[0m : 2.87228
[1mStep[0m  [72/84], [94mLoss[0m : 2.76698
[1mStep[0m  [80/84], [94mLoss[0m : 3.02065

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47880
[1mStep[0m  [8/84], [94mLoss[0m : 2.63784
[1mStep[0m  [16/84], [94mLoss[0m : 2.50350
[1mStep[0m  [24/84], [94mLoss[0m : 2.50284
[1mStep[0m  [32/84], [94mLoss[0m : 2.47574
[1mStep[0m  [40/84], [94mLoss[0m : 2.38801
[1mStep[0m  [48/84], [94mLoss[0m : 2.26060
[1mStep[0m  [56/84], [94mLoss[0m : 2.61547
[1mStep[0m  [64/84], [94mLoss[0m : 2.52185
[1mStep[0m  [72/84], [94mLoss[0m : 2.51835
[1mStep[0m  [80/84], [94mLoss[0m : 2.52675

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65200
[1mStep[0m  [8/84], [94mLoss[0m : 2.39262
[1mStep[0m  [16/84], [94mLoss[0m : 2.74297
[1mStep[0m  [24/84], [94mLoss[0m : 2.61090
[1mStep[0m  [32/84], [94mLoss[0m : 2.35940
[1mStep[0m  [40/84], [94mLoss[0m : 2.65953
[1mStep[0m  [48/84], [94mLoss[0m : 2.48976
[1mStep[0m  [56/84], [94mLoss[0m : 2.57443
[1mStep[0m  [64/84], [94mLoss[0m : 2.75531
[1mStep[0m  [72/84], [94mLoss[0m : 2.51568
[1mStep[0m  [80/84], [94mLoss[0m : 2.45227

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29058
[1mStep[0m  [8/84], [94mLoss[0m : 2.57367
[1mStep[0m  [16/84], [94mLoss[0m : 1.99414
[1mStep[0m  [24/84], [94mLoss[0m : 2.77284
[1mStep[0m  [32/84], [94mLoss[0m : 2.42939
[1mStep[0m  [40/84], [94mLoss[0m : 2.20889
[1mStep[0m  [48/84], [94mLoss[0m : 2.53901
[1mStep[0m  [56/84], [94mLoss[0m : 2.52239
[1mStep[0m  [64/84], [94mLoss[0m : 2.56749
[1mStep[0m  [72/84], [94mLoss[0m : 2.83334
[1mStep[0m  [80/84], [94mLoss[0m : 2.67519

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.94366
[1mStep[0m  [8/84], [94mLoss[0m : 2.76059
[1mStep[0m  [16/84], [94mLoss[0m : 2.63095
[1mStep[0m  [24/84], [94mLoss[0m : 2.50234
[1mStep[0m  [32/84], [94mLoss[0m : 2.60655
[1mStep[0m  [40/84], [94mLoss[0m : 2.31221
[1mStep[0m  [48/84], [94mLoss[0m : 2.70196
[1mStep[0m  [56/84], [94mLoss[0m : 2.69944
[1mStep[0m  [64/84], [94mLoss[0m : 2.70204
[1mStep[0m  [72/84], [94mLoss[0m : 2.56579
[1mStep[0m  [80/84], [94mLoss[0m : 2.28757

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67129
[1mStep[0m  [8/84], [94mLoss[0m : 2.18655
[1mStep[0m  [16/84], [94mLoss[0m : 2.55926
[1mStep[0m  [24/84], [94mLoss[0m : 2.67067
[1mStep[0m  [32/84], [94mLoss[0m : 2.56803
[1mStep[0m  [40/84], [94mLoss[0m : 2.46272
[1mStep[0m  [48/84], [94mLoss[0m : 2.61593
[1mStep[0m  [56/84], [94mLoss[0m : 2.40173
[1mStep[0m  [64/84], [94mLoss[0m : 2.63536
[1mStep[0m  [72/84], [94mLoss[0m : 2.50392
[1mStep[0m  [80/84], [94mLoss[0m : 2.59495

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50565
[1mStep[0m  [8/84], [94mLoss[0m : 2.72288
[1mStep[0m  [16/84], [94mLoss[0m : 2.45318
[1mStep[0m  [24/84], [94mLoss[0m : 2.29760
[1mStep[0m  [32/84], [94mLoss[0m : 2.43699
[1mStep[0m  [40/84], [94mLoss[0m : 2.57889
[1mStep[0m  [48/84], [94mLoss[0m : 2.41122
[1mStep[0m  [56/84], [94mLoss[0m : 2.32310
[1mStep[0m  [64/84], [94mLoss[0m : 2.43355
[1mStep[0m  [72/84], [94mLoss[0m : 2.76713
[1mStep[0m  [80/84], [94mLoss[0m : 2.71195

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56279
[1mStep[0m  [8/84], [94mLoss[0m : 2.57104
[1mStep[0m  [16/84], [94mLoss[0m : 2.59037
[1mStep[0m  [24/84], [94mLoss[0m : 2.63935
[1mStep[0m  [32/84], [94mLoss[0m : 2.65159
[1mStep[0m  [40/84], [94mLoss[0m : 2.52320
[1mStep[0m  [48/84], [94mLoss[0m : 2.48677
[1mStep[0m  [56/84], [94mLoss[0m : 2.53679
[1mStep[0m  [64/84], [94mLoss[0m : 2.53472
[1mStep[0m  [72/84], [94mLoss[0m : 2.50982
[1mStep[0m  [80/84], [94mLoss[0m : 2.48614

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69517
[1mStep[0m  [8/84], [94mLoss[0m : 2.58146
[1mStep[0m  [16/84], [94mLoss[0m : 2.40244
[1mStep[0m  [24/84], [94mLoss[0m : 2.52451
[1mStep[0m  [32/84], [94mLoss[0m : 2.41704
[1mStep[0m  [40/84], [94mLoss[0m : 2.56638
[1mStep[0m  [48/84], [94mLoss[0m : 2.68026
[1mStep[0m  [56/84], [94mLoss[0m : 2.74112
[1mStep[0m  [64/84], [94mLoss[0m : 2.63890
[1mStep[0m  [72/84], [94mLoss[0m : 2.31767
[1mStep[0m  [80/84], [94mLoss[0m : 2.51030

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61693
[1mStep[0m  [8/84], [94mLoss[0m : 2.63226
[1mStep[0m  [16/84], [94mLoss[0m : 2.24724
[1mStep[0m  [24/84], [94mLoss[0m : 2.52294
[1mStep[0m  [32/84], [94mLoss[0m : 2.74743
[1mStep[0m  [40/84], [94mLoss[0m : 2.42066
[1mStep[0m  [48/84], [94mLoss[0m : 2.78565
[1mStep[0m  [56/84], [94mLoss[0m : 2.52022
[1mStep[0m  [64/84], [94mLoss[0m : 2.60638
[1mStep[0m  [72/84], [94mLoss[0m : 2.70905
[1mStep[0m  [80/84], [94mLoss[0m : 2.42778

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62150
[1mStep[0m  [8/84], [94mLoss[0m : 2.69975
[1mStep[0m  [16/84], [94mLoss[0m : 2.71574
[1mStep[0m  [24/84], [94mLoss[0m : 2.41182
[1mStep[0m  [32/84], [94mLoss[0m : 2.35930
[1mStep[0m  [40/84], [94mLoss[0m : 2.84132
[1mStep[0m  [48/84], [94mLoss[0m : 2.55107
[1mStep[0m  [56/84], [94mLoss[0m : 2.39135
[1mStep[0m  [64/84], [94mLoss[0m : 2.63406
[1mStep[0m  [72/84], [94mLoss[0m : 2.40522
[1mStep[0m  [80/84], [94mLoss[0m : 2.65731

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25832
[1mStep[0m  [8/84], [94mLoss[0m : 2.16738
[1mStep[0m  [16/84], [94mLoss[0m : 2.38768
[1mStep[0m  [24/84], [94mLoss[0m : 2.63504
[1mStep[0m  [32/84], [94mLoss[0m : 2.88880
[1mStep[0m  [40/84], [94mLoss[0m : 2.55077
[1mStep[0m  [48/84], [94mLoss[0m : 2.53378
[1mStep[0m  [56/84], [94mLoss[0m : 2.50743
[1mStep[0m  [64/84], [94mLoss[0m : 2.29017
[1mStep[0m  [72/84], [94mLoss[0m : 2.32892
[1mStep[0m  [80/84], [94mLoss[0m : 2.31626

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46779
[1mStep[0m  [8/84], [94mLoss[0m : 2.35780
[1mStep[0m  [16/84], [94mLoss[0m : 2.54827
[1mStep[0m  [24/84], [94mLoss[0m : 2.62905
[1mStep[0m  [32/84], [94mLoss[0m : 2.44173
[1mStep[0m  [40/84], [94mLoss[0m : 2.53640
[1mStep[0m  [48/84], [94mLoss[0m : 2.69293
[1mStep[0m  [56/84], [94mLoss[0m : 2.26197
[1mStep[0m  [64/84], [94mLoss[0m : 2.73091
[1mStep[0m  [72/84], [94mLoss[0m : 2.47854
[1mStep[0m  [80/84], [94mLoss[0m : 2.47274

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30642
[1mStep[0m  [8/84], [94mLoss[0m : 2.66955
[1mStep[0m  [16/84], [94mLoss[0m : 2.94623
[1mStep[0m  [24/84], [94mLoss[0m : 2.51258
[1mStep[0m  [32/84], [94mLoss[0m : 2.54879
[1mStep[0m  [40/84], [94mLoss[0m : 2.36967
[1mStep[0m  [48/84], [94mLoss[0m : 2.25279
[1mStep[0m  [56/84], [94mLoss[0m : 2.69798
[1mStep[0m  [64/84], [94mLoss[0m : 2.63171
[1mStep[0m  [72/84], [94mLoss[0m : 2.99306
[1mStep[0m  [80/84], [94mLoss[0m : 2.33932

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69454
[1mStep[0m  [8/84], [94mLoss[0m : 2.31881
[1mStep[0m  [16/84], [94mLoss[0m : 2.47227
[1mStep[0m  [24/84], [94mLoss[0m : 2.44478
[1mStep[0m  [32/84], [94mLoss[0m : 2.77553
[1mStep[0m  [40/84], [94mLoss[0m : 2.80844
[1mStep[0m  [48/84], [94mLoss[0m : 2.24381
[1mStep[0m  [56/84], [94mLoss[0m : 2.53440
[1mStep[0m  [64/84], [94mLoss[0m : 2.24957
[1mStep[0m  [72/84], [94mLoss[0m : 2.34130
[1mStep[0m  [80/84], [94mLoss[0m : 2.56641

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73031
[1mStep[0m  [8/84], [94mLoss[0m : 2.52198
[1mStep[0m  [16/84], [94mLoss[0m : 2.34387
[1mStep[0m  [24/84], [94mLoss[0m : 2.57222
[1mStep[0m  [32/84], [94mLoss[0m : 2.41739
[1mStep[0m  [40/84], [94mLoss[0m : 2.17901
[1mStep[0m  [48/84], [94mLoss[0m : 2.52112
[1mStep[0m  [56/84], [94mLoss[0m : 2.44317
[1mStep[0m  [64/84], [94mLoss[0m : 2.50534
[1mStep[0m  [72/84], [94mLoss[0m : 2.57143
[1mStep[0m  [80/84], [94mLoss[0m : 2.51515

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49872
[1mStep[0m  [8/84], [94mLoss[0m : 2.35316
[1mStep[0m  [16/84], [94mLoss[0m : 2.77708
[1mStep[0m  [24/84], [94mLoss[0m : 2.84020
[1mStep[0m  [32/84], [94mLoss[0m : 2.54582
[1mStep[0m  [40/84], [94mLoss[0m : 2.26301
[1mStep[0m  [48/84], [94mLoss[0m : 2.52999
[1mStep[0m  [56/84], [94mLoss[0m : 2.54575
[1mStep[0m  [64/84], [94mLoss[0m : 2.51178
[1mStep[0m  [72/84], [94mLoss[0m : 2.89816
[1mStep[0m  [80/84], [94mLoss[0m : 2.89107

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37150
[1mStep[0m  [8/84], [94mLoss[0m : 2.64703
[1mStep[0m  [16/84], [94mLoss[0m : 2.44779
[1mStep[0m  [24/84], [94mLoss[0m : 2.62025
[1mStep[0m  [32/84], [94mLoss[0m : 2.62550
[1mStep[0m  [40/84], [94mLoss[0m : 2.63346
[1mStep[0m  [48/84], [94mLoss[0m : 2.58549
[1mStep[0m  [56/84], [94mLoss[0m : 2.57428
[1mStep[0m  [64/84], [94mLoss[0m : 2.38828
[1mStep[0m  [72/84], [94mLoss[0m : 2.47723
[1mStep[0m  [80/84], [94mLoss[0m : 2.39868

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77814
[1mStep[0m  [8/84], [94mLoss[0m : 2.61736
[1mStep[0m  [16/84], [94mLoss[0m : 2.59125
[1mStep[0m  [24/84], [94mLoss[0m : 2.59385
[1mStep[0m  [32/84], [94mLoss[0m : 2.78221
[1mStep[0m  [40/84], [94mLoss[0m : 2.18830
[1mStep[0m  [48/84], [94mLoss[0m : 2.47565
[1mStep[0m  [56/84], [94mLoss[0m : 2.61583
[1mStep[0m  [64/84], [94mLoss[0m : 2.45590
[1mStep[0m  [72/84], [94mLoss[0m : 2.48072
[1mStep[0m  [80/84], [94mLoss[0m : 2.45845

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40128
[1mStep[0m  [8/84], [94mLoss[0m : 2.53969
[1mStep[0m  [16/84], [94mLoss[0m : 2.55210
[1mStep[0m  [24/84], [94mLoss[0m : 2.44469
[1mStep[0m  [32/84], [94mLoss[0m : 2.88912
[1mStep[0m  [40/84], [94mLoss[0m : 2.54435
[1mStep[0m  [48/84], [94mLoss[0m : 2.93073
[1mStep[0m  [56/84], [94mLoss[0m : 2.43791
[1mStep[0m  [64/84], [94mLoss[0m : 2.49708
[1mStep[0m  [72/84], [94mLoss[0m : 2.43825
[1mStep[0m  [80/84], [94mLoss[0m : 2.30121

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64114
[1mStep[0m  [8/84], [94mLoss[0m : 2.58544
[1mStep[0m  [16/84], [94mLoss[0m : 2.56621
[1mStep[0m  [24/84], [94mLoss[0m : 2.33383
[1mStep[0m  [32/84], [94mLoss[0m : 2.61847
[1mStep[0m  [40/84], [94mLoss[0m : 2.57713
[1mStep[0m  [48/84], [94mLoss[0m : 2.64847
[1mStep[0m  [56/84], [94mLoss[0m : 2.72412
[1mStep[0m  [64/84], [94mLoss[0m : 2.48929
[1mStep[0m  [72/84], [94mLoss[0m : 2.60125
[1mStep[0m  [80/84], [94mLoss[0m : 2.71783

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41801
[1mStep[0m  [8/84], [94mLoss[0m : 2.74104
[1mStep[0m  [16/84], [94mLoss[0m : 2.41714
[1mStep[0m  [24/84], [94mLoss[0m : 2.66647
[1mStep[0m  [32/84], [94mLoss[0m : 2.46440
[1mStep[0m  [40/84], [94mLoss[0m : 2.29069
[1mStep[0m  [48/84], [94mLoss[0m : 2.42329
[1mStep[0m  [56/84], [94mLoss[0m : 2.38050
[1mStep[0m  [64/84], [94mLoss[0m : 2.63180
[1mStep[0m  [72/84], [94mLoss[0m : 2.64007
[1mStep[0m  [80/84], [94mLoss[0m : 2.23811

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.337
====================================

Phase 1 - Evaluation MAE:  2.3365153755460466
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.58459
[1mStep[0m  [8/84], [94mLoss[0m : 2.86425
[1mStep[0m  [16/84], [94mLoss[0m : 2.82457
[1mStep[0m  [24/84], [94mLoss[0m : 2.66353
[1mStep[0m  [32/84], [94mLoss[0m : 2.32218
[1mStep[0m  [40/84], [94mLoss[0m : 2.64695
[1mStep[0m  [48/84], [94mLoss[0m : 2.53518
[1mStep[0m  [56/84], [94mLoss[0m : 2.64218
[1mStep[0m  [64/84], [94mLoss[0m : 2.40550
[1mStep[0m  [72/84], [94mLoss[0m : 2.59496
[1mStep[0m  [80/84], [94mLoss[0m : 2.39208

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88441
[1mStep[0m  [8/84], [94mLoss[0m : 2.38462
[1mStep[0m  [16/84], [94mLoss[0m : 2.58091
[1mStep[0m  [24/84], [94mLoss[0m : 2.39673
[1mStep[0m  [32/84], [94mLoss[0m : 2.54216
[1mStep[0m  [40/84], [94mLoss[0m : 3.03454
[1mStep[0m  [48/84], [94mLoss[0m : 2.35226
[1mStep[0m  [56/84], [94mLoss[0m : 2.38843
[1mStep[0m  [64/84], [94mLoss[0m : 2.44819
[1mStep[0m  [72/84], [94mLoss[0m : 2.58182
[1mStep[0m  [80/84], [94mLoss[0m : 2.58994

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68197
[1mStep[0m  [8/84], [94mLoss[0m : 2.66749
[1mStep[0m  [16/84], [94mLoss[0m : 2.71818
[1mStep[0m  [24/84], [94mLoss[0m : 2.25919
[1mStep[0m  [32/84], [94mLoss[0m : 2.16328
[1mStep[0m  [40/84], [94mLoss[0m : 2.48832
[1mStep[0m  [48/84], [94mLoss[0m : 2.33994
[1mStep[0m  [56/84], [94mLoss[0m : 2.46906
[1mStep[0m  [64/84], [94mLoss[0m : 2.54237
[1mStep[0m  [72/84], [94mLoss[0m : 2.16657
[1mStep[0m  [80/84], [94mLoss[0m : 2.65000

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33889
[1mStep[0m  [8/84], [94mLoss[0m : 2.53113
[1mStep[0m  [16/84], [94mLoss[0m : 2.88166
[1mStep[0m  [24/84], [94mLoss[0m : 2.53480
[1mStep[0m  [32/84], [94mLoss[0m : 3.19247
[1mStep[0m  [40/84], [94mLoss[0m : 2.53908
[1mStep[0m  [48/84], [94mLoss[0m : 2.45141
[1mStep[0m  [56/84], [94mLoss[0m : 2.62274
[1mStep[0m  [64/84], [94mLoss[0m : 2.33644
[1mStep[0m  [72/84], [94mLoss[0m : 2.27809
[1mStep[0m  [80/84], [94mLoss[0m : 2.27755

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41891
[1mStep[0m  [8/84], [94mLoss[0m : 2.39120
[1mStep[0m  [16/84], [94mLoss[0m : 2.49235
[1mStep[0m  [24/84], [94mLoss[0m : 2.30279
[1mStep[0m  [32/84], [94mLoss[0m : 2.26967
[1mStep[0m  [40/84], [94mLoss[0m : 2.39654
[1mStep[0m  [48/84], [94mLoss[0m : 2.71462
[1mStep[0m  [56/84], [94mLoss[0m : 2.61493
[1mStep[0m  [64/84], [94mLoss[0m : 2.42340
[1mStep[0m  [72/84], [94mLoss[0m : 2.41312
[1mStep[0m  [80/84], [94mLoss[0m : 2.75650

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37497
[1mStep[0m  [8/84], [94mLoss[0m : 2.61705
[1mStep[0m  [16/84], [94mLoss[0m : 2.35130
[1mStep[0m  [24/84], [94mLoss[0m : 2.65970
[1mStep[0m  [32/84], [94mLoss[0m : 2.15824
[1mStep[0m  [40/84], [94mLoss[0m : 2.28430
[1mStep[0m  [48/84], [94mLoss[0m : 2.77327
[1mStep[0m  [56/84], [94mLoss[0m : 2.56504
[1mStep[0m  [64/84], [94mLoss[0m : 2.48301
[1mStep[0m  [72/84], [94mLoss[0m : 2.77767
[1mStep[0m  [80/84], [94mLoss[0m : 2.48623

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79934
[1mStep[0m  [8/84], [94mLoss[0m : 2.21599
[1mStep[0m  [16/84], [94mLoss[0m : 2.37568
[1mStep[0m  [24/84], [94mLoss[0m : 2.29932
[1mStep[0m  [32/84], [94mLoss[0m : 2.24887
[1mStep[0m  [40/84], [94mLoss[0m : 2.25231
[1mStep[0m  [48/84], [94mLoss[0m : 2.53467
[1mStep[0m  [56/84], [94mLoss[0m : 2.18081
[1mStep[0m  [64/84], [94mLoss[0m : 2.50008
[1mStep[0m  [72/84], [94mLoss[0m : 2.53639
[1mStep[0m  [80/84], [94mLoss[0m : 2.49311

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26022
[1mStep[0m  [8/84], [94mLoss[0m : 2.22366
[1mStep[0m  [16/84], [94mLoss[0m : 2.29286
[1mStep[0m  [24/84], [94mLoss[0m : 2.51050
[1mStep[0m  [32/84], [94mLoss[0m : 2.30339
[1mStep[0m  [40/84], [94mLoss[0m : 2.82826
[1mStep[0m  [48/84], [94mLoss[0m : 2.62346
[1mStep[0m  [56/84], [94mLoss[0m : 1.93187
[1mStep[0m  [64/84], [94mLoss[0m : 2.38779
[1mStep[0m  [72/84], [94mLoss[0m : 2.11691
[1mStep[0m  [80/84], [94mLoss[0m : 2.77164

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11795
[1mStep[0m  [8/84], [94mLoss[0m : 2.25662
[1mStep[0m  [16/84], [94mLoss[0m : 2.48196
[1mStep[0m  [24/84], [94mLoss[0m : 2.44165
[1mStep[0m  [32/84], [94mLoss[0m : 2.37074
[1mStep[0m  [40/84], [94mLoss[0m : 2.14342
[1mStep[0m  [48/84], [94mLoss[0m : 2.35883
[1mStep[0m  [56/84], [94mLoss[0m : 2.40201
[1mStep[0m  [64/84], [94mLoss[0m : 2.68227
[1mStep[0m  [72/84], [94mLoss[0m : 2.37074
[1mStep[0m  [80/84], [94mLoss[0m : 2.72797

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.560, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29599
[1mStep[0m  [8/84], [94mLoss[0m : 2.38504
[1mStep[0m  [16/84], [94mLoss[0m : 2.23036
[1mStep[0m  [24/84], [94mLoss[0m : 2.33855
[1mStep[0m  [32/84], [94mLoss[0m : 2.41288
[1mStep[0m  [40/84], [94mLoss[0m : 2.20399
[1mStep[0m  [48/84], [94mLoss[0m : 2.20495
[1mStep[0m  [56/84], [94mLoss[0m : 2.40779
[1mStep[0m  [64/84], [94mLoss[0m : 2.17506
[1mStep[0m  [72/84], [94mLoss[0m : 2.13940
[1mStep[0m  [80/84], [94mLoss[0m : 2.29651

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.570, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22313
[1mStep[0m  [8/84], [94mLoss[0m : 2.13742
[1mStep[0m  [16/84], [94mLoss[0m : 2.20310
[1mStep[0m  [24/84], [94mLoss[0m : 2.44609
[1mStep[0m  [32/84], [94mLoss[0m : 2.27483
[1mStep[0m  [40/84], [94mLoss[0m : 2.39370
[1mStep[0m  [48/84], [94mLoss[0m : 2.20000
[1mStep[0m  [56/84], [94mLoss[0m : 2.29167
[1mStep[0m  [64/84], [94mLoss[0m : 2.79463
[1mStep[0m  [72/84], [94mLoss[0m : 2.25158
[1mStep[0m  [80/84], [94mLoss[0m : 2.23889

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03090
[1mStep[0m  [8/84], [94mLoss[0m : 2.27182
[1mStep[0m  [16/84], [94mLoss[0m : 2.45010
[1mStep[0m  [24/84], [94mLoss[0m : 2.07729
[1mStep[0m  [32/84], [94mLoss[0m : 2.34282
[1mStep[0m  [40/84], [94mLoss[0m : 2.10726
[1mStep[0m  [48/84], [94mLoss[0m : 2.19964
[1mStep[0m  [56/84], [94mLoss[0m : 2.15035
[1mStep[0m  [64/84], [94mLoss[0m : 1.91733
[1mStep[0m  [72/84], [94mLoss[0m : 2.63289
[1mStep[0m  [80/84], [94mLoss[0m : 2.32374

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06341
[1mStep[0m  [8/84], [94mLoss[0m : 2.31230
[1mStep[0m  [16/84], [94mLoss[0m : 2.05548
[1mStep[0m  [24/84], [94mLoss[0m : 2.25890
[1mStep[0m  [32/84], [94mLoss[0m : 2.29984
[1mStep[0m  [40/84], [94mLoss[0m : 2.17231
[1mStep[0m  [48/84], [94mLoss[0m : 2.40559
[1mStep[0m  [56/84], [94mLoss[0m : 2.35711
[1mStep[0m  [64/84], [94mLoss[0m : 2.34599
[1mStep[0m  [72/84], [94mLoss[0m : 2.47462
[1mStep[0m  [80/84], [94mLoss[0m : 2.15196

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26094
[1mStep[0m  [8/84], [94mLoss[0m : 2.24554
[1mStep[0m  [16/84], [94mLoss[0m : 2.29828
[1mStep[0m  [24/84], [94mLoss[0m : 2.00351
[1mStep[0m  [32/84], [94mLoss[0m : 2.19557
[1mStep[0m  [40/84], [94mLoss[0m : 2.15007
[1mStep[0m  [48/84], [94mLoss[0m : 2.30125
[1mStep[0m  [56/84], [94mLoss[0m : 2.23920
[1mStep[0m  [64/84], [94mLoss[0m : 2.14648
[1mStep[0m  [72/84], [94mLoss[0m : 2.05175
[1mStep[0m  [80/84], [94mLoss[0m : 2.04307

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.540, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28429
[1mStep[0m  [8/84], [94mLoss[0m : 2.11401
[1mStep[0m  [16/84], [94mLoss[0m : 2.23209
[1mStep[0m  [24/84], [94mLoss[0m : 2.39480
[1mStep[0m  [32/84], [94mLoss[0m : 2.13517
[1mStep[0m  [40/84], [94mLoss[0m : 2.11870
[1mStep[0m  [48/84], [94mLoss[0m : 2.25791
[1mStep[0m  [56/84], [94mLoss[0m : 2.41880
[1mStep[0m  [64/84], [94mLoss[0m : 1.97534
[1mStep[0m  [72/84], [94mLoss[0m : 2.40867
[1mStep[0m  [80/84], [94mLoss[0m : 2.07710

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24013
[1mStep[0m  [8/84], [94mLoss[0m : 2.06299
[1mStep[0m  [16/84], [94mLoss[0m : 2.08109
[1mStep[0m  [24/84], [94mLoss[0m : 2.14732
[1mStep[0m  [32/84], [94mLoss[0m : 2.17167
[1mStep[0m  [40/84], [94mLoss[0m : 2.05139
[1mStep[0m  [48/84], [94mLoss[0m : 2.17205
[1mStep[0m  [56/84], [94mLoss[0m : 2.39108
[1mStep[0m  [64/84], [94mLoss[0m : 2.09843
[1mStep[0m  [72/84], [94mLoss[0m : 2.24208
[1mStep[0m  [80/84], [94mLoss[0m : 2.40366

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20657
[1mStep[0m  [8/84], [94mLoss[0m : 1.97229
[1mStep[0m  [16/84], [94mLoss[0m : 2.01470
[1mStep[0m  [24/84], [94mLoss[0m : 2.04103
[1mStep[0m  [32/84], [94mLoss[0m : 1.84804
[1mStep[0m  [40/84], [94mLoss[0m : 2.23830
[1mStep[0m  [48/84], [94mLoss[0m : 2.15683
[1mStep[0m  [56/84], [94mLoss[0m : 2.26660
[1mStep[0m  [64/84], [94mLoss[0m : 2.13909
[1mStep[0m  [72/84], [94mLoss[0m : 2.15248
[1mStep[0m  [80/84], [94mLoss[0m : 2.20281

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.115, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04590
[1mStep[0m  [8/84], [94mLoss[0m : 2.05327
[1mStep[0m  [16/84], [94mLoss[0m : 1.98825
[1mStep[0m  [24/84], [94mLoss[0m : 1.92205
[1mStep[0m  [32/84], [94mLoss[0m : 2.10282
[1mStep[0m  [40/84], [94mLoss[0m : 2.00646
[1mStep[0m  [48/84], [94mLoss[0m : 1.97257
[1mStep[0m  [56/84], [94mLoss[0m : 2.12665
[1mStep[0m  [64/84], [94mLoss[0m : 2.10609
[1mStep[0m  [72/84], [94mLoss[0m : 1.92953
[1mStep[0m  [80/84], [94mLoss[0m : 2.35464

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12334
[1mStep[0m  [8/84], [94mLoss[0m : 2.02146
[1mStep[0m  [16/84], [94mLoss[0m : 2.14001
[1mStep[0m  [24/84], [94mLoss[0m : 2.13780
[1mStep[0m  [32/84], [94mLoss[0m : 2.08951
[1mStep[0m  [40/84], [94mLoss[0m : 2.19194
[1mStep[0m  [48/84], [94mLoss[0m : 1.92434
[1mStep[0m  [56/84], [94mLoss[0m : 1.95435
[1mStep[0m  [64/84], [94mLoss[0m : 2.37447
[1mStep[0m  [72/84], [94mLoss[0m : 2.17900
[1mStep[0m  [80/84], [94mLoss[0m : 1.95536

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84135
[1mStep[0m  [8/84], [94mLoss[0m : 2.05029
[1mStep[0m  [16/84], [94mLoss[0m : 1.74407
[1mStep[0m  [24/84], [94mLoss[0m : 2.13386
[1mStep[0m  [32/84], [94mLoss[0m : 2.13379
[1mStep[0m  [40/84], [94mLoss[0m : 1.97632
[1mStep[0m  [48/84], [94mLoss[0m : 1.92990
[1mStep[0m  [56/84], [94mLoss[0m : 2.04903
[1mStep[0m  [64/84], [94mLoss[0m : 2.55935
[1mStep[0m  [72/84], [94mLoss[0m : 1.89966
[1mStep[0m  [80/84], [94mLoss[0m : 2.19313

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99279
[1mStep[0m  [8/84], [94mLoss[0m : 2.36506
[1mStep[0m  [16/84], [94mLoss[0m : 1.95179
[1mStep[0m  [24/84], [94mLoss[0m : 1.80249
[1mStep[0m  [32/84], [94mLoss[0m : 1.71470
[1mStep[0m  [40/84], [94mLoss[0m : 1.85882
[1mStep[0m  [48/84], [94mLoss[0m : 1.82036
[1mStep[0m  [56/84], [94mLoss[0m : 1.74512
[1mStep[0m  [64/84], [94mLoss[0m : 1.97072
[1mStep[0m  [72/84], [94mLoss[0m : 1.81485
[1mStep[0m  [80/84], [94mLoss[0m : 2.15188

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09043
[1mStep[0m  [8/84], [94mLoss[0m : 1.99806
[1mStep[0m  [16/84], [94mLoss[0m : 1.90916
[1mStep[0m  [24/84], [94mLoss[0m : 1.80195
[1mStep[0m  [32/84], [94mLoss[0m : 2.13454
[1mStep[0m  [40/84], [94mLoss[0m : 1.86704
[1mStep[0m  [48/84], [94mLoss[0m : 1.68050
[1mStep[0m  [56/84], [94mLoss[0m : 1.90304
[1mStep[0m  [64/84], [94mLoss[0m : 2.03723
[1mStep[0m  [72/84], [94mLoss[0m : 2.19525
[1mStep[0m  [80/84], [94mLoss[0m : 1.82146

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86358
[1mStep[0m  [8/84], [94mLoss[0m : 2.01683
[1mStep[0m  [16/84], [94mLoss[0m : 1.90338
[1mStep[0m  [24/84], [94mLoss[0m : 1.93674
[1mStep[0m  [32/84], [94mLoss[0m : 1.84647
[1mStep[0m  [40/84], [94mLoss[0m : 1.69724
[1mStep[0m  [48/84], [94mLoss[0m : 1.97906
[1mStep[0m  [56/84], [94mLoss[0m : 2.12695
[1mStep[0m  [64/84], [94mLoss[0m : 1.73907
[1mStep[0m  [72/84], [94mLoss[0m : 1.90101
[1mStep[0m  [80/84], [94mLoss[0m : 2.16372

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89595
[1mStep[0m  [8/84], [94mLoss[0m : 1.94492
[1mStep[0m  [16/84], [94mLoss[0m : 1.74622
[1mStep[0m  [24/84], [94mLoss[0m : 1.69161
[1mStep[0m  [32/84], [94mLoss[0m : 1.79779
[1mStep[0m  [40/84], [94mLoss[0m : 2.22802
[1mStep[0m  [48/84], [94mLoss[0m : 1.74042
[1mStep[0m  [56/84], [94mLoss[0m : 2.00139
[1mStep[0m  [64/84], [94mLoss[0m : 1.88039
[1mStep[0m  [72/84], [94mLoss[0m : 1.77551
[1mStep[0m  [80/84], [94mLoss[0m : 2.07893

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78725
[1mStep[0m  [8/84], [94mLoss[0m : 2.06391
[1mStep[0m  [16/84], [94mLoss[0m : 1.69648
[1mStep[0m  [24/84], [94mLoss[0m : 1.91811
[1mStep[0m  [32/84], [94mLoss[0m : 1.74325
[1mStep[0m  [40/84], [94mLoss[0m : 1.69692
[1mStep[0m  [48/84], [94mLoss[0m : 1.90494
[1mStep[0m  [56/84], [94mLoss[0m : 1.75274
[1mStep[0m  [64/84], [94mLoss[0m : 1.89779
[1mStep[0m  [72/84], [94mLoss[0m : 1.68364
[1mStep[0m  [80/84], [94mLoss[0m : 2.31157

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.877, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83137
[1mStep[0m  [8/84], [94mLoss[0m : 1.67684
[1mStep[0m  [16/84], [94mLoss[0m : 1.57676
[1mStep[0m  [24/84], [94mLoss[0m : 1.82540
[1mStep[0m  [32/84], [94mLoss[0m : 1.88189
[1mStep[0m  [40/84], [94mLoss[0m : 1.78758
[1mStep[0m  [48/84], [94mLoss[0m : 1.73248
[1mStep[0m  [56/84], [94mLoss[0m : 1.68592
[1mStep[0m  [64/84], [94mLoss[0m : 1.72604
[1mStep[0m  [72/84], [94mLoss[0m : 1.81909
[1mStep[0m  [80/84], [94mLoss[0m : 1.70979

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84548
[1mStep[0m  [8/84], [94mLoss[0m : 1.61986
[1mStep[0m  [16/84], [94mLoss[0m : 1.77139
[1mStep[0m  [24/84], [94mLoss[0m : 1.80602
[1mStep[0m  [32/84], [94mLoss[0m : 1.70912
[1mStep[0m  [40/84], [94mLoss[0m : 1.60702
[1mStep[0m  [48/84], [94mLoss[0m : 2.04203
[1mStep[0m  [56/84], [94mLoss[0m : 1.79390
[1mStep[0m  [64/84], [94mLoss[0m : 1.76520
[1mStep[0m  [72/84], [94mLoss[0m : 1.99845
[1mStep[0m  [80/84], [94mLoss[0m : 2.05104

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.53900
[1mStep[0m  [8/84], [94mLoss[0m : 1.76373
[1mStep[0m  [16/84], [94mLoss[0m : 1.82000
[1mStep[0m  [24/84], [94mLoss[0m : 1.97852
[1mStep[0m  [32/84], [94mLoss[0m : 1.74861
[1mStep[0m  [40/84], [94mLoss[0m : 1.82618
[1mStep[0m  [48/84], [94mLoss[0m : 1.74686
[1mStep[0m  [56/84], [94mLoss[0m : 1.97913
[1mStep[0m  [64/84], [94mLoss[0m : 1.68072
[1mStep[0m  [72/84], [94mLoss[0m : 1.80536
[1mStep[0m  [80/84], [94mLoss[0m : 1.95021

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.447, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88206
[1mStep[0m  [8/84], [94mLoss[0m : 1.57900
[1mStep[0m  [16/84], [94mLoss[0m : 1.85696
[1mStep[0m  [24/84], [94mLoss[0m : 1.83825
[1mStep[0m  [32/84], [94mLoss[0m : 1.94062
[1mStep[0m  [40/84], [94mLoss[0m : 1.66548
[1mStep[0m  [48/84], [94mLoss[0m : 1.85982
[1mStep[0m  [56/84], [94mLoss[0m : 1.76986
[1mStep[0m  [64/84], [94mLoss[0m : 1.91887
[1mStep[0m  [72/84], [94mLoss[0m : 1.84812
[1mStep[0m  [80/84], [94mLoss[0m : 2.09682

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73210
[1mStep[0m  [8/84], [94mLoss[0m : 1.96250
[1mStep[0m  [16/84], [94mLoss[0m : 1.68958
[1mStep[0m  [24/84], [94mLoss[0m : 1.72924
[1mStep[0m  [32/84], [94mLoss[0m : 1.79730
[1mStep[0m  [40/84], [94mLoss[0m : 1.71587
[1mStep[0m  [48/84], [94mLoss[0m : 2.03140
[1mStep[0m  [56/84], [94mLoss[0m : 1.92965
[1mStep[0m  [64/84], [94mLoss[0m : 1.85069
[1mStep[0m  [72/84], [94mLoss[0m : 1.47563
[1mStep[0m  [80/84], [94mLoss[0m : 1.77083

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.528
====================================

Phase 2 - Evaluation MAE:  2.5283713936805725
MAE score P1       2.336515
MAE score P2       2.528371
loss               1.775091
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay           0.01
Name: 22, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.01147
[1mStep[0m  [4/42], [94mLoss[0m : 10.92874
[1mStep[0m  [8/42], [94mLoss[0m : 10.03258
[1mStep[0m  [12/42], [94mLoss[0m : 9.66659
[1mStep[0m  [16/42], [94mLoss[0m : 8.88900
[1mStep[0m  [20/42], [94mLoss[0m : 8.21902
[1mStep[0m  [24/42], [94mLoss[0m : 7.47990
[1mStep[0m  [28/42], [94mLoss[0m : 7.30398
[1mStep[0m  [32/42], [94mLoss[0m : 6.55925
[1mStep[0m  [36/42], [94mLoss[0m : 6.04656
[1mStep[0m  [40/42], [94mLoss[0m : 5.21049

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.163, [92mTest[0m: 11.165, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.51976
[1mStep[0m  [4/42], [94mLoss[0m : 4.61569
[1mStep[0m  [8/42], [94mLoss[0m : 4.34828
[1mStep[0m  [12/42], [94mLoss[0m : 3.89105
[1mStep[0m  [16/42], [94mLoss[0m : 3.66585
[1mStep[0m  [20/42], [94mLoss[0m : 3.34819
[1mStep[0m  [24/42], [94mLoss[0m : 3.17090
[1mStep[0m  [28/42], [94mLoss[0m : 3.18040
[1mStep[0m  [32/42], [94mLoss[0m : 2.93342
[1mStep[0m  [36/42], [94mLoss[0m : 2.92885
[1mStep[0m  [40/42], [94mLoss[0m : 2.76059

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.558, [92mTest[0m: 5.094, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.81859
[1mStep[0m  [4/42], [94mLoss[0m : 2.62203
[1mStep[0m  [8/42], [94mLoss[0m : 2.61378
[1mStep[0m  [12/42], [94mLoss[0m : 2.51846
[1mStep[0m  [16/42], [94mLoss[0m : 2.65540
[1mStep[0m  [20/42], [94mLoss[0m : 2.57790
[1mStep[0m  [24/42], [94mLoss[0m : 2.60007
[1mStep[0m  [28/42], [94mLoss[0m : 2.59384
[1mStep[0m  [32/42], [94mLoss[0m : 2.47207
[1mStep[0m  [36/42], [94mLoss[0m : 2.66538
[1mStep[0m  [40/42], [94mLoss[0m : 2.43893

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.691, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35843
[1mStep[0m  [4/42], [94mLoss[0m : 2.48141
[1mStep[0m  [8/42], [94mLoss[0m : 2.60767
[1mStep[0m  [12/42], [94mLoss[0m : 2.42168
[1mStep[0m  [16/42], [94mLoss[0m : 2.70470
[1mStep[0m  [20/42], [94mLoss[0m : 2.42131
[1mStep[0m  [24/42], [94mLoss[0m : 2.39504
[1mStep[0m  [28/42], [94mLoss[0m : 2.51639
[1mStep[0m  [32/42], [94mLoss[0m : 2.38304
[1mStep[0m  [36/42], [94mLoss[0m : 2.54174
[1mStep[0m  [40/42], [94mLoss[0m : 2.56735

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65769
[1mStep[0m  [4/42], [94mLoss[0m : 2.44455
[1mStep[0m  [8/42], [94mLoss[0m : 2.46260
[1mStep[0m  [12/42], [94mLoss[0m : 2.61360
[1mStep[0m  [16/42], [94mLoss[0m : 2.40449
[1mStep[0m  [20/42], [94mLoss[0m : 2.43959
[1mStep[0m  [24/42], [94mLoss[0m : 2.55845
[1mStep[0m  [28/42], [94mLoss[0m : 2.49999
[1mStep[0m  [32/42], [94mLoss[0m : 2.60457
[1mStep[0m  [36/42], [94mLoss[0m : 2.32034
[1mStep[0m  [40/42], [94mLoss[0m : 2.41573

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38984
[1mStep[0m  [4/42], [94mLoss[0m : 2.22092
[1mStep[0m  [8/42], [94mLoss[0m : 2.55685
[1mStep[0m  [12/42], [94mLoss[0m : 2.48961
[1mStep[0m  [16/42], [94mLoss[0m : 2.49735
[1mStep[0m  [20/42], [94mLoss[0m : 2.51067
[1mStep[0m  [24/42], [94mLoss[0m : 2.39242
[1mStep[0m  [28/42], [94mLoss[0m : 2.70031
[1mStep[0m  [32/42], [94mLoss[0m : 2.60868
[1mStep[0m  [36/42], [94mLoss[0m : 2.50645
[1mStep[0m  [40/42], [94mLoss[0m : 2.27809

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40059
[1mStep[0m  [4/42], [94mLoss[0m : 2.57468
[1mStep[0m  [8/42], [94mLoss[0m : 2.51466
[1mStep[0m  [12/42], [94mLoss[0m : 2.39527
[1mStep[0m  [16/42], [94mLoss[0m : 2.68616
[1mStep[0m  [20/42], [94mLoss[0m : 2.49255
[1mStep[0m  [24/42], [94mLoss[0m : 2.43120
[1mStep[0m  [28/42], [94mLoss[0m : 2.43633
[1mStep[0m  [32/42], [94mLoss[0m : 2.48865
[1mStep[0m  [36/42], [94mLoss[0m : 2.69079
[1mStep[0m  [40/42], [94mLoss[0m : 2.37455

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31653
[1mStep[0m  [4/42], [94mLoss[0m : 2.43880
[1mStep[0m  [8/42], [94mLoss[0m : 2.31442
[1mStep[0m  [12/42], [94mLoss[0m : 2.34597
[1mStep[0m  [16/42], [94mLoss[0m : 2.42753
[1mStep[0m  [20/42], [94mLoss[0m : 2.34496
[1mStep[0m  [24/42], [94mLoss[0m : 2.58313
[1mStep[0m  [28/42], [94mLoss[0m : 2.31272
[1mStep[0m  [32/42], [94mLoss[0m : 2.43611
[1mStep[0m  [36/42], [94mLoss[0m : 2.60277
[1mStep[0m  [40/42], [94mLoss[0m : 2.39601

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40012
[1mStep[0m  [4/42], [94mLoss[0m : 2.63424
[1mStep[0m  [8/42], [94mLoss[0m : 2.46032
[1mStep[0m  [12/42], [94mLoss[0m : 2.34807
[1mStep[0m  [16/42], [94mLoss[0m : 2.33162
[1mStep[0m  [20/42], [94mLoss[0m : 2.29216
[1mStep[0m  [24/42], [94mLoss[0m : 2.74790
[1mStep[0m  [28/42], [94mLoss[0m : 2.30388
[1mStep[0m  [32/42], [94mLoss[0m : 2.51952
[1mStep[0m  [36/42], [94mLoss[0m : 2.52447
[1mStep[0m  [40/42], [94mLoss[0m : 2.57179

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72101
[1mStep[0m  [4/42], [94mLoss[0m : 2.63519
[1mStep[0m  [8/42], [94mLoss[0m : 2.37077
[1mStep[0m  [12/42], [94mLoss[0m : 2.49967
[1mStep[0m  [16/42], [94mLoss[0m : 2.54756
[1mStep[0m  [20/42], [94mLoss[0m : 2.50678
[1mStep[0m  [24/42], [94mLoss[0m : 2.31876
[1mStep[0m  [28/42], [94mLoss[0m : 2.37262
[1mStep[0m  [32/42], [94mLoss[0m : 2.48136
[1mStep[0m  [36/42], [94mLoss[0m : 2.40393
[1mStep[0m  [40/42], [94mLoss[0m : 2.36070

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67660
[1mStep[0m  [4/42], [94mLoss[0m : 2.36701
[1mStep[0m  [8/42], [94mLoss[0m : 2.39654
[1mStep[0m  [12/42], [94mLoss[0m : 2.70469
[1mStep[0m  [16/42], [94mLoss[0m : 2.37524
[1mStep[0m  [20/42], [94mLoss[0m : 2.46674
[1mStep[0m  [24/42], [94mLoss[0m : 2.28791
[1mStep[0m  [28/42], [94mLoss[0m : 2.58280
[1mStep[0m  [32/42], [94mLoss[0m : 2.40396
[1mStep[0m  [36/42], [94mLoss[0m : 2.27502
[1mStep[0m  [40/42], [94mLoss[0m : 2.28093

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55285
[1mStep[0m  [4/42], [94mLoss[0m : 2.21484
[1mStep[0m  [8/42], [94mLoss[0m : 2.44920
[1mStep[0m  [12/42], [94mLoss[0m : 2.34065
[1mStep[0m  [16/42], [94mLoss[0m : 2.37988
[1mStep[0m  [20/42], [94mLoss[0m : 2.56536
[1mStep[0m  [24/42], [94mLoss[0m : 2.37489
[1mStep[0m  [28/42], [94mLoss[0m : 2.48628
[1mStep[0m  [32/42], [94mLoss[0m : 2.34390
[1mStep[0m  [36/42], [94mLoss[0m : 2.54384
[1mStep[0m  [40/42], [94mLoss[0m : 2.43145

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46693
[1mStep[0m  [4/42], [94mLoss[0m : 2.47735
[1mStep[0m  [8/42], [94mLoss[0m : 2.41725
[1mStep[0m  [12/42], [94mLoss[0m : 2.35676
[1mStep[0m  [16/42], [94mLoss[0m : 2.67376
[1mStep[0m  [20/42], [94mLoss[0m : 2.38254
[1mStep[0m  [24/42], [94mLoss[0m : 2.44502
[1mStep[0m  [28/42], [94mLoss[0m : 2.67964
[1mStep[0m  [32/42], [94mLoss[0m : 2.35772
[1mStep[0m  [36/42], [94mLoss[0m : 2.48911
[1mStep[0m  [40/42], [94mLoss[0m : 2.46591

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39726
[1mStep[0m  [4/42], [94mLoss[0m : 2.45672
[1mStep[0m  [8/42], [94mLoss[0m : 2.29447
[1mStep[0m  [12/42], [94mLoss[0m : 2.46355
[1mStep[0m  [16/42], [94mLoss[0m : 2.61786
[1mStep[0m  [20/42], [94mLoss[0m : 2.52426
[1mStep[0m  [24/42], [94mLoss[0m : 2.54790
[1mStep[0m  [28/42], [94mLoss[0m : 2.30031
[1mStep[0m  [32/42], [94mLoss[0m : 2.60029
[1mStep[0m  [36/42], [94mLoss[0m : 2.39642
[1mStep[0m  [40/42], [94mLoss[0m : 2.47085

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24671
[1mStep[0m  [4/42], [94mLoss[0m : 2.39422
[1mStep[0m  [8/42], [94mLoss[0m : 2.41489
[1mStep[0m  [12/42], [94mLoss[0m : 2.65218
[1mStep[0m  [16/42], [94mLoss[0m : 2.45610
[1mStep[0m  [20/42], [94mLoss[0m : 2.68223
[1mStep[0m  [24/42], [94mLoss[0m : 2.50577
[1mStep[0m  [28/42], [94mLoss[0m : 2.66259
[1mStep[0m  [32/42], [94mLoss[0m : 2.53967
[1mStep[0m  [36/42], [94mLoss[0m : 2.25017
[1mStep[0m  [40/42], [94mLoss[0m : 2.54226

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44309
[1mStep[0m  [4/42], [94mLoss[0m : 2.37485
[1mStep[0m  [8/42], [94mLoss[0m : 2.31726
[1mStep[0m  [12/42], [94mLoss[0m : 2.66190
[1mStep[0m  [16/42], [94mLoss[0m : 2.10950
[1mStep[0m  [20/42], [94mLoss[0m : 2.33340
[1mStep[0m  [24/42], [94mLoss[0m : 2.35323
[1mStep[0m  [28/42], [94mLoss[0m : 2.53248
[1mStep[0m  [32/42], [94mLoss[0m : 2.56114
[1mStep[0m  [36/42], [94mLoss[0m : 2.34242
[1mStep[0m  [40/42], [94mLoss[0m : 2.69508

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61459
[1mStep[0m  [4/42], [94mLoss[0m : 2.50821
[1mStep[0m  [8/42], [94mLoss[0m : 2.58269
[1mStep[0m  [12/42], [94mLoss[0m : 2.47509
[1mStep[0m  [16/42], [94mLoss[0m : 2.46607
[1mStep[0m  [20/42], [94mLoss[0m : 2.52105
[1mStep[0m  [24/42], [94mLoss[0m : 2.41506
[1mStep[0m  [28/42], [94mLoss[0m : 2.57736
[1mStep[0m  [32/42], [94mLoss[0m : 2.49383
[1mStep[0m  [36/42], [94mLoss[0m : 2.44649
[1mStep[0m  [40/42], [94mLoss[0m : 2.47154

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69444
[1mStep[0m  [4/42], [94mLoss[0m : 2.80706
[1mStep[0m  [8/42], [94mLoss[0m : 2.28649
[1mStep[0m  [12/42], [94mLoss[0m : 2.33657
[1mStep[0m  [16/42], [94mLoss[0m : 2.22293
[1mStep[0m  [20/42], [94mLoss[0m : 2.62485
[1mStep[0m  [24/42], [94mLoss[0m : 2.53974
[1mStep[0m  [28/42], [94mLoss[0m : 2.60390
[1mStep[0m  [32/42], [94mLoss[0m : 2.58695
[1mStep[0m  [36/42], [94mLoss[0m : 2.54671
[1mStep[0m  [40/42], [94mLoss[0m : 2.46149

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28588
[1mStep[0m  [4/42], [94mLoss[0m : 2.34908
[1mStep[0m  [8/42], [94mLoss[0m : 2.40050
[1mStep[0m  [12/42], [94mLoss[0m : 2.37362
[1mStep[0m  [16/42], [94mLoss[0m : 2.48478
[1mStep[0m  [20/42], [94mLoss[0m : 2.24548
[1mStep[0m  [24/42], [94mLoss[0m : 2.28842
[1mStep[0m  [28/42], [94mLoss[0m : 2.44807
[1mStep[0m  [32/42], [94mLoss[0m : 2.32781
[1mStep[0m  [36/42], [94mLoss[0m : 2.19049
[1mStep[0m  [40/42], [94mLoss[0m : 2.62905

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60266
[1mStep[0m  [4/42], [94mLoss[0m : 2.39120
[1mStep[0m  [8/42], [94mLoss[0m : 2.46542
[1mStep[0m  [12/42], [94mLoss[0m : 2.33573
[1mStep[0m  [16/42], [94mLoss[0m : 2.37747
[1mStep[0m  [20/42], [94mLoss[0m : 2.23373
[1mStep[0m  [24/42], [94mLoss[0m : 2.51580
[1mStep[0m  [28/42], [94mLoss[0m : 2.53145
[1mStep[0m  [32/42], [94mLoss[0m : 2.54155
[1mStep[0m  [36/42], [94mLoss[0m : 2.40995
[1mStep[0m  [40/42], [94mLoss[0m : 2.58222

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37902
[1mStep[0m  [4/42], [94mLoss[0m : 2.57498
[1mStep[0m  [8/42], [94mLoss[0m : 2.49728
[1mStep[0m  [12/42], [94mLoss[0m : 2.59052
[1mStep[0m  [16/42], [94mLoss[0m : 2.47020
[1mStep[0m  [20/42], [94mLoss[0m : 2.42076
[1mStep[0m  [24/42], [94mLoss[0m : 2.44970
[1mStep[0m  [28/42], [94mLoss[0m : 2.60138
[1mStep[0m  [32/42], [94mLoss[0m : 2.35038
[1mStep[0m  [36/42], [94mLoss[0m : 2.32597
[1mStep[0m  [40/42], [94mLoss[0m : 2.28705

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32137
[1mStep[0m  [4/42], [94mLoss[0m : 2.56112
[1mStep[0m  [8/42], [94mLoss[0m : 2.35367
[1mStep[0m  [12/42], [94mLoss[0m : 2.31397
[1mStep[0m  [16/42], [94mLoss[0m : 2.45195
[1mStep[0m  [20/42], [94mLoss[0m : 2.61728
[1mStep[0m  [24/42], [94mLoss[0m : 2.49305
[1mStep[0m  [28/42], [94mLoss[0m : 2.37044
[1mStep[0m  [32/42], [94mLoss[0m : 2.38766
[1mStep[0m  [36/42], [94mLoss[0m : 2.41017
[1mStep[0m  [40/42], [94mLoss[0m : 2.41885

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47609
[1mStep[0m  [4/42], [94mLoss[0m : 2.57529
[1mStep[0m  [8/42], [94mLoss[0m : 2.31811
[1mStep[0m  [12/42], [94mLoss[0m : 2.18084
[1mStep[0m  [16/42], [94mLoss[0m : 2.54605
[1mStep[0m  [20/42], [94mLoss[0m : 2.29314
[1mStep[0m  [24/42], [94mLoss[0m : 2.38592
[1mStep[0m  [28/42], [94mLoss[0m : 2.63938
[1mStep[0m  [32/42], [94mLoss[0m : 2.41299
[1mStep[0m  [36/42], [94mLoss[0m : 2.67262
[1mStep[0m  [40/42], [94mLoss[0m : 2.44678

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57793
[1mStep[0m  [4/42], [94mLoss[0m : 2.43636
[1mStep[0m  [8/42], [94mLoss[0m : 2.35127
[1mStep[0m  [12/42], [94mLoss[0m : 2.21817
[1mStep[0m  [16/42], [94mLoss[0m : 2.37353
[1mStep[0m  [20/42], [94mLoss[0m : 2.37569
[1mStep[0m  [24/42], [94mLoss[0m : 2.42900
[1mStep[0m  [28/42], [94mLoss[0m : 2.40672
[1mStep[0m  [32/42], [94mLoss[0m : 2.32758
[1mStep[0m  [36/42], [94mLoss[0m : 2.57367
[1mStep[0m  [40/42], [94mLoss[0m : 2.37796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41034
[1mStep[0m  [4/42], [94mLoss[0m : 2.31664
[1mStep[0m  [8/42], [94mLoss[0m : 2.51177
[1mStep[0m  [12/42], [94mLoss[0m : 2.30980
[1mStep[0m  [16/42], [94mLoss[0m : 2.35784
[1mStep[0m  [20/42], [94mLoss[0m : 2.42882
[1mStep[0m  [24/42], [94mLoss[0m : 2.46838
[1mStep[0m  [28/42], [94mLoss[0m : 2.51762
[1mStep[0m  [32/42], [94mLoss[0m : 2.48718
[1mStep[0m  [36/42], [94mLoss[0m : 2.42098
[1mStep[0m  [40/42], [94mLoss[0m : 2.37688

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45088
[1mStep[0m  [4/42], [94mLoss[0m : 2.41205
[1mStep[0m  [8/42], [94mLoss[0m : 2.25915
[1mStep[0m  [12/42], [94mLoss[0m : 2.64998
[1mStep[0m  [16/42], [94mLoss[0m : 2.42735
[1mStep[0m  [20/42], [94mLoss[0m : 2.42832
[1mStep[0m  [24/42], [94mLoss[0m : 2.48399
[1mStep[0m  [28/42], [94mLoss[0m : 2.43817
[1mStep[0m  [32/42], [94mLoss[0m : 2.59841
[1mStep[0m  [36/42], [94mLoss[0m : 2.67001
[1mStep[0m  [40/42], [94mLoss[0m : 2.33795

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47331
[1mStep[0m  [4/42], [94mLoss[0m : 2.55180
[1mStep[0m  [8/42], [94mLoss[0m : 2.36853
[1mStep[0m  [12/42], [94mLoss[0m : 2.36412
[1mStep[0m  [16/42], [94mLoss[0m : 2.33041
[1mStep[0m  [20/42], [94mLoss[0m : 2.30427
[1mStep[0m  [24/42], [94mLoss[0m : 2.18527
[1mStep[0m  [28/42], [94mLoss[0m : 2.51035
[1mStep[0m  [32/42], [94mLoss[0m : 2.70225
[1mStep[0m  [36/42], [94mLoss[0m : 2.66185
[1mStep[0m  [40/42], [94mLoss[0m : 2.23792

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49782
[1mStep[0m  [4/42], [94mLoss[0m : 2.60749
[1mStep[0m  [8/42], [94mLoss[0m : 2.24646
[1mStep[0m  [12/42], [94mLoss[0m : 2.49954
[1mStep[0m  [16/42], [94mLoss[0m : 2.66130
[1mStep[0m  [20/42], [94mLoss[0m : 2.35413
[1mStep[0m  [24/42], [94mLoss[0m : 2.28108
[1mStep[0m  [28/42], [94mLoss[0m : 2.47142
[1mStep[0m  [32/42], [94mLoss[0m : 2.43462
[1mStep[0m  [36/42], [94mLoss[0m : 2.43900
[1mStep[0m  [40/42], [94mLoss[0m : 2.44650

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45082
[1mStep[0m  [4/42], [94mLoss[0m : 2.35518
[1mStep[0m  [8/42], [94mLoss[0m : 2.59383
[1mStep[0m  [12/42], [94mLoss[0m : 2.47443
[1mStep[0m  [16/42], [94mLoss[0m : 2.39445
[1mStep[0m  [20/42], [94mLoss[0m : 2.33357
[1mStep[0m  [24/42], [94mLoss[0m : 2.49382
[1mStep[0m  [28/42], [94mLoss[0m : 2.49714
[1mStep[0m  [32/42], [94mLoss[0m : 2.37441
[1mStep[0m  [36/42], [94mLoss[0m : 2.64207
[1mStep[0m  [40/42], [94mLoss[0m : 2.45905

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68411
[1mStep[0m  [4/42], [94mLoss[0m : 2.43221
[1mStep[0m  [8/42], [94mLoss[0m : 2.43689
[1mStep[0m  [12/42], [94mLoss[0m : 2.23012
[1mStep[0m  [16/42], [94mLoss[0m : 2.79827
[1mStep[0m  [20/42], [94mLoss[0m : 2.40702
[1mStep[0m  [24/42], [94mLoss[0m : 2.44344
[1mStep[0m  [28/42], [94mLoss[0m : 2.37462
[1mStep[0m  [32/42], [94mLoss[0m : 2.57657
[1mStep[0m  [36/42], [94mLoss[0m : 2.32379
[1mStep[0m  [40/42], [94mLoss[0m : 2.53098

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.3333235468183244
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.46165
[1mStep[0m  [4/42], [94mLoss[0m : 2.54404
[1mStep[0m  [8/42], [94mLoss[0m : 2.47517
[1mStep[0m  [12/42], [94mLoss[0m : 2.48557
[1mStep[0m  [16/42], [94mLoss[0m : 2.59673
[1mStep[0m  [20/42], [94mLoss[0m : 2.55500
[1mStep[0m  [24/42], [94mLoss[0m : 2.46891
[1mStep[0m  [28/42], [94mLoss[0m : 2.49553
[1mStep[0m  [32/42], [94mLoss[0m : 2.56184
[1mStep[0m  [36/42], [94mLoss[0m : 2.49716
[1mStep[0m  [40/42], [94mLoss[0m : 2.23897

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31454
[1mStep[0m  [4/42], [94mLoss[0m : 2.43973
[1mStep[0m  [8/42], [94mLoss[0m : 2.40336
[1mStep[0m  [12/42], [94mLoss[0m : 2.51480
[1mStep[0m  [16/42], [94mLoss[0m : 2.24993
[1mStep[0m  [20/42], [94mLoss[0m : 2.32393
[1mStep[0m  [24/42], [94mLoss[0m : 2.18708
[1mStep[0m  [28/42], [94mLoss[0m : 2.53639
[1mStep[0m  [32/42], [94mLoss[0m : 2.48134
[1mStep[0m  [36/42], [94mLoss[0m : 2.37460
[1mStep[0m  [40/42], [94mLoss[0m : 2.38158

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38810
[1mStep[0m  [4/42], [94mLoss[0m : 2.26294
[1mStep[0m  [8/42], [94mLoss[0m : 2.69253
[1mStep[0m  [12/42], [94mLoss[0m : 2.41416
[1mStep[0m  [16/42], [94mLoss[0m : 2.17042
[1mStep[0m  [20/42], [94mLoss[0m : 2.33778
[1mStep[0m  [24/42], [94mLoss[0m : 2.64127
[1mStep[0m  [28/42], [94mLoss[0m : 2.53302
[1mStep[0m  [32/42], [94mLoss[0m : 2.36170
[1mStep[0m  [36/42], [94mLoss[0m : 2.32019
[1mStep[0m  [40/42], [94mLoss[0m : 2.53805

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43238
[1mStep[0m  [4/42], [94mLoss[0m : 2.43368
[1mStep[0m  [8/42], [94mLoss[0m : 2.45156
[1mStep[0m  [12/42], [94mLoss[0m : 2.54747
[1mStep[0m  [16/42], [94mLoss[0m : 2.37605
[1mStep[0m  [20/42], [94mLoss[0m : 2.36267
[1mStep[0m  [24/42], [94mLoss[0m : 2.38681
[1mStep[0m  [28/42], [94mLoss[0m : 2.64925
[1mStep[0m  [32/42], [94mLoss[0m : 2.10558
[1mStep[0m  [36/42], [94mLoss[0m : 2.39723
[1mStep[0m  [40/42], [94mLoss[0m : 2.40990

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41452
[1mStep[0m  [4/42], [94mLoss[0m : 2.34159
[1mStep[0m  [8/42], [94mLoss[0m : 2.24458
[1mStep[0m  [12/42], [94mLoss[0m : 2.44140
[1mStep[0m  [16/42], [94mLoss[0m : 2.51889
[1mStep[0m  [20/42], [94mLoss[0m : 2.32928
[1mStep[0m  [24/42], [94mLoss[0m : 2.40551
[1mStep[0m  [28/42], [94mLoss[0m : 2.23111
[1mStep[0m  [32/42], [94mLoss[0m : 2.44670
[1mStep[0m  [36/42], [94mLoss[0m : 2.29958
[1mStep[0m  [40/42], [94mLoss[0m : 2.29203

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62295
[1mStep[0m  [4/42], [94mLoss[0m : 2.34395
[1mStep[0m  [8/42], [94mLoss[0m : 2.26064
[1mStep[0m  [12/42], [94mLoss[0m : 2.36125
[1mStep[0m  [16/42], [94mLoss[0m : 2.30925
[1mStep[0m  [20/42], [94mLoss[0m : 2.29075
[1mStep[0m  [24/42], [94mLoss[0m : 2.37153
[1mStep[0m  [28/42], [94mLoss[0m : 2.25489
[1mStep[0m  [32/42], [94mLoss[0m : 2.31985
[1mStep[0m  [36/42], [94mLoss[0m : 2.40512
[1mStep[0m  [40/42], [94mLoss[0m : 2.08206

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34247
[1mStep[0m  [4/42], [94mLoss[0m : 2.65183
[1mStep[0m  [8/42], [94mLoss[0m : 2.38761
[1mStep[0m  [12/42], [94mLoss[0m : 2.25563
[1mStep[0m  [16/42], [94mLoss[0m : 2.22947
[1mStep[0m  [20/42], [94mLoss[0m : 2.44900
[1mStep[0m  [24/42], [94mLoss[0m : 2.39362
[1mStep[0m  [28/42], [94mLoss[0m : 2.30624
[1mStep[0m  [32/42], [94mLoss[0m : 2.30348
[1mStep[0m  [36/42], [94mLoss[0m : 2.47939
[1mStep[0m  [40/42], [94mLoss[0m : 2.45709

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23948
[1mStep[0m  [4/42], [94mLoss[0m : 2.42407
[1mStep[0m  [8/42], [94mLoss[0m : 2.41848
[1mStep[0m  [12/42], [94mLoss[0m : 2.28489
[1mStep[0m  [16/42], [94mLoss[0m : 2.26294
[1mStep[0m  [20/42], [94mLoss[0m : 2.31149
[1mStep[0m  [24/42], [94mLoss[0m : 2.22917
[1mStep[0m  [28/42], [94mLoss[0m : 2.37064
[1mStep[0m  [32/42], [94mLoss[0m : 2.19676
[1mStep[0m  [36/42], [94mLoss[0m : 2.38501
[1mStep[0m  [40/42], [94mLoss[0m : 2.30847

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22151
[1mStep[0m  [4/42], [94mLoss[0m : 2.24295
[1mStep[0m  [8/42], [94mLoss[0m : 2.34238
[1mStep[0m  [12/42], [94mLoss[0m : 2.21592
[1mStep[0m  [16/42], [94mLoss[0m : 2.45315
[1mStep[0m  [20/42], [94mLoss[0m : 2.46037
[1mStep[0m  [24/42], [94mLoss[0m : 2.13664
[1mStep[0m  [28/42], [94mLoss[0m : 2.13437
[1mStep[0m  [32/42], [94mLoss[0m : 2.33828
[1mStep[0m  [36/42], [94mLoss[0m : 2.29085
[1mStep[0m  [40/42], [94mLoss[0m : 2.44083

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.553, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30902
[1mStep[0m  [4/42], [94mLoss[0m : 2.36862
[1mStep[0m  [8/42], [94mLoss[0m : 2.14304
[1mStep[0m  [12/42], [94mLoss[0m : 2.23427
[1mStep[0m  [16/42], [94mLoss[0m : 2.24675
[1mStep[0m  [20/42], [94mLoss[0m : 2.21751
[1mStep[0m  [24/42], [94mLoss[0m : 2.14922
[1mStep[0m  [28/42], [94mLoss[0m : 2.35817
[1mStep[0m  [32/42], [94mLoss[0m : 2.31430
[1mStep[0m  [36/42], [94mLoss[0m : 2.16771
[1mStep[0m  [40/42], [94mLoss[0m : 2.18854

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14804
[1mStep[0m  [4/42], [94mLoss[0m : 2.20324
[1mStep[0m  [8/42], [94mLoss[0m : 2.20180
[1mStep[0m  [12/42], [94mLoss[0m : 2.29611
[1mStep[0m  [16/42], [94mLoss[0m : 2.20620
[1mStep[0m  [20/42], [94mLoss[0m : 2.46592
[1mStep[0m  [24/42], [94mLoss[0m : 2.14315
[1mStep[0m  [28/42], [94mLoss[0m : 2.29510
[1mStep[0m  [32/42], [94mLoss[0m : 2.11201
[1mStep[0m  [36/42], [94mLoss[0m : 2.18339
[1mStep[0m  [40/42], [94mLoss[0m : 2.22676

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08904
[1mStep[0m  [4/42], [94mLoss[0m : 2.09990
[1mStep[0m  [8/42], [94mLoss[0m : 2.15491
[1mStep[0m  [12/42], [94mLoss[0m : 2.13899
[1mStep[0m  [16/42], [94mLoss[0m : 2.14192
[1mStep[0m  [20/42], [94mLoss[0m : 2.33895
[1mStep[0m  [24/42], [94mLoss[0m : 2.21451
[1mStep[0m  [28/42], [94mLoss[0m : 2.26183
[1mStep[0m  [32/42], [94mLoss[0m : 2.08148
[1mStep[0m  [36/42], [94mLoss[0m : 2.29416
[1mStep[0m  [40/42], [94mLoss[0m : 2.31115

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24519
[1mStep[0m  [4/42], [94mLoss[0m : 2.35755
[1mStep[0m  [8/42], [94mLoss[0m : 2.21435
[1mStep[0m  [12/42], [94mLoss[0m : 2.23671
[1mStep[0m  [16/42], [94mLoss[0m : 2.16248
[1mStep[0m  [20/42], [94mLoss[0m : 2.26549
[1mStep[0m  [24/42], [94mLoss[0m : 2.18587
[1mStep[0m  [28/42], [94mLoss[0m : 2.13597
[1mStep[0m  [32/42], [94mLoss[0m : 2.19947
[1mStep[0m  [36/42], [94mLoss[0m : 2.06437
[1mStep[0m  [40/42], [94mLoss[0m : 2.34709

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31068
[1mStep[0m  [4/42], [94mLoss[0m : 2.44573
[1mStep[0m  [8/42], [94mLoss[0m : 2.42221
[1mStep[0m  [12/42], [94mLoss[0m : 2.14226
[1mStep[0m  [16/42], [94mLoss[0m : 2.31224
[1mStep[0m  [20/42], [94mLoss[0m : 2.16886
[1mStep[0m  [24/42], [94mLoss[0m : 2.31304
[1mStep[0m  [28/42], [94mLoss[0m : 2.16404
[1mStep[0m  [32/42], [94mLoss[0m : 2.24568
[1mStep[0m  [36/42], [94mLoss[0m : 2.15335
[1mStep[0m  [40/42], [94mLoss[0m : 2.18531

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.549, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.11783
[1mStep[0m  [4/42], [94mLoss[0m : 2.06294
[1mStep[0m  [8/42], [94mLoss[0m : 2.04845
[1mStep[0m  [12/42], [94mLoss[0m : 2.17431
[1mStep[0m  [16/42], [94mLoss[0m : 2.14957
[1mStep[0m  [20/42], [94mLoss[0m : 2.29263
[1mStep[0m  [24/42], [94mLoss[0m : 2.12927
[1mStep[0m  [28/42], [94mLoss[0m : 2.28807
[1mStep[0m  [32/42], [94mLoss[0m : 2.02930
[1mStep[0m  [36/42], [94mLoss[0m : 2.41522
[1mStep[0m  [40/42], [94mLoss[0m : 1.92350

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15406
[1mStep[0m  [4/42], [94mLoss[0m : 2.03204
[1mStep[0m  [8/42], [94mLoss[0m : 2.10715
[1mStep[0m  [12/42], [94mLoss[0m : 2.05168
[1mStep[0m  [16/42], [94mLoss[0m : 2.43105
[1mStep[0m  [20/42], [94mLoss[0m : 2.40031
[1mStep[0m  [24/42], [94mLoss[0m : 2.16265
[1mStep[0m  [28/42], [94mLoss[0m : 2.11451
[1mStep[0m  [32/42], [94mLoss[0m : 2.24800
[1mStep[0m  [36/42], [94mLoss[0m : 2.12858
[1mStep[0m  [40/42], [94mLoss[0m : 2.30080

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08465
[1mStep[0m  [4/42], [94mLoss[0m : 2.29175
[1mStep[0m  [8/42], [94mLoss[0m : 2.06461
[1mStep[0m  [12/42], [94mLoss[0m : 2.03673
[1mStep[0m  [16/42], [94mLoss[0m : 2.10833
[1mStep[0m  [20/42], [94mLoss[0m : 1.97142
[1mStep[0m  [24/42], [94mLoss[0m : 2.38660
[1mStep[0m  [28/42], [94mLoss[0m : 2.08714
[1mStep[0m  [32/42], [94mLoss[0m : 2.18895
[1mStep[0m  [36/42], [94mLoss[0m : 2.09614
[1mStep[0m  [40/42], [94mLoss[0m : 2.30813

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.19717
[1mStep[0m  [4/42], [94mLoss[0m : 2.20678
[1mStep[0m  [8/42], [94mLoss[0m : 2.11024
[1mStep[0m  [12/42], [94mLoss[0m : 2.05753
[1mStep[0m  [16/42], [94mLoss[0m : 2.05284
[1mStep[0m  [20/42], [94mLoss[0m : 2.12252
[1mStep[0m  [24/42], [94mLoss[0m : 2.04143
[1mStep[0m  [28/42], [94mLoss[0m : 2.34189
[1mStep[0m  [32/42], [94mLoss[0m : 2.31337
[1mStep[0m  [36/42], [94mLoss[0m : 2.03425
[1mStep[0m  [40/42], [94mLoss[0m : 1.94667

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.98082
[1mStep[0m  [4/42], [94mLoss[0m : 2.02619
[1mStep[0m  [8/42], [94mLoss[0m : 2.02243
[1mStep[0m  [12/42], [94mLoss[0m : 2.05832
[1mStep[0m  [16/42], [94mLoss[0m : 2.25665
[1mStep[0m  [20/42], [94mLoss[0m : 1.91779
[1mStep[0m  [24/42], [94mLoss[0m : 1.96029
[1mStep[0m  [28/42], [94mLoss[0m : 2.14048
[1mStep[0m  [32/42], [94mLoss[0m : 1.95990
[1mStep[0m  [36/42], [94mLoss[0m : 2.11961
[1mStep[0m  [40/42], [94mLoss[0m : 2.17705

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.84019
[1mStep[0m  [4/42], [94mLoss[0m : 1.95183
[1mStep[0m  [8/42], [94mLoss[0m : 2.10132
[1mStep[0m  [12/42], [94mLoss[0m : 2.20197
[1mStep[0m  [16/42], [94mLoss[0m : 2.17582
[1mStep[0m  [20/42], [94mLoss[0m : 2.05063
[1mStep[0m  [24/42], [94mLoss[0m : 2.10999
[1mStep[0m  [28/42], [94mLoss[0m : 2.15686
[1mStep[0m  [32/42], [94mLoss[0m : 2.07488
[1mStep[0m  [36/42], [94mLoss[0m : 1.98164
[1mStep[0m  [40/42], [94mLoss[0m : 2.12799

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.608, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03679
[1mStep[0m  [4/42], [94mLoss[0m : 1.83215
[1mStep[0m  [8/42], [94mLoss[0m : 1.92918
[1mStep[0m  [12/42], [94mLoss[0m : 2.15033
[1mStep[0m  [16/42], [94mLoss[0m : 2.13270
[1mStep[0m  [20/42], [94mLoss[0m : 1.95224
[1mStep[0m  [24/42], [94mLoss[0m : 1.99136
[1mStep[0m  [28/42], [94mLoss[0m : 1.96497
[1mStep[0m  [32/42], [94mLoss[0m : 2.02423
[1mStep[0m  [36/42], [94mLoss[0m : 2.05855
[1mStep[0m  [40/42], [94mLoss[0m : 1.99879

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95686
[1mStep[0m  [4/42], [94mLoss[0m : 2.05299
[1mStep[0m  [8/42], [94mLoss[0m : 1.84773
[1mStep[0m  [12/42], [94mLoss[0m : 1.97912
[1mStep[0m  [16/42], [94mLoss[0m : 1.94698
[1mStep[0m  [20/42], [94mLoss[0m : 1.95933
[1mStep[0m  [24/42], [94mLoss[0m : 2.03064
[1mStep[0m  [28/42], [94mLoss[0m : 2.08385
[1mStep[0m  [32/42], [94mLoss[0m : 2.06503
[1mStep[0m  [36/42], [94mLoss[0m : 2.07223
[1mStep[0m  [40/42], [94mLoss[0m : 1.84602

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08354
[1mStep[0m  [4/42], [94mLoss[0m : 1.88808
[1mStep[0m  [8/42], [94mLoss[0m : 1.81318
[1mStep[0m  [12/42], [94mLoss[0m : 2.11371
[1mStep[0m  [16/42], [94mLoss[0m : 1.96144
[1mStep[0m  [20/42], [94mLoss[0m : 1.94391
[1mStep[0m  [24/42], [94mLoss[0m : 1.85321
[1mStep[0m  [28/42], [94mLoss[0m : 1.92875
[1mStep[0m  [32/42], [94mLoss[0m : 2.11724
[1mStep[0m  [36/42], [94mLoss[0m : 2.15362
[1mStep[0m  [40/42], [94mLoss[0m : 1.81758

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.87992
[1mStep[0m  [4/42], [94mLoss[0m : 2.15732
[1mStep[0m  [8/42], [94mLoss[0m : 1.90972
[1mStep[0m  [12/42], [94mLoss[0m : 1.96711
[1mStep[0m  [16/42], [94mLoss[0m : 1.72723
[1mStep[0m  [20/42], [94mLoss[0m : 1.87907
[1mStep[0m  [24/42], [94mLoss[0m : 2.05455
[1mStep[0m  [28/42], [94mLoss[0m : 2.05423
[1mStep[0m  [32/42], [94mLoss[0m : 1.94018
[1mStep[0m  [36/42], [94mLoss[0m : 1.92577
[1mStep[0m  [40/42], [94mLoss[0m : 2.05946

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.938, [92mTest[0m: 2.504, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92191
[1mStep[0m  [4/42], [94mLoss[0m : 1.98154
[1mStep[0m  [8/42], [94mLoss[0m : 1.76886
[1mStep[0m  [12/42], [94mLoss[0m : 1.82931
[1mStep[0m  [16/42], [94mLoss[0m : 1.87034
[1mStep[0m  [20/42], [94mLoss[0m : 1.94654
[1mStep[0m  [24/42], [94mLoss[0m : 1.99550
[1mStep[0m  [28/42], [94mLoss[0m : 1.97128
[1mStep[0m  [32/42], [94mLoss[0m : 1.96988
[1mStep[0m  [36/42], [94mLoss[0m : 1.76521
[1mStep[0m  [40/42], [94mLoss[0m : 1.84385

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.86127
[1mStep[0m  [4/42], [94mLoss[0m : 2.09362
[1mStep[0m  [8/42], [94mLoss[0m : 1.87549
[1mStep[0m  [12/42], [94mLoss[0m : 1.83755
[1mStep[0m  [16/42], [94mLoss[0m : 1.93872
[1mStep[0m  [20/42], [94mLoss[0m : 1.94884
[1mStep[0m  [24/42], [94mLoss[0m : 2.18664
[1mStep[0m  [28/42], [94mLoss[0m : 2.09237
[1mStep[0m  [32/42], [94mLoss[0m : 1.84398
[1mStep[0m  [36/42], [94mLoss[0m : 1.85202
[1mStep[0m  [40/42], [94mLoss[0m : 1.95513

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72225
[1mStep[0m  [4/42], [94mLoss[0m : 1.85325
[1mStep[0m  [8/42], [94mLoss[0m : 1.66280
[1mStep[0m  [12/42], [94mLoss[0m : 1.91175
[1mStep[0m  [16/42], [94mLoss[0m : 1.70899
[1mStep[0m  [20/42], [94mLoss[0m : 1.82619
[1mStep[0m  [24/42], [94mLoss[0m : 1.90126
[1mStep[0m  [28/42], [94mLoss[0m : 1.76511
[1mStep[0m  [32/42], [94mLoss[0m : 1.98268
[1mStep[0m  [36/42], [94mLoss[0m : 1.88232
[1mStep[0m  [40/42], [94mLoss[0m : 1.83145

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.600, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54754
[1mStep[0m  [4/42], [94mLoss[0m : 1.71008
[1mStep[0m  [8/42], [94mLoss[0m : 1.84879
[1mStep[0m  [12/42], [94mLoss[0m : 1.75708
[1mStep[0m  [16/42], [94mLoss[0m : 1.88732
[1mStep[0m  [20/42], [94mLoss[0m : 1.82938
[1mStep[0m  [24/42], [94mLoss[0m : 1.81374
[1mStep[0m  [28/42], [94mLoss[0m : 1.75850
[1mStep[0m  [32/42], [94mLoss[0m : 1.72287
[1mStep[0m  [36/42], [94mLoss[0m : 1.70291
[1mStep[0m  [40/42], [94mLoss[0m : 1.89609

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.838, [92mTest[0m: 2.505, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90852
[1mStep[0m  [4/42], [94mLoss[0m : 1.60657
[1mStep[0m  [8/42], [94mLoss[0m : 1.80826
[1mStep[0m  [12/42], [94mLoss[0m : 1.86192
[1mStep[0m  [16/42], [94mLoss[0m : 1.85139
[1mStep[0m  [20/42], [94mLoss[0m : 1.90251
[1mStep[0m  [24/42], [94mLoss[0m : 1.64056
[1mStep[0m  [28/42], [94mLoss[0m : 1.76737
[1mStep[0m  [32/42], [94mLoss[0m : 1.76668
[1mStep[0m  [36/42], [94mLoss[0m : 1.70309
[1mStep[0m  [40/42], [94mLoss[0m : 1.66394

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.599, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92236
[1mStep[0m  [4/42], [94mLoss[0m : 1.77773
[1mStep[0m  [8/42], [94mLoss[0m : 1.80942
[1mStep[0m  [12/42], [94mLoss[0m : 1.68196
[1mStep[0m  [16/42], [94mLoss[0m : 1.69261
[1mStep[0m  [20/42], [94mLoss[0m : 1.65197
[1mStep[0m  [24/42], [94mLoss[0m : 1.67136
[1mStep[0m  [28/42], [94mLoss[0m : 1.63946
[1mStep[0m  [32/42], [94mLoss[0m : 1.82266
[1mStep[0m  [36/42], [94mLoss[0m : 1.82399
[1mStep[0m  [40/42], [94mLoss[0m : 1.80058

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.530, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.515
====================================

Phase 2 - Evaluation MAE:  2.5147034100123813
MAE score P1      2.333324
MAE score P2      2.514703
loss              1.781524
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 23, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.92380
[1mStep[0m  [4/42], [94mLoss[0m : 10.72869
[1mStep[0m  [8/42], [94mLoss[0m : 9.99177
[1mStep[0m  [12/42], [94mLoss[0m : 9.64790
[1mStep[0m  [16/42], [94mLoss[0m : 9.71453
[1mStep[0m  [20/42], [94mLoss[0m : 8.71612
[1mStep[0m  [24/42], [94mLoss[0m : 8.81854
[1mStep[0m  [28/42], [94mLoss[0m : 8.69673
[1mStep[0m  [32/42], [94mLoss[0m : 8.37228
[1mStep[0m  [36/42], [94mLoss[0m : 7.89683
[1mStep[0m  [40/42], [94mLoss[0m : 7.40812

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.126, [92mTest[0m: 10.942, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.19854
[1mStep[0m  [4/42], [94mLoss[0m : 6.74004
[1mStep[0m  [8/42], [94mLoss[0m : 6.37662
[1mStep[0m  [12/42], [94mLoss[0m : 6.38067
[1mStep[0m  [16/42], [94mLoss[0m : 5.87483
[1mStep[0m  [20/42], [94mLoss[0m : 5.75436
[1mStep[0m  [24/42], [94mLoss[0m : 5.20442
[1mStep[0m  [28/42], [94mLoss[0m : 4.85298
[1mStep[0m  [32/42], [94mLoss[0m : 4.55373
[1mStep[0m  [36/42], [94mLoss[0m : 4.17093
[1mStep[0m  [40/42], [94mLoss[0m : 4.60818

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.542, [92mTest[0m: 8.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.97221
[1mStep[0m  [4/42], [94mLoss[0m : 3.84767
[1mStep[0m  [8/42], [94mLoss[0m : 4.04632
[1mStep[0m  [12/42], [94mLoss[0m : 3.39559
[1mStep[0m  [16/42], [94mLoss[0m : 3.22510
[1mStep[0m  [20/42], [94mLoss[0m : 3.38313
[1mStep[0m  [24/42], [94mLoss[0m : 3.13352
[1mStep[0m  [28/42], [94mLoss[0m : 3.38313
[1mStep[0m  [32/42], [94mLoss[0m : 3.15688
[1mStep[0m  [36/42], [94mLoss[0m : 2.99947
[1mStep[0m  [40/42], [94mLoss[0m : 2.61352

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.342, [92mTest[0m: 5.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82513
[1mStep[0m  [4/42], [94mLoss[0m : 3.14486
[1mStep[0m  [8/42], [94mLoss[0m : 3.07252
[1mStep[0m  [12/42], [94mLoss[0m : 3.07183
[1mStep[0m  [16/42], [94mLoss[0m : 2.75642
[1mStep[0m  [20/42], [94mLoss[0m : 2.87556
[1mStep[0m  [24/42], [94mLoss[0m : 2.57827
[1mStep[0m  [28/42], [94mLoss[0m : 2.58689
[1mStep[0m  [32/42], [94mLoss[0m : 2.72296
[1mStep[0m  [36/42], [94mLoss[0m : 2.69967
[1mStep[0m  [40/42], [94mLoss[0m : 2.87218

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.793, [92mTest[0m: 3.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68991
[1mStep[0m  [4/42], [94mLoss[0m : 2.59403
[1mStep[0m  [8/42], [94mLoss[0m : 2.74739
[1mStep[0m  [12/42], [94mLoss[0m : 2.66595
[1mStep[0m  [16/42], [94mLoss[0m : 2.62734
[1mStep[0m  [20/42], [94mLoss[0m : 2.67913
[1mStep[0m  [24/42], [94mLoss[0m : 2.75871
[1mStep[0m  [28/42], [94mLoss[0m : 2.85514
[1mStep[0m  [32/42], [94mLoss[0m : 2.81354
[1mStep[0m  [36/42], [94mLoss[0m : 2.78657
[1mStep[0m  [40/42], [94mLoss[0m : 2.76177

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.720, [92mTest[0m: 3.043, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60386
[1mStep[0m  [4/42], [94mLoss[0m : 2.74978
[1mStep[0m  [8/42], [94mLoss[0m : 2.53461
[1mStep[0m  [12/42], [94mLoss[0m : 2.71283
[1mStep[0m  [16/42], [94mLoss[0m : 2.54396
[1mStep[0m  [20/42], [94mLoss[0m : 2.68530
[1mStep[0m  [24/42], [94mLoss[0m : 2.75916
[1mStep[0m  [28/42], [94mLoss[0m : 2.54983
[1mStep[0m  [32/42], [94mLoss[0m : 2.70687
[1mStep[0m  [36/42], [94mLoss[0m : 2.78855
[1mStep[0m  [40/42], [94mLoss[0m : 2.55903

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.824, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.67756
[1mStep[0m  [4/42], [94mLoss[0m : 2.83391
[1mStep[0m  [8/42], [94mLoss[0m : 2.49789
[1mStep[0m  [12/42], [94mLoss[0m : 2.75771
[1mStep[0m  [16/42], [94mLoss[0m : 2.66204
[1mStep[0m  [20/42], [94mLoss[0m : 2.70216
[1mStep[0m  [24/42], [94mLoss[0m : 2.61284
[1mStep[0m  [28/42], [94mLoss[0m : 2.78062
[1mStep[0m  [32/42], [94mLoss[0m : 2.70920
[1mStep[0m  [36/42], [94mLoss[0m : 2.76646
[1mStep[0m  [40/42], [94mLoss[0m : 2.62421

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.746, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74088
[1mStep[0m  [4/42], [94mLoss[0m : 2.58957
[1mStep[0m  [8/42], [94mLoss[0m : 2.72294
[1mStep[0m  [12/42], [94mLoss[0m : 2.51145
[1mStep[0m  [16/42], [94mLoss[0m : 2.69454
[1mStep[0m  [20/42], [94mLoss[0m : 2.43705
[1mStep[0m  [24/42], [94mLoss[0m : 2.46179
[1mStep[0m  [28/42], [94mLoss[0m : 2.63779
[1mStep[0m  [32/42], [94mLoss[0m : 2.65064
[1mStep[0m  [36/42], [94mLoss[0m : 2.67366
[1mStep[0m  [40/42], [94mLoss[0m : 2.60264

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.698, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65734
[1mStep[0m  [4/42], [94mLoss[0m : 2.73798
[1mStep[0m  [8/42], [94mLoss[0m : 2.49802
[1mStep[0m  [12/42], [94mLoss[0m : 2.54406
[1mStep[0m  [16/42], [94mLoss[0m : 2.70271
[1mStep[0m  [20/42], [94mLoss[0m : 2.82213
[1mStep[0m  [24/42], [94mLoss[0m : 2.75647
[1mStep[0m  [28/42], [94mLoss[0m : 2.62584
[1mStep[0m  [32/42], [94mLoss[0m : 2.48736
[1mStep[0m  [36/42], [94mLoss[0m : 2.33398
[1mStep[0m  [40/42], [94mLoss[0m : 2.58599

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.673, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61131
[1mStep[0m  [4/42], [94mLoss[0m : 2.77826
[1mStep[0m  [8/42], [94mLoss[0m : 2.59147
[1mStep[0m  [12/42], [94mLoss[0m : 2.58624
[1mStep[0m  [16/42], [94mLoss[0m : 2.58998
[1mStep[0m  [20/42], [94mLoss[0m : 2.85209
[1mStep[0m  [24/42], [94mLoss[0m : 2.69551
[1mStep[0m  [28/42], [94mLoss[0m : 2.60798
[1mStep[0m  [32/42], [94mLoss[0m : 2.68138
[1mStep[0m  [36/42], [94mLoss[0m : 2.50457
[1mStep[0m  [40/42], [94mLoss[0m : 2.55546

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.616, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.83655
[1mStep[0m  [4/42], [94mLoss[0m : 2.50370
[1mStep[0m  [8/42], [94mLoss[0m : 2.89104
[1mStep[0m  [12/42], [94mLoss[0m : 2.38527
[1mStep[0m  [16/42], [94mLoss[0m : 2.56971
[1mStep[0m  [20/42], [94mLoss[0m : 2.56280
[1mStep[0m  [24/42], [94mLoss[0m : 2.41065
[1mStep[0m  [28/42], [94mLoss[0m : 2.49730
[1mStep[0m  [32/42], [94mLoss[0m : 2.70530
[1mStep[0m  [36/42], [94mLoss[0m : 2.39257
[1mStep[0m  [40/42], [94mLoss[0m : 2.71855

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.620, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68453
[1mStep[0m  [4/42], [94mLoss[0m : 2.61422
[1mStep[0m  [8/42], [94mLoss[0m : 2.58311
[1mStep[0m  [12/42], [94mLoss[0m : 2.40949
[1mStep[0m  [16/42], [94mLoss[0m : 2.46499
[1mStep[0m  [20/42], [94mLoss[0m : 2.57658
[1mStep[0m  [24/42], [94mLoss[0m : 2.48448
[1mStep[0m  [28/42], [94mLoss[0m : 2.40149
[1mStep[0m  [32/42], [94mLoss[0m : 2.51971
[1mStep[0m  [36/42], [94mLoss[0m : 2.60728
[1mStep[0m  [40/42], [94mLoss[0m : 2.47909

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.83206
[1mStep[0m  [4/42], [94mLoss[0m : 2.71983
[1mStep[0m  [8/42], [94mLoss[0m : 2.53699
[1mStep[0m  [12/42], [94mLoss[0m : 2.58880
[1mStep[0m  [16/42], [94mLoss[0m : 2.63580
[1mStep[0m  [20/42], [94mLoss[0m : 2.57171
[1mStep[0m  [24/42], [94mLoss[0m : 2.59289
[1mStep[0m  [28/42], [94mLoss[0m : 2.54743
[1mStep[0m  [32/42], [94mLoss[0m : 2.48731
[1mStep[0m  [36/42], [94mLoss[0m : 2.47559
[1mStep[0m  [40/42], [94mLoss[0m : 2.67538

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.584, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74865
[1mStep[0m  [4/42], [94mLoss[0m : 2.72759
[1mStep[0m  [8/42], [94mLoss[0m : 2.67113
[1mStep[0m  [12/42], [94mLoss[0m : 2.67905
[1mStep[0m  [16/42], [94mLoss[0m : 2.63219
[1mStep[0m  [20/42], [94mLoss[0m : 2.58033
[1mStep[0m  [24/42], [94mLoss[0m : 2.44782
[1mStep[0m  [28/42], [94mLoss[0m : 2.51014
[1mStep[0m  [32/42], [94mLoss[0m : 2.67733
[1mStep[0m  [36/42], [94mLoss[0m : 2.47223
[1mStep[0m  [40/42], [94mLoss[0m : 2.60364

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62187
[1mStep[0m  [4/42], [94mLoss[0m : 2.44944
[1mStep[0m  [8/42], [94mLoss[0m : 2.50468
[1mStep[0m  [12/42], [94mLoss[0m : 2.57326
[1mStep[0m  [16/42], [94mLoss[0m : 2.62419
[1mStep[0m  [20/42], [94mLoss[0m : 2.55296
[1mStep[0m  [24/42], [94mLoss[0m : 2.71417
[1mStep[0m  [28/42], [94mLoss[0m : 2.66861
[1mStep[0m  [32/42], [94mLoss[0m : 2.44504
[1mStep[0m  [36/42], [94mLoss[0m : 2.78258
[1mStep[0m  [40/42], [94mLoss[0m : 2.70177

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68560
[1mStep[0m  [4/42], [94mLoss[0m : 2.45351
[1mStep[0m  [8/42], [94mLoss[0m : 2.30518
[1mStep[0m  [12/42], [94mLoss[0m : 2.57626
[1mStep[0m  [16/42], [94mLoss[0m : 2.47338
[1mStep[0m  [20/42], [94mLoss[0m : 2.56044
[1mStep[0m  [24/42], [94mLoss[0m : 2.72681
[1mStep[0m  [28/42], [94mLoss[0m : 2.70330
[1mStep[0m  [32/42], [94mLoss[0m : 2.60905
[1mStep[0m  [36/42], [94mLoss[0m : 2.54435
[1mStep[0m  [40/42], [94mLoss[0m : 2.51828

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.557, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48371
[1mStep[0m  [4/42], [94mLoss[0m : 2.41247
[1mStep[0m  [8/42], [94mLoss[0m : 2.74255
[1mStep[0m  [12/42], [94mLoss[0m : 2.64835
[1mStep[0m  [16/42], [94mLoss[0m : 2.66460
[1mStep[0m  [20/42], [94mLoss[0m : 2.43675
[1mStep[0m  [24/42], [94mLoss[0m : 2.52895
[1mStep[0m  [28/42], [94mLoss[0m : 2.60284
[1mStep[0m  [32/42], [94mLoss[0m : 2.75329
[1mStep[0m  [36/42], [94mLoss[0m : 2.45765
[1mStep[0m  [40/42], [94mLoss[0m : 2.54534

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52664
[1mStep[0m  [4/42], [94mLoss[0m : 2.49232
[1mStep[0m  [8/42], [94mLoss[0m : 2.38012
[1mStep[0m  [12/42], [94mLoss[0m : 2.25881
[1mStep[0m  [16/42], [94mLoss[0m : 2.72700
[1mStep[0m  [20/42], [94mLoss[0m : 2.60908
[1mStep[0m  [24/42], [94mLoss[0m : 2.46469
[1mStep[0m  [28/42], [94mLoss[0m : 2.44526
[1mStep[0m  [32/42], [94mLoss[0m : 2.47968
[1mStep[0m  [36/42], [94mLoss[0m : 2.66831
[1mStep[0m  [40/42], [94mLoss[0m : 2.71753

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50355
[1mStep[0m  [4/42], [94mLoss[0m : 2.69681
[1mStep[0m  [8/42], [94mLoss[0m : 2.40737
[1mStep[0m  [12/42], [94mLoss[0m : 2.70266
[1mStep[0m  [16/42], [94mLoss[0m : 2.36673
[1mStep[0m  [20/42], [94mLoss[0m : 2.62081
[1mStep[0m  [24/42], [94mLoss[0m : 2.55270
[1mStep[0m  [28/42], [94mLoss[0m : 2.74837
[1mStep[0m  [32/42], [94mLoss[0m : 2.51875
[1mStep[0m  [36/42], [94mLoss[0m : 2.43637
[1mStep[0m  [40/42], [94mLoss[0m : 2.43833

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61139
[1mStep[0m  [4/42], [94mLoss[0m : 2.46160
[1mStep[0m  [8/42], [94mLoss[0m : 2.62179
[1mStep[0m  [12/42], [94mLoss[0m : 2.51976
[1mStep[0m  [16/42], [94mLoss[0m : 2.68361
[1mStep[0m  [20/42], [94mLoss[0m : 2.66569
[1mStep[0m  [24/42], [94mLoss[0m : 2.48254
[1mStep[0m  [28/42], [94mLoss[0m : 2.37406
[1mStep[0m  [32/42], [94mLoss[0m : 2.68723
[1mStep[0m  [36/42], [94mLoss[0m : 2.61960
[1mStep[0m  [40/42], [94mLoss[0m : 2.46227

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48070
[1mStep[0m  [4/42], [94mLoss[0m : 2.54757
[1mStep[0m  [8/42], [94mLoss[0m : 2.33599
[1mStep[0m  [12/42], [94mLoss[0m : 2.59214
[1mStep[0m  [16/42], [94mLoss[0m : 2.55236
[1mStep[0m  [20/42], [94mLoss[0m : 2.66152
[1mStep[0m  [24/42], [94mLoss[0m : 2.45066
[1mStep[0m  [28/42], [94mLoss[0m : 2.52689
[1mStep[0m  [32/42], [94mLoss[0m : 2.47818
[1mStep[0m  [36/42], [94mLoss[0m : 2.71910
[1mStep[0m  [40/42], [94mLoss[0m : 2.64145

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59469
[1mStep[0m  [4/42], [94mLoss[0m : 2.50291
[1mStep[0m  [8/42], [94mLoss[0m : 2.31976
[1mStep[0m  [12/42], [94mLoss[0m : 2.40934
[1mStep[0m  [16/42], [94mLoss[0m : 2.66435
[1mStep[0m  [20/42], [94mLoss[0m : 2.44178
[1mStep[0m  [24/42], [94mLoss[0m : 2.48998
[1mStep[0m  [28/42], [94mLoss[0m : 2.98823
[1mStep[0m  [32/42], [94mLoss[0m : 2.51502
[1mStep[0m  [36/42], [94mLoss[0m : 2.45393
[1mStep[0m  [40/42], [94mLoss[0m : 2.51557

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58062
[1mStep[0m  [4/42], [94mLoss[0m : 2.59226
[1mStep[0m  [8/42], [94mLoss[0m : 2.38098
[1mStep[0m  [12/42], [94mLoss[0m : 2.47351
[1mStep[0m  [16/42], [94mLoss[0m : 2.46875
[1mStep[0m  [20/42], [94mLoss[0m : 2.82279
[1mStep[0m  [24/42], [94mLoss[0m : 2.51536
[1mStep[0m  [28/42], [94mLoss[0m : 2.44565
[1mStep[0m  [32/42], [94mLoss[0m : 2.28627
[1mStep[0m  [36/42], [94mLoss[0m : 2.48618
[1mStep[0m  [40/42], [94mLoss[0m : 2.14718

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56935
[1mStep[0m  [4/42], [94mLoss[0m : 2.42059
[1mStep[0m  [8/42], [94mLoss[0m : 2.45174
[1mStep[0m  [12/42], [94mLoss[0m : 2.72582
[1mStep[0m  [16/42], [94mLoss[0m : 2.56359
[1mStep[0m  [20/42], [94mLoss[0m : 2.48650
[1mStep[0m  [24/42], [94mLoss[0m : 2.51945
[1mStep[0m  [28/42], [94mLoss[0m : 2.62412
[1mStep[0m  [32/42], [94mLoss[0m : 2.55934
[1mStep[0m  [36/42], [94mLoss[0m : 2.40340
[1mStep[0m  [40/42], [94mLoss[0m : 2.57905

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77607
[1mStep[0m  [4/42], [94mLoss[0m : 2.52151
[1mStep[0m  [8/42], [94mLoss[0m : 2.53155
[1mStep[0m  [12/42], [94mLoss[0m : 2.33269
[1mStep[0m  [16/42], [94mLoss[0m : 2.59386
[1mStep[0m  [20/42], [94mLoss[0m : 2.51274
[1mStep[0m  [24/42], [94mLoss[0m : 2.68034
[1mStep[0m  [28/42], [94mLoss[0m : 2.69775
[1mStep[0m  [32/42], [94mLoss[0m : 2.24673
[1mStep[0m  [36/42], [94mLoss[0m : 2.40882
[1mStep[0m  [40/42], [94mLoss[0m : 2.77409

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56207
[1mStep[0m  [4/42], [94mLoss[0m : 2.61218
[1mStep[0m  [8/42], [94mLoss[0m : 2.63146
[1mStep[0m  [12/42], [94mLoss[0m : 2.53746
[1mStep[0m  [16/42], [94mLoss[0m : 2.45154
[1mStep[0m  [20/42], [94mLoss[0m : 2.66714
[1mStep[0m  [24/42], [94mLoss[0m : 2.39442
[1mStep[0m  [28/42], [94mLoss[0m : 2.61599
[1mStep[0m  [32/42], [94mLoss[0m : 2.58869
[1mStep[0m  [36/42], [94mLoss[0m : 2.65166
[1mStep[0m  [40/42], [94mLoss[0m : 2.50284

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52607
[1mStep[0m  [4/42], [94mLoss[0m : 2.32972
[1mStep[0m  [8/42], [94mLoss[0m : 2.34544
[1mStep[0m  [12/42], [94mLoss[0m : 2.49604
[1mStep[0m  [16/42], [94mLoss[0m : 2.56062
[1mStep[0m  [20/42], [94mLoss[0m : 2.49959
[1mStep[0m  [24/42], [94mLoss[0m : 2.65191
[1mStep[0m  [28/42], [94mLoss[0m : 2.42169
[1mStep[0m  [32/42], [94mLoss[0m : 2.77415
[1mStep[0m  [36/42], [94mLoss[0m : 2.52123
[1mStep[0m  [40/42], [94mLoss[0m : 2.28798

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71083
[1mStep[0m  [4/42], [94mLoss[0m : 2.75336
[1mStep[0m  [8/42], [94mLoss[0m : 2.57368
[1mStep[0m  [12/42], [94mLoss[0m : 2.57968
[1mStep[0m  [16/42], [94mLoss[0m : 2.44831
[1mStep[0m  [20/42], [94mLoss[0m : 2.42344
[1mStep[0m  [24/42], [94mLoss[0m : 2.46825
[1mStep[0m  [28/42], [94mLoss[0m : 2.47450
[1mStep[0m  [32/42], [94mLoss[0m : 2.62033
[1mStep[0m  [36/42], [94mLoss[0m : 2.39205
[1mStep[0m  [40/42], [94mLoss[0m : 2.52065

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63264
[1mStep[0m  [4/42], [94mLoss[0m : 2.62230
[1mStep[0m  [8/42], [94mLoss[0m : 2.52953
[1mStep[0m  [12/42], [94mLoss[0m : 2.45430
[1mStep[0m  [16/42], [94mLoss[0m : 2.39208
[1mStep[0m  [20/42], [94mLoss[0m : 2.53005
[1mStep[0m  [24/42], [94mLoss[0m : 2.37902
[1mStep[0m  [28/42], [94mLoss[0m : 2.21581
[1mStep[0m  [32/42], [94mLoss[0m : 2.38898
[1mStep[0m  [36/42], [94mLoss[0m : 2.34473
[1mStep[0m  [40/42], [94mLoss[0m : 2.58387

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55797
[1mStep[0m  [4/42], [94mLoss[0m : 2.50733
[1mStep[0m  [8/42], [94mLoss[0m : 2.34425
[1mStep[0m  [12/42], [94mLoss[0m : 2.58436
[1mStep[0m  [16/42], [94mLoss[0m : 2.38569
[1mStep[0m  [20/42], [94mLoss[0m : 2.58986
[1mStep[0m  [24/42], [94mLoss[0m : 2.45451
[1mStep[0m  [28/42], [94mLoss[0m : 2.52967
[1mStep[0m  [32/42], [94mLoss[0m : 2.40799
[1mStep[0m  [36/42], [94mLoss[0m : 2.54705
[1mStep[0m  [40/42], [94mLoss[0m : 2.34044

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.436
====================================

Phase 1 - Evaluation MAE:  2.4361869607652937
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.49437
[1mStep[0m  [4/42], [94mLoss[0m : 2.60379
[1mStep[0m  [8/42], [94mLoss[0m : 2.69557
[1mStep[0m  [12/42], [94mLoss[0m : 2.48715
[1mStep[0m  [16/42], [94mLoss[0m : 2.68208
[1mStep[0m  [20/42], [94mLoss[0m : 2.64367
[1mStep[0m  [24/42], [94mLoss[0m : 2.59145
[1mStep[0m  [28/42], [94mLoss[0m : 2.51292
[1mStep[0m  [32/42], [94mLoss[0m : 2.60735
[1mStep[0m  [36/42], [94mLoss[0m : 2.65573
[1mStep[0m  [40/42], [94mLoss[0m : 2.57430

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44920
[1mStep[0m  [4/42], [94mLoss[0m : 2.77179
[1mStep[0m  [8/42], [94mLoss[0m : 2.64674
[1mStep[0m  [12/42], [94mLoss[0m : 2.59094
[1mStep[0m  [16/42], [94mLoss[0m : 2.75425
[1mStep[0m  [20/42], [94mLoss[0m : 2.54797
[1mStep[0m  [24/42], [94mLoss[0m : 2.66361
[1mStep[0m  [28/42], [94mLoss[0m : 2.60245
[1mStep[0m  [32/42], [94mLoss[0m : 2.59636
[1mStep[0m  [36/42], [94mLoss[0m : 2.69906
[1mStep[0m  [40/42], [94mLoss[0m : 2.54611

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46072
[1mStep[0m  [4/42], [94mLoss[0m : 2.50723
[1mStep[0m  [8/42], [94mLoss[0m : 2.50457
[1mStep[0m  [12/42], [94mLoss[0m : 2.71046
[1mStep[0m  [16/42], [94mLoss[0m : 2.45912
[1mStep[0m  [20/42], [94mLoss[0m : 2.54719
[1mStep[0m  [24/42], [94mLoss[0m : 2.59478
[1mStep[0m  [28/42], [94mLoss[0m : 2.67316
[1mStep[0m  [32/42], [94mLoss[0m : 2.45733
[1mStep[0m  [36/42], [94mLoss[0m : 2.47711
[1mStep[0m  [40/42], [94mLoss[0m : 2.69734

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59181
[1mStep[0m  [4/42], [94mLoss[0m : 2.30121
[1mStep[0m  [8/42], [94mLoss[0m : 2.49546
[1mStep[0m  [12/42], [94mLoss[0m : 2.58142
[1mStep[0m  [16/42], [94mLoss[0m : 2.72605
[1mStep[0m  [20/42], [94mLoss[0m : 2.29068
[1mStep[0m  [24/42], [94mLoss[0m : 2.48085
[1mStep[0m  [28/42], [94mLoss[0m : 2.65035
[1mStep[0m  [32/42], [94mLoss[0m : 2.50689
[1mStep[0m  [36/42], [94mLoss[0m : 2.41673
[1mStep[0m  [40/42], [94mLoss[0m : 2.36925

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59963
[1mStep[0m  [4/42], [94mLoss[0m : 2.70967
[1mStep[0m  [8/42], [94mLoss[0m : 2.48649
[1mStep[0m  [12/42], [94mLoss[0m : 2.73646
[1mStep[0m  [16/42], [94mLoss[0m : 2.47694
[1mStep[0m  [20/42], [94mLoss[0m : 2.49475
[1mStep[0m  [24/42], [94mLoss[0m : 2.38059
[1mStep[0m  [28/42], [94mLoss[0m : 2.38882
[1mStep[0m  [32/42], [94mLoss[0m : 2.32175
[1mStep[0m  [36/42], [94mLoss[0m : 2.39637
[1mStep[0m  [40/42], [94mLoss[0m : 2.37320

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28335
[1mStep[0m  [4/42], [94mLoss[0m : 2.36301
[1mStep[0m  [8/42], [94mLoss[0m : 2.64648
[1mStep[0m  [12/42], [94mLoss[0m : 2.56972
[1mStep[0m  [16/42], [94mLoss[0m : 2.42482
[1mStep[0m  [20/42], [94mLoss[0m : 2.51506
[1mStep[0m  [24/42], [94mLoss[0m : 2.62214
[1mStep[0m  [28/42], [94mLoss[0m : 2.49738
[1mStep[0m  [32/42], [94mLoss[0m : 2.54407
[1mStep[0m  [36/42], [94mLoss[0m : 2.56521
[1mStep[0m  [40/42], [94mLoss[0m : 2.53848

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33108
[1mStep[0m  [4/42], [94mLoss[0m : 2.43054
[1mStep[0m  [8/42], [94mLoss[0m : 2.47367
[1mStep[0m  [12/42], [94mLoss[0m : 2.59451
[1mStep[0m  [16/42], [94mLoss[0m : 2.49871
[1mStep[0m  [20/42], [94mLoss[0m : 2.46133
[1mStep[0m  [24/42], [94mLoss[0m : 2.33823
[1mStep[0m  [28/42], [94mLoss[0m : 2.52849
[1mStep[0m  [32/42], [94mLoss[0m : 2.64160
[1mStep[0m  [36/42], [94mLoss[0m : 2.49125
[1mStep[0m  [40/42], [94mLoss[0m : 2.57035

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57870
[1mStep[0m  [4/42], [94mLoss[0m : 2.51803
[1mStep[0m  [8/42], [94mLoss[0m : 2.50852
[1mStep[0m  [12/42], [94mLoss[0m : 2.71217
[1mStep[0m  [16/42], [94mLoss[0m : 2.45449
[1mStep[0m  [20/42], [94mLoss[0m : 2.67036
[1mStep[0m  [24/42], [94mLoss[0m : 2.44811
[1mStep[0m  [28/42], [94mLoss[0m : 2.54004
[1mStep[0m  [32/42], [94mLoss[0m : 2.41450
[1mStep[0m  [36/42], [94mLoss[0m : 2.51567
[1mStep[0m  [40/42], [94mLoss[0m : 2.47297

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57604
[1mStep[0m  [4/42], [94mLoss[0m : 2.47105
[1mStep[0m  [8/42], [94mLoss[0m : 2.31555
[1mStep[0m  [12/42], [94mLoss[0m : 2.44829
[1mStep[0m  [16/42], [94mLoss[0m : 2.41882
[1mStep[0m  [20/42], [94mLoss[0m : 2.51516
[1mStep[0m  [24/42], [94mLoss[0m : 2.43739
[1mStep[0m  [28/42], [94mLoss[0m : 2.40774
[1mStep[0m  [32/42], [94mLoss[0m : 2.49196
[1mStep[0m  [36/42], [94mLoss[0m : 2.52863
[1mStep[0m  [40/42], [94mLoss[0m : 2.36749

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42197
[1mStep[0m  [4/42], [94mLoss[0m : 2.62252
[1mStep[0m  [8/42], [94mLoss[0m : 2.31515
[1mStep[0m  [12/42], [94mLoss[0m : 2.44440
[1mStep[0m  [16/42], [94mLoss[0m : 2.65538
[1mStep[0m  [20/42], [94mLoss[0m : 2.33259
[1mStep[0m  [24/42], [94mLoss[0m : 2.56393
[1mStep[0m  [28/42], [94mLoss[0m : 2.28710
[1mStep[0m  [32/42], [94mLoss[0m : 2.36035
[1mStep[0m  [36/42], [94mLoss[0m : 2.30169
[1mStep[0m  [40/42], [94mLoss[0m : 2.53554

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47809
[1mStep[0m  [4/42], [94mLoss[0m : 2.14023
[1mStep[0m  [8/42], [94mLoss[0m : 2.42915
[1mStep[0m  [12/42], [94mLoss[0m : 2.19627
[1mStep[0m  [16/42], [94mLoss[0m : 2.64873
[1mStep[0m  [20/42], [94mLoss[0m : 2.36676
[1mStep[0m  [24/42], [94mLoss[0m : 2.52992
[1mStep[0m  [28/42], [94mLoss[0m : 2.39377
[1mStep[0m  [32/42], [94mLoss[0m : 2.24807
[1mStep[0m  [36/42], [94mLoss[0m : 2.54728
[1mStep[0m  [40/42], [94mLoss[0m : 2.33415

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31583
[1mStep[0m  [4/42], [94mLoss[0m : 2.62762
[1mStep[0m  [8/42], [94mLoss[0m : 2.18516
[1mStep[0m  [12/42], [94mLoss[0m : 2.13454
[1mStep[0m  [16/42], [94mLoss[0m : 2.45010
[1mStep[0m  [20/42], [94mLoss[0m : 2.24255
[1mStep[0m  [24/42], [94mLoss[0m : 2.38329
[1mStep[0m  [28/42], [94mLoss[0m : 2.35243
[1mStep[0m  [32/42], [94mLoss[0m : 2.31725
[1mStep[0m  [36/42], [94mLoss[0m : 2.37781
[1mStep[0m  [40/42], [94mLoss[0m : 2.45455

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35117
[1mStep[0m  [4/42], [94mLoss[0m : 2.27800
[1mStep[0m  [8/42], [94mLoss[0m : 2.21501
[1mStep[0m  [12/42], [94mLoss[0m : 2.30874
[1mStep[0m  [16/42], [94mLoss[0m : 2.46959
[1mStep[0m  [20/42], [94mLoss[0m : 2.41511
[1mStep[0m  [24/42], [94mLoss[0m : 2.34334
[1mStep[0m  [28/42], [94mLoss[0m : 2.04315
[1mStep[0m  [32/42], [94mLoss[0m : 2.54211
[1mStep[0m  [36/42], [94mLoss[0m : 2.40151
[1mStep[0m  [40/42], [94mLoss[0m : 2.29398

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31926
[1mStep[0m  [4/42], [94mLoss[0m : 2.21872
[1mStep[0m  [8/42], [94mLoss[0m : 2.18865
[1mStep[0m  [12/42], [94mLoss[0m : 2.27020
[1mStep[0m  [16/42], [94mLoss[0m : 2.21837
[1mStep[0m  [20/42], [94mLoss[0m : 2.28709
[1mStep[0m  [24/42], [94mLoss[0m : 2.34327
[1mStep[0m  [28/42], [94mLoss[0m : 2.33061
[1mStep[0m  [32/42], [94mLoss[0m : 2.46337
[1mStep[0m  [36/42], [94mLoss[0m : 2.37723
[1mStep[0m  [40/42], [94mLoss[0m : 2.37296

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36589
[1mStep[0m  [4/42], [94mLoss[0m : 2.35277
[1mStep[0m  [8/42], [94mLoss[0m : 2.14558
[1mStep[0m  [12/42], [94mLoss[0m : 2.00142
[1mStep[0m  [16/42], [94mLoss[0m : 2.29339
[1mStep[0m  [20/42], [94mLoss[0m : 2.30717
[1mStep[0m  [24/42], [94mLoss[0m : 2.12965
[1mStep[0m  [28/42], [94mLoss[0m : 2.33214
[1mStep[0m  [32/42], [94mLoss[0m : 2.23287
[1mStep[0m  [36/42], [94mLoss[0m : 2.65920
[1mStep[0m  [40/42], [94mLoss[0m : 2.15741

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.12144
[1mStep[0m  [4/42], [94mLoss[0m : 2.24537
[1mStep[0m  [8/42], [94mLoss[0m : 2.26288
[1mStep[0m  [12/42], [94mLoss[0m : 2.31051
[1mStep[0m  [16/42], [94mLoss[0m : 2.23735
[1mStep[0m  [20/42], [94mLoss[0m : 2.48044
[1mStep[0m  [24/42], [94mLoss[0m : 2.38266
[1mStep[0m  [28/42], [94mLoss[0m : 2.03894
[1mStep[0m  [32/42], [94mLoss[0m : 2.38909
[1mStep[0m  [36/42], [94mLoss[0m : 2.15290
[1mStep[0m  [40/42], [94mLoss[0m : 2.25030

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21816
[1mStep[0m  [4/42], [94mLoss[0m : 2.22456
[1mStep[0m  [8/42], [94mLoss[0m : 2.30671
[1mStep[0m  [12/42], [94mLoss[0m : 2.48189
[1mStep[0m  [16/42], [94mLoss[0m : 2.31226
[1mStep[0m  [20/42], [94mLoss[0m : 2.18229
[1mStep[0m  [24/42], [94mLoss[0m : 2.41156
[1mStep[0m  [28/42], [94mLoss[0m : 2.25981
[1mStep[0m  [32/42], [94mLoss[0m : 2.25464
[1mStep[0m  [36/42], [94mLoss[0m : 2.41410
[1mStep[0m  [40/42], [94mLoss[0m : 2.24500

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40227
[1mStep[0m  [4/42], [94mLoss[0m : 2.08081
[1mStep[0m  [8/42], [94mLoss[0m : 2.27732
[1mStep[0m  [12/42], [94mLoss[0m : 2.27723
[1mStep[0m  [16/42], [94mLoss[0m : 2.21219
[1mStep[0m  [20/42], [94mLoss[0m : 2.18978
[1mStep[0m  [24/42], [94mLoss[0m : 2.20053
[1mStep[0m  [28/42], [94mLoss[0m : 2.02544
[1mStep[0m  [32/42], [94mLoss[0m : 2.26478
[1mStep[0m  [36/42], [94mLoss[0m : 2.34527
[1mStep[0m  [40/42], [94mLoss[0m : 2.22711

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23939
[1mStep[0m  [4/42], [94mLoss[0m : 2.13483
[1mStep[0m  [8/42], [94mLoss[0m : 2.30167
[1mStep[0m  [12/42], [94mLoss[0m : 2.19518
[1mStep[0m  [16/42], [94mLoss[0m : 2.31025
[1mStep[0m  [20/42], [94mLoss[0m : 2.25703
[1mStep[0m  [24/42], [94mLoss[0m : 2.34731
[1mStep[0m  [28/42], [94mLoss[0m : 2.10287
[1mStep[0m  [32/42], [94mLoss[0m : 2.34056
[1mStep[0m  [36/42], [94mLoss[0m : 2.31597
[1mStep[0m  [40/42], [94mLoss[0m : 2.32724

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17419
[1mStep[0m  [4/42], [94mLoss[0m : 2.36940
[1mStep[0m  [8/42], [94mLoss[0m : 2.15162
[1mStep[0m  [12/42], [94mLoss[0m : 2.16318
[1mStep[0m  [16/42], [94mLoss[0m : 2.32754
[1mStep[0m  [20/42], [94mLoss[0m : 2.14942
[1mStep[0m  [24/42], [94mLoss[0m : 2.13076
[1mStep[0m  [28/42], [94mLoss[0m : 2.31078
[1mStep[0m  [32/42], [94mLoss[0m : 2.26271
[1mStep[0m  [36/42], [94mLoss[0m : 2.11106
[1mStep[0m  [40/42], [94mLoss[0m : 2.32473

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04140
[1mStep[0m  [4/42], [94mLoss[0m : 2.39030
[1mStep[0m  [8/42], [94mLoss[0m : 2.11247
[1mStep[0m  [12/42], [94mLoss[0m : 2.30142
[1mStep[0m  [16/42], [94mLoss[0m : 2.21096
[1mStep[0m  [20/42], [94mLoss[0m : 2.19263
[1mStep[0m  [24/42], [94mLoss[0m : 2.12975
[1mStep[0m  [28/42], [94mLoss[0m : 2.10276
[1mStep[0m  [32/42], [94mLoss[0m : 2.27046
[1mStep[0m  [36/42], [94mLoss[0m : 2.16352
[1mStep[0m  [40/42], [94mLoss[0m : 2.33435

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24079
[1mStep[0m  [4/42], [94mLoss[0m : 2.36196
[1mStep[0m  [8/42], [94mLoss[0m : 2.21814
[1mStep[0m  [12/42], [94mLoss[0m : 2.12712
[1mStep[0m  [16/42], [94mLoss[0m : 2.19523
[1mStep[0m  [20/42], [94mLoss[0m : 2.02198
[1mStep[0m  [24/42], [94mLoss[0m : 2.09735
[1mStep[0m  [28/42], [94mLoss[0m : 2.12381
[1mStep[0m  [32/42], [94mLoss[0m : 2.20813
[1mStep[0m  [36/42], [94mLoss[0m : 2.35845
[1mStep[0m  [40/42], [94mLoss[0m : 2.16532

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.184, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92693
[1mStep[0m  [4/42], [94mLoss[0m : 2.31558
[1mStep[0m  [8/42], [94mLoss[0m : 2.19444
[1mStep[0m  [12/42], [94mLoss[0m : 2.06369
[1mStep[0m  [16/42], [94mLoss[0m : 2.09820
[1mStep[0m  [20/42], [94mLoss[0m : 2.30828
[1mStep[0m  [24/42], [94mLoss[0m : 2.09564
[1mStep[0m  [28/42], [94mLoss[0m : 2.17924
[1mStep[0m  [32/42], [94mLoss[0m : 2.02057
[1mStep[0m  [36/42], [94mLoss[0m : 2.31221
[1mStep[0m  [40/42], [94mLoss[0m : 2.02890

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.527, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17469
[1mStep[0m  [4/42], [94mLoss[0m : 2.12074
[1mStep[0m  [8/42], [94mLoss[0m : 2.08021
[1mStep[0m  [12/42], [94mLoss[0m : 1.91086
[1mStep[0m  [16/42], [94mLoss[0m : 2.34545
[1mStep[0m  [20/42], [94mLoss[0m : 2.01537
[1mStep[0m  [24/42], [94mLoss[0m : 2.02550
[1mStep[0m  [28/42], [94mLoss[0m : 2.13822
[1mStep[0m  [32/42], [94mLoss[0m : 2.19558
[1mStep[0m  [36/42], [94mLoss[0m : 2.14349
[1mStep[0m  [40/42], [94mLoss[0m : 2.17887

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.548, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92950
[1mStep[0m  [4/42], [94mLoss[0m : 2.10092
[1mStep[0m  [8/42], [94mLoss[0m : 2.08898
[1mStep[0m  [12/42], [94mLoss[0m : 2.28668
[1mStep[0m  [16/42], [94mLoss[0m : 2.33343
[1mStep[0m  [20/42], [94mLoss[0m : 2.13189
[1mStep[0m  [24/42], [94mLoss[0m : 2.33413
[1mStep[0m  [28/42], [94mLoss[0m : 2.16072
[1mStep[0m  [32/42], [94mLoss[0m : 2.09431
[1mStep[0m  [36/42], [94mLoss[0m : 2.08866
[1mStep[0m  [40/42], [94mLoss[0m : 2.19831

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.567, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06277
[1mStep[0m  [4/42], [94mLoss[0m : 2.01421
[1mStep[0m  [8/42], [94mLoss[0m : 2.09808
[1mStep[0m  [12/42], [94mLoss[0m : 2.06260
[1mStep[0m  [16/42], [94mLoss[0m : 2.11573
[1mStep[0m  [20/42], [94mLoss[0m : 2.24224
[1mStep[0m  [24/42], [94mLoss[0m : 2.05548
[1mStep[0m  [28/42], [94mLoss[0m : 2.07051
[1mStep[0m  [32/42], [94mLoss[0m : 2.02342
[1mStep[0m  [36/42], [94mLoss[0m : 2.08141
[1mStep[0m  [40/42], [94mLoss[0m : 2.13293

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.527, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18837
[1mStep[0m  [4/42], [94mLoss[0m : 1.96779
[1mStep[0m  [8/42], [94mLoss[0m : 2.18754
[1mStep[0m  [12/42], [94mLoss[0m : 2.11517
[1mStep[0m  [16/42], [94mLoss[0m : 2.19838
[1mStep[0m  [20/42], [94mLoss[0m : 2.04767
[1mStep[0m  [24/42], [94mLoss[0m : 2.10218
[1mStep[0m  [28/42], [94mLoss[0m : 2.07409
[1mStep[0m  [32/42], [94mLoss[0m : 1.97326
[1mStep[0m  [36/42], [94mLoss[0m : 2.04202
[1mStep[0m  [40/42], [94mLoss[0m : 2.08292

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.098, [92mTest[0m: 2.559, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.03047
[1mStep[0m  [4/42], [94mLoss[0m : 2.12974
[1mStep[0m  [8/42], [94mLoss[0m : 1.96202
[1mStep[0m  [12/42], [94mLoss[0m : 2.07433
[1mStep[0m  [16/42], [94mLoss[0m : 2.16057
[1mStep[0m  [20/42], [94mLoss[0m : 2.09548
[1mStep[0m  [24/42], [94mLoss[0m : 1.99533
[1mStep[0m  [28/42], [94mLoss[0m : 2.25206
[1mStep[0m  [32/42], [94mLoss[0m : 2.24982
[1mStep[0m  [36/42], [94mLoss[0m : 2.13643
[1mStep[0m  [40/42], [94mLoss[0m : 1.96441

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.587, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.06730
[1mStep[0m  [4/42], [94mLoss[0m : 2.16999
[1mStep[0m  [8/42], [94mLoss[0m : 1.73567
[1mStep[0m  [12/42], [94mLoss[0m : 2.05319
[1mStep[0m  [16/42], [94mLoss[0m : 1.81724
[1mStep[0m  [20/42], [94mLoss[0m : 2.04704
[1mStep[0m  [24/42], [94mLoss[0m : 2.14888
[1mStep[0m  [28/42], [94mLoss[0m : 1.90899
[1mStep[0m  [32/42], [94mLoss[0m : 1.94946
[1mStep[0m  [36/42], [94mLoss[0m : 2.01059
[1mStep[0m  [40/42], [94mLoss[0m : 1.88385

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.527, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.92754
[1mStep[0m  [4/42], [94mLoss[0m : 2.07300
[1mStep[0m  [8/42], [94mLoss[0m : 2.10966
[1mStep[0m  [12/42], [94mLoss[0m : 1.94513
[1mStep[0m  [16/42], [94mLoss[0m : 2.05377
[1mStep[0m  [20/42], [94mLoss[0m : 1.98558
[1mStep[0m  [24/42], [94mLoss[0m : 2.01653
[1mStep[0m  [28/42], [94mLoss[0m : 2.19484
[1mStep[0m  [32/42], [94mLoss[0m : 2.11343
[1mStep[0m  [36/42], [94mLoss[0m : 2.15277
[1mStep[0m  [40/42], [94mLoss[0m : 1.96902

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.524, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.577
====================================

Phase 2 - Evaluation MAE:  2.5765216520854404
MAE score P1        2.436187
MAE score P2        2.576522
loss                2.032981
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay          0.0001
Name: 24, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.91950
[1mStep[0m  [8/84], [94mLoss[0m : 7.53355
[1mStep[0m  [16/84], [94mLoss[0m : 2.91438
[1mStep[0m  [24/84], [94mLoss[0m : 3.40712
[1mStep[0m  [32/84], [94mLoss[0m : 2.78448
[1mStep[0m  [40/84], [94mLoss[0m : 2.87644
[1mStep[0m  [48/84], [94mLoss[0m : 2.81818
[1mStep[0m  [56/84], [94mLoss[0m : 2.72733
[1mStep[0m  [64/84], [94mLoss[0m : 2.66626
[1mStep[0m  [72/84], [94mLoss[0m : 2.63948
[1mStep[0m  [80/84], [94mLoss[0m : 2.52834

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.604, [92mTest[0m: 11.127, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58837
[1mStep[0m  [8/84], [94mLoss[0m : 2.78660
[1mStep[0m  [16/84], [94mLoss[0m : 2.49253
[1mStep[0m  [24/84], [94mLoss[0m : 2.64290
[1mStep[0m  [32/84], [94mLoss[0m : 2.67446
[1mStep[0m  [40/84], [94mLoss[0m : 2.41914
[1mStep[0m  [48/84], [94mLoss[0m : 2.52332
[1mStep[0m  [56/84], [94mLoss[0m : 2.43734
[1mStep[0m  [64/84], [94mLoss[0m : 2.75200
[1mStep[0m  [72/84], [94mLoss[0m : 2.39295
[1mStep[0m  [80/84], [94mLoss[0m : 2.48591

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38585
[1mStep[0m  [8/84], [94mLoss[0m : 2.64155
[1mStep[0m  [16/84], [94mLoss[0m : 2.54183
[1mStep[0m  [24/84], [94mLoss[0m : 2.80904
[1mStep[0m  [32/84], [94mLoss[0m : 2.41659
[1mStep[0m  [40/84], [94mLoss[0m : 2.39842
[1mStep[0m  [48/84], [94mLoss[0m : 2.57511
[1mStep[0m  [56/84], [94mLoss[0m : 2.40257
[1mStep[0m  [64/84], [94mLoss[0m : 2.54981
[1mStep[0m  [72/84], [94mLoss[0m : 2.51627
[1mStep[0m  [80/84], [94mLoss[0m : 2.35786

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20910
[1mStep[0m  [8/84], [94mLoss[0m : 2.25241
[1mStep[0m  [16/84], [94mLoss[0m : 2.74566
[1mStep[0m  [24/84], [94mLoss[0m : 2.52412
[1mStep[0m  [32/84], [94mLoss[0m : 2.33123
[1mStep[0m  [40/84], [94mLoss[0m : 2.72813
[1mStep[0m  [48/84], [94mLoss[0m : 2.45108
[1mStep[0m  [56/84], [94mLoss[0m : 2.42496
[1mStep[0m  [64/84], [94mLoss[0m : 2.65493
[1mStep[0m  [72/84], [94mLoss[0m : 2.28974
[1mStep[0m  [80/84], [94mLoss[0m : 2.71515

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50645
[1mStep[0m  [8/84], [94mLoss[0m : 2.58474
[1mStep[0m  [16/84], [94mLoss[0m : 2.24387
[1mStep[0m  [24/84], [94mLoss[0m : 2.44964
[1mStep[0m  [32/84], [94mLoss[0m : 2.49003
[1mStep[0m  [40/84], [94mLoss[0m : 2.51282
[1mStep[0m  [48/84], [94mLoss[0m : 2.55108
[1mStep[0m  [56/84], [94mLoss[0m : 2.49452
[1mStep[0m  [64/84], [94mLoss[0m : 2.43214
[1mStep[0m  [72/84], [94mLoss[0m : 2.25290
[1mStep[0m  [80/84], [94mLoss[0m : 2.41808

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39368
[1mStep[0m  [8/84], [94mLoss[0m : 2.48131
[1mStep[0m  [16/84], [94mLoss[0m : 2.40523
[1mStep[0m  [24/84], [94mLoss[0m : 2.68954
[1mStep[0m  [32/84], [94mLoss[0m : 2.79147
[1mStep[0m  [40/84], [94mLoss[0m : 2.28529
[1mStep[0m  [48/84], [94mLoss[0m : 2.52659
[1mStep[0m  [56/84], [94mLoss[0m : 2.45833
[1mStep[0m  [64/84], [94mLoss[0m : 2.42714
[1mStep[0m  [72/84], [94mLoss[0m : 2.30927
[1mStep[0m  [80/84], [94mLoss[0m : 2.62800

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44838
[1mStep[0m  [8/84], [94mLoss[0m : 2.29790
[1mStep[0m  [16/84], [94mLoss[0m : 2.71092
[1mStep[0m  [24/84], [94mLoss[0m : 2.67677
[1mStep[0m  [32/84], [94mLoss[0m : 2.12188
[1mStep[0m  [40/84], [94mLoss[0m : 2.80122
[1mStep[0m  [48/84], [94mLoss[0m : 2.07685
[1mStep[0m  [56/84], [94mLoss[0m : 2.53148
[1mStep[0m  [64/84], [94mLoss[0m : 2.41842
[1mStep[0m  [72/84], [94mLoss[0m : 2.56100
[1mStep[0m  [80/84], [94mLoss[0m : 2.24652

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46389
[1mStep[0m  [8/84], [94mLoss[0m : 2.33242
[1mStep[0m  [16/84], [94mLoss[0m : 2.49771
[1mStep[0m  [24/84], [94mLoss[0m : 2.63436
[1mStep[0m  [32/84], [94mLoss[0m : 2.20190
[1mStep[0m  [40/84], [94mLoss[0m : 2.30282
[1mStep[0m  [48/84], [94mLoss[0m : 2.53117
[1mStep[0m  [56/84], [94mLoss[0m : 2.28704
[1mStep[0m  [64/84], [94mLoss[0m : 2.26022
[1mStep[0m  [72/84], [94mLoss[0m : 2.63823
[1mStep[0m  [80/84], [94mLoss[0m : 2.61350

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32026
[1mStep[0m  [8/84], [94mLoss[0m : 2.45954
[1mStep[0m  [16/84], [94mLoss[0m : 2.58374
[1mStep[0m  [24/84], [94mLoss[0m : 2.55047
[1mStep[0m  [32/84], [94mLoss[0m : 2.20966
[1mStep[0m  [40/84], [94mLoss[0m : 2.27674
[1mStep[0m  [48/84], [94mLoss[0m : 2.58991
[1mStep[0m  [56/84], [94mLoss[0m : 2.68592
[1mStep[0m  [64/84], [94mLoss[0m : 2.38627
[1mStep[0m  [72/84], [94mLoss[0m : 2.31874
[1mStep[0m  [80/84], [94mLoss[0m : 2.49224

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43217
[1mStep[0m  [8/84], [94mLoss[0m : 2.45607
[1mStep[0m  [16/84], [94mLoss[0m : 2.49003
[1mStep[0m  [24/84], [94mLoss[0m : 2.54324
[1mStep[0m  [32/84], [94mLoss[0m : 2.22417
[1mStep[0m  [40/84], [94mLoss[0m : 2.38790
[1mStep[0m  [48/84], [94mLoss[0m : 2.46816
[1mStep[0m  [56/84], [94mLoss[0m : 2.75848
[1mStep[0m  [64/84], [94mLoss[0m : 2.31712
[1mStep[0m  [72/84], [94mLoss[0m : 2.51876
[1mStep[0m  [80/84], [94mLoss[0m : 2.44099

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32135
[1mStep[0m  [8/84], [94mLoss[0m : 2.83790
[1mStep[0m  [16/84], [94mLoss[0m : 2.34639
[1mStep[0m  [24/84], [94mLoss[0m : 2.58152
[1mStep[0m  [32/84], [94mLoss[0m : 2.67758
[1mStep[0m  [40/84], [94mLoss[0m : 2.51315
[1mStep[0m  [48/84], [94mLoss[0m : 2.43460
[1mStep[0m  [56/84], [94mLoss[0m : 2.19424
[1mStep[0m  [64/84], [94mLoss[0m : 2.66456
[1mStep[0m  [72/84], [94mLoss[0m : 2.59212
[1mStep[0m  [80/84], [94mLoss[0m : 2.37941

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41466
[1mStep[0m  [8/84], [94mLoss[0m : 2.90602
[1mStep[0m  [16/84], [94mLoss[0m : 2.72474
[1mStep[0m  [24/84], [94mLoss[0m : 2.26883
[1mStep[0m  [32/84], [94mLoss[0m : 2.22345
[1mStep[0m  [40/84], [94mLoss[0m : 2.44915
[1mStep[0m  [48/84], [94mLoss[0m : 2.56623
[1mStep[0m  [56/84], [94mLoss[0m : 2.76829
[1mStep[0m  [64/84], [94mLoss[0m : 2.25494
[1mStep[0m  [72/84], [94mLoss[0m : 2.53930
[1mStep[0m  [80/84], [94mLoss[0m : 2.37174

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58812
[1mStep[0m  [8/84], [94mLoss[0m : 2.69261
[1mStep[0m  [16/84], [94mLoss[0m : 2.38003
[1mStep[0m  [24/84], [94mLoss[0m : 2.57590
[1mStep[0m  [32/84], [94mLoss[0m : 2.54534
[1mStep[0m  [40/84], [94mLoss[0m : 2.60927
[1mStep[0m  [48/84], [94mLoss[0m : 2.10523
[1mStep[0m  [56/84], [94mLoss[0m : 2.48410
[1mStep[0m  [64/84], [94mLoss[0m : 2.34441
[1mStep[0m  [72/84], [94mLoss[0m : 2.54419
[1mStep[0m  [80/84], [94mLoss[0m : 2.57512

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54004
[1mStep[0m  [8/84], [94mLoss[0m : 2.44342
[1mStep[0m  [16/84], [94mLoss[0m : 2.49952
[1mStep[0m  [24/84], [94mLoss[0m : 2.52482
[1mStep[0m  [32/84], [94mLoss[0m : 2.67882
[1mStep[0m  [40/84], [94mLoss[0m : 2.75372
[1mStep[0m  [48/84], [94mLoss[0m : 2.43046
[1mStep[0m  [56/84], [94mLoss[0m : 2.41323
[1mStep[0m  [64/84], [94mLoss[0m : 2.52561
[1mStep[0m  [72/84], [94mLoss[0m : 2.42236
[1mStep[0m  [80/84], [94mLoss[0m : 2.89101

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33082
[1mStep[0m  [8/84], [94mLoss[0m : 2.58033
[1mStep[0m  [16/84], [94mLoss[0m : 2.31635
[1mStep[0m  [24/84], [94mLoss[0m : 2.18300
[1mStep[0m  [32/84], [94mLoss[0m : 2.29352
[1mStep[0m  [40/84], [94mLoss[0m : 2.58274
[1mStep[0m  [48/84], [94mLoss[0m : 2.57726
[1mStep[0m  [56/84], [94mLoss[0m : 2.37447
[1mStep[0m  [64/84], [94mLoss[0m : 2.43812
[1mStep[0m  [72/84], [94mLoss[0m : 2.66464
[1mStep[0m  [80/84], [94mLoss[0m : 2.46987

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46309
[1mStep[0m  [8/84], [94mLoss[0m : 2.49287
[1mStep[0m  [16/84], [94mLoss[0m : 2.50932
[1mStep[0m  [24/84], [94mLoss[0m : 2.49707
[1mStep[0m  [32/84], [94mLoss[0m : 2.79498
[1mStep[0m  [40/84], [94mLoss[0m : 2.47240
[1mStep[0m  [48/84], [94mLoss[0m : 2.69943
[1mStep[0m  [56/84], [94mLoss[0m : 2.48814
[1mStep[0m  [64/84], [94mLoss[0m : 2.33993
[1mStep[0m  [72/84], [94mLoss[0m : 2.61530
[1mStep[0m  [80/84], [94mLoss[0m : 2.45520

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55670
[1mStep[0m  [8/84], [94mLoss[0m : 2.99951
[1mStep[0m  [16/84], [94mLoss[0m : 2.53488
[1mStep[0m  [24/84], [94mLoss[0m : 2.68426
[1mStep[0m  [32/84], [94mLoss[0m : 2.44500
[1mStep[0m  [40/84], [94mLoss[0m : 2.60299
[1mStep[0m  [48/84], [94mLoss[0m : 2.46241
[1mStep[0m  [56/84], [94mLoss[0m : 2.56436
[1mStep[0m  [64/84], [94mLoss[0m : 2.15946
[1mStep[0m  [72/84], [94mLoss[0m : 2.50746
[1mStep[0m  [80/84], [94mLoss[0m : 2.34348

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59680
[1mStep[0m  [8/84], [94mLoss[0m : 2.33335
[1mStep[0m  [16/84], [94mLoss[0m : 2.58026
[1mStep[0m  [24/84], [94mLoss[0m : 2.31553
[1mStep[0m  [32/84], [94mLoss[0m : 2.61423
[1mStep[0m  [40/84], [94mLoss[0m : 2.66330
[1mStep[0m  [48/84], [94mLoss[0m : 2.40695
[1mStep[0m  [56/84], [94mLoss[0m : 2.54271
[1mStep[0m  [64/84], [94mLoss[0m : 2.41085
[1mStep[0m  [72/84], [94mLoss[0m : 2.43487
[1mStep[0m  [80/84], [94mLoss[0m : 2.42229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39268
[1mStep[0m  [8/84], [94mLoss[0m : 2.34932
[1mStep[0m  [16/84], [94mLoss[0m : 2.17335
[1mStep[0m  [24/84], [94mLoss[0m : 2.31123
[1mStep[0m  [32/84], [94mLoss[0m : 2.43112
[1mStep[0m  [40/84], [94mLoss[0m : 2.41247
[1mStep[0m  [48/84], [94mLoss[0m : 2.25340
[1mStep[0m  [56/84], [94mLoss[0m : 2.22678
[1mStep[0m  [64/84], [94mLoss[0m : 2.45380
[1mStep[0m  [72/84], [94mLoss[0m : 2.26170
[1mStep[0m  [80/84], [94mLoss[0m : 2.56793

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28872
[1mStep[0m  [8/84], [94mLoss[0m : 2.30582
[1mStep[0m  [16/84], [94mLoss[0m : 2.73411
[1mStep[0m  [24/84], [94mLoss[0m : 2.27484
[1mStep[0m  [32/84], [94mLoss[0m : 2.58649
[1mStep[0m  [40/84], [94mLoss[0m : 2.47169
[1mStep[0m  [48/84], [94mLoss[0m : 2.62421
[1mStep[0m  [56/84], [94mLoss[0m : 2.45263
[1mStep[0m  [64/84], [94mLoss[0m : 2.29669
[1mStep[0m  [72/84], [94mLoss[0m : 2.45254
[1mStep[0m  [80/84], [94mLoss[0m : 2.43763

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37686
[1mStep[0m  [8/84], [94mLoss[0m : 2.35489
[1mStep[0m  [16/84], [94mLoss[0m : 2.37504
[1mStep[0m  [24/84], [94mLoss[0m : 2.21596
[1mStep[0m  [32/84], [94mLoss[0m : 2.83841
[1mStep[0m  [40/84], [94mLoss[0m : 2.40291
[1mStep[0m  [48/84], [94mLoss[0m : 2.50402
[1mStep[0m  [56/84], [94mLoss[0m : 2.42111
[1mStep[0m  [64/84], [94mLoss[0m : 2.37969
[1mStep[0m  [72/84], [94mLoss[0m : 2.32649
[1mStep[0m  [80/84], [94mLoss[0m : 2.65816

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46465
[1mStep[0m  [8/84], [94mLoss[0m : 2.58765
[1mStep[0m  [16/84], [94mLoss[0m : 2.35458
[1mStep[0m  [24/84], [94mLoss[0m : 2.30933
[1mStep[0m  [32/84], [94mLoss[0m : 2.50484
[1mStep[0m  [40/84], [94mLoss[0m : 2.27135
[1mStep[0m  [48/84], [94mLoss[0m : 2.37520
[1mStep[0m  [56/84], [94mLoss[0m : 2.49118
[1mStep[0m  [64/84], [94mLoss[0m : 2.45082
[1mStep[0m  [72/84], [94mLoss[0m : 2.41321
[1mStep[0m  [80/84], [94mLoss[0m : 2.56656

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30586
[1mStep[0m  [8/84], [94mLoss[0m : 2.16195
[1mStep[0m  [16/84], [94mLoss[0m : 2.81873
[1mStep[0m  [24/84], [94mLoss[0m : 2.55060
[1mStep[0m  [32/84], [94mLoss[0m : 2.55413
[1mStep[0m  [40/84], [94mLoss[0m : 2.24200
[1mStep[0m  [48/84], [94mLoss[0m : 2.73792
[1mStep[0m  [56/84], [94mLoss[0m : 2.19405
[1mStep[0m  [64/84], [94mLoss[0m : 2.35612
[1mStep[0m  [72/84], [94mLoss[0m : 2.29044
[1mStep[0m  [80/84], [94mLoss[0m : 2.18061

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53970
[1mStep[0m  [8/84], [94mLoss[0m : 2.58293
[1mStep[0m  [16/84], [94mLoss[0m : 2.63093
[1mStep[0m  [24/84], [94mLoss[0m : 2.45898
[1mStep[0m  [32/84], [94mLoss[0m : 2.41996
[1mStep[0m  [40/84], [94mLoss[0m : 2.42380
[1mStep[0m  [48/84], [94mLoss[0m : 2.52572
[1mStep[0m  [56/84], [94mLoss[0m : 2.47163
[1mStep[0m  [64/84], [94mLoss[0m : 2.49641
[1mStep[0m  [72/84], [94mLoss[0m : 2.68981
[1mStep[0m  [80/84], [94mLoss[0m : 2.33928

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42076
[1mStep[0m  [8/84], [94mLoss[0m : 2.36743
[1mStep[0m  [16/84], [94mLoss[0m : 2.72297
[1mStep[0m  [24/84], [94mLoss[0m : 2.28758
[1mStep[0m  [32/84], [94mLoss[0m : 2.52464
[1mStep[0m  [40/84], [94mLoss[0m : 2.66873
[1mStep[0m  [48/84], [94mLoss[0m : 2.49229
[1mStep[0m  [56/84], [94mLoss[0m : 2.32065
[1mStep[0m  [64/84], [94mLoss[0m : 2.51926
[1mStep[0m  [72/84], [94mLoss[0m : 2.51098
[1mStep[0m  [80/84], [94mLoss[0m : 2.62753

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17600
[1mStep[0m  [8/84], [94mLoss[0m : 2.52890
[1mStep[0m  [16/84], [94mLoss[0m : 2.41751
[1mStep[0m  [24/84], [94mLoss[0m : 2.38963
[1mStep[0m  [32/84], [94mLoss[0m : 2.38584
[1mStep[0m  [40/84], [94mLoss[0m : 2.62377
[1mStep[0m  [48/84], [94mLoss[0m : 2.33633
[1mStep[0m  [56/84], [94mLoss[0m : 2.54918
[1mStep[0m  [64/84], [94mLoss[0m : 2.55665
[1mStep[0m  [72/84], [94mLoss[0m : 2.53458
[1mStep[0m  [80/84], [94mLoss[0m : 2.37112

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56987
[1mStep[0m  [8/84], [94mLoss[0m : 2.23680
[1mStep[0m  [16/84], [94mLoss[0m : 2.64989
[1mStep[0m  [24/84], [94mLoss[0m : 2.25047
[1mStep[0m  [32/84], [94mLoss[0m : 2.36153
[1mStep[0m  [40/84], [94mLoss[0m : 2.33771
[1mStep[0m  [48/84], [94mLoss[0m : 2.62176
[1mStep[0m  [56/84], [94mLoss[0m : 2.46489
[1mStep[0m  [64/84], [94mLoss[0m : 2.22163
[1mStep[0m  [72/84], [94mLoss[0m : 2.49725
[1mStep[0m  [80/84], [94mLoss[0m : 2.57273

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24289
[1mStep[0m  [8/84], [94mLoss[0m : 2.51695
[1mStep[0m  [16/84], [94mLoss[0m : 2.47910
[1mStep[0m  [24/84], [94mLoss[0m : 2.45239
[1mStep[0m  [32/84], [94mLoss[0m : 2.46928
[1mStep[0m  [40/84], [94mLoss[0m : 1.97326
[1mStep[0m  [48/84], [94mLoss[0m : 2.55013
[1mStep[0m  [56/84], [94mLoss[0m : 2.24717
[1mStep[0m  [64/84], [94mLoss[0m : 2.48274
[1mStep[0m  [72/84], [94mLoss[0m : 2.42735
[1mStep[0m  [80/84], [94mLoss[0m : 2.22852

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16990
[1mStep[0m  [8/84], [94mLoss[0m : 2.40351
[1mStep[0m  [16/84], [94mLoss[0m : 2.62901
[1mStep[0m  [24/84], [94mLoss[0m : 2.26719
[1mStep[0m  [32/84], [94mLoss[0m : 2.38516
[1mStep[0m  [40/84], [94mLoss[0m : 2.66834
[1mStep[0m  [48/84], [94mLoss[0m : 2.21158
[1mStep[0m  [56/84], [94mLoss[0m : 2.40714
[1mStep[0m  [64/84], [94mLoss[0m : 2.28189
[1mStep[0m  [72/84], [94mLoss[0m : 2.64430
[1mStep[0m  [80/84], [94mLoss[0m : 2.65608

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33996
[1mStep[0m  [8/84], [94mLoss[0m : 2.37389
[1mStep[0m  [16/84], [94mLoss[0m : 2.53579
[1mStep[0m  [24/84], [94mLoss[0m : 2.65541
[1mStep[0m  [32/84], [94mLoss[0m : 2.54711
[1mStep[0m  [40/84], [94mLoss[0m : 2.26275
[1mStep[0m  [48/84], [94mLoss[0m : 2.42169
[1mStep[0m  [56/84], [94mLoss[0m : 2.71050
[1mStep[0m  [64/84], [94mLoss[0m : 2.36349
[1mStep[0m  [72/84], [94mLoss[0m : 2.18161
[1mStep[0m  [80/84], [94mLoss[0m : 2.52529

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.361, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.371
====================================

Phase 1 - Evaluation MAE:  2.371207160609109
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.59905
[1mStep[0m  [8/84], [94mLoss[0m : 2.43531
[1mStep[0m  [16/84], [94mLoss[0m : 2.47760
[1mStep[0m  [24/84], [94mLoss[0m : 2.47008
[1mStep[0m  [32/84], [94mLoss[0m : 2.58364
[1mStep[0m  [40/84], [94mLoss[0m : 2.44599
[1mStep[0m  [48/84], [94mLoss[0m : 2.32594
[1mStep[0m  [56/84], [94mLoss[0m : 2.49455
[1mStep[0m  [64/84], [94mLoss[0m : 2.37030
[1mStep[0m  [72/84], [94mLoss[0m : 2.25773
[1mStep[0m  [80/84], [94mLoss[0m : 2.40538

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54137
[1mStep[0m  [8/84], [94mLoss[0m : 2.18622
[1mStep[0m  [16/84], [94mLoss[0m : 2.32817
[1mStep[0m  [24/84], [94mLoss[0m : 2.16043
[1mStep[0m  [32/84], [94mLoss[0m : 2.37876
[1mStep[0m  [40/84], [94mLoss[0m : 2.32683
[1mStep[0m  [48/84], [94mLoss[0m : 2.74920
[1mStep[0m  [56/84], [94mLoss[0m : 2.54850
[1mStep[0m  [64/84], [94mLoss[0m : 2.33852
[1mStep[0m  [72/84], [94mLoss[0m : 2.07706
[1mStep[0m  [80/84], [94mLoss[0m : 2.46686

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58503
[1mStep[0m  [8/84], [94mLoss[0m : 2.16938
[1mStep[0m  [16/84], [94mLoss[0m : 2.44955
[1mStep[0m  [24/84], [94mLoss[0m : 2.45171
[1mStep[0m  [32/84], [94mLoss[0m : 2.43142
[1mStep[0m  [40/84], [94mLoss[0m : 2.65111
[1mStep[0m  [48/84], [94mLoss[0m : 2.10195
[1mStep[0m  [56/84], [94mLoss[0m : 2.44074
[1mStep[0m  [64/84], [94mLoss[0m : 2.18612
[1mStep[0m  [72/84], [94mLoss[0m : 2.35480
[1mStep[0m  [80/84], [94mLoss[0m : 2.28921

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05804
[1mStep[0m  [8/84], [94mLoss[0m : 2.24195
[1mStep[0m  [16/84], [94mLoss[0m : 2.09686
[1mStep[0m  [24/84], [94mLoss[0m : 2.34298
[1mStep[0m  [32/84], [94mLoss[0m : 2.25927
[1mStep[0m  [40/84], [94mLoss[0m : 2.48807
[1mStep[0m  [48/84], [94mLoss[0m : 2.20748
[1mStep[0m  [56/84], [94mLoss[0m : 2.22636
[1mStep[0m  [64/84], [94mLoss[0m : 2.30742
[1mStep[0m  [72/84], [94mLoss[0m : 2.24723
[1mStep[0m  [80/84], [94mLoss[0m : 2.38439

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43720
[1mStep[0m  [8/84], [94mLoss[0m : 2.33343
[1mStep[0m  [16/84], [94mLoss[0m : 2.37299
[1mStep[0m  [24/84], [94mLoss[0m : 2.15720
[1mStep[0m  [32/84], [94mLoss[0m : 2.49389
[1mStep[0m  [40/84], [94mLoss[0m : 2.12503
[1mStep[0m  [48/84], [94mLoss[0m : 2.14280
[1mStep[0m  [56/84], [94mLoss[0m : 2.14701
[1mStep[0m  [64/84], [94mLoss[0m : 2.12454
[1mStep[0m  [72/84], [94mLoss[0m : 2.32313
[1mStep[0m  [80/84], [94mLoss[0m : 2.14829

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05109
[1mStep[0m  [8/84], [94mLoss[0m : 2.44155
[1mStep[0m  [16/84], [94mLoss[0m : 2.13239
[1mStep[0m  [24/84], [94mLoss[0m : 2.13319
[1mStep[0m  [32/84], [94mLoss[0m : 2.14498
[1mStep[0m  [40/84], [94mLoss[0m : 1.93219
[1mStep[0m  [48/84], [94mLoss[0m : 2.01046
[1mStep[0m  [56/84], [94mLoss[0m : 1.92950
[1mStep[0m  [64/84], [94mLoss[0m : 2.10402
[1mStep[0m  [72/84], [94mLoss[0m : 2.11574
[1mStep[0m  [80/84], [94mLoss[0m : 1.75206

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91143
[1mStep[0m  [8/84], [94mLoss[0m : 1.98647
[1mStep[0m  [16/84], [94mLoss[0m : 2.15507
[1mStep[0m  [24/84], [94mLoss[0m : 2.09607
[1mStep[0m  [32/84], [94mLoss[0m : 2.04131
[1mStep[0m  [40/84], [94mLoss[0m : 1.91586
[1mStep[0m  [48/84], [94mLoss[0m : 1.91225
[1mStep[0m  [56/84], [94mLoss[0m : 2.22582
[1mStep[0m  [64/84], [94mLoss[0m : 2.16633
[1mStep[0m  [72/84], [94mLoss[0m : 2.19951
[1mStep[0m  [80/84], [94mLoss[0m : 2.13765

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03364
[1mStep[0m  [8/84], [94mLoss[0m : 2.29687
[1mStep[0m  [16/84], [94mLoss[0m : 1.83758
[1mStep[0m  [24/84], [94mLoss[0m : 1.82105
[1mStep[0m  [32/84], [94mLoss[0m : 2.20040
[1mStep[0m  [40/84], [94mLoss[0m : 2.36179
[1mStep[0m  [48/84], [94mLoss[0m : 2.06973
[1mStep[0m  [56/84], [94mLoss[0m : 2.13604
[1mStep[0m  [64/84], [94mLoss[0m : 1.99637
[1mStep[0m  [72/84], [94mLoss[0m : 2.16331
[1mStep[0m  [80/84], [94mLoss[0m : 2.05275

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00106
[1mStep[0m  [8/84], [94mLoss[0m : 1.84961
[1mStep[0m  [16/84], [94mLoss[0m : 1.88750
[1mStep[0m  [24/84], [94mLoss[0m : 1.81200
[1mStep[0m  [32/84], [94mLoss[0m : 2.24219
[1mStep[0m  [40/84], [94mLoss[0m : 1.87162
[1mStep[0m  [48/84], [94mLoss[0m : 2.04471
[1mStep[0m  [56/84], [94mLoss[0m : 2.00434
[1mStep[0m  [64/84], [94mLoss[0m : 1.94943
[1mStep[0m  [72/84], [94mLoss[0m : 2.11809
[1mStep[0m  [80/84], [94mLoss[0m : 2.30330

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91168
[1mStep[0m  [8/84], [94mLoss[0m : 1.92195
[1mStep[0m  [16/84], [94mLoss[0m : 1.86860
[1mStep[0m  [24/84], [94mLoss[0m : 2.09241
[1mStep[0m  [32/84], [94mLoss[0m : 2.34577
[1mStep[0m  [40/84], [94mLoss[0m : 1.93774
[1mStep[0m  [48/84], [94mLoss[0m : 2.24683
[1mStep[0m  [56/84], [94mLoss[0m : 1.75470
[1mStep[0m  [64/84], [94mLoss[0m : 2.11688
[1mStep[0m  [72/84], [94mLoss[0m : 1.95422
[1mStep[0m  [80/84], [94mLoss[0m : 2.07621

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67019
[1mStep[0m  [8/84], [94mLoss[0m : 1.97899
[1mStep[0m  [16/84], [94mLoss[0m : 2.08621
[1mStep[0m  [24/84], [94mLoss[0m : 1.84046
[1mStep[0m  [32/84], [94mLoss[0m : 1.93415
[1mStep[0m  [40/84], [94mLoss[0m : 1.90540
[1mStep[0m  [48/84], [94mLoss[0m : 1.80910
[1mStep[0m  [56/84], [94mLoss[0m : 2.00584
[1mStep[0m  [64/84], [94mLoss[0m : 1.98395
[1mStep[0m  [72/84], [94mLoss[0m : 1.94460
[1mStep[0m  [80/84], [94mLoss[0m : 2.27221

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85087
[1mStep[0m  [8/84], [94mLoss[0m : 1.75833
[1mStep[0m  [16/84], [94mLoss[0m : 2.48546
[1mStep[0m  [24/84], [94mLoss[0m : 1.99104
[1mStep[0m  [32/84], [94mLoss[0m : 1.97912
[1mStep[0m  [40/84], [94mLoss[0m : 1.95660
[1mStep[0m  [48/84], [94mLoss[0m : 2.07650
[1mStep[0m  [56/84], [94mLoss[0m : 2.30339
[1mStep[0m  [64/84], [94mLoss[0m : 1.70542
[1mStep[0m  [72/84], [94mLoss[0m : 1.88567
[1mStep[0m  [80/84], [94mLoss[0m : 1.95687

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86621
[1mStep[0m  [8/84], [94mLoss[0m : 2.04928
[1mStep[0m  [16/84], [94mLoss[0m : 1.97206
[1mStep[0m  [24/84], [94mLoss[0m : 2.10612
[1mStep[0m  [32/84], [94mLoss[0m : 2.24201
[1mStep[0m  [40/84], [94mLoss[0m : 2.09158
[1mStep[0m  [48/84], [94mLoss[0m : 1.94987
[1mStep[0m  [56/84], [94mLoss[0m : 1.84107
[1mStep[0m  [64/84], [94mLoss[0m : 1.81562
[1mStep[0m  [72/84], [94mLoss[0m : 1.96201
[1mStep[0m  [80/84], [94mLoss[0m : 1.88958

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55316
[1mStep[0m  [8/84], [94mLoss[0m : 1.67952
[1mStep[0m  [16/84], [94mLoss[0m : 2.01712
[1mStep[0m  [24/84], [94mLoss[0m : 1.86303
[1mStep[0m  [32/84], [94mLoss[0m : 2.06870
[1mStep[0m  [40/84], [94mLoss[0m : 1.91396
[1mStep[0m  [48/84], [94mLoss[0m : 1.57908
[1mStep[0m  [56/84], [94mLoss[0m : 1.76908
[1mStep[0m  [64/84], [94mLoss[0m : 1.99569
[1mStep[0m  [72/84], [94mLoss[0m : 2.27417
[1mStep[0m  [80/84], [94mLoss[0m : 2.06252

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02243
[1mStep[0m  [8/84], [94mLoss[0m : 1.89808
[1mStep[0m  [16/84], [94mLoss[0m : 2.23388
[1mStep[0m  [24/84], [94mLoss[0m : 2.13367
[1mStep[0m  [32/84], [94mLoss[0m : 1.86314
[1mStep[0m  [40/84], [94mLoss[0m : 1.81124
[1mStep[0m  [48/84], [94mLoss[0m : 2.10123
[1mStep[0m  [56/84], [94mLoss[0m : 1.70708
[1mStep[0m  [64/84], [94mLoss[0m : 1.96666
[1mStep[0m  [72/84], [94mLoss[0m : 1.80337
[1mStep[0m  [80/84], [94mLoss[0m : 1.70654

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.483, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87214
[1mStep[0m  [8/84], [94mLoss[0m : 1.69036
[1mStep[0m  [16/84], [94mLoss[0m : 1.75427
[1mStep[0m  [24/84], [94mLoss[0m : 1.80411
[1mStep[0m  [32/84], [94mLoss[0m : 1.93790
[1mStep[0m  [40/84], [94mLoss[0m : 1.71199
[1mStep[0m  [48/84], [94mLoss[0m : 1.84375
[1mStep[0m  [56/84], [94mLoss[0m : 1.75998
[1mStep[0m  [64/84], [94mLoss[0m : 1.71731
[1mStep[0m  [72/84], [94mLoss[0m : 1.87803
[1mStep[0m  [80/84], [94mLoss[0m : 1.81557

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72458
[1mStep[0m  [8/84], [94mLoss[0m : 1.84714
[1mStep[0m  [16/84], [94mLoss[0m : 1.97620
[1mStep[0m  [24/84], [94mLoss[0m : 1.92617
[1mStep[0m  [32/84], [94mLoss[0m : 1.97516
[1mStep[0m  [40/84], [94mLoss[0m : 1.81931
[1mStep[0m  [48/84], [94mLoss[0m : 1.93212
[1mStep[0m  [56/84], [94mLoss[0m : 1.95092
[1mStep[0m  [64/84], [94mLoss[0m : 2.05830
[1mStep[0m  [72/84], [94mLoss[0m : 2.16095
[1mStep[0m  [80/84], [94mLoss[0m : 2.04208

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85584
[1mStep[0m  [8/84], [94mLoss[0m : 1.72256
[1mStep[0m  [16/84], [94mLoss[0m : 1.89902
[1mStep[0m  [24/84], [94mLoss[0m : 1.64456
[1mStep[0m  [32/84], [94mLoss[0m : 1.76504
[1mStep[0m  [40/84], [94mLoss[0m : 1.70777
[1mStep[0m  [48/84], [94mLoss[0m : 2.07554
[1mStep[0m  [56/84], [94mLoss[0m : 1.68117
[1mStep[0m  [64/84], [94mLoss[0m : 1.89338
[1mStep[0m  [72/84], [94mLoss[0m : 1.99773
[1mStep[0m  [80/84], [94mLoss[0m : 2.12120

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80916
[1mStep[0m  [8/84], [94mLoss[0m : 1.64837
[1mStep[0m  [16/84], [94mLoss[0m : 1.72525
[1mStep[0m  [24/84], [94mLoss[0m : 1.86056
[1mStep[0m  [32/84], [94mLoss[0m : 1.74124
[1mStep[0m  [40/84], [94mLoss[0m : 2.02768
[1mStep[0m  [48/84], [94mLoss[0m : 1.99479
[1mStep[0m  [56/84], [94mLoss[0m : 1.98541
[1mStep[0m  [64/84], [94mLoss[0m : 1.94213
[1mStep[0m  [72/84], [94mLoss[0m : 1.88733
[1mStep[0m  [80/84], [94mLoss[0m : 1.87161

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72675
[1mStep[0m  [8/84], [94mLoss[0m : 2.15344
[1mStep[0m  [16/84], [94mLoss[0m : 1.64320
[1mStep[0m  [24/84], [94mLoss[0m : 1.59755
[1mStep[0m  [32/84], [94mLoss[0m : 2.06303
[1mStep[0m  [40/84], [94mLoss[0m : 2.00463
[1mStep[0m  [48/84], [94mLoss[0m : 1.72142
[1mStep[0m  [56/84], [94mLoss[0m : 1.87252
[1mStep[0m  [64/84], [94mLoss[0m : 1.74792
[1mStep[0m  [72/84], [94mLoss[0m : 1.95253
[1mStep[0m  [80/84], [94mLoss[0m : 1.74516

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51433
[1mStep[0m  [8/84], [94mLoss[0m : 1.60271
[1mStep[0m  [16/84], [94mLoss[0m : 1.78913
[1mStep[0m  [24/84], [94mLoss[0m : 1.75692
[1mStep[0m  [32/84], [94mLoss[0m : 1.58698
[1mStep[0m  [40/84], [94mLoss[0m : 1.56746
[1mStep[0m  [48/84], [94mLoss[0m : 1.93878
[1mStep[0m  [56/84], [94mLoss[0m : 1.74448
[1mStep[0m  [64/84], [94mLoss[0m : 1.80813
[1mStep[0m  [72/84], [94mLoss[0m : 1.77374
[1mStep[0m  [80/84], [94mLoss[0m : 1.83803

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77228
[1mStep[0m  [8/84], [94mLoss[0m : 1.63247
[1mStep[0m  [16/84], [94mLoss[0m : 1.76600
[1mStep[0m  [24/84], [94mLoss[0m : 1.88651
[1mStep[0m  [32/84], [94mLoss[0m : 1.89548
[1mStep[0m  [40/84], [94mLoss[0m : 1.64703
[1mStep[0m  [48/84], [94mLoss[0m : 1.79957
[1mStep[0m  [56/84], [94mLoss[0m : 1.54548
[1mStep[0m  [64/84], [94mLoss[0m : 1.94208
[1mStep[0m  [72/84], [94mLoss[0m : 1.76534
[1mStep[0m  [80/84], [94mLoss[0m : 1.79062

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78663
[1mStep[0m  [8/84], [94mLoss[0m : 1.77089
[1mStep[0m  [16/84], [94mLoss[0m : 1.59135
[1mStep[0m  [24/84], [94mLoss[0m : 1.85313
[1mStep[0m  [32/84], [94mLoss[0m : 1.96268
[1mStep[0m  [40/84], [94mLoss[0m : 1.82132
[1mStep[0m  [48/84], [94mLoss[0m : 1.89165
[1mStep[0m  [56/84], [94mLoss[0m : 1.70183
[1mStep[0m  [64/84], [94mLoss[0m : 1.75239
[1mStep[0m  [72/84], [94mLoss[0m : 1.70284
[1mStep[0m  [80/84], [94mLoss[0m : 1.71045

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.780, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95287
[1mStep[0m  [8/84], [94mLoss[0m : 1.83275
[1mStep[0m  [16/84], [94mLoss[0m : 1.84062
[1mStep[0m  [24/84], [94mLoss[0m : 1.75495
[1mStep[0m  [32/84], [94mLoss[0m : 1.88506
[1mStep[0m  [40/84], [94mLoss[0m : 1.62735
[1mStep[0m  [48/84], [94mLoss[0m : 1.55434
[1mStep[0m  [56/84], [94mLoss[0m : 1.47623
[1mStep[0m  [64/84], [94mLoss[0m : 1.56878
[1mStep[0m  [72/84], [94mLoss[0m : 1.77988
[1mStep[0m  [80/84], [94mLoss[0m : 1.57613

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.509, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.75161
[1mStep[0m  [8/84], [94mLoss[0m : 2.01828
[1mStep[0m  [16/84], [94mLoss[0m : 1.98487
[1mStep[0m  [24/84], [94mLoss[0m : 1.85370
[1mStep[0m  [32/84], [94mLoss[0m : 1.74936
[1mStep[0m  [40/84], [94mLoss[0m : 1.75441
[1mStep[0m  [48/84], [94mLoss[0m : 1.79228
[1mStep[0m  [56/84], [94mLoss[0m : 2.08329
[1mStep[0m  [64/84], [94mLoss[0m : 1.70539
[1mStep[0m  [72/84], [94mLoss[0m : 1.54743
[1mStep[0m  [80/84], [94mLoss[0m : 1.86970

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.748, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65300
[1mStep[0m  [8/84], [94mLoss[0m : 1.80836
[1mStep[0m  [16/84], [94mLoss[0m : 1.65123
[1mStep[0m  [24/84], [94mLoss[0m : 1.90090
[1mStep[0m  [32/84], [94mLoss[0m : 1.73685
[1mStep[0m  [40/84], [94mLoss[0m : 2.02768
[1mStep[0m  [48/84], [94mLoss[0m : 1.75467
[1mStep[0m  [56/84], [94mLoss[0m : 1.66062
[1mStep[0m  [64/84], [94mLoss[0m : 1.89906
[1mStep[0m  [72/84], [94mLoss[0m : 1.66453
[1mStep[0m  [80/84], [94mLoss[0m : 1.75277

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74314
[1mStep[0m  [8/84], [94mLoss[0m : 1.69664
[1mStep[0m  [16/84], [94mLoss[0m : 1.73544
[1mStep[0m  [24/84], [94mLoss[0m : 1.71351
[1mStep[0m  [32/84], [94mLoss[0m : 1.58002
[1mStep[0m  [40/84], [94mLoss[0m : 2.10737
[1mStep[0m  [48/84], [94mLoss[0m : 1.59372
[1mStep[0m  [56/84], [94mLoss[0m : 1.62707
[1mStep[0m  [64/84], [94mLoss[0m : 2.07730
[1mStep[0m  [72/84], [94mLoss[0m : 1.82387
[1mStep[0m  [80/84], [94mLoss[0m : 1.78921

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63369
[1mStep[0m  [8/84], [94mLoss[0m : 1.59785
[1mStep[0m  [16/84], [94mLoss[0m : 1.70773
[1mStep[0m  [24/84], [94mLoss[0m : 1.63134
[1mStep[0m  [32/84], [94mLoss[0m : 1.80622
[1mStep[0m  [40/84], [94mLoss[0m : 1.80563
[1mStep[0m  [48/84], [94mLoss[0m : 1.95937
[1mStep[0m  [56/84], [94mLoss[0m : 1.92082
[1mStep[0m  [64/84], [94mLoss[0m : 1.89625
[1mStep[0m  [72/84], [94mLoss[0m : 1.54755
[1mStep[0m  [80/84], [94mLoss[0m : 1.69862

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57633
[1mStep[0m  [8/84], [94mLoss[0m : 1.67001
[1mStep[0m  [16/84], [94mLoss[0m : 1.62902
[1mStep[0m  [24/84], [94mLoss[0m : 1.65068
[1mStep[0m  [32/84], [94mLoss[0m : 1.78845
[1mStep[0m  [40/84], [94mLoss[0m : 1.62584
[1mStep[0m  [48/84], [94mLoss[0m : 1.72776
[1mStep[0m  [56/84], [94mLoss[0m : 1.75187
[1mStep[0m  [64/84], [94mLoss[0m : 1.79086
[1mStep[0m  [72/84], [94mLoss[0m : 1.75937
[1mStep[0m  [80/84], [94mLoss[0m : 1.88193

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78047
[1mStep[0m  [8/84], [94mLoss[0m : 1.69104
[1mStep[0m  [16/84], [94mLoss[0m : 1.74033
[1mStep[0m  [24/84], [94mLoss[0m : 1.75307
[1mStep[0m  [32/84], [94mLoss[0m : 1.91638
[1mStep[0m  [40/84], [94mLoss[0m : 1.57999
[1mStep[0m  [48/84], [94mLoss[0m : 1.71514
[1mStep[0m  [56/84], [94mLoss[0m : 1.61248
[1mStep[0m  [64/84], [94mLoss[0m : 1.56947
[1mStep[0m  [72/84], [94mLoss[0m : 1.51768
[1mStep[0m  [80/84], [94mLoss[0m : 1.80293

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.464
====================================

Phase 2 - Evaluation MAE:  2.4636712670326233
MAE score P1        2.371207
MAE score P2        2.463671
loss                1.705102
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay            0.01
Name: 25, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.80984
[1mStep[0m  [8/84], [94mLoss[0m : 9.34334
[1mStep[0m  [16/84], [94mLoss[0m : 8.41471
[1mStep[0m  [24/84], [94mLoss[0m : 7.96503
[1mStep[0m  [32/84], [94mLoss[0m : 6.22282
[1mStep[0m  [40/84], [94mLoss[0m : 5.52987
[1mStep[0m  [48/84], [94mLoss[0m : 4.26306
[1mStep[0m  [56/84], [94mLoss[0m : 4.14424
[1mStep[0m  [64/84], [94mLoss[0m : 3.23940
[1mStep[0m  [72/84], [94mLoss[0m : 3.42154
[1mStep[0m  [80/84], [94mLoss[0m : 3.26238

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.867, [92mTest[0m: 10.662, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70640
[1mStep[0m  [8/84], [94mLoss[0m : 2.67665
[1mStep[0m  [16/84], [94mLoss[0m : 2.80491
[1mStep[0m  [24/84], [94mLoss[0m : 2.75083
[1mStep[0m  [32/84], [94mLoss[0m : 2.49243
[1mStep[0m  [40/84], [94mLoss[0m : 2.53967
[1mStep[0m  [48/84], [94mLoss[0m : 2.72963
[1mStep[0m  [56/84], [94mLoss[0m : 2.31787
[1mStep[0m  [64/84], [94mLoss[0m : 2.71239
[1mStep[0m  [72/84], [94mLoss[0m : 2.53596
[1mStep[0m  [80/84], [94mLoss[0m : 2.48330

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.740, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36130
[1mStep[0m  [8/84], [94mLoss[0m : 2.52486
[1mStep[0m  [16/84], [94mLoss[0m : 2.72579
[1mStep[0m  [24/84], [94mLoss[0m : 2.70860
[1mStep[0m  [32/84], [94mLoss[0m : 2.13849
[1mStep[0m  [40/84], [94mLoss[0m : 2.19348
[1mStep[0m  [48/84], [94mLoss[0m : 2.34327
[1mStep[0m  [56/84], [94mLoss[0m : 2.73571
[1mStep[0m  [64/84], [94mLoss[0m : 2.30519
[1mStep[0m  [72/84], [94mLoss[0m : 2.40530
[1mStep[0m  [80/84], [94mLoss[0m : 2.49717

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54392
[1mStep[0m  [8/84], [94mLoss[0m : 2.52805
[1mStep[0m  [16/84], [94mLoss[0m : 2.66774
[1mStep[0m  [24/84], [94mLoss[0m : 2.35854
[1mStep[0m  [32/84], [94mLoss[0m : 2.50749
[1mStep[0m  [40/84], [94mLoss[0m : 2.82194
[1mStep[0m  [48/84], [94mLoss[0m : 2.39308
[1mStep[0m  [56/84], [94mLoss[0m : 2.40418
[1mStep[0m  [64/84], [94mLoss[0m : 2.67882
[1mStep[0m  [72/84], [94mLoss[0m : 2.66376
[1mStep[0m  [80/84], [94mLoss[0m : 2.56986

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56498
[1mStep[0m  [8/84], [94mLoss[0m : 2.38685
[1mStep[0m  [16/84], [94mLoss[0m : 2.72324
[1mStep[0m  [24/84], [94mLoss[0m : 2.54707
[1mStep[0m  [32/84], [94mLoss[0m : 2.68873
[1mStep[0m  [40/84], [94mLoss[0m : 2.78374
[1mStep[0m  [48/84], [94mLoss[0m : 2.25226
[1mStep[0m  [56/84], [94mLoss[0m : 2.45706
[1mStep[0m  [64/84], [94mLoss[0m : 2.37692
[1mStep[0m  [72/84], [94mLoss[0m : 2.53016
[1mStep[0m  [80/84], [94mLoss[0m : 2.38326

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62088
[1mStep[0m  [8/84], [94mLoss[0m : 2.53419
[1mStep[0m  [16/84], [94mLoss[0m : 2.52654
[1mStep[0m  [24/84], [94mLoss[0m : 2.47330
[1mStep[0m  [32/84], [94mLoss[0m : 2.80516
[1mStep[0m  [40/84], [94mLoss[0m : 2.35853
[1mStep[0m  [48/84], [94mLoss[0m : 2.69485
[1mStep[0m  [56/84], [94mLoss[0m : 2.25788
[1mStep[0m  [64/84], [94mLoss[0m : 2.70275
[1mStep[0m  [72/84], [94mLoss[0m : 2.38160
[1mStep[0m  [80/84], [94mLoss[0m : 2.50891

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64294
[1mStep[0m  [8/84], [94mLoss[0m : 2.66352
[1mStep[0m  [16/84], [94mLoss[0m : 2.71349
[1mStep[0m  [24/84], [94mLoss[0m : 2.43101
[1mStep[0m  [32/84], [94mLoss[0m : 2.56357
[1mStep[0m  [40/84], [94mLoss[0m : 2.53277
[1mStep[0m  [48/84], [94mLoss[0m : 2.27702
[1mStep[0m  [56/84], [94mLoss[0m : 2.45279
[1mStep[0m  [64/84], [94mLoss[0m : 2.45117
[1mStep[0m  [72/84], [94mLoss[0m : 2.50976
[1mStep[0m  [80/84], [94mLoss[0m : 2.30299

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72636
[1mStep[0m  [8/84], [94mLoss[0m : 2.63339
[1mStep[0m  [16/84], [94mLoss[0m : 2.11953
[1mStep[0m  [24/84], [94mLoss[0m : 2.24821
[1mStep[0m  [32/84], [94mLoss[0m : 2.73594
[1mStep[0m  [40/84], [94mLoss[0m : 2.73763
[1mStep[0m  [48/84], [94mLoss[0m : 2.95717
[1mStep[0m  [56/84], [94mLoss[0m : 2.35643
[1mStep[0m  [64/84], [94mLoss[0m : 2.47106
[1mStep[0m  [72/84], [94mLoss[0m : 2.64032
[1mStep[0m  [80/84], [94mLoss[0m : 2.39304

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50421
[1mStep[0m  [8/84], [94mLoss[0m : 2.54515
[1mStep[0m  [16/84], [94mLoss[0m : 2.80660
[1mStep[0m  [24/84], [94mLoss[0m : 2.38332
[1mStep[0m  [32/84], [94mLoss[0m : 2.61329
[1mStep[0m  [40/84], [94mLoss[0m : 2.47151
[1mStep[0m  [48/84], [94mLoss[0m : 2.33540
[1mStep[0m  [56/84], [94mLoss[0m : 2.58648
[1mStep[0m  [64/84], [94mLoss[0m : 2.29869
[1mStep[0m  [72/84], [94mLoss[0m : 2.70399
[1mStep[0m  [80/84], [94mLoss[0m : 2.61155

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56370
[1mStep[0m  [8/84], [94mLoss[0m : 2.63965
[1mStep[0m  [16/84], [94mLoss[0m : 2.44497
[1mStep[0m  [24/84], [94mLoss[0m : 2.56548
[1mStep[0m  [32/84], [94mLoss[0m : 2.81294
[1mStep[0m  [40/84], [94mLoss[0m : 2.63717
[1mStep[0m  [48/84], [94mLoss[0m : 2.42161
[1mStep[0m  [56/84], [94mLoss[0m : 2.38424
[1mStep[0m  [64/84], [94mLoss[0m : 2.81317
[1mStep[0m  [72/84], [94mLoss[0m : 2.96305
[1mStep[0m  [80/84], [94mLoss[0m : 2.61254

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60551
[1mStep[0m  [8/84], [94mLoss[0m : 2.62721
[1mStep[0m  [16/84], [94mLoss[0m : 2.42073
[1mStep[0m  [24/84], [94mLoss[0m : 2.41007
[1mStep[0m  [32/84], [94mLoss[0m : 2.43508
[1mStep[0m  [40/84], [94mLoss[0m : 2.15658
[1mStep[0m  [48/84], [94mLoss[0m : 2.66223
[1mStep[0m  [56/84], [94mLoss[0m : 2.43078
[1mStep[0m  [64/84], [94mLoss[0m : 2.40409
[1mStep[0m  [72/84], [94mLoss[0m : 2.35109
[1mStep[0m  [80/84], [94mLoss[0m : 2.52559

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50806
[1mStep[0m  [8/84], [94mLoss[0m : 2.66804
[1mStep[0m  [16/84], [94mLoss[0m : 2.63863
[1mStep[0m  [24/84], [94mLoss[0m : 2.67521
[1mStep[0m  [32/84], [94mLoss[0m : 2.47454
[1mStep[0m  [40/84], [94mLoss[0m : 2.40738
[1mStep[0m  [48/84], [94mLoss[0m : 2.63189
[1mStep[0m  [56/84], [94mLoss[0m : 2.25693
[1mStep[0m  [64/84], [94mLoss[0m : 2.51432
[1mStep[0m  [72/84], [94mLoss[0m : 2.15695
[1mStep[0m  [80/84], [94mLoss[0m : 2.44555

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40407
[1mStep[0m  [8/84], [94mLoss[0m : 2.16394
[1mStep[0m  [16/84], [94mLoss[0m : 2.38604
[1mStep[0m  [24/84], [94mLoss[0m : 2.34754
[1mStep[0m  [32/84], [94mLoss[0m : 2.49925
[1mStep[0m  [40/84], [94mLoss[0m : 2.51522
[1mStep[0m  [48/84], [94mLoss[0m : 2.34397
[1mStep[0m  [56/84], [94mLoss[0m : 2.44579
[1mStep[0m  [64/84], [94mLoss[0m : 2.73697
[1mStep[0m  [72/84], [94mLoss[0m : 2.49633
[1mStep[0m  [80/84], [94mLoss[0m : 2.58208

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24225
[1mStep[0m  [8/84], [94mLoss[0m : 2.24086
[1mStep[0m  [16/84], [94mLoss[0m : 2.40551
[1mStep[0m  [24/84], [94mLoss[0m : 2.55332
[1mStep[0m  [32/84], [94mLoss[0m : 2.30335
[1mStep[0m  [40/84], [94mLoss[0m : 2.57761
[1mStep[0m  [48/84], [94mLoss[0m : 2.67128
[1mStep[0m  [56/84], [94mLoss[0m : 2.57024
[1mStep[0m  [64/84], [94mLoss[0m : 2.56823
[1mStep[0m  [72/84], [94mLoss[0m : 2.66608
[1mStep[0m  [80/84], [94mLoss[0m : 2.51279

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20207
[1mStep[0m  [8/84], [94mLoss[0m : 2.65249
[1mStep[0m  [16/84], [94mLoss[0m : 2.37129
[1mStep[0m  [24/84], [94mLoss[0m : 2.78910
[1mStep[0m  [32/84], [94mLoss[0m : 2.24931
[1mStep[0m  [40/84], [94mLoss[0m : 2.36098
[1mStep[0m  [48/84], [94mLoss[0m : 2.75571
[1mStep[0m  [56/84], [94mLoss[0m : 2.69414
[1mStep[0m  [64/84], [94mLoss[0m : 2.50897
[1mStep[0m  [72/84], [94mLoss[0m : 2.47088
[1mStep[0m  [80/84], [94mLoss[0m : 2.36904

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.321, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40328
[1mStep[0m  [8/84], [94mLoss[0m : 2.18001
[1mStep[0m  [16/84], [94mLoss[0m : 2.71422
[1mStep[0m  [24/84], [94mLoss[0m : 2.60846
[1mStep[0m  [32/84], [94mLoss[0m : 2.03448
[1mStep[0m  [40/84], [94mLoss[0m : 2.31387
[1mStep[0m  [48/84], [94mLoss[0m : 2.96271
[1mStep[0m  [56/84], [94mLoss[0m : 2.74627
[1mStep[0m  [64/84], [94mLoss[0m : 2.24260
[1mStep[0m  [72/84], [94mLoss[0m : 2.67604
[1mStep[0m  [80/84], [94mLoss[0m : 2.56922

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57722
[1mStep[0m  [8/84], [94mLoss[0m : 2.62939
[1mStep[0m  [16/84], [94mLoss[0m : 2.65991
[1mStep[0m  [24/84], [94mLoss[0m : 2.18712
[1mStep[0m  [32/84], [94mLoss[0m : 2.24846
[1mStep[0m  [40/84], [94mLoss[0m : 2.26696
[1mStep[0m  [48/84], [94mLoss[0m : 2.62761
[1mStep[0m  [56/84], [94mLoss[0m : 2.41339
[1mStep[0m  [64/84], [94mLoss[0m : 2.55724
[1mStep[0m  [72/84], [94mLoss[0m : 2.51381
[1mStep[0m  [80/84], [94mLoss[0m : 2.37523

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43739
[1mStep[0m  [8/84], [94mLoss[0m : 2.24772
[1mStep[0m  [16/84], [94mLoss[0m : 2.31913
[1mStep[0m  [24/84], [94mLoss[0m : 2.33254
[1mStep[0m  [32/84], [94mLoss[0m : 2.61082
[1mStep[0m  [40/84], [94mLoss[0m : 2.45769
[1mStep[0m  [48/84], [94mLoss[0m : 2.49946
[1mStep[0m  [56/84], [94mLoss[0m : 2.61290
[1mStep[0m  [64/84], [94mLoss[0m : 2.56198
[1mStep[0m  [72/84], [94mLoss[0m : 2.65733
[1mStep[0m  [80/84], [94mLoss[0m : 2.72841

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23814
[1mStep[0m  [8/84], [94mLoss[0m : 2.43904
[1mStep[0m  [16/84], [94mLoss[0m : 2.45809
[1mStep[0m  [24/84], [94mLoss[0m : 2.60875
[1mStep[0m  [32/84], [94mLoss[0m : 2.81981
[1mStep[0m  [40/84], [94mLoss[0m : 2.35055
[1mStep[0m  [48/84], [94mLoss[0m : 2.67797
[1mStep[0m  [56/84], [94mLoss[0m : 2.59184
[1mStep[0m  [64/84], [94mLoss[0m : 2.29920
[1mStep[0m  [72/84], [94mLoss[0m : 2.51181
[1mStep[0m  [80/84], [94mLoss[0m : 2.43655

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55912
[1mStep[0m  [8/84], [94mLoss[0m : 2.30945
[1mStep[0m  [16/84], [94mLoss[0m : 2.36039
[1mStep[0m  [24/84], [94mLoss[0m : 2.70135
[1mStep[0m  [32/84], [94mLoss[0m : 2.75688
[1mStep[0m  [40/84], [94mLoss[0m : 2.23703
[1mStep[0m  [48/84], [94mLoss[0m : 2.28418
[1mStep[0m  [56/84], [94mLoss[0m : 2.28373
[1mStep[0m  [64/84], [94mLoss[0m : 2.69748
[1mStep[0m  [72/84], [94mLoss[0m : 2.34105
[1mStep[0m  [80/84], [94mLoss[0m : 2.60647

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53104
[1mStep[0m  [8/84], [94mLoss[0m : 2.27762
[1mStep[0m  [16/84], [94mLoss[0m : 2.45184
[1mStep[0m  [24/84], [94mLoss[0m : 2.64772
[1mStep[0m  [32/84], [94mLoss[0m : 2.61439
[1mStep[0m  [40/84], [94mLoss[0m : 2.18733
[1mStep[0m  [48/84], [94mLoss[0m : 2.38386
[1mStep[0m  [56/84], [94mLoss[0m : 2.53549
[1mStep[0m  [64/84], [94mLoss[0m : 2.79208
[1mStep[0m  [72/84], [94mLoss[0m : 2.31427
[1mStep[0m  [80/84], [94mLoss[0m : 2.49087

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50907
[1mStep[0m  [8/84], [94mLoss[0m : 2.06000
[1mStep[0m  [16/84], [94mLoss[0m : 2.70305
[1mStep[0m  [24/84], [94mLoss[0m : 2.58818
[1mStep[0m  [32/84], [94mLoss[0m : 2.36720
[1mStep[0m  [40/84], [94mLoss[0m : 2.51839
[1mStep[0m  [48/84], [94mLoss[0m : 2.44853
[1mStep[0m  [56/84], [94mLoss[0m : 2.14798
[1mStep[0m  [64/84], [94mLoss[0m : 2.56775
[1mStep[0m  [72/84], [94mLoss[0m : 2.60100
[1mStep[0m  [80/84], [94mLoss[0m : 2.68824

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73002
[1mStep[0m  [8/84], [94mLoss[0m : 2.42976
[1mStep[0m  [16/84], [94mLoss[0m : 2.47465
[1mStep[0m  [24/84], [94mLoss[0m : 2.59195
[1mStep[0m  [32/84], [94mLoss[0m : 2.25223
[1mStep[0m  [40/84], [94mLoss[0m : 2.33467
[1mStep[0m  [48/84], [94mLoss[0m : 2.50226
[1mStep[0m  [56/84], [94mLoss[0m : 2.32275
[1mStep[0m  [64/84], [94mLoss[0m : 2.36352
[1mStep[0m  [72/84], [94mLoss[0m : 2.37735
[1mStep[0m  [80/84], [94mLoss[0m : 2.34528

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62746
[1mStep[0m  [8/84], [94mLoss[0m : 2.23691
[1mStep[0m  [16/84], [94mLoss[0m : 2.38579
[1mStep[0m  [24/84], [94mLoss[0m : 2.32967
[1mStep[0m  [32/84], [94mLoss[0m : 2.72946
[1mStep[0m  [40/84], [94mLoss[0m : 2.34778
[1mStep[0m  [48/84], [94mLoss[0m : 2.33046
[1mStep[0m  [56/84], [94mLoss[0m : 2.31701
[1mStep[0m  [64/84], [94mLoss[0m : 2.90799
[1mStep[0m  [72/84], [94mLoss[0m : 2.47921
[1mStep[0m  [80/84], [94mLoss[0m : 2.58923

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34195
[1mStep[0m  [8/84], [94mLoss[0m : 2.17890
[1mStep[0m  [16/84], [94mLoss[0m : 2.64128
[1mStep[0m  [24/84], [94mLoss[0m : 2.49279
[1mStep[0m  [32/84], [94mLoss[0m : 2.63008
[1mStep[0m  [40/84], [94mLoss[0m : 2.39440
[1mStep[0m  [48/84], [94mLoss[0m : 2.51573
[1mStep[0m  [56/84], [94mLoss[0m : 2.64818
[1mStep[0m  [64/84], [94mLoss[0m : 2.40284
[1mStep[0m  [72/84], [94mLoss[0m : 2.27851
[1mStep[0m  [80/84], [94mLoss[0m : 2.61357

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21195
[1mStep[0m  [8/84], [94mLoss[0m : 2.21777
[1mStep[0m  [16/84], [94mLoss[0m : 2.57846
[1mStep[0m  [24/84], [94mLoss[0m : 2.71950
[1mStep[0m  [32/84], [94mLoss[0m : 2.35083
[1mStep[0m  [40/84], [94mLoss[0m : 2.42385
[1mStep[0m  [48/84], [94mLoss[0m : 2.66154
[1mStep[0m  [56/84], [94mLoss[0m : 2.34779
[1mStep[0m  [64/84], [94mLoss[0m : 2.49290
[1mStep[0m  [72/84], [94mLoss[0m : 2.54768
[1mStep[0m  [80/84], [94mLoss[0m : 2.33686

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48379
[1mStep[0m  [8/84], [94mLoss[0m : 2.29192
[1mStep[0m  [16/84], [94mLoss[0m : 2.61906
[1mStep[0m  [24/84], [94mLoss[0m : 2.54722
[1mStep[0m  [32/84], [94mLoss[0m : 2.41793
[1mStep[0m  [40/84], [94mLoss[0m : 2.59848
[1mStep[0m  [48/84], [94mLoss[0m : 2.49252
[1mStep[0m  [56/84], [94mLoss[0m : 2.45951
[1mStep[0m  [64/84], [94mLoss[0m : 2.41659
[1mStep[0m  [72/84], [94mLoss[0m : 2.61207
[1mStep[0m  [80/84], [94mLoss[0m : 2.51910

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60083
[1mStep[0m  [8/84], [94mLoss[0m : 2.18237
[1mStep[0m  [16/84], [94mLoss[0m : 2.40059
[1mStep[0m  [24/84], [94mLoss[0m : 2.43265
[1mStep[0m  [32/84], [94mLoss[0m : 2.34776
[1mStep[0m  [40/84], [94mLoss[0m : 2.67008
[1mStep[0m  [48/84], [94mLoss[0m : 2.84171
[1mStep[0m  [56/84], [94mLoss[0m : 2.53217
[1mStep[0m  [64/84], [94mLoss[0m : 2.26600
[1mStep[0m  [72/84], [94mLoss[0m : 2.42164
[1mStep[0m  [80/84], [94mLoss[0m : 2.24225

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32795
[1mStep[0m  [8/84], [94mLoss[0m : 2.53783
[1mStep[0m  [16/84], [94mLoss[0m : 2.24696
[1mStep[0m  [24/84], [94mLoss[0m : 2.41963
[1mStep[0m  [32/84], [94mLoss[0m : 2.38080
[1mStep[0m  [40/84], [94mLoss[0m : 2.58448
[1mStep[0m  [48/84], [94mLoss[0m : 2.61090
[1mStep[0m  [56/84], [94mLoss[0m : 2.63025
[1mStep[0m  [64/84], [94mLoss[0m : 2.53085
[1mStep[0m  [72/84], [94mLoss[0m : 2.46286
[1mStep[0m  [80/84], [94mLoss[0m : 2.29620

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48854
[1mStep[0m  [8/84], [94mLoss[0m : 2.49314
[1mStep[0m  [16/84], [94mLoss[0m : 2.46873
[1mStep[0m  [24/84], [94mLoss[0m : 2.39450
[1mStep[0m  [32/84], [94mLoss[0m : 2.52812
[1mStep[0m  [40/84], [94mLoss[0m : 2.40975
[1mStep[0m  [48/84], [94mLoss[0m : 2.64169
[1mStep[0m  [56/84], [94mLoss[0m : 2.53416
[1mStep[0m  [64/84], [94mLoss[0m : 2.55485
[1mStep[0m  [72/84], [94mLoss[0m : 2.71767
[1mStep[0m  [80/84], [94mLoss[0m : 2.44411

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.335
====================================

Phase 1 - Evaluation MAE:  2.3352642910821095
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.58148
[1mStep[0m  [8/84], [94mLoss[0m : 2.66084
[1mStep[0m  [16/84], [94mLoss[0m : 2.30047
[1mStep[0m  [24/84], [94mLoss[0m : 2.28358
[1mStep[0m  [32/84], [94mLoss[0m : 2.59182
[1mStep[0m  [40/84], [94mLoss[0m : 2.83852
[1mStep[0m  [48/84], [94mLoss[0m : 2.44062
[1mStep[0m  [56/84], [94mLoss[0m : 2.46674
[1mStep[0m  [64/84], [94mLoss[0m : 2.45359
[1mStep[0m  [72/84], [94mLoss[0m : 2.82264
[1mStep[0m  [80/84], [94mLoss[0m : 2.36938

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06775
[1mStep[0m  [8/84], [94mLoss[0m : 2.55579
[1mStep[0m  [16/84], [94mLoss[0m : 2.66510
[1mStep[0m  [24/84], [94mLoss[0m : 2.50296
[1mStep[0m  [32/84], [94mLoss[0m : 2.57126
[1mStep[0m  [40/84], [94mLoss[0m : 2.35558
[1mStep[0m  [48/84], [94mLoss[0m : 2.29348
[1mStep[0m  [56/84], [94mLoss[0m : 2.41280
[1mStep[0m  [64/84], [94mLoss[0m : 2.59581
[1mStep[0m  [72/84], [94mLoss[0m : 2.39566
[1mStep[0m  [80/84], [94mLoss[0m : 2.50803

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58880
[1mStep[0m  [8/84], [94mLoss[0m : 2.77034
[1mStep[0m  [16/84], [94mLoss[0m : 2.70832
[1mStep[0m  [24/84], [94mLoss[0m : 2.50811
[1mStep[0m  [32/84], [94mLoss[0m : 2.41426
[1mStep[0m  [40/84], [94mLoss[0m : 2.44573
[1mStep[0m  [48/84], [94mLoss[0m : 2.61328
[1mStep[0m  [56/84], [94mLoss[0m : 2.08263
[1mStep[0m  [64/84], [94mLoss[0m : 2.36348
[1mStep[0m  [72/84], [94mLoss[0m : 2.32643
[1mStep[0m  [80/84], [94mLoss[0m : 2.07613

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30086
[1mStep[0m  [8/84], [94mLoss[0m : 2.19990
[1mStep[0m  [16/84], [94mLoss[0m : 2.07502
[1mStep[0m  [24/84], [94mLoss[0m : 2.50490
[1mStep[0m  [32/84], [94mLoss[0m : 2.20573
[1mStep[0m  [40/84], [94mLoss[0m : 2.35976
[1mStep[0m  [48/84], [94mLoss[0m : 2.45895
[1mStep[0m  [56/84], [94mLoss[0m : 2.54380
[1mStep[0m  [64/84], [94mLoss[0m : 2.51637
[1mStep[0m  [72/84], [94mLoss[0m : 2.57655
[1mStep[0m  [80/84], [94mLoss[0m : 2.34479

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.318, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29847
[1mStep[0m  [8/84], [94mLoss[0m : 2.41985
[1mStep[0m  [16/84], [94mLoss[0m : 2.16233
[1mStep[0m  [24/84], [94mLoss[0m : 2.31767
[1mStep[0m  [32/84], [94mLoss[0m : 2.22650
[1mStep[0m  [40/84], [94mLoss[0m : 2.21711
[1mStep[0m  [48/84], [94mLoss[0m : 2.21876
[1mStep[0m  [56/84], [94mLoss[0m : 2.41208
[1mStep[0m  [64/84], [94mLoss[0m : 2.33627
[1mStep[0m  [72/84], [94mLoss[0m : 2.31181
[1mStep[0m  [80/84], [94mLoss[0m : 2.56914

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26929
[1mStep[0m  [8/84], [94mLoss[0m : 2.56927
[1mStep[0m  [16/84], [94mLoss[0m : 2.01574
[1mStep[0m  [24/84], [94mLoss[0m : 2.39041
[1mStep[0m  [32/84], [94mLoss[0m : 2.19918
[1mStep[0m  [40/84], [94mLoss[0m : 2.11316
[1mStep[0m  [48/84], [94mLoss[0m : 1.75801
[1mStep[0m  [56/84], [94mLoss[0m : 2.40820
[1mStep[0m  [64/84], [94mLoss[0m : 2.22576
[1mStep[0m  [72/84], [94mLoss[0m : 2.39922
[1mStep[0m  [80/84], [94mLoss[0m : 2.20217

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06267
[1mStep[0m  [8/84], [94mLoss[0m : 2.04471
[1mStep[0m  [16/84], [94mLoss[0m : 2.21351
[1mStep[0m  [24/84], [94mLoss[0m : 2.12815
[1mStep[0m  [32/84], [94mLoss[0m : 2.19380
[1mStep[0m  [40/84], [94mLoss[0m : 2.16257
[1mStep[0m  [48/84], [94mLoss[0m : 2.28141
[1mStep[0m  [56/84], [94mLoss[0m : 2.46400
[1mStep[0m  [64/84], [94mLoss[0m : 2.02677
[1mStep[0m  [72/84], [94mLoss[0m : 2.26131
[1mStep[0m  [80/84], [94mLoss[0m : 2.00616

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36183
[1mStep[0m  [8/84], [94mLoss[0m : 2.17071
[1mStep[0m  [16/84], [94mLoss[0m : 2.13909
[1mStep[0m  [24/84], [94mLoss[0m : 2.11505
[1mStep[0m  [32/84], [94mLoss[0m : 1.90979
[1mStep[0m  [40/84], [94mLoss[0m : 2.34415
[1mStep[0m  [48/84], [94mLoss[0m : 2.24506
[1mStep[0m  [56/84], [94mLoss[0m : 2.10561
[1mStep[0m  [64/84], [94mLoss[0m : 2.27678
[1mStep[0m  [72/84], [94mLoss[0m : 2.15903
[1mStep[0m  [80/84], [94mLoss[0m : 2.05022

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03440
[1mStep[0m  [8/84], [94mLoss[0m : 2.06608
[1mStep[0m  [16/84], [94mLoss[0m : 2.16628
[1mStep[0m  [24/84], [94mLoss[0m : 2.16816
[1mStep[0m  [32/84], [94mLoss[0m : 2.24294
[1mStep[0m  [40/84], [94mLoss[0m : 2.01439
[1mStep[0m  [48/84], [94mLoss[0m : 2.18888
[1mStep[0m  [56/84], [94mLoss[0m : 2.27647
[1mStep[0m  [64/84], [94mLoss[0m : 2.04712
[1mStep[0m  [72/84], [94mLoss[0m : 1.93463
[1mStep[0m  [80/84], [94mLoss[0m : 2.21888

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11187
[1mStep[0m  [8/84], [94mLoss[0m : 1.79784
[1mStep[0m  [16/84], [94mLoss[0m : 2.11766
[1mStep[0m  [24/84], [94mLoss[0m : 1.97317
[1mStep[0m  [32/84], [94mLoss[0m : 1.88869
[1mStep[0m  [40/84], [94mLoss[0m : 1.99040
[1mStep[0m  [48/84], [94mLoss[0m : 1.84230
[1mStep[0m  [56/84], [94mLoss[0m : 1.89693
[1mStep[0m  [64/84], [94mLoss[0m : 2.07053
[1mStep[0m  [72/84], [94mLoss[0m : 2.13027
[1mStep[0m  [80/84], [94mLoss[0m : 2.20516

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.075, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08658
[1mStep[0m  [8/84], [94mLoss[0m : 2.10724
[1mStep[0m  [16/84], [94mLoss[0m : 1.96550
[1mStep[0m  [24/84], [94mLoss[0m : 2.16090
[1mStep[0m  [32/84], [94mLoss[0m : 1.93803
[1mStep[0m  [40/84], [94mLoss[0m : 2.05910
[1mStep[0m  [48/84], [94mLoss[0m : 2.12144
[1mStep[0m  [56/84], [94mLoss[0m : 2.01341
[1mStep[0m  [64/84], [94mLoss[0m : 1.95922
[1mStep[0m  [72/84], [94mLoss[0m : 1.93259
[1mStep[0m  [80/84], [94mLoss[0m : 2.08325

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93556
[1mStep[0m  [8/84], [94mLoss[0m : 1.99239
[1mStep[0m  [16/84], [94mLoss[0m : 1.91576
[1mStep[0m  [24/84], [94mLoss[0m : 2.01706
[1mStep[0m  [32/84], [94mLoss[0m : 1.89541
[1mStep[0m  [40/84], [94mLoss[0m : 2.26271
[1mStep[0m  [48/84], [94mLoss[0m : 2.36620
[1mStep[0m  [56/84], [94mLoss[0m : 1.79110
[1mStep[0m  [64/84], [94mLoss[0m : 2.09202
[1mStep[0m  [72/84], [94mLoss[0m : 2.22870
[1mStep[0m  [80/84], [94mLoss[0m : 2.03277

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09238
[1mStep[0m  [8/84], [94mLoss[0m : 2.00159
[1mStep[0m  [16/84], [94mLoss[0m : 1.68972
[1mStep[0m  [24/84], [94mLoss[0m : 1.94326
[1mStep[0m  [32/84], [94mLoss[0m : 1.78924
[1mStep[0m  [40/84], [94mLoss[0m : 1.92380
[1mStep[0m  [48/84], [94mLoss[0m : 1.86464
[1mStep[0m  [56/84], [94mLoss[0m : 2.03662
[1mStep[0m  [64/84], [94mLoss[0m : 2.02944
[1mStep[0m  [72/84], [94mLoss[0m : 2.03218
[1mStep[0m  [80/84], [94mLoss[0m : 1.77118

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28040
[1mStep[0m  [8/84], [94mLoss[0m : 2.12949
[1mStep[0m  [16/84], [94mLoss[0m : 1.60963
[1mStep[0m  [24/84], [94mLoss[0m : 1.79711
[1mStep[0m  [32/84], [94mLoss[0m : 1.82104
[1mStep[0m  [40/84], [94mLoss[0m : 2.05577
[1mStep[0m  [48/84], [94mLoss[0m : 1.70199
[1mStep[0m  [56/84], [94mLoss[0m : 1.86371
[1mStep[0m  [64/84], [94mLoss[0m : 1.87083
[1mStep[0m  [72/84], [94mLoss[0m : 1.92483
[1mStep[0m  [80/84], [94mLoss[0m : 1.98331

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.62445
[1mStep[0m  [8/84], [94mLoss[0m : 1.87438
[1mStep[0m  [16/84], [94mLoss[0m : 1.76620
[1mStep[0m  [24/84], [94mLoss[0m : 1.79188
[1mStep[0m  [32/84], [94mLoss[0m : 1.70240
[1mStep[0m  [40/84], [94mLoss[0m : 1.79427
[1mStep[0m  [48/84], [94mLoss[0m : 2.00015
[1mStep[0m  [56/84], [94mLoss[0m : 1.91190
[1mStep[0m  [64/84], [94mLoss[0m : 2.03780
[1mStep[0m  [72/84], [94mLoss[0m : 1.90867
[1mStep[0m  [80/84], [94mLoss[0m : 1.93387

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.873, [92mTest[0m: 2.503, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83146
[1mStep[0m  [8/84], [94mLoss[0m : 2.01194
[1mStep[0m  [16/84], [94mLoss[0m : 1.66030
[1mStep[0m  [24/84], [94mLoss[0m : 1.94804
[1mStep[0m  [32/84], [94mLoss[0m : 1.70891
[1mStep[0m  [40/84], [94mLoss[0m : 1.80218
[1mStep[0m  [48/84], [94mLoss[0m : 1.97812
[1mStep[0m  [56/84], [94mLoss[0m : 1.70897
[1mStep[0m  [64/84], [94mLoss[0m : 1.73756
[1mStep[0m  [72/84], [94mLoss[0m : 1.94874
[1mStep[0m  [80/84], [94mLoss[0m : 1.99005

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79399
[1mStep[0m  [8/84], [94mLoss[0m : 1.65230
[1mStep[0m  [16/84], [94mLoss[0m : 1.75763
[1mStep[0m  [24/84], [94mLoss[0m : 1.66702
[1mStep[0m  [32/84], [94mLoss[0m : 1.73745
[1mStep[0m  [40/84], [94mLoss[0m : 1.82737
[1mStep[0m  [48/84], [94mLoss[0m : 1.76672
[1mStep[0m  [56/84], [94mLoss[0m : 1.76331
[1mStep[0m  [64/84], [94mLoss[0m : 2.02088
[1mStep[0m  [72/84], [94mLoss[0m : 2.02079
[1mStep[0m  [80/84], [94mLoss[0m : 1.73213

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59274
[1mStep[0m  [8/84], [94mLoss[0m : 1.54727
[1mStep[0m  [16/84], [94mLoss[0m : 1.86962
[1mStep[0m  [24/84], [94mLoss[0m : 1.79192
[1mStep[0m  [32/84], [94mLoss[0m : 1.74573
[1mStep[0m  [40/84], [94mLoss[0m : 1.88498
[1mStep[0m  [48/84], [94mLoss[0m : 1.77053
[1mStep[0m  [56/84], [94mLoss[0m : 1.99974
[1mStep[0m  [64/84], [94mLoss[0m : 1.81957
[1mStep[0m  [72/84], [94mLoss[0m : 1.85889
[1mStep[0m  [80/84], [94mLoss[0m : 1.85461

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.769, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72654
[1mStep[0m  [8/84], [94mLoss[0m : 1.63451
[1mStep[0m  [16/84], [94mLoss[0m : 1.62915
[1mStep[0m  [24/84], [94mLoss[0m : 1.93340
[1mStep[0m  [32/84], [94mLoss[0m : 1.77089
[1mStep[0m  [40/84], [94mLoss[0m : 1.57435
[1mStep[0m  [48/84], [94mLoss[0m : 1.84279
[1mStep[0m  [56/84], [94mLoss[0m : 1.65191
[1mStep[0m  [64/84], [94mLoss[0m : 1.85484
[1mStep[0m  [72/84], [94mLoss[0m : 1.81445
[1mStep[0m  [80/84], [94mLoss[0m : 1.60012

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.732, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83799
[1mStep[0m  [8/84], [94mLoss[0m : 1.52216
[1mStep[0m  [16/84], [94mLoss[0m : 1.83773
[1mStep[0m  [24/84], [94mLoss[0m : 1.62704
[1mStep[0m  [32/84], [94mLoss[0m : 1.66966
[1mStep[0m  [40/84], [94mLoss[0m : 1.78405
[1mStep[0m  [48/84], [94mLoss[0m : 1.83465
[1mStep[0m  [56/84], [94mLoss[0m : 1.74411
[1mStep[0m  [64/84], [94mLoss[0m : 1.78569
[1mStep[0m  [72/84], [94mLoss[0m : 1.66432
[1mStep[0m  [80/84], [94mLoss[0m : 1.78384

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.518, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.27422
[1mStep[0m  [8/84], [94mLoss[0m : 1.76420
[1mStep[0m  [16/84], [94mLoss[0m : 1.76698
[1mStep[0m  [24/84], [94mLoss[0m : 1.57527
[1mStep[0m  [32/84], [94mLoss[0m : 1.68608
[1mStep[0m  [40/84], [94mLoss[0m : 1.63538
[1mStep[0m  [48/84], [94mLoss[0m : 1.63089
[1mStep[0m  [56/84], [94mLoss[0m : 1.54010
[1mStep[0m  [64/84], [94mLoss[0m : 1.83319
[1mStep[0m  [72/84], [94mLoss[0m : 1.71648
[1mStep[0m  [80/84], [94mLoss[0m : 1.57654

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56273
[1mStep[0m  [8/84], [94mLoss[0m : 1.80181
[1mStep[0m  [16/84], [94mLoss[0m : 1.44450
[1mStep[0m  [24/84], [94mLoss[0m : 1.63832
[1mStep[0m  [32/84], [94mLoss[0m : 1.63352
[1mStep[0m  [40/84], [94mLoss[0m : 1.49623
[1mStep[0m  [48/84], [94mLoss[0m : 1.68741
[1mStep[0m  [56/84], [94mLoss[0m : 1.88185
[1mStep[0m  [64/84], [94mLoss[0m : 1.76611
[1mStep[0m  [72/84], [94mLoss[0m : 1.52370
[1mStep[0m  [80/84], [94mLoss[0m : 1.83691

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48622
[1mStep[0m  [8/84], [94mLoss[0m : 1.39011
[1mStep[0m  [16/84], [94mLoss[0m : 1.52785
[1mStep[0m  [24/84], [94mLoss[0m : 1.66723
[1mStep[0m  [32/84], [94mLoss[0m : 1.86280
[1mStep[0m  [40/84], [94mLoss[0m : 1.57252
[1mStep[0m  [48/84], [94mLoss[0m : 1.71344
[1mStep[0m  [56/84], [94mLoss[0m : 1.73416
[1mStep[0m  [64/84], [94mLoss[0m : 1.72397
[1mStep[0m  [72/84], [94mLoss[0m : 1.55365
[1mStep[0m  [80/84], [94mLoss[0m : 1.54056

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61996
[1mStep[0m  [8/84], [94mLoss[0m : 1.44550
[1mStep[0m  [16/84], [94mLoss[0m : 1.62748
[1mStep[0m  [24/84], [94mLoss[0m : 1.58370
[1mStep[0m  [32/84], [94mLoss[0m : 1.69261
[1mStep[0m  [40/84], [94mLoss[0m : 1.52483
[1mStep[0m  [48/84], [94mLoss[0m : 1.67677
[1mStep[0m  [56/84], [94mLoss[0m : 1.60648
[1mStep[0m  [64/84], [94mLoss[0m : 1.51547
[1mStep[0m  [72/84], [94mLoss[0m : 1.55770
[1mStep[0m  [80/84], [94mLoss[0m : 1.62421

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48053
[1mStep[0m  [8/84], [94mLoss[0m : 1.61254
[1mStep[0m  [16/84], [94mLoss[0m : 1.40274
[1mStep[0m  [24/84], [94mLoss[0m : 1.59376
[1mStep[0m  [32/84], [94mLoss[0m : 1.49458
[1mStep[0m  [40/84], [94mLoss[0m : 1.55452
[1mStep[0m  [48/84], [94mLoss[0m : 1.63691
[1mStep[0m  [56/84], [94mLoss[0m : 1.53437
[1mStep[0m  [64/84], [94mLoss[0m : 1.61195
[1mStep[0m  [72/84], [94mLoss[0m : 1.50853
[1mStep[0m  [80/84], [94mLoss[0m : 1.70737

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.592, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.52907
[1mStep[0m  [8/84], [94mLoss[0m : 1.55098
[1mStep[0m  [16/84], [94mLoss[0m : 1.36071
[1mStep[0m  [24/84], [94mLoss[0m : 1.54800
[1mStep[0m  [32/84], [94mLoss[0m : 1.46849
[1mStep[0m  [40/84], [94mLoss[0m : 1.52826
[1mStep[0m  [48/84], [94mLoss[0m : 1.52852
[1mStep[0m  [56/84], [94mLoss[0m : 1.64438
[1mStep[0m  [64/84], [94mLoss[0m : 1.48267
[1mStep[0m  [72/84], [94mLoss[0m : 1.68292
[1mStep[0m  [80/84], [94mLoss[0m : 1.58275

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64799
[1mStep[0m  [8/84], [94mLoss[0m : 1.45503
[1mStep[0m  [16/84], [94mLoss[0m : 1.49852
[1mStep[0m  [24/84], [94mLoss[0m : 1.49138
[1mStep[0m  [32/84], [94mLoss[0m : 1.58634
[1mStep[0m  [40/84], [94mLoss[0m : 1.65039
[1mStep[0m  [48/84], [94mLoss[0m : 1.63267
[1mStep[0m  [56/84], [94mLoss[0m : 1.53441
[1mStep[0m  [64/84], [94mLoss[0m : 1.72842
[1mStep[0m  [72/84], [94mLoss[0m : 1.60207
[1mStep[0m  [80/84], [94mLoss[0m : 1.61757

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.556, [92mTest[0m: 2.471, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.54558
[1mStep[0m  [8/84], [94mLoss[0m : 1.70183
[1mStep[0m  [16/84], [94mLoss[0m : 1.42840
[1mStep[0m  [24/84], [94mLoss[0m : 1.34424
[1mStep[0m  [32/84], [94mLoss[0m : 1.44773
[1mStep[0m  [40/84], [94mLoss[0m : 1.49787
[1mStep[0m  [48/84], [94mLoss[0m : 1.53307
[1mStep[0m  [56/84], [94mLoss[0m : 1.50193
[1mStep[0m  [64/84], [94mLoss[0m : 1.57952
[1mStep[0m  [72/84], [94mLoss[0m : 1.75604
[1mStep[0m  [80/84], [94mLoss[0m : 1.54920

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.58960
[1mStep[0m  [8/84], [94mLoss[0m : 1.73091
[1mStep[0m  [16/84], [94mLoss[0m : 1.56061
[1mStep[0m  [24/84], [94mLoss[0m : 1.53460
[1mStep[0m  [32/84], [94mLoss[0m : 1.63918
[1mStep[0m  [40/84], [94mLoss[0m : 1.55862
[1mStep[0m  [48/84], [94mLoss[0m : 1.61552
[1mStep[0m  [56/84], [94mLoss[0m : 1.47929
[1mStep[0m  [64/84], [94mLoss[0m : 1.55364
[1mStep[0m  [72/84], [94mLoss[0m : 1.56493
[1mStep[0m  [80/84], [94mLoss[0m : 1.22744

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.550, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48676
[1mStep[0m  [8/84], [94mLoss[0m : 1.41648
[1mStep[0m  [16/84], [94mLoss[0m : 1.28294
[1mStep[0m  [24/84], [94mLoss[0m : 1.65645
[1mStep[0m  [32/84], [94mLoss[0m : 1.47485
[1mStep[0m  [40/84], [94mLoss[0m : 1.42935
[1mStep[0m  [48/84], [94mLoss[0m : 1.71902
[1mStep[0m  [56/84], [94mLoss[0m : 1.55363
[1mStep[0m  [64/84], [94mLoss[0m : 1.35261
[1mStep[0m  [72/84], [94mLoss[0m : 1.41618
[1mStep[0m  [80/84], [94mLoss[0m : 1.51032

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.532
====================================

Phase 2 - Evaluation MAE:  2.5323184302874973
MAE score P1       2.335264
MAE score P2       2.532318
loss               1.515988
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 26, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.16821
[1mStep[0m  [8/84], [94mLoss[0m : 9.79248
[1mStep[0m  [16/84], [94mLoss[0m : 6.78217
[1mStep[0m  [24/84], [94mLoss[0m : 3.78623
[1mStep[0m  [32/84], [94mLoss[0m : 3.00787
[1mStep[0m  [40/84], [94mLoss[0m : 3.56344
[1mStep[0m  [48/84], [94mLoss[0m : 3.02545
[1mStep[0m  [56/84], [94mLoss[0m : 3.25671
[1mStep[0m  [64/84], [94mLoss[0m : 3.16036
[1mStep[0m  [72/84], [94mLoss[0m : 3.07654
[1mStep[0m  [80/84], [94mLoss[0m : 2.81012

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.534, [92mTest[0m: 10.790, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.88826
[1mStep[0m  [8/84], [94mLoss[0m : 3.28067
[1mStep[0m  [16/84], [94mLoss[0m : 3.24626
[1mStep[0m  [24/84], [94mLoss[0m : 2.81582
[1mStep[0m  [32/84], [94mLoss[0m : 2.89328
[1mStep[0m  [40/84], [94mLoss[0m : 3.25752
[1mStep[0m  [48/84], [94mLoss[0m : 2.52636
[1mStep[0m  [56/84], [94mLoss[0m : 2.78969
[1mStep[0m  [64/84], [94mLoss[0m : 2.89785
[1mStep[0m  [72/84], [94mLoss[0m : 2.85346
[1mStep[0m  [80/84], [94mLoss[0m : 2.86144

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.852, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84254
[1mStep[0m  [8/84], [94mLoss[0m : 2.35197
[1mStep[0m  [16/84], [94mLoss[0m : 2.72427
[1mStep[0m  [24/84], [94mLoss[0m : 2.72342
[1mStep[0m  [32/84], [94mLoss[0m : 2.86581
[1mStep[0m  [40/84], [94mLoss[0m : 2.37383
[1mStep[0m  [48/84], [94mLoss[0m : 2.70902
[1mStep[0m  [56/84], [94mLoss[0m : 3.43691
[1mStep[0m  [64/84], [94mLoss[0m : 2.65990
[1mStep[0m  [72/84], [94mLoss[0m : 2.96284
[1mStep[0m  [80/84], [94mLoss[0m : 2.83147

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.809, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64184
[1mStep[0m  [8/84], [94mLoss[0m : 2.71919
[1mStep[0m  [16/84], [94mLoss[0m : 3.18317
[1mStep[0m  [24/84], [94mLoss[0m : 2.63506
[1mStep[0m  [32/84], [94mLoss[0m : 2.95683
[1mStep[0m  [40/84], [94mLoss[0m : 2.52960
[1mStep[0m  [48/84], [94mLoss[0m : 2.92010
[1mStep[0m  [56/84], [94mLoss[0m : 2.35293
[1mStep[0m  [64/84], [94mLoss[0m : 2.89828
[1mStep[0m  [72/84], [94mLoss[0m : 2.46797
[1mStep[0m  [80/84], [94mLoss[0m : 2.52064

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55250
[1mStep[0m  [8/84], [94mLoss[0m : 2.89740
[1mStep[0m  [16/84], [94mLoss[0m : 3.04838
[1mStep[0m  [24/84], [94mLoss[0m : 2.58974
[1mStep[0m  [32/84], [94mLoss[0m : 2.62987
[1mStep[0m  [40/84], [94mLoss[0m : 2.60269
[1mStep[0m  [48/84], [94mLoss[0m : 2.73393
[1mStep[0m  [56/84], [94mLoss[0m : 2.88702
[1mStep[0m  [64/84], [94mLoss[0m : 2.56880
[1mStep[0m  [72/84], [94mLoss[0m : 2.61003
[1mStep[0m  [80/84], [94mLoss[0m : 2.74786

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53115
[1mStep[0m  [8/84], [94mLoss[0m : 2.96014
[1mStep[0m  [16/84], [94mLoss[0m : 2.62038
[1mStep[0m  [24/84], [94mLoss[0m : 2.50251
[1mStep[0m  [32/84], [94mLoss[0m : 2.34301
[1mStep[0m  [40/84], [94mLoss[0m : 2.69211
[1mStep[0m  [48/84], [94mLoss[0m : 2.96682
[1mStep[0m  [56/84], [94mLoss[0m : 2.62212
[1mStep[0m  [64/84], [94mLoss[0m : 2.70704
[1mStep[0m  [72/84], [94mLoss[0m : 2.40303
[1mStep[0m  [80/84], [94mLoss[0m : 2.61052

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48885
[1mStep[0m  [8/84], [94mLoss[0m : 2.14194
[1mStep[0m  [16/84], [94mLoss[0m : 2.54998
[1mStep[0m  [24/84], [94mLoss[0m : 2.55359
[1mStep[0m  [32/84], [94mLoss[0m : 2.63676
[1mStep[0m  [40/84], [94mLoss[0m : 2.54904
[1mStep[0m  [48/84], [94mLoss[0m : 2.59256
[1mStep[0m  [56/84], [94mLoss[0m : 2.69649
[1mStep[0m  [64/84], [94mLoss[0m : 2.62291
[1mStep[0m  [72/84], [94mLoss[0m : 2.47308
[1mStep[0m  [80/84], [94mLoss[0m : 2.69656

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90169
[1mStep[0m  [8/84], [94mLoss[0m : 2.87631
[1mStep[0m  [16/84], [94mLoss[0m : 2.69462
[1mStep[0m  [24/84], [94mLoss[0m : 2.86349
[1mStep[0m  [32/84], [94mLoss[0m : 2.48498
[1mStep[0m  [40/84], [94mLoss[0m : 2.61838
[1mStep[0m  [48/84], [94mLoss[0m : 2.60942
[1mStep[0m  [56/84], [94mLoss[0m : 2.57866
[1mStep[0m  [64/84], [94mLoss[0m : 2.51497
[1mStep[0m  [72/84], [94mLoss[0m : 2.27081
[1mStep[0m  [80/84], [94mLoss[0m : 2.76719

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67926
[1mStep[0m  [8/84], [94mLoss[0m : 2.43119
[1mStep[0m  [16/84], [94mLoss[0m : 2.60053
[1mStep[0m  [24/84], [94mLoss[0m : 2.69987
[1mStep[0m  [32/84], [94mLoss[0m : 2.66598
[1mStep[0m  [40/84], [94mLoss[0m : 2.55618
[1mStep[0m  [48/84], [94mLoss[0m : 2.85951
[1mStep[0m  [56/84], [94mLoss[0m : 2.80952
[1mStep[0m  [64/84], [94mLoss[0m : 2.69132
[1mStep[0m  [72/84], [94mLoss[0m : 2.83266
[1mStep[0m  [80/84], [94mLoss[0m : 2.62976

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24999
[1mStep[0m  [8/84], [94mLoss[0m : 2.47662
[1mStep[0m  [16/84], [94mLoss[0m : 2.70969
[1mStep[0m  [24/84], [94mLoss[0m : 2.63275
[1mStep[0m  [32/84], [94mLoss[0m : 2.71748
[1mStep[0m  [40/84], [94mLoss[0m : 2.67444
[1mStep[0m  [48/84], [94mLoss[0m : 2.52920
[1mStep[0m  [56/84], [94mLoss[0m : 2.65822
[1mStep[0m  [64/84], [94mLoss[0m : 2.48702
[1mStep[0m  [72/84], [94mLoss[0m : 2.54743
[1mStep[0m  [80/84], [94mLoss[0m : 2.61567

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54949
[1mStep[0m  [8/84], [94mLoss[0m : 2.48469
[1mStep[0m  [16/84], [94mLoss[0m : 2.60475
[1mStep[0m  [24/84], [94mLoss[0m : 2.45560
[1mStep[0m  [32/84], [94mLoss[0m : 2.84475
[1mStep[0m  [40/84], [94mLoss[0m : 2.60204
[1mStep[0m  [48/84], [94mLoss[0m : 2.82772
[1mStep[0m  [56/84], [94mLoss[0m : 2.24011
[1mStep[0m  [64/84], [94mLoss[0m : 2.51142
[1mStep[0m  [72/84], [94mLoss[0m : 2.43370
[1mStep[0m  [80/84], [94mLoss[0m : 2.75877

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59112
[1mStep[0m  [8/84], [94mLoss[0m : 2.43310
[1mStep[0m  [16/84], [94mLoss[0m : 2.66310
[1mStep[0m  [24/84], [94mLoss[0m : 2.44135
[1mStep[0m  [32/84], [94mLoss[0m : 2.34280
[1mStep[0m  [40/84], [94mLoss[0m : 2.62594
[1mStep[0m  [48/84], [94mLoss[0m : 2.47343
[1mStep[0m  [56/84], [94mLoss[0m : 2.80556
[1mStep[0m  [64/84], [94mLoss[0m : 2.17953
[1mStep[0m  [72/84], [94mLoss[0m : 2.60993
[1mStep[0m  [80/84], [94mLoss[0m : 2.42388

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57246
[1mStep[0m  [8/84], [94mLoss[0m : 2.30092
[1mStep[0m  [16/84], [94mLoss[0m : 2.64751
[1mStep[0m  [24/84], [94mLoss[0m : 2.66007
[1mStep[0m  [32/84], [94mLoss[0m : 2.94368
[1mStep[0m  [40/84], [94mLoss[0m : 2.53311
[1mStep[0m  [48/84], [94mLoss[0m : 2.38169
[1mStep[0m  [56/84], [94mLoss[0m : 2.28841
[1mStep[0m  [64/84], [94mLoss[0m : 2.22946
[1mStep[0m  [72/84], [94mLoss[0m : 2.65782
[1mStep[0m  [80/84], [94mLoss[0m : 2.72495

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41499
[1mStep[0m  [8/84], [94mLoss[0m : 2.53099
[1mStep[0m  [16/84], [94mLoss[0m : 2.33057
[1mStep[0m  [24/84], [94mLoss[0m : 2.57187
[1mStep[0m  [32/84], [94mLoss[0m : 2.29612
[1mStep[0m  [40/84], [94mLoss[0m : 2.60858
[1mStep[0m  [48/84], [94mLoss[0m : 2.30782
[1mStep[0m  [56/84], [94mLoss[0m : 2.50425
[1mStep[0m  [64/84], [94mLoss[0m : 2.42794
[1mStep[0m  [72/84], [94mLoss[0m : 2.45521
[1mStep[0m  [80/84], [94mLoss[0m : 2.54196

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55543
[1mStep[0m  [8/84], [94mLoss[0m : 2.71738
[1mStep[0m  [16/84], [94mLoss[0m : 2.43419
[1mStep[0m  [24/84], [94mLoss[0m : 2.36155
[1mStep[0m  [32/84], [94mLoss[0m : 2.50490
[1mStep[0m  [40/84], [94mLoss[0m : 2.07535
[1mStep[0m  [48/84], [94mLoss[0m : 2.49524
[1mStep[0m  [56/84], [94mLoss[0m : 2.34363
[1mStep[0m  [64/84], [94mLoss[0m : 2.84934
[1mStep[0m  [72/84], [94mLoss[0m : 2.57122
[1mStep[0m  [80/84], [94mLoss[0m : 2.64083

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15192
[1mStep[0m  [8/84], [94mLoss[0m : 2.47927
[1mStep[0m  [16/84], [94mLoss[0m : 2.69788
[1mStep[0m  [24/84], [94mLoss[0m : 2.35772
[1mStep[0m  [32/84], [94mLoss[0m : 2.36547
[1mStep[0m  [40/84], [94mLoss[0m : 2.28434
[1mStep[0m  [48/84], [94mLoss[0m : 2.46910
[1mStep[0m  [56/84], [94mLoss[0m : 2.38319
[1mStep[0m  [64/84], [94mLoss[0m : 2.29829
[1mStep[0m  [72/84], [94mLoss[0m : 2.39098
[1mStep[0m  [80/84], [94mLoss[0m : 2.33635

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.307, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20371
[1mStep[0m  [8/84], [94mLoss[0m : 2.34850
[1mStep[0m  [16/84], [94mLoss[0m : 2.57111
[1mStep[0m  [24/84], [94mLoss[0m : 2.66487
[1mStep[0m  [32/84], [94mLoss[0m : 2.31333
[1mStep[0m  [40/84], [94mLoss[0m : 2.38575
[1mStep[0m  [48/84], [94mLoss[0m : 2.57737
[1mStep[0m  [56/84], [94mLoss[0m : 2.17369
[1mStep[0m  [64/84], [94mLoss[0m : 2.36069
[1mStep[0m  [72/84], [94mLoss[0m : 2.56951
[1mStep[0m  [80/84], [94mLoss[0m : 2.27287

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80545
[1mStep[0m  [8/84], [94mLoss[0m : 2.18884
[1mStep[0m  [16/84], [94mLoss[0m : 2.54517
[1mStep[0m  [24/84], [94mLoss[0m : 2.48709
[1mStep[0m  [32/84], [94mLoss[0m : 2.51758
[1mStep[0m  [40/84], [94mLoss[0m : 2.73383
[1mStep[0m  [48/84], [94mLoss[0m : 2.14279
[1mStep[0m  [56/84], [94mLoss[0m : 2.50727
[1mStep[0m  [64/84], [94mLoss[0m : 2.49236
[1mStep[0m  [72/84], [94mLoss[0m : 2.33202
[1mStep[0m  [80/84], [94mLoss[0m : 2.46299

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24024
[1mStep[0m  [8/84], [94mLoss[0m : 2.47555
[1mStep[0m  [16/84], [94mLoss[0m : 2.56286
[1mStep[0m  [24/84], [94mLoss[0m : 2.47901
[1mStep[0m  [32/84], [94mLoss[0m : 2.13665
[1mStep[0m  [40/84], [94mLoss[0m : 2.49804
[1mStep[0m  [48/84], [94mLoss[0m : 2.18773
[1mStep[0m  [56/84], [94mLoss[0m : 2.68599
[1mStep[0m  [64/84], [94mLoss[0m : 2.16224
[1mStep[0m  [72/84], [94mLoss[0m : 2.15233
[1mStep[0m  [80/84], [94mLoss[0m : 2.29952

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42385
[1mStep[0m  [8/84], [94mLoss[0m : 2.18705
[1mStep[0m  [16/84], [94mLoss[0m : 2.36043
[1mStep[0m  [24/84], [94mLoss[0m : 2.43229
[1mStep[0m  [32/84], [94mLoss[0m : 2.12046
[1mStep[0m  [40/84], [94mLoss[0m : 2.28523
[1mStep[0m  [48/84], [94mLoss[0m : 2.65972
[1mStep[0m  [56/84], [94mLoss[0m : 2.37208
[1mStep[0m  [64/84], [94mLoss[0m : 2.58807
[1mStep[0m  [72/84], [94mLoss[0m : 2.38396
[1mStep[0m  [80/84], [94mLoss[0m : 2.36611

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.311, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87256
[1mStep[0m  [8/84], [94mLoss[0m : 2.36022
[1mStep[0m  [16/84], [94mLoss[0m : 2.11269
[1mStep[0m  [24/84], [94mLoss[0m : 2.67025
[1mStep[0m  [32/84], [94mLoss[0m : 2.21974
[1mStep[0m  [40/84], [94mLoss[0m : 2.48719
[1mStep[0m  [48/84], [94mLoss[0m : 2.45620
[1mStep[0m  [56/84], [94mLoss[0m : 2.44260
[1mStep[0m  [64/84], [94mLoss[0m : 2.31030
[1mStep[0m  [72/84], [94mLoss[0m : 2.65664
[1mStep[0m  [80/84], [94mLoss[0m : 2.34596

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.313, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04650
[1mStep[0m  [8/84], [94mLoss[0m : 2.42180
[1mStep[0m  [16/84], [94mLoss[0m : 2.43282
[1mStep[0m  [24/84], [94mLoss[0m : 2.22581
[1mStep[0m  [32/84], [94mLoss[0m : 2.29363
[1mStep[0m  [40/84], [94mLoss[0m : 2.59186
[1mStep[0m  [48/84], [94mLoss[0m : 2.25589
[1mStep[0m  [56/84], [94mLoss[0m : 2.19951
[1mStep[0m  [64/84], [94mLoss[0m : 2.41711
[1mStep[0m  [72/84], [94mLoss[0m : 2.61783
[1mStep[0m  [80/84], [94mLoss[0m : 2.48435

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.311, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17783
[1mStep[0m  [8/84], [94mLoss[0m : 2.37882
[1mStep[0m  [16/84], [94mLoss[0m : 2.23992
[1mStep[0m  [24/84], [94mLoss[0m : 2.21589
[1mStep[0m  [32/84], [94mLoss[0m : 2.52492
[1mStep[0m  [40/84], [94mLoss[0m : 2.49211
[1mStep[0m  [48/84], [94mLoss[0m : 2.41209
[1mStep[0m  [56/84], [94mLoss[0m : 2.20788
[1mStep[0m  [64/84], [94mLoss[0m : 2.32655
[1mStep[0m  [72/84], [94mLoss[0m : 2.57125
[1mStep[0m  [80/84], [94mLoss[0m : 2.47532

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.307, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15486
[1mStep[0m  [8/84], [94mLoss[0m : 2.19012
[1mStep[0m  [16/84], [94mLoss[0m : 2.70168
[1mStep[0m  [24/84], [94mLoss[0m : 2.15095
[1mStep[0m  [32/84], [94mLoss[0m : 2.35109
[1mStep[0m  [40/84], [94mLoss[0m : 2.42911
[1mStep[0m  [48/84], [94mLoss[0m : 2.32237
[1mStep[0m  [56/84], [94mLoss[0m : 2.80835
[1mStep[0m  [64/84], [94mLoss[0m : 2.39832
[1mStep[0m  [72/84], [94mLoss[0m : 2.39559
[1mStep[0m  [80/84], [94mLoss[0m : 2.19058

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.299, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34520
[1mStep[0m  [8/84], [94mLoss[0m : 2.02076
[1mStep[0m  [16/84], [94mLoss[0m : 2.30900
[1mStep[0m  [24/84], [94mLoss[0m : 2.21213
[1mStep[0m  [32/84], [94mLoss[0m : 2.33283
[1mStep[0m  [40/84], [94mLoss[0m : 2.50048
[1mStep[0m  [48/84], [94mLoss[0m : 2.19397
[1mStep[0m  [56/84], [94mLoss[0m : 2.36176
[1mStep[0m  [64/84], [94mLoss[0m : 2.13190
[1mStep[0m  [72/84], [94mLoss[0m : 2.41451
[1mStep[0m  [80/84], [94mLoss[0m : 1.96811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.300, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60369
[1mStep[0m  [8/84], [94mLoss[0m : 2.84134
[1mStep[0m  [16/84], [94mLoss[0m : 2.39127
[1mStep[0m  [24/84], [94mLoss[0m : 2.41106
[1mStep[0m  [32/84], [94mLoss[0m : 2.52262
[1mStep[0m  [40/84], [94mLoss[0m : 1.88980
[1mStep[0m  [48/84], [94mLoss[0m : 2.35130
[1mStep[0m  [56/84], [94mLoss[0m : 2.52399
[1mStep[0m  [64/84], [94mLoss[0m : 2.45601
[1mStep[0m  [72/84], [94mLoss[0m : 2.54006
[1mStep[0m  [80/84], [94mLoss[0m : 2.24136

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.304, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26282
[1mStep[0m  [8/84], [94mLoss[0m : 2.42717
[1mStep[0m  [16/84], [94mLoss[0m : 2.39725
[1mStep[0m  [24/84], [94mLoss[0m : 2.54116
[1mStep[0m  [32/84], [94mLoss[0m : 2.13023
[1mStep[0m  [40/84], [94mLoss[0m : 2.48518
[1mStep[0m  [48/84], [94mLoss[0m : 2.00120
[1mStep[0m  [56/84], [94mLoss[0m : 1.97578
[1mStep[0m  [64/84], [94mLoss[0m : 2.43814
[1mStep[0m  [72/84], [94mLoss[0m : 2.33604
[1mStep[0m  [80/84], [94mLoss[0m : 2.53326

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.303, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43695
[1mStep[0m  [8/84], [94mLoss[0m : 2.23719
[1mStep[0m  [16/84], [94mLoss[0m : 2.19675
[1mStep[0m  [24/84], [94mLoss[0m : 2.41836
[1mStep[0m  [32/84], [94mLoss[0m : 2.15056
[1mStep[0m  [40/84], [94mLoss[0m : 2.34109
[1mStep[0m  [48/84], [94mLoss[0m : 2.32607
[1mStep[0m  [56/84], [94mLoss[0m : 2.34430
[1mStep[0m  [64/84], [94mLoss[0m : 2.56330
[1mStep[0m  [72/84], [94mLoss[0m : 2.41130
[1mStep[0m  [80/84], [94mLoss[0m : 2.31405

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.289, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01862
[1mStep[0m  [8/84], [94mLoss[0m : 2.23155
[1mStep[0m  [16/84], [94mLoss[0m : 2.51385
[1mStep[0m  [24/84], [94mLoss[0m : 2.24861
[1mStep[0m  [32/84], [94mLoss[0m : 2.54920
[1mStep[0m  [40/84], [94mLoss[0m : 1.99335
[1mStep[0m  [48/84], [94mLoss[0m : 2.51926
[1mStep[0m  [56/84], [94mLoss[0m : 2.52618
[1mStep[0m  [64/84], [94mLoss[0m : 2.27653
[1mStep[0m  [72/84], [94mLoss[0m : 2.53019
[1mStep[0m  [80/84], [94mLoss[0m : 2.38440

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.292, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53889
[1mStep[0m  [8/84], [94mLoss[0m : 2.43084
[1mStep[0m  [16/84], [94mLoss[0m : 2.46492
[1mStep[0m  [24/84], [94mLoss[0m : 2.27826
[1mStep[0m  [32/84], [94mLoss[0m : 2.46711
[1mStep[0m  [40/84], [94mLoss[0m : 2.73392
[1mStep[0m  [48/84], [94mLoss[0m : 2.35598
[1mStep[0m  [56/84], [94mLoss[0m : 2.33278
[1mStep[0m  [64/84], [94mLoss[0m : 2.25448
[1mStep[0m  [72/84], [94mLoss[0m : 2.31811
[1mStep[0m  [80/84], [94mLoss[0m : 2.54817

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.297, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.301
====================================

Phase 1 - Evaluation MAE:  2.30051594546863
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.30205
[1mStep[0m  [8/84], [94mLoss[0m : 2.06440
[1mStep[0m  [16/84], [94mLoss[0m : 2.29064
[1mStep[0m  [24/84], [94mLoss[0m : 2.58733
[1mStep[0m  [32/84], [94mLoss[0m : 2.53954
[1mStep[0m  [40/84], [94mLoss[0m : 2.72804
[1mStep[0m  [48/84], [94mLoss[0m : 2.59142
[1mStep[0m  [56/84], [94mLoss[0m : 2.40101
[1mStep[0m  [64/84], [94mLoss[0m : 2.59977
[1mStep[0m  [72/84], [94mLoss[0m : 2.73611
[1mStep[0m  [80/84], [94mLoss[0m : 2.67110

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.299, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41814
[1mStep[0m  [8/84], [94mLoss[0m : 2.05746
[1mStep[0m  [16/84], [94mLoss[0m : 2.26025
[1mStep[0m  [24/84], [94mLoss[0m : 2.33621
[1mStep[0m  [32/84], [94mLoss[0m : 2.19663
[1mStep[0m  [40/84], [94mLoss[0m : 2.31293
[1mStep[0m  [48/84], [94mLoss[0m : 2.33169
[1mStep[0m  [56/84], [94mLoss[0m : 2.27791
[1mStep[0m  [64/84], [94mLoss[0m : 2.39352
[1mStep[0m  [72/84], [94mLoss[0m : 2.25422
[1mStep[0m  [80/84], [94mLoss[0m : 2.75536

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00525
[1mStep[0m  [8/84], [94mLoss[0m : 2.30985
[1mStep[0m  [16/84], [94mLoss[0m : 2.43120
[1mStep[0m  [24/84], [94mLoss[0m : 1.84490
[1mStep[0m  [32/84], [94mLoss[0m : 2.05252
[1mStep[0m  [40/84], [94mLoss[0m : 2.08789
[1mStep[0m  [48/84], [94mLoss[0m : 2.35422
[1mStep[0m  [56/84], [94mLoss[0m : 2.27445
[1mStep[0m  [64/84], [94mLoss[0m : 2.24924
[1mStep[0m  [72/84], [94mLoss[0m : 2.06898
[1mStep[0m  [80/84], [94mLoss[0m : 2.32321

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31033
[1mStep[0m  [8/84], [94mLoss[0m : 2.12929
[1mStep[0m  [16/84], [94mLoss[0m : 2.01358
[1mStep[0m  [24/84], [94mLoss[0m : 2.25817
[1mStep[0m  [32/84], [94mLoss[0m : 2.21856
[1mStep[0m  [40/84], [94mLoss[0m : 2.30527
[1mStep[0m  [48/84], [94mLoss[0m : 2.23210
[1mStep[0m  [56/84], [94mLoss[0m : 2.03834
[1mStep[0m  [64/84], [94mLoss[0m : 2.05922
[1mStep[0m  [72/84], [94mLoss[0m : 2.27137
[1mStep[0m  [80/84], [94mLoss[0m : 2.20059

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36052
[1mStep[0m  [8/84], [94mLoss[0m : 1.92362
[1mStep[0m  [16/84], [94mLoss[0m : 1.88758
[1mStep[0m  [24/84], [94mLoss[0m : 2.21312
[1mStep[0m  [32/84], [94mLoss[0m : 2.26427
[1mStep[0m  [40/84], [94mLoss[0m : 2.11402
[1mStep[0m  [48/84], [94mLoss[0m : 1.96269
[1mStep[0m  [56/84], [94mLoss[0m : 2.51643
[1mStep[0m  [64/84], [94mLoss[0m : 2.42764
[1mStep[0m  [72/84], [94mLoss[0m : 1.89939
[1mStep[0m  [80/84], [94mLoss[0m : 2.04911

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.97042
[1mStep[0m  [8/84], [94mLoss[0m : 1.88982
[1mStep[0m  [16/84], [94mLoss[0m : 2.29805
[1mStep[0m  [24/84], [94mLoss[0m : 1.80695
[1mStep[0m  [32/84], [94mLoss[0m : 2.18433
[1mStep[0m  [40/84], [94mLoss[0m : 2.22312
[1mStep[0m  [48/84], [94mLoss[0m : 1.87515
[1mStep[0m  [56/84], [94mLoss[0m : 2.15152
[1mStep[0m  [64/84], [94mLoss[0m : 1.91468
[1mStep[0m  [72/84], [94mLoss[0m : 1.89918
[1mStep[0m  [80/84], [94mLoss[0m : 2.07034

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.053, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.77640
[1mStep[0m  [8/84], [94mLoss[0m : 1.84658
[1mStep[0m  [16/84], [94mLoss[0m : 1.97773
[1mStep[0m  [24/84], [94mLoss[0m : 2.04358
[1mStep[0m  [32/84], [94mLoss[0m : 2.14005
[1mStep[0m  [40/84], [94mLoss[0m : 1.90910
[1mStep[0m  [48/84], [94mLoss[0m : 2.20163
[1mStep[0m  [56/84], [94mLoss[0m : 2.05295
[1mStep[0m  [64/84], [94mLoss[0m : 2.12621
[1mStep[0m  [72/84], [94mLoss[0m : 1.81382
[1mStep[0m  [80/84], [94mLoss[0m : 2.14021

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90513
[1mStep[0m  [8/84], [94mLoss[0m : 1.96801
[1mStep[0m  [16/84], [94mLoss[0m : 1.85841
[1mStep[0m  [24/84], [94mLoss[0m : 1.94677
[1mStep[0m  [32/84], [94mLoss[0m : 1.82256
[1mStep[0m  [40/84], [94mLoss[0m : 2.33779
[1mStep[0m  [48/84], [94mLoss[0m : 1.91790
[1mStep[0m  [56/84], [94mLoss[0m : 1.93961
[1mStep[0m  [64/84], [94mLoss[0m : 1.96189
[1mStep[0m  [72/84], [94mLoss[0m : 1.92575
[1mStep[0m  [80/84], [94mLoss[0m : 2.00582

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95694
[1mStep[0m  [8/84], [94mLoss[0m : 1.67014
[1mStep[0m  [16/84], [94mLoss[0m : 1.74257
[1mStep[0m  [24/84], [94mLoss[0m : 1.80785
[1mStep[0m  [32/84], [94mLoss[0m : 1.79065
[1mStep[0m  [40/84], [94mLoss[0m : 2.17198
[1mStep[0m  [48/84], [94mLoss[0m : 2.14305
[1mStep[0m  [56/84], [94mLoss[0m : 2.12290
[1mStep[0m  [64/84], [94mLoss[0m : 1.80424
[1mStep[0m  [72/84], [94mLoss[0m : 1.85573
[1mStep[0m  [80/84], [94mLoss[0m : 2.00220

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86190
[1mStep[0m  [8/84], [94mLoss[0m : 1.78952
[1mStep[0m  [16/84], [94mLoss[0m : 1.81491
[1mStep[0m  [24/84], [94mLoss[0m : 1.74178
[1mStep[0m  [32/84], [94mLoss[0m : 1.73305
[1mStep[0m  [40/84], [94mLoss[0m : 1.95393
[1mStep[0m  [48/84], [94mLoss[0m : 1.81197
[1mStep[0m  [56/84], [94mLoss[0m : 2.04486
[1mStep[0m  [64/84], [94mLoss[0m : 1.79695
[1mStep[0m  [72/84], [94mLoss[0m : 1.96571
[1mStep[0m  [80/84], [94mLoss[0m : 2.02933

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65630
[1mStep[0m  [8/84], [94mLoss[0m : 1.53869
[1mStep[0m  [16/84], [94mLoss[0m : 1.63788
[1mStep[0m  [24/84], [94mLoss[0m : 1.87074
[1mStep[0m  [32/84], [94mLoss[0m : 1.75555
[1mStep[0m  [40/84], [94mLoss[0m : 1.70591
[1mStep[0m  [48/84], [94mLoss[0m : 1.62618
[1mStep[0m  [56/84], [94mLoss[0m : 1.76517
[1mStep[0m  [64/84], [94mLoss[0m : 1.90244
[1mStep[0m  [72/84], [94mLoss[0m : 1.59008
[1mStep[0m  [80/84], [94mLoss[0m : 1.98476

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.72591
[1mStep[0m  [8/84], [94mLoss[0m : 1.68665
[1mStep[0m  [16/84], [94mLoss[0m : 1.90687
[1mStep[0m  [24/84], [94mLoss[0m : 1.59166
[1mStep[0m  [32/84], [94mLoss[0m : 1.94979
[1mStep[0m  [40/84], [94mLoss[0m : 1.51910
[1mStep[0m  [48/84], [94mLoss[0m : 1.85743
[1mStep[0m  [56/84], [94mLoss[0m : 1.72999
[1mStep[0m  [64/84], [94mLoss[0m : 2.00339
[1mStep[0m  [72/84], [94mLoss[0m : 1.89609
[1mStep[0m  [80/84], [94mLoss[0m : 1.72540

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.55001
[1mStep[0m  [8/84], [94mLoss[0m : 1.70389
[1mStep[0m  [16/84], [94mLoss[0m : 1.58615
[1mStep[0m  [24/84], [94mLoss[0m : 1.72207
[1mStep[0m  [32/84], [94mLoss[0m : 1.67021
[1mStep[0m  [40/84], [94mLoss[0m : 1.50960
[1mStep[0m  [48/84], [94mLoss[0m : 1.54168
[1mStep[0m  [56/84], [94mLoss[0m : 1.68473
[1mStep[0m  [64/84], [94mLoss[0m : 1.79392
[1mStep[0m  [72/84], [94mLoss[0m : 1.63507
[1mStep[0m  [80/84], [94mLoss[0m : 1.79396

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82942
[1mStep[0m  [8/84], [94mLoss[0m : 1.76436
[1mStep[0m  [16/84], [94mLoss[0m : 1.56704
[1mStep[0m  [24/84], [94mLoss[0m : 1.44097
[1mStep[0m  [32/84], [94mLoss[0m : 1.64612
[1mStep[0m  [40/84], [94mLoss[0m : 1.76243
[1mStep[0m  [48/84], [94mLoss[0m : 1.90603
[1mStep[0m  [56/84], [94mLoss[0m : 1.61083
[1mStep[0m  [64/84], [94mLoss[0m : 1.77783
[1mStep[0m  [72/84], [94mLoss[0m : 1.68204
[1mStep[0m  [80/84], [94mLoss[0m : 2.02951

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.50115
[1mStep[0m  [8/84], [94mLoss[0m : 1.59455
[1mStep[0m  [16/84], [94mLoss[0m : 1.46721
[1mStep[0m  [24/84], [94mLoss[0m : 1.60082
[1mStep[0m  [32/84], [94mLoss[0m : 1.40743
[1mStep[0m  [40/84], [94mLoss[0m : 1.84028
[1mStep[0m  [48/84], [94mLoss[0m : 1.77139
[1mStep[0m  [56/84], [94mLoss[0m : 1.49684
[1mStep[0m  [64/84], [94mLoss[0m : 1.75855
[1mStep[0m  [72/84], [94mLoss[0m : 1.84086
[1mStep[0m  [80/84], [94mLoss[0m : 1.89763

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96724
[1mStep[0m  [8/84], [94mLoss[0m : 1.52836
[1mStep[0m  [16/84], [94mLoss[0m : 1.40760
[1mStep[0m  [24/84], [94mLoss[0m : 1.47496
[1mStep[0m  [32/84], [94mLoss[0m : 1.56308
[1mStep[0m  [40/84], [94mLoss[0m : 1.78803
[1mStep[0m  [48/84], [94mLoss[0m : 1.52838
[1mStep[0m  [56/84], [94mLoss[0m : 1.74391
[1mStep[0m  [64/84], [94mLoss[0m : 1.48919
[1mStep[0m  [72/84], [94mLoss[0m : 1.86638
[1mStep[0m  [80/84], [94mLoss[0m : 1.53974

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.529, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47943
[1mStep[0m  [8/84], [94mLoss[0m : 1.33446
[1mStep[0m  [16/84], [94mLoss[0m : 1.48829
[1mStep[0m  [24/84], [94mLoss[0m : 1.72114
[1mStep[0m  [32/84], [94mLoss[0m : 1.62741
[1mStep[0m  [40/84], [94mLoss[0m : 1.44622
[1mStep[0m  [48/84], [94mLoss[0m : 1.73082
[1mStep[0m  [56/84], [94mLoss[0m : 1.84955
[1mStep[0m  [64/84], [94mLoss[0m : 1.74704
[1mStep[0m  [72/84], [94mLoss[0m : 1.61423
[1mStep[0m  [80/84], [94mLoss[0m : 1.54660

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.590, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.40428
[1mStep[0m  [8/84], [94mLoss[0m : 1.73430
[1mStep[0m  [16/84], [94mLoss[0m : 1.65016
[1mStep[0m  [24/84], [94mLoss[0m : 1.36171
[1mStep[0m  [32/84], [94mLoss[0m : 1.57769
[1mStep[0m  [40/84], [94mLoss[0m : 1.50411
[1mStep[0m  [48/84], [94mLoss[0m : 1.83132
[1mStep[0m  [56/84], [94mLoss[0m : 1.56496
[1mStep[0m  [64/84], [94mLoss[0m : 1.59670
[1mStep[0m  [72/84], [94mLoss[0m : 1.68238
[1mStep[0m  [80/84], [94mLoss[0m : 1.81392

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.573, [92mTest[0m: 2.638, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.47485
[1mStep[0m  [8/84], [94mLoss[0m : 1.66260
[1mStep[0m  [16/84], [94mLoss[0m : 1.55899
[1mStep[0m  [24/84], [94mLoss[0m : 1.61946
[1mStep[0m  [32/84], [94mLoss[0m : 1.65002
[1mStep[0m  [40/84], [94mLoss[0m : 1.57202
[1mStep[0m  [48/84], [94mLoss[0m : 1.58001
[1mStep[0m  [56/84], [94mLoss[0m : 1.43668
[1mStep[0m  [64/84], [94mLoss[0m : 1.40513
[1mStep[0m  [72/84], [94mLoss[0m : 1.42309
[1mStep[0m  [80/84], [94mLoss[0m : 1.62448

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56067
[1mStep[0m  [8/84], [94mLoss[0m : 1.60794
[1mStep[0m  [16/84], [94mLoss[0m : 1.62947
[1mStep[0m  [24/84], [94mLoss[0m : 1.52435
[1mStep[0m  [32/84], [94mLoss[0m : 1.28159
[1mStep[0m  [40/84], [94mLoss[0m : 1.49038
[1mStep[0m  [48/84], [94mLoss[0m : 1.50400
[1mStep[0m  [56/84], [94mLoss[0m : 1.28707
[1mStep[0m  [64/84], [94mLoss[0m : 1.61190
[1mStep[0m  [72/84], [94mLoss[0m : 1.77161
[1mStep[0m  [80/84], [94mLoss[0m : 1.52239

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.44931
[1mStep[0m  [8/84], [94mLoss[0m : 1.47672
[1mStep[0m  [16/84], [94mLoss[0m : 1.38576
[1mStep[0m  [24/84], [94mLoss[0m : 1.64008
[1mStep[0m  [32/84], [94mLoss[0m : 1.50771
[1mStep[0m  [40/84], [94mLoss[0m : 1.50251
[1mStep[0m  [48/84], [94mLoss[0m : 1.43871
[1mStep[0m  [56/84], [94mLoss[0m : 1.41256
[1mStep[0m  [64/84], [94mLoss[0m : 1.57542
[1mStep[0m  [72/84], [94mLoss[0m : 1.60778
[1mStep[0m  [80/84], [94mLoss[0m : 1.67975

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.468, [92mTest[0m: 2.561, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.31810
[1mStep[0m  [8/84], [94mLoss[0m : 1.52322
[1mStep[0m  [16/84], [94mLoss[0m : 1.35290
[1mStep[0m  [24/84], [94mLoss[0m : 1.59300
[1mStep[0m  [32/84], [94mLoss[0m : 1.40182
[1mStep[0m  [40/84], [94mLoss[0m : 1.46214
[1mStep[0m  [48/84], [94mLoss[0m : 1.45306
[1mStep[0m  [56/84], [94mLoss[0m : 1.41454
[1mStep[0m  [64/84], [94mLoss[0m : 1.25208
[1mStep[0m  [72/84], [94mLoss[0m : 1.50557
[1mStep[0m  [80/84], [94mLoss[0m : 1.27775

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.532, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.33336
[1mStep[0m  [8/84], [94mLoss[0m : 1.56172
[1mStep[0m  [16/84], [94mLoss[0m : 1.22887
[1mStep[0m  [24/84], [94mLoss[0m : 1.43036
[1mStep[0m  [32/84], [94mLoss[0m : 1.45171
[1mStep[0m  [40/84], [94mLoss[0m : 1.34577
[1mStep[0m  [48/84], [94mLoss[0m : 1.39418
[1mStep[0m  [56/84], [94mLoss[0m : 1.58328
[1mStep[0m  [64/84], [94mLoss[0m : 1.47839
[1mStep[0m  [72/84], [94mLoss[0m : 1.37212
[1mStep[0m  [80/84], [94mLoss[0m : 1.67208

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.419, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.504
====================================

Phase 2 - Evaluation MAE:  2.504281665597643
MAE score P1       2.300516
MAE score P2       2.504282
loss                1.41889
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 27, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.20321
[1mStep[0m  [4/42], [94mLoss[0m : 9.91574
[1mStep[0m  [8/42], [94mLoss[0m : 8.06724
[1mStep[0m  [12/42], [94mLoss[0m : 6.73380
[1mStep[0m  [16/42], [94mLoss[0m : 5.62275
[1mStep[0m  [20/42], [94mLoss[0m : 4.28244
[1mStep[0m  [24/42], [94mLoss[0m : 3.80575
[1mStep[0m  [28/42], [94mLoss[0m : 3.28100
[1mStep[0m  [32/42], [94mLoss[0m : 2.81993
[1mStep[0m  [36/42], [94mLoss[0m : 2.69125
[1mStep[0m  [40/42], [94mLoss[0m : 2.77195

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.414, [92mTest[0m: 10.867, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40710
[1mStep[0m  [4/42], [94mLoss[0m : 3.00061
[1mStep[0m  [8/42], [94mLoss[0m : 2.57383
[1mStep[0m  [12/42], [94mLoss[0m : 2.73353
[1mStep[0m  [16/42], [94mLoss[0m : 2.44035
[1mStep[0m  [20/42], [94mLoss[0m : 2.70796
[1mStep[0m  [24/42], [94mLoss[0m : 2.40611
[1mStep[0m  [28/42], [94mLoss[0m : 2.46978
[1mStep[0m  [32/42], [94mLoss[0m : 2.87489
[1mStep[0m  [36/42], [94mLoss[0m : 2.53403
[1mStep[0m  [40/42], [94mLoss[0m : 2.72624

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.624, [92mTest[0m: 3.294, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36285
[1mStep[0m  [4/42], [94mLoss[0m : 2.55227
[1mStep[0m  [8/42], [94mLoss[0m : 2.70514
[1mStep[0m  [12/42], [94mLoss[0m : 2.55786
[1mStep[0m  [16/42], [94mLoss[0m : 2.48589
[1mStep[0m  [20/42], [94mLoss[0m : 2.65244
[1mStep[0m  [24/42], [94mLoss[0m : 2.60559
[1mStep[0m  [28/42], [94mLoss[0m : 2.41133
[1mStep[0m  [32/42], [94mLoss[0m : 2.25757
[1mStep[0m  [36/42], [94mLoss[0m : 2.54452
[1mStep[0m  [40/42], [94mLoss[0m : 2.27394

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.690, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60412
[1mStep[0m  [4/42], [94mLoss[0m : 2.51524
[1mStep[0m  [8/42], [94mLoss[0m : 2.61475
[1mStep[0m  [12/42], [94mLoss[0m : 2.92581
[1mStep[0m  [16/42], [94mLoss[0m : 2.53013
[1mStep[0m  [20/42], [94mLoss[0m : 2.72926
[1mStep[0m  [24/42], [94mLoss[0m : 2.38563
[1mStep[0m  [28/42], [94mLoss[0m : 2.66491
[1mStep[0m  [32/42], [94mLoss[0m : 2.53223
[1mStep[0m  [36/42], [94mLoss[0m : 2.33268
[1mStep[0m  [40/42], [94mLoss[0m : 2.69474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.654, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53590
[1mStep[0m  [4/42], [94mLoss[0m : 2.48536
[1mStep[0m  [8/42], [94mLoss[0m : 2.79658
[1mStep[0m  [12/42], [94mLoss[0m : 2.65952
[1mStep[0m  [16/42], [94mLoss[0m : 2.27315
[1mStep[0m  [20/42], [94mLoss[0m : 2.58003
[1mStep[0m  [24/42], [94mLoss[0m : 2.47784
[1mStep[0m  [28/42], [94mLoss[0m : 2.56547
[1mStep[0m  [32/42], [94mLoss[0m : 2.54251
[1mStep[0m  [36/42], [94mLoss[0m : 2.74803
[1mStep[0m  [40/42], [94mLoss[0m : 2.37519

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.560, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38385
[1mStep[0m  [4/42], [94mLoss[0m : 2.73202
[1mStep[0m  [8/42], [94mLoss[0m : 2.41528
[1mStep[0m  [12/42], [94mLoss[0m : 2.62975
[1mStep[0m  [16/42], [94mLoss[0m : 2.54683
[1mStep[0m  [20/42], [94mLoss[0m : 2.44400
[1mStep[0m  [24/42], [94mLoss[0m : 2.67728
[1mStep[0m  [28/42], [94mLoss[0m : 2.48069
[1mStep[0m  [32/42], [94mLoss[0m : 2.37314
[1mStep[0m  [36/42], [94mLoss[0m : 2.46840
[1mStep[0m  [40/42], [94mLoss[0m : 2.53040

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.555, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50574
[1mStep[0m  [4/42], [94mLoss[0m : 2.34829
[1mStep[0m  [8/42], [94mLoss[0m : 2.75148
[1mStep[0m  [12/42], [94mLoss[0m : 2.54110
[1mStep[0m  [16/42], [94mLoss[0m : 2.81823
[1mStep[0m  [20/42], [94mLoss[0m : 2.50311
[1mStep[0m  [24/42], [94mLoss[0m : 2.53043
[1mStep[0m  [28/42], [94mLoss[0m : 2.59764
[1mStep[0m  [32/42], [94mLoss[0m : 2.32209
[1mStep[0m  [36/42], [94mLoss[0m : 2.50204
[1mStep[0m  [40/42], [94mLoss[0m : 2.38334

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47725
[1mStep[0m  [4/42], [94mLoss[0m : 2.50637
[1mStep[0m  [8/42], [94mLoss[0m : 2.42747
[1mStep[0m  [12/42], [94mLoss[0m : 2.66266
[1mStep[0m  [16/42], [94mLoss[0m : 2.51397
[1mStep[0m  [20/42], [94mLoss[0m : 2.60658
[1mStep[0m  [24/42], [94mLoss[0m : 2.48879
[1mStep[0m  [28/42], [94mLoss[0m : 2.32419
[1mStep[0m  [32/42], [94mLoss[0m : 2.51986
[1mStep[0m  [36/42], [94mLoss[0m : 2.37416
[1mStep[0m  [40/42], [94mLoss[0m : 2.39041

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.567, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51102
[1mStep[0m  [4/42], [94mLoss[0m : 2.45437
[1mStep[0m  [8/42], [94mLoss[0m : 2.36993
[1mStep[0m  [12/42], [94mLoss[0m : 2.57598
[1mStep[0m  [16/42], [94mLoss[0m : 2.63258
[1mStep[0m  [20/42], [94mLoss[0m : 2.57373
[1mStep[0m  [24/42], [94mLoss[0m : 2.50140
[1mStep[0m  [28/42], [94mLoss[0m : 2.33275
[1mStep[0m  [32/42], [94mLoss[0m : 2.34179
[1mStep[0m  [36/42], [94mLoss[0m : 2.26549
[1mStep[0m  [40/42], [94mLoss[0m : 2.69518

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37336
[1mStep[0m  [4/42], [94mLoss[0m : 2.51226
[1mStep[0m  [8/42], [94mLoss[0m : 2.57897
[1mStep[0m  [12/42], [94mLoss[0m : 2.44219
[1mStep[0m  [16/42], [94mLoss[0m : 2.37876
[1mStep[0m  [20/42], [94mLoss[0m : 2.26500
[1mStep[0m  [24/42], [94mLoss[0m : 2.34471
[1mStep[0m  [28/42], [94mLoss[0m : 2.57565
[1mStep[0m  [32/42], [94mLoss[0m : 2.52167
[1mStep[0m  [36/42], [94mLoss[0m : 2.51274
[1mStep[0m  [40/42], [94mLoss[0m : 2.34498

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47549
[1mStep[0m  [4/42], [94mLoss[0m : 2.49012
[1mStep[0m  [8/42], [94mLoss[0m : 2.31728
[1mStep[0m  [12/42], [94mLoss[0m : 2.32250
[1mStep[0m  [16/42], [94mLoss[0m : 2.35632
[1mStep[0m  [20/42], [94mLoss[0m : 2.57262
[1mStep[0m  [24/42], [94mLoss[0m : 2.44685
[1mStep[0m  [28/42], [94mLoss[0m : 2.32167
[1mStep[0m  [32/42], [94mLoss[0m : 2.31963
[1mStep[0m  [36/42], [94mLoss[0m : 2.65898
[1mStep[0m  [40/42], [94mLoss[0m : 2.16426

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26807
[1mStep[0m  [4/42], [94mLoss[0m : 2.36693
[1mStep[0m  [8/42], [94mLoss[0m : 2.42963
[1mStep[0m  [12/42], [94mLoss[0m : 2.48036
[1mStep[0m  [16/42], [94mLoss[0m : 2.40919
[1mStep[0m  [20/42], [94mLoss[0m : 2.33857
[1mStep[0m  [24/42], [94mLoss[0m : 2.37898
[1mStep[0m  [28/42], [94mLoss[0m : 2.38684
[1mStep[0m  [32/42], [94mLoss[0m : 2.13399
[1mStep[0m  [36/42], [94mLoss[0m : 2.52868
[1mStep[0m  [40/42], [94mLoss[0m : 2.48680

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47360
[1mStep[0m  [4/42], [94mLoss[0m : 2.45392
[1mStep[0m  [8/42], [94mLoss[0m : 2.25211
[1mStep[0m  [12/42], [94mLoss[0m : 2.37487
[1mStep[0m  [16/42], [94mLoss[0m : 2.24837
[1mStep[0m  [20/42], [94mLoss[0m : 2.51367
[1mStep[0m  [24/42], [94mLoss[0m : 2.62813
[1mStep[0m  [28/42], [94mLoss[0m : 2.45852
[1mStep[0m  [32/42], [94mLoss[0m : 2.44654
[1mStep[0m  [36/42], [94mLoss[0m : 2.65805
[1mStep[0m  [40/42], [94mLoss[0m : 2.47567

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47995
[1mStep[0m  [4/42], [94mLoss[0m : 2.55021
[1mStep[0m  [8/42], [94mLoss[0m : 2.42312
[1mStep[0m  [12/42], [94mLoss[0m : 2.43221
[1mStep[0m  [16/42], [94mLoss[0m : 2.42241
[1mStep[0m  [20/42], [94mLoss[0m : 2.62949
[1mStep[0m  [24/42], [94mLoss[0m : 2.47185
[1mStep[0m  [28/42], [94mLoss[0m : 2.44036
[1mStep[0m  [32/42], [94mLoss[0m : 2.40206
[1mStep[0m  [36/42], [94mLoss[0m : 2.64963
[1mStep[0m  [40/42], [94mLoss[0m : 2.23895

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39627
[1mStep[0m  [4/42], [94mLoss[0m : 2.41489
[1mStep[0m  [8/42], [94mLoss[0m : 2.49918
[1mStep[0m  [12/42], [94mLoss[0m : 2.27870
[1mStep[0m  [16/42], [94mLoss[0m : 2.54455
[1mStep[0m  [20/42], [94mLoss[0m : 2.40057
[1mStep[0m  [24/42], [94mLoss[0m : 2.42752
[1mStep[0m  [28/42], [94mLoss[0m : 2.39716
[1mStep[0m  [32/42], [94mLoss[0m : 2.27480
[1mStep[0m  [36/42], [94mLoss[0m : 2.35072
[1mStep[0m  [40/42], [94mLoss[0m : 2.36901

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38753
[1mStep[0m  [4/42], [94mLoss[0m : 2.31616
[1mStep[0m  [8/42], [94mLoss[0m : 2.45714
[1mStep[0m  [12/42], [94mLoss[0m : 2.26844
[1mStep[0m  [16/42], [94mLoss[0m : 2.30399
[1mStep[0m  [20/42], [94mLoss[0m : 2.41277
[1mStep[0m  [24/42], [94mLoss[0m : 2.48398
[1mStep[0m  [28/42], [94mLoss[0m : 2.58265
[1mStep[0m  [32/42], [94mLoss[0m : 2.36693
[1mStep[0m  [36/42], [94mLoss[0m : 2.51814
[1mStep[0m  [40/42], [94mLoss[0m : 2.25184

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28451
[1mStep[0m  [4/42], [94mLoss[0m : 2.46754
[1mStep[0m  [8/42], [94mLoss[0m : 2.47454
[1mStep[0m  [12/42], [94mLoss[0m : 2.18835
[1mStep[0m  [16/42], [94mLoss[0m : 2.36264
[1mStep[0m  [20/42], [94mLoss[0m : 2.53862
[1mStep[0m  [24/42], [94mLoss[0m : 2.32355
[1mStep[0m  [28/42], [94mLoss[0m : 2.56067
[1mStep[0m  [32/42], [94mLoss[0m : 2.71554
[1mStep[0m  [36/42], [94mLoss[0m : 2.23364
[1mStep[0m  [40/42], [94mLoss[0m : 2.46285

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42422
[1mStep[0m  [4/42], [94mLoss[0m : 2.16901
[1mStep[0m  [8/42], [94mLoss[0m : 2.54559
[1mStep[0m  [12/42], [94mLoss[0m : 2.42169
[1mStep[0m  [16/42], [94mLoss[0m : 2.31050
[1mStep[0m  [20/42], [94mLoss[0m : 2.52205
[1mStep[0m  [24/42], [94mLoss[0m : 2.45921
[1mStep[0m  [28/42], [94mLoss[0m : 2.37592
[1mStep[0m  [32/42], [94mLoss[0m : 2.37154
[1mStep[0m  [36/42], [94mLoss[0m : 2.37857
[1mStep[0m  [40/42], [94mLoss[0m : 2.44168

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.525, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49714
[1mStep[0m  [4/42], [94mLoss[0m : 2.61617
[1mStep[0m  [8/42], [94mLoss[0m : 2.33481
[1mStep[0m  [12/42], [94mLoss[0m : 2.63398
[1mStep[0m  [16/42], [94mLoss[0m : 2.37300
[1mStep[0m  [20/42], [94mLoss[0m : 2.36003
[1mStep[0m  [24/42], [94mLoss[0m : 2.36645
[1mStep[0m  [28/42], [94mLoss[0m : 2.39855
[1mStep[0m  [32/42], [94mLoss[0m : 2.35427
[1mStep[0m  [36/42], [94mLoss[0m : 2.36363
[1mStep[0m  [40/42], [94mLoss[0m : 2.31513

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45514
[1mStep[0m  [4/42], [94mLoss[0m : 2.48741
[1mStep[0m  [8/42], [94mLoss[0m : 2.37204
[1mStep[0m  [12/42], [94mLoss[0m : 2.47080
[1mStep[0m  [16/42], [94mLoss[0m : 2.55877
[1mStep[0m  [20/42], [94mLoss[0m : 2.61022
[1mStep[0m  [24/42], [94mLoss[0m : 2.46155
[1mStep[0m  [28/42], [94mLoss[0m : 2.35551
[1mStep[0m  [32/42], [94mLoss[0m : 2.46522
[1mStep[0m  [36/42], [94mLoss[0m : 2.39847
[1mStep[0m  [40/42], [94mLoss[0m : 2.35946

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37476
[1mStep[0m  [4/42], [94mLoss[0m : 2.28627
[1mStep[0m  [8/42], [94mLoss[0m : 2.51731
[1mStep[0m  [12/42], [94mLoss[0m : 2.49233
[1mStep[0m  [16/42], [94mLoss[0m : 2.40440
[1mStep[0m  [20/42], [94mLoss[0m : 2.45515
[1mStep[0m  [24/42], [94mLoss[0m : 2.31389
[1mStep[0m  [28/42], [94mLoss[0m : 2.41328
[1mStep[0m  [32/42], [94mLoss[0m : 2.41825
[1mStep[0m  [36/42], [94mLoss[0m : 2.22717
[1mStep[0m  [40/42], [94mLoss[0m : 2.38363

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.534, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43123
[1mStep[0m  [4/42], [94mLoss[0m : 2.42584
[1mStep[0m  [8/42], [94mLoss[0m : 2.36159
[1mStep[0m  [12/42], [94mLoss[0m : 2.39257
[1mStep[0m  [16/42], [94mLoss[0m : 2.65610
[1mStep[0m  [20/42], [94mLoss[0m : 2.16186
[1mStep[0m  [24/42], [94mLoss[0m : 2.44421
[1mStep[0m  [28/42], [94mLoss[0m : 2.63837
[1mStep[0m  [32/42], [94mLoss[0m : 2.31677
[1mStep[0m  [36/42], [94mLoss[0m : 2.44721
[1mStep[0m  [40/42], [94mLoss[0m : 2.62679

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58883
[1mStep[0m  [4/42], [94mLoss[0m : 2.52970
[1mStep[0m  [8/42], [94mLoss[0m : 2.41050
[1mStep[0m  [12/42], [94mLoss[0m : 2.23189
[1mStep[0m  [16/42], [94mLoss[0m : 2.57866
[1mStep[0m  [20/42], [94mLoss[0m : 2.40424
[1mStep[0m  [24/42], [94mLoss[0m : 2.19496
[1mStep[0m  [28/42], [94mLoss[0m : 2.40802
[1mStep[0m  [32/42], [94mLoss[0m : 2.52801
[1mStep[0m  [36/42], [94mLoss[0m : 2.42324
[1mStep[0m  [40/42], [94mLoss[0m : 2.48202

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14449
[1mStep[0m  [4/42], [94mLoss[0m : 2.46872
[1mStep[0m  [8/42], [94mLoss[0m : 2.40941
[1mStep[0m  [12/42], [94mLoss[0m : 2.13612
[1mStep[0m  [16/42], [94mLoss[0m : 2.32597
[1mStep[0m  [20/42], [94mLoss[0m : 2.33582
[1mStep[0m  [24/42], [94mLoss[0m : 2.52044
[1mStep[0m  [28/42], [94mLoss[0m : 2.43924
[1mStep[0m  [32/42], [94mLoss[0m : 2.26984
[1mStep[0m  [36/42], [94mLoss[0m : 2.61407
[1mStep[0m  [40/42], [94mLoss[0m : 2.55521

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52277
[1mStep[0m  [4/42], [94mLoss[0m : 2.27682
[1mStep[0m  [8/42], [94mLoss[0m : 2.26103
[1mStep[0m  [12/42], [94mLoss[0m : 2.23993
[1mStep[0m  [16/42], [94mLoss[0m : 2.64788
[1mStep[0m  [20/42], [94mLoss[0m : 2.41020
[1mStep[0m  [24/42], [94mLoss[0m : 2.48649
[1mStep[0m  [28/42], [94mLoss[0m : 2.26967
[1mStep[0m  [32/42], [94mLoss[0m : 2.33808
[1mStep[0m  [36/42], [94mLoss[0m : 2.30011
[1mStep[0m  [40/42], [94mLoss[0m : 2.29284

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.425, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47717
[1mStep[0m  [4/42], [94mLoss[0m : 2.38939
[1mStep[0m  [8/42], [94mLoss[0m : 2.28029
[1mStep[0m  [12/42], [94mLoss[0m : 2.45909
[1mStep[0m  [16/42], [94mLoss[0m : 2.37186
[1mStep[0m  [20/42], [94mLoss[0m : 2.22403
[1mStep[0m  [24/42], [94mLoss[0m : 2.13859
[1mStep[0m  [28/42], [94mLoss[0m : 2.51676
[1mStep[0m  [32/42], [94mLoss[0m : 2.42116
[1mStep[0m  [36/42], [94mLoss[0m : 2.52238
[1mStep[0m  [40/42], [94mLoss[0m : 2.35062

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45747
[1mStep[0m  [4/42], [94mLoss[0m : 2.22418
[1mStep[0m  [8/42], [94mLoss[0m : 2.43005
[1mStep[0m  [12/42], [94mLoss[0m : 2.53914
[1mStep[0m  [16/42], [94mLoss[0m : 2.60534
[1mStep[0m  [20/42], [94mLoss[0m : 2.39153
[1mStep[0m  [24/42], [94mLoss[0m : 2.22819
[1mStep[0m  [28/42], [94mLoss[0m : 2.47096
[1mStep[0m  [32/42], [94mLoss[0m : 2.47810
[1mStep[0m  [36/42], [94mLoss[0m : 2.39995
[1mStep[0m  [40/42], [94mLoss[0m : 2.37377

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.457, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34058
[1mStep[0m  [4/42], [94mLoss[0m : 2.24551
[1mStep[0m  [8/42], [94mLoss[0m : 2.31927
[1mStep[0m  [12/42], [94mLoss[0m : 2.14955
[1mStep[0m  [16/42], [94mLoss[0m : 2.65414
[1mStep[0m  [20/42], [94mLoss[0m : 2.33741
[1mStep[0m  [24/42], [94mLoss[0m : 2.35910
[1mStep[0m  [28/42], [94mLoss[0m : 2.47404
[1mStep[0m  [32/42], [94mLoss[0m : 2.56458
[1mStep[0m  [36/42], [94mLoss[0m : 2.36233
[1mStep[0m  [40/42], [94mLoss[0m : 2.29609

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59051
[1mStep[0m  [4/42], [94mLoss[0m : 2.40364
[1mStep[0m  [8/42], [94mLoss[0m : 2.52070
[1mStep[0m  [12/42], [94mLoss[0m : 2.32113
[1mStep[0m  [16/42], [94mLoss[0m : 2.35687
[1mStep[0m  [20/42], [94mLoss[0m : 2.53399
[1mStep[0m  [24/42], [94mLoss[0m : 2.33609
[1mStep[0m  [28/42], [94mLoss[0m : 2.38720
[1mStep[0m  [32/42], [94mLoss[0m : 2.41307
[1mStep[0m  [36/42], [94mLoss[0m : 2.30183
[1mStep[0m  [40/42], [94mLoss[0m : 2.36297

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37874
[1mStep[0m  [4/42], [94mLoss[0m : 2.44004
[1mStep[0m  [8/42], [94mLoss[0m : 2.42175
[1mStep[0m  [12/42], [94mLoss[0m : 2.37604
[1mStep[0m  [16/42], [94mLoss[0m : 2.26318
[1mStep[0m  [20/42], [94mLoss[0m : 2.29074
[1mStep[0m  [24/42], [94mLoss[0m : 2.46167
[1mStep[0m  [28/42], [94mLoss[0m : 2.44902
[1mStep[0m  [32/42], [94mLoss[0m : 2.29957
[1mStep[0m  [36/42], [94mLoss[0m : 2.61562
[1mStep[0m  [40/42], [94mLoss[0m : 2.54208

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.433
====================================

Phase 1 - Evaluation MAE:  2.432936634336199
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.37741
[1mStep[0m  [4/42], [94mLoss[0m : 2.57706
[1mStep[0m  [8/42], [94mLoss[0m : 2.37745
[1mStep[0m  [12/42], [94mLoss[0m : 2.09124
[1mStep[0m  [16/42], [94mLoss[0m : 2.42579
[1mStep[0m  [20/42], [94mLoss[0m : 2.52212
[1mStep[0m  [24/42], [94mLoss[0m : 2.58359
[1mStep[0m  [28/42], [94mLoss[0m : 2.49215
[1mStep[0m  [32/42], [94mLoss[0m : 2.35409
[1mStep[0m  [36/42], [94mLoss[0m : 2.59542
[1mStep[0m  [40/42], [94mLoss[0m : 2.28571

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55386
[1mStep[0m  [4/42], [94mLoss[0m : 2.52661
[1mStep[0m  [8/42], [94mLoss[0m : 2.38966
[1mStep[0m  [12/42], [94mLoss[0m : 2.42565
[1mStep[0m  [16/42], [94mLoss[0m : 2.23520
[1mStep[0m  [20/42], [94mLoss[0m : 2.32374
[1mStep[0m  [24/42], [94mLoss[0m : 2.49059
[1mStep[0m  [28/42], [94mLoss[0m : 2.39750
[1mStep[0m  [32/42], [94mLoss[0m : 2.32909
[1mStep[0m  [36/42], [94mLoss[0m : 2.32005
[1mStep[0m  [40/42], [94mLoss[0m : 2.41873

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23640
[1mStep[0m  [4/42], [94mLoss[0m : 2.33407
[1mStep[0m  [8/42], [94mLoss[0m : 2.52830
[1mStep[0m  [12/42], [94mLoss[0m : 2.46637
[1mStep[0m  [16/42], [94mLoss[0m : 2.28123
[1mStep[0m  [20/42], [94mLoss[0m : 2.77166
[1mStep[0m  [24/42], [94mLoss[0m : 2.33450
[1mStep[0m  [28/42], [94mLoss[0m : 2.21932
[1mStep[0m  [32/42], [94mLoss[0m : 2.46785
[1mStep[0m  [36/42], [94mLoss[0m : 2.49640
[1mStep[0m  [40/42], [94mLoss[0m : 2.40481

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.735, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.15266
[1mStep[0m  [4/42], [94mLoss[0m : 2.36574
[1mStep[0m  [8/42], [94mLoss[0m : 2.24085
[1mStep[0m  [12/42], [94mLoss[0m : 2.34119
[1mStep[0m  [16/42], [94mLoss[0m : 2.41304
[1mStep[0m  [20/42], [94mLoss[0m : 2.20732
[1mStep[0m  [24/42], [94mLoss[0m : 2.10945
[1mStep[0m  [28/42], [94mLoss[0m : 2.44931
[1mStep[0m  [32/42], [94mLoss[0m : 2.34393
[1mStep[0m  [36/42], [94mLoss[0m : 2.25132
[1mStep[0m  [40/42], [94mLoss[0m : 2.35740

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.548, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30028
[1mStep[0m  [4/42], [94mLoss[0m : 2.25055
[1mStep[0m  [8/42], [94mLoss[0m : 2.25563
[1mStep[0m  [12/42], [94mLoss[0m : 2.31548
[1mStep[0m  [16/42], [94mLoss[0m : 2.29894
[1mStep[0m  [20/42], [94mLoss[0m : 2.34838
[1mStep[0m  [24/42], [94mLoss[0m : 2.15779
[1mStep[0m  [28/42], [94mLoss[0m : 2.37920
[1mStep[0m  [32/42], [94mLoss[0m : 2.28390
[1mStep[0m  [36/42], [94mLoss[0m : 2.25925
[1mStep[0m  [40/42], [94mLoss[0m : 2.06885

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.266, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27424
[1mStep[0m  [4/42], [94mLoss[0m : 2.19026
[1mStep[0m  [8/42], [94mLoss[0m : 2.32309
[1mStep[0m  [12/42], [94mLoss[0m : 2.23158
[1mStep[0m  [16/42], [94mLoss[0m : 2.31194
[1mStep[0m  [20/42], [94mLoss[0m : 2.17472
[1mStep[0m  [24/42], [94mLoss[0m : 2.28392
[1mStep[0m  [28/42], [94mLoss[0m : 2.18811
[1mStep[0m  [32/42], [94mLoss[0m : 2.30430
[1mStep[0m  [36/42], [94mLoss[0m : 2.16856
[1mStep[0m  [40/42], [94mLoss[0m : 2.28412

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.221, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.22489
[1mStep[0m  [4/42], [94mLoss[0m : 2.01798
[1mStep[0m  [8/42], [94mLoss[0m : 2.19423
[1mStep[0m  [12/42], [94mLoss[0m : 2.29645
[1mStep[0m  [16/42], [94mLoss[0m : 2.46165
[1mStep[0m  [20/42], [94mLoss[0m : 2.27996
[1mStep[0m  [24/42], [94mLoss[0m : 2.33952
[1mStep[0m  [28/42], [94mLoss[0m : 2.23149
[1mStep[0m  [32/42], [94mLoss[0m : 2.35802
[1mStep[0m  [36/42], [94mLoss[0m : 2.09404
[1mStep[0m  [40/42], [94mLoss[0m : 2.15866

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04885
[1mStep[0m  [4/42], [94mLoss[0m : 2.25448
[1mStep[0m  [8/42], [94mLoss[0m : 2.18005
[1mStep[0m  [12/42], [94mLoss[0m : 2.19615
[1mStep[0m  [16/42], [94mLoss[0m : 2.09952
[1mStep[0m  [20/42], [94mLoss[0m : 2.17004
[1mStep[0m  [24/42], [94mLoss[0m : 2.25345
[1mStep[0m  [28/42], [94mLoss[0m : 2.02040
[1mStep[0m  [32/42], [94mLoss[0m : 2.11288
[1mStep[0m  [36/42], [94mLoss[0m : 1.88440
[1mStep[0m  [40/42], [94mLoss[0m : 2.18715

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14335
[1mStep[0m  [4/42], [94mLoss[0m : 1.93045
[1mStep[0m  [8/42], [94mLoss[0m : 2.06221
[1mStep[0m  [12/42], [94mLoss[0m : 2.09141
[1mStep[0m  [16/42], [94mLoss[0m : 1.97884
[1mStep[0m  [20/42], [94mLoss[0m : 1.96691
[1mStep[0m  [24/42], [94mLoss[0m : 2.04294
[1mStep[0m  [28/42], [94mLoss[0m : 2.06759
[1mStep[0m  [32/42], [94mLoss[0m : 2.05665
[1mStep[0m  [36/42], [94mLoss[0m : 2.03845
[1mStep[0m  [40/42], [94mLoss[0m : 2.05402

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.00034
[1mStep[0m  [4/42], [94mLoss[0m : 1.96614
[1mStep[0m  [8/42], [94mLoss[0m : 2.00761
[1mStep[0m  [12/42], [94mLoss[0m : 1.92299
[1mStep[0m  [16/42], [94mLoss[0m : 2.19972
[1mStep[0m  [20/42], [94mLoss[0m : 1.99047
[1mStep[0m  [24/42], [94mLoss[0m : 1.91233
[1mStep[0m  [28/42], [94mLoss[0m : 2.37525
[1mStep[0m  [32/42], [94mLoss[0m : 2.07091
[1mStep[0m  [36/42], [94mLoss[0m : 2.21133
[1mStep[0m  [40/42], [94mLoss[0m : 2.02254

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04276
[1mStep[0m  [4/42], [94mLoss[0m : 1.98251
[1mStep[0m  [8/42], [94mLoss[0m : 2.10243
[1mStep[0m  [12/42], [94mLoss[0m : 1.77256
[1mStep[0m  [16/42], [94mLoss[0m : 1.85257
[1mStep[0m  [20/42], [94mLoss[0m : 2.11246
[1mStep[0m  [24/42], [94mLoss[0m : 2.17563
[1mStep[0m  [28/42], [94mLoss[0m : 2.11758
[1mStep[0m  [32/42], [94mLoss[0m : 2.03338
[1mStep[0m  [36/42], [94mLoss[0m : 2.01571
[1mStep[0m  [40/42], [94mLoss[0m : 2.00060

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14737
[1mStep[0m  [4/42], [94mLoss[0m : 1.83870
[1mStep[0m  [8/42], [94mLoss[0m : 1.90140
[1mStep[0m  [12/42], [94mLoss[0m : 1.95540
[1mStep[0m  [16/42], [94mLoss[0m : 2.14288
[1mStep[0m  [20/42], [94mLoss[0m : 1.90740
[1mStep[0m  [24/42], [94mLoss[0m : 1.95530
[1mStep[0m  [28/42], [94mLoss[0m : 1.88454
[1mStep[0m  [32/42], [94mLoss[0m : 2.00402
[1mStep[0m  [36/42], [94mLoss[0m : 1.84222
[1mStep[0m  [40/42], [94mLoss[0m : 1.98631

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.93559
[1mStep[0m  [4/42], [94mLoss[0m : 1.82747
[1mStep[0m  [8/42], [94mLoss[0m : 1.97122
[1mStep[0m  [12/42], [94mLoss[0m : 1.74374
[1mStep[0m  [16/42], [94mLoss[0m : 1.88026
[1mStep[0m  [20/42], [94mLoss[0m : 2.13681
[1mStep[0m  [24/42], [94mLoss[0m : 2.01096
[1mStep[0m  [28/42], [94mLoss[0m : 1.73214
[1mStep[0m  [32/42], [94mLoss[0m : 1.91505
[1mStep[0m  [36/42], [94mLoss[0m : 1.87402
[1mStep[0m  [40/42], [94mLoss[0m : 1.98475

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90186
[1mStep[0m  [4/42], [94mLoss[0m : 1.88458
[1mStep[0m  [8/42], [94mLoss[0m : 1.79787
[1mStep[0m  [12/42], [94mLoss[0m : 2.16803
[1mStep[0m  [16/42], [94mLoss[0m : 1.88703
[1mStep[0m  [20/42], [94mLoss[0m : 1.85831
[1mStep[0m  [24/42], [94mLoss[0m : 1.76222
[1mStep[0m  [28/42], [94mLoss[0m : 1.80302
[1mStep[0m  [32/42], [94mLoss[0m : 1.92718
[1mStep[0m  [36/42], [94mLoss[0m : 1.88744
[1mStep[0m  [40/42], [94mLoss[0m : 2.05989

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.876, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.89776
[1mStep[0m  [4/42], [94mLoss[0m : 1.77434
[1mStep[0m  [8/42], [94mLoss[0m : 1.72280
[1mStep[0m  [12/42], [94mLoss[0m : 1.71478
[1mStep[0m  [16/42], [94mLoss[0m : 1.93806
[1mStep[0m  [20/42], [94mLoss[0m : 1.90668
[1mStep[0m  [24/42], [94mLoss[0m : 2.14974
[1mStep[0m  [28/42], [94mLoss[0m : 1.84904
[1mStep[0m  [32/42], [94mLoss[0m : 1.78271
[1mStep[0m  [36/42], [94mLoss[0m : 1.96315
[1mStep[0m  [40/42], [94mLoss[0m : 1.74236

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.637, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72304
[1mStep[0m  [4/42], [94mLoss[0m : 1.69236
[1mStep[0m  [8/42], [94mLoss[0m : 1.86460
[1mStep[0m  [12/42], [94mLoss[0m : 1.77160
[1mStep[0m  [16/42], [94mLoss[0m : 1.75433
[1mStep[0m  [20/42], [94mLoss[0m : 1.72452
[1mStep[0m  [24/42], [94mLoss[0m : 1.67066
[1mStep[0m  [28/42], [94mLoss[0m : 1.85788
[1mStep[0m  [32/42], [94mLoss[0m : 1.78191
[1mStep[0m  [36/42], [94mLoss[0m : 1.76484
[1mStep[0m  [40/42], [94mLoss[0m : 1.87976

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.585, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.65440
[1mStep[0m  [4/42], [94mLoss[0m : 1.75887
[1mStep[0m  [8/42], [94mLoss[0m : 1.83768
[1mStep[0m  [12/42], [94mLoss[0m : 1.85945
[1mStep[0m  [16/42], [94mLoss[0m : 1.67118
[1mStep[0m  [20/42], [94mLoss[0m : 1.66645
[1mStep[0m  [24/42], [94mLoss[0m : 1.87494
[1mStep[0m  [28/42], [94mLoss[0m : 1.86660
[1mStep[0m  [32/42], [94mLoss[0m : 1.70092
[1mStep[0m  [36/42], [94mLoss[0m : 1.85749
[1mStep[0m  [40/42], [94mLoss[0m : 1.66796

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.68102
[1mStep[0m  [4/42], [94mLoss[0m : 1.59615
[1mStep[0m  [8/42], [94mLoss[0m : 1.76073
[1mStep[0m  [12/42], [94mLoss[0m : 1.62487
[1mStep[0m  [16/42], [94mLoss[0m : 1.88458
[1mStep[0m  [20/42], [94mLoss[0m : 1.74661
[1mStep[0m  [24/42], [94mLoss[0m : 1.92868
[1mStep[0m  [28/42], [94mLoss[0m : 1.74951
[1mStep[0m  [32/42], [94mLoss[0m : 1.85174
[1mStep[0m  [36/42], [94mLoss[0m : 1.65519
[1mStep[0m  [40/42], [94mLoss[0m : 1.72539

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.608, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66000
[1mStep[0m  [4/42], [94mLoss[0m : 1.73585
[1mStep[0m  [8/42], [94mLoss[0m : 1.56853
[1mStep[0m  [12/42], [94mLoss[0m : 1.84756
[1mStep[0m  [16/42], [94mLoss[0m : 1.63925
[1mStep[0m  [20/42], [94mLoss[0m : 1.73657
[1mStep[0m  [24/42], [94mLoss[0m : 1.67230
[1mStep[0m  [28/42], [94mLoss[0m : 1.62757
[1mStep[0m  [32/42], [94mLoss[0m : 1.51772
[1mStep[0m  [36/42], [94mLoss[0m : 1.69786
[1mStep[0m  [40/42], [94mLoss[0m : 1.76235

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.538, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.67370
[1mStep[0m  [4/42], [94mLoss[0m : 1.70953
[1mStep[0m  [8/42], [94mLoss[0m : 1.81047
[1mStep[0m  [12/42], [94mLoss[0m : 1.70259
[1mStep[0m  [16/42], [94mLoss[0m : 1.76023
[1mStep[0m  [20/42], [94mLoss[0m : 1.73078
[1mStep[0m  [24/42], [94mLoss[0m : 1.67553
[1mStep[0m  [28/42], [94mLoss[0m : 1.82840
[1mStep[0m  [32/42], [94mLoss[0m : 1.75944
[1mStep[0m  [36/42], [94mLoss[0m : 1.46955
[1mStep[0m  [40/42], [94mLoss[0m : 1.68689

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.601, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.77877
[1mStep[0m  [4/42], [94mLoss[0m : 1.65510
[1mStep[0m  [8/42], [94mLoss[0m : 1.56914
[1mStep[0m  [12/42], [94mLoss[0m : 1.56745
[1mStep[0m  [16/42], [94mLoss[0m : 1.92948
[1mStep[0m  [20/42], [94mLoss[0m : 1.60229
[1mStep[0m  [24/42], [94mLoss[0m : 1.71827
[1mStep[0m  [28/42], [94mLoss[0m : 1.69598
[1mStep[0m  [32/42], [94mLoss[0m : 1.54345
[1mStep[0m  [36/42], [94mLoss[0m : 1.67964
[1mStep[0m  [40/42], [94mLoss[0m : 1.80039

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.642, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.56977
[1mStep[0m  [4/42], [94mLoss[0m : 1.52061
[1mStep[0m  [8/42], [94mLoss[0m : 1.68759
[1mStep[0m  [12/42], [94mLoss[0m : 1.64243
[1mStep[0m  [16/42], [94mLoss[0m : 1.53718
[1mStep[0m  [20/42], [94mLoss[0m : 1.62754
[1mStep[0m  [24/42], [94mLoss[0m : 1.66211
[1mStep[0m  [28/42], [94mLoss[0m : 1.58903
[1mStep[0m  [32/42], [94mLoss[0m : 1.77164
[1mStep[0m  [36/42], [94mLoss[0m : 1.84818
[1mStep[0m  [40/42], [94mLoss[0m : 1.71983

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.549, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.58910
[1mStep[0m  [4/42], [94mLoss[0m : 1.71795
[1mStep[0m  [8/42], [94mLoss[0m : 1.58605
[1mStep[0m  [12/42], [94mLoss[0m : 1.73034
[1mStep[0m  [16/42], [94mLoss[0m : 1.71144
[1mStep[0m  [20/42], [94mLoss[0m : 1.65036
[1mStep[0m  [24/42], [94mLoss[0m : 1.77252
[1mStep[0m  [28/42], [94mLoss[0m : 1.57404
[1mStep[0m  [32/42], [94mLoss[0m : 1.71541
[1mStep[0m  [36/42], [94mLoss[0m : 1.63337
[1mStep[0m  [40/42], [94mLoss[0m : 1.52328

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.580, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.46858
[1mStep[0m  [4/42], [94mLoss[0m : 1.45520
[1mStep[0m  [8/42], [94mLoss[0m : 1.39261
[1mStep[0m  [12/42], [94mLoss[0m : 1.57993
[1mStep[0m  [16/42], [94mLoss[0m : 1.39769
[1mStep[0m  [20/42], [94mLoss[0m : 1.64687
[1mStep[0m  [24/42], [94mLoss[0m : 1.52676
[1mStep[0m  [28/42], [94mLoss[0m : 1.59788
[1mStep[0m  [32/42], [94mLoss[0m : 1.49198
[1mStep[0m  [36/42], [94mLoss[0m : 1.64886
[1mStep[0m  [40/42], [94mLoss[0m : 1.40576

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.545, [92mTest[0m: 2.574, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.52587
[1mStep[0m  [4/42], [94mLoss[0m : 1.50407
[1mStep[0m  [8/42], [94mLoss[0m : 1.54100
[1mStep[0m  [12/42], [94mLoss[0m : 1.44294
[1mStep[0m  [16/42], [94mLoss[0m : 1.50310
[1mStep[0m  [20/42], [94mLoss[0m : 1.45195
[1mStep[0m  [24/42], [94mLoss[0m : 1.54217
[1mStep[0m  [28/42], [94mLoss[0m : 1.59669
[1mStep[0m  [32/42], [94mLoss[0m : 1.56179
[1mStep[0m  [36/42], [94mLoss[0m : 1.60180
[1mStep[0m  [40/42], [94mLoss[0m : 1.51991

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.646, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.55153
[1mStep[0m  [4/42], [94mLoss[0m : 1.53379
[1mStep[0m  [8/42], [94mLoss[0m : 1.55268
[1mStep[0m  [12/42], [94mLoss[0m : 1.44212
[1mStep[0m  [16/42], [94mLoss[0m : 1.42970
[1mStep[0m  [20/42], [94mLoss[0m : 1.55091
[1mStep[0m  [24/42], [94mLoss[0m : 1.48804
[1mStep[0m  [28/42], [94mLoss[0m : 1.57517
[1mStep[0m  [32/42], [94mLoss[0m : 1.45752
[1mStep[0m  [36/42], [94mLoss[0m : 1.45094
[1mStep[0m  [40/42], [94mLoss[0m : 1.50734

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.596, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.50723
[1mStep[0m  [4/42], [94mLoss[0m : 1.42147
[1mStep[0m  [8/42], [94mLoss[0m : 1.44680
[1mStep[0m  [12/42], [94mLoss[0m : 1.42511
[1mStep[0m  [16/42], [94mLoss[0m : 1.44609
[1mStep[0m  [20/42], [94mLoss[0m : 1.40087
[1mStep[0m  [24/42], [94mLoss[0m : 1.48388
[1mStep[0m  [28/42], [94mLoss[0m : 1.54500
[1mStep[0m  [32/42], [94mLoss[0m : 1.45610
[1mStep[0m  [36/42], [94mLoss[0m : 1.56162
[1mStep[0m  [40/42], [94mLoss[0m : 1.45346

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.620, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.648
====================================

Phase 2 - Evaluation MAE:  2.648450493812561
MAE score P1      2.432937
MAE score P2       2.64845
loss              1.500726
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 28, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.11772
[1mStep[0m  [8/84], [94mLoss[0m : 10.94740
[1mStep[0m  [16/84], [94mLoss[0m : 10.35021
[1mStep[0m  [24/84], [94mLoss[0m : 10.90241
[1mStep[0m  [32/84], [94mLoss[0m : 10.97589
[1mStep[0m  [40/84], [94mLoss[0m : 11.14996
[1mStep[0m  [48/84], [94mLoss[0m : 10.70487
[1mStep[0m  [56/84], [94mLoss[0m : 10.92063
[1mStep[0m  [64/84], [94mLoss[0m : 10.79625
[1mStep[0m  [72/84], [94mLoss[0m : 10.55628
[1mStep[0m  [80/84], [94mLoss[0m : 10.93101

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.925, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.38237
[1mStep[0m  [8/84], [94mLoss[0m : 10.16346
[1mStep[0m  [16/84], [94mLoss[0m : 10.26652
[1mStep[0m  [24/84], [94mLoss[0m : 10.60460
[1mStep[0m  [32/84], [94mLoss[0m : 10.45854
[1mStep[0m  [40/84], [94mLoss[0m : 10.09444
[1mStep[0m  [48/84], [94mLoss[0m : 10.19614
[1mStep[0m  [56/84], [94mLoss[0m : 10.52245
[1mStep[0m  [64/84], [94mLoss[0m : 10.05339
[1mStep[0m  [72/84], [94mLoss[0m : 10.24760
[1mStep[0m  [80/84], [94mLoss[0m : 10.17139

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.113, [92mTest[0m: 10.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.62817
[1mStep[0m  [8/84], [94mLoss[0m : 9.95279
[1mStep[0m  [16/84], [94mLoss[0m : 9.86959
[1mStep[0m  [24/84], [94mLoss[0m : 10.25733
[1mStep[0m  [32/84], [94mLoss[0m : 9.74774
[1mStep[0m  [40/84], [94mLoss[0m : 10.02290
[1mStep[0m  [48/84], [94mLoss[0m : 9.52619
[1mStep[0m  [56/84], [94mLoss[0m : 9.29790
[1mStep[0m  [64/84], [94mLoss[0m : 8.37050
[1mStep[0m  [72/84], [94mLoss[0m : 9.29619
[1mStep[0m  [80/84], [94mLoss[0m : 8.87120

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.504, [92mTest[0m: 9.720, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.37438
[1mStep[0m  [8/84], [94mLoss[0m : 8.93361
[1mStep[0m  [16/84], [94mLoss[0m : 8.67966
[1mStep[0m  [24/84], [94mLoss[0m : 8.98004
[1mStep[0m  [32/84], [94mLoss[0m : 8.38306
[1mStep[0m  [40/84], [94mLoss[0m : 9.13561
[1mStep[0m  [48/84], [94mLoss[0m : 8.37305
[1mStep[0m  [56/84], [94mLoss[0m : 8.49695
[1mStep[0m  [64/84], [94mLoss[0m : 8.69299
[1mStep[0m  [72/84], [94mLoss[0m : 8.27176
[1mStep[0m  [80/84], [94mLoss[0m : 8.59139

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.787, [92mTest[0m: 8.946, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.28468
[1mStep[0m  [8/84], [94mLoss[0m : 8.56144
[1mStep[0m  [16/84], [94mLoss[0m : 7.89089
[1mStep[0m  [24/84], [94mLoss[0m : 8.13442
[1mStep[0m  [32/84], [94mLoss[0m : 8.14269
[1mStep[0m  [40/84], [94mLoss[0m : 7.36141
[1mStep[0m  [48/84], [94mLoss[0m : 7.19445
[1mStep[0m  [56/84], [94mLoss[0m : 7.54329
[1mStep[0m  [64/84], [94mLoss[0m : 7.36879
[1mStep[0m  [72/84], [94mLoss[0m : 7.42169
[1mStep[0m  [80/84], [94mLoss[0m : 7.50095

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.840, [92mTest[0m: 8.065, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.07592
[1mStep[0m  [8/84], [94mLoss[0m : 7.39916
[1mStep[0m  [16/84], [94mLoss[0m : 7.76349
[1mStep[0m  [24/84], [94mLoss[0m : 6.98719
[1mStep[0m  [32/84], [94mLoss[0m : 6.49788
[1mStep[0m  [40/84], [94mLoss[0m : 6.71527
[1mStep[0m  [48/84], [94mLoss[0m : 6.99733
[1mStep[0m  [56/84], [94mLoss[0m : 6.22226
[1mStep[0m  [64/84], [94mLoss[0m : 6.78816
[1mStep[0m  [72/84], [94mLoss[0m : 5.91872
[1mStep[0m  [80/84], [94mLoss[0m : 6.44005

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.708, [92mTest[0m: 6.655, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.65694
[1mStep[0m  [8/84], [94mLoss[0m : 5.84390
[1mStep[0m  [16/84], [94mLoss[0m : 5.62070
[1mStep[0m  [24/84], [94mLoss[0m : 5.50176
[1mStep[0m  [32/84], [94mLoss[0m : 5.90839
[1mStep[0m  [40/84], [94mLoss[0m : 5.67529
[1mStep[0m  [48/84], [94mLoss[0m : 5.78103
[1mStep[0m  [56/84], [94mLoss[0m : 5.73516
[1mStep[0m  [64/84], [94mLoss[0m : 5.04830
[1mStep[0m  [72/84], [94mLoss[0m : 5.11392
[1mStep[0m  [80/84], [94mLoss[0m : 5.26222

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.661, [92mTest[0m: 5.499, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.93643
[1mStep[0m  [8/84], [94mLoss[0m : 5.05413
[1mStep[0m  [16/84], [94mLoss[0m : 4.72912
[1mStep[0m  [24/84], [94mLoss[0m : 4.81563
[1mStep[0m  [32/84], [94mLoss[0m : 4.75073
[1mStep[0m  [40/84], [94mLoss[0m : 4.40118
[1mStep[0m  [48/84], [94mLoss[0m : 4.89928
[1mStep[0m  [56/84], [94mLoss[0m : 4.89232
[1mStep[0m  [64/84], [94mLoss[0m : 4.63349
[1mStep[0m  [72/84], [94mLoss[0m : 4.66624
[1mStep[0m  [80/84], [94mLoss[0m : 4.69435

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.789, [92mTest[0m: 4.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.15255
[1mStep[0m  [8/84], [94mLoss[0m : 4.77979
[1mStep[0m  [16/84], [94mLoss[0m : 3.97836
[1mStep[0m  [24/84], [94mLoss[0m : 4.11457
[1mStep[0m  [32/84], [94mLoss[0m : 4.13825
[1mStep[0m  [40/84], [94mLoss[0m : 4.54529
[1mStep[0m  [48/84], [94mLoss[0m : 3.90956
[1mStep[0m  [56/84], [94mLoss[0m : 3.52732
[1mStep[0m  [64/84], [94mLoss[0m : 3.72382
[1mStep[0m  [72/84], [94mLoss[0m : 3.73586
[1mStep[0m  [80/84], [94mLoss[0m : 3.73619

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.041, [92mTest[0m: 3.729, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.35915
[1mStep[0m  [8/84], [94mLoss[0m : 3.62201
[1mStep[0m  [16/84], [94mLoss[0m : 3.63616
[1mStep[0m  [24/84], [94mLoss[0m : 3.18840
[1mStep[0m  [32/84], [94mLoss[0m : 3.12813
[1mStep[0m  [40/84], [94mLoss[0m : 3.18395
[1mStep[0m  [48/84], [94mLoss[0m : 2.86324
[1mStep[0m  [56/84], [94mLoss[0m : 3.39271
[1mStep[0m  [64/84], [94mLoss[0m : 3.06090
[1mStep[0m  [72/84], [94mLoss[0m : 3.32765
[1mStep[0m  [80/84], [94mLoss[0m : 3.32669

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.302, [92mTest[0m: 2.910, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.00192
[1mStep[0m  [8/84], [94mLoss[0m : 3.05830
[1mStep[0m  [16/84], [94mLoss[0m : 3.42687
[1mStep[0m  [24/84], [94mLoss[0m : 3.04449
[1mStep[0m  [32/84], [94mLoss[0m : 2.92050
[1mStep[0m  [40/84], [94mLoss[0m : 2.24823
[1mStep[0m  [48/84], [94mLoss[0m : 2.47723
[1mStep[0m  [56/84], [94mLoss[0m : 2.70972
[1mStep[0m  [64/84], [94mLoss[0m : 3.03705
[1mStep[0m  [72/84], [94mLoss[0m : 2.70279
[1mStep[0m  [80/84], [94mLoss[0m : 2.96486

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.960, [92mTest[0m: 2.498, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.20748
[1mStep[0m  [8/84], [94mLoss[0m : 2.82437
[1mStep[0m  [16/84], [94mLoss[0m : 2.63672
[1mStep[0m  [24/84], [94mLoss[0m : 2.84654
[1mStep[0m  [32/84], [94mLoss[0m : 2.66949
[1mStep[0m  [40/84], [94mLoss[0m : 2.95714
[1mStep[0m  [48/84], [94mLoss[0m : 2.79255
[1mStep[0m  [56/84], [94mLoss[0m : 2.71711
[1mStep[0m  [64/84], [94mLoss[0m : 2.79843
[1mStep[0m  [72/84], [94mLoss[0m : 2.69058
[1mStep[0m  [80/84], [94mLoss[0m : 2.86722

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.851, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.14436
[1mStep[0m  [8/84], [94mLoss[0m : 2.56061
[1mStep[0m  [16/84], [94mLoss[0m : 3.12256
[1mStep[0m  [24/84], [94mLoss[0m : 2.35195
[1mStep[0m  [32/84], [94mLoss[0m : 2.39112
[1mStep[0m  [40/84], [94mLoss[0m : 2.81967
[1mStep[0m  [48/84], [94mLoss[0m : 3.12948
[1mStep[0m  [56/84], [94mLoss[0m : 2.75175
[1mStep[0m  [64/84], [94mLoss[0m : 2.68555
[1mStep[0m  [72/84], [94mLoss[0m : 2.87969
[1mStep[0m  [80/84], [94mLoss[0m : 2.67125

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.804, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99253
[1mStep[0m  [8/84], [94mLoss[0m : 2.67958
[1mStep[0m  [16/84], [94mLoss[0m : 2.90367
[1mStep[0m  [24/84], [94mLoss[0m : 2.61869
[1mStep[0m  [32/84], [94mLoss[0m : 2.69109
[1mStep[0m  [40/84], [94mLoss[0m : 2.99732
[1mStep[0m  [48/84], [94mLoss[0m : 2.66303
[1mStep[0m  [56/84], [94mLoss[0m : 2.67079
[1mStep[0m  [64/84], [94mLoss[0m : 2.88411
[1mStep[0m  [72/84], [94mLoss[0m : 2.59616
[1mStep[0m  [80/84], [94mLoss[0m : 2.28256

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.766, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67279
[1mStep[0m  [8/84], [94mLoss[0m : 2.76094
[1mStep[0m  [16/84], [94mLoss[0m : 2.63646
[1mStep[0m  [24/84], [94mLoss[0m : 2.81583
[1mStep[0m  [32/84], [94mLoss[0m : 2.64930
[1mStep[0m  [40/84], [94mLoss[0m : 3.35731
[1mStep[0m  [48/84], [94mLoss[0m : 2.66192
[1mStep[0m  [56/84], [94mLoss[0m : 2.84022
[1mStep[0m  [64/84], [94mLoss[0m : 2.65634
[1mStep[0m  [72/84], [94mLoss[0m : 2.61132
[1mStep[0m  [80/84], [94mLoss[0m : 3.00606

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84430
[1mStep[0m  [8/84], [94mLoss[0m : 2.48651
[1mStep[0m  [16/84], [94mLoss[0m : 2.95908
[1mStep[0m  [24/84], [94mLoss[0m : 2.72248
[1mStep[0m  [32/84], [94mLoss[0m : 2.71850
[1mStep[0m  [40/84], [94mLoss[0m : 2.56349
[1mStep[0m  [48/84], [94mLoss[0m : 2.61325
[1mStep[0m  [56/84], [94mLoss[0m : 2.71863
[1mStep[0m  [64/84], [94mLoss[0m : 2.83128
[1mStep[0m  [72/84], [94mLoss[0m : 2.58117
[1mStep[0m  [80/84], [94mLoss[0m : 2.49481

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64927
[1mStep[0m  [8/84], [94mLoss[0m : 2.79506
[1mStep[0m  [16/84], [94mLoss[0m : 3.13562
[1mStep[0m  [24/84], [94mLoss[0m : 3.00092
[1mStep[0m  [32/84], [94mLoss[0m : 2.89007
[1mStep[0m  [40/84], [94mLoss[0m : 2.56046
[1mStep[0m  [48/84], [94mLoss[0m : 2.43496
[1mStep[0m  [56/84], [94mLoss[0m : 2.30613
[1mStep[0m  [64/84], [94mLoss[0m : 2.61701
[1mStep[0m  [72/84], [94mLoss[0m : 2.68067
[1mStep[0m  [80/84], [94mLoss[0m : 2.64170

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69426
[1mStep[0m  [8/84], [94mLoss[0m : 2.69342
[1mStep[0m  [16/84], [94mLoss[0m : 3.08497
[1mStep[0m  [24/84], [94mLoss[0m : 2.74640
[1mStep[0m  [32/84], [94mLoss[0m : 2.62687
[1mStep[0m  [40/84], [94mLoss[0m : 2.70839
[1mStep[0m  [48/84], [94mLoss[0m : 2.51042
[1mStep[0m  [56/84], [94mLoss[0m : 2.61441
[1mStep[0m  [64/84], [94mLoss[0m : 2.88785
[1mStep[0m  [72/84], [94mLoss[0m : 2.52287
[1mStep[0m  [80/84], [94mLoss[0m : 2.79308

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78891
[1mStep[0m  [8/84], [94mLoss[0m : 2.85734
[1mStep[0m  [16/84], [94mLoss[0m : 2.62094
[1mStep[0m  [24/84], [94mLoss[0m : 2.63858
[1mStep[0m  [32/84], [94mLoss[0m : 2.39992
[1mStep[0m  [40/84], [94mLoss[0m : 2.88211
[1mStep[0m  [48/84], [94mLoss[0m : 2.56427
[1mStep[0m  [56/84], [94mLoss[0m : 2.88703
[1mStep[0m  [64/84], [94mLoss[0m : 2.36380
[1mStep[0m  [72/84], [94mLoss[0m : 2.52642
[1mStep[0m  [80/84], [94mLoss[0m : 2.85023

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36101
[1mStep[0m  [8/84], [94mLoss[0m : 2.60217
[1mStep[0m  [16/84], [94mLoss[0m : 3.01403
[1mStep[0m  [24/84], [94mLoss[0m : 2.38776
[1mStep[0m  [32/84], [94mLoss[0m : 2.68366
[1mStep[0m  [40/84], [94mLoss[0m : 2.47833
[1mStep[0m  [48/84], [94mLoss[0m : 2.48312
[1mStep[0m  [56/84], [94mLoss[0m : 2.69887
[1mStep[0m  [64/84], [94mLoss[0m : 2.98034
[1mStep[0m  [72/84], [94mLoss[0m : 2.99983
[1mStep[0m  [80/84], [94mLoss[0m : 2.61315

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59981
[1mStep[0m  [8/84], [94mLoss[0m : 2.78782
[1mStep[0m  [16/84], [94mLoss[0m : 2.63567
[1mStep[0m  [24/84], [94mLoss[0m : 2.32559
[1mStep[0m  [32/84], [94mLoss[0m : 2.59873
[1mStep[0m  [40/84], [94mLoss[0m : 2.62684
[1mStep[0m  [48/84], [94mLoss[0m : 2.54766
[1mStep[0m  [56/84], [94mLoss[0m : 2.58818
[1mStep[0m  [64/84], [94mLoss[0m : 2.84393
[1mStep[0m  [72/84], [94mLoss[0m : 2.87304
[1mStep[0m  [80/84], [94mLoss[0m : 2.76341

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57637
[1mStep[0m  [8/84], [94mLoss[0m : 2.91822
[1mStep[0m  [16/84], [94mLoss[0m : 2.82753
[1mStep[0m  [24/84], [94mLoss[0m : 2.56916
[1mStep[0m  [32/84], [94mLoss[0m : 2.42780
[1mStep[0m  [40/84], [94mLoss[0m : 2.87695
[1mStep[0m  [48/84], [94mLoss[0m : 2.75366
[1mStep[0m  [56/84], [94mLoss[0m : 2.56173
[1mStep[0m  [64/84], [94mLoss[0m : 2.71696
[1mStep[0m  [72/84], [94mLoss[0m : 2.45253
[1mStep[0m  [80/84], [94mLoss[0m : 2.31258

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53891
[1mStep[0m  [8/84], [94mLoss[0m : 2.85535
[1mStep[0m  [16/84], [94mLoss[0m : 2.65233
[1mStep[0m  [24/84], [94mLoss[0m : 2.52936
[1mStep[0m  [32/84], [94mLoss[0m : 2.48378
[1mStep[0m  [40/84], [94mLoss[0m : 2.62250
[1mStep[0m  [48/84], [94mLoss[0m : 2.29380
[1mStep[0m  [56/84], [94mLoss[0m : 2.53807
[1mStep[0m  [64/84], [94mLoss[0m : 2.97478
[1mStep[0m  [72/84], [94mLoss[0m : 2.68516
[1mStep[0m  [80/84], [94mLoss[0m : 2.66273

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79302
[1mStep[0m  [8/84], [94mLoss[0m : 2.27171
[1mStep[0m  [16/84], [94mLoss[0m : 2.68905
[1mStep[0m  [24/84], [94mLoss[0m : 2.86007
[1mStep[0m  [32/84], [94mLoss[0m : 2.85182
[1mStep[0m  [40/84], [94mLoss[0m : 2.54010
[1mStep[0m  [48/84], [94mLoss[0m : 2.56360
[1mStep[0m  [56/84], [94mLoss[0m : 2.96671
[1mStep[0m  [64/84], [94mLoss[0m : 2.37996
[1mStep[0m  [72/84], [94mLoss[0m : 2.61168
[1mStep[0m  [80/84], [94mLoss[0m : 2.53998

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64755
[1mStep[0m  [8/84], [94mLoss[0m : 2.63443
[1mStep[0m  [16/84], [94mLoss[0m : 2.64993
[1mStep[0m  [24/84], [94mLoss[0m : 2.83200
[1mStep[0m  [32/84], [94mLoss[0m : 2.67034
[1mStep[0m  [40/84], [94mLoss[0m : 2.76753
[1mStep[0m  [48/84], [94mLoss[0m : 2.54698
[1mStep[0m  [56/84], [94mLoss[0m : 2.73903
[1mStep[0m  [64/84], [94mLoss[0m : 2.61540
[1mStep[0m  [72/84], [94mLoss[0m : 2.47082
[1mStep[0m  [80/84], [94mLoss[0m : 2.47989

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56408
[1mStep[0m  [8/84], [94mLoss[0m : 2.38693
[1mStep[0m  [16/84], [94mLoss[0m : 2.71543
[1mStep[0m  [24/84], [94mLoss[0m : 2.61504
[1mStep[0m  [32/84], [94mLoss[0m : 2.70743
[1mStep[0m  [40/84], [94mLoss[0m : 2.89353
[1mStep[0m  [48/84], [94mLoss[0m : 2.75972
[1mStep[0m  [56/84], [94mLoss[0m : 2.64327
[1mStep[0m  [64/84], [94mLoss[0m : 2.74615
[1mStep[0m  [72/84], [94mLoss[0m : 2.44453
[1mStep[0m  [80/84], [94mLoss[0m : 2.65461

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.08918
[1mStep[0m  [8/84], [94mLoss[0m : 2.67960
[1mStep[0m  [16/84], [94mLoss[0m : 2.17619
[1mStep[0m  [24/84], [94mLoss[0m : 3.17362
[1mStep[0m  [32/84], [94mLoss[0m : 2.55768
[1mStep[0m  [40/84], [94mLoss[0m : 2.67428
[1mStep[0m  [48/84], [94mLoss[0m : 2.75475
[1mStep[0m  [56/84], [94mLoss[0m : 2.79766
[1mStep[0m  [64/84], [94mLoss[0m : 2.73510
[1mStep[0m  [72/84], [94mLoss[0m : 2.61922
[1mStep[0m  [80/84], [94mLoss[0m : 2.68164

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59355
[1mStep[0m  [8/84], [94mLoss[0m : 2.73161
[1mStep[0m  [16/84], [94mLoss[0m : 2.72436
[1mStep[0m  [24/84], [94mLoss[0m : 2.62267
[1mStep[0m  [32/84], [94mLoss[0m : 2.67041
[1mStep[0m  [40/84], [94mLoss[0m : 2.62487
[1mStep[0m  [48/84], [94mLoss[0m : 2.41658
[1mStep[0m  [56/84], [94mLoss[0m : 2.62980
[1mStep[0m  [64/84], [94mLoss[0m : 2.35425
[1mStep[0m  [72/84], [94mLoss[0m : 2.22822
[1mStep[0m  [80/84], [94mLoss[0m : 2.73628

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89218
[1mStep[0m  [8/84], [94mLoss[0m : 2.31910
[1mStep[0m  [16/84], [94mLoss[0m : 2.67117
[1mStep[0m  [24/84], [94mLoss[0m : 2.57878
[1mStep[0m  [32/84], [94mLoss[0m : 2.54484
[1mStep[0m  [40/84], [94mLoss[0m : 2.62343
[1mStep[0m  [48/84], [94mLoss[0m : 2.55572
[1mStep[0m  [56/84], [94mLoss[0m : 2.80123
[1mStep[0m  [64/84], [94mLoss[0m : 2.38308
[1mStep[0m  [72/84], [94mLoss[0m : 2.90023
[1mStep[0m  [80/84], [94mLoss[0m : 2.55293

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43043
[1mStep[0m  [8/84], [94mLoss[0m : 2.48241
[1mStep[0m  [16/84], [94mLoss[0m : 2.62398
[1mStep[0m  [24/84], [94mLoss[0m : 2.74457
[1mStep[0m  [32/84], [94mLoss[0m : 2.71617
[1mStep[0m  [40/84], [94mLoss[0m : 2.49927
[1mStep[0m  [48/84], [94mLoss[0m : 3.18404
[1mStep[0m  [56/84], [94mLoss[0m : 2.73292
[1mStep[0m  [64/84], [94mLoss[0m : 2.62942
[1mStep[0m  [72/84], [94mLoss[0m : 2.54998
[1mStep[0m  [80/84], [94mLoss[0m : 2.55911

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3264797585351125
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.60089
[1mStep[0m  [8/84], [94mLoss[0m : 2.51337
[1mStep[0m  [16/84], [94mLoss[0m : 2.85001
[1mStep[0m  [24/84], [94mLoss[0m : 2.44762
[1mStep[0m  [32/84], [94mLoss[0m : 2.86497
[1mStep[0m  [40/84], [94mLoss[0m : 2.74256
[1mStep[0m  [48/84], [94mLoss[0m : 2.68863
[1mStep[0m  [56/84], [94mLoss[0m : 2.51664
[1mStep[0m  [64/84], [94mLoss[0m : 2.99029
[1mStep[0m  [72/84], [94mLoss[0m : 2.92554
[1mStep[0m  [80/84], [94mLoss[0m : 2.92333

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57206
[1mStep[0m  [8/84], [94mLoss[0m : 2.70060
[1mStep[0m  [16/84], [94mLoss[0m : 2.53599
[1mStep[0m  [24/84], [94mLoss[0m : 2.78774
[1mStep[0m  [32/84], [94mLoss[0m : 2.65598
[1mStep[0m  [40/84], [94mLoss[0m : 2.69380
[1mStep[0m  [48/84], [94mLoss[0m : 2.66530
[1mStep[0m  [56/84], [94mLoss[0m : 2.43598
[1mStep[0m  [64/84], [94mLoss[0m : 2.58687
[1mStep[0m  [72/84], [94mLoss[0m : 2.52795
[1mStep[0m  [80/84], [94mLoss[0m : 2.98738

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75971
[1mStep[0m  [8/84], [94mLoss[0m : 2.49972
[1mStep[0m  [16/84], [94mLoss[0m : 2.66748
[1mStep[0m  [24/84], [94mLoss[0m : 2.55347
[1mStep[0m  [32/84], [94mLoss[0m : 2.33187
[1mStep[0m  [40/84], [94mLoss[0m : 2.72680
[1mStep[0m  [48/84], [94mLoss[0m : 2.48857
[1mStep[0m  [56/84], [94mLoss[0m : 2.29267
[1mStep[0m  [64/84], [94mLoss[0m : 2.64159
[1mStep[0m  [72/84], [94mLoss[0m : 2.71064
[1mStep[0m  [80/84], [94mLoss[0m : 2.83053

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26556
[1mStep[0m  [8/84], [94mLoss[0m : 2.43732
[1mStep[0m  [16/84], [94mLoss[0m : 2.55930
[1mStep[0m  [24/84], [94mLoss[0m : 2.25892
[1mStep[0m  [32/84], [94mLoss[0m : 2.34781
[1mStep[0m  [40/84], [94mLoss[0m : 2.65338
[1mStep[0m  [48/84], [94mLoss[0m : 2.73172
[1mStep[0m  [56/84], [94mLoss[0m : 2.32097
[1mStep[0m  [64/84], [94mLoss[0m : 2.28012
[1mStep[0m  [72/84], [94mLoss[0m : 2.71934
[1mStep[0m  [80/84], [94mLoss[0m : 2.62124

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90155
[1mStep[0m  [8/84], [94mLoss[0m : 2.50507
[1mStep[0m  [16/84], [94mLoss[0m : 2.40130
[1mStep[0m  [24/84], [94mLoss[0m : 2.31401
[1mStep[0m  [32/84], [94mLoss[0m : 2.41291
[1mStep[0m  [40/84], [94mLoss[0m : 2.37049
[1mStep[0m  [48/84], [94mLoss[0m : 2.24814
[1mStep[0m  [56/84], [94mLoss[0m : 2.32992
[1mStep[0m  [64/84], [94mLoss[0m : 2.77822
[1mStep[0m  [72/84], [94mLoss[0m : 2.52333
[1mStep[0m  [80/84], [94mLoss[0m : 2.28821

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35648
[1mStep[0m  [8/84], [94mLoss[0m : 2.47303
[1mStep[0m  [16/84], [94mLoss[0m : 2.41576
[1mStep[0m  [24/84], [94mLoss[0m : 2.45038
[1mStep[0m  [32/84], [94mLoss[0m : 2.63143
[1mStep[0m  [40/84], [94mLoss[0m : 2.64786
[1mStep[0m  [48/84], [94mLoss[0m : 2.71819
[1mStep[0m  [56/84], [94mLoss[0m : 2.41000
[1mStep[0m  [64/84], [94mLoss[0m : 2.79963
[1mStep[0m  [72/84], [94mLoss[0m : 2.59344
[1mStep[0m  [80/84], [94mLoss[0m : 2.71626

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17158
[1mStep[0m  [8/84], [94mLoss[0m : 2.40690
[1mStep[0m  [16/84], [94mLoss[0m : 2.59362
[1mStep[0m  [24/84], [94mLoss[0m : 2.61297
[1mStep[0m  [32/84], [94mLoss[0m : 2.64816
[1mStep[0m  [40/84], [94mLoss[0m : 2.65306
[1mStep[0m  [48/84], [94mLoss[0m : 2.19183
[1mStep[0m  [56/84], [94mLoss[0m : 2.38777
[1mStep[0m  [64/84], [94mLoss[0m : 2.55378
[1mStep[0m  [72/84], [94mLoss[0m : 2.44907
[1mStep[0m  [80/84], [94mLoss[0m : 2.36197

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37401
[1mStep[0m  [8/84], [94mLoss[0m : 2.42976
[1mStep[0m  [16/84], [94mLoss[0m : 2.34230
[1mStep[0m  [24/84], [94mLoss[0m : 2.41461
[1mStep[0m  [32/84], [94mLoss[0m : 2.14625
[1mStep[0m  [40/84], [94mLoss[0m : 2.44052
[1mStep[0m  [48/84], [94mLoss[0m : 2.43235
[1mStep[0m  [56/84], [94mLoss[0m : 2.52877
[1mStep[0m  [64/84], [94mLoss[0m : 2.28963
[1mStep[0m  [72/84], [94mLoss[0m : 2.56396
[1mStep[0m  [80/84], [94mLoss[0m : 2.49388

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54117
[1mStep[0m  [8/84], [94mLoss[0m : 2.60736
[1mStep[0m  [16/84], [94mLoss[0m : 2.45140
[1mStep[0m  [24/84], [94mLoss[0m : 2.58076
[1mStep[0m  [32/84], [94mLoss[0m : 2.51305
[1mStep[0m  [40/84], [94mLoss[0m : 2.61479
[1mStep[0m  [48/84], [94mLoss[0m : 2.33459
[1mStep[0m  [56/84], [94mLoss[0m : 2.41325
[1mStep[0m  [64/84], [94mLoss[0m : 2.13295
[1mStep[0m  [72/84], [94mLoss[0m : 2.39444
[1mStep[0m  [80/84], [94mLoss[0m : 2.10306

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33070
[1mStep[0m  [8/84], [94mLoss[0m : 2.26728
[1mStep[0m  [16/84], [94mLoss[0m : 2.30543
[1mStep[0m  [24/84], [94mLoss[0m : 2.38160
[1mStep[0m  [32/84], [94mLoss[0m : 2.21944
[1mStep[0m  [40/84], [94mLoss[0m : 2.24415
[1mStep[0m  [48/84], [94mLoss[0m : 2.48081
[1mStep[0m  [56/84], [94mLoss[0m : 2.41111
[1mStep[0m  [64/84], [94mLoss[0m : 2.65998
[1mStep[0m  [72/84], [94mLoss[0m : 2.36694
[1mStep[0m  [80/84], [94mLoss[0m : 2.22391

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26697
[1mStep[0m  [8/84], [94mLoss[0m : 2.15115
[1mStep[0m  [16/84], [94mLoss[0m : 2.09152
[1mStep[0m  [24/84], [94mLoss[0m : 2.22068
[1mStep[0m  [32/84], [94mLoss[0m : 2.29325
[1mStep[0m  [40/84], [94mLoss[0m : 2.29607
[1mStep[0m  [48/84], [94mLoss[0m : 2.11946
[1mStep[0m  [56/84], [94mLoss[0m : 2.05242
[1mStep[0m  [64/84], [94mLoss[0m : 2.27370
[1mStep[0m  [72/84], [94mLoss[0m : 2.30195
[1mStep[0m  [80/84], [94mLoss[0m : 2.63267

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.10134
[1mStep[0m  [8/84], [94mLoss[0m : 2.23601
[1mStep[0m  [16/84], [94mLoss[0m : 2.19498
[1mStep[0m  [24/84], [94mLoss[0m : 2.29611
[1mStep[0m  [32/84], [94mLoss[0m : 2.13127
[1mStep[0m  [40/84], [94mLoss[0m : 2.51304
[1mStep[0m  [48/84], [94mLoss[0m : 2.18072
[1mStep[0m  [56/84], [94mLoss[0m : 2.30000
[1mStep[0m  [64/84], [94mLoss[0m : 2.02464
[1mStep[0m  [72/84], [94mLoss[0m : 2.33337
[1mStep[0m  [80/84], [94mLoss[0m : 2.29981

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23533
[1mStep[0m  [8/84], [94mLoss[0m : 2.45682
[1mStep[0m  [16/84], [94mLoss[0m : 2.01509
[1mStep[0m  [24/84], [94mLoss[0m : 2.42163
[1mStep[0m  [32/84], [94mLoss[0m : 2.47946
[1mStep[0m  [40/84], [94mLoss[0m : 2.21009
[1mStep[0m  [48/84], [94mLoss[0m : 2.06669
[1mStep[0m  [56/84], [94mLoss[0m : 2.08052
[1mStep[0m  [64/84], [94mLoss[0m : 2.26010
[1mStep[0m  [72/84], [94mLoss[0m : 2.25113
[1mStep[0m  [80/84], [94mLoss[0m : 2.29404

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01208
[1mStep[0m  [8/84], [94mLoss[0m : 2.14751
[1mStep[0m  [16/84], [94mLoss[0m : 2.46343
[1mStep[0m  [24/84], [94mLoss[0m : 2.14268
[1mStep[0m  [32/84], [94mLoss[0m : 2.50257
[1mStep[0m  [40/84], [94mLoss[0m : 2.22542
[1mStep[0m  [48/84], [94mLoss[0m : 2.44776
[1mStep[0m  [56/84], [94mLoss[0m : 2.31323
[1mStep[0m  [64/84], [94mLoss[0m : 2.37684
[1mStep[0m  [72/84], [94mLoss[0m : 2.26357
[1mStep[0m  [80/84], [94mLoss[0m : 2.25194

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18376
[1mStep[0m  [8/84], [94mLoss[0m : 2.52078
[1mStep[0m  [16/84], [94mLoss[0m : 2.26697
[1mStep[0m  [24/84], [94mLoss[0m : 1.90373
[1mStep[0m  [32/84], [94mLoss[0m : 1.73892
[1mStep[0m  [40/84], [94mLoss[0m : 1.94411
[1mStep[0m  [48/84], [94mLoss[0m : 2.12045
[1mStep[0m  [56/84], [94mLoss[0m : 2.00624
[1mStep[0m  [64/84], [94mLoss[0m : 2.29330
[1mStep[0m  [72/84], [94mLoss[0m : 2.12648
[1mStep[0m  [80/84], [94mLoss[0m : 2.46901

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06817
[1mStep[0m  [8/84], [94mLoss[0m : 2.04199
[1mStep[0m  [16/84], [94mLoss[0m : 1.86321
[1mStep[0m  [24/84], [94mLoss[0m : 2.03393
[1mStep[0m  [32/84], [94mLoss[0m : 2.35123
[1mStep[0m  [40/84], [94mLoss[0m : 2.02109
[1mStep[0m  [48/84], [94mLoss[0m : 2.14235
[1mStep[0m  [56/84], [94mLoss[0m : 2.41489
[1mStep[0m  [64/84], [94mLoss[0m : 1.92676
[1mStep[0m  [72/84], [94mLoss[0m : 2.20739
[1mStep[0m  [80/84], [94mLoss[0m : 2.16829

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08186
[1mStep[0m  [8/84], [94mLoss[0m : 2.40397
[1mStep[0m  [16/84], [94mLoss[0m : 1.93157
[1mStep[0m  [24/84], [94mLoss[0m : 1.98915
[1mStep[0m  [32/84], [94mLoss[0m : 2.11561
[1mStep[0m  [40/84], [94mLoss[0m : 2.08786
[1mStep[0m  [48/84], [94mLoss[0m : 2.13639
[1mStep[0m  [56/84], [94mLoss[0m : 2.17877
[1mStep[0m  [64/84], [94mLoss[0m : 1.93477
[1mStep[0m  [72/84], [94mLoss[0m : 2.21108
[1mStep[0m  [80/84], [94mLoss[0m : 2.18819

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92576
[1mStep[0m  [8/84], [94mLoss[0m : 1.97530
[1mStep[0m  [16/84], [94mLoss[0m : 1.98053
[1mStep[0m  [24/84], [94mLoss[0m : 1.85346
[1mStep[0m  [32/84], [94mLoss[0m : 2.46802
[1mStep[0m  [40/84], [94mLoss[0m : 2.18240
[1mStep[0m  [48/84], [94mLoss[0m : 2.16009
[1mStep[0m  [56/84], [94mLoss[0m : 1.95611
[1mStep[0m  [64/84], [94mLoss[0m : 2.02216
[1mStep[0m  [72/84], [94mLoss[0m : 2.22188
[1mStep[0m  [80/84], [94mLoss[0m : 2.33339

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11822
[1mStep[0m  [8/84], [94mLoss[0m : 2.04938
[1mStep[0m  [16/84], [94mLoss[0m : 1.90403
[1mStep[0m  [24/84], [94mLoss[0m : 2.21451
[1mStep[0m  [32/84], [94mLoss[0m : 1.86210
[1mStep[0m  [40/84], [94mLoss[0m : 2.11644
[1mStep[0m  [48/84], [94mLoss[0m : 2.03751
[1mStep[0m  [56/84], [94mLoss[0m : 2.04443
[1mStep[0m  [64/84], [94mLoss[0m : 2.10744
[1mStep[0m  [72/84], [94mLoss[0m : 1.84662
[1mStep[0m  [80/84], [94mLoss[0m : 2.34924

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06724
[1mStep[0m  [8/84], [94mLoss[0m : 1.92507
[1mStep[0m  [16/84], [94mLoss[0m : 2.15189
[1mStep[0m  [24/84], [94mLoss[0m : 2.04125
[1mStep[0m  [32/84], [94mLoss[0m : 2.04731
[1mStep[0m  [40/84], [94mLoss[0m : 2.20678
[1mStep[0m  [48/84], [94mLoss[0m : 2.05048
[1mStep[0m  [56/84], [94mLoss[0m : 2.06533
[1mStep[0m  [64/84], [94mLoss[0m : 2.04720
[1mStep[0m  [72/84], [94mLoss[0m : 1.77135
[1mStep[0m  [80/84], [94mLoss[0m : 2.01807

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91269
[1mStep[0m  [8/84], [94mLoss[0m : 2.26289
[1mStep[0m  [16/84], [94mLoss[0m : 2.07093
[1mStep[0m  [24/84], [94mLoss[0m : 2.19466
[1mStep[0m  [32/84], [94mLoss[0m : 1.88501
[1mStep[0m  [40/84], [94mLoss[0m : 2.02581
[1mStep[0m  [48/84], [94mLoss[0m : 1.82771
[1mStep[0m  [56/84], [94mLoss[0m : 2.19813
[1mStep[0m  [64/84], [94mLoss[0m : 1.86761
[1mStep[0m  [72/84], [94mLoss[0m : 1.86342
[1mStep[0m  [80/84], [94mLoss[0m : 1.85464

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92150
[1mStep[0m  [8/84], [94mLoss[0m : 1.80936
[1mStep[0m  [16/84], [94mLoss[0m : 1.86972
[1mStep[0m  [24/84], [94mLoss[0m : 2.11636
[1mStep[0m  [32/84], [94mLoss[0m : 2.02822
[1mStep[0m  [40/84], [94mLoss[0m : 2.16079
[1mStep[0m  [48/84], [94mLoss[0m : 2.07311
[1mStep[0m  [56/84], [94mLoss[0m : 1.94839
[1mStep[0m  [64/84], [94mLoss[0m : 2.11507
[1mStep[0m  [72/84], [94mLoss[0m : 2.15567
[1mStep[0m  [80/84], [94mLoss[0m : 2.05168

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83321
[1mStep[0m  [8/84], [94mLoss[0m : 1.85316
[1mStep[0m  [16/84], [94mLoss[0m : 1.83404
[1mStep[0m  [24/84], [94mLoss[0m : 2.04780
[1mStep[0m  [32/84], [94mLoss[0m : 2.01932
[1mStep[0m  [40/84], [94mLoss[0m : 2.00906
[1mStep[0m  [48/84], [94mLoss[0m : 1.81790
[1mStep[0m  [56/84], [94mLoss[0m : 2.12437
[1mStep[0m  [64/84], [94mLoss[0m : 2.10852
[1mStep[0m  [72/84], [94mLoss[0m : 1.97092
[1mStep[0m  [80/84], [94mLoss[0m : 2.14523

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74411
[1mStep[0m  [8/84], [94mLoss[0m : 1.88315
[1mStep[0m  [16/84], [94mLoss[0m : 1.88576
[1mStep[0m  [24/84], [94mLoss[0m : 2.08347
[1mStep[0m  [32/84], [94mLoss[0m : 1.74202
[1mStep[0m  [40/84], [94mLoss[0m : 1.88050
[1mStep[0m  [48/84], [94mLoss[0m : 2.02811
[1mStep[0m  [56/84], [94mLoss[0m : 1.85716
[1mStep[0m  [64/84], [94mLoss[0m : 1.82032
[1mStep[0m  [72/84], [94mLoss[0m : 1.99107
[1mStep[0m  [80/84], [94mLoss[0m : 1.69496

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.927, [92mTest[0m: 2.471, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69539
[1mStep[0m  [8/84], [94mLoss[0m : 2.01394
[1mStep[0m  [16/84], [94mLoss[0m : 1.93078
[1mStep[0m  [24/84], [94mLoss[0m : 1.85146
[1mStep[0m  [32/84], [94mLoss[0m : 1.87731
[1mStep[0m  [40/84], [94mLoss[0m : 1.87025
[1mStep[0m  [48/84], [94mLoss[0m : 1.72894
[1mStep[0m  [56/84], [94mLoss[0m : 2.08251
[1mStep[0m  [64/84], [94mLoss[0m : 1.78306
[1mStep[0m  [72/84], [94mLoss[0m : 1.79511
[1mStep[0m  [80/84], [94mLoss[0m : 1.89137

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96983
[1mStep[0m  [8/84], [94mLoss[0m : 1.79964
[1mStep[0m  [16/84], [94mLoss[0m : 1.67533
[1mStep[0m  [24/84], [94mLoss[0m : 1.72436
[1mStep[0m  [32/84], [94mLoss[0m : 1.83398
[1mStep[0m  [40/84], [94mLoss[0m : 1.86328
[1mStep[0m  [48/84], [94mLoss[0m : 2.06616
[1mStep[0m  [56/84], [94mLoss[0m : 1.93548
[1mStep[0m  [64/84], [94mLoss[0m : 1.77869
[1mStep[0m  [72/84], [94mLoss[0m : 1.79544
[1mStep[0m  [80/84], [94mLoss[0m : 1.99678

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91657
[1mStep[0m  [8/84], [94mLoss[0m : 1.66637
[1mStep[0m  [16/84], [94mLoss[0m : 1.96916
[1mStep[0m  [24/84], [94mLoss[0m : 2.14428
[1mStep[0m  [32/84], [94mLoss[0m : 2.00145
[1mStep[0m  [40/84], [94mLoss[0m : 1.90678
[1mStep[0m  [48/84], [94mLoss[0m : 1.90365
[1mStep[0m  [56/84], [94mLoss[0m : 1.81164
[1mStep[0m  [64/84], [94mLoss[0m : 2.01000
[1mStep[0m  [72/84], [94mLoss[0m : 2.00898
[1mStep[0m  [80/84], [94mLoss[0m : 1.87700

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99942
[1mStep[0m  [8/84], [94mLoss[0m : 1.66654
[1mStep[0m  [16/84], [94mLoss[0m : 1.71956
[1mStep[0m  [24/84], [94mLoss[0m : 1.72899
[1mStep[0m  [32/84], [94mLoss[0m : 1.95085
[1mStep[0m  [40/84], [94mLoss[0m : 1.97902
[1mStep[0m  [48/84], [94mLoss[0m : 1.87047
[1mStep[0m  [56/84], [94mLoss[0m : 1.84340
[1mStep[0m  [64/84], [94mLoss[0m : 1.93544
[1mStep[0m  [72/84], [94mLoss[0m : 1.97261
[1mStep[0m  [80/84], [94mLoss[0m : 2.01838

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85348
[1mStep[0m  [8/84], [94mLoss[0m : 1.85661
[1mStep[0m  [16/84], [94mLoss[0m : 1.83431
[1mStep[0m  [24/84], [94mLoss[0m : 1.85799
[1mStep[0m  [32/84], [94mLoss[0m : 1.71545
[1mStep[0m  [40/84], [94mLoss[0m : 1.68532
[1mStep[0m  [48/84], [94mLoss[0m : 1.66511
[1mStep[0m  [56/84], [94mLoss[0m : 1.55412
[1mStep[0m  [64/84], [94mLoss[0m : 1.78409
[1mStep[0m  [72/84], [94mLoss[0m : 2.06303
[1mStep[0m  [80/84], [94mLoss[0m : 1.92912

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.539, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.03790
[1mStep[0m  [8/84], [94mLoss[0m : 1.77082
[1mStep[0m  [16/84], [94mLoss[0m : 1.78605
[1mStep[0m  [24/84], [94mLoss[0m : 1.71909
[1mStep[0m  [32/84], [94mLoss[0m : 1.65519
[1mStep[0m  [40/84], [94mLoss[0m : 1.90223
[1mStep[0m  [48/84], [94mLoss[0m : 1.80982
[1mStep[0m  [56/84], [94mLoss[0m : 1.80486
[1mStep[0m  [64/84], [94mLoss[0m : 1.72950
[1mStep[0m  [72/84], [94mLoss[0m : 1.84176
[1mStep[0m  [80/84], [94mLoss[0m : 1.52954

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.569, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.510
====================================

Phase 2 - Evaluation MAE:  2.509869192327772
MAE score P1        2.32648
MAE score P2       2.509869
loss                1.80767
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 29, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.10234
[1mStep[0m  [8/84], [94mLoss[0m : 11.10630
[1mStep[0m  [16/84], [94mLoss[0m : 11.04741
[1mStep[0m  [24/84], [94mLoss[0m : 10.60980
[1mStep[0m  [32/84], [94mLoss[0m : 10.82582
[1mStep[0m  [40/84], [94mLoss[0m : 9.92105
[1mStep[0m  [48/84], [94mLoss[0m : 10.06816
[1mStep[0m  [56/84], [94mLoss[0m : 9.81763
[1mStep[0m  [64/84], [94mLoss[0m : 10.41792
[1mStep[0m  [72/84], [94mLoss[0m : 9.69163
[1mStep[0m  [80/84], [94mLoss[0m : 9.77147

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.524, [92mTest[0m: 10.964, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.13180
[1mStep[0m  [8/84], [94mLoss[0m : 10.00903
[1mStep[0m  [16/84], [94mLoss[0m : 9.60886
[1mStep[0m  [24/84], [94mLoss[0m : 9.95917
[1mStep[0m  [32/84], [94mLoss[0m : 9.53840
[1mStep[0m  [40/84], [94mLoss[0m : 9.51303
[1mStep[0m  [48/84], [94mLoss[0m : 9.34984
[1mStep[0m  [56/84], [94mLoss[0m : 9.12104
[1mStep[0m  [64/84], [94mLoss[0m : 9.51522
[1mStep[0m  [72/84], [94mLoss[0m : 8.59335
[1mStep[0m  [80/84], [94mLoss[0m : 7.86450

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.311, [92mTest[0m: 9.739, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.48054
[1mStep[0m  [8/84], [94mLoss[0m : 8.55984
[1mStep[0m  [16/84], [94mLoss[0m : 8.37120
[1mStep[0m  [24/84], [94mLoss[0m : 7.67312
[1mStep[0m  [32/84], [94mLoss[0m : 7.77759
[1mStep[0m  [40/84], [94mLoss[0m : 7.34014
[1mStep[0m  [48/84], [94mLoss[0m : 7.35563
[1mStep[0m  [56/84], [94mLoss[0m : 7.16072
[1mStep[0m  [64/84], [94mLoss[0m : 6.50377
[1mStep[0m  [72/84], [94mLoss[0m : 7.04304
[1mStep[0m  [80/84], [94mLoss[0m : 6.76145

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.512, [92mTest[0m: 7.797, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.59157
[1mStep[0m  [8/84], [94mLoss[0m : 6.03464
[1mStep[0m  [16/84], [94mLoss[0m : 5.81212
[1mStep[0m  [24/84], [94mLoss[0m : 6.24892
[1mStep[0m  [32/84], [94mLoss[0m : 5.58559
[1mStep[0m  [40/84], [94mLoss[0m : 5.27150
[1mStep[0m  [48/84], [94mLoss[0m : 5.38689
[1mStep[0m  [56/84], [94mLoss[0m : 5.38153
[1mStep[0m  [64/84], [94mLoss[0m : 4.80346
[1mStep[0m  [72/84], [94mLoss[0m : 4.92823
[1mStep[0m  [80/84], [94mLoss[0m : 4.65608

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.522, [92mTest[0m: 5.627, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.16558
[1mStep[0m  [8/84], [94mLoss[0m : 4.73723
[1mStep[0m  [16/84], [94mLoss[0m : 4.61680
[1mStep[0m  [24/84], [94mLoss[0m : 4.35031
[1mStep[0m  [32/84], [94mLoss[0m : 3.85747
[1mStep[0m  [40/84], [94mLoss[0m : 3.34280
[1mStep[0m  [48/84], [94mLoss[0m : 3.52096
[1mStep[0m  [56/84], [94mLoss[0m : 3.56861
[1mStep[0m  [64/84], [94mLoss[0m : 3.01898
[1mStep[0m  [72/84], [94mLoss[0m : 2.89945
[1mStep[0m  [80/84], [94mLoss[0m : 3.23277

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.645, [92mTest[0m: 3.712, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96020
[1mStep[0m  [8/84], [94mLoss[0m : 3.01927
[1mStep[0m  [16/84], [94mLoss[0m : 2.77251
[1mStep[0m  [24/84], [94mLoss[0m : 2.78300
[1mStep[0m  [32/84], [94mLoss[0m : 3.21411
[1mStep[0m  [40/84], [94mLoss[0m : 2.94760
[1mStep[0m  [48/84], [94mLoss[0m : 2.60050
[1mStep[0m  [56/84], [94mLoss[0m : 2.69628
[1mStep[0m  [64/84], [94mLoss[0m : 2.63949
[1mStep[0m  [72/84], [94mLoss[0m : 2.40323
[1mStep[0m  [80/84], [94mLoss[0m : 2.74618

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.793, [92mTest[0m: 2.504, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85628
[1mStep[0m  [8/84], [94mLoss[0m : 2.33483
[1mStep[0m  [16/84], [94mLoss[0m : 2.48823
[1mStep[0m  [24/84], [94mLoss[0m : 2.73947
[1mStep[0m  [32/84], [94mLoss[0m : 2.72811
[1mStep[0m  [40/84], [94mLoss[0m : 2.87363
[1mStep[0m  [48/84], [94mLoss[0m : 2.65509
[1mStep[0m  [56/84], [94mLoss[0m : 3.02517
[1mStep[0m  [64/84], [94mLoss[0m : 2.36121
[1mStep[0m  [72/84], [94mLoss[0m : 2.71261
[1mStep[0m  [80/84], [94mLoss[0m : 3.09539

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25694
[1mStep[0m  [8/84], [94mLoss[0m : 2.82026
[1mStep[0m  [16/84], [94mLoss[0m : 2.62858
[1mStep[0m  [24/84], [94mLoss[0m : 2.99283
[1mStep[0m  [32/84], [94mLoss[0m : 2.96259
[1mStep[0m  [40/84], [94mLoss[0m : 2.63002
[1mStep[0m  [48/84], [94mLoss[0m : 2.79808
[1mStep[0m  [56/84], [94mLoss[0m : 2.55955
[1mStep[0m  [64/84], [94mLoss[0m : 2.51358
[1mStep[0m  [72/84], [94mLoss[0m : 2.57151
[1mStep[0m  [80/84], [94mLoss[0m : 2.52851

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89105
[1mStep[0m  [8/84], [94mLoss[0m : 2.78855
[1mStep[0m  [16/84], [94mLoss[0m : 2.90036
[1mStep[0m  [24/84], [94mLoss[0m : 2.65641
[1mStep[0m  [32/84], [94mLoss[0m : 2.88801
[1mStep[0m  [40/84], [94mLoss[0m : 2.62060
[1mStep[0m  [48/84], [94mLoss[0m : 2.55879
[1mStep[0m  [56/84], [94mLoss[0m : 2.65727
[1mStep[0m  [64/84], [94mLoss[0m : 2.66675
[1mStep[0m  [72/84], [94mLoss[0m : 2.91412
[1mStep[0m  [80/84], [94mLoss[0m : 2.49674

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62232
[1mStep[0m  [8/84], [94mLoss[0m : 2.29919
[1mStep[0m  [16/84], [94mLoss[0m : 2.85470
[1mStep[0m  [24/84], [94mLoss[0m : 2.46081
[1mStep[0m  [32/84], [94mLoss[0m : 2.83300
[1mStep[0m  [40/84], [94mLoss[0m : 2.60754
[1mStep[0m  [48/84], [94mLoss[0m : 2.84826
[1mStep[0m  [56/84], [94mLoss[0m : 2.58540
[1mStep[0m  [64/84], [94mLoss[0m : 2.76987
[1mStep[0m  [72/84], [94mLoss[0m : 2.59375
[1mStep[0m  [80/84], [94mLoss[0m : 2.45525

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75214
[1mStep[0m  [8/84], [94mLoss[0m : 2.62351
[1mStep[0m  [16/84], [94mLoss[0m : 2.42948
[1mStep[0m  [24/84], [94mLoss[0m : 2.22324
[1mStep[0m  [32/84], [94mLoss[0m : 2.60869
[1mStep[0m  [40/84], [94mLoss[0m : 2.72183
[1mStep[0m  [48/84], [94mLoss[0m : 2.41697
[1mStep[0m  [56/84], [94mLoss[0m : 2.81395
[1mStep[0m  [64/84], [94mLoss[0m : 2.66595
[1mStep[0m  [72/84], [94mLoss[0m : 2.91357
[1mStep[0m  [80/84], [94mLoss[0m : 2.47625

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85292
[1mStep[0m  [8/84], [94mLoss[0m : 2.37750
[1mStep[0m  [16/84], [94mLoss[0m : 2.44964
[1mStep[0m  [24/84], [94mLoss[0m : 2.70106
[1mStep[0m  [32/84], [94mLoss[0m : 2.78149
[1mStep[0m  [40/84], [94mLoss[0m : 2.35783
[1mStep[0m  [48/84], [94mLoss[0m : 2.36692
[1mStep[0m  [56/84], [94mLoss[0m : 2.53443
[1mStep[0m  [64/84], [94mLoss[0m : 2.78463
[1mStep[0m  [72/84], [94mLoss[0m : 2.73039
[1mStep[0m  [80/84], [94mLoss[0m : 2.32538

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67183
[1mStep[0m  [8/84], [94mLoss[0m : 2.89071
[1mStep[0m  [16/84], [94mLoss[0m : 2.49224
[1mStep[0m  [24/84], [94mLoss[0m : 2.70242
[1mStep[0m  [32/84], [94mLoss[0m : 2.58299
[1mStep[0m  [40/84], [94mLoss[0m : 2.84998
[1mStep[0m  [48/84], [94mLoss[0m : 2.56626
[1mStep[0m  [56/84], [94mLoss[0m : 2.23507
[1mStep[0m  [64/84], [94mLoss[0m : 2.65283
[1mStep[0m  [72/84], [94mLoss[0m : 2.52770
[1mStep[0m  [80/84], [94mLoss[0m : 2.50664

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40568
[1mStep[0m  [8/84], [94mLoss[0m : 2.57209
[1mStep[0m  [16/84], [94mLoss[0m : 2.47882
[1mStep[0m  [24/84], [94mLoss[0m : 2.45343
[1mStep[0m  [32/84], [94mLoss[0m : 2.48066
[1mStep[0m  [40/84], [94mLoss[0m : 2.60361
[1mStep[0m  [48/84], [94mLoss[0m : 2.75468
[1mStep[0m  [56/84], [94mLoss[0m : 2.65954
[1mStep[0m  [64/84], [94mLoss[0m : 2.61541
[1mStep[0m  [72/84], [94mLoss[0m : 2.82458
[1mStep[0m  [80/84], [94mLoss[0m : 2.40665

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78321
[1mStep[0m  [8/84], [94mLoss[0m : 2.51285
[1mStep[0m  [16/84], [94mLoss[0m : 2.83359
[1mStep[0m  [24/84], [94mLoss[0m : 2.48806
[1mStep[0m  [32/84], [94mLoss[0m : 2.43977
[1mStep[0m  [40/84], [94mLoss[0m : 2.67655
[1mStep[0m  [48/84], [94mLoss[0m : 2.65917
[1mStep[0m  [56/84], [94mLoss[0m : 2.56053
[1mStep[0m  [64/84], [94mLoss[0m : 2.32652
[1mStep[0m  [72/84], [94mLoss[0m : 2.33480
[1mStep[0m  [80/84], [94mLoss[0m : 2.56384

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69651
[1mStep[0m  [8/84], [94mLoss[0m : 2.32768
[1mStep[0m  [16/84], [94mLoss[0m : 2.43717
[1mStep[0m  [24/84], [94mLoss[0m : 2.58197
[1mStep[0m  [32/84], [94mLoss[0m : 2.69228
[1mStep[0m  [40/84], [94mLoss[0m : 2.67749
[1mStep[0m  [48/84], [94mLoss[0m : 2.40259
[1mStep[0m  [56/84], [94mLoss[0m : 2.56660
[1mStep[0m  [64/84], [94mLoss[0m : 2.56844
[1mStep[0m  [72/84], [94mLoss[0m : 2.77364
[1mStep[0m  [80/84], [94mLoss[0m : 2.54612

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53798
[1mStep[0m  [8/84], [94mLoss[0m : 2.42252
[1mStep[0m  [16/84], [94mLoss[0m : 2.53030
[1mStep[0m  [24/84], [94mLoss[0m : 2.37302
[1mStep[0m  [32/84], [94mLoss[0m : 2.62942
[1mStep[0m  [40/84], [94mLoss[0m : 2.66445
[1mStep[0m  [48/84], [94mLoss[0m : 2.86595
[1mStep[0m  [56/84], [94mLoss[0m : 2.63207
[1mStep[0m  [64/84], [94mLoss[0m : 2.81689
[1mStep[0m  [72/84], [94mLoss[0m : 2.58367
[1mStep[0m  [80/84], [94mLoss[0m : 2.65980

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49944
[1mStep[0m  [8/84], [94mLoss[0m : 2.36712
[1mStep[0m  [16/84], [94mLoss[0m : 2.81825
[1mStep[0m  [24/84], [94mLoss[0m : 2.51389
[1mStep[0m  [32/84], [94mLoss[0m : 2.62328
[1mStep[0m  [40/84], [94mLoss[0m : 2.73028
[1mStep[0m  [48/84], [94mLoss[0m : 2.24768
[1mStep[0m  [56/84], [94mLoss[0m : 2.80423
[1mStep[0m  [64/84], [94mLoss[0m : 2.51770
[1mStep[0m  [72/84], [94mLoss[0m : 2.94190
[1mStep[0m  [80/84], [94mLoss[0m : 2.57160

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38908
[1mStep[0m  [8/84], [94mLoss[0m : 2.59370
[1mStep[0m  [16/84], [94mLoss[0m : 2.46509
[1mStep[0m  [24/84], [94mLoss[0m : 2.58737
[1mStep[0m  [32/84], [94mLoss[0m : 2.51765
[1mStep[0m  [40/84], [94mLoss[0m : 2.44433
[1mStep[0m  [48/84], [94mLoss[0m : 2.55739
[1mStep[0m  [56/84], [94mLoss[0m : 2.53033
[1mStep[0m  [64/84], [94mLoss[0m : 2.13073
[1mStep[0m  [72/84], [94mLoss[0m : 2.33348
[1mStep[0m  [80/84], [94mLoss[0m : 2.24933

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60153
[1mStep[0m  [8/84], [94mLoss[0m : 2.03499
[1mStep[0m  [16/84], [94mLoss[0m : 2.45840
[1mStep[0m  [24/84], [94mLoss[0m : 2.14389
[1mStep[0m  [32/84], [94mLoss[0m : 2.72655
[1mStep[0m  [40/84], [94mLoss[0m : 2.62927
[1mStep[0m  [48/84], [94mLoss[0m : 2.73124
[1mStep[0m  [56/84], [94mLoss[0m : 2.29474
[1mStep[0m  [64/84], [94mLoss[0m : 2.86512
[1mStep[0m  [72/84], [94mLoss[0m : 2.63138
[1mStep[0m  [80/84], [94mLoss[0m : 2.12465

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83868
[1mStep[0m  [8/84], [94mLoss[0m : 2.69316
[1mStep[0m  [16/84], [94mLoss[0m : 2.64728
[1mStep[0m  [24/84], [94mLoss[0m : 2.02775
[1mStep[0m  [32/84], [94mLoss[0m : 2.78622
[1mStep[0m  [40/84], [94mLoss[0m : 2.48750
[1mStep[0m  [48/84], [94mLoss[0m : 2.40657
[1mStep[0m  [56/84], [94mLoss[0m : 2.89699
[1mStep[0m  [64/84], [94mLoss[0m : 2.46633
[1mStep[0m  [72/84], [94mLoss[0m : 2.52293
[1mStep[0m  [80/84], [94mLoss[0m : 2.55098

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43279
[1mStep[0m  [8/84], [94mLoss[0m : 2.62266
[1mStep[0m  [16/84], [94mLoss[0m : 2.35056
[1mStep[0m  [24/84], [94mLoss[0m : 2.33975
[1mStep[0m  [32/84], [94mLoss[0m : 2.15781
[1mStep[0m  [40/84], [94mLoss[0m : 2.88588
[1mStep[0m  [48/84], [94mLoss[0m : 2.44528
[1mStep[0m  [56/84], [94mLoss[0m : 2.33674
[1mStep[0m  [64/84], [94mLoss[0m : 2.52224
[1mStep[0m  [72/84], [94mLoss[0m : 2.56699
[1mStep[0m  [80/84], [94mLoss[0m : 2.62392

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26284
[1mStep[0m  [8/84], [94mLoss[0m : 2.40951
[1mStep[0m  [16/84], [94mLoss[0m : 2.42987
[1mStep[0m  [24/84], [94mLoss[0m : 2.24726
[1mStep[0m  [32/84], [94mLoss[0m : 2.42057
[1mStep[0m  [40/84], [94mLoss[0m : 2.34568
[1mStep[0m  [48/84], [94mLoss[0m : 2.84883
[1mStep[0m  [56/84], [94mLoss[0m : 2.68481
[1mStep[0m  [64/84], [94mLoss[0m : 2.30994
[1mStep[0m  [72/84], [94mLoss[0m : 2.05839
[1mStep[0m  [80/84], [94mLoss[0m : 2.75605

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58444
[1mStep[0m  [8/84], [94mLoss[0m : 2.66844
[1mStep[0m  [16/84], [94mLoss[0m : 2.37022
[1mStep[0m  [24/84], [94mLoss[0m : 2.52550
[1mStep[0m  [32/84], [94mLoss[0m : 2.45445
[1mStep[0m  [40/84], [94mLoss[0m : 2.72380
[1mStep[0m  [48/84], [94mLoss[0m : 2.63874
[1mStep[0m  [56/84], [94mLoss[0m : 2.45112
[1mStep[0m  [64/84], [94mLoss[0m : 2.56211
[1mStep[0m  [72/84], [94mLoss[0m : 2.41664
[1mStep[0m  [80/84], [94mLoss[0m : 2.61783

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31352
[1mStep[0m  [8/84], [94mLoss[0m : 2.50773
[1mStep[0m  [16/84], [94mLoss[0m : 2.20820
[1mStep[0m  [24/84], [94mLoss[0m : 2.59731
[1mStep[0m  [32/84], [94mLoss[0m : 2.66441
[1mStep[0m  [40/84], [94mLoss[0m : 2.62185
[1mStep[0m  [48/84], [94mLoss[0m : 2.26677
[1mStep[0m  [56/84], [94mLoss[0m : 2.52737
[1mStep[0m  [64/84], [94mLoss[0m : 2.37490
[1mStep[0m  [72/84], [94mLoss[0m : 2.65460
[1mStep[0m  [80/84], [94mLoss[0m : 2.74092

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79741
[1mStep[0m  [8/84], [94mLoss[0m : 2.40593
[1mStep[0m  [16/84], [94mLoss[0m : 2.32054
[1mStep[0m  [24/84], [94mLoss[0m : 2.66014
[1mStep[0m  [32/84], [94mLoss[0m : 2.27585
[1mStep[0m  [40/84], [94mLoss[0m : 2.43668
[1mStep[0m  [48/84], [94mLoss[0m : 2.60179
[1mStep[0m  [56/84], [94mLoss[0m : 2.57099
[1mStep[0m  [64/84], [94mLoss[0m : 2.22399
[1mStep[0m  [72/84], [94mLoss[0m : 2.73315
[1mStep[0m  [80/84], [94mLoss[0m : 2.29038

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40827
[1mStep[0m  [8/84], [94mLoss[0m : 2.82517
[1mStep[0m  [16/84], [94mLoss[0m : 2.87437
[1mStep[0m  [24/84], [94mLoss[0m : 2.44632
[1mStep[0m  [32/84], [94mLoss[0m : 2.50192
[1mStep[0m  [40/84], [94mLoss[0m : 2.84111
[1mStep[0m  [48/84], [94mLoss[0m : 2.59542
[1mStep[0m  [56/84], [94mLoss[0m : 2.60647
[1mStep[0m  [64/84], [94mLoss[0m : 2.47697
[1mStep[0m  [72/84], [94mLoss[0m : 2.27138
[1mStep[0m  [80/84], [94mLoss[0m : 2.29168

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40552
[1mStep[0m  [8/84], [94mLoss[0m : 2.72787
[1mStep[0m  [16/84], [94mLoss[0m : 2.55208
[1mStep[0m  [24/84], [94mLoss[0m : 2.47348
[1mStep[0m  [32/84], [94mLoss[0m : 2.22461
[1mStep[0m  [40/84], [94mLoss[0m : 2.34657
[1mStep[0m  [48/84], [94mLoss[0m : 2.24960
[1mStep[0m  [56/84], [94mLoss[0m : 2.52915
[1mStep[0m  [64/84], [94mLoss[0m : 2.56041
[1mStep[0m  [72/84], [94mLoss[0m : 2.79867
[1mStep[0m  [80/84], [94mLoss[0m : 2.39361

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53561
[1mStep[0m  [8/84], [94mLoss[0m : 2.40862
[1mStep[0m  [16/84], [94mLoss[0m : 2.32101
[1mStep[0m  [24/84], [94mLoss[0m : 2.37699
[1mStep[0m  [32/84], [94mLoss[0m : 2.28611
[1mStep[0m  [40/84], [94mLoss[0m : 2.37871
[1mStep[0m  [48/84], [94mLoss[0m : 2.13329
[1mStep[0m  [56/84], [94mLoss[0m : 2.50330
[1mStep[0m  [64/84], [94mLoss[0m : 2.25944
[1mStep[0m  [72/84], [94mLoss[0m : 2.44276
[1mStep[0m  [80/84], [94mLoss[0m : 2.21957

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26651
[1mStep[0m  [8/84], [94mLoss[0m : 2.43827
[1mStep[0m  [16/84], [94mLoss[0m : 2.44007
[1mStep[0m  [24/84], [94mLoss[0m : 2.40276
[1mStep[0m  [32/84], [94mLoss[0m : 2.22713
[1mStep[0m  [40/84], [94mLoss[0m : 2.04761
[1mStep[0m  [48/84], [94mLoss[0m : 2.27897
[1mStep[0m  [56/84], [94mLoss[0m : 2.44041
[1mStep[0m  [64/84], [94mLoss[0m : 2.41820
[1mStep[0m  [72/84], [94mLoss[0m : 2.67026
[1mStep[0m  [80/84], [94mLoss[0m : 2.58464

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.349
====================================

Phase 1 - Evaluation MAE:  2.348614743777684
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.56899
[1mStep[0m  [8/84], [94mLoss[0m : 2.78446
[1mStep[0m  [16/84], [94mLoss[0m : 2.14516
[1mStep[0m  [24/84], [94mLoss[0m : 2.36213
[1mStep[0m  [32/84], [94mLoss[0m : 2.42630
[1mStep[0m  [40/84], [94mLoss[0m : 2.60741
[1mStep[0m  [48/84], [94mLoss[0m : 2.53552
[1mStep[0m  [56/84], [94mLoss[0m : 2.44692
[1mStep[0m  [64/84], [94mLoss[0m : 2.50389
[1mStep[0m  [72/84], [94mLoss[0m : 2.65084
[1mStep[0m  [80/84], [94mLoss[0m : 2.34555

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51567
[1mStep[0m  [8/84], [94mLoss[0m : 2.86548
[1mStep[0m  [16/84], [94mLoss[0m : 2.68916
[1mStep[0m  [24/84], [94mLoss[0m : 2.75610
[1mStep[0m  [32/84], [94mLoss[0m : 2.42628
[1mStep[0m  [40/84], [94mLoss[0m : 2.49265
[1mStep[0m  [48/84], [94mLoss[0m : 2.37184
[1mStep[0m  [56/84], [94mLoss[0m : 2.42311
[1mStep[0m  [64/84], [94mLoss[0m : 2.35648
[1mStep[0m  [72/84], [94mLoss[0m : 2.33740
[1mStep[0m  [80/84], [94mLoss[0m : 2.24457

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38453
[1mStep[0m  [8/84], [94mLoss[0m : 2.44825
[1mStep[0m  [16/84], [94mLoss[0m : 2.50502
[1mStep[0m  [24/84], [94mLoss[0m : 2.38635
[1mStep[0m  [32/84], [94mLoss[0m : 2.71265
[1mStep[0m  [40/84], [94mLoss[0m : 2.32759
[1mStep[0m  [48/84], [94mLoss[0m : 2.47750
[1mStep[0m  [56/84], [94mLoss[0m : 2.54974
[1mStep[0m  [64/84], [94mLoss[0m : 2.33477
[1mStep[0m  [72/84], [94mLoss[0m : 2.35244
[1mStep[0m  [80/84], [94mLoss[0m : 2.35000

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41492
[1mStep[0m  [8/84], [94mLoss[0m : 2.23057
[1mStep[0m  [16/84], [94mLoss[0m : 2.20993
[1mStep[0m  [24/84], [94mLoss[0m : 2.66427
[1mStep[0m  [32/84], [94mLoss[0m : 2.24624
[1mStep[0m  [40/84], [94mLoss[0m : 2.51967
[1mStep[0m  [48/84], [94mLoss[0m : 2.10835
[1mStep[0m  [56/84], [94mLoss[0m : 2.12465
[1mStep[0m  [64/84], [94mLoss[0m : 2.39312
[1mStep[0m  [72/84], [94mLoss[0m : 2.44284
[1mStep[0m  [80/84], [94mLoss[0m : 2.15756

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52733
[1mStep[0m  [8/84], [94mLoss[0m : 2.57289
[1mStep[0m  [16/84], [94mLoss[0m : 2.35599
[1mStep[0m  [24/84], [94mLoss[0m : 2.30734
[1mStep[0m  [32/84], [94mLoss[0m : 2.59781
[1mStep[0m  [40/84], [94mLoss[0m : 2.21517
[1mStep[0m  [48/84], [94mLoss[0m : 2.38802
[1mStep[0m  [56/84], [94mLoss[0m : 2.71923
[1mStep[0m  [64/84], [94mLoss[0m : 2.13612
[1mStep[0m  [72/84], [94mLoss[0m : 2.15262
[1mStep[0m  [80/84], [94mLoss[0m : 2.26369

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25820
[1mStep[0m  [8/84], [94mLoss[0m : 2.27157
[1mStep[0m  [16/84], [94mLoss[0m : 2.27513
[1mStep[0m  [24/84], [94mLoss[0m : 2.07791
[1mStep[0m  [32/84], [94mLoss[0m : 2.32925
[1mStep[0m  [40/84], [94mLoss[0m : 2.24201
[1mStep[0m  [48/84], [94mLoss[0m : 2.21316
[1mStep[0m  [56/84], [94mLoss[0m : 2.15728
[1mStep[0m  [64/84], [94mLoss[0m : 2.35558
[1mStep[0m  [72/84], [94mLoss[0m : 2.13620
[1mStep[0m  [80/84], [94mLoss[0m : 2.25755

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05167
[1mStep[0m  [8/84], [94mLoss[0m : 2.43639
[1mStep[0m  [16/84], [94mLoss[0m : 2.10636
[1mStep[0m  [24/84], [94mLoss[0m : 2.51466
[1mStep[0m  [32/84], [94mLoss[0m : 2.20435
[1mStep[0m  [40/84], [94mLoss[0m : 2.32646
[1mStep[0m  [48/84], [94mLoss[0m : 2.22147
[1mStep[0m  [56/84], [94mLoss[0m : 2.46668
[1mStep[0m  [64/84], [94mLoss[0m : 2.10637
[1mStep[0m  [72/84], [94mLoss[0m : 2.18751
[1mStep[0m  [80/84], [94mLoss[0m : 2.41302

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.242, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11407
[1mStep[0m  [8/84], [94mLoss[0m : 2.13752
[1mStep[0m  [16/84], [94mLoss[0m : 2.15738
[1mStep[0m  [24/84], [94mLoss[0m : 2.18452
[1mStep[0m  [32/84], [94mLoss[0m : 2.23688
[1mStep[0m  [40/84], [94mLoss[0m : 2.34547
[1mStep[0m  [48/84], [94mLoss[0m : 2.41332
[1mStep[0m  [56/84], [94mLoss[0m : 1.86082
[1mStep[0m  [64/84], [94mLoss[0m : 2.25885
[1mStep[0m  [72/84], [94mLoss[0m : 2.24188
[1mStep[0m  [80/84], [94mLoss[0m : 2.01913

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92191
[1mStep[0m  [8/84], [94mLoss[0m : 2.07178
[1mStep[0m  [16/84], [94mLoss[0m : 2.15481
[1mStep[0m  [24/84], [94mLoss[0m : 1.84251
[1mStep[0m  [32/84], [94mLoss[0m : 2.14266
[1mStep[0m  [40/84], [94mLoss[0m : 2.19545
[1mStep[0m  [48/84], [94mLoss[0m : 2.48397
[1mStep[0m  [56/84], [94mLoss[0m : 2.48267
[1mStep[0m  [64/84], [94mLoss[0m : 2.14898
[1mStep[0m  [72/84], [94mLoss[0m : 2.22639
[1mStep[0m  [80/84], [94mLoss[0m : 2.12024

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12643
[1mStep[0m  [8/84], [94mLoss[0m : 2.27639
[1mStep[0m  [16/84], [94mLoss[0m : 2.05658
[1mStep[0m  [24/84], [94mLoss[0m : 2.21164
[1mStep[0m  [32/84], [94mLoss[0m : 2.33210
[1mStep[0m  [40/84], [94mLoss[0m : 2.63759
[1mStep[0m  [48/84], [94mLoss[0m : 1.92314
[1mStep[0m  [56/84], [94mLoss[0m : 2.32696
[1mStep[0m  [64/84], [94mLoss[0m : 2.18062
[1mStep[0m  [72/84], [94mLoss[0m : 2.09458
[1mStep[0m  [80/84], [94mLoss[0m : 2.21731

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88955
[1mStep[0m  [8/84], [94mLoss[0m : 2.01895
[1mStep[0m  [16/84], [94mLoss[0m : 1.86049
[1mStep[0m  [24/84], [94mLoss[0m : 1.88129
[1mStep[0m  [32/84], [94mLoss[0m : 1.91662
[1mStep[0m  [40/84], [94mLoss[0m : 1.98676
[1mStep[0m  [48/84], [94mLoss[0m : 2.14777
[1mStep[0m  [56/84], [94mLoss[0m : 1.77996
[1mStep[0m  [64/84], [94mLoss[0m : 2.02523
[1mStep[0m  [72/84], [94mLoss[0m : 2.14606
[1mStep[0m  [80/84], [94mLoss[0m : 2.20430

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06461
[1mStep[0m  [8/84], [94mLoss[0m : 2.18732
[1mStep[0m  [16/84], [94mLoss[0m : 2.22315
[1mStep[0m  [24/84], [94mLoss[0m : 1.97711
[1mStep[0m  [32/84], [94mLoss[0m : 1.85343
[1mStep[0m  [40/84], [94mLoss[0m : 1.74280
[1mStep[0m  [48/84], [94mLoss[0m : 2.09756
[1mStep[0m  [56/84], [94mLoss[0m : 2.06971
[1mStep[0m  [64/84], [94mLoss[0m : 1.99590
[1mStep[0m  [72/84], [94mLoss[0m : 2.27896
[1mStep[0m  [80/84], [94mLoss[0m : 2.29046

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91004
[1mStep[0m  [8/84], [94mLoss[0m : 2.02795
[1mStep[0m  [16/84], [94mLoss[0m : 1.70941
[1mStep[0m  [24/84], [94mLoss[0m : 1.95719
[1mStep[0m  [32/84], [94mLoss[0m : 1.98838
[1mStep[0m  [40/84], [94mLoss[0m : 1.82352
[1mStep[0m  [48/84], [94mLoss[0m : 1.82134
[1mStep[0m  [56/84], [94mLoss[0m : 2.11327
[1mStep[0m  [64/84], [94mLoss[0m : 1.71728
[1mStep[0m  [72/84], [94mLoss[0m : 2.05306
[1mStep[0m  [80/84], [94mLoss[0m : 1.98653

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.71631
[1mStep[0m  [8/84], [94mLoss[0m : 1.84643
[1mStep[0m  [16/84], [94mLoss[0m : 2.19907
[1mStep[0m  [24/84], [94mLoss[0m : 2.10871
[1mStep[0m  [32/84], [94mLoss[0m : 2.08701
[1mStep[0m  [40/84], [94mLoss[0m : 1.98144
[1mStep[0m  [48/84], [94mLoss[0m : 1.96384
[1mStep[0m  [56/84], [94mLoss[0m : 1.71482
[1mStep[0m  [64/84], [94mLoss[0m : 1.95022
[1mStep[0m  [72/84], [94mLoss[0m : 1.79614
[1mStep[0m  [80/84], [94mLoss[0m : 1.86671

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.946, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92596
[1mStep[0m  [8/84], [94mLoss[0m : 1.90340
[1mStep[0m  [16/84], [94mLoss[0m : 1.97623
[1mStep[0m  [24/84], [94mLoss[0m : 1.87417
[1mStep[0m  [32/84], [94mLoss[0m : 1.55220
[1mStep[0m  [40/84], [94mLoss[0m : 1.78711
[1mStep[0m  [48/84], [94mLoss[0m : 2.16304
[1mStep[0m  [56/84], [94mLoss[0m : 1.90346
[1mStep[0m  [64/84], [94mLoss[0m : 2.01490
[1mStep[0m  [72/84], [94mLoss[0m : 1.95120
[1mStep[0m  [80/84], [94mLoss[0m : 2.18134

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74297
[1mStep[0m  [8/84], [94mLoss[0m : 1.70653
[1mStep[0m  [16/84], [94mLoss[0m : 1.96794
[1mStep[0m  [24/84], [94mLoss[0m : 1.79963
[1mStep[0m  [32/84], [94mLoss[0m : 1.87818
[1mStep[0m  [40/84], [94mLoss[0m : 1.88826
[1mStep[0m  [48/84], [94mLoss[0m : 1.83071
[1mStep[0m  [56/84], [94mLoss[0m : 1.67260
[1mStep[0m  [64/84], [94mLoss[0m : 1.70981
[1mStep[0m  [72/84], [94mLoss[0m : 1.84668
[1mStep[0m  [80/84], [94mLoss[0m : 1.76615

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.89054
[1mStep[0m  [8/84], [94mLoss[0m : 1.81429
[1mStep[0m  [16/84], [94mLoss[0m : 2.08894
[1mStep[0m  [24/84], [94mLoss[0m : 1.72038
[1mStep[0m  [32/84], [94mLoss[0m : 2.13480
[1mStep[0m  [40/84], [94mLoss[0m : 1.98570
[1mStep[0m  [48/84], [94mLoss[0m : 1.73367
[1mStep[0m  [56/84], [94mLoss[0m : 1.88797
[1mStep[0m  [64/84], [94mLoss[0m : 2.01910
[1mStep[0m  [72/84], [94mLoss[0m : 1.84870
[1mStep[0m  [80/84], [94mLoss[0m : 1.52580

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.499, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68878
[1mStep[0m  [8/84], [94mLoss[0m : 1.80119
[1mStep[0m  [16/84], [94mLoss[0m : 1.71038
[1mStep[0m  [24/84], [94mLoss[0m : 1.92418
[1mStep[0m  [32/84], [94mLoss[0m : 1.80502
[1mStep[0m  [40/84], [94mLoss[0m : 1.83628
[1mStep[0m  [48/84], [94mLoss[0m : 2.05449
[1mStep[0m  [56/84], [94mLoss[0m : 1.64278
[1mStep[0m  [64/84], [94mLoss[0m : 1.59721
[1mStep[0m  [72/84], [94mLoss[0m : 1.52099
[1mStep[0m  [80/84], [94mLoss[0m : 1.89826

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.65642
[1mStep[0m  [8/84], [94mLoss[0m : 1.99786
[1mStep[0m  [16/84], [94mLoss[0m : 1.81749
[1mStep[0m  [24/84], [94mLoss[0m : 1.73661
[1mStep[0m  [32/84], [94mLoss[0m : 1.86991
[1mStep[0m  [40/84], [94mLoss[0m : 1.71245
[1mStep[0m  [48/84], [94mLoss[0m : 1.84000
[1mStep[0m  [56/84], [94mLoss[0m : 1.83529
[1mStep[0m  [64/84], [94mLoss[0m : 1.67829
[1mStep[0m  [72/84], [94mLoss[0m : 1.86139
[1mStep[0m  [80/84], [94mLoss[0m : 1.73218

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84064
[1mStep[0m  [8/84], [94mLoss[0m : 1.91817
[1mStep[0m  [16/84], [94mLoss[0m : 1.62404
[1mStep[0m  [24/84], [94mLoss[0m : 1.82494
[1mStep[0m  [32/84], [94mLoss[0m : 1.52835
[1mStep[0m  [40/84], [94mLoss[0m : 1.84038
[1mStep[0m  [48/84], [94mLoss[0m : 1.86620
[1mStep[0m  [56/84], [94mLoss[0m : 1.75170
[1mStep[0m  [64/84], [94mLoss[0m : 1.77175
[1mStep[0m  [72/84], [94mLoss[0m : 1.83704
[1mStep[0m  [80/84], [94mLoss[0m : 1.59192

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.573, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66074
[1mStep[0m  [8/84], [94mLoss[0m : 1.79335
[1mStep[0m  [16/84], [94mLoss[0m : 1.56724
[1mStep[0m  [24/84], [94mLoss[0m : 1.65890
[1mStep[0m  [32/84], [94mLoss[0m : 1.71544
[1mStep[0m  [40/84], [94mLoss[0m : 1.67708
[1mStep[0m  [48/84], [94mLoss[0m : 1.51723
[1mStep[0m  [56/84], [94mLoss[0m : 1.81631
[1mStep[0m  [64/84], [94mLoss[0m : 1.69597
[1mStep[0m  [72/84], [94mLoss[0m : 1.54991
[1mStep[0m  [80/84], [94mLoss[0m : 1.61063

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61515
[1mStep[0m  [8/84], [94mLoss[0m : 1.57886
[1mStep[0m  [16/84], [94mLoss[0m : 1.57498
[1mStep[0m  [24/84], [94mLoss[0m : 1.57697
[1mStep[0m  [32/84], [94mLoss[0m : 1.71702
[1mStep[0m  [40/84], [94mLoss[0m : 1.71278
[1mStep[0m  [48/84], [94mLoss[0m : 1.72963
[1mStep[0m  [56/84], [94mLoss[0m : 1.60447
[1mStep[0m  [64/84], [94mLoss[0m : 1.50142
[1mStep[0m  [72/84], [94mLoss[0m : 1.83179
[1mStep[0m  [80/84], [94mLoss[0m : 1.62817

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74553
[1mStep[0m  [8/84], [94mLoss[0m : 1.45759
[1mStep[0m  [16/84], [94mLoss[0m : 1.42290
[1mStep[0m  [24/84], [94mLoss[0m : 1.70915
[1mStep[0m  [32/84], [94mLoss[0m : 1.68714
[1mStep[0m  [40/84], [94mLoss[0m : 1.73820
[1mStep[0m  [48/84], [94mLoss[0m : 1.61484
[1mStep[0m  [56/84], [94mLoss[0m : 1.67305
[1mStep[0m  [64/84], [94mLoss[0m : 1.55389
[1mStep[0m  [72/84], [94mLoss[0m : 1.88517
[1mStep[0m  [80/84], [94mLoss[0m : 1.69901

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.501, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36754
[1mStep[0m  [8/84], [94mLoss[0m : 1.57301
[1mStep[0m  [16/84], [94mLoss[0m : 1.65549
[1mStep[0m  [24/84], [94mLoss[0m : 1.57735
[1mStep[0m  [32/84], [94mLoss[0m : 1.72972
[1mStep[0m  [40/84], [94mLoss[0m : 1.47496
[1mStep[0m  [48/84], [94mLoss[0m : 1.48540
[1mStep[0m  [56/84], [94mLoss[0m : 1.72483
[1mStep[0m  [64/84], [94mLoss[0m : 1.60124
[1mStep[0m  [72/84], [94mLoss[0m : 1.57893
[1mStep[0m  [80/84], [94mLoss[0m : 1.67572

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37317
[1mStep[0m  [8/84], [94mLoss[0m : 1.47665
[1mStep[0m  [16/84], [94mLoss[0m : 1.48923
[1mStep[0m  [24/84], [94mLoss[0m : 1.49314
[1mStep[0m  [32/84], [94mLoss[0m : 1.57992
[1mStep[0m  [40/84], [94mLoss[0m : 1.59432
[1mStep[0m  [48/84], [94mLoss[0m : 1.82264
[1mStep[0m  [56/84], [94mLoss[0m : 1.41347
[1mStep[0m  [64/84], [94mLoss[0m : 1.71482
[1mStep[0m  [72/84], [94mLoss[0m : 1.52485
[1mStep[0m  [80/84], [94mLoss[0m : 1.57424

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.584, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.32020
[1mStep[0m  [8/84], [94mLoss[0m : 1.34431
[1mStep[0m  [16/84], [94mLoss[0m : 1.68782
[1mStep[0m  [24/84], [94mLoss[0m : 1.46476
[1mStep[0m  [32/84], [94mLoss[0m : 1.73030
[1mStep[0m  [40/84], [94mLoss[0m : 1.35891
[1mStep[0m  [48/84], [94mLoss[0m : 1.68622
[1mStep[0m  [56/84], [94mLoss[0m : 1.53028
[1mStep[0m  [64/84], [94mLoss[0m : 1.58710
[1mStep[0m  [72/84], [94mLoss[0m : 1.55878
[1mStep[0m  [80/84], [94mLoss[0m : 1.49907

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.60525
[1mStep[0m  [8/84], [94mLoss[0m : 1.40489
[1mStep[0m  [16/84], [94mLoss[0m : 1.57399
[1mStep[0m  [24/84], [94mLoss[0m : 1.51167
[1mStep[0m  [32/84], [94mLoss[0m : 1.86441
[1mStep[0m  [40/84], [94mLoss[0m : 1.59416
[1mStep[0m  [48/84], [94mLoss[0m : 1.69368
[1mStep[0m  [56/84], [94mLoss[0m : 1.77720
[1mStep[0m  [64/84], [94mLoss[0m : 1.52412
[1mStep[0m  [72/84], [94mLoss[0m : 1.53078
[1mStep[0m  [80/84], [94mLoss[0m : 1.71259

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.497, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.67057
[1mStep[0m  [8/84], [94mLoss[0m : 1.50378
[1mStep[0m  [16/84], [94mLoss[0m : 1.47259
[1mStep[0m  [24/84], [94mLoss[0m : 1.58741
[1mStep[0m  [32/84], [94mLoss[0m : 1.46155
[1mStep[0m  [40/84], [94mLoss[0m : 1.34594
[1mStep[0m  [48/84], [94mLoss[0m : 1.56347
[1mStep[0m  [56/84], [94mLoss[0m : 1.54505
[1mStep[0m  [64/84], [94mLoss[0m : 1.33615
[1mStep[0m  [72/84], [94mLoss[0m : 1.33982
[1mStep[0m  [80/84], [94mLoss[0m : 1.45572

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46071
[1mStep[0m  [8/84], [94mLoss[0m : 1.58241
[1mStep[0m  [16/84], [94mLoss[0m : 1.63942
[1mStep[0m  [24/84], [94mLoss[0m : 1.50762
[1mStep[0m  [32/84], [94mLoss[0m : 1.43797
[1mStep[0m  [40/84], [94mLoss[0m : 1.63097
[1mStep[0m  [48/84], [94mLoss[0m : 1.46340
[1mStep[0m  [56/84], [94mLoss[0m : 1.60312
[1mStep[0m  [64/84], [94mLoss[0m : 1.35707
[1mStep[0m  [72/84], [94mLoss[0m : 1.41647
[1mStep[0m  [80/84], [94mLoss[0m : 1.50324

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76413
[1mStep[0m  [8/84], [94mLoss[0m : 1.75016
[1mStep[0m  [16/84], [94mLoss[0m : 1.47796
[1mStep[0m  [24/84], [94mLoss[0m : 1.75702
[1mStep[0m  [32/84], [94mLoss[0m : 1.41593
[1mStep[0m  [40/84], [94mLoss[0m : 1.51279
[1mStep[0m  [48/84], [94mLoss[0m : 1.54130
[1mStep[0m  [56/84], [94mLoss[0m : 1.71223
[1mStep[0m  [64/84], [94mLoss[0m : 1.54536
[1mStep[0m  [72/84], [94mLoss[0m : 1.44925
[1mStep[0m  [80/84], [94mLoss[0m : 1.45527

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.522, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.500
====================================

Phase 2 - Evaluation MAE:  2.500001175062997
MAE score P1        2.348615
MAE score P2        2.500001
loss                1.476811
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay           0.001
Name: 30, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.55880
[1mStep[0m  [4/42], [94mLoss[0m : 11.17169
[1mStep[0m  [8/42], [94mLoss[0m : 10.56949
[1mStep[0m  [12/42], [94mLoss[0m : 10.51223
[1mStep[0m  [16/42], [94mLoss[0m : 10.46595
[1mStep[0m  [20/42], [94mLoss[0m : 9.91680
[1mStep[0m  [24/42], [94mLoss[0m : 10.27123
[1mStep[0m  [28/42], [94mLoss[0m : 9.47887
[1mStep[0m  [32/42], [94mLoss[0m : 9.45896
[1mStep[0m  [36/42], [94mLoss[0m : 8.99434
[1mStep[0m  [40/42], [94mLoss[0m : 8.82445

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.115, [92mTest[0m: 10.895, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.80322
[1mStep[0m  [4/42], [94mLoss[0m : 8.03296
[1mStep[0m  [8/42], [94mLoss[0m : 7.82363
[1mStep[0m  [12/42], [94mLoss[0m : 7.72314
[1mStep[0m  [16/42], [94mLoss[0m : 7.12025
[1mStep[0m  [20/42], [94mLoss[0m : 6.58670
[1mStep[0m  [24/42], [94mLoss[0m : 6.45586
[1mStep[0m  [28/42], [94mLoss[0m : 6.00352
[1mStep[0m  [32/42], [94mLoss[0m : 5.68185
[1mStep[0m  [36/42], [94mLoss[0m : 5.26150
[1mStep[0m  [40/42], [94mLoss[0m : 4.98622

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.737, [92mTest[0m: 8.503, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.55183
[1mStep[0m  [4/42], [94mLoss[0m : 3.84354
[1mStep[0m  [8/42], [94mLoss[0m : 3.39019
[1mStep[0m  [12/42], [94mLoss[0m : 3.21663
[1mStep[0m  [16/42], [94mLoss[0m : 2.75079
[1mStep[0m  [20/42], [94mLoss[0m : 2.60531
[1mStep[0m  [24/42], [94mLoss[0m : 2.95715
[1mStep[0m  [28/42], [94mLoss[0m : 2.77450
[1mStep[0m  [32/42], [94mLoss[0m : 2.72338
[1mStep[0m  [36/42], [94mLoss[0m : 2.89424
[1mStep[0m  [40/42], [94mLoss[0m : 2.79808

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.095, [92mTest[0m: 3.632, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79296
[1mStep[0m  [4/42], [94mLoss[0m : 2.68064
[1mStep[0m  [8/42], [94mLoss[0m : 2.76636
[1mStep[0m  [12/42], [94mLoss[0m : 2.71162
[1mStep[0m  [16/42], [94mLoss[0m : 2.80720
[1mStep[0m  [20/42], [94mLoss[0m : 2.39532
[1mStep[0m  [24/42], [94mLoss[0m : 2.38824
[1mStep[0m  [28/42], [94mLoss[0m : 2.69923
[1mStep[0m  [32/42], [94mLoss[0m : 2.42461
[1mStep[0m  [36/42], [94mLoss[0m : 2.62729
[1mStep[0m  [40/42], [94mLoss[0m : 2.56195

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64343
[1mStep[0m  [4/42], [94mLoss[0m : 2.53359
[1mStep[0m  [8/42], [94mLoss[0m : 2.69003
[1mStep[0m  [12/42], [94mLoss[0m : 2.57259
[1mStep[0m  [16/42], [94mLoss[0m : 2.72935
[1mStep[0m  [20/42], [94mLoss[0m : 2.72019
[1mStep[0m  [24/42], [94mLoss[0m : 2.86716
[1mStep[0m  [28/42], [94mLoss[0m : 2.76358
[1mStep[0m  [32/42], [94mLoss[0m : 2.59629
[1mStep[0m  [36/42], [94mLoss[0m : 2.80535
[1mStep[0m  [40/42], [94mLoss[0m : 2.59337

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47263
[1mStep[0m  [4/42], [94mLoss[0m : 2.54452
[1mStep[0m  [8/42], [94mLoss[0m : 2.58595
[1mStep[0m  [12/42], [94mLoss[0m : 2.56665
[1mStep[0m  [16/42], [94mLoss[0m : 2.54144
[1mStep[0m  [20/42], [94mLoss[0m : 2.59614
[1mStep[0m  [24/42], [94mLoss[0m : 2.43812
[1mStep[0m  [28/42], [94mLoss[0m : 2.47993
[1mStep[0m  [32/42], [94mLoss[0m : 2.46782
[1mStep[0m  [36/42], [94mLoss[0m : 2.52646
[1mStep[0m  [40/42], [94mLoss[0m : 2.48209

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47966
[1mStep[0m  [4/42], [94mLoss[0m : 2.70774
[1mStep[0m  [8/42], [94mLoss[0m : 2.47084
[1mStep[0m  [12/42], [94mLoss[0m : 2.56824
[1mStep[0m  [16/42], [94mLoss[0m : 2.56743
[1mStep[0m  [20/42], [94mLoss[0m : 2.66293
[1mStep[0m  [24/42], [94mLoss[0m : 2.57940
[1mStep[0m  [28/42], [94mLoss[0m : 2.59297
[1mStep[0m  [32/42], [94mLoss[0m : 2.61360
[1mStep[0m  [36/42], [94mLoss[0m : 2.37634
[1mStep[0m  [40/42], [94mLoss[0m : 2.67971

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29374
[1mStep[0m  [4/42], [94mLoss[0m : 2.54646
[1mStep[0m  [8/42], [94mLoss[0m : 2.73747
[1mStep[0m  [12/42], [94mLoss[0m : 2.38947
[1mStep[0m  [16/42], [94mLoss[0m : 2.78405
[1mStep[0m  [20/42], [94mLoss[0m : 2.64190
[1mStep[0m  [24/42], [94mLoss[0m : 2.72082
[1mStep[0m  [28/42], [94mLoss[0m : 2.64775
[1mStep[0m  [32/42], [94mLoss[0m : 2.76135
[1mStep[0m  [36/42], [94mLoss[0m : 2.49010
[1mStep[0m  [40/42], [94mLoss[0m : 2.35379

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52647
[1mStep[0m  [4/42], [94mLoss[0m : 2.35605
[1mStep[0m  [8/42], [94mLoss[0m : 2.51686
[1mStep[0m  [12/42], [94mLoss[0m : 2.47098
[1mStep[0m  [16/42], [94mLoss[0m : 2.56348
[1mStep[0m  [20/42], [94mLoss[0m : 2.61995
[1mStep[0m  [24/42], [94mLoss[0m : 2.57719
[1mStep[0m  [28/42], [94mLoss[0m : 2.48453
[1mStep[0m  [32/42], [94mLoss[0m : 2.45370
[1mStep[0m  [36/42], [94mLoss[0m : 2.53546
[1mStep[0m  [40/42], [94mLoss[0m : 2.41492

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65464
[1mStep[0m  [4/42], [94mLoss[0m : 2.49652
[1mStep[0m  [8/42], [94mLoss[0m : 2.67029
[1mStep[0m  [12/42], [94mLoss[0m : 2.62921
[1mStep[0m  [16/42], [94mLoss[0m : 2.43013
[1mStep[0m  [20/42], [94mLoss[0m : 2.68382
[1mStep[0m  [24/42], [94mLoss[0m : 2.53258
[1mStep[0m  [28/42], [94mLoss[0m : 2.65010
[1mStep[0m  [32/42], [94mLoss[0m : 2.31892
[1mStep[0m  [36/42], [94mLoss[0m : 2.55536
[1mStep[0m  [40/42], [94mLoss[0m : 2.63424

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60482
[1mStep[0m  [4/42], [94mLoss[0m : 2.35559
[1mStep[0m  [8/42], [94mLoss[0m : 2.60792
[1mStep[0m  [12/42], [94mLoss[0m : 2.64021
[1mStep[0m  [16/42], [94mLoss[0m : 2.65928
[1mStep[0m  [20/42], [94mLoss[0m : 2.64276
[1mStep[0m  [24/42], [94mLoss[0m : 2.46309
[1mStep[0m  [28/42], [94mLoss[0m : 2.50102
[1mStep[0m  [32/42], [94mLoss[0m : 2.52902
[1mStep[0m  [36/42], [94mLoss[0m : 2.32222
[1mStep[0m  [40/42], [94mLoss[0m : 2.48129

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34965
[1mStep[0m  [4/42], [94mLoss[0m : 2.43881
[1mStep[0m  [8/42], [94mLoss[0m : 2.65068
[1mStep[0m  [12/42], [94mLoss[0m : 2.54949
[1mStep[0m  [16/42], [94mLoss[0m : 2.64550
[1mStep[0m  [20/42], [94mLoss[0m : 2.61587
[1mStep[0m  [24/42], [94mLoss[0m : 2.55654
[1mStep[0m  [28/42], [94mLoss[0m : 2.32453
[1mStep[0m  [32/42], [94mLoss[0m : 2.39427
[1mStep[0m  [36/42], [94mLoss[0m : 2.59671
[1mStep[0m  [40/42], [94mLoss[0m : 2.30277

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62627
[1mStep[0m  [4/42], [94mLoss[0m : 2.54637
[1mStep[0m  [8/42], [94mLoss[0m : 2.51482
[1mStep[0m  [12/42], [94mLoss[0m : 2.34111
[1mStep[0m  [16/42], [94mLoss[0m : 2.43743
[1mStep[0m  [20/42], [94mLoss[0m : 2.58057
[1mStep[0m  [24/42], [94mLoss[0m : 2.47156
[1mStep[0m  [28/42], [94mLoss[0m : 2.30116
[1mStep[0m  [32/42], [94mLoss[0m : 2.43551
[1mStep[0m  [36/42], [94mLoss[0m : 2.46440
[1mStep[0m  [40/42], [94mLoss[0m : 2.31710

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20373
[1mStep[0m  [4/42], [94mLoss[0m : 2.18462
[1mStep[0m  [8/42], [94mLoss[0m : 2.48779
[1mStep[0m  [12/42], [94mLoss[0m : 2.62439
[1mStep[0m  [16/42], [94mLoss[0m : 2.48226
[1mStep[0m  [20/42], [94mLoss[0m : 2.45469
[1mStep[0m  [24/42], [94mLoss[0m : 2.68350
[1mStep[0m  [28/42], [94mLoss[0m : 2.62474
[1mStep[0m  [32/42], [94mLoss[0m : 2.72131
[1mStep[0m  [36/42], [94mLoss[0m : 2.54822
[1mStep[0m  [40/42], [94mLoss[0m : 2.38161

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35151
[1mStep[0m  [4/42], [94mLoss[0m : 2.58669
[1mStep[0m  [8/42], [94mLoss[0m : 2.48470
[1mStep[0m  [12/42], [94mLoss[0m : 2.51367
[1mStep[0m  [16/42], [94mLoss[0m : 2.44459
[1mStep[0m  [20/42], [94mLoss[0m : 2.34866
[1mStep[0m  [24/42], [94mLoss[0m : 2.60023
[1mStep[0m  [28/42], [94mLoss[0m : 2.30680
[1mStep[0m  [32/42], [94mLoss[0m : 2.34312
[1mStep[0m  [36/42], [94mLoss[0m : 2.46898
[1mStep[0m  [40/42], [94mLoss[0m : 2.21603

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38411
[1mStep[0m  [4/42], [94mLoss[0m : 2.46900
[1mStep[0m  [8/42], [94mLoss[0m : 2.49317
[1mStep[0m  [12/42], [94mLoss[0m : 2.46119
[1mStep[0m  [16/42], [94mLoss[0m : 2.34667
[1mStep[0m  [20/42], [94mLoss[0m : 2.52173
[1mStep[0m  [24/42], [94mLoss[0m : 2.51515
[1mStep[0m  [28/42], [94mLoss[0m : 2.35927
[1mStep[0m  [32/42], [94mLoss[0m : 2.58931
[1mStep[0m  [36/42], [94mLoss[0m : 2.56179
[1mStep[0m  [40/42], [94mLoss[0m : 2.26547

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39172
[1mStep[0m  [4/42], [94mLoss[0m : 2.35061
[1mStep[0m  [8/42], [94mLoss[0m : 2.26792
[1mStep[0m  [12/42], [94mLoss[0m : 2.52844
[1mStep[0m  [16/42], [94mLoss[0m : 2.31608
[1mStep[0m  [20/42], [94mLoss[0m : 2.50489
[1mStep[0m  [24/42], [94mLoss[0m : 2.48835
[1mStep[0m  [28/42], [94mLoss[0m : 2.36164
[1mStep[0m  [32/42], [94mLoss[0m : 2.38660
[1mStep[0m  [36/42], [94mLoss[0m : 2.18240
[1mStep[0m  [40/42], [94mLoss[0m : 2.43583

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38892
[1mStep[0m  [4/42], [94mLoss[0m : 2.36877
[1mStep[0m  [8/42], [94mLoss[0m : 2.35937
[1mStep[0m  [12/42], [94mLoss[0m : 2.35357
[1mStep[0m  [16/42], [94mLoss[0m : 2.62553
[1mStep[0m  [20/42], [94mLoss[0m : 2.35479
[1mStep[0m  [24/42], [94mLoss[0m : 2.15573
[1mStep[0m  [28/42], [94mLoss[0m : 2.27343
[1mStep[0m  [32/42], [94mLoss[0m : 2.41639
[1mStep[0m  [36/42], [94mLoss[0m : 2.30666
[1mStep[0m  [40/42], [94mLoss[0m : 2.37832

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46752
[1mStep[0m  [4/42], [94mLoss[0m : 2.31295
[1mStep[0m  [8/42], [94mLoss[0m : 2.42279
[1mStep[0m  [12/42], [94mLoss[0m : 2.49829
[1mStep[0m  [16/42], [94mLoss[0m : 2.58481
[1mStep[0m  [20/42], [94mLoss[0m : 2.54099
[1mStep[0m  [24/42], [94mLoss[0m : 2.45058
[1mStep[0m  [28/42], [94mLoss[0m : 2.06329
[1mStep[0m  [32/42], [94mLoss[0m : 2.43939
[1mStep[0m  [36/42], [94mLoss[0m : 2.27751
[1mStep[0m  [40/42], [94mLoss[0m : 2.65926

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41258
[1mStep[0m  [4/42], [94mLoss[0m : 2.42530
[1mStep[0m  [8/42], [94mLoss[0m : 2.34188
[1mStep[0m  [12/42], [94mLoss[0m : 2.49534
[1mStep[0m  [16/42], [94mLoss[0m : 2.59119
[1mStep[0m  [20/42], [94mLoss[0m : 2.48794
[1mStep[0m  [24/42], [94mLoss[0m : 2.43347
[1mStep[0m  [28/42], [94mLoss[0m : 2.22652
[1mStep[0m  [32/42], [94mLoss[0m : 2.33104
[1mStep[0m  [36/42], [94mLoss[0m : 2.50079
[1mStep[0m  [40/42], [94mLoss[0m : 2.37682

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29170
[1mStep[0m  [4/42], [94mLoss[0m : 2.20014
[1mStep[0m  [8/42], [94mLoss[0m : 2.30279
[1mStep[0m  [12/42], [94mLoss[0m : 2.52801
[1mStep[0m  [16/42], [94mLoss[0m : 2.61222
[1mStep[0m  [20/42], [94mLoss[0m : 2.51665
[1mStep[0m  [24/42], [94mLoss[0m : 2.48821
[1mStep[0m  [28/42], [94mLoss[0m : 2.28792
[1mStep[0m  [32/42], [94mLoss[0m : 2.41766
[1mStep[0m  [36/42], [94mLoss[0m : 2.24599
[1mStep[0m  [40/42], [94mLoss[0m : 2.37543

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34846
[1mStep[0m  [4/42], [94mLoss[0m : 2.39789
[1mStep[0m  [8/42], [94mLoss[0m : 2.22941
[1mStep[0m  [12/42], [94mLoss[0m : 2.34967
[1mStep[0m  [16/42], [94mLoss[0m : 2.34078
[1mStep[0m  [20/42], [94mLoss[0m : 2.40826
[1mStep[0m  [24/42], [94mLoss[0m : 2.23452
[1mStep[0m  [28/42], [94mLoss[0m : 2.35908
[1mStep[0m  [32/42], [94mLoss[0m : 2.56319
[1mStep[0m  [36/42], [94mLoss[0m : 2.22725
[1mStep[0m  [40/42], [94mLoss[0m : 2.51676

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.316, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40333
[1mStep[0m  [4/42], [94mLoss[0m : 2.55961
[1mStep[0m  [8/42], [94mLoss[0m : 2.43616
[1mStep[0m  [12/42], [94mLoss[0m : 2.49506
[1mStep[0m  [16/42], [94mLoss[0m : 2.46477
[1mStep[0m  [20/42], [94mLoss[0m : 2.42460
[1mStep[0m  [24/42], [94mLoss[0m : 2.39843
[1mStep[0m  [28/42], [94mLoss[0m : 2.52617
[1mStep[0m  [32/42], [94mLoss[0m : 2.37562
[1mStep[0m  [36/42], [94mLoss[0m : 2.36654
[1mStep[0m  [40/42], [94mLoss[0m : 2.27966

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35845
[1mStep[0m  [4/42], [94mLoss[0m : 2.43948
[1mStep[0m  [8/42], [94mLoss[0m : 2.55712
[1mStep[0m  [12/42], [94mLoss[0m : 2.37569
[1mStep[0m  [16/42], [94mLoss[0m : 2.45807
[1mStep[0m  [20/42], [94mLoss[0m : 2.56133
[1mStep[0m  [24/42], [94mLoss[0m : 2.39812
[1mStep[0m  [28/42], [94mLoss[0m : 2.41676
[1mStep[0m  [32/42], [94mLoss[0m : 2.20131
[1mStep[0m  [36/42], [94mLoss[0m : 2.66720
[1mStep[0m  [40/42], [94mLoss[0m : 2.30770

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18736
[1mStep[0m  [4/42], [94mLoss[0m : 2.50146
[1mStep[0m  [8/42], [94mLoss[0m : 2.15264
[1mStep[0m  [12/42], [94mLoss[0m : 2.50955
[1mStep[0m  [16/42], [94mLoss[0m : 2.41469
[1mStep[0m  [20/42], [94mLoss[0m : 2.29436
[1mStep[0m  [24/42], [94mLoss[0m : 2.39031
[1mStep[0m  [28/42], [94mLoss[0m : 2.23598
[1mStep[0m  [32/42], [94mLoss[0m : 2.18941
[1mStep[0m  [36/42], [94mLoss[0m : 2.34437
[1mStep[0m  [40/42], [94mLoss[0m : 2.29704

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.314, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43789
[1mStep[0m  [4/42], [94mLoss[0m : 2.39398
[1mStep[0m  [8/42], [94mLoss[0m : 2.26731
[1mStep[0m  [12/42], [94mLoss[0m : 2.42644
[1mStep[0m  [16/42], [94mLoss[0m : 2.53532
[1mStep[0m  [20/42], [94mLoss[0m : 2.39255
[1mStep[0m  [24/42], [94mLoss[0m : 2.34600
[1mStep[0m  [28/42], [94mLoss[0m : 2.47687
[1mStep[0m  [32/42], [94mLoss[0m : 2.25343
[1mStep[0m  [36/42], [94mLoss[0m : 2.38152
[1mStep[0m  [40/42], [94mLoss[0m : 2.43303

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44550
[1mStep[0m  [4/42], [94mLoss[0m : 2.58578
[1mStep[0m  [8/42], [94mLoss[0m : 2.34920
[1mStep[0m  [12/42], [94mLoss[0m : 2.41063
[1mStep[0m  [16/42], [94mLoss[0m : 2.54348
[1mStep[0m  [20/42], [94mLoss[0m : 2.36982
[1mStep[0m  [24/42], [94mLoss[0m : 2.49455
[1mStep[0m  [28/42], [94mLoss[0m : 2.38253
[1mStep[0m  [32/42], [94mLoss[0m : 2.43245
[1mStep[0m  [36/42], [94mLoss[0m : 2.34413
[1mStep[0m  [40/42], [94mLoss[0m : 2.28607

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.325, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29595
[1mStep[0m  [4/42], [94mLoss[0m : 2.43633
[1mStep[0m  [8/42], [94mLoss[0m : 2.30216
[1mStep[0m  [12/42], [94mLoss[0m : 2.50599
[1mStep[0m  [16/42], [94mLoss[0m : 2.46242
[1mStep[0m  [20/42], [94mLoss[0m : 2.36954
[1mStep[0m  [24/42], [94mLoss[0m : 2.40282
[1mStep[0m  [28/42], [94mLoss[0m : 2.33861
[1mStep[0m  [32/42], [94mLoss[0m : 2.30140
[1mStep[0m  [36/42], [94mLoss[0m : 2.60561
[1mStep[0m  [40/42], [94mLoss[0m : 2.40953

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47538
[1mStep[0m  [4/42], [94mLoss[0m : 2.35569
[1mStep[0m  [8/42], [94mLoss[0m : 2.40948
[1mStep[0m  [12/42], [94mLoss[0m : 2.40873
[1mStep[0m  [16/42], [94mLoss[0m : 2.06961
[1mStep[0m  [20/42], [94mLoss[0m : 2.17415
[1mStep[0m  [24/42], [94mLoss[0m : 2.09172
[1mStep[0m  [28/42], [94mLoss[0m : 2.45268
[1mStep[0m  [32/42], [94mLoss[0m : 2.51774
[1mStep[0m  [36/42], [94mLoss[0m : 2.36843
[1mStep[0m  [40/42], [94mLoss[0m : 2.45572

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41407
[1mStep[0m  [4/42], [94mLoss[0m : 2.44123
[1mStep[0m  [8/42], [94mLoss[0m : 2.22751
[1mStep[0m  [12/42], [94mLoss[0m : 2.19985
[1mStep[0m  [16/42], [94mLoss[0m : 2.15319
[1mStep[0m  [20/42], [94mLoss[0m : 2.54902
[1mStep[0m  [24/42], [94mLoss[0m : 2.32355
[1mStep[0m  [28/42], [94mLoss[0m : 2.49485
[1mStep[0m  [32/42], [94mLoss[0m : 2.48076
[1mStep[0m  [36/42], [94mLoss[0m : 2.31035
[1mStep[0m  [40/42], [94mLoss[0m : 2.35071

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.323, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.319
====================================

Phase 1 - Evaluation MAE:  2.3194199289594377
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.38153
[1mStep[0m  [4/42], [94mLoss[0m : 2.31642
[1mStep[0m  [8/42], [94mLoss[0m : 2.42220
[1mStep[0m  [12/42], [94mLoss[0m : 2.21916
[1mStep[0m  [16/42], [94mLoss[0m : 2.47833
[1mStep[0m  [20/42], [94mLoss[0m : 2.48840
[1mStep[0m  [24/42], [94mLoss[0m : 2.46360
[1mStep[0m  [28/42], [94mLoss[0m : 2.47256
[1mStep[0m  [32/42], [94mLoss[0m : 2.40127
[1mStep[0m  [36/42], [94mLoss[0m : 2.39686
[1mStep[0m  [40/42], [94mLoss[0m : 2.56971

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34676
[1mStep[0m  [4/42], [94mLoss[0m : 2.24044
[1mStep[0m  [8/42], [94mLoss[0m : 2.50217
[1mStep[0m  [12/42], [94mLoss[0m : 2.38701
[1mStep[0m  [16/42], [94mLoss[0m : 2.44176
[1mStep[0m  [20/42], [94mLoss[0m : 2.39422
[1mStep[0m  [24/42], [94mLoss[0m : 2.28764
[1mStep[0m  [28/42], [94mLoss[0m : 2.25312
[1mStep[0m  [32/42], [94mLoss[0m : 2.26769
[1mStep[0m  [36/42], [94mLoss[0m : 2.24869
[1mStep[0m  [40/42], [94mLoss[0m : 2.39893

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.339, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29358
[1mStep[0m  [4/42], [94mLoss[0m : 2.10168
[1mStep[0m  [8/42], [94mLoss[0m : 2.30395
[1mStep[0m  [12/42], [94mLoss[0m : 2.11098
[1mStep[0m  [16/42], [94mLoss[0m : 2.32506
[1mStep[0m  [20/42], [94mLoss[0m : 2.10846
[1mStep[0m  [24/42], [94mLoss[0m : 2.24941
[1mStep[0m  [28/42], [94mLoss[0m : 2.32899
[1mStep[0m  [32/42], [94mLoss[0m : 2.43327
[1mStep[0m  [36/42], [94mLoss[0m : 2.30344
[1mStep[0m  [40/42], [94mLoss[0m : 2.27780

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13263
[1mStep[0m  [4/42], [94mLoss[0m : 2.06750
[1mStep[0m  [8/42], [94mLoss[0m : 2.16293
[1mStep[0m  [12/42], [94mLoss[0m : 2.36458
[1mStep[0m  [16/42], [94mLoss[0m : 2.02168
[1mStep[0m  [20/42], [94mLoss[0m : 2.31579
[1mStep[0m  [24/42], [94mLoss[0m : 2.37881
[1mStep[0m  [28/42], [94mLoss[0m : 2.30783
[1mStep[0m  [32/42], [94mLoss[0m : 2.25575
[1mStep[0m  [36/42], [94mLoss[0m : 2.47467
[1mStep[0m  [40/42], [94mLoss[0m : 2.11549

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14106
[1mStep[0m  [4/42], [94mLoss[0m : 1.98082
[1mStep[0m  [8/42], [94mLoss[0m : 2.18688
[1mStep[0m  [12/42], [94mLoss[0m : 2.04576
[1mStep[0m  [16/42], [94mLoss[0m : 1.97417
[1mStep[0m  [20/42], [94mLoss[0m : 2.10604
[1mStep[0m  [24/42], [94mLoss[0m : 1.93824
[1mStep[0m  [28/42], [94mLoss[0m : 2.24175
[1mStep[0m  [32/42], [94mLoss[0m : 2.11692
[1mStep[0m  [36/42], [94mLoss[0m : 2.04594
[1mStep[0m  [40/42], [94mLoss[0m : 2.04009

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.95749
[1mStep[0m  [4/42], [94mLoss[0m : 1.99399
[1mStep[0m  [8/42], [94mLoss[0m : 2.04440
[1mStep[0m  [12/42], [94mLoss[0m : 1.92881
[1mStep[0m  [16/42], [94mLoss[0m : 2.10355
[1mStep[0m  [20/42], [94mLoss[0m : 2.07253
[1mStep[0m  [24/42], [94mLoss[0m : 2.16784
[1mStep[0m  [28/42], [94mLoss[0m : 2.24082
[1mStep[0m  [32/42], [94mLoss[0m : 2.05802
[1mStep[0m  [36/42], [94mLoss[0m : 2.11878
[1mStep[0m  [40/42], [94mLoss[0m : 2.10532

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.97092
[1mStep[0m  [4/42], [94mLoss[0m : 2.02000
[1mStep[0m  [8/42], [94mLoss[0m : 1.94056
[1mStep[0m  [12/42], [94mLoss[0m : 1.90790
[1mStep[0m  [16/42], [94mLoss[0m : 1.80932
[1mStep[0m  [20/42], [94mLoss[0m : 1.92224
[1mStep[0m  [24/42], [94mLoss[0m : 2.05193
[1mStep[0m  [28/42], [94mLoss[0m : 1.94247
[1mStep[0m  [32/42], [94mLoss[0m : 2.08193
[1mStep[0m  [36/42], [94mLoss[0m : 1.95655
[1mStep[0m  [40/42], [94mLoss[0m : 2.00698

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.04353
[1mStep[0m  [4/42], [94mLoss[0m : 1.97287
[1mStep[0m  [8/42], [94mLoss[0m : 1.98326
[1mStep[0m  [12/42], [94mLoss[0m : 1.79166
[1mStep[0m  [16/42], [94mLoss[0m : 2.04299
[1mStep[0m  [20/42], [94mLoss[0m : 1.74914
[1mStep[0m  [24/42], [94mLoss[0m : 1.86385
[1mStep[0m  [28/42], [94mLoss[0m : 1.89859
[1mStep[0m  [32/42], [94mLoss[0m : 2.02026
[1mStep[0m  [36/42], [94mLoss[0m : 1.84677
[1mStep[0m  [40/42], [94mLoss[0m : 2.01188

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.90060
[1mStep[0m  [4/42], [94mLoss[0m : 1.82553
[1mStep[0m  [8/42], [94mLoss[0m : 1.95421
[1mStep[0m  [12/42], [94mLoss[0m : 1.80718
[1mStep[0m  [16/42], [94mLoss[0m : 1.72310
[1mStep[0m  [20/42], [94mLoss[0m : 1.76172
[1mStep[0m  [24/42], [94mLoss[0m : 1.75579
[1mStep[0m  [28/42], [94mLoss[0m : 1.80024
[1mStep[0m  [32/42], [94mLoss[0m : 1.77156
[1mStep[0m  [36/42], [94mLoss[0m : 1.73872
[1mStep[0m  [40/42], [94mLoss[0m : 1.83420

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.66669
[1mStep[0m  [4/42], [94mLoss[0m : 1.85786
[1mStep[0m  [8/42], [94mLoss[0m : 1.63942
[1mStep[0m  [12/42], [94mLoss[0m : 1.66427
[1mStep[0m  [16/42], [94mLoss[0m : 1.61127
[1mStep[0m  [20/42], [94mLoss[0m : 1.79363
[1mStep[0m  [24/42], [94mLoss[0m : 2.01526
[1mStep[0m  [28/42], [94mLoss[0m : 1.87928
[1mStep[0m  [32/42], [94mLoss[0m : 1.94977
[1mStep[0m  [36/42], [94mLoss[0m : 1.74408
[1mStep[0m  [40/42], [94mLoss[0m : 1.73220

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.71835
[1mStep[0m  [4/42], [94mLoss[0m : 1.64477
[1mStep[0m  [8/42], [94mLoss[0m : 1.74414
[1mStep[0m  [12/42], [94mLoss[0m : 1.86311
[1mStep[0m  [16/42], [94mLoss[0m : 1.87238
[1mStep[0m  [20/42], [94mLoss[0m : 1.57329
[1mStep[0m  [24/42], [94mLoss[0m : 1.53152
[1mStep[0m  [28/42], [94mLoss[0m : 1.81702
[1mStep[0m  [32/42], [94mLoss[0m : 1.74122
[1mStep[0m  [36/42], [94mLoss[0m : 1.75693
[1mStep[0m  [40/42], [94mLoss[0m : 1.85660

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.83495
[1mStep[0m  [4/42], [94mLoss[0m : 1.69297
[1mStep[0m  [8/42], [94mLoss[0m : 1.77446
[1mStep[0m  [12/42], [94mLoss[0m : 1.48624
[1mStep[0m  [16/42], [94mLoss[0m : 1.72280
[1mStep[0m  [20/42], [94mLoss[0m : 1.51311
[1mStep[0m  [24/42], [94mLoss[0m : 1.67773
[1mStep[0m  [28/42], [94mLoss[0m : 1.75681
[1mStep[0m  [32/42], [94mLoss[0m : 1.63613
[1mStep[0m  [36/42], [94mLoss[0m : 1.76903
[1mStep[0m  [40/42], [94mLoss[0m : 1.76693

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.61914
[1mStep[0m  [4/42], [94mLoss[0m : 1.52007
[1mStep[0m  [8/42], [94mLoss[0m : 1.67709
[1mStep[0m  [12/42], [94mLoss[0m : 1.60174
[1mStep[0m  [16/42], [94mLoss[0m : 1.50775
[1mStep[0m  [20/42], [94mLoss[0m : 1.61043
[1mStep[0m  [24/42], [94mLoss[0m : 1.62381
[1mStep[0m  [28/42], [94mLoss[0m : 1.87776
[1mStep[0m  [32/42], [94mLoss[0m : 1.56380
[1mStep[0m  [36/42], [94mLoss[0m : 1.74138
[1mStep[0m  [40/42], [94mLoss[0m : 1.69988

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.54080
[1mStep[0m  [4/42], [94mLoss[0m : 1.59681
[1mStep[0m  [8/42], [94mLoss[0m : 1.51924
[1mStep[0m  [12/42], [94mLoss[0m : 1.51311
[1mStep[0m  [16/42], [94mLoss[0m : 1.59560
[1mStep[0m  [20/42], [94mLoss[0m : 1.44574
[1mStep[0m  [24/42], [94mLoss[0m : 1.41893
[1mStep[0m  [28/42], [94mLoss[0m : 1.75870
[1mStep[0m  [32/42], [94mLoss[0m : 1.61059
[1mStep[0m  [36/42], [94mLoss[0m : 1.70832
[1mStep[0m  [40/42], [94mLoss[0m : 1.59687

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.40486
[1mStep[0m  [4/42], [94mLoss[0m : 1.36544
[1mStep[0m  [8/42], [94mLoss[0m : 1.57979
[1mStep[0m  [12/42], [94mLoss[0m : 1.56002
[1mStep[0m  [16/42], [94mLoss[0m : 1.53075
[1mStep[0m  [20/42], [94mLoss[0m : 1.50519
[1mStep[0m  [24/42], [94mLoss[0m : 1.48913
[1mStep[0m  [28/42], [94mLoss[0m : 1.63697
[1mStep[0m  [32/42], [94mLoss[0m : 1.42142
[1mStep[0m  [36/42], [94mLoss[0m : 1.54898
[1mStep[0m  [40/42], [94mLoss[0m : 1.58014

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.72721
[1mStep[0m  [4/42], [94mLoss[0m : 1.36451
[1mStep[0m  [8/42], [94mLoss[0m : 1.43613
[1mStep[0m  [12/42], [94mLoss[0m : 1.42448
[1mStep[0m  [16/42], [94mLoss[0m : 1.53464
[1mStep[0m  [20/42], [94mLoss[0m : 1.50807
[1mStep[0m  [24/42], [94mLoss[0m : 1.64070
[1mStep[0m  [28/42], [94mLoss[0m : 1.72204
[1mStep[0m  [32/42], [94mLoss[0m : 1.46671
[1mStep[0m  [36/42], [94mLoss[0m : 1.58497
[1mStep[0m  [40/42], [94mLoss[0m : 1.64736

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.63945
[1mStep[0m  [4/42], [94mLoss[0m : 1.50877
[1mStep[0m  [8/42], [94mLoss[0m : 1.50113
[1mStep[0m  [12/42], [94mLoss[0m : 1.56046
[1mStep[0m  [16/42], [94mLoss[0m : 1.58620
[1mStep[0m  [20/42], [94mLoss[0m : 1.55617
[1mStep[0m  [24/42], [94mLoss[0m : 1.49973
[1mStep[0m  [28/42], [94mLoss[0m : 1.50909
[1mStep[0m  [32/42], [94mLoss[0m : 1.42387
[1mStep[0m  [36/42], [94mLoss[0m : 1.56309
[1mStep[0m  [40/42], [94mLoss[0m : 1.50020

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.41163
[1mStep[0m  [4/42], [94mLoss[0m : 1.43003
[1mStep[0m  [8/42], [94mLoss[0m : 1.54239
[1mStep[0m  [12/42], [94mLoss[0m : 1.49945
[1mStep[0m  [16/42], [94mLoss[0m : 1.52462
[1mStep[0m  [20/42], [94mLoss[0m : 1.45558
[1mStep[0m  [24/42], [94mLoss[0m : 1.48611
[1mStep[0m  [28/42], [94mLoss[0m : 1.44971
[1mStep[0m  [32/42], [94mLoss[0m : 1.43463
[1mStep[0m  [36/42], [94mLoss[0m : 1.41519
[1mStep[0m  [40/42], [94mLoss[0m : 1.45006

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.487, [92mTest[0m: 2.669, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.42099
[1mStep[0m  [4/42], [94mLoss[0m : 1.35128
[1mStep[0m  [8/42], [94mLoss[0m : 1.40534
[1mStep[0m  [12/42], [94mLoss[0m : 1.43036
[1mStep[0m  [16/42], [94mLoss[0m : 1.40091
[1mStep[0m  [20/42], [94mLoss[0m : 1.43329
[1mStep[0m  [24/42], [94mLoss[0m : 1.37712
[1mStep[0m  [28/42], [94mLoss[0m : 1.46084
[1mStep[0m  [32/42], [94mLoss[0m : 1.57779
[1mStep[0m  [36/42], [94mLoss[0m : 1.42391
[1mStep[0m  [40/42], [94mLoss[0m : 1.57100

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43164
[1mStep[0m  [4/42], [94mLoss[0m : 1.41360
[1mStep[0m  [8/42], [94mLoss[0m : 1.38655
[1mStep[0m  [12/42], [94mLoss[0m : 1.55413
[1mStep[0m  [16/42], [94mLoss[0m : 1.38078
[1mStep[0m  [20/42], [94mLoss[0m : 1.35537
[1mStep[0m  [24/42], [94mLoss[0m : 1.36853
[1mStep[0m  [28/42], [94mLoss[0m : 1.49236
[1mStep[0m  [32/42], [94mLoss[0m : 1.31679
[1mStep[0m  [36/42], [94mLoss[0m : 1.39423
[1mStep[0m  [40/42], [94mLoss[0m : 1.43881

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.421, [92mTest[0m: 2.564, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.43505
[1mStep[0m  [4/42], [94mLoss[0m : 1.31898
[1mStep[0m  [8/42], [94mLoss[0m : 1.30506
[1mStep[0m  [12/42], [94mLoss[0m : 1.39781
[1mStep[0m  [16/42], [94mLoss[0m : 1.35902
[1mStep[0m  [20/42], [94mLoss[0m : 1.28754
[1mStep[0m  [24/42], [94mLoss[0m : 1.39529
[1mStep[0m  [28/42], [94mLoss[0m : 1.21938
[1mStep[0m  [32/42], [94mLoss[0m : 1.29752
[1mStep[0m  [36/42], [94mLoss[0m : 1.37209
[1mStep[0m  [40/42], [94mLoss[0m : 1.38673

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.404, [92mTest[0m: 2.528, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 1.44291
[1mStep[0m  [4/42], [94mLoss[0m : 1.33148
[1mStep[0m  [8/42], [94mLoss[0m : 1.62451
[1mStep[0m  [12/42], [94mLoss[0m : 1.57585
[1mStep[0m  [16/42], [94mLoss[0m : 1.46881
[1mStep[0m  [20/42], [94mLoss[0m : 1.29076
[1mStep[0m  [24/42], [94mLoss[0m : 1.37756
[1mStep[0m  [28/42], [94mLoss[0m : 1.26628
[1mStep[0m  [32/42], [94mLoss[0m : 1.42988
[1mStep[0m  [36/42], [94mLoss[0m : 1.36916
[1mStep[0m  [40/42], [94mLoss[0m : 1.30184

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.381, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 21 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.531
====================================

Phase 2 - Evaluation MAE:  2.531068273953029
MAE score P1         2.31942
MAE score P2        2.531068
loss                1.381297
learning_rate        0.00505
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 31, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.44099
[1mStep[0m  [4/42], [94mLoss[0m : 10.20905
[1mStep[0m  [8/42], [94mLoss[0m : 8.83756
[1mStep[0m  [12/42], [94mLoss[0m : 7.60366
[1mStep[0m  [16/42], [94mLoss[0m : 7.17140
[1mStep[0m  [20/42], [94mLoss[0m : 6.59991
[1mStep[0m  [24/42], [94mLoss[0m : 5.62309
[1mStep[0m  [28/42], [94mLoss[0m : 4.55993
[1mStep[0m  [32/42], [94mLoss[0m : 4.36490
[1mStep[0m  [36/42], [94mLoss[0m : 4.10482
[1mStep[0m  [40/42], [94mLoss[0m : 3.60322

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.643, [92mTest[0m: 10.997, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.44958
[1mStep[0m  [4/42], [94mLoss[0m : 3.19471
[1mStep[0m  [8/42], [94mLoss[0m : 2.95372
[1mStep[0m  [12/42], [94mLoss[0m : 2.60312
[1mStep[0m  [16/42], [94mLoss[0m : 2.81342
[1mStep[0m  [20/42], [94mLoss[0m : 2.58909
[1mStep[0m  [24/42], [94mLoss[0m : 2.51929
[1mStep[0m  [28/42], [94mLoss[0m : 2.73580
[1mStep[0m  [32/42], [94mLoss[0m : 2.69545
[1mStep[0m  [36/42], [94mLoss[0m : 2.85041
[1mStep[0m  [40/42], [94mLoss[0m : 2.59676

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.792, [92mTest[0m: 4.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75145
[1mStep[0m  [4/42], [94mLoss[0m : 2.60737
[1mStep[0m  [8/42], [94mLoss[0m : 2.65278
[1mStep[0m  [12/42], [94mLoss[0m : 2.51392
[1mStep[0m  [16/42], [94mLoss[0m : 2.48670
[1mStep[0m  [20/42], [94mLoss[0m : 2.80954
[1mStep[0m  [24/42], [94mLoss[0m : 2.73863
[1mStep[0m  [28/42], [94mLoss[0m : 2.64891
[1mStep[0m  [32/42], [94mLoss[0m : 2.64945
[1mStep[0m  [36/42], [94mLoss[0m : 2.43886
[1mStep[0m  [40/42], [94mLoss[0m : 2.48135

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.814, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63636
[1mStep[0m  [4/42], [94mLoss[0m : 2.52884
[1mStep[0m  [8/42], [94mLoss[0m : 2.42127
[1mStep[0m  [12/42], [94mLoss[0m : 2.63990
[1mStep[0m  [16/42], [94mLoss[0m : 2.52687
[1mStep[0m  [20/42], [94mLoss[0m : 2.25778
[1mStep[0m  [24/42], [94mLoss[0m : 2.38527
