no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  8
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 10.86201
[1mStep[0m  [10/106], [94mLoss[0m : 10.88459
[1mStep[0m  [20/106], [94mLoss[0m : 10.65788
[1mStep[0m  [30/106], [94mLoss[0m : 10.41434
[1mStep[0m  [40/106], [94mLoss[0m : 10.26528
[1mStep[0m  [50/106], [94mLoss[0m : 11.10053
[1mStep[0m  [60/106], [94mLoss[0m : 10.58741
[1mStep[0m  [70/106], [94mLoss[0m : 10.33528
[1mStep[0m  [80/106], [94mLoss[0m : 10.05503
[1mStep[0m  [90/106], [94mLoss[0m : 10.53610
[1mStep[0m  [100/106], [94mLoss[0m : 10.42852

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.577, [92mTest[0m: 10.930, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.97425
[1mStep[0m  [10/106], [94mLoss[0m : 9.91316
[1mStep[0m  [20/106], [94mLoss[0m : 10.03955
[1mStep[0m  [30/106], [94mLoss[0m : 10.26907
[1mStep[0m  [40/106], [94mLoss[0m : 9.70270
[1mStep[0m  [50/106], [94mLoss[0m : 10.13982
[1mStep[0m  [60/106], [94mLoss[0m : 10.21042
[1mStep[0m  [70/106], [94mLoss[0m : 9.98845
[1mStep[0m  [80/106], [94mLoss[0m : 9.72364
[1mStep[0m  [90/106], [94mLoss[0m : 9.38153
[1mStep[0m  [100/106], [94mLoss[0m : 9.37727

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.830, [92mTest[0m: 10.001, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.26480
[1mStep[0m  [10/106], [94mLoss[0m : 9.61657
[1mStep[0m  [20/106], [94mLoss[0m : 9.13070
[1mStep[0m  [30/106], [94mLoss[0m : 9.21420
[1mStep[0m  [40/106], [94mLoss[0m : 9.09405
[1mStep[0m  [50/106], [94mLoss[0m : 8.95034
[1mStep[0m  [60/106], [94mLoss[0m : 8.51630
[1mStep[0m  [70/106], [94mLoss[0m : 8.40890
[1mStep[0m  [80/106], [94mLoss[0m : 8.87679
[1mStep[0m  [90/106], [94mLoss[0m : 7.99223
[1mStep[0m  [100/106], [94mLoss[0m : 8.48914

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.915, [92mTest[0m: 8.983, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.28697
[1mStep[0m  [10/106], [94mLoss[0m : 8.24956
[1mStep[0m  [20/106], [94mLoss[0m : 8.32083
[1mStep[0m  [30/106], [94mLoss[0m : 7.94962
[1mStep[0m  [40/106], [94mLoss[0m : 7.69182
[1mStep[0m  [50/106], [94mLoss[0m : 7.44543
[1mStep[0m  [60/106], [94mLoss[0m : 8.00882
[1mStep[0m  [70/106], [94mLoss[0m : 7.73346
[1mStep[0m  [80/106], [94mLoss[0m : 7.44034
[1mStep[0m  [90/106], [94mLoss[0m : 7.08846
[1mStep[0m  [100/106], [94mLoss[0m : 6.95743

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.697, [92mTest[0m: 7.532, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.02282
[1mStep[0m  [10/106], [94mLoss[0m : 6.98235
[1mStep[0m  [20/106], [94mLoss[0m : 6.75193
[1mStep[0m  [30/106], [94mLoss[0m : 6.71946
[1mStep[0m  [40/106], [94mLoss[0m : 6.11292
[1mStep[0m  [50/106], [94mLoss[0m : 6.06939
[1mStep[0m  [60/106], [94mLoss[0m : 6.32430
[1mStep[0m  [70/106], [94mLoss[0m : 6.35606
[1mStep[0m  [80/106], [94mLoss[0m : 6.47705
[1mStep[0m  [90/106], [94mLoss[0m : 5.46033
[1mStep[0m  [100/106], [94mLoss[0m : 6.01429

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.363, [92mTest[0m: 6.062, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.60548
[1mStep[0m  [10/106], [94mLoss[0m : 5.39285
[1mStep[0m  [20/106], [94mLoss[0m : 5.21977
[1mStep[0m  [30/106], [94mLoss[0m : 5.36483
[1mStep[0m  [40/106], [94mLoss[0m : 5.02056
[1mStep[0m  [50/106], [94mLoss[0m : 5.48503
[1mStep[0m  [60/106], [94mLoss[0m : 5.05696
[1mStep[0m  [70/106], [94mLoss[0m : 3.96854
[1mStep[0m  [80/106], [94mLoss[0m : 4.52608
[1mStep[0m  [90/106], [94mLoss[0m : 4.78485
[1mStep[0m  [100/106], [94mLoss[0m : 4.35615

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.980, [92mTest[0m: 4.776, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.40459
[1mStep[0m  [10/106], [94mLoss[0m : 3.84971
[1mStep[0m  [20/106], [94mLoss[0m : 4.23646
[1mStep[0m  [30/106], [94mLoss[0m : 4.16167
[1mStep[0m  [40/106], [94mLoss[0m : 3.60381
[1mStep[0m  [50/106], [94mLoss[0m : 3.36005
[1mStep[0m  [60/106], [94mLoss[0m : 3.11258
[1mStep[0m  [70/106], [94mLoss[0m : 3.20276
[1mStep[0m  [80/106], [94mLoss[0m : 3.47092
[1mStep[0m  [90/106], [94mLoss[0m : 3.29851
[1mStep[0m  [100/106], [94mLoss[0m : 3.25512

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.629, [92mTest[0m: 3.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.27524
[1mStep[0m  [10/106], [94mLoss[0m : 2.62849
[1mStep[0m  [20/106], [94mLoss[0m : 3.41388
[1mStep[0m  [30/106], [94mLoss[0m : 3.17767
[1mStep[0m  [40/106], [94mLoss[0m : 3.17904
[1mStep[0m  [50/106], [94mLoss[0m : 2.86814
[1mStep[0m  [60/106], [94mLoss[0m : 3.08139
[1mStep[0m  [70/106], [94mLoss[0m : 2.91650
[1mStep[0m  [80/106], [94mLoss[0m : 2.72824
[1mStep[0m  [90/106], [94mLoss[0m : 3.01774
[1mStep[0m  [100/106], [94mLoss[0m : 2.46856

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.994, [92mTest[0m: 2.600, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.99749
[1mStep[0m  [10/106], [94mLoss[0m : 3.01146
[1mStep[0m  [20/106], [94mLoss[0m : 2.57167
[1mStep[0m  [30/106], [94mLoss[0m : 3.00408
[1mStep[0m  [40/106], [94mLoss[0m : 2.92995
[1mStep[0m  [50/106], [94mLoss[0m : 2.58591
[1mStep[0m  [60/106], [94mLoss[0m : 3.09589
[1mStep[0m  [70/106], [94mLoss[0m : 2.92368
[1mStep[0m  [80/106], [94mLoss[0m : 2.65285
[1mStep[0m  [90/106], [94mLoss[0m : 3.08026
[1mStep[0m  [100/106], [94mLoss[0m : 3.20643

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.866, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.87583
[1mStep[0m  [10/106], [94mLoss[0m : 2.80157
[1mStep[0m  [20/106], [94mLoss[0m : 2.65371
[1mStep[0m  [30/106], [94mLoss[0m : 2.86289
[1mStep[0m  [40/106], [94mLoss[0m : 2.67800
[1mStep[0m  [50/106], [94mLoss[0m : 2.66656
[1mStep[0m  [60/106], [94mLoss[0m : 2.72254
[1mStep[0m  [70/106], [94mLoss[0m : 2.88804
[1mStep[0m  [80/106], [94mLoss[0m : 2.66839
[1mStep[0m  [90/106], [94mLoss[0m : 2.55829
[1mStep[0m  [100/106], [94mLoss[0m : 2.74303

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.798, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48910
[1mStep[0m  [10/106], [94mLoss[0m : 2.65604
[1mStep[0m  [20/106], [94mLoss[0m : 2.77088
[1mStep[0m  [30/106], [94mLoss[0m : 2.79245
[1mStep[0m  [40/106], [94mLoss[0m : 2.66806
[1mStep[0m  [50/106], [94mLoss[0m : 2.48722
[1mStep[0m  [60/106], [94mLoss[0m : 2.59376
[1mStep[0m  [70/106], [94mLoss[0m : 2.90027
[1mStep[0m  [80/106], [94mLoss[0m : 2.67531
[1mStep[0m  [90/106], [94mLoss[0m : 2.63699
[1mStep[0m  [100/106], [94mLoss[0m : 2.56880

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.781, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73101
[1mStep[0m  [10/106], [94mLoss[0m : 3.18746
[1mStep[0m  [20/106], [94mLoss[0m : 2.71878
[1mStep[0m  [30/106], [94mLoss[0m : 2.45834
[1mStep[0m  [40/106], [94mLoss[0m : 2.66942
[1mStep[0m  [50/106], [94mLoss[0m : 2.73626
[1mStep[0m  [60/106], [94mLoss[0m : 2.83144
[1mStep[0m  [70/106], [94mLoss[0m : 2.90863
[1mStep[0m  [80/106], [94mLoss[0m : 2.72094
[1mStep[0m  [90/106], [94mLoss[0m : 2.64903
[1mStep[0m  [100/106], [94mLoss[0m : 2.77198

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66353
[1mStep[0m  [10/106], [94mLoss[0m : 2.50048
[1mStep[0m  [20/106], [94mLoss[0m : 2.93651
[1mStep[0m  [30/106], [94mLoss[0m : 2.61547
[1mStep[0m  [40/106], [94mLoss[0m : 2.49384
[1mStep[0m  [50/106], [94mLoss[0m : 2.55340
[1mStep[0m  [60/106], [94mLoss[0m : 2.42702
[1mStep[0m  [70/106], [94mLoss[0m : 2.73031
[1mStep[0m  [80/106], [94mLoss[0m : 3.03408
[1mStep[0m  [90/106], [94mLoss[0m : 2.77720
[1mStep[0m  [100/106], [94mLoss[0m : 2.67867

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.38760
[1mStep[0m  [10/106], [94mLoss[0m : 2.47251
[1mStep[0m  [20/106], [94mLoss[0m : 2.82107
[1mStep[0m  [30/106], [94mLoss[0m : 2.59746
[1mStep[0m  [40/106], [94mLoss[0m : 2.73544
[1mStep[0m  [50/106], [94mLoss[0m : 2.80669
[1mStep[0m  [60/106], [94mLoss[0m : 2.63754
[1mStep[0m  [70/106], [94mLoss[0m : 2.26079
[1mStep[0m  [80/106], [94mLoss[0m : 2.51225
[1mStep[0m  [90/106], [94mLoss[0m : 2.72265
[1mStep[0m  [100/106], [94mLoss[0m : 2.75093

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57890
[1mStep[0m  [10/106], [94mLoss[0m : 2.53706
[1mStep[0m  [20/106], [94mLoss[0m : 2.98876
[1mStep[0m  [30/106], [94mLoss[0m : 2.75676
[1mStep[0m  [40/106], [94mLoss[0m : 2.66175
[1mStep[0m  [50/106], [94mLoss[0m : 2.54966
[1mStep[0m  [60/106], [94mLoss[0m : 2.61564
[1mStep[0m  [70/106], [94mLoss[0m : 3.05182
[1mStep[0m  [80/106], [94mLoss[0m : 2.73181
[1mStep[0m  [90/106], [94mLoss[0m : 2.71224
[1mStep[0m  [100/106], [94mLoss[0m : 2.55326

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84350
[1mStep[0m  [10/106], [94mLoss[0m : 2.67433
[1mStep[0m  [20/106], [94mLoss[0m : 2.54236
[1mStep[0m  [30/106], [94mLoss[0m : 2.72319
[1mStep[0m  [40/106], [94mLoss[0m : 2.89766
[1mStep[0m  [50/106], [94mLoss[0m : 2.31073
[1mStep[0m  [60/106], [94mLoss[0m : 2.61658
[1mStep[0m  [70/106], [94mLoss[0m : 2.52378
[1mStep[0m  [80/106], [94mLoss[0m : 2.54879
[1mStep[0m  [90/106], [94mLoss[0m : 2.69344
[1mStep[0m  [100/106], [94mLoss[0m : 2.30525

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56126
[1mStep[0m  [10/106], [94mLoss[0m : 2.81003
[1mStep[0m  [20/106], [94mLoss[0m : 2.33525
[1mStep[0m  [30/106], [94mLoss[0m : 2.49129
[1mStep[0m  [40/106], [94mLoss[0m : 3.02197
[1mStep[0m  [50/106], [94mLoss[0m : 2.48817
[1mStep[0m  [60/106], [94mLoss[0m : 2.78664
[1mStep[0m  [70/106], [94mLoss[0m : 2.64658
[1mStep[0m  [80/106], [94mLoss[0m : 2.57786
[1mStep[0m  [90/106], [94mLoss[0m : 3.05000
[1mStep[0m  [100/106], [94mLoss[0m : 2.96389

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41619
[1mStep[0m  [10/106], [94mLoss[0m : 2.56326
[1mStep[0m  [20/106], [94mLoss[0m : 3.01073
[1mStep[0m  [30/106], [94mLoss[0m : 2.75212
[1mStep[0m  [40/106], [94mLoss[0m : 2.51486
[1mStep[0m  [50/106], [94mLoss[0m : 2.92810
[1mStep[0m  [60/106], [94mLoss[0m : 2.78095
[1mStep[0m  [70/106], [94mLoss[0m : 2.70753
[1mStep[0m  [80/106], [94mLoss[0m : 2.41920
[1mStep[0m  [90/106], [94mLoss[0m : 2.81434
[1mStep[0m  [100/106], [94mLoss[0m : 2.95229

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74679
[1mStep[0m  [10/106], [94mLoss[0m : 2.65875
[1mStep[0m  [20/106], [94mLoss[0m : 2.89162
[1mStep[0m  [30/106], [94mLoss[0m : 2.93137
[1mStep[0m  [40/106], [94mLoss[0m : 2.36161
[1mStep[0m  [50/106], [94mLoss[0m : 2.83946
[1mStep[0m  [60/106], [94mLoss[0m : 2.70982
[1mStep[0m  [70/106], [94mLoss[0m : 2.56687
[1mStep[0m  [80/106], [94mLoss[0m : 2.80524
[1mStep[0m  [90/106], [94mLoss[0m : 2.63354
[1mStep[0m  [100/106], [94mLoss[0m : 2.48636

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38125
[1mStep[0m  [10/106], [94mLoss[0m : 2.45498
[1mStep[0m  [20/106], [94mLoss[0m : 2.77721
[1mStep[0m  [30/106], [94mLoss[0m : 2.64190
[1mStep[0m  [40/106], [94mLoss[0m : 2.64828
[1mStep[0m  [50/106], [94mLoss[0m : 2.62359
[1mStep[0m  [60/106], [94mLoss[0m : 2.81072
[1mStep[0m  [70/106], [94mLoss[0m : 2.47379
[1mStep[0m  [80/106], [94mLoss[0m : 2.48527
[1mStep[0m  [90/106], [94mLoss[0m : 2.71003
[1mStep[0m  [100/106], [94mLoss[0m : 2.80948

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45818
[1mStep[0m  [10/106], [94mLoss[0m : 2.34572
[1mStep[0m  [20/106], [94mLoss[0m : 2.57439
[1mStep[0m  [30/106], [94mLoss[0m : 2.48461
[1mStep[0m  [40/106], [94mLoss[0m : 2.41254
[1mStep[0m  [50/106], [94mLoss[0m : 2.36505
[1mStep[0m  [60/106], [94mLoss[0m : 2.29659
[1mStep[0m  [70/106], [94mLoss[0m : 2.86332
[1mStep[0m  [80/106], [94mLoss[0m : 2.44466
[1mStep[0m  [90/106], [94mLoss[0m : 2.68854
[1mStep[0m  [100/106], [94mLoss[0m : 2.56794

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.647, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45429
[1mStep[0m  [10/106], [94mLoss[0m : 2.77873
[1mStep[0m  [20/106], [94mLoss[0m : 2.74267
[1mStep[0m  [30/106], [94mLoss[0m : 2.65272
[1mStep[0m  [40/106], [94mLoss[0m : 2.59598
[1mStep[0m  [50/106], [94mLoss[0m : 2.91994
[1mStep[0m  [60/106], [94mLoss[0m : 2.28594
[1mStep[0m  [70/106], [94mLoss[0m : 2.96307
[1mStep[0m  [80/106], [94mLoss[0m : 3.04915
[1mStep[0m  [90/106], [94mLoss[0m : 2.64527
[1mStep[0m  [100/106], [94mLoss[0m : 2.72854

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65478
[1mStep[0m  [10/106], [94mLoss[0m : 2.49114
[1mStep[0m  [20/106], [94mLoss[0m : 2.59514
[1mStep[0m  [30/106], [94mLoss[0m : 2.55306
[1mStep[0m  [40/106], [94mLoss[0m : 2.76250
[1mStep[0m  [50/106], [94mLoss[0m : 2.46247
[1mStep[0m  [60/106], [94mLoss[0m : 2.45152
[1mStep[0m  [70/106], [94mLoss[0m : 2.87197
[1mStep[0m  [80/106], [94mLoss[0m : 2.84811
[1mStep[0m  [90/106], [94mLoss[0m : 2.91629
[1mStep[0m  [100/106], [94mLoss[0m : 2.42757

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35863
[1mStep[0m  [10/106], [94mLoss[0m : 2.50030
[1mStep[0m  [20/106], [94mLoss[0m : 2.52630
[1mStep[0m  [30/106], [94mLoss[0m : 2.24666
[1mStep[0m  [40/106], [94mLoss[0m : 2.74492
[1mStep[0m  [50/106], [94mLoss[0m : 2.59975
[1mStep[0m  [60/106], [94mLoss[0m : 2.68806
[1mStep[0m  [70/106], [94mLoss[0m : 2.78338
[1mStep[0m  [80/106], [94mLoss[0m : 2.24425
[1mStep[0m  [90/106], [94mLoss[0m : 2.44968
[1mStep[0m  [100/106], [94mLoss[0m : 2.63122

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69458
[1mStep[0m  [10/106], [94mLoss[0m : 2.55520
[1mStep[0m  [20/106], [94mLoss[0m : 2.34895
[1mStep[0m  [30/106], [94mLoss[0m : 2.83899
[1mStep[0m  [40/106], [94mLoss[0m : 2.79121
[1mStep[0m  [50/106], [94mLoss[0m : 2.72213
[1mStep[0m  [60/106], [94mLoss[0m : 3.07636
[1mStep[0m  [70/106], [94mLoss[0m : 2.57098
[1mStep[0m  [80/106], [94mLoss[0m : 2.74485
[1mStep[0m  [90/106], [94mLoss[0m : 2.52983
[1mStep[0m  [100/106], [94mLoss[0m : 2.62554

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46451
[1mStep[0m  [10/106], [94mLoss[0m : 2.44474
[1mStep[0m  [20/106], [94mLoss[0m : 2.75992
[1mStep[0m  [30/106], [94mLoss[0m : 2.28980
[1mStep[0m  [40/106], [94mLoss[0m : 2.61511
[1mStep[0m  [50/106], [94mLoss[0m : 2.46836
[1mStep[0m  [60/106], [94mLoss[0m : 2.33691
[1mStep[0m  [70/106], [94mLoss[0m : 2.62106
[1mStep[0m  [80/106], [94mLoss[0m : 2.37739
[1mStep[0m  [90/106], [94mLoss[0m : 2.90016
[1mStep[0m  [100/106], [94mLoss[0m : 2.97118

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47071
[1mStep[0m  [10/106], [94mLoss[0m : 2.37819
[1mStep[0m  [20/106], [94mLoss[0m : 2.26347
[1mStep[0m  [30/106], [94mLoss[0m : 2.65840
[1mStep[0m  [40/106], [94mLoss[0m : 2.76485
[1mStep[0m  [50/106], [94mLoss[0m : 2.88968
[1mStep[0m  [60/106], [94mLoss[0m : 2.76424
[1mStep[0m  [70/106], [94mLoss[0m : 2.70468
[1mStep[0m  [80/106], [94mLoss[0m : 2.13528
[1mStep[0m  [90/106], [94mLoss[0m : 2.55387
[1mStep[0m  [100/106], [94mLoss[0m : 2.52417

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43471
[1mStep[0m  [10/106], [94mLoss[0m : 2.69675
[1mStep[0m  [20/106], [94mLoss[0m : 2.67663
[1mStep[0m  [30/106], [94mLoss[0m : 2.63336
[1mStep[0m  [40/106], [94mLoss[0m : 2.52213
[1mStep[0m  [50/106], [94mLoss[0m : 2.39022
[1mStep[0m  [60/106], [94mLoss[0m : 2.73826
[1mStep[0m  [70/106], [94mLoss[0m : 2.70163
[1mStep[0m  [80/106], [94mLoss[0m : 2.31895
[1mStep[0m  [90/106], [94mLoss[0m : 2.40584
[1mStep[0m  [100/106], [94mLoss[0m : 2.56010

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60295
[1mStep[0m  [10/106], [94mLoss[0m : 2.48440
[1mStep[0m  [20/106], [94mLoss[0m : 2.59493
[1mStep[0m  [30/106], [94mLoss[0m : 2.76949
[1mStep[0m  [40/106], [94mLoss[0m : 2.44816
[1mStep[0m  [50/106], [94mLoss[0m : 2.74145
[1mStep[0m  [60/106], [94mLoss[0m : 2.31347
[1mStep[0m  [70/106], [94mLoss[0m : 2.47506
[1mStep[0m  [80/106], [94mLoss[0m : 2.57201
[1mStep[0m  [90/106], [94mLoss[0m : 2.71215
[1mStep[0m  [100/106], [94mLoss[0m : 2.40302

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72139
[1mStep[0m  [10/106], [94mLoss[0m : 2.41519
[1mStep[0m  [20/106], [94mLoss[0m : 2.70827
[1mStep[0m  [30/106], [94mLoss[0m : 2.25117
[1mStep[0m  [40/106], [94mLoss[0m : 2.62290
[1mStep[0m  [50/106], [94mLoss[0m : 2.26752
[1mStep[0m  [60/106], [94mLoss[0m : 2.73370
[1mStep[0m  [70/106], [94mLoss[0m : 2.74372
[1mStep[0m  [80/106], [94mLoss[0m : 2.37896
[1mStep[0m  [90/106], [94mLoss[0m : 2.77134
[1mStep[0m  [100/106], [94mLoss[0m : 2.52551

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.416
====================================

Phase 1 - Evaluation MAE:  2.41619902961659
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.99607
[1mStep[0m  [10/106], [94mLoss[0m : 2.70351
[1mStep[0m  [20/106], [94mLoss[0m : 2.43070
[1mStep[0m  [30/106], [94mLoss[0m : 2.77320
[1mStep[0m  [40/106], [94mLoss[0m : 2.78123
[1mStep[0m  [50/106], [94mLoss[0m : 2.51513
[1mStep[0m  [60/106], [94mLoss[0m : 2.68902
[1mStep[0m  [70/106], [94mLoss[0m : 2.61727
[1mStep[0m  [80/106], [94mLoss[0m : 2.35526
[1mStep[0m  [90/106], [94mLoss[0m : 2.43584
[1mStep[0m  [100/106], [94mLoss[0m : 2.59025

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83117
[1mStep[0m  [10/106], [94mLoss[0m : 2.70035
[1mStep[0m  [20/106], [94mLoss[0m : 2.38246
[1mStep[0m  [30/106], [94mLoss[0m : 2.76920
[1mStep[0m  [40/106], [94mLoss[0m : 2.47019
[1mStep[0m  [50/106], [94mLoss[0m : 2.55205
[1mStep[0m  [60/106], [94mLoss[0m : 2.66356
[1mStep[0m  [70/106], [94mLoss[0m : 2.64572
[1mStep[0m  [80/106], [94mLoss[0m : 2.66629
[1mStep[0m  [90/106], [94mLoss[0m : 2.55216
[1mStep[0m  [100/106], [94mLoss[0m : 2.52651

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.653, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38322
[1mStep[0m  [10/106], [94mLoss[0m : 2.61121
[1mStep[0m  [20/106], [94mLoss[0m : 2.67128
[1mStep[0m  [30/106], [94mLoss[0m : 2.71448
[1mStep[0m  [40/106], [94mLoss[0m : 2.49485
[1mStep[0m  [50/106], [94mLoss[0m : 2.36115
[1mStep[0m  [60/106], [94mLoss[0m : 2.49287
[1mStep[0m  [70/106], [94mLoss[0m : 2.43248
[1mStep[0m  [80/106], [94mLoss[0m : 2.77474
[1mStep[0m  [90/106], [94mLoss[0m : 2.93258
[1mStep[0m  [100/106], [94mLoss[0m : 2.53767

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.546, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41501
[1mStep[0m  [10/106], [94mLoss[0m : 2.30827
[1mStep[0m  [20/106], [94mLoss[0m : 2.23630
[1mStep[0m  [30/106], [94mLoss[0m : 2.44934
[1mStep[0m  [40/106], [94mLoss[0m : 2.51640
[1mStep[0m  [50/106], [94mLoss[0m : 2.73124
[1mStep[0m  [60/106], [94mLoss[0m : 2.64492
[1mStep[0m  [70/106], [94mLoss[0m : 2.73499
[1mStep[0m  [80/106], [94mLoss[0m : 2.47704
[1mStep[0m  [90/106], [94mLoss[0m : 2.65681
[1mStep[0m  [100/106], [94mLoss[0m : 2.65027

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.615, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41081
[1mStep[0m  [10/106], [94mLoss[0m : 2.65805
[1mStep[0m  [20/106], [94mLoss[0m : 2.40103
[1mStep[0m  [30/106], [94mLoss[0m : 2.58888
[1mStep[0m  [40/106], [94mLoss[0m : 2.57916
[1mStep[0m  [50/106], [94mLoss[0m : 2.61584
[1mStep[0m  [60/106], [94mLoss[0m : 2.52075
[1mStep[0m  [70/106], [94mLoss[0m : 2.42346
[1mStep[0m  [80/106], [94mLoss[0m : 2.45823
[1mStep[0m  [90/106], [94mLoss[0m : 2.25149
[1mStep[0m  [100/106], [94mLoss[0m : 2.59042

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.513, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36798
[1mStep[0m  [10/106], [94mLoss[0m : 2.29830
[1mStep[0m  [20/106], [94mLoss[0m : 2.25199
[1mStep[0m  [30/106], [94mLoss[0m : 2.58099
[1mStep[0m  [40/106], [94mLoss[0m : 2.48431
[1mStep[0m  [50/106], [94mLoss[0m : 2.47171
[1mStep[0m  [60/106], [94mLoss[0m : 2.60279
[1mStep[0m  [70/106], [94mLoss[0m : 2.38991
[1mStep[0m  [80/106], [94mLoss[0m : 2.58574
[1mStep[0m  [90/106], [94mLoss[0m : 2.19241
[1mStep[0m  [100/106], [94mLoss[0m : 2.61874

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59437
[1mStep[0m  [10/106], [94mLoss[0m : 2.11456
[1mStep[0m  [20/106], [94mLoss[0m : 2.55222
[1mStep[0m  [30/106], [94mLoss[0m : 2.05404
[1mStep[0m  [40/106], [94mLoss[0m : 2.35217
[1mStep[0m  [50/106], [94mLoss[0m : 2.02565
[1mStep[0m  [60/106], [94mLoss[0m : 2.44290
[1mStep[0m  [70/106], [94mLoss[0m : 2.23125
[1mStep[0m  [80/106], [94mLoss[0m : 2.53412
[1mStep[0m  [90/106], [94mLoss[0m : 2.86541
[1mStep[0m  [100/106], [94mLoss[0m : 2.54569

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42572
[1mStep[0m  [10/106], [94mLoss[0m : 2.50036
[1mStep[0m  [20/106], [94mLoss[0m : 2.24062
[1mStep[0m  [30/106], [94mLoss[0m : 2.46917
[1mStep[0m  [40/106], [94mLoss[0m : 2.16094
[1mStep[0m  [50/106], [94mLoss[0m : 2.62203
[1mStep[0m  [60/106], [94mLoss[0m : 2.39304
[1mStep[0m  [70/106], [94mLoss[0m : 2.52665
[1mStep[0m  [80/106], [94mLoss[0m : 2.35994
[1mStep[0m  [90/106], [94mLoss[0m : 2.27892
[1mStep[0m  [100/106], [94mLoss[0m : 2.39404

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35963
[1mStep[0m  [10/106], [94mLoss[0m : 2.51848
[1mStep[0m  [20/106], [94mLoss[0m : 2.32542
[1mStep[0m  [30/106], [94mLoss[0m : 2.51225
[1mStep[0m  [40/106], [94mLoss[0m : 2.43675
[1mStep[0m  [50/106], [94mLoss[0m : 2.29024
[1mStep[0m  [60/106], [94mLoss[0m : 2.35062
[1mStep[0m  [70/106], [94mLoss[0m : 2.46054
[1mStep[0m  [80/106], [94mLoss[0m : 2.34802
[1mStep[0m  [90/106], [94mLoss[0m : 2.10667
[1mStep[0m  [100/106], [94mLoss[0m : 2.42042

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21512
[1mStep[0m  [10/106], [94mLoss[0m : 2.56893
[1mStep[0m  [20/106], [94mLoss[0m : 2.68029
[1mStep[0m  [30/106], [94mLoss[0m : 2.14463
[1mStep[0m  [40/106], [94mLoss[0m : 2.09652
[1mStep[0m  [50/106], [94mLoss[0m : 2.32797
[1mStep[0m  [60/106], [94mLoss[0m : 2.28807
[1mStep[0m  [70/106], [94mLoss[0m : 2.22506
[1mStep[0m  [80/106], [94mLoss[0m : 2.43191
[1mStep[0m  [90/106], [94mLoss[0m : 2.22592
[1mStep[0m  [100/106], [94mLoss[0m : 2.19778

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20090
[1mStep[0m  [10/106], [94mLoss[0m : 1.99065
[1mStep[0m  [20/106], [94mLoss[0m : 2.37094
[1mStep[0m  [30/106], [94mLoss[0m : 2.30903
[1mStep[0m  [40/106], [94mLoss[0m : 2.49230
[1mStep[0m  [50/106], [94mLoss[0m : 2.41560
[1mStep[0m  [60/106], [94mLoss[0m : 2.59790
[1mStep[0m  [70/106], [94mLoss[0m : 2.41469
[1mStep[0m  [80/106], [94mLoss[0m : 2.24162
[1mStep[0m  [90/106], [94mLoss[0m : 2.15478
[1mStep[0m  [100/106], [94mLoss[0m : 2.40362

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30462
[1mStep[0m  [10/106], [94mLoss[0m : 1.86099
[1mStep[0m  [20/106], [94mLoss[0m : 2.36625
[1mStep[0m  [30/106], [94mLoss[0m : 2.26097
[1mStep[0m  [40/106], [94mLoss[0m : 1.94644
[1mStep[0m  [50/106], [94mLoss[0m : 2.25327
[1mStep[0m  [60/106], [94mLoss[0m : 2.34849
[1mStep[0m  [70/106], [94mLoss[0m : 2.31230
[1mStep[0m  [80/106], [94mLoss[0m : 2.54324
[1mStep[0m  [90/106], [94mLoss[0m : 1.90430
[1mStep[0m  [100/106], [94mLoss[0m : 2.31186

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27510
[1mStep[0m  [10/106], [94mLoss[0m : 2.24082
[1mStep[0m  [20/106], [94mLoss[0m : 1.93261
[1mStep[0m  [30/106], [94mLoss[0m : 2.09529
[1mStep[0m  [40/106], [94mLoss[0m : 2.23392
[1mStep[0m  [50/106], [94mLoss[0m : 2.06163
[1mStep[0m  [60/106], [94mLoss[0m : 1.92985
[1mStep[0m  [70/106], [94mLoss[0m : 2.21909
[1mStep[0m  [80/106], [94mLoss[0m : 2.49921
[1mStep[0m  [90/106], [94mLoss[0m : 2.35492
[1mStep[0m  [100/106], [94mLoss[0m : 2.06852

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04566
[1mStep[0m  [10/106], [94mLoss[0m : 2.01126
[1mStep[0m  [20/106], [94mLoss[0m : 2.29892
[1mStep[0m  [30/106], [94mLoss[0m : 2.16942
[1mStep[0m  [40/106], [94mLoss[0m : 2.28341
[1mStep[0m  [50/106], [94mLoss[0m : 2.31309
[1mStep[0m  [60/106], [94mLoss[0m : 2.22689
[1mStep[0m  [70/106], [94mLoss[0m : 2.35741
[1mStep[0m  [80/106], [94mLoss[0m : 2.10358
[1mStep[0m  [90/106], [94mLoss[0m : 1.95294
[1mStep[0m  [100/106], [94mLoss[0m : 1.96553

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67295
[1mStep[0m  [10/106], [94mLoss[0m : 1.93564
[1mStep[0m  [20/106], [94mLoss[0m : 2.26315
[1mStep[0m  [30/106], [94mLoss[0m : 2.14769
[1mStep[0m  [40/106], [94mLoss[0m : 2.08778
[1mStep[0m  [50/106], [94mLoss[0m : 2.13537
[1mStep[0m  [60/106], [94mLoss[0m : 1.99481
[1mStep[0m  [70/106], [94mLoss[0m : 2.39023
[1mStep[0m  [80/106], [94mLoss[0m : 1.95380
[1mStep[0m  [90/106], [94mLoss[0m : 2.22986
[1mStep[0m  [100/106], [94mLoss[0m : 2.11553

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94563
[1mStep[0m  [10/106], [94mLoss[0m : 2.11395
[1mStep[0m  [20/106], [94mLoss[0m : 2.03249
[1mStep[0m  [30/106], [94mLoss[0m : 2.28336
[1mStep[0m  [40/106], [94mLoss[0m : 2.07322
[1mStep[0m  [50/106], [94mLoss[0m : 2.09941
[1mStep[0m  [60/106], [94mLoss[0m : 1.89775
[1mStep[0m  [70/106], [94mLoss[0m : 2.04603
[1mStep[0m  [80/106], [94mLoss[0m : 1.86941
[1mStep[0m  [90/106], [94mLoss[0m : 2.31785
[1mStep[0m  [100/106], [94mLoss[0m : 1.99682

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18322
[1mStep[0m  [10/106], [94mLoss[0m : 2.10766
[1mStep[0m  [20/106], [94mLoss[0m : 1.94021
[1mStep[0m  [30/106], [94mLoss[0m : 1.99412
[1mStep[0m  [40/106], [94mLoss[0m : 2.32401
[1mStep[0m  [50/106], [94mLoss[0m : 2.27658
[1mStep[0m  [60/106], [94mLoss[0m : 2.28500
[1mStep[0m  [70/106], [94mLoss[0m : 1.83147
[1mStep[0m  [80/106], [94mLoss[0m : 2.21908
[1mStep[0m  [90/106], [94mLoss[0m : 2.13322
[1mStep[0m  [100/106], [94mLoss[0m : 2.18954

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.112, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38539
[1mStep[0m  [10/106], [94mLoss[0m : 2.20212
[1mStep[0m  [20/106], [94mLoss[0m : 2.08460
[1mStep[0m  [30/106], [94mLoss[0m : 1.96643
[1mStep[0m  [40/106], [94mLoss[0m : 2.01747
[1mStep[0m  [50/106], [94mLoss[0m : 1.87680
[1mStep[0m  [60/106], [94mLoss[0m : 2.21483
[1mStep[0m  [70/106], [94mLoss[0m : 1.95192
[1mStep[0m  [80/106], [94mLoss[0m : 2.37480
[1mStep[0m  [90/106], [94mLoss[0m : 2.41360
[1mStep[0m  [100/106], [94mLoss[0m : 1.86505

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.01882
[1mStep[0m  [10/106], [94mLoss[0m : 2.31047
[1mStep[0m  [20/106], [94mLoss[0m : 1.99740
[1mStep[0m  [30/106], [94mLoss[0m : 2.24858
[1mStep[0m  [40/106], [94mLoss[0m : 2.22569
[1mStep[0m  [50/106], [94mLoss[0m : 1.61050
[1mStep[0m  [60/106], [94mLoss[0m : 2.03074
[1mStep[0m  [70/106], [94mLoss[0m : 2.01796
[1mStep[0m  [80/106], [94mLoss[0m : 2.12744
[1mStep[0m  [90/106], [94mLoss[0m : 1.91361
[1mStep[0m  [100/106], [94mLoss[0m : 2.06805

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.84230
[1mStep[0m  [10/106], [94mLoss[0m : 2.06932
[1mStep[0m  [20/106], [94mLoss[0m : 2.08233
[1mStep[0m  [30/106], [94mLoss[0m : 1.94596
[1mStep[0m  [40/106], [94mLoss[0m : 2.17333
[1mStep[0m  [50/106], [94mLoss[0m : 2.21867
[1mStep[0m  [60/106], [94mLoss[0m : 2.10898
[1mStep[0m  [70/106], [94mLoss[0m : 1.95023
[1mStep[0m  [80/106], [94mLoss[0m : 2.00904
[1mStep[0m  [90/106], [94mLoss[0m : 2.16935
[1mStep[0m  [100/106], [94mLoss[0m : 1.98794

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74014
[1mStep[0m  [10/106], [94mLoss[0m : 1.89098
[1mStep[0m  [20/106], [94mLoss[0m : 1.90370
[1mStep[0m  [30/106], [94mLoss[0m : 1.86017
[1mStep[0m  [40/106], [94mLoss[0m : 1.98052
[1mStep[0m  [50/106], [94mLoss[0m : 1.97395
[1mStep[0m  [60/106], [94mLoss[0m : 2.28791
[1mStep[0m  [70/106], [94mLoss[0m : 2.05271
[1mStep[0m  [80/106], [94mLoss[0m : 2.20838
[1mStep[0m  [90/106], [94mLoss[0m : 2.19364
[1mStep[0m  [100/106], [94mLoss[0m : 1.99548

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.99085
[1mStep[0m  [10/106], [94mLoss[0m : 1.87927
[1mStep[0m  [20/106], [94mLoss[0m : 2.03784
[1mStep[0m  [30/106], [94mLoss[0m : 1.92219
[1mStep[0m  [40/106], [94mLoss[0m : 1.92464
[1mStep[0m  [50/106], [94mLoss[0m : 1.72332
[1mStep[0m  [60/106], [94mLoss[0m : 1.86581
[1mStep[0m  [70/106], [94mLoss[0m : 1.93786
[1mStep[0m  [80/106], [94mLoss[0m : 1.74203
[1mStep[0m  [90/106], [94mLoss[0m : 1.98808
[1mStep[0m  [100/106], [94mLoss[0m : 2.10115

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98148
[1mStep[0m  [10/106], [94mLoss[0m : 2.00035
[1mStep[0m  [20/106], [94mLoss[0m : 1.55066
[1mStep[0m  [30/106], [94mLoss[0m : 1.78403
[1mStep[0m  [40/106], [94mLoss[0m : 2.01520
[1mStep[0m  [50/106], [94mLoss[0m : 1.85947
[1mStep[0m  [60/106], [94mLoss[0m : 1.79292
[1mStep[0m  [70/106], [94mLoss[0m : 1.78233
[1mStep[0m  [80/106], [94mLoss[0m : 2.00104
[1mStep[0m  [90/106], [94mLoss[0m : 1.82428
[1mStep[0m  [100/106], [94mLoss[0m : 1.83090

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62947
[1mStep[0m  [10/106], [94mLoss[0m : 1.84233
[1mStep[0m  [20/106], [94mLoss[0m : 1.87694
[1mStep[0m  [30/106], [94mLoss[0m : 1.68921
[1mStep[0m  [40/106], [94mLoss[0m : 2.05689
[1mStep[0m  [50/106], [94mLoss[0m : 2.08269
[1mStep[0m  [60/106], [94mLoss[0m : 1.77645
[1mStep[0m  [70/106], [94mLoss[0m : 2.21994
[1mStep[0m  [80/106], [94mLoss[0m : 1.85280
[1mStep[0m  [90/106], [94mLoss[0m : 1.94852
[1mStep[0m  [100/106], [94mLoss[0m : 1.65143

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75178
[1mStep[0m  [10/106], [94mLoss[0m : 1.56183
[1mStep[0m  [20/106], [94mLoss[0m : 1.67068
[1mStep[0m  [30/106], [94mLoss[0m : 1.84771
[1mStep[0m  [40/106], [94mLoss[0m : 1.69481
[1mStep[0m  [50/106], [94mLoss[0m : 1.86224
[1mStep[0m  [60/106], [94mLoss[0m : 1.99693
[1mStep[0m  [70/106], [94mLoss[0m : 2.09256
[1mStep[0m  [80/106], [94mLoss[0m : 1.89137
[1mStep[0m  [90/106], [94mLoss[0m : 1.69645
[1mStep[0m  [100/106], [94mLoss[0m : 1.89322

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75998
[1mStep[0m  [10/106], [94mLoss[0m : 1.70728
[1mStep[0m  [20/106], [94mLoss[0m : 1.74883
[1mStep[0m  [30/106], [94mLoss[0m : 1.89762
[1mStep[0m  [40/106], [94mLoss[0m : 1.85229
[1mStep[0m  [50/106], [94mLoss[0m : 1.96786
[1mStep[0m  [60/106], [94mLoss[0m : 1.94653
[1mStep[0m  [70/106], [94mLoss[0m : 2.05053
[1mStep[0m  [80/106], [94mLoss[0m : 2.02232
[1mStep[0m  [90/106], [94mLoss[0m : 2.22092
[1mStep[0m  [100/106], [94mLoss[0m : 1.90194

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.453, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83957
[1mStep[0m  [10/106], [94mLoss[0m : 1.82537
[1mStep[0m  [20/106], [94mLoss[0m : 1.82138
[1mStep[0m  [30/106], [94mLoss[0m : 1.72897
[1mStep[0m  [40/106], [94mLoss[0m : 1.67899
[1mStep[0m  [50/106], [94mLoss[0m : 1.83599
[1mStep[0m  [60/106], [94mLoss[0m : 1.61112
[1mStep[0m  [70/106], [94mLoss[0m : 1.64581
[1mStep[0m  [80/106], [94mLoss[0m : 1.83572
[1mStep[0m  [90/106], [94mLoss[0m : 1.80660
[1mStep[0m  [100/106], [94mLoss[0m : 1.89417

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65641
[1mStep[0m  [10/106], [94mLoss[0m : 1.71333
[1mStep[0m  [20/106], [94mLoss[0m : 1.82302
[1mStep[0m  [30/106], [94mLoss[0m : 1.76581
[1mStep[0m  [40/106], [94mLoss[0m : 1.68761
[1mStep[0m  [50/106], [94mLoss[0m : 1.81453
[1mStep[0m  [60/106], [94mLoss[0m : 1.78101
[1mStep[0m  [70/106], [94mLoss[0m : 1.84578
[1mStep[0m  [80/106], [94mLoss[0m : 2.13080
[1mStep[0m  [90/106], [94mLoss[0m : 1.83224
[1mStep[0m  [100/106], [94mLoss[0m : 1.88461

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65345
[1mStep[0m  [10/106], [94mLoss[0m : 1.72145
[1mStep[0m  [20/106], [94mLoss[0m : 1.89285
[1mStep[0m  [30/106], [94mLoss[0m : 1.77940
[1mStep[0m  [40/106], [94mLoss[0m : 1.89570
[1mStep[0m  [50/106], [94mLoss[0m : 1.73905
[1mStep[0m  [60/106], [94mLoss[0m : 2.04062
[1mStep[0m  [70/106], [94mLoss[0m : 1.83260
[1mStep[0m  [80/106], [94mLoss[0m : 1.75387
[1mStep[0m  [90/106], [94mLoss[0m : 1.74486
[1mStep[0m  [100/106], [94mLoss[0m : 2.30537

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67973
[1mStep[0m  [10/106], [94mLoss[0m : 1.68096
[1mStep[0m  [20/106], [94mLoss[0m : 2.08200
[1mStep[0m  [30/106], [94mLoss[0m : 2.17613
[1mStep[0m  [40/106], [94mLoss[0m : 1.85288
[1mStep[0m  [50/106], [94mLoss[0m : 1.68761
[1mStep[0m  [60/106], [94mLoss[0m : 1.74852
[1mStep[0m  [70/106], [94mLoss[0m : 1.83123
[1mStep[0m  [80/106], [94mLoss[0m : 1.93750
[1mStep[0m  [90/106], [94mLoss[0m : 1.86499
[1mStep[0m  [100/106], [94mLoss[0m : 1.77876

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.535
====================================

Phase 2 - Evaluation MAE:  2.5350167166511968
MAE score P1        2.416199
MAE score P2        2.535017
loss                1.789333
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.1
weight_decay           0.001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.94882
[1mStep[0m  [2/26], [94mLoss[0m : 9.79128
[1mStep[0m  [4/26], [94mLoss[0m : 8.78089
[1mStep[0m  [6/26], [94mLoss[0m : 6.67998
[1mStep[0m  [8/26], [94mLoss[0m : 5.00680
[1mStep[0m  [10/26], [94mLoss[0m : 4.13847
[1mStep[0m  [12/26], [94mLoss[0m : 3.00773
[1mStep[0m  [14/26], [94mLoss[0m : 3.21659
[1mStep[0m  [16/26], [94mLoss[0m : 3.61718
[1mStep[0m  [18/26], [94mLoss[0m : 4.05825
[1mStep[0m  [20/26], [94mLoss[0m : 3.79481
[1mStep[0m  [22/26], [94mLoss[0m : 3.06242
[1mStep[0m  [24/26], [94mLoss[0m : 2.59877

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.126, [92mTest[0m: 10.787, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71494
[1mStep[0m  [2/26], [94mLoss[0m : 2.84075
[1mStep[0m  [4/26], [94mLoss[0m : 2.84019
[1mStep[0m  [6/26], [94mLoss[0m : 2.94602
[1mStep[0m  [8/26], [94mLoss[0m : 2.75456
[1mStep[0m  [10/26], [94mLoss[0m : 2.59660
[1mStep[0m  [12/26], [94mLoss[0m : 2.76271
[1mStep[0m  [14/26], [94mLoss[0m : 2.66798
[1mStep[0m  [16/26], [94mLoss[0m : 2.68963
[1mStep[0m  [18/26], [94mLoss[0m : 2.64950
[1mStep[0m  [20/26], [94mLoss[0m : 2.53366
[1mStep[0m  [22/26], [94mLoss[0m : 2.52775
[1mStep[0m  [24/26], [94mLoss[0m : 2.57652

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.697, [92mTest[0m: 4.603, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51625
[1mStep[0m  [2/26], [94mLoss[0m : 2.56313
[1mStep[0m  [4/26], [94mLoss[0m : 2.68339
[1mStep[0m  [6/26], [94mLoss[0m : 2.65674
[1mStep[0m  [8/26], [94mLoss[0m : 2.50899
[1mStep[0m  [10/26], [94mLoss[0m : 2.35595
[1mStep[0m  [12/26], [94mLoss[0m : 2.71198
[1mStep[0m  [14/26], [94mLoss[0m : 2.49931
[1mStep[0m  [16/26], [94mLoss[0m : 2.60949
[1mStep[0m  [18/26], [94mLoss[0m : 2.59686
[1mStep[0m  [20/26], [94mLoss[0m : 2.58366
[1mStep[0m  [22/26], [94mLoss[0m : 2.60068
[1mStep[0m  [24/26], [94mLoss[0m : 2.52276

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.756, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56380
[1mStep[0m  [2/26], [94mLoss[0m : 2.44703
[1mStep[0m  [4/26], [94mLoss[0m : 2.48700
[1mStep[0m  [6/26], [94mLoss[0m : 2.47410
[1mStep[0m  [8/26], [94mLoss[0m : 2.54123
[1mStep[0m  [10/26], [94mLoss[0m : 2.45354
[1mStep[0m  [12/26], [94mLoss[0m : 2.47699
[1mStep[0m  [14/26], [94mLoss[0m : 2.46453
[1mStep[0m  [16/26], [94mLoss[0m : 2.54117
[1mStep[0m  [18/26], [94mLoss[0m : 2.55068
[1mStep[0m  [20/26], [94mLoss[0m : 2.49965
[1mStep[0m  [22/26], [94mLoss[0m : 2.44405
[1mStep[0m  [24/26], [94mLoss[0m : 2.46102

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.698, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44670
[1mStep[0m  [2/26], [94mLoss[0m : 2.62162
[1mStep[0m  [4/26], [94mLoss[0m : 2.50062
[1mStep[0m  [6/26], [94mLoss[0m : 2.43817
[1mStep[0m  [8/26], [94mLoss[0m : 2.41421
[1mStep[0m  [10/26], [94mLoss[0m : 2.71153
[1mStep[0m  [12/26], [94mLoss[0m : 2.42081
[1mStep[0m  [14/26], [94mLoss[0m : 2.48791
[1mStep[0m  [16/26], [94mLoss[0m : 2.51520
[1mStep[0m  [18/26], [94mLoss[0m : 2.47948
[1mStep[0m  [20/26], [94mLoss[0m : 2.61146
[1mStep[0m  [22/26], [94mLoss[0m : 2.50322
[1mStep[0m  [24/26], [94mLoss[0m : 2.38879

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.584, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43531
[1mStep[0m  [2/26], [94mLoss[0m : 2.31965
[1mStep[0m  [4/26], [94mLoss[0m : 2.31757
[1mStep[0m  [6/26], [94mLoss[0m : 2.48862
[1mStep[0m  [8/26], [94mLoss[0m : 2.37127
[1mStep[0m  [10/26], [94mLoss[0m : 2.50959
[1mStep[0m  [12/26], [94mLoss[0m : 2.55944
[1mStep[0m  [14/26], [94mLoss[0m : 2.38236
[1mStep[0m  [16/26], [94mLoss[0m : 2.56058
[1mStep[0m  [18/26], [94mLoss[0m : 2.45470
[1mStep[0m  [20/26], [94mLoss[0m : 2.41458
[1mStep[0m  [22/26], [94mLoss[0m : 2.62400
[1mStep[0m  [24/26], [94mLoss[0m : 2.43065

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.45050
[1mStep[0m  [2/26], [94mLoss[0m : 2.69145
[1mStep[0m  [4/26], [94mLoss[0m : 2.31298
[1mStep[0m  [6/26], [94mLoss[0m : 2.57797
[1mStep[0m  [8/26], [94mLoss[0m : 2.63449
[1mStep[0m  [10/26], [94mLoss[0m : 2.40618
[1mStep[0m  [12/26], [94mLoss[0m : 2.54446
[1mStep[0m  [14/26], [94mLoss[0m : 2.49027
[1mStep[0m  [16/26], [94mLoss[0m : 2.27894
[1mStep[0m  [18/26], [94mLoss[0m : 2.55155
[1mStep[0m  [20/26], [94mLoss[0m : 2.41879
[1mStep[0m  [22/26], [94mLoss[0m : 2.37864
[1mStep[0m  [24/26], [94mLoss[0m : 2.44101

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49117
[1mStep[0m  [2/26], [94mLoss[0m : 2.43585
[1mStep[0m  [4/26], [94mLoss[0m : 2.32972
[1mStep[0m  [6/26], [94mLoss[0m : 2.36430
[1mStep[0m  [8/26], [94mLoss[0m : 2.63123
[1mStep[0m  [10/26], [94mLoss[0m : 2.61413
[1mStep[0m  [12/26], [94mLoss[0m : 2.56780
[1mStep[0m  [14/26], [94mLoss[0m : 2.42307
[1mStep[0m  [16/26], [94mLoss[0m : 2.41223
[1mStep[0m  [18/26], [94mLoss[0m : 2.46979
[1mStep[0m  [20/26], [94mLoss[0m : 2.48442
[1mStep[0m  [22/26], [94mLoss[0m : 2.44446
[1mStep[0m  [24/26], [94mLoss[0m : 2.43773

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.576, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51550
[1mStep[0m  [2/26], [94mLoss[0m : 2.43374
[1mStep[0m  [4/26], [94mLoss[0m : 2.24143
[1mStep[0m  [6/26], [94mLoss[0m : 2.46262
[1mStep[0m  [8/26], [94mLoss[0m : 2.35007
[1mStep[0m  [10/26], [94mLoss[0m : 2.43972
[1mStep[0m  [12/26], [94mLoss[0m : 2.54189
[1mStep[0m  [14/26], [94mLoss[0m : 2.47141
[1mStep[0m  [16/26], [94mLoss[0m : 2.51641
[1mStep[0m  [18/26], [94mLoss[0m : 2.46223
[1mStep[0m  [20/26], [94mLoss[0m : 2.32735
[1mStep[0m  [22/26], [94mLoss[0m : 2.36548
[1mStep[0m  [24/26], [94mLoss[0m : 2.47422

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.525, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37873
[1mStep[0m  [2/26], [94mLoss[0m : 2.39250
[1mStep[0m  [4/26], [94mLoss[0m : 2.44421
[1mStep[0m  [6/26], [94mLoss[0m : 2.38775
[1mStep[0m  [8/26], [94mLoss[0m : 2.51917
[1mStep[0m  [10/26], [94mLoss[0m : 2.46703
[1mStep[0m  [12/26], [94mLoss[0m : 2.57607
[1mStep[0m  [14/26], [94mLoss[0m : 2.58981
[1mStep[0m  [16/26], [94mLoss[0m : 2.63135
[1mStep[0m  [18/26], [94mLoss[0m : 2.59300
[1mStep[0m  [20/26], [94mLoss[0m : 2.40547
[1mStep[0m  [22/26], [94mLoss[0m : 2.44045
[1mStep[0m  [24/26], [94mLoss[0m : 2.32706

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42332
[1mStep[0m  [2/26], [94mLoss[0m : 2.40570
[1mStep[0m  [4/26], [94mLoss[0m : 2.33365
[1mStep[0m  [6/26], [94mLoss[0m : 2.44468
[1mStep[0m  [8/26], [94mLoss[0m : 2.37976
[1mStep[0m  [10/26], [94mLoss[0m : 2.50360
[1mStep[0m  [12/26], [94mLoss[0m : 2.42492
[1mStep[0m  [14/26], [94mLoss[0m : 2.46028
[1mStep[0m  [16/26], [94mLoss[0m : 2.60019
[1mStep[0m  [18/26], [94mLoss[0m : 2.48913
[1mStep[0m  [20/26], [94mLoss[0m : 2.49799
[1mStep[0m  [22/26], [94mLoss[0m : 2.33383
[1mStep[0m  [24/26], [94mLoss[0m : 2.50769

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.519, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33716
[1mStep[0m  [2/26], [94mLoss[0m : 2.43041
[1mStep[0m  [4/26], [94mLoss[0m : 2.44691
[1mStep[0m  [6/26], [94mLoss[0m : 2.48780
[1mStep[0m  [8/26], [94mLoss[0m : 2.37091
[1mStep[0m  [10/26], [94mLoss[0m : 2.48902
[1mStep[0m  [12/26], [94mLoss[0m : 2.48699
[1mStep[0m  [14/26], [94mLoss[0m : 2.44432
[1mStep[0m  [16/26], [94mLoss[0m : 2.38405
[1mStep[0m  [18/26], [94mLoss[0m : 2.40511
[1mStep[0m  [20/26], [94mLoss[0m : 2.43869
[1mStep[0m  [22/26], [94mLoss[0m : 2.33885
[1mStep[0m  [24/26], [94mLoss[0m : 2.53684

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.44576
[1mStep[0m  [2/26], [94mLoss[0m : 2.31517
[1mStep[0m  [4/26], [94mLoss[0m : 2.25194
[1mStep[0m  [6/26], [94mLoss[0m : 2.37625
[1mStep[0m  [8/26], [94mLoss[0m : 2.31654
[1mStep[0m  [10/26], [94mLoss[0m : 2.39149
[1mStep[0m  [12/26], [94mLoss[0m : 2.64991
[1mStep[0m  [14/26], [94mLoss[0m : 2.37171
[1mStep[0m  [16/26], [94mLoss[0m : 2.34945
[1mStep[0m  [18/26], [94mLoss[0m : 2.38077
[1mStep[0m  [20/26], [94mLoss[0m : 2.41371
[1mStep[0m  [22/26], [94mLoss[0m : 2.51365
[1mStep[0m  [24/26], [94mLoss[0m : 2.46974

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40059
[1mStep[0m  [2/26], [94mLoss[0m : 2.45654
[1mStep[0m  [4/26], [94mLoss[0m : 2.50024
[1mStep[0m  [6/26], [94mLoss[0m : 2.29635
[1mStep[0m  [8/26], [94mLoss[0m : 2.32652
[1mStep[0m  [10/26], [94mLoss[0m : 2.43510
[1mStep[0m  [12/26], [94mLoss[0m : 2.40333
[1mStep[0m  [14/26], [94mLoss[0m : 2.34483
[1mStep[0m  [16/26], [94mLoss[0m : 2.41883
[1mStep[0m  [18/26], [94mLoss[0m : 2.40457
[1mStep[0m  [20/26], [94mLoss[0m : 2.39102
[1mStep[0m  [22/26], [94mLoss[0m : 2.38527
[1mStep[0m  [24/26], [94mLoss[0m : 2.28848

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25898
[1mStep[0m  [2/26], [94mLoss[0m : 2.32075
[1mStep[0m  [4/26], [94mLoss[0m : 2.51823
[1mStep[0m  [6/26], [94mLoss[0m : 2.46212
[1mStep[0m  [8/26], [94mLoss[0m : 2.45386
[1mStep[0m  [10/26], [94mLoss[0m : 2.49262
[1mStep[0m  [12/26], [94mLoss[0m : 2.32616
[1mStep[0m  [14/26], [94mLoss[0m : 2.51013
[1mStep[0m  [16/26], [94mLoss[0m : 2.26267
[1mStep[0m  [18/26], [94mLoss[0m : 2.52860
[1mStep[0m  [20/26], [94mLoss[0m : 2.39834
[1mStep[0m  [22/26], [94mLoss[0m : 2.43622
[1mStep[0m  [24/26], [94mLoss[0m : 2.47719

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43089
[1mStep[0m  [2/26], [94mLoss[0m : 2.41644
[1mStep[0m  [4/26], [94mLoss[0m : 2.42688
[1mStep[0m  [6/26], [94mLoss[0m : 2.48624
[1mStep[0m  [8/26], [94mLoss[0m : 2.33072
[1mStep[0m  [10/26], [94mLoss[0m : 2.33083
[1mStep[0m  [12/26], [94mLoss[0m : 2.45241
[1mStep[0m  [14/26], [94mLoss[0m : 2.39200
[1mStep[0m  [16/26], [94mLoss[0m : 2.40172
[1mStep[0m  [18/26], [94mLoss[0m : 2.27012
[1mStep[0m  [20/26], [94mLoss[0m : 2.50172
[1mStep[0m  [22/26], [94mLoss[0m : 2.41399
[1mStep[0m  [24/26], [94mLoss[0m : 2.48124

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.27875
[1mStep[0m  [2/26], [94mLoss[0m : 2.35376
[1mStep[0m  [4/26], [94mLoss[0m : 2.45387
[1mStep[0m  [6/26], [94mLoss[0m : 2.46785
[1mStep[0m  [8/26], [94mLoss[0m : 2.48115
[1mStep[0m  [10/26], [94mLoss[0m : 2.37515
[1mStep[0m  [12/26], [94mLoss[0m : 2.37029
[1mStep[0m  [14/26], [94mLoss[0m : 2.46948
[1mStep[0m  [16/26], [94mLoss[0m : 2.38704
[1mStep[0m  [18/26], [94mLoss[0m : 2.32167
[1mStep[0m  [20/26], [94mLoss[0m : 2.20561
[1mStep[0m  [22/26], [94mLoss[0m : 2.35585
[1mStep[0m  [24/26], [94mLoss[0m : 2.48559

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33870
[1mStep[0m  [2/26], [94mLoss[0m : 2.27582
[1mStep[0m  [4/26], [94mLoss[0m : 2.46410
[1mStep[0m  [6/26], [94mLoss[0m : 2.37431
[1mStep[0m  [8/26], [94mLoss[0m : 2.43703
[1mStep[0m  [10/26], [94mLoss[0m : 2.42891
[1mStep[0m  [12/26], [94mLoss[0m : 2.47871
[1mStep[0m  [14/26], [94mLoss[0m : 2.25779
[1mStep[0m  [16/26], [94mLoss[0m : 2.43756
[1mStep[0m  [18/26], [94mLoss[0m : 2.34912
[1mStep[0m  [20/26], [94mLoss[0m : 2.44329
[1mStep[0m  [22/26], [94mLoss[0m : 2.46242
[1mStep[0m  [24/26], [94mLoss[0m : 2.33748

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.22047
[1mStep[0m  [2/26], [94mLoss[0m : 2.38996
[1mStep[0m  [4/26], [94mLoss[0m : 2.17151
[1mStep[0m  [6/26], [94mLoss[0m : 2.37920
[1mStep[0m  [8/26], [94mLoss[0m : 2.26332
[1mStep[0m  [10/26], [94mLoss[0m : 2.35955
[1mStep[0m  [12/26], [94mLoss[0m : 2.42203
[1mStep[0m  [14/26], [94mLoss[0m : 2.46195
[1mStep[0m  [16/26], [94mLoss[0m : 2.29994
[1mStep[0m  [18/26], [94mLoss[0m : 2.36676
[1mStep[0m  [20/26], [94mLoss[0m : 2.37974
[1mStep[0m  [22/26], [94mLoss[0m : 2.46629
[1mStep[0m  [24/26], [94mLoss[0m : 2.46702

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43193
[1mStep[0m  [2/26], [94mLoss[0m : 2.38914
[1mStep[0m  [4/26], [94mLoss[0m : 2.26168
[1mStep[0m  [6/26], [94mLoss[0m : 2.42692
[1mStep[0m  [8/26], [94mLoss[0m : 2.54188
[1mStep[0m  [10/26], [94mLoss[0m : 2.13216
[1mStep[0m  [12/26], [94mLoss[0m : 2.50534
[1mStep[0m  [14/26], [94mLoss[0m : 2.36600
[1mStep[0m  [16/26], [94mLoss[0m : 2.34286
[1mStep[0m  [18/26], [94mLoss[0m : 2.44015
[1mStep[0m  [20/26], [94mLoss[0m : 2.31553
[1mStep[0m  [22/26], [94mLoss[0m : 2.46083
[1mStep[0m  [24/26], [94mLoss[0m : 2.57529

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53273
[1mStep[0m  [2/26], [94mLoss[0m : 2.35263
[1mStep[0m  [4/26], [94mLoss[0m : 2.38396
[1mStep[0m  [6/26], [94mLoss[0m : 2.31985
[1mStep[0m  [8/26], [94mLoss[0m : 2.39711
[1mStep[0m  [10/26], [94mLoss[0m : 2.25153
[1mStep[0m  [12/26], [94mLoss[0m : 2.35143
[1mStep[0m  [14/26], [94mLoss[0m : 2.17256
[1mStep[0m  [16/26], [94mLoss[0m : 2.41013
[1mStep[0m  [18/26], [94mLoss[0m : 2.39566
[1mStep[0m  [20/26], [94mLoss[0m : 2.51405
[1mStep[0m  [22/26], [94mLoss[0m : 2.32144
[1mStep[0m  [24/26], [94mLoss[0m : 2.34441

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35295
[1mStep[0m  [2/26], [94mLoss[0m : 2.43995
[1mStep[0m  [4/26], [94mLoss[0m : 2.25129
[1mStep[0m  [6/26], [94mLoss[0m : 2.59190
[1mStep[0m  [8/26], [94mLoss[0m : 2.15897
[1mStep[0m  [10/26], [94mLoss[0m : 2.22036
[1mStep[0m  [12/26], [94mLoss[0m : 2.47273
[1mStep[0m  [14/26], [94mLoss[0m : 2.43818
[1mStep[0m  [16/26], [94mLoss[0m : 2.39012
[1mStep[0m  [18/26], [94mLoss[0m : 2.34107
[1mStep[0m  [20/26], [94mLoss[0m : 2.42229
[1mStep[0m  [22/26], [94mLoss[0m : 2.28910
[1mStep[0m  [24/26], [94mLoss[0m : 2.41354

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36771
[1mStep[0m  [2/26], [94mLoss[0m : 2.57514
[1mStep[0m  [4/26], [94mLoss[0m : 2.34024
[1mStep[0m  [6/26], [94mLoss[0m : 2.26669
[1mStep[0m  [8/26], [94mLoss[0m : 2.38471
[1mStep[0m  [10/26], [94mLoss[0m : 2.29316
[1mStep[0m  [12/26], [94mLoss[0m : 2.39480
[1mStep[0m  [14/26], [94mLoss[0m : 2.49357
[1mStep[0m  [16/26], [94mLoss[0m : 2.40949
[1mStep[0m  [18/26], [94mLoss[0m : 2.32712
[1mStep[0m  [20/26], [94mLoss[0m : 2.41194
[1mStep[0m  [22/26], [94mLoss[0m : 2.31483
[1mStep[0m  [24/26], [94mLoss[0m : 2.40686

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41793
[1mStep[0m  [2/26], [94mLoss[0m : 2.31774
[1mStep[0m  [4/26], [94mLoss[0m : 2.37201
[1mStep[0m  [6/26], [94mLoss[0m : 2.43067
[1mStep[0m  [8/26], [94mLoss[0m : 2.52039
[1mStep[0m  [10/26], [94mLoss[0m : 2.39145
[1mStep[0m  [12/26], [94mLoss[0m : 2.37449
[1mStep[0m  [14/26], [94mLoss[0m : 2.46465
[1mStep[0m  [16/26], [94mLoss[0m : 2.27582
[1mStep[0m  [18/26], [94mLoss[0m : 2.37500
[1mStep[0m  [20/26], [94mLoss[0m : 2.36010
[1mStep[0m  [22/26], [94mLoss[0m : 2.36725
[1mStep[0m  [24/26], [94mLoss[0m : 2.36489

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.32259
[1mStep[0m  [2/26], [94mLoss[0m : 2.39696
[1mStep[0m  [4/26], [94mLoss[0m : 2.27865
[1mStep[0m  [6/26], [94mLoss[0m : 2.34299
[1mStep[0m  [8/26], [94mLoss[0m : 2.45294
[1mStep[0m  [10/26], [94mLoss[0m : 2.48594
[1mStep[0m  [12/26], [94mLoss[0m : 2.35737
[1mStep[0m  [14/26], [94mLoss[0m : 2.41485
[1mStep[0m  [16/26], [94mLoss[0m : 2.22267
[1mStep[0m  [18/26], [94mLoss[0m : 2.54697
[1mStep[0m  [20/26], [94mLoss[0m : 2.43363
[1mStep[0m  [22/26], [94mLoss[0m : 2.42072
[1mStep[0m  [24/26], [94mLoss[0m : 2.26258

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25343
[1mStep[0m  [2/26], [94mLoss[0m : 2.20469
[1mStep[0m  [4/26], [94mLoss[0m : 2.44988
[1mStep[0m  [6/26], [94mLoss[0m : 2.28671
[1mStep[0m  [8/26], [94mLoss[0m : 2.51072
[1mStep[0m  [10/26], [94mLoss[0m : 2.40531
[1mStep[0m  [12/26], [94mLoss[0m : 2.36651
[1mStep[0m  [14/26], [94mLoss[0m : 2.51366
[1mStep[0m  [16/26], [94mLoss[0m : 2.26326
[1mStep[0m  [18/26], [94mLoss[0m : 2.35154
[1mStep[0m  [20/26], [94mLoss[0m : 2.36307
[1mStep[0m  [22/26], [94mLoss[0m : 2.41537
[1mStep[0m  [24/26], [94mLoss[0m : 2.29021

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47384
[1mStep[0m  [2/26], [94mLoss[0m : 2.25416
[1mStep[0m  [4/26], [94mLoss[0m : 2.40817
[1mStep[0m  [6/26], [94mLoss[0m : 2.32459
[1mStep[0m  [8/26], [94mLoss[0m : 2.35878
[1mStep[0m  [10/26], [94mLoss[0m : 2.30841
[1mStep[0m  [12/26], [94mLoss[0m : 2.32855
[1mStep[0m  [14/26], [94mLoss[0m : 2.40923
[1mStep[0m  [16/26], [94mLoss[0m : 2.53323
[1mStep[0m  [18/26], [94mLoss[0m : 2.35891
[1mStep[0m  [20/26], [94mLoss[0m : 2.30738
[1mStep[0m  [22/26], [94mLoss[0m : 2.40339
[1mStep[0m  [24/26], [94mLoss[0m : 2.37898

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.444, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.25666
[1mStep[0m  [2/26], [94mLoss[0m : 2.48782
[1mStep[0m  [4/26], [94mLoss[0m : 2.32283
[1mStep[0m  [6/26], [94mLoss[0m : 2.24514
[1mStep[0m  [8/26], [94mLoss[0m : 2.27124
[1mStep[0m  [10/26], [94mLoss[0m : 2.41617
[1mStep[0m  [12/26], [94mLoss[0m : 2.35407
[1mStep[0m  [14/26], [94mLoss[0m : 2.31016
[1mStep[0m  [16/26], [94mLoss[0m : 2.31430
[1mStep[0m  [18/26], [94mLoss[0m : 2.37429
[1mStep[0m  [20/26], [94mLoss[0m : 2.34904
[1mStep[0m  [22/26], [94mLoss[0m : 2.47117
[1mStep[0m  [24/26], [94mLoss[0m : 2.31909

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33200
[1mStep[0m  [2/26], [94mLoss[0m : 2.24379
[1mStep[0m  [4/26], [94mLoss[0m : 2.58454
[1mStep[0m  [6/26], [94mLoss[0m : 2.34714
[1mStep[0m  [8/26], [94mLoss[0m : 2.33974
[1mStep[0m  [10/26], [94mLoss[0m : 2.34814
[1mStep[0m  [12/26], [94mLoss[0m : 2.47882
[1mStep[0m  [14/26], [94mLoss[0m : 2.35936
[1mStep[0m  [16/26], [94mLoss[0m : 2.35010
[1mStep[0m  [18/26], [94mLoss[0m : 2.38155
[1mStep[0m  [20/26], [94mLoss[0m : 2.42478
[1mStep[0m  [22/26], [94mLoss[0m : 2.26374
[1mStep[0m  [24/26], [94mLoss[0m : 2.45295

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.392, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.33441
[1mStep[0m  [2/26], [94mLoss[0m : 2.44967
[1mStep[0m  [4/26], [94mLoss[0m : 2.24753
[1mStep[0m  [6/26], [94mLoss[0m : 2.38122
[1mStep[0m  [8/26], [94mLoss[0m : 2.48591
[1mStep[0m  [10/26], [94mLoss[0m : 2.28011
[1mStep[0m  [12/26], [94mLoss[0m : 2.40478
[1mStep[0m  [14/26], [94mLoss[0m : 2.31477
[1mStep[0m  [16/26], [94mLoss[0m : 2.46499
[1mStep[0m  [18/26], [94mLoss[0m : 2.31604
[1mStep[0m  [20/26], [94mLoss[0m : 2.29699
[1mStep[0m  [22/26], [94mLoss[0m : 2.43872
[1mStep[0m  [24/26], [94mLoss[0m : 2.45729

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.394
====================================

Phase 1 - Evaluation MAE:  2.394154750383817
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.57778
[1mStep[0m  [2/26], [94mLoss[0m : 2.49046
[1mStep[0m  [4/26], [94mLoss[0m : 2.51162
[1mStep[0m  [6/26], [94mLoss[0m : 2.44039
[1mStep[0m  [8/26], [94mLoss[0m : 2.51395
[1mStep[0m  [10/26], [94mLoss[0m : 2.51263
[1mStep[0m  [12/26], [94mLoss[0m : 2.36297
[1mStep[0m  [14/26], [94mLoss[0m : 2.53974
[1mStep[0m  [16/26], [94mLoss[0m : 2.49129
[1mStep[0m  [18/26], [94mLoss[0m : 2.39714
[1mStep[0m  [20/26], [94mLoss[0m : 2.48518
[1mStep[0m  [22/26], [94mLoss[0m : 2.33197
[1mStep[0m  [24/26], [94mLoss[0m : 2.46084

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.40037
[1mStep[0m  [2/26], [94mLoss[0m : 2.30204
[1mStep[0m  [4/26], [94mLoss[0m : 2.49440
[1mStep[0m  [6/26], [94mLoss[0m : 2.50405
[1mStep[0m  [8/26], [94mLoss[0m : 2.37349
[1mStep[0m  [10/26], [94mLoss[0m : 2.46171
[1mStep[0m  [12/26], [94mLoss[0m : 2.18634
[1mStep[0m  [14/26], [94mLoss[0m : 2.47877
[1mStep[0m  [16/26], [94mLoss[0m : 2.33345
[1mStep[0m  [18/26], [94mLoss[0m : 2.49016
[1mStep[0m  [20/26], [94mLoss[0m : 2.33205
[1mStep[0m  [22/26], [94mLoss[0m : 2.27727
[1mStep[0m  [24/26], [94mLoss[0m : 2.26619

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.372, [92mTest[0m: 5.863, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41636
[1mStep[0m  [2/26], [94mLoss[0m : 2.25119
[1mStep[0m  [4/26], [94mLoss[0m : 2.35593
[1mStep[0m  [6/26], [94mLoss[0m : 2.08918
[1mStep[0m  [8/26], [94mLoss[0m : 2.22070
[1mStep[0m  [10/26], [94mLoss[0m : 2.25823
[1mStep[0m  [12/26], [94mLoss[0m : 2.37386
[1mStep[0m  [14/26], [94mLoss[0m : 2.23005
[1mStep[0m  [16/26], [94mLoss[0m : 2.24587
[1mStep[0m  [18/26], [94mLoss[0m : 2.25667
[1mStep[0m  [20/26], [94mLoss[0m : 2.29707
[1mStep[0m  [22/26], [94mLoss[0m : 2.16542
[1mStep[0m  [24/26], [94mLoss[0m : 2.34452

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.264, [92mTest[0m: 3.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.05818
[1mStep[0m  [2/26], [94mLoss[0m : 2.21014
[1mStep[0m  [4/26], [94mLoss[0m : 2.21228
[1mStep[0m  [6/26], [94mLoss[0m : 2.17525
[1mStep[0m  [8/26], [94mLoss[0m : 2.20320
[1mStep[0m  [10/26], [94mLoss[0m : 2.03436
[1mStep[0m  [12/26], [94mLoss[0m : 2.10878
[1mStep[0m  [14/26], [94mLoss[0m : 2.28744
[1mStep[0m  [16/26], [94mLoss[0m : 2.26061
[1mStep[0m  [18/26], [94mLoss[0m : 2.27134
[1mStep[0m  [20/26], [94mLoss[0m : 2.20498
[1mStep[0m  [22/26], [94mLoss[0m : 2.19359
[1mStep[0m  [24/26], [94mLoss[0m : 2.29200

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.203, [92mTest[0m: 2.757, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.10728
[1mStep[0m  [2/26], [94mLoss[0m : 2.02946
[1mStep[0m  [4/26], [94mLoss[0m : 2.01113
[1mStep[0m  [6/26], [94mLoss[0m : 2.18501
[1mStep[0m  [8/26], [94mLoss[0m : 2.07750
[1mStep[0m  [10/26], [94mLoss[0m : 2.18279
[1mStep[0m  [12/26], [94mLoss[0m : 2.19866
[1mStep[0m  [14/26], [94mLoss[0m : 2.22294
[1mStep[0m  [16/26], [94mLoss[0m : 2.26808
[1mStep[0m  [18/26], [94mLoss[0m : 2.14021
[1mStep[0m  [20/26], [94mLoss[0m : 2.09742
[1mStep[0m  [22/26], [94mLoss[0m : 2.18817
[1mStep[0m  [24/26], [94mLoss[0m : 2.19926

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.97472
[1mStep[0m  [2/26], [94mLoss[0m : 2.07103
[1mStep[0m  [4/26], [94mLoss[0m : 1.92340
[1mStep[0m  [6/26], [94mLoss[0m : 1.95013
[1mStep[0m  [8/26], [94mLoss[0m : 2.00342
[1mStep[0m  [10/26], [94mLoss[0m : 2.22335
[1mStep[0m  [12/26], [94mLoss[0m : 2.07898
[1mStep[0m  [14/26], [94mLoss[0m : 2.12440
[1mStep[0m  [16/26], [94mLoss[0m : 2.08307
[1mStep[0m  [18/26], [94mLoss[0m : 1.98669
[1mStep[0m  [20/26], [94mLoss[0m : 2.06122
[1mStep[0m  [22/26], [94mLoss[0m : 2.11310
[1mStep[0m  [24/26], [94mLoss[0m : 2.05591

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.07820
[1mStep[0m  [2/26], [94mLoss[0m : 1.90808
[1mStep[0m  [4/26], [94mLoss[0m : 2.06464
[1mStep[0m  [6/26], [94mLoss[0m : 2.04570
[1mStep[0m  [8/26], [94mLoss[0m : 1.85814
[1mStep[0m  [10/26], [94mLoss[0m : 1.93628
[1mStep[0m  [12/26], [94mLoss[0m : 2.03988
[1mStep[0m  [14/26], [94mLoss[0m : 2.05810
[1mStep[0m  [16/26], [94mLoss[0m : 2.05182
[1mStep[0m  [18/26], [94mLoss[0m : 2.13896
[1mStep[0m  [20/26], [94mLoss[0m : 2.07905
[1mStep[0m  [22/26], [94mLoss[0m : 1.98649
[1mStep[0m  [24/26], [94mLoss[0m : 1.87479

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.96574
[1mStep[0m  [2/26], [94mLoss[0m : 2.01707
[1mStep[0m  [4/26], [94mLoss[0m : 2.01711
[1mStep[0m  [6/26], [94mLoss[0m : 1.73726
[1mStep[0m  [8/26], [94mLoss[0m : 1.95533
[1mStep[0m  [10/26], [94mLoss[0m : 2.01840
[1mStep[0m  [12/26], [94mLoss[0m : 1.93992
[1mStep[0m  [14/26], [94mLoss[0m : 1.95471
[1mStep[0m  [16/26], [94mLoss[0m : 2.07354
[1mStep[0m  [18/26], [94mLoss[0m : 1.94272
[1mStep[0m  [20/26], [94mLoss[0m : 1.90290
[1mStep[0m  [22/26], [94mLoss[0m : 2.03271
[1mStep[0m  [24/26], [94mLoss[0m : 1.94371

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.82911
[1mStep[0m  [2/26], [94mLoss[0m : 1.85018
[1mStep[0m  [4/26], [94mLoss[0m : 1.82254
[1mStep[0m  [6/26], [94mLoss[0m : 1.81973
[1mStep[0m  [8/26], [94mLoss[0m : 1.77892
[1mStep[0m  [10/26], [94mLoss[0m : 1.86242
[1mStep[0m  [12/26], [94mLoss[0m : 2.03172
[1mStep[0m  [14/26], [94mLoss[0m : 2.03006
[1mStep[0m  [16/26], [94mLoss[0m : 1.97326
[1mStep[0m  [18/26], [94mLoss[0m : 1.87412
[1mStep[0m  [20/26], [94mLoss[0m : 1.93661
[1mStep[0m  [22/26], [94mLoss[0m : 2.01702
[1mStep[0m  [24/26], [94mLoss[0m : 2.04491

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.75562
[1mStep[0m  [2/26], [94mLoss[0m : 1.87530
[1mStep[0m  [4/26], [94mLoss[0m : 1.74069
[1mStep[0m  [6/26], [94mLoss[0m : 1.91174
[1mStep[0m  [8/26], [94mLoss[0m : 1.90238
[1mStep[0m  [10/26], [94mLoss[0m : 1.72358
[1mStep[0m  [12/26], [94mLoss[0m : 1.87143
[1mStep[0m  [14/26], [94mLoss[0m : 2.04336
[1mStep[0m  [16/26], [94mLoss[0m : 1.85896
[1mStep[0m  [18/26], [94mLoss[0m : 1.84033
[1mStep[0m  [20/26], [94mLoss[0m : 1.86902
[1mStep[0m  [22/26], [94mLoss[0m : 1.84720
[1mStep[0m  [24/26], [94mLoss[0m : 1.91893

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.74572
[1mStep[0m  [2/26], [94mLoss[0m : 1.72092
[1mStep[0m  [4/26], [94mLoss[0m : 1.84191
[1mStep[0m  [6/26], [94mLoss[0m : 1.83638
[1mStep[0m  [8/26], [94mLoss[0m : 1.79218
[1mStep[0m  [10/26], [94mLoss[0m : 1.85108
[1mStep[0m  [12/26], [94mLoss[0m : 1.88085
[1mStep[0m  [14/26], [94mLoss[0m : 1.88578
[1mStep[0m  [16/26], [94mLoss[0m : 1.87261
[1mStep[0m  [18/26], [94mLoss[0m : 1.70641
[1mStep[0m  [20/26], [94mLoss[0m : 1.84273
[1mStep[0m  [22/26], [94mLoss[0m : 1.75894
[1mStep[0m  [24/26], [94mLoss[0m : 1.74588

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.73482
[1mStep[0m  [2/26], [94mLoss[0m : 1.74700
[1mStep[0m  [4/26], [94mLoss[0m : 1.74974
[1mStep[0m  [6/26], [94mLoss[0m : 1.87241
[1mStep[0m  [8/26], [94mLoss[0m : 1.69820
[1mStep[0m  [10/26], [94mLoss[0m : 1.74387
[1mStep[0m  [12/26], [94mLoss[0m : 1.76622
[1mStep[0m  [14/26], [94mLoss[0m : 1.78309
[1mStep[0m  [16/26], [94mLoss[0m : 1.66066
[1mStep[0m  [18/26], [94mLoss[0m : 1.91828
[1mStep[0m  [20/26], [94mLoss[0m : 1.84147
[1mStep[0m  [22/26], [94mLoss[0m : 1.76629
[1mStep[0m  [24/26], [94mLoss[0m : 1.76139

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.67705
[1mStep[0m  [2/26], [94mLoss[0m : 1.75431
[1mStep[0m  [4/26], [94mLoss[0m : 1.81726
[1mStep[0m  [6/26], [94mLoss[0m : 1.84764
[1mStep[0m  [8/26], [94mLoss[0m : 1.83517
[1mStep[0m  [10/26], [94mLoss[0m : 1.72324
[1mStep[0m  [12/26], [94mLoss[0m : 1.76755
[1mStep[0m  [14/26], [94mLoss[0m : 1.74456
[1mStep[0m  [16/26], [94mLoss[0m : 1.58168
[1mStep[0m  [18/26], [94mLoss[0m : 1.75940
[1mStep[0m  [20/26], [94mLoss[0m : 1.69762
[1mStep[0m  [22/26], [94mLoss[0m : 1.81759
[1mStep[0m  [24/26], [94mLoss[0m : 1.75717

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.64387
[1mStep[0m  [2/26], [94mLoss[0m : 1.66727
[1mStep[0m  [4/26], [94mLoss[0m : 1.58393
[1mStep[0m  [6/26], [94mLoss[0m : 1.61113
[1mStep[0m  [8/26], [94mLoss[0m : 1.67069
[1mStep[0m  [10/26], [94mLoss[0m : 1.60903
[1mStep[0m  [12/26], [94mLoss[0m : 1.58038
[1mStep[0m  [14/26], [94mLoss[0m : 1.63158
[1mStep[0m  [16/26], [94mLoss[0m : 1.81009
[1mStep[0m  [18/26], [94mLoss[0m : 1.72329
[1mStep[0m  [20/26], [94mLoss[0m : 1.65569
[1mStep[0m  [22/26], [94mLoss[0m : 1.77043
[1mStep[0m  [24/26], [94mLoss[0m : 1.76659

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.55368
[1mStep[0m  [2/26], [94mLoss[0m : 1.56186
[1mStep[0m  [4/26], [94mLoss[0m : 1.65013
[1mStep[0m  [6/26], [94mLoss[0m : 1.69043
[1mStep[0m  [8/26], [94mLoss[0m : 1.60138
[1mStep[0m  [10/26], [94mLoss[0m : 1.68406
[1mStep[0m  [12/26], [94mLoss[0m : 1.67012
[1mStep[0m  [14/26], [94mLoss[0m : 1.66138
[1mStep[0m  [16/26], [94mLoss[0m : 1.61908
[1mStep[0m  [18/26], [94mLoss[0m : 1.62047
[1mStep[0m  [20/26], [94mLoss[0m : 1.64774
[1mStep[0m  [22/26], [94mLoss[0m : 1.81269
[1mStep[0m  [24/26], [94mLoss[0m : 1.73595

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56924
[1mStep[0m  [2/26], [94mLoss[0m : 1.63632
[1mStep[0m  [4/26], [94mLoss[0m : 1.61920
[1mStep[0m  [6/26], [94mLoss[0m : 1.50348
[1mStep[0m  [8/26], [94mLoss[0m : 1.61237
[1mStep[0m  [10/26], [94mLoss[0m : 1.47547
[1mStep[0m  [12/26], [94mLoss[0m : 1.69011
[1mStep[0m  [14/26], [94mLoss[0m : 1.57523
[1mStep[0m  [16/26], [94mLoss[0m : 1.57270
[1mStep[0m  [18/26], [94mLoss[0m : 1.52928
[1mStep[0m  [20/26], [94mLoss[0m : 1.74046
[1mStep[0m  [22/26], [94mLoss[0m : 1.51902
[1mStep[0m  [24/26], [94mLoss[0m : 1.63685

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.60108
[1mStep[0m  [2/26], [94mLoss[0m : 1.50223
[1mStep[0m  [4/26], [94mLoss[0m : 1.62409
[1mStep[0m  [6/26], [94mLoss[0m : 1.54769
[1mStep[0m  [8/26], [94mLoss[0m : 1.59900
[1mStep[0m  [10/26], [94mLoss[0m : 1.61614
[1mStep[0m  [12/26], [94mLoss[0m : 1.58281
[1mStep[0m  [14/26], [94mLoss[0m : 1.66189
[1mStep[0m  [16/26], [94mLoss[0m : 1.68806
[1mStep[0m  [18/26], [94mLoss[0m : 1.62991
[1mStep[0m  [20/26], [94mLoss[0m : 1.66116
[1mStep[0m  [22/26], [94mLoss[0m : 1.60762
[1mStep[0m  [24/26], [94mLoss[0m : 1.57547

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.61298
[1mStep[0m  [2/26], [94mLoss[0m : 1.61401
[1mStep[0m  [4/26], [94mLoss[0m : 1.49284
[1mStep[0m  [6/26], [94mLoss[0m : 1.54193
[1mStep[0m  [8/26], [94mLoss[0m : 1.55506
[1mStep[0m  [10/26], [94mLoss[0m : 1.65519
[1mStep[0m  [12/26], [94mLoss[0m : 1.50585
[1mStep[0m  [14/26], [94mLoss[0m : 1.48365
[1mStep[0m  [16/26], [94mLoss[0m : 1.59706
[1mStep[0m  [18/26], [94mLoss[0m : 1.59734
[1mStep[0m  [20/26], [94mLoss[0m : 1.60483
[1mStep[0m  [22/26], [94mLoss[0m : 1.57698
[1mStep[0m  [24/26], [94mLoss[0m : 1.51305

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.56882
[1mStep[0m  [2/26], [94mLoss[0m : 1.47181
[1mStep[0m  [4/26], [94mLoss[0m : 1.56255
[1mStep[0m  [6/26], [94mLoss[0m : 1.40518
[1mStep[0m  [8/26], [94mLoss[0m : 1.59486
[1mStep[0m  [10/26], [94mLoss[0m : 1.50818
[1mStep[0m  [12/26], [94mLoss[0m : 1.52135
[1mStep[0m  [14/26], [94mLoss[0m : 1.58050
[1mStep[0m  [16/26], [94mLoss[0m : 1.66480
[1mStep[0m  [18/26], [94mLoss[0m : 1.62154
[1mStep[0m  [20/26], [94mLoss[0m : 1.61196
[1mStep[0m  [22/26], [94mLoss[0m : 1.56110
[1mStep[0m  [24/26], [94mLoss[0m : 1.56740

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.498, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.50164
[1mStep[0m  [2/26], [94mLoss[0m : 1.45412
[1mStep[0m  [4/26], [94mLoss[0m : 1.55747
[1mStep[0m  [6/26], [94mLoss[0m : 1.48940
[1mStep[0m  [8/26], [94mLoss[0m : 1.48019
[1mStep[0m  [10/26], [94mLoss[0m : 1.47404
[1mStep[0m  [12/26], [94mLoss[0m : 1.50983
[1mStep[0m  [14/26], [94mLoss[0m : 1.55806
[1mStep[0m  [16/26], [94mLoss[0m : 1.55476
[1mStep[0m  [18/26], [94mLoss[0m : 1.51625
[1mStep[0m  [20/26], [94mLoss[0m : 1.63363
[1mStep[0m  [22/26], [94mLoss[0m : 1.55229
[1mStep[0m  [24/26], [94mLoss[0m : 1.58034

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.533, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.47634
[1mStep[0m  [2/26], [94mLoss[0m : 1.46028
[1mStep[0m  [4/26], [94mLoss[0m : 1.45220
[1mStep[0m  [6/26], [94mLoss[0m : 1.37292
[1mStep[0m  [8/26], [94mLoss[0m : 1.47260
[1mStep[0m  [10/26], [94mLoss[0m : 1.49359
[1mStep[0m  [12/26], [94mLoss[0m : 1.50706
[1mStep[0m  [14/26], [94mLoss[0m : 1.48694
[1mStep[0m  [16/26], [94mLoss[0m : 1.55194
[1mStep[0m  [18/26], [94mLoss[0m : 1.51231
[1mStep[0m  [20/26], [94mLoss[0m : 1.51005
[1mStep[0m  [22/26], [94mLoss[0m : 1.52539
[1mStep[0m  [24/26], [94mLoss[0m : 1.58641

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49405
[1mStep[0m  [2/26], [94mLoss[0m : 1.45780
[1mStep[0m  [4/26], [94mLoss[0m : 1.46715
[1mStep[0m  [6/26], [94mLoss[0m : 1.52722
[1mStep[0m  [8/26], [94mLoss[0m : 1.60488
[1mStep[0m  [10/26], [94mLoss[0m : 1.43444
[1mStep[0m  [12/26], [94mLoss[0m : 1.46354
[1mStep[0m  [14/26], [94mLoss[0m : 1.47775
[1mStep[0m  [16/26], [94mLoss[0m : 1.58427
[1mStep[0m  [18/26], [94mLoss[0m : 1.45663
[1mStep[0m  [20/26], [94mLoss[0m : 1.42305
[1mStep[0m  [22/26], [94mLoss[0m : 1.46459
[1mStep[0m  [24/26], [94mLoss[0m : 1.54755

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.49318
[1mStep[0m  [2/26], [94mLoss[0m : 1.50612
[1mStep[0m  [4/26], [94mLoss[0m : 1.30914
[1mStep[0m  [6/26], [94mLoss[0m : 1.43134
[1mStep[0m  [8/26], [94mLoss[0m : 1.45577
[1mStep[0m  [10/26], [94mLoss[0m : 1.42152
[1mStep[0m  [12/26], [94mLoss[0m : 1.42081
[1mStep[0m  [14/26], [94mLoss[0m : 1.49085
[1mStep[0m  [16/26], [94mLoss[0m : 1.50721
[1mStep[0m  [18/26], [94mLoss[0m : 1.43998
[1mStep[0m  [20/26], [94mLoss[0m : 1.54182
[1mStep[0m  [22/26], [94mLoss[0m : 1.42664
[1mStep[0m  [24/26], [94mLoss[0m : 1.41326

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.576, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 1.41891
[1mStep[0m  [2/26], [94mLoss[0m : 1.38861
[1mStep[0m  [4/26], [94mLoss[0m : 1.36393
[1mStep[0m  [6/26], [94mLoss[0m : 1.41987
[1mStep[0m  [8/26], [94mLoss[0m : 1.53673
[1mStep[0m  [10/26], [94mLoss[0m : 1.39672
[1mStep[0m  [12/26], [94mLoss[0m : 1.42907
[1mStep[0m  [14/26], [94mLoss[0m : 1.43976
[1mStep[0m  [16/26], [94mLoss[0m : 1.39625
[1mStep[0m  [18/26], [94mLoss[0m : 1.42407
[1mStep[0m  [20/26], [94mLoss[0m : 1.45621
[1mStep[0m  [22/26], [94mLoss[0m : 1.45297
[1mStep[0m  [24/26], [94mLoss[0m : 1.48084

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.432, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.463
====================================

Phase 2 - Evaluation MAE:  2.4630018380972056
MAE score P1      2.394155
MAE score P2      2.463002
loss                1.4324
learning_rate      0.00505
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 11.49498
[1mStep[0m  [5/53], [94mLoss[0m : 9.17547
[1mStep[0m  [10/53], [94mLoss[0m : 7.62144
[1mStep[0m  [15/53], [94mLoss[0m : 5.62164
[1mStep[0m  [20/53], [94mLoss[0m : 4.16403
[1mStep[0m  [25/53], [94mLoss[0m : 3.66713
[1mStep[0m  [30/53], [94mLoss[0m : 3.23129
[1mStep[0m  [35/53], [94mLoss[0m : 2.67069
[1mStep[0m  [40/53], [94mLoss[0m : 2.75190
[1mStep[0m  [45/53], [94mLoss[0m : 2.95092
[1mStep[0m  [50/53], [94mLoss[0m : 2.63506

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.774, [92mTest[0m: 10.956, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55903
[1mStep[0m  [5/53], [94mLoss[0m : 2.60942
[1mStep[0m  [10/53], [94mLoss[0m : 2.51297
[1mStep[0m  [15/53], [94mLoss[0m : 2.79935
[1mStep[0m  [20/53], [94mLoss[0m : 2.71278
[1mStep[0m  [25/53], [94mLoss[0m : 2.47227
[1mStep[0m  [30/53], [94mLoss[0m : 2.56144
[1mStep[0m  [35/53], [94mLoss[0m : 2.53664
[1mStep[0m  [40/53], [94mLoss[0m : 2.81909
[1mStep[0m  [45/53], [94mLoss[0m : 2.47434
[1mStep[0m  [50/53], [94mLoss[0m : 2.37688

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.651, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48197
[1mStep[0m  [5/53], [94mLoss[0m : 2.47147
[1mStep[0m  [10/53], [94mLoss[0m : 2.61053
[1mStep[0m  [15/53], [94mLoss[0m : 2.68267
[1mStep[0m  [20/53], [94mLoss[0m : 2.65008
[1mStep[0m  [25/53], [94mLoss[0m : 2.36569
[1mStep[0m  [30/53], [94mLoss[0m : 2.33529
[1mStep[0m  [35/53], [94mLoss[0m : 2.39788
[1mStep[0m  [40/53], [94mLoss[0m : 2.47224
[1mStep[0m  [45/53], [94mLoss[0m : 2.42184
[1mStep[0m  [50/53], [94mLoss[0m : 2.35172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42320
[1mStep[0m  [5/53], [94mLoss[0m : 2.73376
[1mStep[0m  [10/53], [94mLoss[0m : 2.53962
[1mStep[0m  [15/53], [94mLoss[0m : 2.48506
[1mStep[0m  [20/53], [94mLoss[0m : 2.69895
[1mStep[0m  [25/53], [94mLoss[0m : 2.56270
[1mStep[0m  [30/53], [94mLoss[0m : 2.51634
[1mStep[0m  [35/53], [94mLoss[0m : 2.63260
[1mStep[0m  [40/53], [94mLoss[0m : 2.50076
[1mStep[0m  [45/53], [94mLoss[0m : 2.59133
[1mStep[0m  [50/53], [94mLoss[0m : 2.74815

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33069
[1mStep[0m  [5/53], [94mLoss[0m : 2.65607
[1mStep[0m  [10/53], [94mLoss[0m : 2.42572
[1mStep[0m  [15/53], [94mLoss[0m : 2.53299
[1mStep[0m  [20/53], [94mLoss[0m : 2.53342
[1mStep[0m  [25/53], [94mLoss[0m : 2.35147
[1mStep[0m  [30/53], [94mLoss[0m : 2.42042
[1mStep[0m  [35/53], [94mLoss[0m : 2.37306
[1mStep[0m  [40/53], [94mLoss[0m : 2.56179
[1mStep[0m  [45/53], [94mLoss[0m : 2.38207
[1mStep[0m  [50/53], [94mLoss[0m : 2.47423

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43860
[1mStep[0m  [5/53], [94mLoss[0m : 2.34633
[1mStep[0m  [10/53], [94mLoss[0m : 2.57322
[1mStep[0m  [15/53], [94mLoss[0m : 2.41468
[1mStep[0m  [20/53], [94mLoss[0m : 2.70386
[1mStep[0m  [25/53], [94mLoss[0m : 2.51809
[1mStep[0m  [30/53], [94mLoss[0m : 2.44171
[1mStep[0m  [35/53], [94mLoss[0m : 2.54920
[1mStep[0m  [40/53], [94mLoss[0m : 2.50963
[1mStep[0m  [45/53], [94mLoss[0m : 2.55363
[1mStep[0m  [50/53], [94mLoss[0m : 2.42574

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.485, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59821
[1mStep[0m  [5/53], [94mLoss[0m : 2.39816
[1mStep[0m  [10/53], [94mLoss[0m : 2.47858
[1mStep[0m  [15/53], [94mLoss[0m : 2.24292
[1mStep[0m  [20/53], [94mLoss[0m : 2.39384
[1mStep[0m  [25/53], [94mLoss[0m : 2.58544
[1mStep[0m  [30/53], [94mLoss[0m : 2.47344
[1mStep[0m  [35/53], [94mLoss[0m : 2.33959
[1mStep[0m  [40/53], [94mLoss[0m : 2.38366
[1mStep[0m  [45/53], [94mLoss[0m : 2.36275
[1mStep[0m  [50/53], [94mLoss[0m : 2.29636

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46784
[1mStep[0m  [5/53], [94mLoss[0m : 2.66757
[1mStep[0m  [10/53], [94mLoss[0m : 2.61457
[1mStep[0m  [15/53], [94mLoss[0m : 2.49973
[1mStep[0m  [20/53], [94mLoss[0m : 2.39980
[1mStep[0m  [25/53], [94mLoss[0m : 2.56540
[1mStep[0m  [30/53], [94mLoss[0m : 2.53715
[1mStep[0m  [35/53], [94mLoss[0m : 2.49323
[1mStep[0m  [40/53], [94mLoss[0m : 2.25442
[1mStep[0m  [45/53], [94mLoss[0m : 2.35129
[1mStep[0m  [50/53], [94mLoss[0m : 2.44290

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45668
[1mStep[0m  [5/53], [94mLoss[0m : 2.54351
[1mStep[0m  [10/53], [94mLoss[0m : 2.29913
[1mStep[0m  [15/53], [94mLoss[0m : 2.41250
[1mStep[0m  [20/53], [94mLoss[0m : 2.53652
[1mStep[0m  [25/53], [94mLoss[0m : 2.50978
[1mStep[0m  [30/53], [94mLoss[0m : 2.51023
[1mStep[0m  [35/53], [94mLoss[0m : 2.27617
[1mStep[0m  [40/53], [94mLoss[0m : 2.59653
[1mStep[0m  [45/53], [94mLoss[0m : 2.51077
[1mStep[0m  [50/53], [94mLoss[0m : 2.53047

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37220
[1mStep[0m  [5/53], [94mLoss[0m : 2.52180
[1mStep[0m  [10/53], [94mLoss[0m : 2.38235
[1mStep[0m  [15/53], [94mLoss[0m : 2.52671
[1mStep[0m  [20/53], [94mLoss[0m : 2.69594
[1mStep[0m  [25/53], [94mLoss[0m : 2.36150
[1mStep[0m  [30/53], [94mLoss[0m : 2.43425
[1mStep[0m  [35/53], [94mLoss[0m : 2.27144
[1mStep[0m  [40/53], [94mLoss[0m : 2.46763
[1mStep[0m  [45/53], [94mLoss[0m : 2.51031
[1mStep[0m  [50/53], [94mLoss[0m : 2.50504

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53063
[1mStep[0m  [5/53], [94mLoss[0m : 2.56458
[1mStep[0m  [10/53], [94mLoss[0m : 2.55961
[1mStep[0m  [15/53], [94mLoss[0m : 2.63667
[1mStep[0m  [20/53], [94mLoss[0m : 2.17779
[1mStep[0m  [25/53], [94mLoss[0m : 2.61585
[1mStep[0m  [30/53], [94mLoss[0m : 2.61072
[1mStep[0m  [35/53], [94mLoss[0m : 2.49452
[1mStep[0m  [40/53], [94mLoss[0m : 2.43470
[1mStep[0m  [45/53], [94mLoss[0m : 2.43198
[1mStep[0m  [50/53], [94mLoss[0m : 2.28426

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66970
[1mStep[0m  [5/53], [94mLoss[0m : 2.53101
[1mStep[0m  [10/53], [94mLoss[0m : 2.52297
[1mStep[0m  [15/53], [94mLoss[0m : 2.45438
[1mStep[0m  [20/53], [94mLoss[0m : 2.38027
[1mStep[0m  [25/53], [94mLoss[0m : 2.57053
[1mStep[0m  [30/53], [94mLoss[0m : 2.51433
[1mStep[0m  [35/53], [94mLoss[0m : 2.27388
[1mStep[0m  [40/53], [94mLoss[0m : 2.43495
[1mStep[0m  [45/53], [94mLoss[0m : 2.52283
[1mStep[0m  [50/53], [94mLoss[0m : 2.48054

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50976
[1mStep[0m  [5/53], [94mLoss[0m : 2.43737
[1mStep[0m  [10/53], [94mLoss[0m : 2.49657
[1mStep[0m  [15/53], [94mLoss[0m : 2.70698
[1mStep[0m  [20/53], [94mLoss[0m : 2.43638
[1mStep[0m  [25/53], [94mLoss[0m : 2.33732
[1mStep[0m  [30/53], [94mLoss[0m : 2.38570
[1mStep[0m  [35/53], [94mLoss[0m : 2.40223
[1mStep[0m  [40/53], [94mLoss[0m : 2.45427
[1mStep[0m  [45/53], [94mLoss[0m : 2.42425
[1mStep[0m  [50/53], [94mLoss[0m : 2.42308

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60465
[1mStep[0m  [5/53], [94mLoss[0m : 2.43207
[1mStep[0m  [10/53], [94mLoss[0m : 2.43420
[1mStep[0m  [15/53], [94mLoss[0m : 2.41206
[1mStep[0m  [20/53], [94mLoss[0m : 2.35547
[1mStep[0m  [25/53], [94mLoss[0m : 2.43565
[1mStep[0m  [30/53], [94mLoss[0m : 2.72326
[1mStep[0m  [35/53], [94mLoss[0m : 2.55967
[1mStep[0m  [40/53], [94mLoss[0m : 2.32618
[1mStep[0m  [45/53], [94mLoss[0m : 2.53796
[1mStep[0m  [50/53], [94mLoss[0m : 2.30421

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52944
[1mStep[0m  [5/53], [94mLoss[0m : 2.36694
[1mStep[0m  [10/53], [94mLoss[0m : 2.52665
[1mStep[0m  [15/53], [94mLoss[0m : 2.44569
[1mStep[0m  [20/53], [94mLoss[0m : 2.43443
[1mStep[0m  [25/53], [94mLoss[0m : 2.44578
[1mStep[0m  [30/53], [94mLoss[0m : 2.47282
[1mStep[0m  [35/53], [94mLoss[0m : 2.66021
[1mStep[0m  [40/53], [94mLoss[0m : 2.46584
[1mStep[0m  [45/53], [94mLoss[0m : 2.49496
[1mStep[0m  [50/53], [94mLoss[0m : 2.24907

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32059
[1mStep[0m  [5/53], [94mLoss[0m : 2.48517
[1mStep[0m  [10/53], [94mLoss[0m : 2.54774
[1mStep[0m  [15/53], [94mLoss[0m : 2.30807
[1mStep[0m  [20/53], [94mLoss[0m : 2.46661
[1mStep[0m  [25/53], [94mLoss[0m : 2.44372
[1mStep[0m  [30/53], [94mLoss[0m : 2.40236
[1mStep[0m  [35/53], [94mLoss[0m : 2.64909
[1mStep[0m  [40/53], [94mLoss[0m : 2.47425
[1mStep[0m  [45/53], [94mLoss[0m : 2.61273
[1mStep[0m  [50/53], [94mLoss[0m : 2.59436

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19161
[1mStep[0m  [5/53], [94mLoss[0m : 2.62401
[1mStep[0m  [10/53], [94mLoss[0m : 2.34300
[1mStep[0m  [15/53], [94mLoss[0m : 2.30314
[1mStep[0m  [20/53], [94mLoss[0m : 2.30288
[1mStep[0m  [25/53], [94mLoss[0m : 2.38210
[1mStep[0m  [30/53], [94mLoss[0m : 2.23895
[1mStep[0m  [35/53], [94mLoss[0m : 2.42582
[1mStep[0m  [40/53], [94mLoss[0m : 2.40921
[1mStep[0m  [45/53], [94mLoss[0m : 2.42510
[1mStep[0m  [50/53], [94mLoss[0m : 2.49614

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54200
[1mStep[0m  [5/53], [94mLoss[0m : 2.57184
[1mStep[0m  [10/53], [94mLoss[0m : 2.34237
[1mStep[0m  [15/53], [94mLoss[0m : 2.48631
[1mStep[0m  [20/53], [94mLoss[0m : 2.41381
[1mStep[0m  [25/53], [94mLoss[0m : 2.50267
[1mStep[0m  [30/53], [94mLoss[0m : 2.37569
[1mStep[0m  [35/53], [94mLoss[0m : 2.43569
[1mStep[0m  [40/53], [94mLoss[0m : 2.18176
[1mStep[0m  [45/53], [94mLoss[0m : 2.52872
[1mStep[0m  [50/53], [94mLoss[0m : 2.37526

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24664
[1mStep[0m  [5/53], [94mLoss[0m : 2.49648
[1mStep[0m  [10/53], [94mLoss[0m : 2.31552
[1mStep[0m  [15/53], [94mLoss[0m : 2.45759
[1mStep[0m  [20/53], [94mLoss[0m : 2.46389
[1mStep[0m  [25/53], [94mLoss[0m : 2.36629
[1mStep[0m  [30/53], [94mLoss[0m : 2.45857
[1mStep[0m  [35/53], [94mLoss[0m : 2.56073
[1mStep[0m  [40/53], [94mLoss[0m : 2.46690
[1mStep[0m  [45/53], [94mLoss[0m : 2.39685
[1mStep[0m  [50/53], [94mLoss[0m : 2.47022

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35658
[1mStep[0m  [5/53], [94mLoss[0m : 2.43725
[1mStep[0m  [10/53], [94mLoss[0m : 2.50371
[1mStep[0m  [15/53], [94mLoss[0m : 2.56440
[1mStep[0m  [20/53], [94mLoss[0m : 2.39399
[1mStep[0m  [25/53], [94mLoss[0m : 2.71844
[1mStep[0m  [30/53], [94mLoss[0m : 2.38331
[1mStep[0m  [35/53], [94mLoss[0m : 2.19956
[1mStep[0m  [40/53], [94mLoss[0m : 2.44102
[1mStep[0m  [45/53], [94mLoss[0m : 2.49857
[1mStep[0m  [50/53], [94mLoss[0m : 2.34909

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43858
[1mStep[0m  [5/53], [94mLoss[0m : 2.39111
[1mStep[0m  [10/53], [94mLoss[0m : 2.45434
[1mStep[0m  [15/53], [94mLoss[0m : 2.40542
[1mStep[0m  [20/53], [94mLoss[0m : 2.30750
[1mStep[0m  [25/53], [94mLoss[0m : 2.45117
[1mStep[0m  [30/53], [94mLoss[0m : 2.45465
[1mStep[0m  [35/53], [94mLoss[0m : 2.50613
[1mStep[0m  [40/53], [94mLoss[0m : 2.27839
[1mStep[0m  [45/53], [94mLoss[0m : 2.38118
[1mStep[0m  [50/53], [94mLoss[0m : 2.39449

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38430
[1mStep[0m  [5/53], [94mLoss[0m : 2.59039
[1mStep[0m  [10/53], [94mLoss[0m : 2.52416
[1mStep[0m  [15/53], [94mLoss[0m : 2.55896
[1mStep[0m  [20/53], [94mLoss[0m : 2.36677
[1mStep[0m  [25/53], [94mLoss[0m : 2.54321
[1mStep[0m  [30/53], [94mLoss[0m : 2.42082
[1mStep[0m  [35/53], [94mLoss[0m : 2.41714
[1mStep[0m  [40/53], [94mLoss[0m : 2.33722
[1mStep[0m  [45/53], [94mLoss[0m : 2.23154
[1mStep[0m  [50/53], [94mLoss[0m : 2.51078

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45303
[1mStep[0m  [5/53], [94mLoss[0m : 2.64427
[1mStep[0m  [10/53], [94mLoss[0m : 2.38041
[1mStep[0m  [15/53], [94mLoss[0m : 2.52735
[1mStep[0m  [20/53], [94mLoss[0m : 2.43219
[1mStep[0m  [25/53], [94mLoss[0m : 2.57135
[1mStep[0m  [30/53], [94mLoss[0m : 2.33145
[1mStep[0m  [35/53], [94mLoss[0m : 2.47882
[1mStep[0m  [40/53], [94mLoss[0m : 2.42919
[1mStep[0m  [45/53], [94mLoss[0m : 2.32131
[1mStep[0m  [50/53], [94mLoss[0m : 2.44019

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.434, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68425
[1mStep[0m  [5/53], [94mLoss[0m : 2.36846
[1mStep[0m  [10/53], [94mLoss[0m : 2.46504
[1mStep[0m  [15/53], [94mLoss[0m : 2.25784
[1mStep[0m  [20/53], [94mLoss[0m : 2.49904
[1mStep[0m  [25/53], [94mLoss[0m : 2.52595
[1mStep[0m  [30/53], [94mLoss[0m : 2.37477
[1mStep[0m  [35/53], [94mLoss[0m : 2.40950
[1mStep[0m  [40/53], [94mLoss[0m : 2.52304
[1mStep[0m  [45/53], [94mLoss[0m : 2.28077
[1mStep[0m  [50/53], [94mLoss[0m : 2.61742

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48756
[1mStep[0m  [5/53], [94mLoss[0m : 2.39878
[1mStep[0m  [10/53], [94mLoss[0m : 2.41762
[1mStep[0m  [15/53], [94mLoss[0m : 2.45338
[1mStep[0m  [20/53], [94mLoss[0m : 2.28040
[1mStep[0m  [25/53], [94mLoss[0m : 2.37213
[1mStep[0m  [30/53], [94mLoss[0m : 2.25160
[1mStep[0m  [35/53], [94mLoss[0m : 2.44109
[1mStep[0m  [40/53], [94mLoss[0m : 2.58204
[1mStep[0m  [45/53], [94mLoss[0m : 2.38209
[1mStep[0m  [50/53], [94mLoss[0m : 2.55922

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48625
[1mStep[0m  [5/53], [94mLoss[0m : 2.41866
[1mStep[0m  [10/53], [94mLoss[0m : 2.40845
[1mStep[0m  [15/53], [94mLoss[0m : 2.65214
[1mStep[0m  [20/53], [94mLoss[0m : 2.34586
[1mStep[0m  [25/53], [94mLoss[0m : 2.28742
[1mStep[0m  [30/53], [94mLoss[0m : 2.52775
[1mStep[0m  [35/53], [94mLoss[0m : 2.31500
[1mStep[0m  [40/53], [94mLoss[0m : 2.37253
[1mStep[0m  [45/53], [94mLoss[0m : 2.47730
[1mStep[0m  [50/53], [94mLoss[0m : 2.38350

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39809
[1mStep[0m  [5/53], [94mLoss[0m : 2.51368
[1mStep[0m  [10/53], [94mLoss[0m : 2.50061
[1mStep[0m  [15/53], [94mLoss[0m : 2.49862
[1mStep[0m  [20/53], [94mLoss[0m : 2.34147
[1mStep[0m  [25/53], [94mLoss[0m : 2.49436
[1mStep[0m  [30/53], [94mLoss[0m : 2.67110
[1mStep[0m  [35/53], [94mLoss[0m : 2.42964
[1mStep[0m  [40/53], [94mLoss[0m : 2.35524
[1mStep[0m  [45/53], [94mLoss[0m : 2.23960
[1mStep[0m  [50/53], [94mLoss[0m : 2.59031

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34137
[1mStep[0m  [5/53], [94mLoss[0m : 2.44268
[1mStep[0m  [10/53], [94mLoss[0m : 2.37890
[1mStep[0m  [15/53], [94mLoss[0m : 2.47808
[1mStep[0m  [20/53], [94mLoss[0m : 2.41629
[1mStep[0m  [25/53], [94mLoss[0m : 2.52214
[1mStep[0m  [30/53], [94mLoss[0m : 2.41738
[1mStep[0m  [35/53], [94mLoss[0m : 2.55056
[1mStep[0m  [40/53], [94mLoss[0m : 2.21307
[1mStep[0m  [45/53], [94mLoss[0m : 2.47886
[1mStep[0m  [50/53], [94mLoss[0m : 2.38387

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46063
[1mStep[0m  [5/53], [94mLoss[0m : 2.32918
[1mStep[0m  [10/53], [94mLoss[0m : 2.19725
[1mStep[0m  [15/53], [94mLoss[0m : 2.45378
[1mStep[0m  [20/53], [94mLoss[0m : 2.45476
[1mStep[0m  [25/53], [94mLoss[0m : 2.31726
[1mStep[0m  [30/53], [94mLoss[0m : 2.50340
[1mStep[0m  [35/53], [94mLoss[0m : 2.07667
[1mStep[0m  [40/53], [94mLoss[0m : 2.38698
[1mStep[0m  [45/53], [94mLoss[0m : 2.59509
[1mStep[0m  [50/53], [94mLoss[0m : 2.26233

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45923
[1mStep[0m  [5/53], [94mLoss[0m : 2.49680
[1mStep[0m  [10/53], [94mLoss[0m : 2.57981
[1mStep[0m  [15/53], [94mLoss[0m : 2.37215
[1mStep[0m  [20/53], [94mLoss[0m : 2.51893
[1mStep[0m  [25/53], [94mLoss[0m : 2.31216
[1mStep[0m  [30/53], [94mLoss[0m : 2.44017
[1mStep[0m  [35/53], [94mLoss[0m : 2.33316
[1mStep[0m  [40/53], [94mLoss[0m : 2.26054
[1mStep[0m  [45/53], [94mLoss[0m : 2.50448
[1mStep[0m  [50/53], [94mLoss[0m : 2.28908

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.418, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.429
====================================

Phase 1 - Evaluation MAE:  2.4285195882503805
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.60237
[1mStep[0m  [5/53], [94mLoss[0m : 2.40011
[1mStep[0m  [10/53], [94mLoss[0m : 2.59132
[1mStep[0m  [15/53], [94mLoss[0m : 2.51516
[1mStep[0m  [20/53], [94mLoss[0m : 2.48264
[1mStep[0m  [25/53], [94mLoss[0m : 2.40478
[1mStep[0m  [30/53], [94mLoss[0m : 2.30859
[1mStep[0m  [35/53], [94mLoss[0m : 2.49984
[1mStep[0m  [40/53], [94mLoss[0m : 2.33341
[1mStep[0m  [45/53], [94mLoss[0m : 2.49558
[1mStep[0m  [50/53], [94mLoss[0m : 2.33512

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19703
[1mStep[0m  [5/53], [94mLoss[0m : 2.75230
[1mStep[0m  [10/53], [94mLoss[0m : 2.44392
[1mStep[0m  [15/53], [94mLoss[0m : 2.38773
[1mStep[0m  [20/53], [94mLoss[0m : 2.33026
[1mStep[0m  [25/53], [94mLoss[0m : 2.31757
[1mStep[0m  [30/53], [94mLoss[0m : 2.66511
[1mStep[0m  [35/53], [94mLoss[0m : 2.30876
[1mStep[0m  [40/53], [94mLoss[0m : 2.38423
[1mStep[0m  [45/53], [94mLoss[0m : 2.37487
[1mStep[0m  [50/53], [94mLoss[0m : 2.61679

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21950
[1mStep[0m  [5/53], [94mLoss[0m : 2.38273
[1mStep[0m  [10/53], [94mLoss[0m : 2.34778
[1mStep[0m  [15/53], [94mLoss[0m : 2.35510
[1mStep[0m  [20/53], [94mLoss[0m : 2.41360
[1mStep[0m  [25/53], [94mLoss[0m : 2.43599
[1mStep[0m  [30/53], [94mLoss[0m : 2.35801
[1mStep[0m  [35/53], [94mLoss[0m : 2.44857
[1mStep[0m  [40/53], [94mLoss[0m : 2.41971
[1mStep[0m  [45/53], [94mLoss[0m : 2.28493
[1mStep[0m  [50/53], [94mLoss[0m : 2.50937

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46449
[1mStep[0m  [5/53], [94mLoss[0m : 2.35847
[1mStep[0m  [10/53], [94mLoss[0m : 2.53354
[1mStep[0m  [15/53], [94mLoss[0m : 2.48739
[1mStep[0m  [20/53], [94mLoss[0m : 2.31255
[1mStep[0m  [25/53], [94mLoss[0m : 2.39904
[1mStep[0m  [30/53], [94mLoss[0m : 2.31332
[1mStep[0m  [35/53], [94mLoss[0m : 2.32781
[1mStep[0m  [40/53], [94mLoss[0m : 2.50523
[1mStep[0m  [45/53], [94mLoss[0m : 2.40445
[1mStep[0m  [50/53], [94mLoss[0m : 2.31362

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30218
[1mStep[0m  [5/53], [94mLoss[0m : 2.44532
[1mStep[0m  [10/53], [94mLoss[0m : 2.45871
[1mStep[0m  [15/53], [94mLoss[0m : 2.39662
[1mStep[0m  [20/53], [94mLoss[0m : 2.36034
[1mStep[0m  [25/53], [94mLoss[0m : 2.20681
[1mStep[0m  [30/53], [94mLoss[0m : 2.35967
[1mStep[0m  [35/53], [94mLoss[0m : 2.41923
[1mStep[0m  [40/53], [94mLoss[0m : 2.47773
[1mStep[0m  [45/53], [94mLoss[0m : 2.26153
[1mStep[0m  [50/53], [94mLoss[0m : 2.47007

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36540
[1mStep[0m  [5/53], [94mLoss[0m : 2.37568
[1mStep[0m  [10/53], [94mLoss[0m : 2.42399
[1mStep[0m  [15/53], [94mLoss[0m : 2.42520
[1mStep[0m  [20/53], [94mLoss[0m : 2.28570
[1mStep[0m  [25/53], [94mLoss[0m : 2.58104
[1mStep[0m  [30/53], [94mLoss[0m : 2.40599
[1mStep[0m  [35/53], [94mLoss[0m : 2.30489
[1mStep[0m  [40/53], [94mLoss[0m : 2.57656
[1mStep[0m  [45/53], [94mLoss[0m : 2.30991
[1mStep[0m  [50/53], [94mLoss[0m : 2.53865

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.589, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36325
[1mStep[0m  [5/53], [94mLoss[0m : 2.24207
[1mStep[0m  [10/53], [94mLoss[0m : 2.30203
[1mStep[0m  [15/53], [94mLoss[0m : 2.31400
[1mStep[0m  [20/53], [94mLoss[0m : 2.35961
[1mStep[0m  [25/53], [94mLoss[0m : 2.14551
[1mStep[0m  [30/53], [94mLoss[0m : 2.38238
[1mStep[0m  [35/53], [94mLoss[0m : 2.12829
[1mStep[0m  [40/53], [94mLoss[0m : 2.48533
[1mStep[0m  [45/53], [94mLoss[0m : 2.09946
[1mStep[0m  [50/53], [94mLoss[0m : 2.49849

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28882
[1mStep[0m  [5/53], [94mLoss[0m : 2.60957
[1mStep[0m  [10/53], [94mLoss[0m : 2.20782
[1mStep[0m  [15/53], [94mLoss[0m : 2.30813
[1mStep[0m  [20/53], [94mLoss[0m : 2.38217
[1mStep[0m  [25/53], [94mLoss[0m : 2.43628
[1mStep[0m  [30/53], [94mLoss[0m : 2.34437
[1mStep[0m  [35/53], [94mLoss[0m : 2.35079
[1mStep[0m  [40/53], [94mLoss[0m : 2.47455
[1mStep[0m  [45/53], [94mLoss[0m : 2.34724
[1mStep[0m  [50/53], [94mLoss[0m : 2.43260

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.658, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28161
[1mStep[0m  [5/53], [94mLoss[0m : 2.14390
[1mStep[0m  [10/53], [94mLoss[0m : 2.36334
[1mStep[0m  [15/53], [94mLoss[0m : 2.18775
[1mStep[0m  [20/53], [94mLoss[0m : 2.34608
[1mStep[0m  [25/53], [94mLoss[0m : 2.21226
[1mStep[0m  [30/53], [94mLoss[0m : 2.32443
[1mStep[0m  [35/53], [94mLoss[0m : 2.15350
[1mStep[0m  [40/53], [94mLoss[0m : 2.37574
[1mStep[0m  [45/53], [94mLoss[0m : 2.43834
[1mStep[0m  [50/53], [94mLoss[0m : 2.30559

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.613, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.13990
[1mStep[0m  [5/53], [94mLoss[0m : 2.46837
[1mStep[0m  [10/53], [94mLoss[0m : 2.33935
[1mStep[0m  [15/53], [94mLoss[0m : 2.27923
[1mStep[0m  [20/53], [94mLoss[0m : 2.28391
[1mStep[0m  [25/53], [94mLoss[0m : 2.25463
[1mStep[0m  [30/53], [94mLoss[0m : 2.18454
[1mStep[0m  [35/53], [94mLoss[0m : 2.32345
[1mStep[0m  [40/53], [94mLoss[0m : 2.37419
[1mStep[0m  [45/53], [94mLoss[0m : 2.12880
[1mStep[0m  [50/53], [94mLoss[0m : 2.48787

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.601, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39873
[1mStep[0m  [5/53], [94mLoss[0m : 2.48935
[1mStep[0m  [10/53], [94mLoss[0m : 2.11710
[1mStep[0m  [15/53], [94mLoss[0m : 2.47003
[1mStep[0m  [20/53], [94mLoss[0m : 2.21576
[1mStep[0m  [25/53], [94mLoss[0m : 2.39328
[1mStep[0m  [30/53], [94mLoss[0m : 2.39697
[1mStep[0m  [35/53], [94mLoss[0m : 2.29916
[1mStep[0m  [40/53], [94mLoss[0m : 2.31052
[1mStep[0m  [45/53], [94mLoss[0m : 2.31000
[1mStep[0m  [50/53], [94mLoss[0m : 2.03285

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.648, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27149
[1mStep[0m  [5/53], [94mLoss[0m : 2.52590
[1mStep[0m  [10/53], [94mLoss[0m : 2.32705
[1mStep[0m  [15/53], [94mLoss[0m : 2.11869
[1mStep[0m  [20/53], [94mLoss[0m : 2.22768
[1mStep[0m  [25/53], [94mLoss[0m : 2.22207
[1mStep[0m  [30/53], [94mLoss[0m : 2.36511
[1mStep[0m  [35/53], [94mLoss[0m : 2.22656
[1mStep[0m  [40/53], [94mLoss[0m : 2.31338
[1mStep[0m  [45/53], [94mLoss[0m : 2.28136
[1mStep[0m  [50/53], [94mLoss[0m : 2.17654

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.632, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17376
[1mStep[0m  [5/53], [94mLoss[0m : 2.33697
[1mStep[0m  [10/53], [94mLoss[0m : 2.39067
[1mStep[0m  [15/53], [94mLoss[0m : 2.36633
[1mStep[0m  [20/53], [94mLoss[0m : 2.25862
[1mStep[0m  [25/53], [94mLoss[0m : 2.08791
[1mStep[0m  [30/53], [94mLoss[0m : 2.19686
[1mStep[0m  [35/53], [94mLoss[0m : 2.29936
[1mStep[0m  [40/53], [94mLoss[0m : 2.06965
[1mStep[0m  [45/53], [94mLoss[0m : 2.41115
[1mStep[0m  [50/53], [94mLoss[0m : 2.18030

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.612, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.16375
[1mStep[0m  [5/53], [94mLoss[0m : 2.27116
[1mStep[0m  [10/53], [94mLoss[0m : 2.14831
[1mStep[0m  [15/53], [94mLoss[0m : 2.11693
[1mStep[0m  [20/53], [94mLoss[0m : 2.14837
[1mStep[0m  [25/53], [94mLoss[0m : 2.23259
[1mStep[0m  [30/53], [94mLoss[0m : 2.14675
[1mStep[0m  [35/53], [94mLoss[0m : 2.30795
[1mStep[0m  [40/53], [94mLoss[0m : 2.31725
[1mStep[0m  [45/53], [94mLoss[0m : 2.19796
[1mStep[0m  [50/53], [94mLoss[0m : 2.31318

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.583, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19245
[1mStep[0m  [5/53], [94mLoss[0m : 1.93912
[1mStep[0m  [10/53], [94mLoss[0m : 2.05241
[1mStep[0m  [15/53], [94mLoss[0m : 2.20707
[1mStep[0m  [20/53], [94mLoss[0m : 2.04046
[1mStep[0m  [25/53], [94mLoss[0m : 2.41398
[1mStep[0m  [30/53], [94mLoss[0m : 2.10527
[1mStep[0m  [35/53], [94mLoss[0m : 2.24998
[1mStep[0m  [40/53], [94mLoss[0m : 2.15792
[1mStep[0m  [45/53], [94mLoss[0m : 2.28489
[1mStep[0m  [50/53], [94mLoss[0m : 2.10208

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.184, [92mTest[0m: 2.637, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18989
[1mStep[0m  [5/53], [94mLoss[0m : 2.17876
[1mStep[0m  [10/53], [94mLoss[0m : 1.94264
[1mStep[0m  [15/53], [94mLoss[0m : 2.34901
[1mStep[0m  [20/53], [94mLoss[0m : 2.06692
[1mStep[0m  [25/53], [94mLoss[0m : 2.20552
[1mStep[0m  [30/53], [94mLoss[0m : 1.99539
[1mStep[0m  [35/53], [94mLoss[0m : 2.24861
[1mStep[0m  [40/53], [94mLoss[0m : 2.28852
[1mStep[0m  [45/53], [94mLoss[0m : 2.01806
[1mStep[0m  [50/53], [94mLoss[0m : 2.27159

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20463
[1mStep[0m  [5/53], [94mLoss[0m : 2.11999
[1mStep[0m  [10/53], [94mLoss[0m : 2.10405
[1mStep[0m  [15/53], [94mLoss[0m : 2.21902
[1mStep[0m  [20/53], [94mLoss[0m : 2.17833
[1mStep[0m  [25/53], [94mLoss[0m : 2.18269
[1mStep[0m  [30/53], [94mLoss[0m : 2.21496
[1mStep[0m  [35/53], [94mLoss[0m : 2.26326
[1mStep[0m  [40/53], [94mLoss[0m : 2.02894
[1mStep[0m  [45/53], [94mLoss[0m : 2.02793
[1mStep[0m  [50/53], [94mLoss[0m : 2.36878

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.558, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11295
[1mStep[0m  [5/53], [94mLoss[0m : 2.07808
[1mStep[0m  [10/53], [94mLoss[0m : 2.11545
[1mStep[0m  [15/53], [94mLoss[0m : 2.13933
[1mStep[0m  [20/53], [94mLoss[0m : 2.20947
[1mStep[0m  [25/53], [94mLoss[0m : 1.88267
[1mStep[0m  [30/53], [94mLoss[0m : 2.03423
[1mStep[0m  [35/53], [94mLoss[0m : 2.05802
[1mStep[0m  [40/53], [94mLoss[0m : 2.00591
[1mStep[0m  [45/53], [94mLoss[0m : 2.21027
[1mStep[0m  [50/53], [94mLoss[0m : 2.26170

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.541, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19247
[1mStep[0m  [5/53], [94mLoss[0m : 2.18696
[1mStep[0m  [10/53], [94mLoss[0m : 2.06487
[1mStep[0m  [15/53], [94mLoss[0m : 2.04967
[1mStep[0m  [20/53], [94mLoss[0m : 1.84038
[1mStep[0m  [25/53], [94mLoss[0m : 1.98919
[1mStep[0m  [30/53], [94mLoss[0m : 2.24127
[1mStep[0m  [35/53], [94mLoss[0m : 2.09572
[1mStep[0m  [40/53], [94mLoss[0m : 2.20204
[1mStep[0m  [45/53], [94mLoss[0m : 2.16985
[1mStep[0m  [50/53], [94mLoss[0m : 2.13940

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.04674
[1mStep[0m  [5/53], [94mLoss[0m : 1.86918
[1mStep[0m  [10/53], [94mLoss[0m : 1.92113
[1mStep[0m  [15/53], [94mLoss[0m : 2.13674
[1mStep[0m  [20/53], [94mLoss[0m : 2.09204
[1mStep[0m  [25/53], [94mLoss[0m : 1.94808
[1mStep[0m  [30/53], [94mLoss[0m : 2.10615
[1mStep[0m  [35/53], [94mLoss[0m : 2.10480
[1mStep[0m  [40/53], [94mLoss[0m : 1.92384
[1mStep[0m  [45/53], [94mLoss[0m : 2.15404
[1mStep[0m  [50/53], [94mLoss[0m : 2.09139

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.590, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19049
[1mStep[0m  [5/53], [94mLoss[0m : 1.97064
[1mStep[0m  [10/53], [94mLoss[0m : 2.17307
[1mStep[0m  [15/53], [94mLoss[0m : 1.85228
[1mStep[0m  [20/53], [94mLoss[0m : 2.03958
[1mStep[0m  [25/53], [94mLoss[0m : 2.09538
[1mStep[0m  [30/53], [94mLoss[0m : 1.92259
[1mStep[0m  [35/53], [94mLoss[0m : 2.19285
[1mStep[0m  [40/53], [94mLoss[0m : 2.03039
[1mStep[0m  [45/53], [94mLoss[0m : 1.93943
[1mStep[0m  [50/53], [94mLoss[0m : 2.16388

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90464
[1mStep[0m  [5/53], [94mLoss[0m : 2.07967
[1mStep[0m  [10/53], [94mLoss[0m : 2.20817
[1mStep[0m  [15/53], [94mLoss[0m : 2.10577
[1mStep[0m  [20/53], [94mLoss[0m : 1.89438
[1mStep[0m  [25/53], [94mLoss[0m : 2.00373
[1mStep[0m  [30/53], [94mLoss[0m : 1.97599
[1mStep[0m  [35/53], [94mLoss[0m : 2.00706
[1mStep[0m  [40/53], [94mLoss[0m : 1.90373
[1mStep[0m  [45/53], [94mLoss[0m : 2.00447
[1mStep[0m  [50/53], [94mLoss[0m : 2.03992

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79784
[1mStep[0m  [5/53], [94mLoss[0m : 1.86366
[1mStep[0m  [10/53], [94mLoss[0m : 1.75017
[1mStep[0m  [15/53], [94mLoss[0m : 1.85866
[1mStep[0m  [20/53], [94mLoss[0m : 2.01570
[1mStep[0m  [25/53], [94mLoss[0m : 1.89993
[1mStep[0m  [30/53], [94mLoss[0m : 1.82175
[1mStep[0m  [35/53], [94mLoss[0m : 2.03538
[1mStep[0m  [40/53], [94mLoss[0m : 2.03985
[1mStep[0m  [45/53], [94mLoss[0m : 1.84520
[1mStep[0m  [50/53], [94mLoss[0m : 1.97010

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.973, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.01729
[1mStep[0m  [5/53], [94mLoss[0m : 1.98336
[1mStep[0m  [10/53], [94mLoss[0m : 2.10110
[1mStep[0m  [15/53], [94mLoss[0m : 1.81055
[1mStep[0m  [20/53], [94mLoss[0m : 1.95882
[1mStep[0m  [25/53], [94mLoss[0m : 2.01328
[1mStep[0m  [30/53], [94mLoss[0m : 1.87743
[1mStep[0m  [35/53], [94mLoss[0m : 2.07079
[1mStep[0m  [40/53], [94mLoss[0m : 1.91600
[1mStep[0m  [45/53], [94mLoss[0m : 1.95497
[1mStep[0m  [50/53], [94mLoss[0m : 1.93530

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.87848
[1mStep[0m  [5/53], [94mLoss[0m : 2.03187
[1mStep[0m  [10/53], [94mLoss[0m : 1.93368
[1mStep[0m  [15/53], [94mLoss[0m : 1.91677
[1mStep[0m  [20/53], [94mLoss[0m : 1.98930
[1mStep[0m  [25/53], [94mLoss[0m : 1.79905
[1mStep[0m  [30/53], [94mLoss[0m : 1.88730
[1mStep[0m  [35/53], [94mLoss[0m : 1.99567
[1mStep[0m  [40/53], [94mLoss[0m : 1.81022
[1mStep[0m  [45/53], [94mLoss[0m : 1.89075
[1mStep[0m  [50/53], [94mLoss[0m : 1.94144

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.924, [92mTest[0m: 2.440, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.03307
[1mStep[0m  [5/53], [94mLoss[0m : 1.94863
[1mStep[0m  [10/53], [94mLoss[0m : 1.72632
[1mStep[0m  [15/53], [94mLoss[0m : 2.02920
[1mStep[0m  [20/53], [94mLoss[0m : 1.81416
[1mStep[0m  [25/53], [94mLoss[0m : 1.91617
[1mStep[0m  [30/53], [94mLoss[0m : 1.75285
[1mStep[0m  [35/53], [94mLoss[0m : 1.99558
[1mStep[0m  [40/53], [94mLoss[0m : 1.83953
[1mStep[0m  [45/53], [94mLoss[0m : 1.91615
[1mStep[0m  [50/53], [94mLoss[0m : 1.86055

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.891, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.81340
[1mStep[0m  [5/53], [94mLoss[0m : 2.08021
[1mStep[0m  [10/53], [94mLoss[0m : 1.99765
[1mStep[0m  [15/53], [94mLoss[0m : 1.59111
[1mStep[0m  [20/53], [94mLoss[0m : 1.92137
[1mStep[0m  [25/53], [94mLoss[0m : 1.91195
[1mStep[0m  [30/53], [94mLoss[0m : 1.91089
[1mStep[0m  [35/53], [94mLoss[0m : 1.98451
[1mStep[0m  [40/53], [94mLoss[0m : 1.67002
[1mStep[0m  [45/53], [94mLoss[0m : 1.79956
[1mStep[0m  [50/53], [94mLoss[0m : 1.87252

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73041
[1mStep[0m  [5/53], [94mLoss[0m : 1.90542
[1mStep[0m  [10/53], [94mLoss[0m : 1.87107
[1mStep[0m  [15/53], [94mLoss[0m : 1.89093
[1mStep[0m  [20/53], [94mLoss[0m : 1.82048
[1mStep[0m  [25/53], [94mLoss[0m : 1.65049
[1mStep[0m  [30/53], [94mLoss[0m : 1.60491
[1mStep[0m  [35/53], [94mLoss[0m : 1.75097
[1mStep[0m  [40/53], [94mLoss[0m : 1.95214
[1mStep[0m  [45/53], [94mLoss[0m : 1.94529
[1mStep[0m  [50/53], [94mLoss[0m : 1.91185

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78783
[1mStep[0m  [5/53], [94mLoss[0m : 2.00201
[1mStep[0m  [10/53], [94mLoss[0m : 1.80902
[1mStep[0m  [15/53], [94mLoss[0m : 1.92089
[1mStep[0m  [20/53], [94mLoss[0m : 1.60285
[1mStep[0m  [25/53], [94mLoss[0m : 1.63163
[1mStep[0m  [30/53], [94mLoss[0m : 1.78664
[1mStep[0m  [35/53], [94mLoss[0m : 1.85459
[1mStep[0m  [40/53], [94mLoss[0m : 1.78313
[1mStep[0m  [45/53], [94mLoss[0m : 1.71231
[1mStep[0m  [50/53], [94mLoss[0m : 1.91042

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.543, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64445
[1mStep[0m  [5/53], [94mLoss[0m : 1.65976
[1mStep[0m  [10/53], [94mLoss[0m : 2.04040
[1mStep[0m  [15/53], [94mLoss[0m : 1.90428
[1mStep[0m  [20/53], [94mLoss[0m : 1.77175
[1mStep[0m  [25/53], [94mLoss[0m : 1.63034
[1mStep[0m  [30/53], [94mLoss[0m : 1.84203
[1mStep[0m  [35/53], [94mLoss[0m : 1.72068
[1mStep[0m  [40/53], [94mLoss[0m : 1.76425
[1mStep[0m  [45/53], [94mLoss[0m : 1.84799
[1mStep[0m  [50/53], [94mLoss[0m : 1.64795

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.799, [92mTest[0m: 2.463, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.450
====================================

Phase 2 - Evaluation MAE:  2.4500131056858945
MAE score P1       2.42852
MAE score P2      2.450013
loss               1.79933
learning_rate      0.00505
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.98944
[1mStep[0m  [5/53], [94mLoss[0m : 11.13317
[1mStep[0m  [10/53], [94mLoss[0m : 10.27948
[1mStep[0m  [15/53], [94mLoss[0m : 9.76861
[1mStep[0m  [20/53], [94mLoss[0m : 9.08616
[1mStep[0m  [25/53], [94mLoss[0m : 8.59521
[1mStep[0m  [30/53], [94mLoss[0m : 8.25248
[1mStep[0m  [35/53], [94mLoss[0m : 8.01954
[1mStep[0m  [40/53], [94mLoss[0m : 7.82733
[1mStep[0m  [45/53], [94mLoss[0m : 7.29921
[1mStep[0m  [50/53], [94mLoss[0m : 6.93594

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.873, [92mTest[0m: 10.965, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.31233
[1mStep[0m  [5/53], [94mLoss[0m : 5.67649
[1mStep[0m  [10/53], [94mLoss[0m : 5.03418
[1mStep[0m  [15/53], [94mLoss[0m : 5.22412
[1mStep[0m  [20/53], [94mLoss[0m : 4.38143
[1mStep[0m  [25/53], [94mLoss[0m : 4.37008
[1mStep[0m  [30/53], [94mLoss[0m : 3.66853
[1mStep[0m  [35/53], [94mLoss[0m : 3.64778
[1mStep[0m  [40/53], [94mLoss[0m : 3.23831
[1mStep[0m  [45/53], [94mLoss[0m : 3.01960
[1mStep[0m  [50/53], [94mLoss[0m : 3.21731

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.329, [92mTest[0m: 7.593, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.24693
[1mStep[0m  [5/53], [94mLoss[0m : 2.62786
[1mStep[0m  [10/53], [94mLoss[0m : 3.08018
[1mStep[0m  [15/53], [94mLoss[0m : 2.73299
[1mStep[0m  [20/53], [94mLoss[0m : 3.02204
[1mStep[0m  [25/53], [94mLoss[0m : 2.88493
[1mStep[0m  [30/53], [94mLoss[0m : 2.42125
[1mStep[0m  [35/53], [94mLoss[0m : 2.76416
[1mStep[0m  [40/53], [94mLoss[0m : 2.78378
[1mStep[0m  [45/53], [94mLoss[0m : 2.52617
[1mStep[0m  [50/53], [94mLoss[0m : 2.54474

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.819, [92mTest[0m: 3.620, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63613
[1mStep[0m  [5/53], [94mLoss[0m : 2.80627
[1mStep[0m  [10/53], [94mLoss[0m : 2.76879
[1mStep[0m  [15/53], [94mLoss[0m : 2.55091
[1mStep[0m  [20/53], [94mLoss[0m : 2.96999
[1mStep[0m  [25/53], [94mLoss[0m : 2.61802
[1mStep[0m  [30/53], [94mLoss[0m : 2.44505
[1mStep[0m  [35/53], [94mLoss[0m : 2.78142
[1mStep[0m  [40/53], [94mLoss[0m : 2.72719
[1mStep[0m  [45/53], [94mLoss[0m : 2.85195
[1mStep[0m  [50/53], [94mLoss[0m : 2.65332

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.704, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63945
[1mStep[0m  [5/53], [94mLoss[0m : 2.46008
[1mStep[0m  [10/53], [94mLoss[0m : 2.65434
[1mStep[0m  [15/53], [94mLoss[0m : 2.61351
[1mStep[0m  [20/53], [94mLoss[0m : 2.72078
[1mStep[0m  [25/53], [94mLoss[0m : 2.63784
[1mStep[0m  [30/53], [94mLoss[0m : 2.55274
[1mStep[0m  [35/53], [94mLoss[0m : 2.59874
[1mStep[0m  [40/53], [94mLoss[0m : 2.64933
[1mStep[0m  [45/53], [94mLoss[0m : 2.68481
[1mStep[0m  [50/53], [94mLoss[0m : 2.49251

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.529, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.91746
[1mStep[0m  [5/53], [94mLoss[0m : 2.85925
[1mStep[0m  [10/53], [94mLoss[0m : 2.55510
[1mStep[0m  [15/53], [94mLoss[0m : 2.66194
[1mStep[0m  [20/53], [94mLoss[0m : 2.79279
[1mStep[0m  [25/53], [94mLoss[0m : 2.79548
[1mStep[0m  [30/53], [94mLoss[0m : 2.44043
[1mStep[0m  [35/53], [94mLoss[0m : 2.47051
[1mStep[0m  [40/53], [94mLoss[0m : 2.56755
[1mStep[0m  [45/53], [94mLoss[0m : 2.52306
[1mStep[0m  [50/53], [94mLoss[0m : 2.61467

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52642
[1mStep[0m  [5/53], [94mLoss[0m : 2.50555
[1mStep[0m  [10/53], [94mLoss[0m : 2.51714
[1mStep[0m  [15/53], [94mLoss[0m : 2.64634
[1mStep[0m  [20/53], [94mLoss[0m : 2.42072
[1mStep[0m  [25/53], [94mLoss[0m : 2.42247
[1mStep[0m  [30/53], [94mLoss[0m : 2.50817
[1mStep[0m  [35/53], [94mLoss[0m : 2.63421
[1mStep[0m  [40/53], [94mLoss[0m : 2.70723
[1mStep[0m  [45/53], [94mLoss[0m : 2.69100
[1mStep[0m  [50/53], [94mLoss[0m : 2.56649

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53593
[1mStep[0m  [5/53], [94mLoss[0m : 2.63344
[1mStep[0m  [10/53], [94mLoss[0m : 2.54259
[1mStep[0m  [15/53], [94mLoss[0m : 2.69093
[1mStep[0m  [20/53], [94mLoss[0m : 2.56228
[1mStep[0m  [25/53], [94mLoss[0m : 2.67636
[1mStep[0m  [30/53], [94mLoss[0m : 2.64682
[1mStep[0m  [35/53], [94mLoss[0m : 2.72623
[1mStep[0m  [40/53], [94mLoss[0m : 2.51183
[1mStep[0m  [45/53], [94mLoss[0m : 2.77987
[1mStep[0m  [50/53], [94mLoss[0m : 2.54173

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.76832
[1mStep[0m  [5/53], [94mLoss[0m : 2.59088
[1mStep[0m  [10/53], [94mLoss[0m : 2.73821
[1mStep[0m  [15/53], [94mLoss[0m : 2.86834
[1mStep[0m  [20/53], [94mLoss[0m : 2.58902
[1mStep[0m  [25/53], [94mLoss[0m : 2.52570
[1mStep[0m  [30/53], [94mLoss[0m : 2.58912
[1mStep[0m  [35/53], [94mLoss[0m : 2.70887
[1mStep[0m  [40/53], [94mLoss[0m : 2.61060
[1mStep[0m  [45/53], [94mLoss[0m : 2.64511
[1mStep[0m  [50/53], [94mLoss[0m : 2.56782

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55777
[1mStep[0m  [5/53], [94mLoss[0m : 2.47756
[1mStep[0m  [10/53], [94mLoss[0m : 2.53511
[1mStep[0m  [15/53], [94mLoss[0m : 2.50225
[1mStep[0m  [20/53], [94mLoss[0m : 2.51232
[1mStep[0m  [25/53], [94mLoss[0m : 2.32361
[1mStep[0m  [30/53], [94mLoss[0m : 2.53817
[1mStep[0m  [35/53], [94mLoss[0m : 2.68391
[1mStep[0m  [40/53], [94mLoss[0m : 2.49424
[1mStep[0m  [45/53], [94mLoss[0m : 2.62723
[1mStep[0m  [50/53], [94mLoss[0m : 2.80375

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67166
[1mStep[0m  [5/53], [94mLoss[0m : 2.58083
[1mStep[0m  [10/53], [94mLoss[0m : 2.51192
[1mStep[0m  [15/53], [94mLoss[0m : 2.51112
[1mStep[0m  [20/53], [94mLoss[0m : 2.55279
[1mStep[0m  [25/53], [94mLoss[0m : 2.42419
[1mStep[0m  [30/53], [94mLoss[0m : 2.74993
[1mStep[0m  [35/53], [94mLoss[0m : 2.61520
[1mStep[0m  [40/53], [94mLoss[0m : 2.58321
[1mStep[0m  [45/53], [94mLoss[0m : 2.46294
[1mStep[0m  [50/53], [94mLoss[0m : 2.49791

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46654
[1mStep[0m  [5/53], [94mLoss[0m : 2.61508
[1mStep[0m  [10/53], [94mLoss[0m : 2.56589
[1mStep[0m  [15/53], [94mLoss[0m : 2.73965
[1mStep[0m  [20/53], [94mLoss[0m : 2.69686
[1mStep[0m  [25/53], [94mLoss[0m : 2.35954
[1mStep[0m  [30/53], [94mLoss[0m : 2.68199
[1mStep[0m  [35/53], [94mLoss[0m : 2.39101
[1mStep[0m  [40/53], [94mLoss[0m : 2.42266
[1mStep[0m  [45/53], [94mLoss[0m : 2.52590
[1mStep[0m  [50/53], [94mLoss[0m : 2.52173

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49753
[1mStep[0m  [5/53], [94mLoss[0m : 2.31398
[1mStep[0m  [10/53], [94mLoss[0m : 2.67303
[1mStep[0m  [15/53], [94mLoss[0m : 2.29591
[1mStep[0m  [20/53], [94mLoss[0m : 2.46407
[1mStep[0m  [25/53], [94mLoss[0m : 2.50741
[1mStep[0m  [30/53], [94mLoss[0m : 2.78010
[1mStep[0m  [35/53], [94mLoss[0m : 2.57321
[1mStep[0m  [40/53], [94mLoss[0m : 2.50309
[1mStep[0m  [45/53], [94mLoss[0m : 2.52968
[1mStep[0m  [50/53], [94mLoss[0m : 2.57209

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54880
[1mStep[0m  [5/53], [94mLoss[0m : 2.36533
[1mStep[0m  [10/53], [94mLoss[0m : 2.52380
[1mStep[0m  [15/53], [94mLoss[0m : 2.60835
[1mStep[0m  [20/53], [94mLoss[0m : 2.61553
[1mStep[0m  [25/53], [94mLoss[0m : 2.40982
[1mStep[0m  [30/53], [94mLoss[0m : 2.42280
[1mStep[0m  [35/53], [94mLoss[0m : 2.55490
[1mStep[0m  [40/53], [94mLoss[0m : 2.36560
[1mStep[0m  [45/53], [94mLoss[0m : 2.51585
[1mStep[0m  [50/53], [94mLoss[0m : 2.55537

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52789
[1mStep[0m  [5/53], [94mLoss[0m : 2.70851
[1mStep[0m  [10/53], [94mLoss[0m : 2.55496
[1mStep[0m  [15/53], [94mLoss[0m : 2.60038
[1mStep[0m  [20/53], [94mLoss[0m : 2.53204
[1mStep[0m  [25/53], [94mLoss[0m : 2.39106
[1mStep[0m  [30/53], [94mLoss[0m : 2.38873
[1mStep[0m  [35/53], [94mLoss[0m : 2.39665
[1mStep[0m  [40/53], [94mLoss[0m : 2.54110
[1mStep[0m  [45/53], [94mLoss[0m : 2.47102
[1mStep[0m  [50/53], [94mLoss[0m : 2.43856

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49523
[1mStep[0m  [5/53], [94mLoss[0m : 2.37998
[1mStep[0m  [10/53], [94mLoss[0m : 2.57056
[1mStep[0m  [15/53], [94mLoss[0m : 2.56629
[1mStep[0m  [20/53], [94mLoss[0m : 2.41957
[1mStep[0m  [25/53], [94mLoss[0m : 2.63610
[1mStep[0m  [30/53], [94mLoss[0m : 2.48489
[1mStep[0m  [35/53], [94mLoss[0m : 2.31304
[1mStep[0m  [40/53], [94mLoss[0m : 2.59268
[1mStep[0m  [45/53], [94mLoss[0m : 2.46025
[1mStep[0m  [50/53], [94mLoss[0m : 2.23979

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50611
[1mStep[0m  [5/53], [94mLoss[0m : 2.57764
[1mStep[0m  [10/53], [94mLoss[0m : 2.49898
[1mStep[0m  [15/53], [94mLoss[0m : 2.59277
[1mStep[0m  [20/53], [94mLoss[0m : 2.40428
[1mStep[0m  [25/53], [94mLoss[0m : 2.48138
[1mStep[0m  [30/53], [94mLoss[0m : 2.34999
[1mStep[0m  [35/53], [94mLoss[0m : 2.50090
[1mStep[0m  [40/53], [94mLoss[0m : 2.66954
[1mStep[0m  [45/53], [94mLoss[0m : 2.31732
[1mStep[0m  [50/53], [94mLoss[0m : 2.78168

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31002
[1mStep[0m  [5/53], [94mLoss[0m : 2.54844
[1mStep[0m  [10/53], [94mLoss[0m : 2.46582
[1mStep[0m  [15/53], [94mLoss[0m : 2.45180
[1mStep[0m  [20/53], [94mLoss[0m : 2.49228
[1mStep[0m  [25/53], [94mLoss[0m : 2.43366
[1mStep[0m  [30/53], [94mLoss[0m : 2.64295
[1mStep[0m  [35/53], [94mLoss[0m : 2.41712
[1mStep[0m  [40/53], [94mLoss[0m : 2.63118
[1mStep[0m  [45/53], [94mLoss[0m : 2.54196
[1mStep[0m  [50/53], [94mLoss[0m : 2.50767

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54690
[1mStep[0m  [5/53], [94mLoss[0m : 2.29526
[1mStep[0m  [10/53], [94mLoss[0m : 2.42303
[1mStep[0m  [15/53], [94mLoss[0m : 2.63192
[1mStep[0m  [20/53], [94mLoss[0m : 2.72536
[1mStep[0m  [25/53], [94mLoss[0m : 2.43539
[1mStep[0m  [30/53], [94mLoss[0m : 2.53537
[1mStep[0m  [35/53], [94mLoss[0m : 2.52663
[1mStep[0m  [40/53], [94mLoss[0m : 2.71543
[1mStep[0m  [45/53], [94mLoss[0m : 2.69015
[1mStep[0m  [50/53], [94mLoss[0m : 2.37204

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63521
[1mStep[0m  [5/53], [94mLoss[0m : 2.61467
[1mStep[0m  [10/53], [94mLoss[0m : 2.40973
[1mStep[0m  [15/53], [94mLoss[0m : 2.55693
[1mStep[0m  [20/53], [94mLoss[0m : 2.42508
[1mStep[0m  [25/53], [94mLoss[0m : 2.65957
[1mStep[0m  [30/53], [94mLoss[0m : 2.61617
[1mStep[0m  [35/53], [94mLoss[0m : 2.54397
[1mStep[0m  [40/53], [94mLoss[0m : 2.65441
[1mStep[0m  [45/53], [94mLoss[0m : 2.64534
[1mStep[0m  [50/53], [94mLoss[0m : 2.48253

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46448
[1mStep[0m  [5/53], [94mLoss[0m : 2.27760
[1mStep[0m  [10/53], [94mLoss[0m : 2.51159
[1mStep[0m  [15/53], [94mLoss[0m : 2.20693
[1mStep[0m  [20/53], [94mLoss[0m : 2.51737
[1mStep[0m  [25/53], [94mLoss[0m : 2.40915
[1mStep[0m  [30/53], [94mLoss[0m : 2.33124
[1mStep[0m  [35/53], [94mLoss[0m : 2.79033
[1mStep[0m  [40/53], [94mLoss[0m : 2.50996
[1mStep[0m  [45/53], [94mLoss[0m : 2.59905
[1mStep[0m  [50/53], [94mLoss[0m : 2.59783

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.366, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33625
[1mStep[0m  [5/53], [94mLoss[0m : 2.30558
[1mStep[0m  [10/53], [94mLoss[0m : 2.37337
[1mStep[0m  [15/53], [94mLoss[0m : 2.49233
[1mStep[0m  [20/53], [94mLoss[0m : 2.59972
[1mStep[0m  [25/53], [94mLoss[0m : 2.28255
[1mStep[0m  [30/53], [94mLoss[0m : 2.54248
[1mStep[0m  [35/53], [94mLoss[0m : 2.53467
[1mStep[0m  [40/53], [94mLoss[0m : 2.54485
[1mStep[0m  [45/53], [94mLoss[0m : 2.47700
[1mStep[0m  [50/53], [94mLoss[0m : 2.53126

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.376, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33527
[1mStep[0m  [5/53], [94mLoss[0m : 2.65470
[1mStep[0m  [10/53], [94mLoss[0m : 2.40306
[1mStep[0m  [15/53], [94mLoss[0m : 2.13403
[1mStep[0m  [20/53], [94mLoss[0m : 2.38767
[1mStep[0m  [25/53], [94mLoss[0m : 2.52940
[1mStep[0m  [30/53], [94mLoss[0m : 2.34854
[1mStep[0m  [35/53], [94mLoss[0m : 2.22111
[1mStep[0m  [40/53], [94mLoss[0m : 2.55743
[1mStep[0m  [45/53], [94mLoss[0m : 2.45005
[1mStep[0m  [50/53], [94mLoss[0m : 2.34575

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70159
[1mStep[0m  [5/53], [94mLoss[0m : 2.60260
[1mStep[0m  [10/53], [94mLoss[0m : 2.18116
[1mStep[0m  [15/53], [94mLoss[0m : 2.67285
[1mStep[0m  [20/53], [94mLoss[0m : 2.49056
[1mStep[0m  [25/53], [94mLoss[0m : 2.37854
[1mStep[0m  [30/53], [94mLoss[0m : 2.29361
[1mStep[0m  [35/53], [94mLoss[0m : 2.47746
[1mStep[0m  [40/53], [94mLoss[0m : 2.42479
[1mStep[0m  [45/53], [94mLoss[0m : 2.50628
[1mStep[0m  [50/53], [94mLoss[0m : 2.61592

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38934
[1mStep[0m  [5/53], [94mLoss[0m : 2.43311
[1mStep[0m  [10/53], [94mLoss[0m : 2.29539
[1mStep[0m  [15/53], [94mLoss[0m : 2.43995
[1mStep[0m  [20/53], [94mLoss[0m : 2.50113
[1mStep[0m  [25/53], [94mLoss[0m : 2.46106
[1mStep[0m  [30/53], [94mLoss[0m : 2.51435
[1mStep[0m  [35/53], [94mLoss[0m : 2.38054
[1mStep[0m  [40/53], [94mLoss[0m : 2.69101
[1mStep[0m  [45/53], [94mLoss[0m : 2.46652
[1mStep[0m  [50/53], [94mLoss[0m : 2.44181

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29592
[1mStep[0m  [5/53], [94mLoss[0m : 2.62757
[1mStep[0m  [10/53], [94mLoss[0m : 2.38114
[1mStep[0m  [15/53], [94mLoss[0m : 2.44434
[1mStep[0m  [20/53], [94mLoss[0m : 2.45594
[1mStep[0m  [25/53], [94mLoss[0m : 2.44928
[1mStep[0m  [30/53], [94mLoss[0m : 2.44512
[1mStep[0m  [35/53], [94mLoss[0m : 2.48308
[1mStep[0m  [40/53], [94mLoss[0m : 2.47995
[1mStep[0m  [45/53], [94mLoss[0m : 2.34334
[1mStep[0m  [50/53], [94mLoss[0m : 2.60954

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.357, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37603
[1mStep[0m  [5/53], [94mLoss[0m : 2.46743
[1mStep[0m  [10/53], [94mLoss[0m : 2.43376
[1mStep[0m  [15/53], [94mLoss[0m : 2.45170
[1mStep[0m  [20/53], [94mLoss[0m : 2.38624
[1mStep[0m  [25/53], [94mLoss[0m : 2.59893
[1mStep[0m  [30/53], [94mLoss[0m : 2.37034
[1mStep[0m  [35/53], [94mLoss[0m : 2.42483
[1mStep[0m  [40/53], [94mLoss[0m : 2.57278
[1mStep[0m  [45/53], [94mLoss[0m : 2.60171
[1mStep[0m  [50/53], [94mLoss[0m : 2.31473

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.366, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34744
[1mStep[0m  [5/53], [94mLoss[0m : 2.39465
[1mStep[0m  [10/53], [94mLoss[0m : 2.52818
[1mStep[0m  [15/53], [94mLoss[0m : 2.26559
[1mStep[0m  [20/53], [94mLoss[0m : 2.57228
[1mStep[0m  [25/53], [94mLoss[0m : 2.65873
[1mStep[0m  [30/53], [94mLoss[0m : 2.29320
[1mStep[0m  [35/53], [94mLoss[0m : 2.29433
[1mStep[0m  [40/53], [94mLoss[0m : 2.40006
[1mStep[0m  [45/53], [94mLoss[0m : 2.50874
[1mStep[0m  [50/53], [94mLoss[0m : 2.33788

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53528
[1mStep[0m  [5/53], [94mLoss[0m : 2.42397
[1mStep[0m  [10/53], [94mLoss[0m : 2.59855
[1mStep[0m  [15/53], [94mLoss[0m : 2.38942
[1mStep[0m  [20/53], [94mLoss[0m : 2.50941
[1mStep[0m  [25/53], [94mLoss[0m : 2.50154
[1mStep[0m  [30/53], [94mLoss[0m : 2.32021
[1mStep[0m  [35/53], [94mLoss[0m : 2.28726
[1mStep[0m  [40/53], [94mLoss[0m : 2.44776
[1mStep[0m  [45/53], [94mLoss[0m : 2.44867
[1mStep[0m  [50/53], [94mLoss[0m : 2.42501

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54761
[1mStep[0m  [5/53], [94mLoss[0m : 2.46322
[1mStep[0m  [10/53], [94mLoss[0m : 2.45117
[1mStep[0m  [15/53], [94mLoss[0m : 2.42305
[1mStep[0m  [20/53], [94mLoss[0m : 2.55241
[1mStep[0m  [25/53], [94mLoss[0m : 2.46315
[1mStep[0m  [30/53], [94mLoss[0m : 2.40502
[1mStep[0m  [35/53], [94mLoss[0m : 2.49252
[1mStep[0m  [40/53], [94mLoss[0m : 2.51554
[1mStep[0m  [45/53], [94mLoss[0m : 2.32507
[1mStep[0m  [50/53], [94mLoss[0m : 2.46198

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.338892780817472
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.29785
[1mStep[0m  [5/53], [94mLoss[0m : 2.55756
[1mStep[0m  [10/53], [94mLoss[0m : 2.56618
[1mStep[0m  [15/53], [94mLoss[0m : 2.57269
[1mStep[0m  [20/53], [94mLoss[0m : 2.52427
[1mStep[0m  [25/53], [94mLoss[0m : 2.54201
[1mStep[0m  [30/53], [94mLoss[0m : 2.66247
[1mStep[0m  [35/53], [94mLoss[0m : 2.46818
[1mStep[0m  [40/53], [94mLoss[0m : 2.43864
[1mStep[0m  [45/53], [94mLoss[0m : 2.65403
[1mStep[0m  [50/53], [94mLoss[0m : 2.59767

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49559
[1mStep[0m  [5/53], [94mLoss[0m : 2.57619
[1mStep[0m  [10/53], [94mLoss[0m : 2.75225
[1mStep[0m  [15/53], [94mLoss[0m : 2.31645
[1mStep[0m  [20/53], [94mLoss[0m : 2.51508
[1mStep[0m  [25/53], [94mLoss[0m : 2.68910
[1mStep[0m  [30/53], [94mLoss[0m : 2.49694
[1mStep[0m  [35/53], [94mLoss[0m : 2.58910
[1mStep[0m  [40/53], [94mLoss[0m : 2.28748
[1mStep[0m  [45/53], [94mLoss[0m : 2.45505
[1mStep[0m  [50/53], [94mLoss[0m : 2.48933

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.509, [92mTest[0m: 3.123, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56656
[1mStep[0m  [5/53], [94mLoss[0m : 2.61968
[1mStep[0m  [10/53], [94mLoss[0m : 2.45579
[1mStep[0m  [15/53], [94mLoss[0m : 2.45798
[1mStep[0m  [20/53], [94mLoss[0m : 2.46386
[1mStep[0m  [25/53], [94mLoss[0m : 2.43890
[1mStep[0m  [30/53], [94mLoss[0m : 2.50041
[1mStep[0m  [35/53], [94mLoss[0m : 2.39560
[1mStep[0m  [40/53], [94mLoss[0m : 2.65801
[1mStep[0m  [45/53], [94mLoss[0m : 2.42038
[1mStep[0m  [50/53], [94mLoss[0m : 2.34237

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.596, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67405
[1mStep[0m  [5/53], [94mLoss[0m : 2.45683
[1mStep[0m  [10/53], [94mLoss[0m : 2.26928
[1mStep[0m  [15/53], [94mLoss[0m : 2.47150
[1mStep[0m  [20/53], [94mLoss[0m : 2.31519
[1mStep[0m  [25/53], [94mLoss[0m : 2.44940
[1mStep[0m  [30/53], [94mLoss[0m : 2.32544
[1mStep[0m  [35/53], [94mLoss[0m : 2.62535
[1mStep[0m  [40/53], [94mLoss[0m : 2.68407
[1mStep[0m  [45/53], [94mLoss[0m : 2.45124
[1mStep[0m  [50/53], [94mLoss[0m : 2.47736

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45910
[1mStep[0m  [5/53], [94mLoss[0m : 2.34618
[1mStep[0m  [10/53], [94mLoss[0m : 2.64239
[1mStep[0m  [15/53], [94mLoss[0m : 2.37129
[1mStep[0m  [20/53], [94mLoss[0m : 2.52766
[1mStep[0m  [25/53], [94mLoss[0m : 2.52462
[1mStep[0m  [30/53], [94mLoss[0m : 2.49542
[1mStep[0m  [35/53], [94mLoss[0m : 2.31896
[1mStep[0m  [40/53], [94mLoss[0m : 2.44681
[1mStep[0m  [45/53], [94mLoss[0m : 2.49265
[1mStep[0m  [50/53], [94mLoss[0m : 2.38971

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.563, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46194
[1mStep[0m  [5/53], [94mLoss[0m : 2.41953
[1mStep[0m  [10/53], [94mLoss[0m : 2.41852
[1mStep[0m  [15/53], [94mLoss[0m : 2.30189
[1mStep[0m  [20/53], [94mLoss[0m : 2.42368
[1mStep[0m  [25/53], [94mLoss[0m : 2.40619
[1mStep[0m  [30/53], [94mLoss[0m : 2.31268
[1mStep[0m  [35/53], [94mLoss[0m : 2.26862
[1mStep[0m  [40/53], [94mLoss[0m : 2.45657
[1mStep[0m  [45/53], [94mLoss[0m : 2.45032
[1mStep[0m  [50/53], [94mLoss[0m : 2.43751

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48128
[1mStep[0m  [5/53], [94mLoss[0m : 2.48174
[1mStep[0m  [10/53], [94mLoss[0m : 2.52236
[1mStep[0m  [15/53], [94mLoss[0m : 2.45022
[1mStep[0m  [20/53], [94mLoss[0m : 2.17344
[1mStep[0m  [25/53], [94mLoss[0m : 2.47079
[1mStep[0m  [30/53], [94mLoss[0m : 2.31015
[1mStep[0m  [35/53], [94mLoss[0m : 2.47030
[1mStep[0m  [40/53], [94mLoss[0m : 2.46399
[1mStep[0m  [45/53], [94mLoss[0m : 2.21803
[1mStep[0m  [50/53], [94mLoss[0m : 2.37570

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.419, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51366
[1mStep[0m  [5/53], [94mLoss[0m : 2.41026
[1mStep[0m  [10/53], [94mLoss[0m : 2.38410
[1mStep[0m  [15/53], [94mLoss[0m : 2.22895
[1mStep[0m  [20/53], [94mLoss[0m : 2.37688
[1mStep[0m  [25/53], [94mLoss[0m : 2.38913
[1mStep[0m  [30/53], [94mLoss[0m : 2.17949
[1mStep[0m  [35/53], [94mLoss[0m : 2.12587
[1mStep[0m  [40/53], [94mLoss[0m : 2.25209
[1mStep[0m  [45/53], [94mLoss[0m : 2.25541
[1mStep[0m  [50/53], [94mLoss[0m : 2.18799

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31566
[1mStep[0m  [5/53], [94mLoss[0m : 2.12255
[1mStep[0m  [10/53], [94mLoss[0m : 2.11939
[1mStep[0m  [15/53], [94mLoss[0m : 2.36892
[1mStep[0m  [20/53], [94mLoss[0m : 2.24547
[1mStep[0m  [25/53], [94mLoss[0m : 2.16922
[1mStep[0m  [30/53], [94mLoss[0m : 2.31229
[1mStep[0m  [35/53], [94mLoss[0m : 2.36442
[1mStep[0m  [40/53], [94mLoss[0m : 2.31951
[1mStep[0m  [45/53], [94mLoss[0m : 2.18648
[1mStep[0m  [50/53], [94mLoss[0m : 2.38154

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33458
[1mStep[0m  [5/53], [94mLoss[0m : 2.45642
[1mStep[0m  [10/53], [94mLoss[0m : 2.22514
[1mStep[0m  [15/53], [94mLoss[0m : 2.11852
[1mStep[0m  [20/53], [94mLoss[0m : 2.19198
[1mStep[0m  [25/53], [94mLoss[0m : 2.29032
[1mStep[0m  [30/53], [94mLoss[0m : 2.13927
[1mStep[0m  [35/53], [94mLoss[0m : 2.15587
[1mStep[0m  [40/53], [94mLoss[0m : 2.38611
[1mStep[0m  [45/53], [94mLoss[0m : 2.13085
[1mStep[0m  [50/53], [94mLoss[0m : 2.12892

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24641
[1mStep[0m  [5/53], [94mLoss[0m : 2.17935
[1mStep[0m  [10/53], [94mLoss[0m : 2.12573
[1mStep[0m  [15/53], [94mLoss[0m : 2.29641
[1mStep[0m  [20/53], [94mLoss[0m : 1.99090
[1mStep[0m  [25/53], [94mLoss[0m : 2.27510
[1mStep[0m  [30/53], [94mLoss[0m : 2.01405
[1mStep[0m  [35/53], [94mLoss[0m : 2.13310
[1mStep[0m  [40/53], [94mLoss[0m : 2.11379
[1mStep[0m  [45/53], [94mLoss[0m : 2.26959
[1mStep[0m  [50/53], [94mLoss[0m : 2.19161

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09919
[1mStep[0m  [5/53], [94mLoss[0m : 1.97651
[1mStep[0m  [10/53], [94mLoss[0m : 2.27908
[1mStep[0m  [15/53], [94mLoss[0m : 2.10876
[1mStep[0m  [20/53], [94mLoss[0m : 2.25235
[1mStep[0m  [25/53], [94mLoss[0m : 2.15570
[1mStep[0m  [30/53], [94mLoss[0m : 2.31718
[1mStep[0m  [35/53], [94mLoss[0m : 1.99127
[1mStep[0m  [40/53], [94mLoss[0m : 2.08738
[1mStep[0m  [45/53], [94mLoss[0m : 2.29415
[1mStep[0m  [50/53], [94mLoss[0m : 2.38041

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.97180
[1mStep[0m  [5/53], [94mLoss[0m : 2.00683
[1mStep[0m  [10/53], [94mLoss[0m : 1.76341
[1mStep[0m  [15/53], [94mLoss[0m : 2.14025
[1mStep[0m  [20/53], [94mLoss[0m : 2.20261
[1mStep[0m  [25/53], [94mLoss[0m : 2.33977
[1mStep[0m  [30/53], [94mLoss[0m : 2.03543
[1mStep[0m  [35/53], [94mLoss[0m : 2.16379
[1mStep[0m  [40/53], [94mLoss[0m : 2.26140
[1mStep[0m  [45/53], [94mLoss[0m : 2.06350
[1mStep[0m  [50/53], [94mLoss[0m : 1.98879

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.04106
[1mStep[0m  [5/53], [94mLoss[0m : 2.06401
[1mStep[0m  [10/53], [94mLoss[0m : 2.07800
[1mStep[0m  [15/53], [94mLoss[0m : 2.14853
[1mStep[0m  [20/53], [94mLoss[0m : 2.10335
[1mStep[0m  [25/53], [94mLoss[0m : 2.22287
[1mStep[0m  [30/53], [94mLoss[0m : 2.10893
[1mStep[0m  [35/53], [94mLoss[0m : 2.44675
[1mStep[0m  [40/53], [94mLoss[0m : 2.23658
[1mStep[0m  [45/53], [94mLoss[0m : 2.08534
[1mStep[0m  [50/53], [94mLoss[0m : 2.14701

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21483
[1mStep[0m  [5/53], [94mLoss[0m : 2.16008
[1mStep[0m  [10/53], [94mLoss[0m : 1.91939
[1mStep[0m  [15/53], [94mLoss[0m : 2.10174
[1mStep[0m  [20/53], [94mLoss[0m : 2.10281
[1mStep[0m  [25/53], [94mLoss[0m : 1.96742
[1mStep[0m  [30/53], [94mLoss[0m : 2.28021
[1mStep[0m  [35/53], [94mLoss[0m : 2.01310
[1mStep[0m  [40/53], [94mLoss[0m : 2.08063
[1mStep[0m  [45/53], [94mLoss[0m : 2.15953
[1mStep[0m  [50/53], [94mLoss[0m : 2.09101

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.07237
[1mStep[0m  [5/53], [94mLoss[0m : 2.25186
[1mStep[0m  [10/53], [94mLoss[0m : 1.96296
[1mStep[0m  [15/53], [94mLoss[0m : 2.08207
[1mStep[0m  [20/53], [94mLoss[0m : 1.97112
[1mStep[0m  [25/53], [94mLoss[0m : 2.20099
[1mStep[0m  [30/53], [94mLoss[0m : 1.94190
[1mStep[0m  [35/53], [94mLoss[0m : 2.06121
[1mStep[0m  [40/53], [94mLoss[0m : 1.87526
[1mStep[0m  [45/53], [94mLoss[0m : 2.05285
[1mStep[0m  [50/53], [94mLoss[0m : 2.02191

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.99191
[1mStep[0m  [5/53], [94mLoss[0m : 1.96182
[1mStep[0m  [10/53], [94mLoss[0m : 2.20852
[1mStep[0m  [15/53], [94mLoss[0m : 2.03772
[1mStep[0m  [20/53], [94mLoss[0m : 2.10164
[1mStep[0m  [25/53], [94mLoss[0m : 1.90875
[1mStep[0m  [30/53], [94mLoss[0m : 1.83652
[1mStep[0m  [35/53], [94mLoss[0m : 2.02253
[1mStep[0m  [40/53], [94mLoss[0m : 2.06355
[1mStep[0m  [45/53], [94mLoss[0m : 2.15314
[1mStep[0m  [50/53], [94mLoss[0m : 2.18948

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.89009
[1mStep[0m  [5/53], [94mLoss[0m : 1.99827
[1mStep[0m  [10/53], [94mLoss[0m : 2.11301
[1mStep[0m  [15/53], [94mLoss[0m : 2.06343
[1mStep[0m  [20/53], [94mLoss[0m : 1.88404
[1mStep[0m  [25/53], [94mLoss[0m : 2.00698
[1mStep[0m  [30/53], [94mLoss[0m : 1.85488
[1mStep[0m  [35/53], [94mLoss[0m : 1.95310
[1mStep[0m  [40/53], [94mLoss[0m : 2.08863
[1mStep[0m  [45/53], [94mLoss[0m : 1.93582
[1mStep[0m  [50/53], [94mLoss[0m : 2.13517

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.995, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17668
[1mStep[0m  [5/53], [94mLoss[0m : 2.06875
[1mStep[0m  [10/53], [94mLoss[0m : 1.85369
[1mStep[0m  [15/53], [94mLoss[0m : 2.00399
[1mStep[0m  [20/53], [94mLoss[0m : 1.95255
[1mStep[0m  [25/53], [94mLoss[0m : 1.91579
[1mStep[0m  [30/53], [94mLoss[0m : 1.93254
[1mStep[0m  [35/53], [94mLoss[0m : 2.01148
[1mStep[0m  [40/53], [94mLoss[0m : 2.18940
[1mStep[0m  [45/53], [94mLoss[0m : 2.13154
[1mStep[0m  [50/53], [94mLoss[0m : 1.81089

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79373
[1mStep[0m  [5/53], [94mLoss[0m : 1.93963
[1mStep[0m  [10/53], [94mLoss[0m : 1.92775
[1mStep[0m  [15/53], [94mLoss[0m : 1.91269
[1mStep[0m  [20/53], [94mLoss[0m : 1.83460
[1mStep[0m  [25/53], [94mLoss[0m : 1.89290
[1mStep[0m  [30/53], [94mLoss[0m : 2.01036
[1mStep[0m  [35/53], [94mLoss[0m : 2.16566
[1mStep[0m  [40/53], [94mLoss[0m : 1.90537
[1mStep[0m  [45/53], [94mLoss[0m : 2.07867
[1mStep[0m  [50/53], [94mLoss[0m : 2.02903

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.438, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93343
[1mStep[0m  [5/53], [94mLoss[0m : 1.96995
[1mStep[0m  [10/53], [94mLoss[0m : 1.80027
[1mStep[0m  [15/53], [94mLoss[0m : 2.16269
[1mStep[0m  [20/53], [94mLoss[0m : 1.76275
[1mStep[0m  [25/53], [94mLoss[0m : 1.79874
[1mStep[0m  [30/53], [94mLoss[0m : 1.86043
[1mStep[0m  [35/53], [94mLoss[0m : 1.80467
[1mStep[0m  [40/53], [94mLoss[0m : 1.98201
[1mStep[0m  [45/53], [94mLoss[0m : 1.92212
[1mStep[0m  [50/53], [94mLoss[0m : 1.94058

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78465
[1mStep[0m  [5/53], [94mLoss[0m : 1.94966
[1mStep[0m  [10/53], [94mLoss[0m : 1.85986
[1mStep[0m  [15/53], [94mLoss[0m : 1.89581
[1mStep[0m  [20/53], [94mLoss[0m : 1.60071
[1mStep[0m  [25/53], [94mLoss[0m : 1.80460
[1mStep[0m  [30/53], [94mLoss[0m : 1.80784
[1mStep[0m  [35/53], [94mLoss[0m : 1.96315
[1mStep[0m  [40/53], [94mLoss[0m : 1.82592
[1mStep[0m  [45/53], [94mLoss[0m : 1.88095
[1mStep[0m  [50/53], [94mLoss[0m : 1.92060

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.92922
[1mStep[0m  [5/53], [94mLoss[0m : 1.95179
[1mStep[0m  [10/53], [94mLoss[0m : 1.85111
[1mStep[0m  [15/53], [94mLoss[0m : 1.81629
[1mStep[0m  [20/53], [94mLoss[0m : 1.94035
[1mStep[0m  [25/53], [94mLoss[0m : 1.80599
[1mStep[0m  [30/53], [94mLoss[0m : 2.02666
[1mStep[0m  [35/53], [94mLoss[0m : 1.82505
[1mStep[0m  [40/53], [94mLoss[0m : 1.78416
[1mStep[0m  [45/53], [94mLoss[0m : 1.88771
[1mStep[0m  [50/53], [94mLoss[0m : 1.81385

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.76776
[1mStep[0m  [5/53], [94mLoss[0m : 1.62754
[1mStep[0m  [10/53], [94mLoss[0m : 1.82765
[1mStep[0m  [15/53], [94mLoss[0m : 1.70174
[1mStep[0m  [20/53], [94mLoss[0m : 1.87043
[1mStep[0m  [25/53], [94mLoss[0m : 1.83935
[1mStep[0m  [30/53], [94mLoss[0m : 1.80106
[1mStep[0m  [35/53], [94mLoss[0m : 1.94146
[1mStep[0m  [40/53], [94mLoss[0m : 1.93653
[1mStep[0m  [45/53], [94mLoss[0m : 1.80842
[1mStep[0m  [50/53], [94mLoss[0m : 2.00513

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.450, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68067
[1mStep[0m  [5/53], [94mLoss[0m : 1.84167
[1mStep[0m  [10/53], [94mLoss[0m : 1.68673
[1mStep[0m  [15/53], [94mLoss[0m : 1.87472
[1mStep[0m  [20/53], [94mLoss[0m : 1.83466
[1mStep[0m  [25/53], [94mLoss[0m : 1.81667
[1mStep[0m  [30/53], [94mLoss[0m : 1.59460
[1mStep[0m  [35/53], [94mLoss[0m : 1.74417
[1mStep[0m  [40/53], [94mLoss[0m : 1.88816
[1mStep[0m  [45/53], [94mLoss[0m : 1.98359
[1mStep[0m  [50/53], [94mLoss[0m : 1.73167

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.79069
[1mStep[0m  [5/53], [94mLoss[0m : 1.79999
[1mStep[0m  [10/53], [94mLoss[0m : 1.78701
[1mStep[0m  [15/53], [94mLoss[0m : 1.81147
[1mStep[0m  [20/53], [94mLoss[0m : 1.81312
[1mStep[0m  [25/53], [94mLoss[0m : 1.83630
[1mStep[0m  [30/53], [94mLoss[0m : 1.74623
[1mStep[0m  [35/53], [94mLoss[0m : 1.71605
[1mStep[0m  [40/53], [94mLoss[0m : 1.92164
[1mStep[0m  [45/53], [94mLoss[0m : 1.75432
[1mStep[0m  [50/53], [94mLoss[0m : 1.89810

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78553
[1mStep[0m  [5/53], [94mLoss[0m : 1.76652
[1mStep[0m  [10/53], [94mLoss[0m : 1.64824
[1mStep[0m  [15/53], [94mLoss[0m : 1.75104
[1mStep[0m  [20/53], [94mLoss[0m : 1.74148
[1mStep[0m  [25/53], [94mLoss[0m : 1.89746
[1mStep[0m  [30/53], [94mLoss[0m : 1.70939
[1mStep[0m  [35/53], [94mLoss[0m : 1.92249
[1mStep[0m  [40/53], [94mLoss[0m : 1.72339
[1mStep[0m  [45/53], [94mLoss[0m : 1.76039
[1mStep[0m  [50/53], [94mLoss[0m : 1.61763

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75942
[1mStep[0m  [5/53], [94mLoss[0m : 1.82611
[1mStep[0m  [10/53], [94mLoss[0m : 1.67618
[1mStep[0m  [15/53], [94mLoss[0m : 1.83561
[1mStep[0m  [20/53], [94mLoss[0m : 1.72822
[1mStep[0m  [25/53], [94mLoss[0m : 1.54505
[1mStep[0m  [30/53], [94mLoss[0m : 1.77151
[1mStep[0m  [35/53], [94mLoss[0m : 1.65495
[1mStep[0m  [40/53], [94mLoss[0m : 1.72383
[1mStep[0m  [45/53], [94mLoss[0m : 1.70795
[1mStep[0m  [50/53], [94mLoss[0m : 1.78184

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.68869
[1mStep[0m  [5/53], [94mLoss[0m : 1.67530
[1mStep[0m  [10/53], [94mLoss[0m : 1.64251
[1mStep[0m  [15/53], [94mLoss[0m : 1.85491
[1mStep[0m  [20/53], [94mLoss[0m : 1.69882
[1mStep[0m  [25/53], [94mLoss[0m : 1.84707
[1mStep[0m  [30/53], [94mLoss[0m : 1.68162
[1mStep[0m  [35/53], [94mLoss[0m : 1.73147
[1mStep[0m  [40/53], [94mLoss[0m : 1.71585
[1mStep[0m  [45/53], [94mLoss[0m : 1.87061
[1mStep[0m  [50/53], [94mLoss[0m : 1.83282

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.80697
[1mStep[0m  [5/53], [94mLoss[0m : 1.73113
[1mStep[0m  [10/53], [94mLoss[0m : 1.73097
[1mStep[0m  [15/53], [94mLoss[0m : 1.68073
[1mStep[0m  [20/53], [94mLoss[0m : 1.71501
[1mStep[0m  [25/53], [94mLoss[0m : 1.67405
[1mStep[0m  [30/53], [94mLoss[0m : 1.89172
[1mStep[0m  [35/53], [94mLoss[0m : 1.67575
[1mStep[0m  [40/53], [94mLoss[0m : 1.60787
[1mStep[0m  [45/53], [94mLoss[0m : 1.67460
[1mStep[0m  [50/53], [94mLoss[0m : 1.65150

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.466
====================================

Phase 2 - Evaluation MAE:  2.4660043257933397
MAE score P1       2.338893
MAE score P2       2.466004
loss               1.708155
learning_rate       0.00505
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.34118
[1mStep[0m  [5/53], [94mLoss[0m : 10.75208
[1mStep[0m  [10/53], [94mLoss[0m : 11.05136
[1mStep[0m  [15/53], [94mLoss[0m : 10.87048
[1mStep[0m  [20/53], [94mLoss[0m : 10.55935
[1mStep[0m  [25/53], [94mLoss[0m : 10.78262
[1mStep[0m  [30/53], [94mLoss[0m : 10.63412
[1mStep[0m  [35/53], [94mLoss[0m : 10.87159
[1mStep[0m  [40/53], [94mLoss[0m : 10.50783
[1mStep[0m  [45/53], [94mLoss[0m : 11.01159
[1mStep[0m  [50/53], [94mLoss[0m : 10.50294

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.659, [92mTest[0m: 10.860, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.73911
[1mStep[0m  [5/53], [94mLoss[0m : 10.54046
[1mStep[0m  [10/53], [94mLoss[0m : 10.12150
[1mStep[0m  [15/53], [94mLoss[0m : 10.26527
[1mStep[0m  [20/53], [94mLoss[0m : 10.68456
[1mStep[0m  [25/53], [94mLoss[0m : 10.65484
[1mStep[0m  [30/53], [94mLoss[0m : 10.00586
[1mStep[0m  [35/53], [94mLoss[0m : 10.11133
[1mStep[0m  [40/53], [94mLoss[0m : 10.44509
[1mStep[0m  [45/53], [94mLoss[0m : 10.45006
[1mStep[0m  [50/53], [94mLoss[0m : 10.00655

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.295, [92mTest[0m: 10.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.06032
[1mStep[0m  [5/53], [94mLoss[0m : 10.24213
[1mStep[0m  [10/53], [94mLoss[0m : 10.22540
[1mStep[0m  [15/53], [94mLoss[0m : 10.26458
[1mStep[0m  [20/53], [94mLoss[0m : 9.61060
[1mStep[0m  [25/53], [94mLoss[0m : 10.05548
[1mStep[0m  [30/53], [94mLoss[0m : 9.99282
[1mStep[0m  [35/53], [94mLoss[0m : 9.65508
[1mStep[0m  [40/53], [94mLoss[0m : 9.68995
[1mStep[0m  [45/53], [94mLoss[0m : 9.55701
[1mStep[0m  [50/53], [94mLoss[0m : 10.10556

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.914, [92mTest[0m: 9.963, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.08698
[1mStep[0m  [5/53], [94mLoss[0m : 9.65954
[1mStep[0m  [10/53], [94mLoss[0m : 9.29607
[1mStep[0m  [15/53], [94mLoss[0m : 9.73671
[1mStep[0m  [20/53], [94mLoss[0m : 9.48219
[1mStep[0m  [25/53], [94mLoss[0m : 9.48064
[1mStep[0m  [30/53], [94mLoss[0m : 9.17104
[1mStep[0m  [35/53], [94mLoss[0m : 9.37884
[1mStep[0m  [40/53], [94mLoss[0m : 9.34122
[1mStep[0m  [45/53], [94mLoss[0m : 9.10940
[1mStep[0m  [50/53], [94mLoss[0m : 9.77973

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.494, [92mTest[0m: 9.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.42094
[1mStep[0m  [5/53], [94mLoss[0m : 9.24489
[1mStep[0m  [10/53], [94mLoss[0m : 9.51606
[1mStep[0m  [15/53], [94mLoss[0m : 9.24114
[1mStep[0m  [20/53], [94mLoss[0m : 8.97387
[1mStep[0m  [25/53], [94mLoss[0m : 9.16825
[1mStep[0m  [30/53], [94mLoss[0m : 8.95969
[1mStep[0m  [35/53], [94mLoss[0m : 8.95041
[1mStep[0m  [40/53], [94mLoss[0m : 8.72190
[1mStep[0m  [45/53], [94mLoss[0m : 8.69410
[1mStep[0m  [50/53], [94mLoss[0m : 9.08275

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.022, [92mTest[0m: 9.006, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.71978
[1mStep[0m  [5/53], [94mLoss[0m : 8.69181
[1mStep[0m  [10/53], [94mLoss[0m : 8.71559
[1mStep[0m  [15/53], [94mLoss[0m : 8.16965
[1mStep[0m  [20/53], [94mLoss[0m : 8.56310
[1mStep[0m  [25/53], [94mLoss[0m : 8.37872
[1mStep[0m  [30/53], [94mLoss[0m : 8.62154
[1mStep[0m  [35/53], [94mLoss[0m : 8.59281
[1mStep[0m  [40/53], [94mLoss[0m : 8.05313
[1mStep[0m  [45/53], [94mLoss[0m : 8.42934
[1mStep[0m  [50/53], [94mLoss[0m : 8.46138

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.447, [92mTest[0m: 8.408, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.30630
[1mStep[0m  [5/53], [94mLoss[0m : 8.01098
[1mStep[0m  [10/53], [94mLoss[0m : 8.21878
[1mStep[0m  [15/53], [94mLoss[0m : 7.70810
[1mStep[0m  [20/53], [94mLoss[0m : 7.87648
[1mStep[0m  [25/53], [94mLoss[0m : 8.03279
[1mStep[0m  [30/53], [94mLoss[0m : 8.14449
[1mStep[0m  [35/53], [94mLoss[0m : 7.91868
[1mStep[0m  [40/53], [94mLoss[0m : 7.56676
[1mStep[0m  [45/53], [94mLoss[0m : 7.51443
[1mStep[0m  [50/53], [94mLoss[0m : 7.61423

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.766, [92mTest[0m: 7.701, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.44731
[1mStep[0m  [5/53], [94mLoss[0m : 7.49285
[1mStep[0m  [10/53], [94mLoss[0m : 7.21374
[1mStep[0m  [15/53], [94mLoss[0m : 7.19860
[1mStep[0m  [20/53], [94mLoss[0m : 7.59864
[1mStep[0m  [25/53], [94mLoss[0m : 7.02803
[1mStep[0m  [30/53], [94mLoss[0m : 7.03225
[1mStep[0m  [35/53], [94mLoss[0m : 6.67479
[1mStep[0m  [40/53], [94mLoss[0m : 6.75430
[1mStep[0m  [45/53], [94mLoss[0m : 6.62850
[1mStep[0m  [50/53], [94mLoss[0m : 6.78627

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.979, [92mTest[0m: 6.939, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.64057
[1mStep[0m  [5/53], [94mLoss[0m : 6.28681
[1mStep[0m  [10/53], [94mLoss[0m : 6.30383
[1mStep[0m  [15/53], [94mLoss[0m : 6.57194
[1mStep[0m  [20/53], [94mLoss[0m : 6.43410
[1mStep[0m  [25/53], [94mLoss[0m : 6.46067
[1mStep[0m  [30/53], [94mLoss[0m : 6.20655
[1mStep[0m  [35/53], [94mLoss[0m : 6.29687
[1mStep[0m  [40/53], [94mLoss[0m : 6.49703
[1mStep[0m  [45/53], [94mLoss[0m : 5.88633
[1mStep[0m  [50/53], [94mLoss[0m : 6.06143

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.226, [92mTest[0m: 5.948, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.87731
[1mStep[0m  [5/53], [94mLoss[0m : 6.04506
[1mStep[0m  [10/53], [94mLoss[0m : 5.72246
[1mStep[0m  [15/53], [94mLoss[0m : 5.89626
[1mStep[0m  [20/53], [94mLoss[0m : 5.52649
[1mStep[0m  [25/53], [94mLoss[0m : 5.73483
[1mStep[0m  [30/53], [94mLoss[0m : 5.73852
[1mStep[0m  [35/53], [94mLoss[0m : 5.61040
[1mStep[0m  [40/53], [94mLoss[0m : 5.57156
[1mStep[0m  [45/53], [94mLoss[0m : 5.61271
[1mStep[0m  [50/53], [94mLoss[0m : 5.11915

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.546, [92mTest[0m: 5.272, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.32450
[1mStep[0m  [5/53], [94mLoss[0m : 5.24371
[1mStep[0m  [10/53], [94mLoss[0m : 5.12740
[1mStep[0m  [15/53], [94mLoss[0m : 4.98723
[1mStep[0m  [20/53], [94mLoss[0m : 5.32441
[1mStep[0m  [25/53], [94mLoss[0m : 5.02975
[1mStep[0m  [30/53], [94mLoss[0m : 4.71089
[1mStep[0m  [35/53], [94mLoss[0m : 4.61981
[1mStep[0m  [40/53], [94mLoss[0m : 4.95519
[1mStep[0m  [45/53], [94mLoss[0m : 5.39814
[1mStep[0m  [50/53], [94mLoss[0m : 4.54388

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.934, [92mTest[0m: 4.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.64256
[1mStep[0m  [5/53], [94mLoss[0m : 4.48914
[1mStep[0m  [10/53], [94mLoss[0m : 4.64838
[1mStep[0m  [15/53], [94mLoss[0m : 4.69186
[1mStep[0m  [20/53], [94mLoss[0m : 4.62619
[1mStep[0m  [25/53], [94mLoss[0m : 4.30484
[1mStep[0m  [30/53], [94mLoss[0m : 4.32705
[1mStep[0m  [35/53], [94mLoss[0m : 4.35211
[1mStep[0m  [40/53], [94mLoss[0m : 4.22758
[1mStep[0m  [45/53], [94mLoss[0m : 4.23166
[1mStep[0m  [50/53], [94mLoss[0m : 4.02803

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.297, [92mTest[0m: 4.009, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.04100
[1mStep[0m  [5/53], [94mLoss[0m : 3.50552
[1mStep[0m  [10/53], [94mLoss[0m : 4.01853
[1mStep[0m  [15/53], [94mLoss[0m : 3.85222
[1mStep[0m  [20/53], [94mLoss[0m : 3.54432
[1mStep[0m  [25/53], [94mLoss[0m : 3.77252
[1mStep[0m  [30/53], [94mLoss[0m : 3.82922
[1mStep[0m  [35/53], [94mLoss[0m : 3.71750
[1mStep[0m  [40/53], [94mLoss[0m : 3.93255
[1mStep[0m  [45/53], [94mLoss[0m : 3.66825
[1mStep[0m  [50/53], [94mLoss[0m : 3.36236

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.757, [92mTest[0m: 3.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.37242
[1mStep[0m  [5/53], [94mLoss[0m : 3.30118
[1mStep[0m  [10/53], [94mLoss[0m : 3.66347
[1mStep[0m  [15/53], [94mLoss[0m : 3.30468
[1mStep[0m  [20/53], [94mLoss[0m : 3.43504
[1mStep[0m  [25/53], [94mLoss[0m : 3.06723
[1mStep[0m  [30/53], [94mLoss[0m : 3.38555
[1mStep[0m  [35/53], [94mLoss[0m : 3.26806
[1mStep[0m  [40/53], [94mLoss[0m : 3.15019
[1mStep[0m  [45/53], [94mLoss[0m : 3.11854
[1mStep[0m  [50/53], [94mLoss[0m : 3.12589

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.306, [92mTest[0m: 3.093, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.19232
[1mStep[0m  [5/53], [94mLoss[0m : 3.21558
[1mStep[0m  [10/53], [94mLoss[0m : 3.18667
[1mStep[0m  [15/53], [94mLoss[0m : 3.11792
[1mStep[0m  [20/53], [94mLoss[0m : 3.27111
[1mStep[0m  [25/53], [94mLoss[0m : 2.94730
[1mStep[0m  [30/53], [94mLoss[0m : 2.81543
[1mStep[0m  [35/53], [94mLoss[0m : 3.05208
[1mStep[0m  [40/53], [94mLoss[0m : 2.81180
[1mStep[0m  [45/53], [94mLoss[0m : 3.06231
[1mStep[0m  [50/53], [94mLoss[0m : 3.01200

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.014, [92mTest[0m: 2.748, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.12932
[1mStep[0m  [5/53], [94mLoss[0m : 3.06075
[1mStep[0m  [10/53], [94mLoss[0m : 2.76694
[1mStep[0m  [15/53], [94mLoss[0m : 2.81661
[1mStep[0m  [20/53], [94mLoss[0m : 2.72848
[1mStep[0m  [25/53], [94mLoss[0m : 2.65037
[1mStep[0m  [30/53], [94mLoss[0m : 2.60941
[1mStep[0m  [35/53], [94mLoss[0m : 2.90938
[1mStep[0m  [40/53], [94mLoss[0m : 2.67110
[1mStep[0m  [45/53], [94mLoss[0m : 2.93515
[1mStep[0m  [50/53], [94mLoss[0m : 2.61032

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.816, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.03900
[1mStep[0m  [5/53], [94mLoss[0m : 2.69414
[1mStep[0m  [10/53], [94mLoss[0m : 3.15426
[1mStep[0m  [15/53], [94mLoss[0m : 2.51112
[1mStep[0m  [20/53], [94mLoss[0m : 2.66353
[1mStep[0m  [25/53], [94mLoss[0m : 2.75033
[1mStep[0m  [30/53], [94mLoss[0m : 2.81319
[1mStep[0m  [35/53], [94mLoss[0m : 3.02365
[1mStep[0m  [40/53], [94mLoss[0m : 2.77326
[1mStep[0m  [45/53], [94mLoss[0m : 2.60986
[1mStep[0m  [50/53], [94mLoss[0m : 2.80605

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.761, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.79990
[1mStep[0m  [5/53], [94mLoss[0m : 2.66356
[1mStep[0m  [10/53], [94mLoss[0m : 2.55891
[1mStep[0m  [15/53], [94mLoss[0m : 2.41982
[1mStep[0m  [20/53], [94mLoss[0m : 2.86994
[1mStep[0m  [25/53], [94mLoss[0m : 2.93357
[1mStep[0m  [30/53], [94mLoss[0m : 2.69716
[1mStep[0m  [35/53], [94mLoss[0m : 2.84855
[1mStep[0m  [40/53], [94mLoss[0m : 2.67844
[1mStep[0m  [45/53], [94mLoss[0m : 2.59189
[1mStep[0m  [50/53], [94mLoss[0m : 2.86682

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46659
[1mStep[0m  [5/53], [94mLoss[0m : 2.49210
[1mStep[0m  [10/53], [94mLoss[0m : 2.67110
[1mStep[0m  [15/53], [94mLoss[0m : 2.90176
[1mStep[0m  [20/53], [94mLoss[0m : 2.82390
[1mStep[0m  [25/53], [94mLoss[0m : 2.50191
[1mStep[0m  [30/53], [94mLoss[0m : 2.66463
[1mStep[0m  [35/53], [94mLoss[0m : 2.90846
[1mStep[0m  [40/53], [94mLoss[0m : 2.67993
[1mStep[0m  [45/53], [94mLoss[0m : 2.69016
[1mStep[0m  [50/53], [94mLoss[0m : 2.73541

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.710, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.92078
[1mStep[0m  [5/53], [94mLoss[0m : 2.70013
[1mStep[0m  [10/53], [94mLoss[0m : 2.64380
[1mStep[0m  [15/53], [94mLoss[0m : 2.62995
[1mStep[0m  [20/53], [94mLoss[0m : 2.80911
[1mStep[0m  [25/53], [94mLoss[0m : 2.71925
[1mStep[0m  [30/53], [94mLoss[0m : 2.86215
[1mStep[0m  [35/53], [94mLoss[0m : 2.41651
[1mStep[0m  [40/53], [94mLoss[0m : 2.76988
[1mStep[0m  [45/53], [94mLoss[0m : 2.74962
[1mStep[0m  [50/53], [94mLoss[0m : 2.66272

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.446, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66646
[1mStep[0m  [5/53], [94mLoss[0m : 2.92468
[1mStep[0m  [10/53], [94mLoss[0m : 2.48586
[1mStep[0m  [15/53], [94mLoss[0m : 2.54481
[1mStep[0m  [20/53], [94mLoss[0m : 2.61944
[1mStep[0m  [25/53], [94mLoss[0m : 2.90516
[1mStep[0m  [30/53], [94mLoss[0m : 2.59809
[1mStep[0m  [35/53], [94mLoss[0m : 2.81887
[1mStep[0m  [40/53], [94mLoss[0m : 2.77194
[1mStep[0m  [45/53], [94mLoss[0m : 2.83618
[1mStep[0m  [50/53], [94mLoss[0m : 2.82957

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66500
[1mStep[0m  [5/53], [94mLoss[0m : 3.03488
[1mStep[0m  [10/53], [94mLoss[0m : 2.74934
[1mStep[0m  [15/53], [94mLoss[0m : 2.61025
[1mStep[0m  [20/53], [94mLoss[0m : 2.92979
[1mStep[0m  [25/53], [94mLoss[0m : 2.65556
[1mStep[0m  [30/53], [94mLoss[0m : 2.50838
[1mStep[0m  [35/53], [94mLoss[0m : 2.49073
[1mStep[0m  [40/53], [94mLoss[0m : 2.68365
[1mStep[0m  [45/53], [94mLoss[0m : 2.36385
[1mStep[0m  [50/53], [94mLoss[0m : 2.67303

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61417
[1mStep[0m  [5/53], [94mLoss[0m : 2.65296
[1mStep[0m  [10/53], [94mLoss[0m : 2.61796
[1mStep[0m  [15/53], [94mLoss[0m : 2.92294
[1mStep[0m  [20/53], [94mLoss[0m : 2.69200
[1mStep[0m  [25/53], [94mLoss[0m : 2.76490
[1mStep[0m  [30/53], [94mLoss[0m : 2.58487
[1mStep[0m  [35/53], [94mLoss[0m : 2.94229
[1mStep[0m  [40/53], [94mLoss[0m : 2.59970
[1mStep[0m  [45/53], [94mLoss[0m : 2.85824
[1mStep[0m  [50/53], [94mLoss[0m : 2.84354

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.429, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.66116
[1mStep[0m  [5/53], [94mLoss[0m : 2.92097
[1mStep[0m  [10/53], [94mLoss[0m : 2.76761
[1mStep[0m  [15/53], [94mLoss[0m : 2.58786
[1mStep[0m  [20/53], [94mLoss[0m : 3.06613
[1mStep[0m  [25/53], [94mLoss[0m : 2.73960
[1mStep[0m  [30/53], [94mLoss[0m : 2.62115
[1mStep[0m  [35/53], [94mLoss[0m : 2.51545
[1mStep[0m  [40/53], [94mLoss[0m : 2.56903
[1mStep[0m  [45/53], [94mLoss[0m : 2.91562
[1mStep[0m  [50/53], [94mLoss[0m : 2.79635

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.677, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86980
[1mStep[0m  [5/53], [94mLoss[0m : 2.63881
[1mStep[0m  [10/53], [94mLoss[0m : 2.51799
[1mStep[0m  [15/53], [94mLoss[0m : 2.48682
[1mStep[0m  [20/53], [94mLoss[0m : 2.58912
[1mStep[0m  [25/53], [94mLoss[0m : 2.76846
[1mStep[0m  [30/53], [94mLoss[0m : 2.81576
[1mStep[0m  [35/53], [94mLoss[0m : 2.51989
[1mStep[0m  [40/53], [94mLoss[0m : 2.76157
[1mStep[0m  [45/53], [94mLoss[0m : 2.50903
[1mStep[0m  [50/53], [94mLoss[0m : 2.48773

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64397
[1mStep[0m  [5/53], [94mLoss[0m : 2.59715
[1mStep[0m  [10/53], [94mLoss[0m : 2.68985
[1mStep[0m  [15/53], [94mLoss[0m : 2.67292
[1mStep[0m  [20/53], [94mLoss[0m : 2.95912
[1mStep[0m  [25/53], [94mLoss[0m : 2.51207
[1mStep[0m  [30/53], [94mLoss[0m : 2.61838
[1mStep[0m  [35/53], [94mLoss[0m : 2.39209
[1mStep[0m  [40/53], [94mLoss[0m : 2.73727
[1mStep[0m  [45/53], [94mLoss[0m : 2.53354
[1mStep[0m  [50/53], [94mLoss[0m : 2.74873

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42156
[1mStep[0m  [5/53], [94mLoss[0m : 2.62445
[1mStep[0m  [10/53], [94mLoss[0m : 2.75229
[1mStep[0m  [15/53], [94mLoss[0m : 2.65918
[1mStep[0m  [20/53], [94mLoss[0m : 2.61593
[1mStep[0m  [25/53], [94mLoss[0m : 2.62057
[1mStep[0m  [30/53], [94mLoss[0m : 2.82590
[1mStep[0m  [35/53], [94mLoss[0m : 2.88441
[1mStep[0m  [40/53], [94mLoss[0m : 2.67506
[1mStep[0m  [45/53], [94mLoss[0m : 2.76989
[1mStep[0m  [50/53], [94mLoss[0m : 2.65965

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64608
[1mStep[0m  [5/53], [94mLoss[0m : 2.65368
[1mStep[0m  [10/53], [94mLoss[0m : 2.84588
[1mStep[0m  [15/53], [94mLoss[0m : 2.56498
[1mStep[0m  [20/53], [94mLoss[0m : 2.63495
[1mStep[0m  [25/53], [94mLoss[0m : 2.59931
[1mStep[0m  [30/53], [94mLoss[0m : 2.71028
[1mStep[0m  [35/53], [94mLoss[0m : 2.78823
[1mStep[0m  [40/53], [94mLoss[0m : 2.65863
[1mStep[0m  [45/53], [94mLoss[0m : 2.57157
[1mStep[0m  [50/53], [94mLoss[0m : 2.76732

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53198
[1mStep[0m  [5/53], [94mLoss[0m : 2.64522
[1mStep[0m  [10/53], [94mLoss[0m : 2.92446
[1mStep[0m  [15/53], [94mLoss[0m : 2.81203
[1mStep[0m  [20/53], [94mLoss[0m : 2.51983
[1mStep[0m  [25/53], [94mLoss[0m : 2.66176
[1mStep[0m  [30/53], [94mLoss[0m : 2.64746
[1mStep[0m  [35/53], [94mLoss[0m : 2.59019
[1mStep[0m  [40/53], [94mLoss[0m : 2.47535
[1mStep[0m  [45/53], [94mLoss[0m : 2.75470
[1mStep[0m  [50/53], [94mLoss[0m : 2.86501

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51335
[1mStep[0m  [5/53], [94mLoss[0m : 2.71751
[1mStep[0m  [10/53], [94mLoss[0m : 2.58452
[1mStep[0m  [15/53], [94mLoss[0m : 2.66707
[1mStep[0m  [20/53], [94mLoss[0m : 2.62104
[1mStep[0m  [25/53], [94mLoss[0m : 2.52127
[1mStep[0m  [30/53], [94mLoss[0m : 2.72143
[1mStep[0m  [35/53], [94mLoss[0m : 2.79357
[1mStep[0m  [40/53], [94mLoss[0m : 2.78422
[1mStep[0m  [45/53], [94mLoss[0m : 2.75726
[1mStep[0m  [50/53], [94mLoss[0m : 2.64122

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.417
====================================

Phase 1 - Evaluation MAE:  2.416669176175044
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.55833
[1mStep[0m  [5/53], [94mLoss[0m : 2.54006
[1mStep[0m  [10/53], [94mLoss[0m : 2.51629
[1mStep[0m  [15/53], [94mLoss[0m : 2.56700
[1mStep[0m  [20/53], [94mLoss[0m : 2.59394
[1mStep[0m  [25/53], [94mLoss[0m : 2.75182
[1mStep[0m  [30/53], [94mLoss[0m : 2.81103
[1mStep[0m  [35/53], [94mLoss[0m : 2.56561
[1mStep[0m  [40/53], [94mLoss[0m : 2.63178
[1mStep[0m  [45/53], [94mLoss[0m : 2.55711
[1mStep[0m  [50/53], [94mLoss[0m : 2.74330

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68857
[1mStep[0m  [5/53], [94mLoss[0m : 2.76370
[1mStep[0m  [10/53], [94mLoss[0m : 2.65638
[1mStep[0m  [15/53], [94mLoss[0m : 2.76587
[1mStep[0m  [20/53], [94mLoss[0m : 2.55608
[1mStep[0m  [25/53], [94mLoss[0m : 2.44700
[1mStep[0m  [30/53], [94mLoss[0m : 2.65253
[1mStep[0m  [35/53], [94mLoss[0m : 2.52374
[1mStep[0m  [40/53], [94mLoss[0m : 2.57436
[1mStep[0m  [45/53], [94mLoss[0m : 2.50397
[1mStep[0m  [50/53], [94mLoss[0m : 2.60849

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59945
[1mStep[0m  [5/53], [94mLoss[0m : 2.75206
[1mStep[0m  [10/53], [94mLoss[0m : 3.09410
[1mStep[0m  [15/53], [94mLoss[0m : 2.66237
[1mStep[0m  [20/53], [94mLoss[0m : 2.54796
[1mStep[0m  [25/53], [94mLoss[0m : 2.53701
[1mStep[0m  [30/53], [94mLoss[0m : 2.74049
[1mStep[0m  [35/53], [94mLoss[0m : 2.52875
[1mStep[0m  [40/53], [94mLoss[0m : 2.56941
[1mStep[0m  [45/53], [94mLoss[0m : 2.81415
[1mStep[0m  [50/53], [94mLoss[0m : 2.43916

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68443
[1mStep[0m  [5/53], [94mLoss[0m : 2.74999
[1mStep[0m  [10/53], [94mLoss[0m : 2.55114
[1mStep[0m  [15/53], [94mLoss[0m : 2.59304
[1mStep[0m  [20/53], [94mLoss[0m : 2.52375
[1mStep[0m  [25/53], [94mLoss[0m : 2.58810
[1mStep[0m  [30/53], [94mLoss[0m : 2.35678
[1mStep[0m  [35/53], [94mLoss[0m : 2.57314
[1mStep[0m  [40/53], [94mLoss[0m : 2.50956
[1mStep[0m  [45/53], [94mLoss[0m : 2.76312
[1mStep[0m  [50/53], [94mLoss[0m : 2.50782

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.83637
[1mStep[0m  [5/53], [94mLoss[0m : 2.72825
[1mStep[0m  [10/53], [94mLoss[0m : 2.28726
[1mStep[0m  [15/53], [94mLoss[0m : 2.39907
[1mStep[0m  [20/53], [94mLoss[0m : 2.45397
[1mStep[0m  [25/53], [94mLoss[0m : 2.60534
[1mStep[0m  [30/53], [94mLoss[0m : 2.50515
[1mStep[0m  [35/53], [94mLoss[0m : 2.37437
[1mStep[0m  [40/53], [94mLoss[0m : 2.37327
[1mStep[0m  [45/53], [94mLoss[0m : 2.60905
[1mStep[0m  [50/53], [94mLoss[0m : 2.72786

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47531
[1mStep[0m  [5/53], [94mLoss[0m : 2.49217
[1mStep[0m  [10/53], [94mLoss[0m : 2.56754
[1mStep[0m  [15/53], [94mLoss[0m : 2.81913
[1mStep[0m  [20/53], [94mLoss[0m : 2.57800
[1mStep[0m  [25/53], [94mLoss[0m : 2.31589
[1mStep[0m  [30/53], [94mLoss[0m : 2.58814
[1mStep[0m  [35/53], [94mLoss[0m : 2.52096
[1mStep[0m  [40/53], [94mLoss[0m : 2.78769
[1mStep[0m  [45/53], [94mLoss[0m : 2.31746
[1mStep[0m  [50/53], [94mLoss[0m : 2.65239

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55906
[1mStep[0m  [5/53], [94mLoss[0m : 2.60414
[1mStep[0m  [10/53], [94mLoss[0m : 2.48728
[1mStep[0m  [15/53], [94mLoss[0m : 2.55712
[1mStep[0m  [20/53], [94mLoss[0m : 2.45411
[1mStep[0m  [25/53], [94mLoss[0m : 2.46874
[1mStep[0m  [30/53], [94mLoss[0m : 2.59485
[1mStep[0m  [35/53], [94mLoss[0m : 2.42471
[1mStep[0m  [40/53], [94mLoss[0m : 2.64238
[1mStep[0m  [45/53], [94mLoss[0m : 2.40782
[1mStep[0m  [50/53], [94mLoss[0m : 2.62033

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.499, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.23895
[1mStep[0m  [5/53], [94mLoss[0m : 2.39000
[1mStep[0m  [10/53], [94mLoss[0m : 2.30739
[1mStep[0m  [15/53], [94mLoss[0m : 2.54705
[1mStep[0m  [20/53], [94mLoss[0m : 2.32852
[1mStep[0m  [25/53], [94mLoss[0m : 2.53820
[1mStep[0m  [30/53], [94mLoss[0m : 2.36513
[1mStep[0m  [35/53], [94mLoss[0m : 2.47109
[1mStep[0m  [40/53], [94mLoss[0m : 2.61212
[1mStep[0m  [45/53], [94mLoss[0m : 2.44728
[1mStep[0m  [50/53], [94mLoss[0m : 2.51402

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60072
[1mStep[0m  [5/53], [94mLoss[0m : 2.68972
[1mStep[0m  [10/53], [94mLoss[0m : 2.48424
[1mStep[0m  [15/53], [94mLoss[0m : 2.45691
[1mStep[0m  [20/53], [94mLoss[0m : 2.26349
[1mStep[0m  [25/53], [94mLoss[0m : 2.69629
[1mStep[0m  [30/53], [94mLoss[0m : 2.31382
[1mStep[0m  [35/53], [94mLoss[0m : 2.52855
[1mStep[0m  [40/53], [94mLoss[0m : 2.48879
[1mStep[0m  [45/53], [94mLoss[0m : 2.38837
[1mStep[0m  [50/53], [94mLoss[0m : 2.60032

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32503
[1mStep[0m  [5/53], [94mLoss[0m : 2.93882
[1mStep[0m  [10/53], [94mLoss[0m : 2.60472
[1mStep[0m  [15/53], [94mLoss[0m : 2.52927
[1mStep[0m  [20/53], [94mLoss[0m : 2.44279
[1mStep[0m  [25/53], [94mLoss[0m : 2.43366
[1mStep[0m  [30/53], [94mLoss[0m : 2.54763
[1mStep[0m  [35/53], [94mLoss[0m : 2.46545
[1mStep[0m  [40/53], [94mLoss[0m : 2.38688
[1mStep[0m  [45/53], [94mLoss[0m : 2.36620
[1mStep[0m  [50/53], [94mLoss[0m : 2.72681

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63134
[1mStep[0m  [5/53], [94mLoss[0m : 2.49386
[1mStep[0m  [10/53], [94mLoss[0m : 2.39409
[1mStep[0m  [15/53], [94mLoss[0m : 2.28858
[1mStep[0m  [20/53], [94mLoss[0m : 2.44862
[1mStep[0m  [25/53], [94mLoss[0m : 2.51012
[1mStep[0m  [30/53], [94mLoss[0m : 2.34948
[1mStep[0m  [35/53], [94mLoss[0m : 2.29981
[1mStep[0m  [40/53], [94mLoss[0m : 2.50296
[1mStep[0m  [45/53], [94mLoss[0m : 2.27212
[1mStep[0m  [50/53], [94mLoss[0m : 2.28789

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52577
[1mStep[0m  [5/53], [94mLoss[0m : 2.44558
[1mStep[0m  [10/53], [94mLoss[0m : 2.33517
[1mStep[0m  [15/53], [94mLoss[0m : 2.30491
[1mStep[0m  [20/53], [94mLoss[0m : 2.48197
[1mStep[0m  [25/53], [94mLoss[0m : 2.40959
[1mStep[0m  [30/53], [94mLoss[0m : 2.45878
[1mStep[0m  [35/53], [94mLoss[0m : 2.50614
[1mStep[0m  [40/53], [94mLoss[0m : 2.61707
[1mStep[0m  [45/53], [94mLoss[0m : 2.36487
[1mStep[0m  [50/53], [94mLoss[0m : 2.30879

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53465
[1mStep[0m  [5/53], [94mLoss[0m : 2.48399
[1mStep[0m  [10/53], [94mLoss[0m : 2.63444
[1mStep[0m  [15/53], [94mLoss[0m : 2.37006
[1mStep[0m  [20/53], [94mLoss[0m : 2.32266
[1mStep[0m  [25/53], [94mLoss[0m : 2.51545
[1mStep[0m  [30/53], [94mLoss[0m : 2.33986
[1mStep[0m  [35/53], [94mLoss[0m : 2.50266
[1mStep[0m  [40/53], [94mLoss[0m : 2.32127
[1mStep[0m  [45/53], [94mLoss[0m : 2.44639
[1mStep[0m  [50/53], [94mLoss[0m : 2.50579

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42110
[1mStep[0m  [5/53], [94mLoss[0m : 2.62413
[1mStep[0m  [10/53], [94mLoss[0m : 2.51722
[1mStep[0m  [15/53], [94mLoss[0m : 2.08267
[1mStep[0m  [20/53], [94mLoss[0m : 2.31374
[1mStep[0m  [25/53], [94mLoss[0m : 2.32899
[1mStep[0m  [30/53], [94mLoss[0m : 2.50322
[1mStep[0m  [35/53], [94mLoss[0m : 2.31838
[1mStep[0m  [40/53], [94mLoss[0m : 2.29133
[1mStep[0m  [45/53], [94mLoss[0m : 2.49648
[1mStep[0m  [50/53], [94mLoss[0m : 2.35362

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39501
[1mStep[0m  [5/53], [94mLoss[0m : 2.41591
[1mStep[0m  [10/53], [94mLoss[0m : 2.25913
[1mStep[0m  [15/53], [94mLoss[0m : 2.30477
[1mStep[0m  [20/53], [94mLoss[0m : 2.32579
[1mStep[0m  [25/53], [94mLoss[0m : 2.50988
[1mStep[0m  [30/53], [94mLoss[0m : 2.03497
[1mStep[0m  [35/53], [94mLoss[0m : 2.47108
[1mStep[0m  [40/53], [94mLoss[0m : 2.40057
[1mStep[0m  [45/53], [94mLoss[0m : 2.37354
[1mStep[0m  [50/53], [94mLoss[0m : 2.35369

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37093
[1mStep[0m  [5/53], [94mLoss[0m : 2.45381
[1mStep[0m  [10/53], [94mLoss[0m : 2.31919
[1mStep[0m  [15/53], [94mLoss[0m : 2.44120
[1mStep[0m  [20/53], [94mLoss[0m : 2.45704
[1mStep[0m  [25/53], [94mLoss[0m : 2.31107
[1mStep[0m  [30/53], [94mLoss[0m : 2.35108
[1mStep[0m  [35/53], [94mLoss[0m : 2.34708
[1mStep[0m  [40/53], [94mLoss[0m : 2.38845
[1mStep[0m  [45/53], [94mLoss[0m : 2.27124
[1mStep[0m  [50/53], [94mLoss[0m : 2.44562

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24955
[1mStep[0m  [5/53], [94mLoss[0m : 2.14472
[1mStep[0m  [10/53], [94mLoss[0m : 2.09705
[1mStep[0m  [15/53], [94mLoss[0m : 2.20333
[1mStep[0m  [20/53], [94mLoss[0m : 2.21035
[1mStep[0m  [25/53], [94mLoss[0m : 2.34199
[1mStep[0m  [30/53], [94mLoss[0m : 2.20259
[1mStep[0m  [35/53], [94mLoss[0m : 2.79573
[1mStep[0m  [40/53], [94mLoss[0m : 2.33502
[1mStep[0m  [45/53], [94mLoss[0m : 2.26214
[1mStep[0m  [50/53], [94mLoss[0m : 2.26241

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.483, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22145
[1mStep[0m  [5/53], [94mLoss[0m : 2.14669
[1mStep[0m  [10/53], [94mLoss[0m : 2.26480
[1mStep[0m  [15/53], [94mLoss[0m : 2.14635
[1mStep[0m  [20/53], [94mLoss[0m : 2.43958
[1mStep[0m  [25/53], [94mLoss[0m : 2.25442
[1mStep[0m  [30/53], [94mLoss[0m : 2.24543
[1mStep[0m  [35/53], [94mLoss[0m : 2.20066
[1mStep[0m  [40/53], [94mLoss[0m : 2.25823
[1mStep[0m  [45/53], [94mLoss[0m : 2.19742
[1mStep[0m  [50/53], [94mLoss[0m : 2.31756

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.241, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.10125
[1mStep[0m  [5/53], [94mLoss[0m : 2.23068
[1mStep[0m  [10/53], [94mLoss[0m : 2.29469
[1mStep[0m  [15/53], [94mLoss[0m : 2.14160
[1mStep[0m  [20/53], [94mLoss[0m : 2.18951
[1mStep[0m  [25/53], [94mLoss[0m : 2.34474
[1mStep[0m  [30/53], [94mLoss[0m : 2.10275
[1mStep[0m  [35/53], [94mLoss[0m : 2.12753
[1mStep[0m  [40/53], [94mLoss[0m : 2.12722
[1mStep[0m  [45/53], [94mLoss[0m : 2.26946
[1mStep[0m  [50/53], [94mLoss[0m : 2.10115

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.216, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.08111
[1mStep[0m  [5/53], [94mLoss[0m : 2.12305
[1mStep[0m  [10/53], [94mLoss[0m : 2.29137
[1mStep[0m  [15/53], [94mLoss[0m : 2.07197
[1mStep[0m  [20/53], [94mLoss[0m : 2.10381
[1mStep[0m  [25/53], [94mLoss[0m : 2.17527
[1mStep[0m  [30/53], [94mLoss[0m : 2.03299
[1mStep[0m  [35/53], [94mLoss[0m : 2.28726
[1mStep[0m  [40/53], [94mLoss[0m : 2.16531
[1mStep[0m  [45/53], [94mLoss[0m : 1.93968
[1mStep[0m  [50/53], [94mLoss[0m : 2.04529

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15518
[1mStep[0m  [5/53], [94mLoss[0m : 2.33306
[1mStep[0m  [10/53], [94mLoss[0m : 1.99640
[1mStep[0m  [15/53], [94mLoss[0m : 2.19358
[1mStep[0m  [20/53], [94mLoss[0m : 2.12747
[1mStep[0m  [25/53], [94mLoss[0m : 2.11584
[1mStep[0m  [30/53], [94mLoss[0m : 2.19293
[1mStep[0m  [35/53], [94mLoss[0m : 2.02567
[1mStep[0m  [40/53], [94mLoss[0m : 2.21969
[1mStep[0m  [45/53], [94mLoss[0m : 2.00352
[1mStep[0m  [50/53], [94mLoss[0m : 2.21487

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.03668
[1mStep[0m  [5/53], [94mLoss[0m : 2.04828
[1mStep[0m  [10/53], [94mLoss[0m : 2.11571
[1mStep[0m  [15/53], [94mLoss[0m : 1.99667
[1mStep[0m  [20/53], [94mLoss[0m : 2.15241
[1mStep[0m  [25/53], [94mLoss[0m : 2.07706
[1mStep[0m  [30/53], [94mLoss[0m : 2.12913
[1mStep[0m  [35/53], [94mLoss[0m : 2.10226
[1mStep[0m  [40/53], [94mLoss[0m : 2.07461
[1mStep[0m  [45/53], [94mLoss[0m : 2.40420
[1mStep[0m  [50/53], [94mLoss[0m : 2.21660

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.129, [92mTest[0m: 2.438, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33740
[1mStep[0m  [5/53], [94mLoss[0m : 2.15800
[1mStep[0m  [10/53], [94mLoss[0m : 2.20145
[1mStep[0m  [15/53], [94mLoss[0m : 2.28447
[1mStep[0m  [20/53], [94mLoss[0m : 2.14657
[1mStep[0m  [25/53], [94mLoss[0m : 2.12491
[1mStep[0m  [30/53], [94mLoss[0m : 2.00465
[1mStep[0m  [35/53], [94mLoss[0m : 2.36234
[1mStep[0m  [40/53], [94mLoss[0m : 2.31030
[1mStep[0m  [45/53], [94mLoss[0m : 1.98873
[1mStep[0m  [50/53], [94mLoss[0m : 2.31700

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.01299
[1mStep[0m  [5/53], [94mLoss[0m : 1.96360
[1mStep[0m  [10/53], [94mLoss[0m : 2.21765
[1mStep[0m  [15/53], [94mLoss[0m : 2.11232
[1mStep[0m  [20/53], [94mLoss[0m : 2.15368
[1mStep[0m  [25/53], [94mLoss[0m : 2.14964
[1mStep[0m  [30/53], [94mLoss[0m : 2.02216
[1mStep[0m  [35/53], [94mLoss[0m : 2.21265
[1mStep[0m  [40/53], [94mLoss[0m : 2.28463
[1mStep[0m  [45/53], [94mLoss[0m : 2.20749
[1mStep[0m  [50/53], [94mLoss[0m : 2.08154

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.97239
[1mStep[0m  [5/53], [94mLoss[0m : 1.99465
[1mStep[0m  [10/53], [94mLoss[0m : 2.04964
[1mStep[0m  [15/53], [94mLoss[0m : 2.04267
[1mStep[0m  [20/53], [94mLoss[0m : 2.03223
[1mStep[0m  [25/53], [94mLoss[0m : 1.99140
[1mStep[0m  [30/53], [94mLoss[0m : 2.10369
[1mStep[0m  [35/53], [94mLoss[0m : 1.99965
[1mStep[0m  [40/53], [94mLoss[0m : 1.96565
[1mStep[0m  [45/53], [94mLoss[0m : 1.99001
[1mStep[0m  [50/53], [94mLoss[0m : 2.00750

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.429, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.05674
[1mStep[0m  [5/53], [94mLoss[0m : 2.08395
[1mStep[0m  [10/53], [94mLoss[0m : 1.89237
[1mStep[0m  [15/53], [94mLoss[0m : 2.08238
[1mStep[0m  [20/53], [94mLoss[0m : 2.06856
[1mStep[0m  [25/53], [94mLoss[0m : 2.03589
[1mStep[0m  [30/53], [94mLoss[0m : 2.11940
[1mStep[0m  [35/53], [94mLoss[0m : 2.09052
[1mStep[0m  [40/53], [94mLoss[0m : 1.92136
[1mStep[0m  [45/53], [94mLoss[0m : 2.17386
[1mStep[0m  [50/53], [94mLoss[0m : 2.06065

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.052, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93436
[1mStep[0m  [5/53], [94mLoss[0m : 1.85944
[1mStep[0m  [10/53], [94mLoss[0m : 2.24053
[1mStep[0m  [15/53], [94mLoss[0m : 1.98468
[1mStep[0m  [20/53], [94mLoss[0m : 2.07408
[1mStep[0m  [25/53], [94mLoss[0m : 2.16567
[1mStep[0m  [30/53], [94mLoss[0m : 2.05963
[1mStep[0m  [35/53], [94mLoss[0m : 1.94809
[1mStep[0m  [40/53], [94mLoss[0m : 2.18334
[1mStep[0m  [45/53], [94mLoss[0m : 1.98218
[1mStep[0m  [50/53], [94mLoss[0m : 2.22163

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.85511
[1mStep[0m  [5/53], [94mLoss[0m : 2.05567
[1mStep[0m  [10/53], [94mLoss[0m : 2.09328
[1mStep[0m  [15/53], [94mLoss[0m : 1.87942
[1mStep[0m  [20/53], [94mLoss[0m : 1.67880
[1mStep[0m  [25/53], [94mLoss[0m : 1.89779
[1mStep[0m  [30/53], [94mLoss[0m : 2.19216
[1mStep[0m  [35/53], [94mLoss[0m : 1.87449
[1mStep[0m  [40/53], [94mLoss[0m : 2.02692
[1mStep[0m  [45/53], [94mLoss[0m : 1.94676
[1mStep[0m  [50/53], [94mLoss[0m : 2.20179

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.395, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.82399
[1mStep[0m  [5/53], [94mLoss[0m : 1.86543
[1mStep[0m  [10/53], [94mLoss[0m : 2.06283
[1mStep[0m  [15/53], [94mLoss[0m : 1.97359
[1mStep[0m  [20/53], [94mLoss[0m : 2.11818
[1mStep[0m  [25/53], [94mLoss[0m : 1.73708
[1mStep[0m  [30/53], [94mLoss[0m : 2.20284
[1mStep[0m  [35/53], [94mLoss[0m : 1.97873
[1mStep[0m  [40/53], [94mLoss[0m : 2.08412
[1mStep[0m  [45/53], [94mLoss[0m : 1.96776
[1mStep[0m  [50/53], [94mLoss[0m : 1.91642

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.990, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94644
[1mStep[0m  [5/53], [94mLoss[0m : 1.88570
[1mStep[0m  [10/53], [94mLoss[0m : 1.77117
[1mStep[0m  [15/53], [94mLoss[0m : 1.88511
[1mStep[0m  [20/53], [94mLoss[0m : 2.08271
[1mStep[0m  [25/53], [94mLoss[0m : 1.77470
[1mStep[0m  [30/53], [94mLoss[0m : 1.98458
[1mStep[0m  [35/53], [94mLoss[0m : 1.94369
[1mStep[0m  [40/53], [94mLoss[0m : 2.04335
[1mStep[0m  [45/53], [94mLoss[0m : 1.89765
[1mStep[0m  [50/53], [94mLoss[0m : 1.96667

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.464, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.413
====================================

Phase 2 - Evaluation MAE:  2.412656536469093
MAE score P1       2.416669
MAE score P2       2.412657
loss               1.974043
learning_rate       0.00505
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay           0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.03937
[1mStep[0m  [10/106], [94mLoss[0m : 11.22509
[1mStep[0m  [20/106], [94mLoss[0m : 11.35389
[1mStep[0m  [30/106], [94mLoss[0m : 10.10239
[1mStep[0m  [40/106], [94mLoss[0m : 10.21741
[1mStep[0m  [50/106], [94mLoss[0m : 10.35860
[1mStep[0m  [60/106], [94mLoss[0m : 9.88832
[1mStep[0m  [70/106], [94mLoss[0m : 10.93106
[1mStep[0m  [80/106], [94mLoss[0m : 10.03419
[1mStep[0m  [90/106], [94mLoss[0m : 10.25273
[1mStep[0m  [100/106], [94mLoss[0m : 9.61877

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.257, [92mTest[0m: 10.913, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.32259
[1mStep[0m  [10/106], [94mLoss[0m : 9.02756
[1mStep[0m  [20/106], [94mLoss[0m : 9.30818
[1mStep[0m  [30/106], [94mLoss[0m : 9.33661
[1mStep[0m  [40/106], [94mLoss[0m : 8.60044
[1mStep[0m  [50/106], [94mLoss[0m : 8.77925
[1mStep[0m  [60/106], [94mLoss[0m : 8.75963
[1mStep[0m  [70/106], [94mLoss[0m : 8.10566
[1mStep[0m  [80/106], [94mLoss[0m : 7.57979
[1mStep[0m  [90/106], [94mLoss[0m : 6.90871
[1mStep[0m  [100/106], [94mLoss[0m : 7.30008

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.441, [92mTest[0m: 9.213, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.25169
[1mStep[0m  [10/106], [94mLoss[0m : 7.19954
[1mStep[0m  [20/106], [94mLoss[0m : 6.53788
[1mStep[0m  [30/106], [94mLoss[0m : 6.11493
[1mStep[0m  [40/106], [94mLoss[0m : 6.23335
[1mStep[0m  [50/106], [94mLoss[0m : 6.11775
[1mStep[0m  [60/106], [94mLoss[0m : 5.10799
[1mStep[0m  [70/106], [94mLoss[0m : 4.83964
[1mStep[0m  [80/106], [94mLoss[0m : 5.71853
[1mStep[0m  [90/106], [94mLoss[0m : 4.70469
[1mStep[0m  [100/106], [94mLoss[0m : 4.44746

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.785, [92mTest[0m: 6.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.13565
[1mStep[0m  [10/106], [94mLoss[0m : 4.52923
[1mStep[0m  [20/106], [94mLoss[0m : 4.14605
[1mStep[0m  [30/106], [94mLoss[0m : 3.98840
[1mStep[0m  [40/106], [94mLoss[0m : 3.77765
[1mStep[0m  [50/106], [94mLoss[0m : 2.88232
[1mStep[0m  [60/106], [94mLoss[0m : 3.42865
[1mStep[0m  [70/106], [94mLoss[0m : 3.39718
[1mStep[0m  [80/106], [94mLoss[0m : 3.25815
[1mStep[0m  [90/106], [94mLoss[0m : 3.06870
[1mStep[0m  [100/106], [94mLoss[0m : 3.07572

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.616, [92mTest[0m: 3.927, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.88013
[1mStep[0m  [10/106], [94mLoss[0m : 2.85237
[1mStep[0m  [20/106], [94mLoss[0m : 2.71512
[1mStep[0m  [30/106], [94mLoss[0m : 2.54794
[1mStep[0m  [40/106], [94mLoss[0m : 3.04613
[1mStep[0m  [50/106], [94mLoss[0m : 2.94035
[1mStep[0m  [60/106], [94mLoss[0m : 2.62240
[1mStep[0m  [70/106], [94mLoss[0m : 2.59927
[1mStep[0m  [80/106], [94mLoss[0m : 2.67400
[1mStep[0m  [90/106], [94mLoss[0m : 2.71181
[1mStep[0m  [100/106], [94mLoss[0m : 2.55076

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.697, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52315
[1mStep[0m  [10/106], [94mLoss[0m : 2.56124
[1mStep[0m  [20/106], [94mLoss[0m : 2.33420
[1mStep[0m  [30/106], [94mLoss[0m : 2.27076
[1mStep[0m  [40/106], [94mLoss[0m : 2.38406
[1mStep[0m  [50/106], [94mLoss[0m : 2.50631
[1mStep[0m  [60/106], [94mLoss[0m : 2.57808
[1mStep[0m  [70/106], [94mLoss[0m : 2.97779
[1mStep[0m  [80/106], [94mLoss[0m : 2.54399
[1mStep[0m  [90/106], [94mLoss[0m : 2.40517
[1mStep[0m  [100/106], [94mLoss[0m : 2.65728

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48091
[1mStep[0m  [10/106], [94mLoss[0m : 2.74251
[1mStep[0m  [20/106], [94mLoss[0m : 2.58603
[1mStep[0m  [30/106], [94mLoss[0m : 2.24236
[1mStep[0m  [40/106], [94mLoss[0m : 2.71806
[1mStep[0m  [50/106], [94mLoss[0m : 2.71152
[1mStep[0m  [60/106], [94mLoss[0m : 2.75657
[1mStep[0m  [70/106], [94mLoss[0m : 2.46174
[1mStep[0m  [80/106], [94mLoss[0m : 2.57380
[1mStep[0m  [90/106], [94mLoss[0m : 2.64994
[1mStep[0m  [100/106], [94mLoss[0m : 2.79604

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32101
[1mStep[0m  [10/106], [94mLoss[0m : 2.54173
[1mStep[0m  [20/106], [94mLoss[0m : 2.70552
[1mStep[0m  [30/106], [94mLoss[0m : 2.11308
[1mStep[0m  [40/106], [94mLoss[0m : 2.44248
[1mStep[0m  [50/106], [94mLoss[0m : 2.82403
[1mStep[0m  [60/106], [94mLoss[0m : 2.53790
[1mStep[0m  [70/106], [94mLoss[0m : 2.44558
[1mStep[0m  [80/106], [94mLoss[0m : 2.39580
[1mStep[0m  [90/106], [94mLoss[0m : 2.81028
[1mStep[0m  [100/106], [94mLoss[0m : 2.31193

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.36259
[1mStep[0m  [10/106], [94mLoss[0m : 2.67132
[1mStep[0m  [20/106], [94mLoss[0m : 2.64979
[1mStep[0m  [30/106], [94mLoss[0m : 2.38980
[1mStep[0m  [40/106], [94mLoss[0m : 2.46472
[1mStep[0m  [50/106], [94mLoss[0m : 2.53253
[1mStep[0m  [60/106], [94mLoss[0m : 2.55499
[1mStep[0m  [70/106], [94mLoss[0m : 2.81568
[1mStep[0m  [80/106], [94mLoss[0m : 2.53539
[1mStep[0m  [90/106], [94mLoss[0m : 2.71436
[1mStep[0m  [100/106], [94mLoss[0m : 2.49928

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52719
[1mStep[0m  [10/106], [94mLoss[0m : 2.51867
[1mStep[0m  [20/106], [94mLoss[0m : 2.61722
[1mStep[0m  [30/106], [94mLoss[0m : 2.57592
[1mStep[0m  [40/106], [94mLoss[0m : 2.72159
[1mStep[0m  [50/106], [94mLoss[0m : 2.42247
[1mStep[0m  [60/106], [94mLoss[0m : 2.26199
[1mStep[0m  [70/106], [94mLoss[0m : 2.23913
[1mStep[0m  [80/106], [94mLoss[0m : 2.63779
[1mStep[0m  [90/106], [94mLoss[0m : 2.70695
[1mStep[0m  [100/106], [94mLoss[0m : 2.65976

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29644
[1mStep[0m  [10/106], [94mLoss[0m : 2.41488
[1mStep[0m  [20/106], [94mLoss[0m : 2.56365
[1mStep[0m  [30/106], [94mLoss[0m : 2.37202
[1mStep[0m  [40/106], [94mLoss[0m : 2.60264
[1mStep[0m  [50/106], [94mLoss[0m : 2.79796
[1mStep[0m  [60/106], [94mLoss[0m : 2.48671
[1mStep[0m  [70/106], [94mLoss[0m : 2.33284
[1mStep[0m  [80/106], [94mLoss[0m : 2.37674
[1mStep[0m  [90/106], [94mLoss[0m : 2.59443
[1mStep[0m  [100/106], [94mLoss[0m : 2.40902

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38207
[1mStep[0m  [10/106], [94mLoss[0m : 2.44533
[1mStep[0m  [20/106], [94mLoss[0m : 2.36219
[1mStep[0m  [30/106], [94mLoss[0m : 2.32689
[1mStep[0m  [40/106], [94mLoss[0m : 2.30154
[1mStep[0m  [50/106], [94mLoss[0m : 2.41500
[1mStep[0m  [60/106], [94mLoss[0m : 2.41289
[1mStep[0m  [70/106], [94mLoss[0m : 2.51732
[1mStep[0m  [80/106], [94mLoss[0m : 2.59645
[1mStep[0m  [90/106], [94mLoss[0m : 2.44579
[1mStep[0m  [100/106], [94mLoss[0m : 2.72831

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34954
[1mStep[0m  [10/106], [94mLoss[0m : 2.43298
[1mStep[0m  [20/106], [94mLoss[0m : 2.59219
[1mStep[0m  [30/106], [94mLoss[0m : 2.55521
[1mStep[0m  [40/106], [94mLoss[0m : 2.67700
[1mStep[0m  [50/106], [94mLoss[0m : 2.70964
[1mStep[0m  [60/106], [94mLoss[0m : 2.64085
[1mStep[0m  [70/106], [94mLoss[0m : 2.28266
[1mStep[0m  [80/106], [94mLoss[0m : 2.65723
[1mStep[0m  [90/106], [94mLoss[0m : 2.36112
[1mStep[0m  [100/106], [94mLoss[0m : 2.40120

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54757
[1mStep[0m  [10/106], [94mLoss[0m : 2.12141
[1mStep[0m  [20/106], [94mLoss[0m : 2.41007
[1mStep[0m  [30/106], [94mLoss[0m : 2.51591
[1mStep[0m  [40/106], [94mLoss[0m : 2.32485
[1mStep[0m  [50/106], [94mLoss[0m : 2.34049
[1mStep[0m  [60/106], [94mLoss[0m : 2.36669
[1mStep[0m  [70/106], [94mLoss[0m : 2.37862
[1mStep[0m  [80/106], [94mLoss[0m : 2.65820
[1mStep[0m  [90/106], [94mLoss[0m : 2.43567
[1mStep[0m  [100/106], [94mLoss[0m : 2.71397

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71639
[1mStep[0m  [10/106], [94mLoss[0m : 2.64187
[1mStep[0m  [20/106], [94mLoss[0m : 2.28234
[1mStep[0m  [30/106], [94mLoss[0m : 2.53310
[1mStep[0m  [40/106], [94mLoss[0m : 2.19390
[1mStep[0m  [50/106], [94mLoss[0m : 2.49173
[1mStep[0m  [60/106], [94mLoss[0m : 2.37135
[1mStep[0m  [70/106], [94mLoss[0m : 2.44854
[1mStep[0m  [80/106], [94mLoss[0m : 2.50813
[1mStep[0m  [90/106], [94mLoss[0m : 2.63438
[1mStep[0m  [100/106], [94mLoss[0m : 2.60359

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90417
[1mStep[0m  [10/106], [94mLoss[0m : 2.61739
[1mStep[0m  [20/106], [94mLoss[0m : 2.26146
[1mStep[0m  [30/106], [94mLoss[0m : 2.46281
[1mStep[0m  [40/106], [94mLoss[0m : 2.40556
[1mStep[0m  [50/106], [94mLoss[0m : 2.26684
[1mStep[0m  [60/106], [94mLoss[0m : 2.57638
[1mStep[0m  [70/106], [94mLoss[0m : 2.56280
[1mStep[0m  [80/106], [94mLoss[0m : 2.14232
[1mStep[0m  [90/106], [94mLoss[0m : 2.93656
[1mStep[0m  [100/106], [94mLoss[0m : 2.76136

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64991
[1mStep[0m  [10/106], [94mLoss[0m : 2.39543
[1mStep[0m  [20/106], [94mLoss[0m : 2.33698
[1mStep[0m  [30/106], [94mLoss[0m : 2.31668
[1mStep[0m  [40/106], [94mLoss[0m : 2.23539
[1mStep[0m  [50/106], [94mLoss[0m : 2.51639
[1mStep[0m  [60/106], [94mLoss[0m : 2.69212
[1mStep[0m  [70/106], [94mLoss[0m : 2.47704
[1mStep[0m  [80/106], [94mLoss[0m : 2.17791
[1mStep[0m  [90/106], [94mLoss[0m : 2.31687
[1mStep[0m  [100/106], [94mLoss[0m : 2.43789

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57621
[1mStep[0m  [10/106], [94mLoss[0m : 2.55891
[1mStep[0m  [20/106], [94mLoss[0m : 2.54411
[1mStep[0m  [30/106], [94mLoss[0m : 2.57459
[1mStep[0m  [40/106], [94mLoss[0m : 2.58738
[1mStep[0m  [50/106], [94mLoss[0m : 2.39468
[1mStep[0m  [60/106], [94mLoss[0m : 2.42725
[1mStep[0m  [70/106], [94mLoss[0m : 2.60439
[1mStep[0m  [80/106], [94mLoss[0m : 2.36269
[1mStep[0m  [90/106], [94mLoss[0m : 2.40641
[1mStep[0m  [100/106], [94mLoss[0m : 2.21745

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56168
[1mStep[0m  [10/106], [94mLoss[0m : 2.37391
[1mStep[0m  [20/106], [94mLoss[0m : 2.62355
[1mStep[0m  [30/106], [94mLoss[0m : 2.38371
[1mStep[0m  [40/106], [94mLoss[0m : 2.79282
[1mStep[0m  [50/106], [94mLoss[0m : 2.54858
[1mStep[0m  [60/106], [94mLoss[0m : 2.73190
[1mStep[0m  [70/106], [94mLoss[0m : 2.43578
[1mStep[0m  [80/106], [94mLoss[0m : 2.55857
[1mStep[0m  [90/106], [94mLoss[0m : 2.51024
[1mStep[0m  [100/106], [94mLoss[0m : 2.78457

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17807
[1mStep[0m  [10/106], [94mLoss[0m : 2.73521
[1mStep[0m  [20/106], [94mLoss[0m : 2.67322
[1mStep[0m  [30/106], [94mLoss[0m : 2.32259
[1mStep[0m  [40/106], [94mLoss[0m : 2.70256
[1mStep[0m  [50/106], [94mLoss[0m : 2.27986
[1mStep[0m  [60/106], [94mLoss[0m : 2.35744
[1mStep[0m  [70/106], [94mLoss[0m : 2.52026
[1mStep[0m  [80/106], [94mLoss[0m : 2.33246
[1mStep[0m  [90/106], [94mLoss[0m : 2.46534
[1mStep[0m  [100/106], [94mLoss[0m : 2.62919

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80101
[1mStep[0m  [10/106], [94mLoss[0m : 2.65993
[1mStep[0m  [20/106], [94mLoss[0m : 2.49807
[1mStep[0m  [30/106], [94mLoss[0m : 2.52139
[1mStep[0m  [40/106], [94mLoss[0m : 2.28696
[1mStep[0m  [50/106], [94mLoss[0m : 2.54187
[1mStep[0m  [60/106], [94mLoss[0m : 2.44542
[1mStep[0m  [70/106], [94mLoss[0m : 2.86565
[1mStep[0m  [80/106], [94mLoss[0m : 2.70234
[1mStep[0m  [90/106], [94mLoss[0m : 2.28768
[1mStep[0m  [100/106], [94mLoss[0m : 2.36040

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.382, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54168
[1mStep[0m  [10/106], [94mLoss[0m : 2.43121
[1mStep[0m  [20/106], [94mLoss[0m : 2.64511
[1mStep[0m  [30/106], [94mLoss[0m : 2.49736
[1mStep[0m  [40/106], [94mLoss[0m : 2.31985
[1mStep[0m  [50/106], [94mLoss[0m : 2.43190
[1mStep[0m  [60/106], [94mLoss[0m : 2.56933
[1mStep[0m  [70/106], [94mLoss[0m : 2.65944
[1mStep[0m  [80/106], [94mLoss[0m : 2.50340
[1mStep[0m  [90/106], [94mLoss[0m : 2.46487
[1mStep[0m  [100/106], [94mLoss[0m : 2.71257

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.83781
[1mStep[0m  [10/106], [94mLoss[0m : 2.59625
[1mStep[0m  [20/106], [94mLoss[0m : 2.56821
[1mStep[0m  [30/106], [94mLoss[0m : 3.03217
[1mStep[0m  [40/106], [94mLoss[0m : 2.69876
[1mStep[0m  [50/106], [94mLoss[0m : 2.50937
[1mStep[0m  [60/106], [94mLoss[0m : 2.57049
[1mStep[0m  [70/106], [94mLoss[0m : 2.27885
[1mStep[0m  [80/106], [94mLoss[0m : 2.60327
[1mStep[0m  [90/106], [94mLoss[0m : 2.63678
[1mStep[0m  [100/106], [94mLoss[0m : 2.38941

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50226
[1mStep[0m  [10/106], [94mLoss[0m : 2.43041
[1mStep[0m  [20/106], [94mLoss[0m : 2.28200
[1mStep[0m  [30/106], [94mLoss[0m : 2.26736
[1mStep[0m  [40/106], [94mLoss[0m : 2.21611
[1mStep[0m  [50/106], [94mLoss[0m : 2.22248
[1mStep[0m  [60/106], [94mLoss[0m : 2.59252
[1mStep[0m  [70/106], [94mLoss[0m : 2.79737
[1mStep[0m  [80/106], [94mLoss[0m : 2.65993
[1mStep[0m  [90/106], [94mLoss[0m : 2.53958
[1mStep[0m  [100/106], [94mLoss[0m : 2.34972

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60149
[1mStep[0m  [10/106], [94mLoss[0m : 2.28253
[1mStep[0m  [20/106], [94mLoss[0m : 2.52508
[1mStep[0m  [30/106], [94mLoss[0m : 2.51882
[1mStep[0m  [40/106], [94mLoss[0m : 2.27334
[1mStep[0m  [50/106], [94mLoss[0m : 2.42714
[1mStep[0m  [60/106], [94mLoss[0m : 2.12934
[1mStep[0m  [70/106], [94mLoss[0m : 2.05666
[1mStep[0m  [80/106], [94mLoss[0m : 2.21972
[1mStep[0m  [90/106], [94mLoss[0m : 2.66960
[1mStep[0m  [100/106], [94mLoss[0m : 2.65487

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65900
[1mStep[0m  [10/106], [94mLoss[0m : 2.61473
[1mStep[0m  [20/106], [94mLoss[0m : 2.11071
[1mStep[0m  [30/106], [94mLoss[0m : 2.66706
[1mStep[0m  [40/106], [94mLoss[0m : 2.49871
[1mStep[0m  [50/106], [94mLoss[0m : 2.52806
[1mStep[0m  [60/106], [94mLoss[0m : 2.50920
[1mStep[0m  [70/106], [94mLoss[0m : 2.34513
[1mStep[0m  [80/106], [94mLoss[0m : 2.51101
[1mStep[0m  [90/106], [94mLoss[0m : 2.23495
[1mStep[0m  [100/106], [94mLoss[0m : 2.31994

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37088
[1mStep[0m  [10/106], [94mLoss[0m : 2.59326
[1mStep[0m  [20/106], [94mLoss[0m : 2.30205
[1mStep[0m  [30/106], [94mLoss[0m : 2.09026
[1mStep[0m  [40/106], [94mLoss[0m : 2.33730
[1mStep[0m  [50/106], [94mLoss[0m : 2.49237
[1mStep[0m  [60/106], [94mLoss[0m : 2.26728
[1mStep[0m  [70/106], [94mLoss[0m : 2.28822
[1mStep[0m  [80/106], [94mLoss[0m : 2.59706
[1mStep[0m  [90/106], [94mLoss[0m : 2.65955
[1mStep[0m  [100/106], [94mLoss[0m : 2.43117

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14025
[1mStep[0m  [10/106], [94mLoss[0m : 2.60375
[1mStep[0m  [20/106], [94mLoss[0m : 2.19825
[1mStep[0m  [30/106], [94mLoss[0m : 2.48940
[1mStep[0m  [40/106], [94mLoss[0m : 2.38021
[1mStep[0m  [50/106], [94mLoss[0m : 2.36185
[1mStep[0m  [60/106], [94mLoss[0m : 2.50476
[1mStep[0m  [70/106], [94mLoss[0m : 2.32916
[1mStep[0m  [80/106], [94mLoss[0m : 2.38013
[1mStep[0m  [90/106], [94mLoss[0m : 2.57388
[1mStep[0m  [100/106], [94mLoss[0m : 2.71013

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.383, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43325
[1mStep[0m  [10/106], [94mLoss[0m : 2.41445
[1mStep[0m  [20/106], [94mLoss[0m : 2.51901
[1mStep[0m  [30/106], [94mLoss[0m : 2.61735
[1mStep[0m  [40/106], [94mLoss[0m : 2.17395
[1mStep[0m  [50/106], [94mLoss[0m : 2.38803
[1mStep[0m  [60/106], [94mLoss[0m : 2.39249
[1mStep[0m  [70/106], [94mLoss[0m : 2.40551
[1mStep[0m  [80/106], [94mLoss[0m : 2.28639
[1mStep[0m  [90/106], [94mLoss[0m : 2.18721
[1mStep[0m  [100/106], [94mLoss[0m : 2.51817

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.363, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56220
[1mStep[0m  [10/106], [94mLoss[0m : 2.43444
[1mStep[0m  [20/106], [94mLoss[0m : 2.33140
[1mStep[0m  [30/106], [94mLoss[0m : 2.72291
[1mStep[0m  [40/106], [94mLoss[0m : 2.15019
[1mStep[0m  [50/106], [94mLoss[0m : 2.28851
[1mStep[0m  [60/106], [94mLoss[0m : 3.13455
[1mStep[0m  [70/106], [94mLoss[0m : 2.30530
[1mStep[0m  [80/106], [94mLoss[0m : 2.28672
[1mStep[0m  [90/106], [94mLoss[0m : 2.51097
[1mStep[0m  [100/106], [94mLoss[0m : 2.48665

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.381, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.381
====================================

Phase 1 - Evaluation MAE:  2.3812716592032954
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.33156
[1mStep[0m  [10/106], [94mLoss[0m : 2.68311
[1mStep[0m  [20/106], [94mLoss[0m : 2.89078
[1mStep[0m  [30/106], [94mLoss[0m : 2.28546
[1mStep[0m  [40/106], [94mLoss[0m : 2.83969
[1mStep[0m  [50/106], [94mLoss[0m : 2.17043
[1mStep[0m  [60/106], [94mLoss[0m : 2.25162
[1mStep[0m  [70/106], [94mLoss[0m : 2.77953
[1mStep[0m  [80/106], [94mLoss[0m : 2.27133
[1mStep[0m  [90/106], [94mLoss[0m : 2.36190
[1mStep[0m  [100/106], [94mLoss[0m : 2.38098

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59102
[1mStep[0m  [10/106], [94mLoss[0m : 2.27201
[1mStep[0m  [20/106], [94mLoss[0m : 2.49859
[1mStep[0m  [30/106], [94mLoss[0m : 2.95966
[1mStep[0m  [40/106], [94mLoss[0m : 2.80636
[1mStep[0m  [50/106], [94mLoss[0m : 2.54904
[1mStep[0m  [60/106], [94mLoss[0m : 2.39721
[1mStep[0m  [70/106], [94mLoss[0m : 2.40782
[1mStep[0m  [80/106], [94mLoss[0m : 2.26814
[1mStep[0m  [90/106], [94mLoss[0m : 2.60646
[1mStep[0m  [100/106], [94mLoss[0m : 2.38826

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.605, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43887
[1mStep[0m  [10/106], [94mLoss[0m : 2.65118
[1mStep[0m  [20/106], [94mLoss[0m : 2.21987
[1mStep[0m  [30/106], [94mLoss[0m : 2.42196
[1mStep[0m  [40/106], [94mLoss[0m : 2.42583
[1mStep[0m  [50/106], [94mLoss[0m : 2.35515
[1mStep[0m  [60/106], [94mLoss[0m : 2.28440
[1mStep[0m  [70/106], [94mLoss[0m : 2.68815
[1mStep[0m  [80/106], [94mLoss[0m : 2.21953
[1mStep[0m  [90/106], [94mLoss[0m : 2.13971
[1mStep[0m  [100/106], [94mLoss[0m : 2.59172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28039
[1mStep[0m  [10/106], [94mLoss[0m : 2.25207
[1mStep[0m  [20/106], [94mLoss[0m : 2.33852
[1mStep[0m  [30/106], [94mLoss[0m : 2.24715
[1mStep[0m  [40/106], [94mLoss[0m : 2.19833
[1mStep[0m  [50/106], [94mLoss[0m : 2.91886
[1mStep[0m  [60/106], [94mLoss[0m : 2.10090
[1mStep[0m  [70/106], [94mLoss[0m : 2.41499
[1mStep[0m  [80/106], [94mLoss[0m : 2.53591
[1mStep[0m  [90/106], [94mLoss[0m : 2.13608
[1mStep[0m  [100/106], [94mLoss[0m : 2.25859

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.757, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31879
[1mStep[0m  [10/106], [94mLoss[0m : 2.07891
[1mStep[0m  [20/106], [94mLoss[0m : 2.15305
[1mStep[0m  [30/106], [94mLoss[0m : 2.11302
[1mStep[0m  [40/106], [94mLoss[0m : 2.15181
[1mStep[0m  [50/106], [94mLoss[0m : 1.84910
[1mStep[0m  [60/106], [94mLoss[0m : 2.20452
[1mStep[0m  [70/106], [94mLoss[0m : 2.51943
[1mStep[0m  [80/106], [94mLoss[0m : 2.33444
[1mStep[0m  [90/106], [94mLoss[0m : 2.04756
[1mStep[0m  [100/106], [94mLoss[0m : 2.29783

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.582, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24905
[1mStep[0m  [10/106], [94mLoss[0m : 1.88814
[1mStep[0m  [20/106], [94mLoss[0m : 2.08962
[1mStep[0m  [30/106], [94mLoss[0m : 2.18241
[1mStep[0m  [40/106], [94mLoss[0m : 2.34006
[1mStep[0m  [50/106], [94mLoss[0m : 1.93727
[1mStep[0m  [60/106], [94mLoss[0m : 2.33582
[1mStep[0m  [70/106], [94mLoss[0m : 2.18266
[1mStep[0m  [80/106], [94mLoss[0m : 2.07752
[1mStep[0m  [90/106], [94mLoss[0m : 2.32950
[1mStep[0m  [100/106], [94mLoss[0m : 2.38806

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98963
[1mStep[0m  [10/106], [94mLoss[0m : 2.00292
[1mStep[0m  [20/106], [94mLoss[0m : 2.52184
[1mStep[0m  [30/106], [94mLoss[0m : 2.42983
[1mStep[0m  [40/106], [94mLoss[0m : 2.12281
[1mStep[0m  [50/106], [94mLoss[0m : 2.12005
[1mStep[0m  [60/106], [94mLoss[0m : 2.06382
[1mStep[0m  [70/106], [94mLoss[0m : 1.93416
[1mStep[0m  [80/106], [94mLoss[0m : 2.05102
[1mStep[0m  [90/106], [94mLoss[0m : 1.98343
[1mStep[0m  [100/106], [94mLoss[0m : 2.15866

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96858
[1mStep[0m  [10/106], [94mLoss[0m : 2.30281
[1mStep[0m  [20/106], [94mLoss[0m : 1.96943
[1mStep[0m  [30/106], [94mLoss[0m : 2.14571
[1mStep[0m  [40/106], [94mLoss[0m : 2.17639
[1mStep[0m  [50/106], [94mLoss[0m : 2.18871
[1mStep[0m  [60/106], [94mLoss[0m : 1.92067
[1mStep[0m  [70/106], [94mLoss[0m : 2.13976
[1mStep[0m  [80/106], [94mLoss[0m : 2.03120
[1mStep[0m  [90/106], [94mLoss[0m : 2.14787
[1mStep[0m  [100/106], [94mLoss[0m : 2.02339

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82097
[1mStep[0m  [10/106], [94mLoss[0m : 2.28601
[1mStep[0m  [20/106], [94mLoss[0m : 1.74132
[1mStep[0m  [30/106], [94mLoss[0m : 2.06938
[1mStep[0m  [40/106], [94mLoss[0m : 2.06526
[1mStep[0m  [50/106], [94mLoss[0m : 1.77044
[1mStep[0m  [60/106], [94mLoss[0m : 2.09202
[1mStep[0m  [70/106], [94mLoss[0m : 2.12470
[1mStep[0m  [80/106], [94mLoss[0m : 1.89816
[1mStep[0m  [90/106], [94mLoss[0m : 1.74137
[1mStep[0m  [100/106], [94mLoss[0m : 2.20768

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00669
[1mStep[0m  [10/106], [94mLoss[0m : 2.16094
[1mStep[0m  [20/106], [94mLoss[0m : 2.06351
[1mStep[0m  [30/106], [94mLoss[0m : 1.89073
[1mStep[0m  [40/106], [94mLoss[0m : 1.98658
[1mStep[0m  [50/106], [94mLoss[0m : 2.19621
[1mStep[0m  [60/106], [94mLoss[0m : 2.36560
[1mStep[0m  [70/106], [94mLoss[0m : 2.07913
[1mStep[0m  [80/106], [94mLoss[0m : 2.05001
[1mStep[0m  [90/106], [94mLoss[0m : 1.80667
[1mStep[0m  [100/106], [94mLoss[0m : 2.06214

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.93799
[1mStep[0m  [10/106], [94mLoss[0m : 2.17299
[1mStep[0m  [20/106], [94mLoss[0m : 1.88157
[1mStep[0m  [30/106], [94mLoss[0m : 1.88656
[1mStep[0m  [40/106], [94mLoss[0m : 2.02900
[1mStep[0m  [50/106], [94mLoss[0m : 2.17520
[1mStep[0m  [60/106], [94mLoss[0m : 1.97135
[1mStep[0m  [70/106], [94mLoss[0m : 1.98982
[1mStep[0m  [80/106], [94mLoss[0m : 1.70115
[1mStep[0m  [90/106], [94mLoss[0m : 1.84753
[1mStep[0m  [100/106], [94mLoss[0m : 2.24823

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94174
[1mStep[0m  [10/106], [94mLoss[0m : 2.14229
[1mStep[0m  [20/106], [94mLoss[0m : 1.80785
[1mStep[0m  [30/106], [94mLoss[0m : 1.71024
[1mStep[0m  [40/106], [94mLoss[0m : 1.90703
[1mStep[0m  [50/106], [94mLoss[0m : 2.18196
[1mStep[0m  [60/106], [94mLoss[0m : 2.00988
[1mStep[0m  [70/106], [94mLoss[0m : 1.93203
[1mStep[0m  [80/106], [94mLoss[0m : 1.84325
[1mStep[0m  [90/106], [94mLoss[0m : 1.85604
[1mStep[0m  [100/106], [94mLoss[0m : 1.78725

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95863
[1mStep[0m  [10/106], [94mLoss[0m : 1.64130
[1mStep[0m  [20/106], [94mLoss[0m : 1.92435
[1mStep[0m  [30/106], [94mLoss[0m : 1.87581
[1mStep[0m  [40/106], [94mLoss[0m : 2.11875
[1mStep[0m  [50/106], [94mLoss[0m : 1.85739
[1mStep[0m  [60/106], [94mLoss[0m : 2.21406
[1mStep[0m  [70/106], [94mLoss[0m : 1.92957
[1mStep[0m  [80/106], [94mLoss[0m : 1.91762
[1mStep[0m  [90/106], [94mLoss[0m : 1.96459
[1mStep[0m  [100/106], [94mLoss[0m : 2.04474

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06222
[1mStep[0m  [10/106], [94mLoss[0m : 1.83570
[1mStep[0m  [20/106], [94mLoss[0m : 1.84562
[1mStep[0m  [30/106], [94mLoss[0m : 1.78066
[1mStep[0m  [40/106], [94mLoss[0m : 1.90426
[1mStep[0m  [50/106], [94mLoss[0m : 2.05917
[1mStep[0m  [60/106], [94mLoss[0m : 1.59191
[1mStep[0m  [70/106], [94mLoss[0m : 1.76841
[1mStep[0m  [80/106], [94mLoss[0m : 1.65168
[1mStep[0m  [90/106], [94mLoss[0m : 2.01514
[1mStep[0m  [100/106], [94mLoss[0m : 1.77966

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85173
[1mStep[0m  [10/106], [94mLoss[0m : 1.69826
[1mStep[0m  [20/106], [94mLoss[0m : 1.70446
[1mStep[0m  [30/106], [94mLoss[0m : 1.77910
[1mStep[0m  [40/106], [94mLoss[0m : 1.91513
[1mStep[0m  [50/106], [94mLoss[0m : 1.96870
[1mStep[0m  [60/106], [94mLoss[0m : 2.07827
[1mStep[0m  [70/106], [94mLoss[0m : 1.85442
[1mStep[0m  [80/106], [94mLoss[0m : 2.11533
[1mStep[0m  [90/106], [94mLoss[0m : 1.91278
[1mStep[0m  [100/106], [94mLoss[0m : 1.85797

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.820, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85795
[1mStep[0m  [10/106], [94mLoss[0m : 1.89733
[1mStep[0m  [20/106], [94mLoss[0m : 1.77942
[1mStep[0m  [30/106], [94mLoss[0m : 1.88299
[1mStep[0m  [40/106], [94mLoss[0m : 1.70370
[1mStep[0m  [50/106], [94mLoss[0m : 1.67279
[1mStep[0m  [60/106], [94mLoss[0m : 1.77303
[1mStep[0m  [70/106], [94mLoss[0m : 1.80112
[1mStep[0m  [80/106], [94mLoss[0m : 1.89272
[1mStep[0m  [90/106], [94mLoss[0m : 1.80522
[1mStep[0m  [100/106], [94mLoss[0m : 1.57345

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92694
[1mStep[0m  [10/106], [94mLoss[0m : 1.91233
[1mStep[0m  [20/106], [94mLoss[0m : 1.68202
[1mStep[0m  [30/106], [94mLoss[0m : 1.73353
[1mStep[0m  [40/106], [94mLoss[0m : 1.60352
[1mStep[0m  [50/106], [94mLoss[0m : 1.97400
[1mStep[0m  [60/106], [94mLoss[0m : 1.73732
[1mStep[0m  [70/106], [94mLoss[0m : 1.92861
[1mStep[0m  [80/106], [94mLoss[0m : 1.48433
[1mStep[0m  [90/106], [94mLoss[0m : 1.55430
[1mStep[0m  [100/106], [94mLoss[0m : 1.78378

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.93436
[1mStep[0m  [10/106], [94mLoss[0m : 1.43752
[1mStep[0m  [20/106], [94mLoss[0m : 1.86362
[1mStep[0m  [30/106], [94mLoss[0m : 1.67301
[1mStep[0m  [40/106], [94mLoss[0m : 1.92461
[1mStep[0m  [50/106], [94mLoss[0m : 1.80234
[1mStep[0m  [60/106], [94mLoss[0m : 1.98203
[1mStep[0m  [70/106], [94mLoss[0m : 1.84776
[1mStep[0m  [80/106], [94mLoss[0m : 1.65754
[1mStep[0m  [90/106], [94mLoss[0m : 1.83620
[1mStep[0m  [100/106], [94mLoss[0m : 1.70906

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.31493
[1mStep[0m  [10/106], [94mLoss[0m : 1.48524
[1mStep[0m  [20/106], [94mLoss[0m : 1.68834
[1mStep[0m  [30/106], [94mLoss[0m : 1.70950
[1mStep[0m  [40/106], [94mLoss[0m : 1.69293
[1mStep[0m  [50/106], [94mLoss[0m : 1.73480
[1mStep[0m  [60/106], [94mLoss[0m : 1.64841
[1mStep[0m  [70/106], [94mLoss[0m : 1.81682
[1mStep[0m  [80/106], [94mLoss[0m : 1.87676
[1mStep[0m  [90/106], [94mLoss[0m : 1.78004
[1mStep[0m  [100/106], [94mLoss[0m : 1.66333

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64131
[1mStep[0m  [10/106], [94mLoss[0m : 1.83707
[1mStep[0m  [20/106], [94mLoss[0m : 1.70584
[1mStep[0m  [30/106], [94mLoss[0m : 1.73050
[1mStep[0m  [40/106], [94mLoss[0m : 1.63055
[1mStep[0m  [50/106], [94mLoss[0m : 1.44748
[1mStep[0m  [60/106], [94mLoss[0m : 1.53301
[1mStep[0m  [70/106], [94mLoss[0m : 1.68557
[1mStep[0m  [80/106], [94mLoss[0m : 1.77404
[1mStep[0m  [90/106], [94mLoss[0m : 1.68043
[1mStep[0m  [100/106], [94mLoss[0m : 1.68415

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58983
[1mStep[0m  [10/106], [94mLoss[0m : 1.68831
[1mStep[0m  [20/106], [94mLoss[0m : 1.58658
[1mStep[0m  [30/106], [94mLoss[0m : 1.49873
[1mStep[0m  [40/106], [94mLoss[0m : 1.61053
[1mStep[0m  [50/106], [94mLoss[0m : 1.47349
[1mStep[0m  [60/106], [94mLoss[0m : 1.49278
[1mStep[0m  [70/106], [94mLoss[0m : 1.82795
[1mStep[0m  [80/106], [94mLoss[0m : 1.69392
[1mStep[0m  [90/106], [94mLoss[0m : 1.58580
[1mStep[0m  [100/106], [94mLoss[0m : 1.54282

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.437, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53574
[1mStep[0m  [10/106], [94mLoss[0m : 1.52969
[1mStep[0m  [20/106], [94mLoss[0m : 1.57368
[1mStep[0m  [30/106], [94mLoss[0m : 1.70293
[1mStep[0m  [40/106], [94mLoss[0m : 1.82007
[1mStep[0m  [50/106], [94mLoss[0m : 1.49477
[1mStep[0m  [60/106], [94mLoss[0m : 1.59712
[1mStep[0m  [70/106], [94mLoss[0m : 1.65010
[1mStep[0m  [80/106], [94mLoss[0m : 1.62541
[1mStep[0m  [90/106], [94mLoss[0m : 1.47871
[1mStep[0m  [100/106], [94mLoss[0m : 1.71977

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42692
[1mStep[0m  [10/106], [94mLoss[0m : 1.69434
[1mStep[0m  [20/106], [94mLoss[0m : 1.40258
[1mStep[0m  [30/106], [94mLoss[0m : 1.75606
[1mStep[0m  [40/106], [94mLoss[0m : 1.48886
[1mStep[0m  [50/106], [94mLoss[0m : 1.72211
[1mStep[0m  [60/106], [94mLoss[0m : 1.50526
[1mStep[0m  [70/106], [94mLoss[0m : 1.62075
[1mStep[0m  [80/106], [94mLoss[0m : 1.43673
[1mStep[0m  [90/106], [94mLoss[0m : 1.70418
[1mStep[0m  [100/106], [94mLoss[0m : 1.77856

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.483, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53634
[1mStep[0m  [10/106], [94mLoss[0m : 1.45724
[1mStep[0m  [20/106], [94mLoss[0m : 1.43666
[1mStep[0m  [30/106], [94mLoss[0m : 1.35260
[1mStep[0m  [40/106], [94mLoss[0m : 1.67414
[1mStep[0m  [50/106], [94mLoss[0m : 1.63375
[1mStep[0m  [60/106], [94mLoss[0m : 1.68992
[1mStep[0m  [70/106], [94mLoss[0m : 1.33198
[1mStep[0m  [80/106], [94mLoss[0m : 1.67153
[1mStep[0m  [90/106], [94mLoss[0m : 1.59060
[1mStep[0m  [100/106], [94mLoss[0m : 1.56485

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.555, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.47417
[1mStep[0m  [10/106], [94mLoss[0m : 1.77622
[1mStep[0m  [20/106], [94mLoss[0m : 1.55935
[1mStep[0m  [30/106], [94mLoss[0m : 1.46389
[1mStep[0m  [40/106], [94mLoss[0m : 1.47154
[1mStep[0m  [50/106], [94mLoss[0m : 1.71695
[1mStep[0m  [60/106], [94mLoss[0m : 1.38261
[1mStep[0m  [70/106], [94mLoss[0m : 1.62657
[1mStep[0m  [80/106], [94mLoss[0m : 1.55574
[1mStep[0m  [90/106], [94mLoss[0m : 1.53933
[1mStep[0m  [100/106], [94mLoss[0m : 1.50569

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.38704
[1mStep[0m  [10/106], [94mLoss[0m : 1.71285
[1mStep[0m  [20/106], [94mLoss[0m : 1.37693
[1mStep[0m  [30/106], [94mLoss[0m : 1.64323
[1mStep[0m  [40/106], [94mLoss[0m : 1.51849
[1mStep[0m  [50/106], [94mLoss[0m : 1.53834
[1mStep[0m  [60/106], [94mLoss[0m : 1.58733
[1mStep[0m  [70/106], [94mLoss[0m : 1.52416
[1mStep[0m  [80/106], [94mLoss[0m : 1.66274
[1mStep[0m  [90/106], [94mLoss[0m : 1.52713
[1mStep[0m  [100/106], [94mLoss[0m : 1.35163

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.520, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60027
[1mStep[0m  [10/106], [94mLoss[0m : 1.47827
[1mStep[0m  [20/106], [94mLoss[0m : 1.52382
[1mStep[0m  [30/106], [94mLoss[0m : 1.47103
[1mStep[0m  [40/106], [94mLoss[0m : 1.41094
[1mStep[0m  [50/106], [94mLoss[0m : 1.17388
[1mStep[0m  [60/106], [94mLoss[0m : 1.45898
[1mStep[0m  [70/106], [94mLoss[0m : 1.61920
[1mStep[0m  [80/106], [94mLoss[0m : 1.54486
[1mStep[0m  [90/106], [94mLoss[0m : 1.53571
[1mStep[0m  [100/106], [94mLoss[0m : 1.56127

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.486, [92mTest[0m: 2.447, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.37434
[1mStep[0m  [10/106], [94mLoss[0m : 1.61503
[1mStep[0m  [20/106], [94mLoss[0m : 1.79890
[1mStep[0m  [30/106], [94mLoss[0m : 1.47354
[1mStep[0m  [40/106], [94mLoss[0m : 1.45735
[1mStep[0m  [50/106], [94mLoss[0m : 1.58132
[1mStep[0m  [60/106], [94mLoss[0m : 1.33341
[1mStep[0m  [70/106], [94mLoss[0m : 1.60526
[1mStep[0m  [80/106], [94mLoss[0m : 1.72979
[1mStep[0m  [90/106], [94mLoss[0m : 1.32191
[1mStep[0m  [100/106], [94mLoss[0m : 1.57130

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.485, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.24044
[1mStep[0m  [10/106], [94mLoss[0m : 1.25495
[1mStep[0m  [20/106], [94mLoss[0m : 1.66155
[1mStep[0m  [30/106], [94mLoss[0m : 1.34893
[1mStep[0m  [40/106], [94mLoss[0m : 1.31362
[1mStep[0m  [50/106], [94mLoss[0m : 1.46402
[1mStep[0m  [60/106], [94mLoss[0m : 1.66623
[1mStep[0m  [70/106], [94mLoss[0m : 1.51383
[1mStep[0m  [80/106], [94mLoss[0m : 1.46645
[1mStep[0m  [90/106], [94mLoss[0m : 1.55387
[1mStep[0m  [100/106], [94mLoss[0m : 1.46253

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.34663
[1mStep[0m  [10/106], [94mLoss[0m : 1.55596
[1mStep[0m  [20/106], [94mLoss[0m : 1.37631
[1mStep[0m  [30/106], [94mLoss[0m : 1.44343
[1mStep[0m  [40/106], [94mLoss[0m : 1.34934
[1mStep[0m  [50/106], [94mLoss[0m : 1.39127
[1mStep[0m  [60/106], [94mLoss[0m : 1.31116
[1mStep[0m  [70/106], [94mLoss[0m : 1.34962
[1mStep[0m  [80/106], [94mLoss[0m : 1.75792
[1mStep[0m  [90/106], [94mLoss[0m : 1.35998
[1mStep[0m  [100/106], [94mLoss[0m : 1.37604

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.450
====================================

Phase 2 - Evaluation MAE:  2.4496583173859796
MAE score P1       2.381272
MAE score P2       2.449658
loss               1.443929
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 11.32262
[1mStep[0m  [5/53], [94mLoss[0m : 10.18493
[1mStep[0m  [10/53], [94mLoss[0m : 7.73034
[1mStep[0m  [15/53], [94mLoss[0m : 4.93157
[1mStep[0m  [20/53], [94mLoss[0m : 2.96843
[1mStep[0m  [25/53], [94mLoss[0m : 3.25508
[1mStep[0m  [30/53], [94mLoss[0m : 3.17293
[1mStep[0m  [35/53], [94mLoss[0m : 3.14823
[1mStep[0m  [40/53], [94mLoss[0m : 2.64591
[1mStep[0m  [45/53], [94mLoss[0m : 2.75762
[1mStep[0m  [50/53], [94mLoss[0m : 2.61292

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.678, [92mTest[0m: 10.960, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.69209
[1mStep[0m  [5/53], [94mLoss[0m : 2.69103
[1mStep[0m  [10/53], [94mLoss[0m : 2.67982
[1mStep[0m  [15/53], [94mLoss[0m : 2.53517
[1mStep[0m  [20/53], [94mLoss[0m : 2.42260
[1mStep[0m  [25/53], [94mLoss[0m : 2.54905
[1mStep[0m  [30/53], [94mLoss[0m : 2.55768
[1mStep[0m  [35/53], [94mLoss[0m : 2.38324
[1mStep[0m  [40/53], [94mLoss[0m : 2.49110
[1mStep[0m  [45/53], [94mLoss[0m : 2.37296
[1mStep[0m  [50/53], [94mLoss[0m : 2.46688

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.984, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.62106
[1mStep[0m  [5/53], [94mLoss[0m : 2.41000
[1mStep[0m  [10/53], [94mLoss[0m : 2.54149
[1mStep[0m  [15/53], [94mLoss[0m : 2.53290
[1mStep[0m  [20/53], [94mLoss[0m : 2.52857
[1mStep[0m  [25/53], [94mLoss[0m : 2.82472
[1mStep[0m  [30/53], [94mLoss[0m : 2.68534
[1mStep[0m  [35/53], [94mLoss[0m : 2.65753
[1mStep[0m  [40/53], [94mLoss[0m : 2.64439
[1mStep[0m  [45/53], [94mLoss[0m : 2.81980
[1mStep[0m  [50/53], [94mLoss[0m : 2.40270

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37766
[1mStep[0m  [5/53], [94mLoss[0m : 2.71040
[1mStep[0m  [10/53], [94mLoss[0m : 2.45847
[1mStep[0m  [15/53], [94mLoss[0m : 2.52653
[1mStep[0m  [20/53], [94mLoss[0m : 2.46101
[1mStep[0m  [25/53], [94mLoss[0m : 2.68136
[1mStep[0m  [30/53], [94mLoss[0m : 2.50063
[1mStep[0m  [35/53], [94mLoss[0m : 2.57018
[1mStep[0m  [40/53], [94mLoss[0m : 2.46914
[1mStep[0m  [45/53], [94mLoss[0m : 2.27189
[1mStep[0m  [50/53], [94mLoss[0m : 2.44978

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50684
[1mStep[0m  [5/53], [94mLoss[0m : 2.45795
[1mStep[0m  [10/53], [94mLoss[0m : 2.39036
[1mStep[0m  [15/53], [94mLoss[0m : 2.66484
[1mStep[0m  [20/53], [94mLoss[0m : 2.44850
[1mStep[0m  [25/53], [94mLoss[0m : 2.25834
[1mStep[0m  [30/53], [94mLoss[0m : 2.43830
[1mStep[0m  [35/53], [94mLoss[0m : 2.38869
[1mStep[0m  [40/53], [94mLoss[0m : 2.39022
[1mStep[0m  [45/53], [94mLoss[0m : 2.66829
[1mStep[0m  [50/53], [94mLoss[0m : 2.25685

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58230
[1mStep[0m  [5/53], [94mLoss[0m : 2.76846
[1mStep[0m  [10/53], [94mLoss[0m : 2.33919
[1mStep[0m  [15/53], [94mLoss[0m : 2.43859
[1mStep[0m  [20/53], [94mLoss[0m : 2.59724
[1mStep[0m  [25/53], [94mLoss[0m : 2.58759
[1mStep[0m  [30/53], [94mLoss[0m : 2.55076
[1mStep[0m  [35/53], [94mLoss[0m : 2.58089
[1mStep[0m  [40/53], [94mLoss[0m : 2.41785
[1mStep[0m  [45/53], [94mLoss[0m : 2.60871
[1mStep[0m  [50/53], [94mLoss[0m : 2.36186

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.68366
[1mStep[0m  [5/53], [94mLoss[0m : 2.41532
[1mStep[0m  [10/53], [94mLoss[0m : 2.48202
[1mStep[0m  [15/53], [94mLoss[0m : 2.56085
[1mStep[0m  [20/53], [94mLoss[0m : 2.32469
[1mStep[0m  [25/53], [94mLoss[0m : 2.19806
[1mStep[0m  [30/53], [94mLoss[0m : 2.37306
[1mStep[0m  [35/53], [94mLoss[0m : 2.27097
[1mStep[0m  [40/53], [94mLoss[0m : 2.37804
[1mStep[0m  [45/53], [94mLoss[0m : 2.36870
[1mStep[0m  [50/53], [94mLoss[0m : 2.64384

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35311
[1mStep[0m  [5/53], [94mLoss[0m : 2.34966
[1mStep[0m  [10/53], [94mLoss[0m : 2.32772
[1mStep[0m  [15/53], [94mLoss[0m : 2.67689
[1mStep[0m  [20/53], [94mLoss[0m : 2.63749
[1mStep[0m  [25/53], [94mLoss[0m : 2.01235
[1mStep[0m  [30/53], [94mLoss[0m : 2.56142
[1mStep[0m  [35/53], [94mLoss[0m : 2.40849
[1mStep[0m  [40/53], [94mLoss[0m : 2.37034
[1mStep[0m  [45/53], [94mLoss[0m : 2.46289
[1mStep[0m  [50/53], [94mLoss[0m : 2.33321

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24971
[1mStep[0m  [5/53], [94mLoss[0m : 2.39314
[1mStep[0m  [10/53], [94mLoss[0m : 2.53885
[1mStep[0m  [15/53], [94mLoss[0m : 2.46351
[1mStep[0m  [20/53], [94mLoss[0m : 2.53845
[1mStep[0m  [25/53], [94mLoss[0m : 2.43004
[1mStep[0m  [30/53], [94mLoss[0m : 2.52927
[1mStep[0m  [35/53], [94mLoss[0m : 2.46615
[1mStep[0m  [40/53], [94mLoss[0m : 2.21947
[1mStep[0m  [45/53], [94mLoss[0m : 2.50671
[1mStep[0m  [50/53], [94mLoss[0m : 2.31815

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27496
[1mStep[0m  [5/53], [94mLoss[0m : 2.35944
[1mStep[0m  [10/53], [94mLoss[0m : 2.40532
[1mStep[0m  [15/53], [94mLoss[0m : 2.53805
[1mStep[0m  [20/53], [94mLoss[0m : 2.40851
[1mStep[0m  [25/53], [94mLoss[0m : 2.14172
[1mStep[0m  [30/53], [94mLoss[0m : 2.39570
[1mStep[0m  [35/53], [94mLoss[0m : 2.34956
[1mStep[0m  [40/53], [94mLoss[0m : 2.38447
[1mStep[0m  [45/53], [94mLoss[0m : 2.28323
[1mStep[0m  [50/53], [94mLoss[0m : 2.38259

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39008
[1mStep[0m  [5/53], [94mLoss[0m : 2.55561
[1mStep[0m  [10/53], [94mLoss[0m : 2.40714
[1mStep[0m  [15/53], [94mLoss[0m : 2.44522
[1mStep[0m  [20/53], [94mLoss[0m : 2.54985
[1mStep[0m  [25/53], [94mLoss[0m : 2.52155
[1mStep[0m  [30/53], [94mLoss[0m : 2.38436
[1mStep[0m  [35/53], [94mLoss[0m : 2.41911
[1mStep[0m  [40/53], [94mLoss[0m : 2.51266
[1mStep[0m  [45/53], [94mLoss[0m : 2.23092
[1mStep[0m  [50/53], [94mLoss[0m : 2.37009

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37823
[1mStep[0m  [5/53], [94mLoss[0m : 2.48893
[1mStep[0m  [10/53], [94mLoss[0m : 2.42568
[1mStep[0m  [15/53], [94mLoss[0m : 2.50782
[1mStep[0m  [20/53], [94mLoss[0m : 2.37111
[1mStep[0m  [25/53], [94mLoss[0m : 2.39252
[1mStep[0m  [30/53], [94mLoss[0m : 2.31137
[1mStep[0m  [35/53], [94mLoss[0m : 2.50136
[1mStep[0m  [40/53], [94mLoss[0m : 2.36263
[1mStep[0m  [45/53], [94mLoss[0m : 2.32497
[1mStep[0m  [50/53], [94mLoss[0m : 2.26266

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48410
[1mStep[0m  [5/53], [94mLoss[0m : 2.40284
[1mStep[0m  [10/53], [94mLoss[0m : 2.34713
[1mStep[0m  [15/53], [94mLoss[0m : 2.29441
[1mStep[0m  [20/53], [94mLoss[0m : 2.21356
[1mStep[0m  [25/53], [94mLoss[0m : 2.36613
[1mStep[0m  [30/53], [94mLoss[0m : 2.22547
[1mStep[0m  [35/53], [94mLoss[0m : 2.40072
[1mStep[0m  [40/53], [94mLoss[0m : 2.27099
[1mStep[0m  [45/53], [94mLoss[0m : 2.48228
[1mStep[0m  [50/53], [94mLoss[0m : 2.31917

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44320
[1mStep[0m  [5/53], [94mLoss[0m : 2.36892
[1mStep[0m  [10/53], [94mLoss[0m : 2.42354
[1mStep[0m  [15/53], [94mLoss[0m : 2.47090
[1mStep[0m  [20/53], [94mLoss[0m : 2.17749
[1mStep[0m  [25/53], [94mLoss[0m : 2.48758
[1mStep[0m  [30/53], [94mLoss[0m : 2.33430
[1mStep[0m  [35/53], [94mLoss[0m : 2.26483
[1mStep[0m  [40/53], [94mLoss[0m : 2.25913
[1mStep[0m  [45/53], [94mLoss[0m : 2.59532
[1mStep[0m  [50/53], [94mLoss[0m : 2.39533

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33771
[1mStep[0m  [5/53], [94mLoss[0m : 2.17823
[1mStep[0m  [10/53], [94mLoss[0m : 2.33087
[1mStep[0m  [15/53], [94mLoss[0m : 2.38645
[1mStep[0m  [20/53], [94mLoss[0m : 2.40739
[1mStep[0m  [25/53], [94mLoss[0m : 2.39356
[1mStep[0m  [30/53], [94mLoss[0m : 2.49082
[1mStep[0m  [35/53], [94mLoss[0m : 2.18311
[1mStep[0m  [40/53], [94mLoss[0m : 2.19845
[1mStep[0m  [45/53], [94mLoss[0m : 2.37793
[1mStep[0m  [50/53], [94mLoss[0m : 2.36979

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.35332
[1mStep[0m  [5/53], [94mLoss[0m : 2.10541
[1mStep[0m  [10/53], [94mLoss[0m : 2.21521
[1mStep[0m  [15/53], [94mLoss[0m : 2.82607
[1mStep[0m  [20/53], [94mLoss[0m : 2.38787
[1mStep[0m  [25/53], [94mLoss[0m : 2.42877
[1mStep[0m  [30/53], [94mLoss[0m : 2.31185
[1mStep[0m  [35/53], [94mLoss[0m : 2.48156
[1mStep[0m  [40/53], [94mLoss[0m : 2.51723
[1mStep[0m  [45/53], [94mLoss[0m : 2.52189
[1mStep[0m  [50/53], [94mLoss[0m : 2.55567

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.28508
[1mStep[0m  [5/53], [94mLoss[0m : 2.31991
[1mStep[0m  [10/53], [94mLoss[0m : 2.24476
[1mStep[0m  [15/53], [94mLoss[0m : 2.37338
[1mStep[0m  [20/53], [94mLoss[0m : 2.19893
[1mStep[0m  [25/53], [94mLoss[0m : 2.41472
[1mStep[0m  [30/53], [94mLoss[0m : 2.62802
[1mStep[0m  [35/53], [94mLoss[0m : 2.39420
[1mStep[0m  [40/53], [94mLoss[0m : 2.19547
[1mStep[0m  [45/53], [94mLoss[0m : 2.32668
[1mStep[0m  [50/53], [94mLoss[0m : 2.19952

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.20046
[1mStep[0m  [5/53], [94mLoss[0m : 2.23220
[1mStep[0m  [10/53], [94mLoss[0m : 2.32799
[1mStep[0m  [15/53], [94mLoss[0m : 2.44203
[1mStep[0m  [20/53], [94mLoss[0m : 2.30745
[1mStep[0m  [25/53], [94mLoss[0m : 2.60397
[1mStep[0m  [30/53], [94mLoss[0m : 2.41701
[1mStep[0m  [35/53], [94mLoss[0m : 2.31809
[1mStep[0m  [40/53], [94mLoss[0m : 2.23522
[1mStep[0m  [45/53], [94mLoss[0m : 2.28285
[1mStep[0m  [50/53], [94mLoss[0m : 2.44488

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44446
[1mStep[0m  [5/53], [94mLoss[0m : 2.58318
[1mStep[0m  [10/53], [94mLoss[0m : 2.34912
[1mStep[0m  [15/53], [94mLoss[0m : 2.28805
[1mStep[0m  [20/53], [94mLoss[0m : 2.04106
[1mStep[0m  [25/53], [94mLoss[0m : 2.47262
[1mStep[0m  [30/53], [94mLoss[0m : 2.26531
[1mStep[0m  [35/53], [94mLoss[0m : 2.39729
[1mStep[0m  [40/53], [94mLoss[0m : 2.22075
[1mStep[0m  [45/53], [94mLoss[0m : 2.48141
[1mStep[0m  [50/53], [94mLoss[0m : 2.39388

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24841
[1mStep[0m  [5/53], [94mLoss[0m : 2.50391
[1mStep[0m  [10/53], [94mLoss[0m : 2.44314
[1mStep[0m  [15/53], [94mLoss[0m : 2.27949
[1mStep[0m  [20/53], [94mLoss[0m : 2.27882
[1mStep[0m  [25/53], [94mLoss[0m : 2.49302
[1mStep[0m  [30/53], [94mLoss[0m : 2.31232
[1mStep[0m  [35/53], [94mLoss[0m : 2.39991
[1mStep[0m  [40/53], [94mLoss[0m : 2.32227
[1mStep[0m  [45/53], [94mLoss[0m : 2.47323
[1mStep[0m  [50/53], [94mLoss[0m : 2.35891

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.375, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18116
[1mStep[0m  [5/53], [94mLoss[0m : 2.18017
[1mStep[0m  [10/53], [94mLoss[0m : 2.22395
[1mStep[0m  [15/53], [94mLoss[0m : 2.16628
[1mStep[0m  [20/53], [94mLoss[0m : 2.42983
[1mStep[0m  [25/53], [94mLoss[0m : 2.45304
[1mStep[0m  [30/53], [94mLoss[0m : 2.51712
[1mStep[0m  [35/53], [94mLoss[0m : 2.48303
[1mStep[0m  [40/53], [94mLoss[0m : 2.52599
[1mStep[0m  [45/53], [94mLoss[0m : 2.23399
[1mStep[0m  [50/53], [94mLoss[0m : 2.49917

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15122
[1mStep[0m  [5/53], [94mLoss[0m : 2.26534
[1mStep[0m  [10/53], [94mLoss[0m : 2.25794
[1mStep[0m  [15/53], [94mLoss[0m : 2.21407
[1mStep[0m  [20/53], [94mLoss[0m : 2.23034
[1mStep[0m  [25/53], [94mLoss[0m : 2.28929
[1mStep[0m  [30/53], [94mLoss[0m : 2.45768
[1mStep[0m  [35/53], [94mLoss[0m : 2.22078
[1mStep[0m  [40/53], [94mLoss[0m : 2.23281
[1mStep[0m  [45/53], [94mLoss[0m : 2.35173
[1mStep[0m  [50/53], [94mLoss[0m : 2.46375

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27556
[1mStep[0m  [5/53], [94mLoss[0m : 2.44202
[1mStep[0m  [10/53], [94mLoss[0m : 2.41998
[1mStep[0m  [15/53], [94mLoss[0m : 2.42727
[1mStep[0m  [20/53], [94mLoss[0m : 2.38910
[1mStep[0m  [25/53], [94mLoss[0m : 2.32003
[1mStep[0m  [30/53], [94mLoss[0m : 2.17590
[1mStep[0m  [35/53], [94mLoss[0m : 2.25212
[1mStep[0m  [40/53], [94mLoss[0m : 2.40841
[1mStep[0m  [45/53], [94mLoss[0m : 2.07869
[1mStep[0m  [50/53], [94mLoss[0m : 2.25719

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.362, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18176
[1mStep[0m  [5/53], [94mLoss[0m : 2.38845
[1mStep[0m  [10/53], [94mLoss[0m : 2.32265
[1mStep[0m  [15/53], [94mLoss[0m : 2.43224
[1mStep[0m  [20/53], [94mLoss[0m : 2.33985
[1mStep[0m  [25/53], [94mLoss[0m : 2.32423
[1mStep[0m  [30/53], [94mLoss[0m : 2.33495
[1mStep[0m  [35/53], [94mLoss[0m : 2.27084
[1mStep[0m  [40/53], [94mLoss[0m : 2.23682
[1mStep[0m  [45/53], [94mLoss[0m : 2.19135
[1mStep[0m  [50/53], [94mLoss[0m : 2.29127

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17047
[1mStep[0m  [5/53], [94mLoss[0m : 2.35968
[1mStep[0m  [10/53], [94mLoss[0m : 2.43870
[1mStep[0m  [15/53], [94mLoss[0m : 2.25426
[1mStep[0m  [20/53], [94mLoss[0m : 2.26182
[1mStep[0m  [25/53], [94mLoss[0m : 2.18071
[1mStep[0m  [30/53], [94mLoss[0m : 2.36800
[1mStep[0m  [35/53], [94mLoss[0m : 2.12372
[1mStep[0m  [40/53], [94mLoss[0m : 2.39668
[1mStep[0m  [45/53], [94mLoss[0m : 2.28774
[1mStep[0m  [50/53], [94mLoss[0m : 2.36614

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.329, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24511
[1mStep[0m  [5/53], [94mLoss[0m : 2.37095
[1mStep[0m  [10/53], [94mLoss[0m : 2.45336
[1mStep[0m  [15/53], [94mLoss[0m : 2.46439
[1mStep[0m  [20/53], [94mLoss[0m : 2.35279
[1mStep[0m  [25/53], [94mLoss[0m : 2.60448
[1mStep[0m  [30/53], [94mLoss[0m : 2.32949
[1mStep[0m  [35/53], [94mLoss[0m : 2.34844
[1mStep[0m  [40/53], [94mLoss[0m : 2.44817
[1mStep[0m  [45/53], [94mLoss[0m : 2.31646
[1mStep[0m  [50/53], [94mLoss[0m : 2.26398

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.316, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.29271
[1mStep[0m  [5/53], [94mLoss[0m : 2.65346
[1mStep[0m  [10/53], [94mLoss[0m : 2.28466
[1mStep[0m  [15/53], [94mLoss[0m : 2.31858
[1mStep[0m  [20/53], [94mLoss[0m : 1.96327
[1mStep[0m  [25/53], [94mLoss[0m : 2.21630
[1mStep[0m  [30/53], [94mLoss[0m : 2.23578
[1mStep[0m  [35/53], [94mLoss[0m : 2.29094
[1mStep[0m  [40/53], [94mLoss[0m : 2.35020
[1mStep[0m  [45/53], [94mLoss[0m : 2.46351
[1mStep[0m  [50/53], [94mLoss[0m : 2.31772

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42831
[1mStep[0m  [5/53], [94mLoss[0m : 2.23727
[1mStep[0m  [10/53], [94mLoss[0m : 2.33657
[1mStep[0m  [15/53], [94mLoss[0m : 2.30485
[1mStep[0m  [20/53], [94mLoss[0m : 2.27482
[1mStep[0m  [25/53], [94mLoss[0m : 2.29236
[1mStep[0m  [30/53], [94mLoss[0m : 2.54769
[1mStep[0m  [35/53], [94mLoss[0m : 2.29482
[1mStep[0m  [40/53], [94mLoss[0m : 2.42951
[1mStep[0m  [45/53], [94mLoss[0m : 2.38053
[1mStep[0m  [50/53], [94mLoss[0m : 2.16223

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33706
[1mStep[0m  [5/53], [94mLoss[0m : 2.45853
[1mStep[0m  [10/53], [94mLoss[0m : 2.09140
[1mStep[0m  [15/53], [94mLoss[0m : 2.30512
[1mStep[0m  [20/53], [94mLoss[0m : 2.35281
[1mStep[0m  [25/53], [94mLoss[0m : 2.33881
[1mStep[0m  [30/53], [94mLoss[0m : 2.18514
[1mStep[0m  [35/53], [94mLoss[0m : 2.42412
[1mStep[0m  [40/53], [94mLoss[0m : 2.30036
[1mStep[0m  [45/53], [94mLoss[0m : 2.33935
[1mStep[0m  [50/53], [94mLoss[0m : 2.32675

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.350, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17366
[1mStep[0m  [5/53], [94mLoss[0m : 2.36080
[1mStep[0m  [10/53], [94mLoss[0m : 2.55783
[1mStep[0m  [15/53], [94mLoss[0m : 2.26835
[1mStep[0m  [20/53], [94mLoss[0m : 2.39047
[1mStep[0m  [25/53], [94mLoss[0m : 2.54084
[1mStep[0m  [30/53], [94mLoss[0m : 2.16315
[1mStep[0m  [35/53], [94mLoss[0m : 2.12913
[1mStep[0m  [40/53], [94mLoss[0m : 2.12819
[1mStep[0m  [45/53], [94mLoss[0m : 2.32603
[1mStep[0m  [50/53], [94mLoss[0m : 2.36060

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.323370254956759
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.35663
[1mStep[0m  [5/53], [94mLoss[0m : 2.32483
[1mStep[0m  [10/53], [94mLoss[0m : 2.37713
[1mStep[0m  [15/53], [94mLoss[0m : 2.56853
[1mStep[0m  [20/53], [94mLoss[0m : 2.56588
[1mStep[0m  [25/53], [94mLoss[0m : 2.57703
[1mStep[0m  [30/53], [94mLoss[0m : 2.53875
[1mStep[0m  [35/53], [94mLoss[0m : 2.58843
[1mStep[0m  [40/53], [94mLoss[0m : 2.39222
[1mStep[0m  [45/53], [94mLoss[0m : 2.40224
[1mStep[0m  [50/53], [94mLoss[0m : 2.50809

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.74425
[1mStep[0m  [5/53], [94mLoss[0m : 2.31216
[1mStep[0m  [10/53], [94mLoss[0m : 2.18545
[1mStep[0m  [15/53], [94mLoss[0m : 2.35791
[1mStep[0m  [20/53], [94mLoss[0m : 2.20660
[1mStep[0m  [25/53], [94mLoss[0m : 2.35667
[1mStep[0m  [30/53], [94mLoss[0m : 2.39373
[1mStep[0m  [35/53], [94mLoss[0m : 2.33852
[1mStep[0m  [40/53], [94mLoss[0m : 2.45947
[1mStep[0m  [45/53], [94mLoss[0m : 2.30206
[1mStep[0m  [50/53], [94mLoss[0m : 2.35921

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.618, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17762
[1mStep[0m  [5/53], [94mLoss[0m : 2.11182
[1mStep[0m  [10/53], [94mLoss[0m : 2.27936
[1mStep[0m  [15/53], [94mLoss[0m : 1.97181
[1mStep[0m  [20/53], [94mLoss[0m : 2.18741
[1mStep[0m  [25/53], [94mLoss[0m : 2.34255
[1mStep[0m  [30/53], [94mLoss[0m : 2.24985
[1mStep[0m  [35/53], [94mLoss[0m : 2.31719
[1mStep[0m  [40/53], [94mLoss[0m : 2.46209
[1mStep[0m  [45/53], [94mLoss[0m : 1.99794
[1mStep[0m  [50/53], [94mLoss[0m : 2.27521

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.540, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.21337
[1mStep[0m  [5/53], [94mLoss[0m : 1.95419
[1mStep[0m  [10/53], [94mLoss[0m : 2.07278
[1mStep[0m  [15/53], [94mLoss[0m : 2.15025
[1mStep[0m  [20/53], [94mLoss[0m : 1.87893
[1mStep[0m  [25/53], [94mLoss[0m : 2.15488
[1mStep[0m  [30/53], [94mLoss[0m : 1.95945
[1mStep[0m  [35/53], [94mLoss[0m : 2.26634
[1mStep[0m  [40/53], [94mLoss[0m : 1.99206
[1mStep[0m  [45/53], [94mLoss[0m : 2.08461
[1mStep[0m  [50/53], [94mLoss[0m : 2.28756

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.499, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.05826
[1mStep[0m  [5/53], [94mLoss[0m : 2.08594
[1mStep[0m  [10/53], [94mLoss[0m : 2.07298
[1mStep[0m  [15/53], [94mLoss[0m : 2.28442
[1mStep[0m  [20/53], [94mLoss[0m : 1.73091
[1mStep[0m  [25/53], [94mLoss[0m : 1.87518
[1mStep[0m  [30/53], [94mLoss[0m : 1.98670
[1mStep[0m  [35/53], [94mLoss[0m : 1.98071
[1mStep[0m  [40/53], [94mLoss[0m : 1.98946
[1mStep[0m  [45/53], [94mLoss[0m : 1.69976
[1mStep[0m  [50/53], [94mLoss[0m : 2.07886

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.99816
[1mStep[0m  [5/53], [94mLoss[0m : 2.03276
[1mStep[0m  [10/53], [94mLoss[0m : 2.16767
[1mStep[0m  [15/53], [94mLoss[0m : 1.91978
[1mStep[0m  [20/53], [94mLoss[0m : 1.95579
[1mStep[0m  [25/53], [94mLoss[0m : 1.91644
[1mStep[0m  [30/53], [94mLoss[0m : 2.06457
[1mStep[0m  [35/53], [94mLoss[0m : 2.25461
[1mStep[0m  [40/53], [94mLoss[0m : 2.01894
[1mStep[0m  [45/53], [94mLoss[0m : 1.98304
[1mStep[0m  [50/53], [94mLoss[0m : 1.96879

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72101
[1mStep[0m  [5/53], [94mLoss[0m : 1.91571
[1mStep[0m  [10/53], [94mLoss[0m : 1.86175
[1mStep[0m  [15/53], [94mLoss[0m : 1.86071
[1mStep[0m  [20/53], [94mLoss[0m : 1.88750
[1mStep[0m  [25/53], [94mLoss[0m : 1.95114
[1mStep[0m  [30/53], [94mLoss[0m : 2.01367
[1mStep[0m  [35/53], [94mLoss[0m : 1.92932
[1mStep[0m  [40/53], [94mLoss[0m : 2.05772
[1mStep[0m  [45/53], [94mLoss[0m : 1.88099
[1mStep[0m  [50/53], [94mLoss[0m : 2.06927

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73409
[1mStep[0m  [5/53], [94mLoss[0m : 1.90251
[1mStep[0m  [10/53], [94mLoss[0m : 1.95694
[1mStep[0m  [15/53], [94mLoss[0m : 1.88041
[1mStep[0m  [20/53], [94mLoss[0m : 1.85249
[1mStep[0m  [25/53], [94mLoss[0m : 1.58144
[1mStep[0m  [30/53], [94mLoss[0m : 1.83171
[1mStep[0m  [35/53], [94mLoss[0m : 1.73737
[1mStep[0m  [40/53], [94mLoss[0m : 1.97086
[1mStep[0m  [45/53], [94mLoss[0m : 1.84716
[1mStep[0m  [50/53], [94mLoss[0m : 1.75502

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.72324
[1mStep[0m  [5/53], [94mLoss[0m : 1.74608
[1mStep[0m  [10/53], [94mLoss[0m : 1.86626
[1mStep[0m  [15/53], [94mLoss[0m : 1.86304
[1mStep[0m  [20/53], [94mLoss[0m : 1.75715
[1mStep[0m  [25/53], [94mLoss[0m : 1.79784
[1mStep[0m  [30/53], [94mLoss[0m : 1.81988
[1mStep[0m  [35/53], [94mLoss[0m : 2.00778
[1mStep[0m  [40/53], [94mLoss[0m : 1.75078
[1mStep[0m  [45/53], [94mLoss[0m : 1.78350
[1mStep[0m  [50/53], [94mLoss[0m : 2.05732

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.807, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.82099
[1mStep[0m  [5/53], [94mLoss[0m : 1.80256
[1mStep[0m  [10/53], [94mLoss[0m : 1.68095
[1mStep[0m  [15/53], [94mLoss[0m : 1.60445
[1mStep[0m  [20/53], [94mLoss[0m : 1.74240
[1mStep[0m  [25/53], [94mLoss[0m : 1.62469
[1mStep[0m  [30/53], [94mLoss[0m : 1.89971
[1mStep[0m  [35/53], [94mLoss[0m : 1.78270
[1mStep[0m  [40/53], [94mLoss[0m : 1.72989
[1mStep[0m  [45/53], [94mLoss[0m : 1.86529
[1mStep[0m  [50/53], [94mLoss[0m : 1.74740

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.59381
[1mStep[0m  [5/53], [94mLoss[0m : 1.63943
[1mStep[0m  [10/53], [94mLoss[0m : 1.77373
[1mStep[0m  [15/53], [94mLoss[0m : 1.59524
[1mStep[0m  [20/53], [94mLoss[0m : 1.64570
[1mStep[0m  [25/53], [94mLoss[0m : 1.81873
[1mStep[0m  [30/53], [94mLoss[0m : 1.95111
[1mStep[0m  [35/53], [94mLoss[0m : 1.65606
[1mStep[0m  [40/53], [94mLoss[0m : 1.81865
[1mStep[0m  [45/53], [94mLoss[0m : 1.74036
[1mStep[0m  [50/53], [94mLoss[0m : 1.83446

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.54381
[1mStep[0m  [5/53], [94mLoss[0m : 1.64381
[1mStep[0m  [10/53], [94mLoss[0m : 1.54349
[1mStep[0m  [15/53], [94mLoss[0m : 1.63537
[1mStep[0m  [20/53], [94mLoss[0m : 1.75448
[1mStep[0m  [25/53], [94mLoss[0m : 1.60889
[1mStep[0m  [30/53], [94mLoss[0m : 1.56551
[1mStep[0m  [35/53], [94mLoss[0m : 1.61005
[1mStep[0m  [40/53], [94mLoss[0m : 1.78452
[1mStep[0m  [45/53], [94mLoss[0m : 1.69392
[1mStep[0m  [50/53], [94mLoss[0m : 1.69084

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.439, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.44353
[1mStep[0m  [5/53], [94mLoss[0m : 1.70590
[1mStep[0m  [10/53], [94mLoss[0m : 1.92010
[1mStep[0m  [15/53], [94mLoss[0m : 1.49416
[1mStep[0m  [20/53], [94mLoss[0m : 1.76647
[1mStep[0m  [25/53], [94mLoss[0m : 1.59465
[1mStep[0m  [30/53], [94mLoss[0m : 1.76060
[1mStep[0m  [35/53], [94mLoss[0m : 1.68469
[1mStep[0m  [40/53], [94mLoss[0m : 1.66132
[1mStep[0m  [45/53], [94mLoss[0m : 1.70234
[1mStep[0m  [50/53], [94mLoss[0m : 1.66728

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.52864
[1mStep[0m  [5/53], [94mLoss[0m : 1.38893
[1mStep[0m  [10/53], [94mLoss[0m : 1.65006
[1mStep[0m  [15/53], [94mLoss[0m : 1.64843
[1mStep[0m  [20/53], [94mLoss[0m : 1.60702
[1mStep[0m  [25/53], [94mLoss[0m : 1.45933
[1mStep[0m  [30/53], [94mLoss[0m : 1.66669
[1mStep[0m  [35/53], [94mLoss[0m : 1.61090
[1mStep[0m  [40/53], [94mLoss[0m : 1.62107
[1mStep[0m  [45/53], [94mLoss[0m : 1.72016
[1mStep[0m  [50/53], [94mLoss[0m : 1.55850

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.42194
[1mStep[0m  [5/53], [94mLoss[0m : 1.58165
[1mStep[0m  [10/53], [94mLoss[0m : 1.37530
[1mStep[0m  [15/53], [94mLoss[0m : 1.64859
[1mStep[0m  [20/53], [94mLoss[0m : 1.62779
[1mStep[0m  [25/53], [94mLoss[0m : 1.48367
[1mStep[0m  [30/53], [94mLoss[0m : 1.37961
[1mStep[0m  [35/53], [94mLoss[0m : 1.43256
[1mStep[0m  [40/53], [94mLoss[0m : 1.51952
[1mStep[0m  [45/53], [94mLoss[0m : 1.70680
[1mStep[0m  [50/53], [94mLoss[0m : 1.73483

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.542, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.44493
[1mStep[0m  [5/53], [94mLoss[0m : 1.72100
[1mStep[0m  [10/53], [94mLoss[0m : 1.45219
[1mStep[0m  [15/53], [94mLoss[0m : 1.56239
[1mStep[0m  [20/53], [94mLoss[0m : 1.66702
[1mStep[0m  [25/53], [94mLoss[0m : 1.52001
[1mStep[0m  [30/53], [94mLoss[0m : 1.58450
[1mStep[0m  [35/53], [94mLoss[0m : 1.48043
[1mStep[0m  [40/53], [94mLoss[0m : 1.56938
[1mStep[0m  [45/53], [94mLoss[0m : 1.53944
[1mStep[0m  [50/53], [94mLoss[0m : 1.52803

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.516, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.47243
[1mStep[0m  [5/53], [94mLoss[0m : 1.43896
[1mStep[0m  [10/53], [94mLoss[0m : 1.51404
[1mStep[0m  [15/53], [94mLoss[0m : 1.55592
[1mStep[0m  [20/53], [94mLoss[0m : 1.54955
[1mStep[0m  [25/53], [94mLoss[0m : 1.40193
[1mStep[0m  [30/53], [94mLoss[0m : 1.48172
[1mStep[0m  [35/53], [94mLoss[0m : 1.41640
[1mStep[0m  [40/53], [94mLoss[0m : 1.57252
[1mStep[0m  [45/53], [94mLoss[0m : 1.54511
[1mStep[0m  [50/53], [94mLoss[0m : 1.48446

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.36092
[1mStep[0m  [5/53], [94mLoss[0m : 1.34482
[1mStep[0m  [10/53], [94mLoss[0m : 1.45275
[1mStep[0m  [15/53], [94mLoss[0m : 1.44515
[1mStep[0m  [20/53], [94mLoss[0m : 1.40361
[1mStep[0m  [25/53], [94mLoss[0m : 1.41445
[1mStep[0m  [30/53], [94mLoss[0m : 1.59411
[1mStep[0m  [35/53], [94mLoss[0m : 1.50006
[1mStep[0m  [40/53], [94mLoss[0m : 1.50818
[1mStep[0m  [45/53], [94mLoss[0m : 1.50767
[1mStep[0m  [50/53], [94mLoss[0m : 1.57551

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.56219
[1mStep[0m  [5/53], [94mLoss[0m : 1.34855
[1mStep[0m  [10/53], [94mLoss[0m : 1.42702
[1mStep[0m  [15/53], [94mLoss[0m : 1.60055
[1mStep[0m  [20/53], [94mLoss[0m : 1.45080
[1mStep[0m  [25/53], [94mLoss[0m : 1.38557
[1mStep[0m  [30/53], [94mLoss[0m : 1.53258
[1mStep[0m  [35/53], [94mLoss[0m : 1.38824
[1mStep[0m  [40/53], [94mLoss[0m : 1.56180
[1mStep[0m  [45/53], [94mLoss[0m : 1.48026
[1mStep[0m  [50/53], [94mLoss[0m : 1.28115

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.434, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.34027
[1mStep[0m  [5/53], [94mLoss[0m : 1.29325
[1mStep[0m  [10/53], [94mLoss[0m : 1.26470
[1mStep[0m  [15/53], [94mLoss[0m : 1.23072
[1mStep[0m  [20/53], [94mLoss[0m : 1.41016
[1mStep[0m  [25/53], [94mLoss[0m : 1.57551
[1mStep[0m  [30/53], [94mLoss[0m : 1.56287
[1mStep[0m  [35/53], [94mLoss[0m : 1.47243
[1mStep[0m  [40/53], [94mLoss[0m : 1.43185
[1mStep[0m  [45/53], [94mLoss[0m : 1.40929
[1mStep[0m  [50/53], [94mLoss[0m : 1.50420

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.414, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.38292
[1mStep[0m  [5/53], [94mLoss[0m : 1.38766
[1mStep[0m  [10/53], [94mLoss[0m : 1.50628
[1mStep[0m  [15/53], [94mLoss[0m : 1.16756
[1mStep[0m  [20/53], [94mLoss[0m : 1.28752
[1mStep[0m  [25/53], [94mLoss[0m : 1.33732
[1mStep[0m  [30/53], [94mLoss[0m : 1.31159
[1mStep[0m  [35/53], [94mLoss[0m : 1.37881
[1mStep[0m  [40/53], [94mLoss[0m : 1.33244
[1mStep[0m  [45/53], [94mLoss[0m : 1.42161
[1mStep[0m  [50/53], [94mLoss[0m : 1.35033

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.361, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.34736
[1mStep[0m  [5/53], [94mLoss[0m : 1.27045
[1mStep[0m  [10/53], [94mLoss[0m : 1.25944
[1mStep[0m  [15/53], [94mLoss[0m : 1.25179
[1mStep[0m  [20/53], [94mLoss[0m : 1.29692
[1mStep[0m  [25/53], [94mLoss[0m : 1.35789
[1mStep[0m  [30/53], [94mLoss[0m : 1.46185
[1mStep[0m  [35/53], [94mLoss[0m : 1.33938
[1mStep[0m  [40/53], [94mLoss[0m : 1.24574
[1mStep[0m  [45/53], [94mLoss[0m : 1.33249
[1mStep[0m  [50/53], [94mLoss[0m : 1.33894

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.345, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.37230
[1mStep[0m  [5/53], [94mLoss[0m : 1.29694
[1mStep[0m  [10/53], [94mLoss[0m : 1.24161
[1mStep[0m  [15/53], [94mLoss[0m : 1.35475
[1mStep[0m  [20/53], [94mLoss[0m : 1.35926
[1mStep[0m  [25/53], [94mLoss[0m : 1.27284
[1mStep[0m  [30/53], [94mLoss[0m : 1.36270
[1mStep[0m  [35/53], [94mLoss[0m : 1.25440
[1mStep[0m  [40/53], [94mLoss[0m : 1.19423
[1mStep[0m  [45/53], [94mLoss[0m : 1.44127
[1mStep[0m  [50/53], [94mLoss[0m : 1.30308

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.321, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.471
====================================

Phase 2 - Evaluation MAE:  2.470776475392855
MAE score P1       2.32337
MAE score P2      2.470776
loss              1.320822
learning_rate      0.00505
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.88188
[1mStep[0m  [5/53], [94mLoss[0m : 10.58969
[1mStep[0m  [10/53], [94mLoss[0m : 10.63005
[1mStep[0m  [15/53], [94mLoss[0m : 10.66859
[1mStep[0m  [20/53], [94mLoss[0m : 10.78271
[1mStep[0m  [25/53], [94mLoss[0m : 10.70539
[1mStep[0m  [30/53], [94mLoss[0m : 10.66986
[1mStep[0m  [35/53], [94mLoss[0m : 10.58381
[1mStep[0m  [40/53], [94mLoss[0m : 10.36196
[1mStep[0m  [45/53], [94mLoss[0m : 10.56018
[1mStep[0m  [50/53], [94mLoss[0m : 10.33635

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.578, [92mTest[0m: 11.002, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.09987
[1mStep[0m  [5/53], [94mLoss[0m : 10.15116
[1mStep[0m  [10/53], [94mLoss[0m : 10.10800
[1mStep[0m  [15/53], [94mLoss[0m : 10.19022
[1mStep[0m  [20/53], [94mLoss[0m : 10.19343
[1mStep[0m  [25/53], [94mLoss[0m : 9.95119
[1mStep[0m  [30/53], [94mLoss[0m : 9.80342
[1mStep[0m  [35/53], [94mLoss[0m : 9.43673
[1mStep[0m  [40/53], [94mLoss[0m : 9.83181
[1mStep[0m  [45/53], [94mLoss[0m : 9.66076
[1mStep[0m  [50/53], [94mLoss[0m : 9.82234

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.899, [92mTest[0m: 10.141, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.78364
[1mStep[0m  [5/53], [94mLoss[0m : 9.44925
[1mStep[0m  [10/53], [94mLoss[0m : 9.03115
[1mStep[0m  [15/53], [94mLoss[0m : 9.04000
[1mStep[0m  [20/53], [94mLoss[0m : 9.32880
[1mStep[0m  [25/53], [94mLoss[0m : 8.82698
[1mStep[0m  [30/53], [94mLoss[0m : 9.14571
[1mStep[0m  [35/53], [94mLoss[0m : 8.83145
[1mStep[0m  [40/53], [94mLoss[0m : 9.01017
[1mStep[0m  [45/53], [94mLoss[0m : 8.81106
[1mStep[0m  [50/53], [94mLoss[0m : 8.75716

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.075, [92mTest[0m: 9.296, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.08957
[1mStep[0m  [5/53], [94mLoss[0m : 8.45516
[1mStep[0m  [10/53], [94mLoss[0m : 8.25358
[1mStep[0m  [15/53], [94mLoss[0m : 8.13505
[1mStep[0m  [20/53], [94mLoss[0m : 7.98827
[1mStep[0m  [25/53], [94mLoss[0m : 8.24322
[1mStep[0m  [30/53], [94mLoss[0m : 7.48556
[1mStep[0m  [35/53], [94mLoss[0m : 7.59538
[1mStep[0m  [40/53], [94mLoss[0m : 7.40214
[1mStep[0m  [45/53], [94mLoss[0m : 7.76596
[1mStep[0m  [50/53], [94mLoss[0m : 7.31670

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.950, [92mTest[0m: 8.228, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.05733
[1mStep[0m  [5/53], [94mLoss[0m : 6.62039
[1mStep[0m  [10/53], [94mLoss[0m : 7.13850
[1mStep[0m  [15/53], [94mLoss[0m : 7.04605
[1mStep[0m  [20/53], [94mLoss[0m : 6.77334
[1mStep[0m  [25/53], [94mLoss[0m : 6.31573
[1mStep[0m  [30/53], [94mLoss[0m : 6.42045
[1mStep[0m  [35/53], [94mLoss[0m : 6.35675
[1mStep[0m  [40/53], [94mLoss[0m : 6.18773
[1mStep[0m  [45/53], [94mLoss[0m : 6.37449
[1mStep[0m  [50/53], [94mLoss[0m : 6.05511

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.543, [92mTest[0m: 6.786, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.79520
[1mStep[0m  [5/53], [94mLoss[0m : 5.83432
[1mStep[0m  [10/53], [94mLoss[0m : 5.58100
[1mStep[0m  [15/53], [94mLoss[0m : 5.26060
[1mStep[0m  [20/53], [94mLoss[0m : 5.62872
[1mStep[0m  [25/53], [94mLoss[0m : 5.28405
[1mStep[0m  [30/53], [94mLoss[0m : 4.89581
[1mStep[0m  [35/53], [94mLoss[0m : 5.17424
[1mStep[0m  [40/53], [94mLoss[0m : 4.51644
[1mStep[0m  [45/53], [94mLoss[0m : 4.68506
[1mStep[0m  [50/53], [94mLoss[0m : 5.05768

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.282, [92mTest[0m: 5.185, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.62225
[1mStep[0m  [5/53], [94mLoss[0m : 4.44550
[1mStep[0m  [10/53], [94mLoss[0m : 4.47782
[1mStep[0m  [15/53], [94mLoss[0m : 4.47990
[1mStep[0m  [20/53], [94mLoss[0m : 4.43561
[1mStep[0m  [25/53], [94mLoss[0m : 4.59641
[1mStep[0m  [30/53], [94mLoss[0m : 4.27497
[1mStep[0m  [35/53], [94mLoss[0m : 4.12080
[1mStep[0m  [40/53], [94mLoss[0m : 3.60824
[1mStep[0m  [45/53], [94mLoss[0m : 3.78927
[1mStep[0m  [50/53], [94mLoss[0m : 4.04764

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.225, [92mTest[0m: 4.112, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.77957
[1mStep[0m  [5/53], [94mLoss[0m : 3.29424
[1mStep[0m  [10/53], [94mLoss[0m : 3.31602
[1mStep[0m  [15/53], [94mLoss[0m : 3.19664
[1mStep[0m  [20/53], [94mLoss[0m : 3.03841
[1mStep[0m  [25/53], [94mLoss[0m : 3.24404
[1mStep[0m  [30/53], [94mLoss[0m : 3.25500
[1mStep[0m  [35/53], [94mLoss[0m : 3.51814
[1mStep[0m  [40/53], [94mLoss[0m : 3.02149
[1mStep[0m  [45/53], [94mLoss[0m : 3.01514
[1mStep[0m  [50/53], [94mLoss[0m : 2.89822

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.269, [92mTest[0m: 3.247, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.98141
[1mStep[0m  [5/53], [94mLoss[0m : 3.03248
[1mStep[0m  [10/53], [94mLoss[0m : 2.77388
[1mStep[0m  [15/53], [94mLoss[0m : 2.95758
[1mStep[0m  [20/53], [94mLoss[0m : 2.73489
[1mStep[0m  [25/53], [94mLoss[0m : 2.87824
[1mStep[0m  [30/53], [94mLoss[0m : 2.75351
[1mStep[0m  [35/53], [94mLoss[0m : 2.83486
[1mStep[0m  [40/53], [94mLoss[0m : 2.85307
[1mStep[0m  [45/53], [94mLoss[0m : 2.61394
[1mStep[0m  [50/53], [94mLoss[0m : 2.92800

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.670, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.81119
[1mStep[0m  [5/53], [94mLoss[0m : 2.54593
[1mStep[0m  [10/53], [94mLoss[0m : 2.58726
[1mStep[0m  [15/53], [94mLoss[0m : 2.46995
[1mStep[0m  [20/53], [94mLoss[0m : 2.70268
[1mStep[0m  [25/53], [94mLoss[0m : 2.62291
[1mStep[0m  [30/53], [94mLoss[0m : 2.72575
[1mStep[0m  [35/53], [94mLoss[0m : 2.43748
[1mStep[0m  [40/53], [94mLoss[0m : 2.48117
[1mStep[0m  [45/53], [94mLoss[0m : 2.66341
[1mStep[0m  [50/53], [94mLoss[0m : 2.62315

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.527, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67619
[1mStep[0m  [5/53], [94mLoss[0m : 2.68251
[1mStep[0m  [10/53], [94mLoss[0m : 2.68696
[1mStep[0m  [15/53], [94mLoss[0m : 2.46945
[1mStep[0m  [20/53], [94mLoss[0m : 2.53181
[1mStep[0m  [25/53], [94mLoss[0m : 2.57895
[1mStep[0m  [30/53], [94mLoss[0m : 2.37459
[1mStep[0m  [35/53], [94mLoss[0m : 2.85568
[1mStep[0m  [40/53], [94mLoss[0m : 2.74161
[1mStep[0m  [45/53], [94mLoss[0m : 2.64173
[1mStep[0m  [50/53], [94mLoss[0m : 2.59229

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.469, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63992
[1mStep[0m  [5/53], [94mLoss[0m : 2.42177
[1mStep[0m  [10/53], [94mLoss[0m : 2.73034
[1mStep[0m  [15/53], [94mLoss[0m : 2.39913
[1mStep[0m  [20/53], [94mLoss[0m : 2.58334
[1mStep[0m  [25/53], [94mLoss[0m : 2.39416
[1mStep[0m  [30/53], [94mLoss[0m : 2.72700
[1mStep[0m  [35/53], [94mLoss[0m : 2.36500
[1mStep[0m  [40/53], [94mLoss[0m : 2.70719
[1mStep[0m  [45/53], [94mLoss[0m : 2.41757
[1mStep[0m  [50/53], [94mLoss[0m : 2.57222

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60863
[1mStep[0m  [5/53], [94mLoss[0m : 2.40592
[1mStep[0m  [10/53], [94mLoss[0m : 2.74273
[1mStep[0m  [15/53], [94mLoss[0m : 2.63363
[1mStep[0m  [20/53], [94mLoss[0m : 2.49081
[1mStep[0m  [25/53], [94mLoss[0m : 2.65570
[1mStep[0m  [30/53], [94mLoss[0m : 2.61841
[1mStep[0m  [35/53], [94mLoss[0m : 2.44319
[1mStep[0m  [40/53], [94mLoss[0m : 2.66425
[1mStep[0m  [45/53], [94mLoss[0m : 2.59763
[1mStep[0m  [50/53], [94mLoss[0m : 2.57660

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54718
[1mStep[0m  [5/53], [94mLoss[0m : 2.79190
[1mStep[0m  [10/53], [94mLoss[0m : 2.82062
[1mStep[0m  [15/53], [94mLoss[0m : 2.64974
[1mStep[0m  [20/53], [94mLoss[0m : 2.66883
[1mStep[0m  [25/53], [94mLoss[0m : 2.46971
[1mStep[0m  [30/53], [94mLoss[0m : 2.63892
[1mStep[0m  [35/53], [94mLoss[0m : 2.54665
[1mStep[0m  [40/53], [94mLoss[0m : 2.56824
[1mStep[0m  [45/53], [94mLoss[0m : 2.63569
[1mStep[0m  [50/53], [94mLoss[0m : 2.36979

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52536
[1mStep[0m  [5/53], [94mLoss[0m : 2.71506
[1mStep[0m  [10/53], [94mLoss[0m : 2.62617
[1mStep[0m  [15/53], [94mLoss[0m : 2.77990
[1mStep[0m  [20/53], [94mLoss[0m : 2.38244
[1mStep[0m  [25/53], [94mLoss[0m : 2.65180
[1mStep[0m  [30/53], [94mLoss[0m : 2.62324
[1mStep[0m  [35/53], [94mLoss[0m : 2.56572
[1mStep[0m  [40/53], [94mLoss[0m : 2.42263
[1mStep[0m  [45/53], [94mLoss[0m : 2.73009
[1mStep[0m  [50/53], [94mLoss[0m : 2.68843

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71757
[1mStep[0m  [5/53], [94mLoss[0m : 2.53900
[1mStep[0m  [10/53], [94mLoss[0m : 2.52409
[1mStep[0m  [15/53], [94mLoss[0m : 2.59323
[1mStep[0m  [20/53], [94mLoss[0m : 2.61771
[1mStep[0m  [25/53], [94mLoss[0m : 2.49504
[1mStep[0m  [30/53], [94mLoss[0m : 2.68070
[1mStep[0m  [35/53], [94mLoss[0m : 2.68340
[1mStep[0m  [40/53], [94mLoss[0m : 2.59249
[1mStep[0m  [45/53], [94mLoss[0m : 2.61792
[1mStep[0m  [50/53], [94mLoss[0m : 2.38489

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39730
[1mStep[0m  [5/53], [94mLoss[0m : 2.50958
[1mStep[0m  [10/53], [94mLoss[0m : 2.51203
[1mStep[0m  [15/53], [94mLoss[0m : 2.44569
[1mStep[0m  [20/53], [94mLoss[0m : 2.38678
[1mStep[0m  [25/53], [94mLoss[0m : 2.67499
[1mStep[0m  [30/53], [94mLoss[0m : 2.78051
[1mStep[0m  [35/53], [94mLoss[0m : 2.65727
[1mStep[0m  [40/53], [94mLoss[0m : 2.85720
[1mStep[0m  [45/53], [94mLoss[0m : 2.43677
[1mStep[0m  [50/53], [94mLoss[0m : 2.35909

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70209
[1mStep[0m  [5/53], [94mLoss[0m : 2.60956
[1mStep[0m  [10/53], [94mLoss[0m : 2.48476
[1mStep[0m  [15/53], [94mLoss[0m : 2.34051
[1mStep[0m  [20/53], [94mLoss[0m : 2.47527
[1mStep[0m  [25/53], [94mLoss[0m : 2.78318
[1mStep[0m  [30/53], [94mLoss[0m : 2.50948
[1mStep[0m  [35/53], [94mLoss[0m : 2.50696
[1mStep[0m  [40/53], [94mLoss[0m : 2.38525
[1mStep[0m  [45/53], [94mLoss[0m : 2.44222
[1mStep[0m  [50/53], [94mLoss[0m : 2.31700

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51771
[1mStep[0m  [5/53], [94mLoss[0m : 2.49728
[1mStep[0m  [10/53], [94mLoss[0m : 2.62522
[1mStep[0m  [15/53], [94mLoss[0m : 2.56461
[1mStep[0m  [20/53], [94mLoss[0m : 2.45622
[1mStep[0m  [25/53], [94mLoss[0m : 2.65206
[1mStep[0m  [30/53], [94mLoss[0m : 2.41572
[1mStep[0m  [35/53], [94mLoss[0m : 2.71749
[1mStep[0m  [40/53], [94mLoss[0m : 2.43085
[1mStep[0m  [45/53], [94mLoss[0m : 2.54115
[1mStep[0m  [50/53], [94mLoss[0m : 2.47462

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58285
[1mStep[0m  [5/53], [94mLoss[0m : 2.66732
[1mStep[0m  [10/53], [94mLoss[0m : 2.48707
[1mStep[0m  [15/53], [94mLoss[0m : 2.50922
[1mStep[0m  [20/53], [94mLoss[0m : 2.47174
[1mStep[0m  [25/53], [94mLoss[0m : 2.45456
[1mStep[0m  [30/53], [94mLoss[0m : 2.59685
[1mStep[0m  [35/53], [94mLoss[0m : 2.42352
[1mStep[0m  [40/53], [94mLoss[0m : 2.54307
[1mStep[0m  [45/53], [94mLoss[0m : 2.47051
[1mStep[0m  [50/53], [94mLoss[0m : 2.50155

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.418, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52819
[1mStep[0m  [5/53], [94mLoss[0m : 2.54782
[1mStep[0m  [10/53], [94mLoss[0m : 2.30584
[1mStep[0m  [15/53], [94mLoss[0m : 2.36403
[1mStep[0m  [20/53], [94mLoss[0m : 2.63943
[1mStep[0m  [25/53], [94mLoss[0m : 2.53452
[1mStep[0m  [30/53], [94mLoss[0m : 2.53805
[1mStep[0m  [35/53], [94mLoss[0m : 2.57178
[1mStep[0m  [40/53], [94mLoss[0m : 2.66112
[1mStep[0m  [45/53], [94mLoss[0m : 2.62457
[1mStep[0m  [50/53], [94mLoss[0m : 2.68383

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58835
[1mStep[0m  [5/53], [94mLoss[0m : 2.41962
[1mStep[0m  [10/53], [94mLoss[0m : 2.59310
[1mStep[0m  [15/53], [94mLoss[0m : 2.62330
[1mStep[0m  [20/53], [94mLoss[0m : 2.62453
[1mStep[0m  [25/53], [94mLoss[0m : 2.31007
[1mStep[0m  [30/53], [94mLoss[0m : 2.70913
[1mStep[0m  [35/53], [94mLoss[0m : 2.69295
[1mStep[0m  [40/53], [94mLoss[0m : 2.58477
[1mStep[0m  [45/53], [94mLoss[0m : 2.43228
[1mStep[0m  [50/53], [94mLoss[0m : 2.42454

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.427, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46513
[1mStep[0m  [5/53], [94mLoss[0m : 2.56147
[1mStep[0m  [10/53], [94mLoss[0m : 2.58894
[1mStep[0m  [15/53], [94mLoss[0m : 2.59413
[1mStep[0m  [20/53], [94mLoss[0m : 2.52085
[1mStep[0m  [25/53], [94mLoss[0m : 2.59071
[1mStep[0m  [30/53], [94mLoss[0m : 2.29434
[1mStep[0m  [35/53], [94mLoss[0m : 2.33178
[1mStep[0m  [40/53], [94mLoss[0m : 2.35520
[1mStep[0m  [45/53], [94mLoss[0m : 2.48637
[1mStep[0m  [50/53], [94mLoss[0m : 2.54626

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61445
[1mStep[0m  [5/53], [94mLoss[0m : 2.50422
[1mStep[0m  [10/53], [94mLoss[0m : 2.56058
[1mStep[0m  [15/53], [94mLoss[0m : 2.61549
[1mStep[0m  [20/53], [94mLoss[0m : 2.59449
[1mStep[0m  [25/53], [94mLoss[0m : 2.65682
[1mStep[0m  [30/53], [94mLoss[0m : 2.63688
[1mStep[0m  [35/53], [94mLoss[0m : 2.52406
[1mStep[0m  [40/53], [94mLoss[0m : 2.23585
[1mStep[0m  [45/53], [94mLoss[0m : 2.33645
[1mStep[0m  [50/53], [94mLoss[0m : 2.60468

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52813
[1mStep[0m  [5/53], [94mLoss[0m : 2.54230
[1mStep[0m  [10/53], [94mLoss[0m : 2.49503
[1mStep[0m  [15/53], [94mLoss[0m : 2.58109
[1mStep[0m  [20/53], [94mLoss[0m : 2.33601
[1mStep[0m  [25/53], [94mLoss[0m : 2.37761
[1mStep[0m  [30/53], [94mLoss[0m : 2.67218
[1mStep[0m  [35/53], [94mLoss[0m : 2.56571
[1mStep[0m  [40/53], [94mLoss[0m : 2.31048
[1mStep[0m  [45/53], [94mLoss[0m : 2.46086
[1mStep[0m  [50/53], [94mLoss[0m : 2.47983

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.418, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47105
[1mStep[0m  [5/53], [94mLoss[0m : 2.58666
[1mStep[0m  [10/53], [94mLoss[0m : 2.53318
[1mStep[0m  [15/53], [94mLoss[0m : 2.27056
[1mStep[0m  [20/53], [94mLoss[0m : 2.56239
[1mStep[0m  [25/53], [94mLoss[0m : 2.60851
[1mStep[0m  [30/53], [94mLoss[0m : 2.43391
[1mStep[0m  [35/53], [94mLoss[0m : 2.46392
[1mStep[0m  [40/53], [94mLoss[0m : 2.38535
[1mStep[0m  [45/53], [94mLoss[0m : 2.36778
[1mStep[0m  [50/53], [94mLoss[0m : 2.53941

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40336
[1mStep[0m  [5/53], [94mLoss[0m : 2.55234
[1mStep[0m  [10/53], [94mLoss[0m : 2.44957
[1mStep[0m  [15/53], [94mLoss[0m : 2.57711
[1mStep[0m  [20/53], [94mLoss[0m : 2.72347
[1mStep[0m  [25/53], [94mLoss[0m : 2.52900
[1mStep[0m  [30/53], [94mLoss[0m : 2.48217
[1mStep[0m  [35/53], [94mLoss[0m : 2.72672
[1mStep[0m  [40/53], [94mLoss[0m : 2.73857
[1mStep[0m  [45/53], [94mLoss[0m : 2.44396
[1mStep[0m  [50/53], [94mLoss[0m : 2.38314

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63853
[1mStep[0m  [5/53], [94mLoss[0m : 2.63073
[1mStep[0m  [10/53], [94mLoss[0m : 2.42062
[1mStep[0m  [15/53], [94mLoss[0m : 2.78633
[1mStep[0m  [20/53], [94mLoss[0m : 2.53054
[1mStep[0m  [25/53], [94mLoss[0m : 2.47294
[1mStep[0m  [30/53], [94mLoss[0m : 2.63400
[1mStep[0m  [35/53], [94mLoss[0m : 2.31628
[1mStep[0m  [40/53], [94mLoss[0m : 2.46648
[1mStep[0m  [45/53], [94mLoss[0m : 2.64567
[1mStep[0m  [50/53], [94mLoss[0m : 2.84869

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64097
[1mStep[0m  [5/53], [94mLoss[0m : 2.53712
[1mStep[0m  [10/53], [94mLoss[0m : 2.52566
[1mStep[0m  [15/53], [94mLoss[0m : 2.37326
[1mStep[0m  [20/53], [94mLoss[0m : 2.34201
[1mStep[0m  [25/53], [94mLoss[0m : 2.36590
[1mStep[0m  [30/53], [94mLoss[0m : 2.40768
[1mStep[0m  [35/53], [94mLoss[0m : 2.30252
[1mStep[0m  [40/53], [94mLoss[0m : 2.47349
[1mStep[0m  [45/53], [94mLoss[0m : 2.68282
[1mStep[0m  [50/53], [94mLoss[0m : 2.39827

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.393, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53344
[1mStep[0m  [5/53], [94mLoss[0m : 2.43519
[1mStep[0m  [10/53], [94mLoss[0m : 2.69009
[1mStep[0m  [15/53], [94mLoss[0m : 2.30862
[1mStep[0m  [20/53], [94mLoss[0m : 2.27520
[1mStep[0m  [25/53], [94mLoss[0m : 2.39182
[1mStep[0m  [30/53], [94mLoss[0m : 2.52420
[1mStep[0m  [35/53], [94mLoss[0m : 2.39025
[1mStep[0m  [40/53], [94mLoss[0m : 2.28868
[1mStep[0m  [45/53], [94mLoss[0m : 2.84949
[1mStep[0m  [50/53], [94mLoss[0m : 2.60645

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.389
====================================

Phase 1 - Evaluation MAE:  2.3886892061967115
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.33019
[1mStep[0m  [5/53], [94mLoss[0m : 2.59317
[1mStep[0m  [10/53], [94mLoss[0m : 2.32727
[1mStep[0m  [15/53], [94mLoss[0m : 2.52165
[1mStep[0m  [20/53], [94mLoss[0m : 2.61261
[1mStep[0m  [25/53], [94mLoss[0m : 2.73336
[1mStep[0m  [30/53], [94mLoss[0m : 2.49254
[1mStep[0m  [35/53], [94mLoss[0m : 2.64944
[1mStep[0m  [40/53], [94mLoss[0m : 2.51524
[1mStep[0m  [45/53], [94mLoss[0m : 2.45059
[1mStep[0m  [50/53], [94mLoss[0m : 2.53690

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.69976
[1mStep[0m  [5/53], [94mLoss[0m : 2.39644
[1mStep[0m  [10/53], [94mLoss[0m : 2.75100
[1mStep[0m  [15/53], [94mLoss[0m : 2.48785
[1mStep[0m  [20/53], [94mLoss[0m : 2.44368
[1mStep[0m  [25/53], [94mLoss[0m : 2.59031
[1mStep[0m  [30/53], [94mLoss[0m : 2.32438
[1mStep[0m  [35/53], [94mLoss[0m : 2.63053
[1mStep[0m  [40/53], [94mLoss[0m : 2.42294
[1mStep[0m  [45/53], [94mLoss[0m : 2.52469
[1mStep[0m  [50/53], [94mLoss[0m : 2.30449

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51526
[1mStep[0m  [5/53], [94mLoss[0m : 2.38922
[1mStep[0m  [10/53], [94mLoss[0m : 2.21125
[1mStep[0m  [15/53], [94mLoss[0m : 2.43595
[1mStep[0m  [20/53], [94mLoss[0m : 2.54700
[1mStep[0m  [25/53], [94mLoss[0m : 2.55654
[1mStep[0m  [30/53], [94mLoss[0m : 2.64886
[1mStep[0m  [35/53], [94mLoss[0m : 2.34820
[1mStep[0m  [40/53], [94mLoss[0m : 2.73329
[1mStep[0m  [45/53], [94mLoss[0m : 2.50265
[1mStep[0m  [50/53], [94mLoss[0m : 2.53250

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.610, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44745
[1mStep[0m  [5/53], [94mLoss[0m : 2.30251
[1mStep[0m  [10/53], [94mLoss[0m : 2.37830
[1mStep[0m  [15/53], [94mLoss[0m : 2.44260
[1mStep[0m  [20/53], [94mLoss[0m : 2.26462
[1mStep[0m  [25/53], [94mLoss[0m : 2.55333
[1mStep[0m  [30/53], [94mLoss[0m : 2.33536
[1mStep[0m  [35/53], [94mLoss[0m : 2.49101
[1mStep[0m  [40/53], [94mLoss[0m : 2.48355
[1mStep[0m  [45/53], [94mLoss[0m : 2.54944
[1mStep[0m  [50/53], [94mLoss[0m : 2.55369

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.761, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52103
[1mStep[0m  [5/53], [94mLoss[0m : 2.62540
[1mStep[0m  [10/53], [94mLoss[0m : 2.25480
[1mStep[0m  [15/53], [94mLoss[0m : 2.53027
[1mStep[0m  [20/53], [94mLoss[0m : 2.33590
[1mStep[0m  [25/53], [94mLoss[0m : 2.32830
[1mStep[0m  [30/53], [94mLoss[0m : 2.49398
[1mStep[0m  [35/53], [94mLoss[0m : 2.51339
[1mStep[0m  [40/53], [94mLoss[0m : 2.40786
[1mStep[0m  [45/53], [94mLoss[0m : 2.11992
[1mStep[0m  [50/53], [94mLoss[0m : 2.17585

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.574, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.26987
[1mStep[0m  [5/53], [94mLoss[0m : 2.56645
[1mStep[0m  [10/53], [94mLoss[0m : 2.72050
[1mStep[0m  [15/53], [94mLoss[0m : 2.48792
[1mStep[0m  [20/53], [94mLoss[0m : 2.42046
[1mStep[0m  [25/53], [94mLoss[0m : 2.21708
[1mStep[0m  [30/53], [94mLoss[0m : 2.38368
[1mStep[0m  [35/53], [94mLoss[0m : 2.39108
[1mStep[0m  [40/53], [94mLoss[0m : 2.41736
[1mStep[0m  [45/53], [94mLoss[0m : 2.35795
[1mStep[0m  [50/53], [94mLoss[0m : 2.38753

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.648, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.11892
[1mStep[0m  [5/53], [94mLoss[0m : 2.14192
[1mStep[0m  [10/53], [94mLoss[0m : 2.26950
[1mStep[0m  [15/53], [94mLoss[0m : 2.17010
[1mStep[0m  [20/53], [94mLoss[0m : 2.29876
[1mStep[0m  [25/53], [94mLoss[0m : 2.38558
[1mStep[0m  [30/53], [94mLoss[0m : 2.21821
[1mStep[0m  [35/53], [94mLoss[0m : 2.31157
[1mStep[0m  [40/53], [94mLoss[0m : 2.22160
[1mStep[0m  [45/53], [94mLoss[0m : 2.27770
[1mStep[0m  [50/53], [94mLoss[0m : 2.54349

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.630, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12934
[1mStep[0m  [5/53], [94mLoss[0m : 2.31221
[1mStep[0m  [10/53], [94mLoss[0m : 2.31439
[1mStep[0m  [15/53], [94mLoss[0m : 2.35552
[1mStep[0m  [20/53], [94mLoss[0m : 2.18627
[1mStep[0m  [25/53], [94mLoss[0m : 2.38815
[1mStep[0m  [30/53], [94mLoss[0m : 2.42371
[1mStep[0m  [35/53], [94mLoss[0m : 2.31871
[1mStep[0m  [40/53], [94mLoss[0m : 2.29752
[1mStep[0m  [45/53], [94mLoss[0m : 2.56628
[1mStep[0m  [50/53], [94mLoss[0m : 2.24342

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.616, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.17945
[1mStep[0m  [5/53], [94mLoss[0m : 2.30863
[1mStep[0m  [10/53], [94mLoss[0m : 2.13152
[1mStep[0m  [15/53], [94mLoss[0m : 2.12804
[1mStep[0m  [20/53], [94mLoss[0m : 2.35155
[1mStep[0m  [25/53], [94mLoss[0m : 2.00955
[1mStep[0m  [30/53], [94mLoss[0m : 2.14869
[1mStep[0m  [35/53], [94mLoss[0m : 2.20950
[1mStep[0m  [40/53], [94mLoss[0m : 2.46111
[1mStep[0m  [45/53], [94mLoss[0m : 2.16647
[1mStep[0m  [50/53], [94mLoss[0m : 2.30705

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.581, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38970
[1mStep[0m  [5/53], [94mLoss[0m : 2.05923
[1mStep[0m  [10/53], [94mLoss[0m : 2.17591
[1mStep[0m  [15/53], [94mLoss[0m : 2.19268
[1mStep[0m  [20/53], [94mLoss[0m : 2.01102
[1mStep[0m  [25/53], [94mLoss[0m : 2.24681
[1mStep[0m  [30/53], [94mLoss[0m : 2.02322
[1mStep[0m  [35/53], [94mLoss[0m : 2.11314
[1mStep[0m  [40/53], [94mLoss[0m : 2.25464
[1mStep[0m  [45/53], [94mLoss[0m : 2.13587
[1mStep[0m  [50/53], [94mLoss[0m : 2.37175

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.19559
[1mStep[0m  [5/53], [94mLoss[0m : 2.12683
[1mStep[0m  [10/53], [94mLoss[0m : 2.20475
[1mStep[0m  [15/53], [94mLoss[0m : 2.10315
[1mStep[0m  [20/53], [94mLoss[0m : 2.08300
[1mStep[0m  [25/53], [94mLoss[0m : 2.06000
[1mStep[0m  [30/53], [94mLoss[0m : 2.38367
[1mStep[0m  [35/53], [94mLoss[0m : 2.14713
[1mStep[0m  [40/53], [94mLoss[0m : 2.26436
[1mStep[0m  [45/53], [94mLoss[0m : 2.17908
[1mStep[0m  [50/53], [94mLoss[0m : 2.16517

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.577, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.13589
[1mStep[0m  [5/53], [94mLoss[0m : 2.25986
[1mStep[0m  [10/53], [94mLoss[0m : 2.13479
[1mStep[0m  [15/53], [94mLoss[0m : 2.15173
[1mStep[0m  [20/53], [94mLoss[0m : 2.19833
[1mStep[0m  [25/53], [94mLoss[0m : 2.15177
[1mStep[0m  [30/53], [94mLoss[0m : 2.08598
[1mStep[0m  [35/53], [94mLoss[0m : 1.97851
[1mStep[0m  [40/53], [94mLoss[0m : 2.19386
[1mStep[0m  [45/53], [94mLoss[0m : 2.22964
[1mStep[0m  [50/53], [94mLoss[0m : 2.06519

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25164
[1mStep[0m  [5/53], [94mLoss[0m : 1.97778
[1mStep[0m  [10/53], [94mLoss[0m : 1.95160
[1mStep[0m  [15/53], [94mLoss[0m : 2.01572
[1mStep[0m  [20/53], [94mLoss[0m : 2.12354
[1mStep[0m  [25/53], [94mLoss[0m : 2.13609
[1mStep[0m  [30/53], [94mLoss[0m : 2.04414
[1mStep[0m  [35/53], [94mLoss[0m : 2.20947
[1mStep[0m  [40/53], [94mLoss[0m : 2.41667
[1mStep[0m  [45/53], [94mLoss[0m : 1.80438
[1mStep[0m  [50/53], [94mLoss[0m : 2.16588

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09660
[1mStep[0m  [5/53], [94mLoss[0m : 1.92065
[1mStep[0m  [10/53], [94mLoss[0m : 2.04068
[1mStep[0m  [15/53], [94mLoss[0m : 1.88607
[1mStep[0m  [20/53], [94mLoss[0m : 2.00567
[1mStep[0m  [25/53], [94mLoss[0m : 1.92376
[1mStep[0m  [30/53], [94mLoss[0m : 2.10168
[1mStep[0m  [35/53], [94mLoss[0m : 1.99806
[1mStep[0m  [40/53], [94mLoss[0m : 2.13549
[1mStep[0m  [45/53], [94mLoss[0m : 2.00556
[1mStep[0m  [50/53], [94mLoss[0m : 2.33073

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.86818
[1mStep[0m  [5/53], [94mLoss[0m : 1.93221
[1mStep[0m  [10/53], [94mLoss[0m : 2.08461
[1mStep[0m  [15/53], [94mLoss[0m : 2.05127
[1mStep[0m  [20/53], [94mLoss[0m : 2.02064
[1mStep[0m  [25/53], [94mLoss[0m : 2.00236
[1mStep[0m  [30/53], [94mLoss[0m : 1.95734
[1mStep[0m  [35/53], [94mLoss[0m : 1.90725
[1mStep[0m  [40/53], [94mLoss[0m : 2.05484
[1mStep[0m  [45/53], [94mLoss[0m : 2.02887
[1mStep[0m  [50/53], [94mLoss[0m : 2.19505

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.028, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90938
[1mStep[0m  [5/53], [94mLoss[0m : 1.83786
[1mStep[0m  [10/53], [94mLoss[0m : 1.98501
[1mStep[0m  [15/53], [94mLoss[0m : 2.01241
[1mStep[0m  [20/53], [94mLoss[0m : 2.08667
[1mStep[0m  [25/53], [94mLoss[0m : 1.96131
[1mStep[0m  [30/53], [94mLoss[0m : 2.03225
[1mStep[0m  [35/53], [94mLoss[0m : 2.24729
[1mStep[0m  [40/53], [94mLoss[0m : 2.04169
[1mStep[0m  [45/53], [94mLoss[0m : 2.07387
[1mStep[0m  [50/53], [94mLoss[0m : 2.14660

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93959
[1mStep[0m  [5/53], [94mLoss[0m : 2.08591
[1mStep[0m  [10/53], [94mLoss[0m : 1.83108
[1mStep[0m  [15/53], [94mLoss[0m : 1.95975
[1mStep[0m  [20/53], [94mLoss[0m : 1.89743
[1mStep[0m  [25/53], [94mLoss[0m : 2.05578
[1mStep[0m  [30/53], [94mLoss[0m : 1.97458
[1mStep[0m  [35/53], [94mLoss[0m : 2.06406
[1mStep[0m  [40/53], [94mLoss[0m : 1.85252
[1mStep[0m  [45/53], [94mLoss[0m : 1.98901
[1mStep[0m  [50/53], [94mLoss[0m : 1.95511

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.99970
[1mStep[0m  [5/53], [94mLoss[0m : 1.91019
[1mStep[0m  [10/53], [94mLoss[0m : 1.88148
[1mStep[0m  [15/53], [94mLoss[0m : 1.93153
[1mStep[0m  [20/53], [94mLoss[0m : 2.01678
[1mStep[0m  [25/53], [94mLoss[0m : 1.98626
[1mStep[0m  [30/53], [94mLoss[0m : 1.82091
[1mStep[0m  [35/53], [94mLoss[0m : 2.12046
[1mStep[0m  [40/53], [94mLoss[0m : 1.93055
[1mStep[0m  [45/53], [94mLoss[0m : 1.91907
[1mStep[0m  [50/53], [94mLoss[0m : 1.85270

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.584, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93599
[1mStep[0m  [5/53], [94mLoss[0m : 1.88772
[1mStep[0m  [10/53], [94mLoss[0m : 1.99888
[1mStep[0m  [15/53], [94mLoss[0m : 1.79130
[1mStep[0m  [20/53], [94mLoss[0m : 1.90104
[1mStep[0m  [25/53], [94mLoss[0m : 2.03767
[1mStep[0m  [30/53], [94mLoss[0m : 1.90156
[1mStep[0m  [35/53], [94mLoss[0m : 2.06153
[1mStep[0m  [40/53], [94mLoss[0m : 1.92023
[1mStep[0m  [45/53], [94mLoss[0m : 1.87260
[1mStep[0m  [50/53], [94mLoss[0m : 1.90482

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.88183
[1mStep[0m  [5/53], [94mLoss[0m : 1.90621
[1mStep[0m  [10/53], [94mLoss[0m : 1.99194
[1mStep[0m  [15/53], [94mLoss[0m : 1.84831
[1mStep[0m  [20/53], [94mLoss[0m : 1.99473
[1mStep[0m  [25/53], [94mLoss[0m : 1.75446
[1mStep[0m  [30/53], [94mLoss[0m : 2.03615
[1mStep[0m  [35/53], [94mLoss[0m : 1.89184
[1mStep[0m  [40/53], [94mLoss[0m : 1.95548
[1mStep[0m  [45/53], [94mLoss[0m : 1.76701
[1mStep[0m  [50/53], [94mLoss[0m : 2.02801

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.87058
[1mStep[0m  [5/53], [94mLoss[0m : 1.72929
[1mStep[0m  [10/53], [94mLoss[0m : 1.71582
[1mStep[0m  [15/53], [94mLoss[0m : 1.81848
[1mStep[0m  [20/53], [94mLoss[0m : 1.80973
[1mStep[0m  [25/53], [94mLoss[0m : 1.95251
[1mStep[0m  [30/53], [94mLoss[0m : 1.63732
[1mStep[0m  [35/53], [94mLoss[0m : 1.96351
[1mStep[0m  [40/53], [94mLoss[0m : 1.92348
[1mStep[0m  [45/53], [94mLoss[0m : 1.87868
[1mStep[0m  [50/53], [94mLoss[0m : 1.80533

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.66079
[1mStep[0m  [5/53], [94mLoss[0m : 1.83020
[1mStep[0m  [10/53], [94mLoss[0m : 1.81338
[1mStep[0m  [15/53], [94mLoss[0m : 1.77039
[1mStep[0m  [20/53], [94mLoss[0m : 1.59101
[1mStep[0m  [25/53], [94mLoss[0m : 1.79898
[1mStep[0m  [30/53], [94mLoss[0m : 2.09482
[1mStep[0m  [35/53], [94mLoss[0m : 1.64779
[1mStep[0m  [40/53], [94mLoss[0m : 1.70120
[1mStep[0m  [45/53], [94mLoss[0m : 1.83637
[1mStep[0m  [50/53], [94mLoss[0m : 1.72494

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.438, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.70894
[1mStep[0m  [5/53], [94mLoss[0m : 1.85827
[1mStep[0m  [10/53], [94mLoss[0m : 1.84091
[1mStep[0m  [15/53], [94mLoss[0m : 1.87436
[1mStep[0m  [20/53], [94mLoss[0m : 1.75802
[1mStep[0m  [25/53], [94mLoss[0m : 1.69145
[1mStep[0m  [30/53], [94mLoss[0m : 1.81169
[1mStep[0m  [35/53], [94mLoss[0m : 1.83539
[1mStep[0m  [40/53], [94mLoss[0m : 1.90321
[1mStep[0m  [45/53], [94mLoss[0m : 1.85721
[1mStep[0m  [50/53], [94mLoss[0m : 1.91868

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.435, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.69164
[1mStep[0m  [5/53], [94mLoss[0m : 1.58363
[1mStep[0m  [10/53], [94mLoss[0m : 1.71283
[1mStep[0m  [15/53], [94mLoss[0m : 1.66914
[1mStep[0m  [20/53], [94mLoss[0m : 1.65552
[1mStep[0m  [25/53], [94mLoss[0m : 1.70246
[1mStep[0m  [30/53], [94mLoss[0m : 1.84319
[1mStep[0m  [35/53], [94mLoss[0m : 1.79846
[1mStep[0m  [40/53], [94mLoss[0m : 1.89978
[1mStep[0m  [45/53], [94mLoss[0m : 1.61396
[1mStep[0m  [50/53], [94mLoss[0m : 1.69273

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.417, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64366
[1mStep[0m  [5/53], [94mLoss[0m : 1.60060
[1mStep[0m  [10/53], [94mLoss[0m : 1.82795
[1mStep[0m  [15/53], [94mLoss[0m : 1.55063
[1mStep[0m  [20/53], [94mLoss[0m : 1.64800
[1mStep[0m  [25/53], [94mLoss[0m : 1.57539
[1mStep[0m  [30/53], [94mLoss[0m : 1.87884
[1mStep[0m  [35/53], [94mLoss[0m : 1.77572
[1mStep[0m  [40/53], [94mLoss[0m : 1.67962
[1mStep[0m  [45/53], [94mLoss[0m : 1.91243
[1mStep[0m  [50/53], [94mLoss[0m : 1.91602

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83527
[1mStep[0m  [5/53], [94mLoss[0m : 1.79348
[1mStep[0m  [10/53], [94mLoss[0m : 1.70786
[1mStep[0m  [15/53], [94mLoss[0m : 1.61597
[1mStep[0m  [20/53], [94mLoss[0m : 1.72049
[1mStep[0m  [25/53], [94mLoss[0m : 1.89324
[1mStep[0m  [30/53], [94mLoss[0m : 1.73599
[1mStep[0m  [35/53], [94mLoss[0m : 1.82469
[1mStep[0m  [40/53], [94mLoss[0m : 1.70897
[1mStep[0m  [45/53], [94mLoss[0m : 1.65707
[1mStep[0m  [50/53], [94mLoss[0m : 1.66515

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.420, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.65336
[1mStep[0m  [5/53], [94mLoss[0m : 1.62629
[1mStep[0m  [10/53], [94mLoss[0m : 1.70501
[1mStep[0m  [15/53], [94mLoss[0m : 1.62309
[1mStep[0m  [20/53], [94mLoss[0m : 1.57794
[1mStep[0m  [25/53], [94mLoss[0m : 1.72823
[1mStep[0m  [30/53], [94mLoss[0m : 1.73469
[1mStep[0m  [35/53], [94mLoss[0m : 1.82315
[1mStep[0m  [40/53], [94mLoss[0m : 1.67185
[1mStep[0m  [45/53], [94mLoss[0m : 1.69986
[1mStep[0m  [50/53], [94mLoss[0m : 1.78517

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.64411
[1mStep[0m  [5/53], [94mLoss[0m : 1.66497
[1mStep[0m  [10/53], [94mLoss[0m : 1.52431
[1mStep[0m  [15/53], [94mLoss[0m : 1.70478
[1mStep[0m  [20/53], [94mLoss[0m : 1.77524
[1mStep[0m  [25/53], [94mLoss[0m : 1.54881
[1mStep[0m  [30/53], [94mLoss[0m : 1.62677
[1mStep[0m  [35/53], [94mLoss[0m : 1.70080
[1mStep[0m  [40/53], [94mLoss[0m : 1.73249
[1mStep[0m  [45/53], [94mLoss[0m : 1.51325
[1mStep[0m  [50/53], [94mLoss[0m : 1.63285

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.672, [92mTest[0m: 2.456, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.50794
[1mStep[0m  [5/53], [94mLoss[0m : 1.46257
[1mStep[0m  [10/53], [94mLoss[0m : 1.77052
[1mStep[0m  [15/53], [94mLoss[0m : 1.63539
[1mStep[0m  [20/53], [94mLoss[0m : 1.63884
[1mStep[0m  [25/53], [94mLoss[0m : 1.77152
[1mStep[0m  [30/53], [94mLoss[0m : 1.65721
[1mStep[0m  [35/53], [94mLoss[0m : 1.65138
[1mStep[0m  [40/53], [94mLoss[0m : 1.83056
[1mStep[0m  [45/53], [94mLoss[0m : 1.72292
[1mStep[0m  [50/53], [94mLoss[0m : 1.75369

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.60060
[1mStep[0m  [5/53], [94mLoss[0m : 1.52712
[1mStep[0m  [10/53], [94mLoss[0m : 1.64762
[1mStep[0m  [15/53], [94mLoss[0m : 1.67234
[1mStep[0m  [20/53], [94mLoss[0m : 1.63783
[1mStep[0m  [25/53], [94mLoss[0m : 1.70806
[1mStep[0m  [30/53], [94mLoss[0m : 1.64732
[1mStep[0m  [35/53], [94mLoss[0m : 1.70884
[1mStep[0m  [40/53], [94mLoss[0m : 1.67171
[1mStep[0m  [45/53], [94mLoss[0m : 1.56977
[1mStep[0m  [50/53], [94mLoss[0m : 1.68520

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.460, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.453
====================================

Phase 2 - Evaluation MAE:  2.4534919995528
MAE score P1       2.388689
MAE score P2       2.453492
loss               1.633352
learning_rate       0.00505
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 10.84651
[1mStep[0m  [5/53], [94mLoss[0m : 10.54513
[1mStep[0m  [10/53], [94mLoss[0m : 10.83163
[1mStep[0m  [15/53], [94mLoss[0m : 10.99298
[1mStep[0m  [20/53], [94mLoss[0m : 10.66738
[1mStep[0m  [25/53], [94mLoss[0m : 10.55983
[1mStep[0m  [30/53], [94mLoss[0m : 10.42153
[1mStep[0m  [35/53], [94mLoss[0m : 10.34178
[1mStep[0m  [40/53], [94mLoss[0m : 10.38828
[1mStep[0m  [45/53], [94mLoss[0m : 10.18696
[1mStep[0m  [50/53], [94mLoss[0m : 10.22139

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.477, [92mTest[0m: 10.834, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.11507
[1mStep[0m  [5/53], [94mLoss[0m : 9.43446
[1mStep[0m  [10/53], [94mLoss[0m : 9.70502
[1mStep[0m  [15/53], [94mLoss[0m : 9.81294
[1mStep[0m  [20/53], [94mLoss[0m : 9.73819
[1mStep[0m  [25/53], [94mLoss[0m : 9.36428
[1mStep[0m  [30/53], [94mLoss[0m : 9.01517
[1mStep[0m  [35/53], [94mLoss[0m : 9.29102
[1mStep[0m  [40/53], [94mLoss[0m : 8.95848
[1mStep[0m  [45/53], [94mLoss[0m : 9.16062
[1mStep[0m  [50/53], [94mLoss[0m : 8.64117

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.423, [92mTest[0m: 9.822, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.76123
[1mStep[0m  [5/53], [94mLoss[0m : 8.59027
[1mStep[0m  [10/53], [94mLoss[0m : 8.51299
[1mStep[0m  [15/53], [94mLoss[0m : 8.42364
[1mStep[0m  [20/53], [94mLoss[0m : 7.93467
[1mStep[0m  [25/53], [94mLoss[0m : 8.05614
[1mStep[0m  [30/53], [94mLoss[0m : 8.17740
[1mStep[0m  [35/53], [94mLoss[0m : 8.06082
[1mStep[0m  [40/53], [94mLoss[0m : 8.19127
[1mStep[0m  [45/53], [94mLoss[0m : 7.62414
[1mStep[0m  [50/53], [94mLoss[0m : 7.32454

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.142, [92mTest[0m: 8.253, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.34283
[1mStep[0m  [5/53], [94mLoss[0m : 7.18312
[1mStep[0m  [10/53], [94mLoss[0m : 7.10410
[1mStep[0m  [15/53], [94mLoss[0m : 7.43248
[1mStep[0m  [20/53], [94mLoss[0m : 7.19701
[1mStep[0m  [25/53], [94mLoss[0m : 7.41021
[1mStep[0m  [30/53], [94mLoss[0m : 6.54450
[1mStep[0m  [35/53], [94mLoss[0m : 6.99564
[1mStep[0m  [40/53], [94mLoss[0m : 6.49878
[1mStep[0m  [45/53], [94mLoss[0m : 6.94580
[1mStep[0m  [50/53], [94mLoss[0m : 6.18132

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.991, [92mTest[0m: 6.778, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.35506
[1mStep[0m  [5/53], [94mLoss[0m : 6.31477
[1mStep[0m  [10/53], [94mLoss[0m : 6.11263
[1mStep[0m  [15/53], [94mLoss[0m : 6.07355
[1mStep[0m  [20/53], [94mLoss[0m : 5.74922
[1mStep[0m  [25/53], [94mLoss[0m : 5.57277
[1mStep[0m  [30/53], [94mLoss[0m : 5.70810
[1mStep[0m  [35/53], [94mLoss[0m : 5.79330
[1mStep[0m  [40/53], [94mLoss[0m : 5.67712
[1mStep[0m  [45/53], [94mLoss[0m : 5.44724
[1mStep[0m  [50/53], [94mLoss[0m : 5.34040

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.015, [92mTest[0m: 5.587, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.17776
[1mStep[0m  [5/53], [94mLoss[0m : 5.39910
[1mStep[0m  [10/53], [94mLoss[0m : 5.09709
[1mStep[0m  [15/53], [94mLoss[0m : 5.02630
[1mStep[0m  [20/53], [94mLoss[0m : 5.45306
[1mStep[0m  [25/53], [94mLoss[0m : 5.25404
[1mStep[0m  [30/53], [94mLoss[0m : 4.72576
[1mStep[0m  [35/53], [94mLoss[0m : 4.67567
[1mStep[0m  [40/53], [94mLoss[0m : 4.05099
[1mStep[0m  [45/53], [94mLoss[0m : 4.80128
[1mStep[0m  [50/53], [94mLoss[0m : 4.52637

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.951, [92mTest[0m: 4.563, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.73753
[1mStep[0m  [5/53], [94mLoss[0m : 4.04128
[1mStep[0m  [10/53], [94mLoss[0m : 4.28678
[1mStep[0m  [15/53], [94mLoss[0m : 3.81185
[1mStep[0m  [20/53], [94mLoss[0m : 3.93150
[1mStep[0m  [25/53], [94mLoss[0m : 3.69102
[1mStep[0m  [30/53], [94mLoss[0m : 3.42093
[1mStep[0m  [35/53], [94mLoss[0m : 3.87005
[1mStep[0m  [40/53], [94mLoss[0m : 3.65240
[1mStep[0m  [45/53], [94mLoss[0m : 3.71246
[1mStep[0m  [50/53], [94mLoss[0m : 3.29870

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.792, [92mTest[0m: 3.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.15495
[1mStep[0m  [5/53], [94mLoss[0m : 3.15666
[1mStep[0m  [10/53], [94mLoss[0m : 3.23789
[1mStep[0m  [15/53], [94mLoss[0m : 3.06335
[1mStep[0m  [20/53], [94mLoss[0m : 2.78954
[1mStep[0m  [25/53], [94mLoss[0m : 3.02297
[1mStep[0m  [30/53], [94mLoss[0m : 3.09372
[1mStep[0m  [35/53], [94mLoss[0m : 2.89212
[1mStep[0m  [40/53], [94mLoss[0m : 2.64508
[1mStep[0m  [45/53], [94mLoss[0m : 3.08956
[1mStep[0m  [50/53], [94mLoss[0m : 2.91791

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.008, [92mTest[0m: 2.692, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63566
[1mStep[0m  [5/53], [94mLoss[0m : 2.86801
[1mStep[0m  [10/53], [94mLoss[0m : 2.79161
[1mStep[0m  [15/53], [94mLoss[0m : 2.69694
[1mStep[0m  [20/53], [94mLoss[0m : 2.77190
[1mStep[0m  [25/53], [94mLoss[0m : 2.75683
[1mStep[0m  [30/53], [94mLoss[0m : 2.83845
[1mStep[0m  [35/53], [94mLoss[0m : 2.62329
[1mStep[0m  [40/53], [94mLoss[0m : 3.03852
[1mStep[0m  [45/53], [94mLoss[0m : 2.78343
[1mStep[0m  [50/53], [94mLoss[0m : 2.96252

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.822, [92mTest[0m: 2.466, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.92768
[1mStep[0m  [5/53], [94mLoss[0m : 2.68160
[1mStep[0m  [10/53], [94mLoss[0m : 2.53585
[1mStep[0m  [15/53], [94mLoss[0m : 2.64721
[1mStep[0m  [20/53], [94mLoss[0m : 2.90913
[1mStep[0m  [25/53], [94mLoss[0m : 2.80541
[1mStep[0m  [30/53], [94mLoss[0m : 2.80977
[1mStep[0m  [35/53], [94mLoss[0m : 2.76786
[1mStep[0m  [40/53], [94mLoss[0m : 2.54290
[1mStep[0m  [45/53], [94mLoss[0m : 2.57642
[1mStep[0m  [50/53], [94mLoss[0m : 2.71589

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.742, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38296
[1mStep[0m  [5/53], [94mLoss[0m : 2.52357
[1mStep[0m  [10/53], [94mLoss[0m : 2.74835
[1mStep[0m  [15/53], [94mLoss[0m : 2.53396
[1mStep[0m  [20/53], [94mLoss[0m : 2.77502
[1mStep[0m  [25/53], [94mLoss[0m : 2.61968
[1mStep[0m  [30/53], [94mLoss[0m : 2.80452
[1mStep[0m  [35/53], [94mLoss[0m : 2.87329
[1mStep[0m  [40/53], [94mLoss[0m : 2.78360
[1mStep[0m  [45/53], [94mLoss[0m : 2.77983
[1mStep[0m  [50/53], [94mLoss[0m : 2.88973

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.715, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52991
[1mStep[0m  [5/53], [94mLoss[0m : 2.69919
[1mStep[0m  [10/53], [94mLoss[0m : 2.80659
[1mStep[0m  [15/53], [94mLoss[0m : 2.91522
[1mStep[0m  [20/53], [94mLoss[0m : 2.55111
[1mStep[0m  [25/53], [94mLoss[0m : 2.62245
[1mStep[0m  [30/53], [94mLoss[0m : 2.63480
[1mStep[0m  [35/53], [94mLoss[0m : 2.76765
[1mStep[0m  [40/53], [94mLoss[0m : 2.56832
[1mStep[0m  [45/53], [94mLoss[0m : 2.65222
[1mStep[0m  [50/53], [94mLoss[0m : 2.67201

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.702, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58769
[1mStep[0m  [5/53], [94mLoss[0m : 2.64433
[1mStep[0m  [10/53], [94mLoss[0m : 2.54407
[1mStep[0m  [15/53], [94mLoss[0m : 2.53963
[1mStep[0m  [20/53], [94mLoss[0m : 2.66934
[1mStep[0m  [25/53], [94mLoss[0m : 2.81479
[1mStep[0m  [30/53], [94mLoss[0m : 2.77652
[1mStep[0m  [35/53], [94mLoss[0m : 2.66286
[1mStep[0m  [40/53], [94mLoss[0m : 2.79897
[1mStep[0m  [45/53], [94mLoss[0m : 2.68794
[1mStep[0m  [50/53], [94mLoss[0m : 2.80128

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55083
[1mStep[0m  [5/53], [94mLoss[0m : 2.74442
[1mStep[0m  [10/53], [94mLoss[0m : 2.64134
[1mStep[0m  [15/53], [94mLoss[0m : 2.48043
[1mStep[0m  [20/53], [94mLoss[0m : 2.68305
[1mStep[0m  [25/53], [94mLoss[0m : 2.61372
[1mStep[0m  [30/53], [94mLoss[0m : 2.53275
[1mStep[0m  [35/53], [94mLoss[0m : 2.72065
[1mStep[0m  [40/53], [94mLoss[0m : 2.68826
[1mStep[0m  [45/53], [94mLoss[0m : 2.65746
[1mStep[0m  [50/53], [94mLoss[0m : 2.57263

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.423, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50622
[1mStep[0m  [5/53], [94mLoss[0m : 2.51263
[1mStep[0m  [10/53], [94mLoss[0m : 2.73854
[1mStep[0m  [15/53], [94mLoss[0m : 2.66873
[1mStep[0m  [20/53], [94mLoss[0m : 2.69695
[1mStep[0m  [25/53], [94mLoss[0m : 2.68781
[1mStep[0m  [30/53], [94mLoss[0m : 2.75822
[1mStep[0m  [35/53], [94mLoss[0m : 2.54605
[1mStep[0m  [40/53], [94mLoss[0m : 2.72620
[1mStep[0m  [45/53], [94mLoss[0m : 2.49189
[1mStep[0m  [50/53], [94mLoss[0m : 2.76871

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52352
[1mStep[0m  [5/53], [94mLoss[0m : 2.52906
[1mStep[0m  [10/53], [94mLoss[0m : 2.47346
[1mStep[0m  [15/53], [94mLoss[0m : 2.68323
[1mStep[0m  [20/53], [94mLoss[0m : 2.72913
[1mStep[0m  [25/53], [94mLoss[0m : 2.56676
[1mStep[0m  [30/53], [94mLoss[0m : 2.76900
[1mStep[0m  [35/53], [94mLoss[0m : 2.62252
[1mStep[0m  [40/53], [94mLoss[0m : 2.62800
[1mStep[0m  [45/53], [94mLoss[0m : 2.41605
[1mStep[0m  [50/53], [94mLoss[0m : 2.59731

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54277
[1mStep[0m  [5/53], [94mLoss[0m : 2.60499
[1mStep[0m  [10/53], [94mLoss[0m : 2.56839
[1mStep[0m  [15/53], [94mLoss[0m : 2.56277
[1mStep[0m  [20/53], [94mLoss[0m : 2.70388
[1mStep[0m  [25/53], [94mLoss[0m : 2.53715
[1mStep[0m  [30/53], [94mLoss[0m : 2.60895
[1mStep[0m  [35/53], [94mLoss[0m : 2.60066
[1mStep[0m  [40/53], [94mLoss[0m : 2.69138
[1mStep[0m  [45/53], [94mLoss[0m : 2.68227
[1mStep[0m  [50/53], [94mLoss[0m : 2.57528

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61682
[1mStep[0m  [5/53], [94mLoss[0m : 2.73497
[1mStep[0m  [10/53], [94mLoss[0m : 2.52176
[1mStep[0m  [15/53], [94mLoss[0m : 2.61746
[1mStep[0m  [20/53], [94mLoss[0m : 2.62401
[1mStep[0m  [25/53], [94mLoss[0m : 2.64065
[1mStep[0m  [30/53], [94mLoss[0m : 2.83695
[1mStep[0m  [35/53], [94mLoss[0m : 2.62174
[1mStep[0m  [40/53], [94mLoss[0m : 2.45037
[1mStep[0m  [45/53], [94mLoss[0m : 2.67930
[1mStep[0m  [50/53], [94mLoss[0m : 2.69962

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38113
[1mStep[0m  [5/53], [94mLoss[0m : 2.73912
[1mStep[0m  [10/53], [94mLoss[0m : 2.61775
[1mStep[0m  [15/53], [94mLoss[0m : 2.46911
[1mStep[0m  [20/53], [94mLoss[0m : 2.68554
[1mStep[0m  [25/53], [94mLoss[0m : 2.69886
[1mStep[0m  [30/53], [94mLoss[0m : 2.46401
[1mStep[0m  [35/53], [94mLoss[0m : 2.72075
[1mStep[0m  [40/53], [94mLoss[0m : 2.55213
[1mStep[0m  [45/53], [94mLoss[0m : 2.60762
[1mStep[0m  [50/53], [94mLoss[0m : 2.62938

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47043
[1mStep[0m  [5/53], [94mLoss[0m : 2.57125
[1mStep[0m  [10/53], [94mLoss[0m : 2.63932
[1mStep[0m  [15/53], [94mLoss[0m : 2.54972
[1mStep[0m  [20/53], [94mLoss[0m : 2.58474
[1mStep[0m  [25/53], [94mLoss[0m : 2.56204
[1mStep[0m  [30/53], [94mLoss[0m : 2.69668
[1mStep[0m  [35/53], [94mLoss[0m : 2.39718
[1mStep[0m  [40/53], [94mLoss[0m : 2.28247
[1mStep[0m  [45/53], [94mLoss[0m : 2.45151
[1mStep[0m  [50/53], [94mLoss[0m : 2.58080

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45251
[1mStep[0m  [5/53], [94mLoss[0m : 2.30690
[1mStep[0m  [10/53], [94mLoss[0m : 2.54667
[1mStep[0m  [15/53], [94mLoss[0m : 2.43328
[1mStep[0m  [20/53], [94mLoss[0m : 2.49168
[1mStep[0m  [25/53], [94mLoss[0m : 2.56119
[1mStep[0m  [30/53], [94mLoss[0m : 2.53522
[1mStep[0m  [35/53], [94mLoss[0m : 2.64527
[1mStep[0m  [40/53], [94mLoss[0m : 2.44550
[1mStep[0m  [45/53], [94mLoss[0m : 2.61434
[1mStep[0m  [50/53], [94mLoss[0m : 2.41592

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52583
[1mStep[0m  [5/53], [94mLoss[0m : 2.55376
[1mStep[0m  [10/53], [94mLoss[0m : 2.54926
[1mStep[0m  [15/53], [94mLoss[0m : 2.59065
[1mStep[0m  [20/53], [94mLoss[0m : 2.93954
[1mStep[0m  [25/53], [94mLoss[0m : 2.65556
[1mStep[0m  [30/53], [94mLoss[0m : 2.48893
[1mStep[0m  [35/53], [94mLoss[0m : 2.65601
[1mStep[0m  [40/53], [94mLoss[0m : 2.89768
[1mStep[0m  [45/53], [94mLoss[0m : 2.67161
[1mStep[0m  [50/53], [94mLoss[0m : 2.47879

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58818
[1mStep[0m  [5/53], [94mLoss[0m : 2.48026
[1mStep[0m  [10/53], [94mLoss[0m : 2.50693
[1mStep[0m  [15/53], [94mLoss[0m : 2.56710
[1mStep[0m  [20/53], [94mLoss[0m : 2.60605
[1mStep[0m  [25/53], [94mLoss[0m : 2.52601
[1mStep[0m  [30/53], [94mLoss[0m : 2.54353
[1mStep[0m  [35/53], [94mLoss[0m : 2.52234
[1mStep[0m  [40/53], [94mLoss[0m : 2.74148
[1mStep[0m  [45/53], [94mLoss[0m : 2.75573
[1mStep[0m  [50/53], [94mLoss[0m : 2.51961

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50036
[1mStep[0m  [5/53], [94mLoss[0m : 2.62829
[1mStep[0m  [10/53], [94mLoss[0m : 2.56135
[1mStep[0m  [15/53], [94mLoss[0m : 2.63891
[1mStep[0m  [20/53], [94mLoss[0m : 2.67169
[1mStep[0m  [25/53], [94mLoss[0m : 2.34173
[1mStep[0m  [30/53], [94mLoss[0m : 2.57566
[1mStep[0m  [35/53], [94mLoss[0m : 2.42080
[1mStep[0m  [40/53], [94mLoss[0m : 2.53760
[1mStep[0m  [45/53], [94mLoss[0m : 2.39108
[1mStep[0m  [50/53], [94mLoss[0m : 2.54845

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.49455
[1mStep[0m  [5/53], [94mLoss[0m : 2.80507
[1mStep[0m  [10/53], [94mLoss[0m : 2.40590
[1mStep[0m  [15/53], [94mLoss[0m : 2.39013
[1mStep[0m  [20/53], [94mLoss[0m : 2.44008
[1mStep[0m  [25/53], [94mLoss[0m : 2.65068
[1mStep[0m  [30/53], [94mLoss[0m : 2.73249
[1mStep[0m  [35/53], [94mLoss[0m : 2.77880
[1mStep[0m  [40/53], [94mLoss[0m : 2.51914
[1mStep[0m  [45/53], [94mLoss[0m : 2.78459
[1mStep[0m  [50/53], [94mLoss[0m : 2.65508

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50913
[1mStep[0m  [5/53], [94mLoss[0m : 2.73543
[1mStep[0m  [10/53], [94mLoss[0m : 2.66714
[1mStep[0m  [15/53], [94mLoss[0m : 2.59009
[1mStep[0m  [20/53], [94mLoss[0m : 2.52496
[1mStep[0m  [25/53], [94mLoss[0m : 2.49211
[1mStep[0m  [30/53], [94mLoss[0m : 2.70548
[1mStep[0m  [35/53], [94mLoss[0m : 2.42183
[1mStep[0m  [40/53], [94mLoss[0m : 2.54587
[1mStep[0m  [45/53], [94mLoss[0m : 2.60358
[1mStep[0m  [50/53], [94mLoss[0m : 2.68172

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.391, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.87030
[1mStep[0m  [5/53], [94mLoss[0m : 2.45939
[1mStep[0m  [10/53], [94mLoss[0m : 2.41134
[1mStep[0m  [15/53], [94mLoss[0m : 2.46522
[1mStep[0m  [20/53], [94mLoss[0m : 2.40816
[1mStep[0m  [25/53], [94mLoss[0m : 2.49870
[1mStep[0m  [30/53], [94mLoss[0m : 2.59936
[1mStep[0m  [35/53], [94mLoss[0m : 2.88064
[1mStep[0m  [40/53], [94mLoss[0m : 2.58439
[1mStep[0m  [45/53], [94mLoss[0m : 2.71737
[1mStep[0m  [50/53], [94mLoss[0m : 2.40399

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42531
[1mStep[0m  [5/53], [94mLoss[0m : 2.36731
[1mStep[0m  [10/53], [94mLoss[0m : 2.39710
[1mStep[0m  [15/53], [94mLoss[0m : 2.56537
[1mStep[0m  [20/53], [94mLoss[0m : 2.26546
[1mStep[0m  [25/53], [94mLoss[0m : 2.65643
[1mStep[0m  [30/53], [94mLoss[0m : 2.44286
[1mStep[0m  [35/53], [94mLoss[0m : 2.57227
[1mStep[0m  [40/53], [94mLoss[0m : 2.41354
[1mStep[0m  [45/53], [94mLoss[0m : 2.78710
[1mStep[0m  [50/53], [94mLoss[0m : 2.47048

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43391
[1mStep[0m  [5/53], [94mLoss[0m : 2.74464
[1mStep[0m  [10/53], [94mLoss[0m : 2.55879
[1mStep[0m  [15/53], [94mLoss[0m : 2.82824
[1mStep[0m  [20/53], [94mLoss[0m : 2.52156
[1mStep[0m  [25/53], [94mLoss[0m : 2.38074
[1mStep[0m  [30/53], [94mLoss[0m : 2.45889
[1mStep[0m  [35/53], [94mLoss[0m : 2.47924
[1mStep[0m  [40/53], [94mLoss[0m : 2.30675
[1mStep[0m  [45/53], [94mLoss[0m : 2.47154
[1mStep[0m  [50/53], [94mLoss[0m : 2.55841

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.384, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67176
[1mStep[0m  [5/53], [94mLoss[0m : 2.35957
[1mStep[0m  [10/53], [94mLoss[0m : 2.36326
[1mStep[0m  [15/53], [94mLoss[0m : 2.27713
[1mStep[0m  [20/53], [94mLoss[0m : 2.59241
[1mStep[0m  [25/53], [94mLoss[0m : 2.46686
[1mStep[0m  [30/53], [94mLoss[0m : 2.65003
[1mStep[0m  [35/53], [94mLoss[0m : 2.56337
[1mStep[0m  [40/53], [94mLoss[0m : 2.45526
[1mStep[0m  [45/53], [94mLoss[0m : 2.66395
[1mStep[0m  [50/53], [94mLoss[0m : 2.71858

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.375
====================================

Phase 1 - Evaluation MAE:  2.374831667313209
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 2.63422
[1mStep[0m  [5/53], [94mLoss[0m : 2.55181
[1mStep[0m  [10/53], [94mLoss[0m : 2.45493
[1mStep[0m  [15/53], [94mLoss[0m : 2.66970
[1mStep[0m  [20/53], [94mLoss[0m : 2.72512
[1mStep[0m  [25/53], [94mLoss[0m : 2.55484
[1mStep[0m  [30/53], [94mLoss[0m : 2.69900
[1mStep[0m  [35/53], [94mLoss[0m : 2.66196
[1mStep[0m  [40/53], [94mLoss[0m : 2.58567
[1mStep[0m  [45/53], [94mLoss[0m : 2.54759
[1mStep[0m  [50/53], [94mLoss[0m : 2.61943

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41729
[1mStep[0m  [5/53], [94mLoss[0m : 2.58030
[1mStep[0m  [10/53], [94mLoss[0m : 2.87830
[1mStep[0m  [15/53], [94mLoss[0m : 2.61626
[1mStep[0m  [20/53], [94mLoss[0m : 2.65974
[1mStep[0m  [25/53], [94mLoss[0m : 2.48399
[1mStep[0m  [30/53], [94mLoss[0m : 2.56829
[1mStep[0m  [35/53], [94mLoss[0m : 2.48741
[1mStep[0m  [40/53], [94mLoss[0m : 2.38082
[1mStep[0m  [45/53], [94mLoss[0m : 2.57019
[1mStep[0m  [50/53], [94mLoss[0m : 2.43614

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.46025
[1mStep[0m  [5/53], [94mLoss[0m : 2.69317
[1mStep[0m  [10/53], [94mLoss[0m : 2.41355
[1mStep[0m  [15/53], [94mLoss[0m : 2.55495
[1mStep[0m  [20/53], [94mLoss[0m : 2.34070
[1mStep[0m  [25/53], [94mLoss[0m : 2.61646
[1mStep[0m  [30/53], [94mLoss[0m : 2.55371
[1mStep[0m  [35/53], [94mLoss[0m : 2.14197
[1mStep[0m  [40/53], [94mLoss[0m : 2.66639
[1mStep[0m  [45/53], [94mLoss[0m : 2.21206
[1mStep[0m  [50/53], [94mLoss[0m : 2.59508

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.555, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56317
[1mStep[0m  [5/53], [94mLoss[0m : 2.47863
[1mStep[0m  [10/53], [94mLoss[0m : 2.44800
[1mStep[0m  [15/53], [94mLoss[0m : 2.47400
[1mStep[0m  [20/53], [94mLoss[0m : 2.51995
[1mStep[0m  [25/53], [94mLoss[0m : 2.52740
[1mStep[0m  [30/53], [94mLoss[0m : 2.34367
[1mStep[0m  [35/53], [94mLoss[0m : 2.60961
[1mStep[0m  [40/53], [94mLoss[0m : 2.56266
[1mStep[0m  [45/53], [94mLoss[0m : 2.52352
[1mStep[0m  [50/53], [94mLoss[0m : 2.36111

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.588, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38525
[1mStep[0m  [5/53], [94mLoss[0m : 2.57288
[1mStep[0m  [10/53], [94mLoss[0m : 2.36506
[1mStep[0m  [15/53], [94mLoss[0m : 2.38894
[1mStep[0m  [20/53], [94mLoss[0m : 2.59679
[1mStep[0m  [25/53], [94mLoss[0m : 2.56168
[1mStep[0m  [30/53], [94mLoss[0m : 2.39668
[1mStep[0m  [35/53], [94mLoss[0m : 2.44282
[1mStep[0m  [40/53], [94mLoss[0m : 2.42898
[1mStep[0m  [45/53], [94mLoss[0m : 2.39078
[1mStep[0m  [50/53], [94mLoss[0m : 2.48769

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.601, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44728
[1mStep[0m  [5/53], [94mLoss[0m : 2.43430
[1mStep[0m  [10/53], [94mLoss[0m : 2.36163
[1mStep[0m  [15/53], [94mLoss[0m : 2.32327
[1mStep[0m  [20/53], [94mLoss[0m : 2.47378
[1mStep[0m  [25/53], [94mLoss[0m : 2.35474
[1mStep[0m  [30/53], [94mLoss[0m : 2.26632
[1mStep[0m  [35/53], [94mLoss[0m : 2.26688
[1mStep[0m  [40/53], [94mLoss[0m : 2.33765
[1mStep[0m  [45/53], [94mLoss[0m : 2.35350
[1mStep[0m  [50/53], [94mLoss[0m : 2.35976

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.549, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.25679
[1mStep[0m  [5/53], [94mLoss[0m : 2.26025
[1mStep[0m  [10/53], [94mLoss[0m : 2.50021
[1mStep[0m  [15/53], [94mLoss[0m : 2.58216
[1mStep[0m  [20/53], [94mLoss[0m : 2.52893
[1mStep[0m  [25/53], [94mLoss[0m : 2.49837
[1mStep[0m  [30/53], [94mLoss[0m : 2.20311
[1mStep[0m  [35/53], [94mLoss[0m : 2.39530
[1mStep[0m  [40/53], [94mLoss[0m : 2.53578
[1mStep[0m  [45/53], [94mLoss[0m : 2.40011
[1mStep[0m  [50/53], [94mLoss[0m : 2.57114

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.650, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45809
[1mStep[0m  [5/53], [94mLoss[0m : 2.52962
[1mStep[0m  [10/53], [94mLoss[0m : 2.11803
[1mStep[0m  [15/53], [94mLoss[0m : 2.26306
[1mStep[0m  [20/53], [94mLoss[0m : 2.22995
[1mStep[0m  [25/53], [94mLoss[0m : 2.24717
[1mStep[0m  [30/53], [94mLoss[0m : 2.48217
[1mStep[0m  [35/53], [94mLoss[0m : 2.39158
[1mStep[0m  [40/53], [94mLoss[0m : 2.38234
[1mStep[0m  [45/53], [94mLoss[0m : 2.52910
[1mStep[0m  [50/53], [94mLoss[0m : 2.20001

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.858, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.24485
[1mStep[0m  [5/53], [94mLoss[0m : 2.40903
[1mStep[0m  [10/53], [94mLoss[0m : 2.34339
[1mStep[0m  [15/53], [94mLoss[0m : 2.30586
[1mStep[0m  [20/53], [94mLoss[0m : 2.05016
[1mStep[0m  [25/53], [94mLoss[0m : 2.25568
[1mStep[0m  [30/53], [94mLoss[0m : 2.15704
[1mStep[0m  [35/53], [94mLoss[0m : 2.54171
[1mStep[0m  [40/53], [94mLoss[0m : 2.04575
[1mStep[0m  [45/53], [94mLoss[0m : 2.33693
[1mStep[0m  [50/53], [94mLoss[0m : 2.25331

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.604, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.15191
[1mStep[0m  [5/53], [94mLoss[0m : 2.39654
[1mStep[0m  [10/53], [94mLoss[0m : 2.09415
[1mStep[0m  [15/53], [94mLoss[0m : 2.22323
[1mStep[0m  [20/53], [94mLoss[0m : 2.15332
[1mStep[0m  [25/53], [94mLoss[0m : 2.23521
[1mStep[0m  [30/53], [94mLoss[0m : 2.31344
[1mStep[0m  [35/53], [94mLoss[0m : 2.20790
[1mStep[0m  [40/53], [94mLoss[0m : 2.25913
[1mStep[0m  [45/53], [94mLoss[0m : 2.11351
[1mStep[0m  [50/53], [94mLoss[0m : 2.23575

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.813, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.02099
[1mStep[0m  [5/53], [94mLoss[0m : 2.04648
[1mStep[0m  [10/53], [94mLoss[0m : 2.13353
[1mStep[0m  [15/53], [94mLoss[0m : 2.22980
[1mStep[0m  [20/53], [94mLoss[0m : 2.37852
[1mStep[0m  [25/53], [94mLoss[0m : 2.01392
[1mStep[0m  [30/53], [94mLoss[0m : 2.09064
[1mStep[0m  [35/53], [94mLoss[0m : 2.32155
[1mStep[0m  [40/53], [94mLoss[0m : 2.24181
[1mStep[0m  [45/53], [94mLoss[0m : 2.23849
[1mStep[0m  [50/53], [94mLoss[0m : 2.09988

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.672, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.99527
[1mStep[0m  [5/53], [94mLoss[0m : 2.08978
[1mStep[0m  [10/53], [94mLoss[0m : 2.15154
[1mStep[0m  [15/53], [94mLoss[0m : 1.93024
[1mStep[0m  [20/53], [94mLoss[0m : 2.26193
[1mStep[0m  [25/53], [94mLoss[0m : 2.10803
[1mStep[0m  [30/53], [94mLoss[0m : 2.01972
[1mStep[0m  [35/53], [94mLoss[0m : 2.37510
[1mStep[0m  [40/53], [94mLoss[0m : 2.20779
[1mStep[0m  [45/53], [94mLoss[0m : 2.20008
[1mStep[0m  [50/53], [94mLoss[0m : 2.10870

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.130, [92mTest[0m: 2.555, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.94811
[1mStep[0m  [5/53], [94mLoss[0m : 1.98315
[1mStep[0m  [10/53], [94mLoss[0m : 1.95558
[1mStep[0m  [15/53], [94mLoss[0m : 2.02723
[1mStep[0m  [20/53], [94mLoss[0m : 1.94263
[1mStep[0m  [25/53], [94mLoss[0m : 2.21251
[1mStep[0m  [30/53], [94mLoss[0m : 2.06936
[1mStep[0m  [35/53], [94mLoss[0m : 2.07935
[1mStep[0m  [40/53], [94mLoss[0m : 2.24831
[1mStep[0m  [45/53], [94mLoss[0m : 2.05685
[1mStep[0m  [50/53], [94mLoss[0m : 2.13003

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.112, [92mTest[0m: 2.599, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.95563
[1mStep[0m  [5/53], [94mLoss[0m : 1.99679
[1mStep[0m  [10/53], [94mLoss[0m : 2.01598
[1mStep[0m  [15/53], [94mLoss[0m : 2.15252
[1mStep[0m  [20/53], [94mLoss[0m : 2.02224
[1mStep[0m  [25/53], [94mLoss[0m : 2.01690
[1mStep[0m  [30/53], [94mLoss[0m : 2.11129
[1mStep[0m  [35/53], [94mLoss[0m : 2.11974
[1mStep[0m  [40/53], [94mLoss[0m : 2.08516
[1mStep[0m  [45/53], [94mLoss[0m : 2.25809
[1mStep[0m  [50/53], [94mLoss[0m : 2.35092

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.626, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.04077
[1mStep[0m  [5/53], [94mLoss[0m : 2.05799
[1mStep[0m  [10/53], [94mLoss[0m : 2.04630
[1mStep[0m  [15/53], [94mLoss[0m : 2.04814
[1mStep[0m  [20/53], [94mLoss[0m : 2.24831
[1mStep[0m  [25/53], [94mLoss[0m : 1.97566
[1mStep[0m  [30/53], [94mLoss[0m : 2.07599
[1mStep[0m  [35/53], [94mLoss[0m : 2.08241
[1mStep[0m  [40/53], [94mLoss[0m : 2.00072
[1mStep[0m  [45/53], [94mLoss[0m : 1.94035
[1mStep[0m  [50/53], [94mLoss[0m : 2.19891

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.88386
[1mStep[0m  [5/53], [94mLoss[0m : 1.80397
[1mStep[0m  [10/53], [94mLoss[0m : 2.07928
[1mStep[0m  [15/53], [94mLoss[0m : 1.98459
[1mStep[0m  [20/53], [94mLoss[0m : 1.89147
[1mStep[0m  [25/53], [94mLoss[0m : 1.89241
[1mStep[0m  [30/53], [94mLoss[0m : 2.01238
[1mStep[0m  [35/53], [94mLoss[0m : 2.00007
[1mStep[0m  [40/53], [94mLoss[0m : 1.95037
[1mStep[0m  [45/53], [94mLoss[0m : 2.17054
[1mStep[0m  [50/53], [94mLoss[0m : 1.96267

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.005, [92mTest[0m: 2.579, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.87729
[1mStep[0m  [5/53], [94mLoss[0m : 1.94906
[1mStep[0m  [10/53], [94mLoss[0m : 2.01546
[1mStep[0m  [15/53], [94mLoss[0m : 1.97009
[1mStep[0m  [20/53], [94mLoss[0m : 1.94551
[1mStep[0m  [25/53], [94mLoss[0m : 2.03554
[1mStep[0m  [30/53], [94mLoss[0m : 1.98321
[1mStep[0m  [35/53], [94mLoss[0m : 1.90697
[1mStep[0m  [40/53], [94mLoss[0m : 2.05657
[1mStep[0m  [45/53], [94mLoss[0m : 1.98503
[1mStep[0m  [50/53], [94mLoss[0m : 1.89621

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.976, [92mTest[0m: 2.526, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.90958
[1mStep[0m  [5/53], [94mLoss[0m : 1.94812
[1mStep[0m  [10/53], [94mLoss[0m : 1.72614
[1mStep[0m  [15/53], [94mLoss[0m : 1.92157
[1mStep[0m  [20/53], [94mLoss[0m : 1.96864
[1mStep[0m  [25/53], [94mLoss[0m : 1.88984
[1mStep[0m  [30/53], [94mLoss[0m : 1.98307
[1mStep[0m  [35/53], [94mLoss[0m : 1.84246
[1mStep[0m  [40/53], [94mLoss[0m : 2.00493
[1mStep[0m  [45/53], [94mLoss[0m : 2.11798
[1mStep[0m  [50/53], [94mLoss[0m : 2.08570

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.97742
[1mStep[0m  [5/53], [94mLoss[0m : 1.83412
[1mStep[0m  [10/53], [94mLoss[0m : 1.82101
[1mStep[0m  [15/53], [94mLoss[0m : 1.86712
[1mStep[0m  [20/53], [94mLoss[0m : 1.80402
[1mStep[0m  [25/53], [94mLoss[0m : 2.00027
[1mStep[0m  [30/53], [94mLoss[0m : 1.79852
[1mStep[0m  [35/53], [94mLoss[0m : 1.96831
[1mStep[0m  [40/53], [94mLoss[0m : 2.06532
[1mStep[0m  [45/53], [94mLoss[0m : 1.85079
[1mStep[0m  [50/53], [94mLoss[0m : 2.11080

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.546, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83177
[1mStep[0m  [5/53], [94mLoss[0m : 1.88385
[1mStep[0m  [10/53], [94mLoss[0m : 1.87205
[1mStep[0m  [15/53], [94mLoss[0m : 1.99806
[1mStep[0m  [20/53], [94mLoss[0m : 1.81324
[1mStep[0m  [25/53], [94mLoss[0m : 2.02852
[1mStep[0m  [30/53], [94mLoss[0m : 1.92600
[1mStep[0m  [35/53], [94mLoss[0m : 1.84452
[1mStep[0m  [40/53], [94mLoss[0m : 1.82654
[1mStep[0m  [45/53], [94mLoss[0m : 1.75792
[1mStep[0m  [50/53], [94mLoss[0m : 1.98045

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.97968
[1mStep[0m  [5/53], [94mLoss[0m : 1.86545
[1mStep[0m  [10/53], [94mLoss[0m : 1.73431
[1mStep[0m  [15/53], [94mLoss[0m : 1.71559
[1mStep[0m  [20/53], [94mLoss[0m : 1.77871
[1mStep[0m  [25/53], [94mLoss[0m : 1.85700
[1mStep[0m  [30/53], [94mLoss[0m : 1.83922
[1mStep[0m  [35/53], [94mLoss[0m : 1.83365
[1mStep[0m  [40/53], [94mLoss[0m : 1.97000
[1mStep[0m  [45/53], [94mLoss[0m : 2.03988
[1mStep[0m  [50/53], [94mLoss[0m : 1.77290

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.70621
[1mStep[0m  [5/53], [94mLoss[0m : 1.83046
[1mStep[0m  [10/53], [94mLoss[0m : 1.87442
[1mStep[0m  [15/53], [94mLoss[0m : 1.79701
[1mStep[0m  [20/53], [94mLoss[0m : 1.82115
[1mStep[0m  [25/53], [94mLoss[0m : 1.93087
[1mStep[0m  [30/53], [94mLoss[0m : 1.85863
[1mStep[0m  [35/53], [94mLoss[0m : 1.74524
[1mStep[0m  [40/53], [94mLoss[0m : 1.67021
[1mStep[0m  [45/53], [94mLoss[0m : 1.67670
[1mStep[0m  [50/53], [94mLoss[0m : 1.63034

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.792, [92mTest[0m: 2.522, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.58636
[1mStep[0m  [5/53], [94mLoss[0m : 1.85096
[1mStep[0m  [10/53], [94mLoss[0m : 1.74515
[1mStep[0m  [15/53], [94mLoss[0m : 1.70595
[1mStep[0m  [20/53], [94mLoss[0m : 1.60403
[1mStep[0m  [25/53], [94mLoss[0m : 1.65236
[1mStep[0m  [30/53], [94mLoss[0m : 1.76279
[1mStep[0m  [35/53], [94mLoss[0m : 1.82588
[1mStep[0m  [40/53], [94mLoss[0m : 1.75398
[1mStep[0m  [45/53], [94mLoss[0m : 1.82395
[1mStep[0m  [50/53], [94mLoss[0m : 1.90584

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.95864
[1mStep[0m  [5/53], [94mLoss[0m : 1.52875
[1mStep[0m  [10/53], [94mLoss[0m : 1.62248
[1mStep[0m  [15/53], [94mLoss[0m : 1.66882
[1mStep[0m  [20/53], [94mLoss[0m : 1.74740
[1mStep[0m  [25/53], [94mLoss[0m : 1.69220
[1mStep[0m  [30/53], [94mLoss[0m : 1.68710
[1mStep[0m  [35/53], [94mLoss[0m : 1.72786
[1mStep[0m  [40/53], [94mLoss[0m : 1.71402
[1mStep[0m  [45/53], [94mLoss[0m : 1.67239
[1mStep[0m  [50/53], [94mLoss[0m : 1.90289

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.58414
[1mStep[0m  [5/53], [94mLoss[0m : 1.57598
[1mStep[0m  [10/53], [94mLoss[0m : 1.78738
[1mStep[0m  [15/53], [94mLoss[0m : 1.65560
[1mStep[0m  [20/53], [94mLoss[0m : 1.93697
[1mStep[0m  [25/53], [94mLoss[0m : 1.74764
[1mStep[0m  [30/53], [94mLoss[0m : 1.67396
[1mStep[0m  [35/53], [94mLoss[0m : 1.69170
[1mStep[0m  [40/53], [94mLoss[0m : 1.68921
[1mStep[0m  [45/53], [94mLoss[0m : 1.66297
[1mStep[0m  [50/53], [94mLoss[0m : 1.71310

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.707, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71058
[1mStep[0m  [5/53], [94mLoss[0m : 1.66400
[1mStep[0m  [10/53], [94mLoss[0m : 1.81897
[1mStep[0m  [15/53], [94mLoss[0m : 1.72541
[1mStep[0m  [20/53], [94mLoss[0m : 1.72872
[1mStep[0m  [25/53], [94mLoss[0m : 1.97091
[1mStep[0m  [30/53], [94mLoss[0m : 1.61657
[1mStep[0m  [35/53], [94mLoss[0m : 1.65324
[1mStep[0m  [40/53], [94mLoss[0m : 1.64275
[1mStep[0m  [45/53], [94mLoss[0m : 1.79188
[1mStep[0m  [50/53], [94mLoss[0m : 1.77543

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73823
[1mStep[0m  [5/53], [94mLoss[0m : 1.61478
[1mStep[0m  [10/53], [94mLoss[0m : 1.67512
[1mStep[0m  [15/53], [94mLoss[0m : 1.74097
[1mStep[0m  [20/53], [94mLoss[0m : 1.56376
[1mStep[0m  [25/53], [94mLoss[0m : 1.66068
[1mStep[0m  [30/53], [94mLoss[0m : 1.72104
[1mStep[0m  [35/53], [94mLoss[0m : 1.82481
[1mStep[0m  [40/53], [94mLoss[0m : 1.45308
[1mStep[0m  [45/53], [94mLoss[0m : 1.98261
[1mStep[0m  [50/53], [94mLoss[0m : 1.66538

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.458, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63367
[1mStep[0m  [5/53], [94mLoss[0m : 1.63062
[1mStep[0m  [10/53], [94mLoss[0m : 1.65440
[1mStep[0m  [15/53], [94mLoss[0m : 1.49162
[1mStep[0m  [20/53], [94mLoss[0m : 1.66563
[1mStep[0m  [25/53], [94mLoss[0m : 1.62961
[1mStep[0m  [30/53], [94mLoss[0m : 1.72796
[1mStep[0m  [35/53], [94mLoss[0m : 1.60144
[1mStep[0m  [40/53], [94mLoss[0m : 1.65658
[1mStep[0m  [45/53], [94mLoss[0m : 1.81302
[1mStep[0m  [50/53], [94mLoss[0m : 1.67088

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.550, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.71886
[1mStep[0m  [5/53], [94mLoss[0m : 1.64307
[1mStep[0m  [10/53], [94mLoss[0m : 1.60421
[1mStep[0m  [15/53], [94mLoss[0m : 1.63091
[1mStep[0m  [20/53], [94mLoss[0m : 1.57398
[1mStep[0m  [25/53], [94mLoss[0m : 1.54111
[1mStep[0m  [30/53], [94mLoss[0m : 1.66193
[1mStep[0m  [35/53], [94mLoss[0m : 1.53483
[1mStep[0m  [40/53], [94mLoss[0m : 1.61941
[1mStep[0m  [45/53], [94mLoss[0m : 1.76047
[1mStep[0m  [50/53], [94mLoss[0m : 1.67104

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.60306
[1mStep[0m  [5/53], [94mLoss[0m : 1.63521
[1mStep[0m  [10/53], [94mLoss[0m : 1.61775
[1mStep[0m  [15/53], [94mLoss[0m : 1.58720
[1mStep[0m  [20/53], [94mLoss[0m : 1.61423
[1mStep[0m  [25/53], [94mLoss[0m : 1.56674
[1mStep[0m  [30/53], [94mLoss[0m : 1.66869
[1mStep[0m  [35/53], [94mLoss[0m : 1.51580
[1mStep[0m  [40/53], [94mLoss[0m : 1.67235
[1mStep[0m  [45/53], [94mLoss[0m : 1.55356
[1mStep[0m  [50/53], [94mLoss[0m : 1.73129

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.466, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.482
====================================

Phase 2 - Evaluation MAE:  2.4819085964789758
MAE score P1      2.374832
MAE score P2      2.481909
loss              1.626875
learning_rate      0.00505
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.40754
[1mStep[0m  [10/106], [94mLoss[0m : 10.97656
[1mStep[0m  [20/106], [94mLoss[0m : 11.10135
[1mStep[0m  [30/106], [94mLoss[0m : 9.84256
[1mStep[0m  [40/106], [94mLoss[0m : 9.80023
[1mStep[0m  [50/106], [94mLoss[0m : 9.18068
[1mStep[0m  [60/106], [94mLoss[0m : 8.05727
[1mStep[0m  [70/106], [94mLoss[0m : 7.53702
[1mStep[0m  [80/106], [94mLoss[0m : 7.24674
[1mStep[0m  [90/106], [94mLoss[0m : 6.84382
[1mStep[0m  [100/106], [94mLoss[0m : 6.85259

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.826, [92mTest[0m: 11.002, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.12267
[1mStep[0m  [10/106], [94mLoss[0m : 5.47396
[1mStep[0m  [20/106], [94mLoss[0m : 4.93050
[1mStep[0m  [30/106], [94mLoss[0m : 4.10550
[1mStep[0m  [40/106], [94mLoss[0m : 3.63487
[1mStep[0m  [50/106], [94mLoss[0m : 4.41433
[1mStep[0m  [60/106], [94mLoss[0m : 3.26414
[1mStep[0m  [70/106], [94mLoss[0m : 3.25088
[1mStep[0m  [80/106], [94mLoss[0m : 3.18251
[1mStep[0m  [90/106], [94mLoss[0m : 3.36799
[1mStep[0m  [100/106], [94mLoss[0m : 2.77328

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.064, [92mTest[0m: 7.192, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.06674
[1mStep[0m  [10/106], [94mLoss[0m : 2.72585
[1mStep[0m  [20/106], [94mLoss[0m : 3.01707
[1mStep[0m  [30/106], [94mLoss[0m : 2.74084
[1mStep[0m  [40/106], [94mLoss[0m : 2.56384
[1mStep[0m  [50/106], [94mLoss[0m : 2.86372
[1mStep[0m  [60/106], [94mLoss[0m : 3.15469
[1mStep[0m  [70/106], [94mLoss[0m : 2.92669
[1mStep[0m  [80/106], [94mLoss[0m : 2.92814
[1mStep[0m  [90/106], [94mLoss[0m : 2.99454
[1mStep[0m  [100/106], [94mLoss[0m : 2.72032

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.780, [92mTest[0m: 3.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36917
[1mStep[0m  [10/106], [94mLoss[0m : 2.67329
[1mStep[0m  [20/106], [94mLoss[0m : 2.83642
[1mStep[0m  [30/106], [94mLoss[0m : 2.70463
[1mStep[0m  [40/106], [94mLoss[0m : 3.00055
[1mStep[0m  [50/106], [94mLoss[0m : 2.83232
[1mStep[0m  [60/106], [94mLoss[0m : 2.56815
[1mStep[0m  [70/106], [94mLoss[0m : 2.62059
[1mStep[0m  [80/106], [94mLoss[0m : 2.78480
[1mStep[0m  [90/106], [94mLoss[0m : 2.76131
[1mStep[0m  [100/106], [94mLoss[0m : 2.55211

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.587, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45384
[1mStep[0m  [10/106], [94mLoss[0m : 2.58924
[1mStep[0m  [20/106], [94mLoss[0m : 2.78918
[1mStep[0m  [30/106], [94mLoss[0m : 2.81362
[1mStep[0m  [40/106], [94mLoss[0m : 2.43673
[1mStep[0m  [50/106], [94mLoss[0m : 3.08321
[1mStep[0m  [60/106], [94mLoss[0m : 2.70946
[1mStep[0m  [70/106], [94mLoss[0m : 2.75537
[1mStep[0m  [80/106], [94mLoss[0m : 2.54574
[1mStep[0m  [90/106], [94mLoss[0m : 2.53301
[1mStep[0m  [100/106], [94mLoss[0m : 2.39545

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71817
[1mStep[0m  [10/106], [94mLoss[0m : 2.50812
[1mStep[0m  [20/106], [94mLoss[0m : 2.57307
[1mStep[0m  [30/106], [94mLoss[0m : 2.70667
[1mStep[0m  [40/106], [94mLoss[0m : 2.63250
[1mStep[0m  [50/106], [94mLoss[0m : 2.68423
[1mStep[0m  [60/106], [94mLoss[0m : 2.57194
[1mStep[0m  [70/106], [94mLoss[0m : 2.71429
[1mStep[0m  [80/106], [94mLoss[0m : 2.65787
[1mStep[0m  [90/106], [94mLoss[0m : 2.55073
[1mStep[0m  [100/106], [94mLoss[0m : 2.86218

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.508, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41394
[1mStep[0m  [10/106], [94mLoss[0m : 2.53545
[1mStep[0m  [20/106], [94mLoss[0m : 2.74443
[1mStep[0m  [30/106], [94mLoss[0m : 2.80643
[1mStep[0m  [40/106], [94mLoss[0m : 2.90183
[1mStep[0m  [50/106], [94mLoss[0m : 2.64590
[1mStep[0m  [60/106], [94mLoss[0m : 2.75731
[1mStep[0m  [70/106], [94mLoss[0m : 2.89690
[1mStep[0m  [80/106], [94mLoss[0m : 2.49340
[1mStep[0m  [90/106], [94mLoss[0m : 2.68470
[1mStep[0m  [100/106], [94mLoss[0m : 2.50398

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.457, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53044
[1mStep[0m  [10/106], [94mLoss[0m : 2.26299
[1mStep[0m  [20/106], [94mLoss[0m : 2.52244
[1mStep[0m  [30/106], [94mLoss[0m : 2.79772
[1mStep[0m  [40/106], [94mLoss[0m : 2.62786
[1mStep[0m  [50/106], [94mLoss[0m : 2.75403
[1mStep[0m  [60/106], [94mLoss[0m : 2.42789
[1mStep[0m  [70/106], [94mLoss[0m : 2.58069
[1mStep[0m  [80/106], [94mLoss[0m : 2.48892
[1mStep[0m  [90/106], [94mLoss[0m : 2.69746
[1mStep[0m  [100/106], [94mLoss[0m : 2.48038

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26978
[1mStep[0m  [10/106], [94mLoss[0m : 2.68292
[1mStep[0m  [20/106], [94mLoss[0m : 2.32035
[1mStep[0m  [30/106], [94mLoss[0m : 2.63195
[1mStep[0m  [40/106], [94mLoss[0m : 2.77216
[1mStep[0m  [50/106], [94mLoss[0m : 2.64845
[1mStep[0m  [60/106], [94mLoss[0m : 2.67968
[1mStep[0m  [70/106], [94mLoss[0m : 2.41811
[1mStep[0m  [80/106], [94mLoss[0m : 2.45698
[1mStep[0m  [90/106], [94mLoss[0m : 2.15176
[1mStep[0m  [100/106], [94mLoss[0m : 2.91210

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39947
[1mStep[0m  [10/106], [94mLoss[0m : 2.74014
[1mStep[0m  [20/106], [94mLoss[0m : 2.61292
[1mStep[0m  [30/106], [94mLoss[0m : 2.66143
[1mStep[0m  [40/106], [94mLoss[0m : 2.89514
[1mStep[0m  [50/106], [94mLoss[0m : 2.32561
[1mStep[0m  [60/106], [94mLoss[0m : 2.42931
[1mStep[0m  [70/106], [94mLoss[0m : 2.51493
[1mStep[0m  [80/106], [94mLoss[0m : 2.38827
[1mStep[0m  [90/106], [94mLoss[0m : 2.49241
[1mStep[0m  [100/106], [94mLoss[0m : 2.48267

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54373
[1mStep[0m  [10/106], [94mLoss[0m : 2.45221
[1mStep[0m  [20/106], [94mLoss[0m : 2.34344
[1mStep[0m  [30/106], [94mLoss[0m : 2.48259
[1mStep[0m  [40/106], [94mLoss[0m : 2.88706
[1mStep[0m  [50/106], [94mLoss[0m : 2.49306
[1mStep[0m  [60/106], [94mLoss[0m : 2.61030
[1mStep[0m  [70/106], [94mLoss[0m : 2.65684
[1mStep[0m  [80/106], [94mLoss[0m : 2.21980
[1mStep[0m  [90/106], [94mLoss[0m : 2.59592
[1mStep[0m  [100/106], [94mLoss[0m : 2.47857

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43085
[1mStep[0m  [10/106], [94mLoss[0m : 2.64568
[1mStep[0m  [20/106], [94mLoss[0m : 2.41593
[1mStep[0m  [30/106], [94mLoss[0m : 2.40148
[1mStep[0m  [40/106], [94mLoss[0m : 2.41802
[1mStep[0m  [50/106], [94mLoss[0m : 2.71771
[1mStep[0m  [60/106], [94mLoss[0m : 2.65060
[1mStep[0m  [70/106], [94mLoss[0m : 2.74343
[1mStep[0m  [80/106], [94mLoss[0m : 2.58900
[1mStep[0m  [90/106], [94mLoss[0m : 2.47002
[1mStep[0m  [100/106], [94mLoss[0m : 2.68602

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50294
[1mStep[0m  [10/106], [94mLoss[0m : 2.40742
[1mStep[0m  [20/106], [94mLoss[0m : 2.57398
[1mStep[0m  [30/106], [94mLoss[0m : 2.12392
[1mStep[0m  [40/106], [94mLoss[0m : 2.91207
[1mStep[0m  [50/106], [94mLoss[0m : 2.50113
[1mStep[0m  [60/106], [94mLoss[0m : 2.54762
[1mStep[0m  [70/106], [94mLoss[0m : 2.45706
[1mStep[0m  [80/106], [94mLoss[0m : 2.28269
[1mStep[0m  [90/106], [94mLoss[0m : 2.58399
[1mStep[0m  [100/106], [94mLoss[0m : 2.41205

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52999
[1mStep[0m  [10/106], [94mLoss[0m : 2.64441
[1mStep[0m  [20/106], [94mLoss[0m : 2.30807
[1mStep[0m  [30/106], [94mLoss[0m : 2.59363
[1mStep[0m  [40/106], [94mLoss[0m : 2.24013
[1mStep[0m  [50/106], [94mLoss[0m : 2.46483
[1mStep[0m  [60/106], [94mLoss[0m : 2.55795
[1mStep[0m  [70/106], [94mLoss[0m : 2.68205
[1mStep[0m  [80/106], [94mLoss[0m : 2.88648
[1mStep[0m  [90/106], [94mLoss[0m : 2.48788
[1mStep[0m  [100/106], [94mLoss[0m : 2.70215

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49611
[1mStep[0m  [10/106], [94mLoss[0m : 2.29382
[1mStep[0m  [20/106], [94mLoss[0m : 2.42953
[1mStep[0m  [30/106], [94mLoss[0m : 2.42837
[1mStep[0m  [40/106], [94mLoss[0m : 2.57055
[1mStep[0m  [50/106], [94mLoss[0m : 2.43610
[1mStep[0m  [60/106], [94mLoss[0m : 2.45514
[1mStep[0m  [70/106], [94mLoss[0m : 2.45876
[1mStep[0m  [80/106], [94mLoss[0m : 2.19624
[1mStep[0m  [90/106], [94mLoss[0m : 2.54295
[1mStep[0m  [100/106], [94mLoss[0m : 2.72975

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64272
[1mStep[0m  [10/106], [94mLoss[0m : 2.51716
[1mStep[0m  [20/106], [94mLoss[0m : 2.54330
[1mStep[0m  [30/106], [94mLoss[0m : 2.62481
[1mStep[0m  [40/106], [94mLoss[0m : 2.52356
[1mStep[0m  [50/106], [94mLoss[0m : 2.64372
[1mStep[0m  [60/106], [94mLoss[0m : 2.69424
[1mStep[0m  [70/106], [94mLoss[0m : 2.72770
[1mStep[0m  [80/106], [94mLoss[0m : 2.51566
[1mStep[0m  [90/106], [94mLoss[0m : 2.71150
[1mStep[0m  [100/106], [94mLoss[0m : 2.32348

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80587
[1mStep[0m  [10/106], [94mLoss[0m : 2.37620
[1mStep[0m  [20/106], [94mLoss[0m : 2.34141
[1mStep[0m  [30/106], [94mLoss[0m : 2.86847
[1mStep[0m  [40/106], [94mLoss[0m : 2.38119
[1mStep[0m  [50/106], [94mLoss[0m : 2.31076
[1mStep[0m  [60/106], [94mLoss[0m : 2.65064
[1mStep[0m  [70/106], [94mLoss[0m : 2.46540
[1mStep[0m  [80/106], [94mLoss[0m : 2.27996
[1mStep[0m  [90/106], [94mLoss[0m : 2.67070
[1mStep[0m  [100/106], [94mLoss[0m : 2.55478

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44124
[1mStep[0m  [10/106], [94mLoss[0m : 2.52430
[1mStep[0m  [20/106], [94mLoss[0m : 2.58983
[1mStep[0m  [30/106], [94mLoss[0m : 2.36521
[1mStep[0m  [40/106], [94mLoss[0m : 2.76366
[1mStep[0m  [50/106], [94mLoss[0m : 2.63200
[1mStep[0m  [60/106], [94mLoss[0m : 2.42847
[1mStep[0m  [70/106], [94mLoss[0m : 2.60097
[1mStep[0m  [80/106], [94mLoss[0m : 2.62076
[1mStep[0m  [90/106], [94mLoss[0m : 2.44255
[1mStep[0m  [100/106], [94mLoss[0m : 2.59272

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73345
[1mStep[0m  [10/106], [94mLoss[0m : 2.43840
[1mStep[0m  [20/106], [94mLoss[0m : 2.62055
[1mStep[0m  [30/106], [94mLoss[0m : 2.43245
[1mStep[0m  [40/106], [94mLoss[0m : 2.28649
[1mStep[0m  [50/106], [94mLoss[0m : 2.64850
[1mStep[0m  [60/106], [94mLoss[0m : 2.72362
[1mStep[0m  [70/106], [94mLoss[0m : 2.49070
[1mStep[0m  [80/106], [94mLoss[0m : 2.47857
[1mStep[0m  [90/106], [94mLoss[0m : 2.21616
[1mStep[0m  [100/106], [94mLoss[0m : 2.52660

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71804
[1mStep[0m  [10/106], [94mLoss[0m : 2.35193
[1mStep[0m  [20/106], [94mLoss[0m : 2.58717
[1mStep[0m  [30/106], [94mLoss[0m : 2.47548
[1mStep[0m  [40/106], [94mLoss[0m : 2.57768
[1mStep[0m  [50/106], [94mLoss[0m : 2.34026
[1mStep[0m  [60/106], [94mLoss[0m : 2.27600
[1mStep[0m  [70/106], [94mLoss[0m : 2.19693
[1mStep[0m  [80/106], [94mLoss[0m : 2.28918
[1mStep[0m  [90/106], [94mLoss[0m : 2.57285
[1mStep[0m  [100/106], [94mLoss[0m : 2.42750

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.362, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24510
[1mStep[0m  [10/106], [94mLoss[0m : 2.68726
[1mStep[0m  [20/106], [94mLoss[0m : 2.48591
[1mStep[0m  [30/106], [94mLoss[0m : 2.63283
[1mStep[0m  [40/106], [94mLoss[0m : 2.37964
[1mStep[0m  [50/106], [94mLoss[0m : 2.24920
[1mStep[0m  [60/106], [94mLoss[0m : 2.26621
[1mStep[0m  [70/106], [94mLoss[0m : 2.52934
[1mStep[0m  [80/106], [94mLoss[0m : 2.82638
[1mStep[0m  [90/106], [94mLoss[0m : 2.52120
[1mStep[0m  [100/106], [94mLoss[0m : 2.82663

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.361, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32720
[1mStep[0m  [10/106], [94mLoss[0m : 2.48538
[1mStep[0m  [20/106], [94mLoss[0m : 2.54761
[1mStep[0m  [30/106], [94mLoss[0m : 2.42074
[1mStep[0m  [40/106], [94mLoss[0m : 2.57537
[1mStep[0m  [50/106], [94mLoss[0m : 2.51238
[1mStep[0m  [60/106], [94mLoss[0m : 2.43498
[1mStep[0m  [70/106], [94mLoss[0m : 2.23206
[1mStep[0m  [80/106], [94mLoss[0m : 2.21450
[1mStep[0m  [90/106], [94mLoss[0m : 2.47967
[1mStep[0m  [100/106], [94mLoss[0m : 2.37484

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78348
[1mStep[0m  [10/106], [94mLoss[0m : 2.35710
[1mStep[0m  [20/106], [94mLoss[0m : 2.63897
[1mStep[0m  [30/106], [94mLoss[0m : 2.13462
[1mStep[0m  [40/106], [94mLoss[0m : 2.33593
[1mStep[0m  [50/106], [94mLoss[0m : 2.53441
[1mStep[0m  [60/106], [94mLoss[0m : 2.62138
[1mStep[0m  [70/106], [94mLoss[0m : 2.75179
[1mStep[0m  [80/106], [94mLoss[0m : 2.53469
[1mStep[0m  [90/106], [94mLoss[0m : 2.62182
[1mStep[0m  [100/106], [94mLoss[0m : 2.52134

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.80244
[1mStep[0m  [10/106], [94mLoss[0m : 2.56269
[1mStep[0m  [20/106], [94mLoss[0m : 2.47011
[1mStep[0m  [30/106], [94mLoss[0m : 2.21254
[1mStep[0m  [40/106], [94mLoss[0m : 2.38131
[1mStep[0m  [50/106], [94mLoss[0m : 2.60779
[1mStep[0m  [60/106], [94mLoss[0m : 2.25890
[1mStep[0m  [70/106], [94mLoss[0m : 2.71167
[1mStep[0m  [80/106], [94mLoss[0m : 2.42457
[1mStep[0m  [90/106], [94mLoss[0m : 2.16045
[1mStep[0m  [100/106], [94mLoss[0m : 2.57227

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.355, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43833
[1mStep[0m  [10/106], [94mLoss[0m : 2.59609
[1mStep[0m  [20/106], [94mLoss[0m : 2.48868
[1mStep[0m  [30/106], [94mLoss[0m : 2.30552
[1mStep[0m  [40/106], [94mLoss[0m : 2.28355
[1mStep[0m  [50/106], [94mLoss[0m : 2.53239
[1mStep[0m  [60/106], [94mLoss[0m : 2.27443
[1mStep[0m  [70/106], [94mLoss[0m : 2.32870
[1mStep[0m  [80/106], [94mLoss[0m : 2.69957
[1mStep[0m  [90/106], [94mLoss[0m : 2.34666
[1mStep[0m  [100/106], [94mLoss[0m : 2.46819

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.368, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36726
[1mStep[0m  [10/106], [94mLoss[0m : 2.47442
[1mStep[0m  [20/106], [94mLoss[0m : 2.38108
[1mStep[0m  [30/106], [94mLoss[0m : 2.09622
[1mStep[0m  [40/106], [94mLoss[0m : 2.48238
[1mStep[0m  [50/106], [94mLoss[0m : 2.70197
[1mStep[0m  [60/106], [94mLoss[0m : 2.11283
[1mStep[0m  [70/106], [94mLoss[0m : 2.69776
[1mStep[0m  [80/106], [94mLoss[0m : 2.25549
[1mStep[0m  [90/106], [94mLoss[0m : 2.73044
[1mStep[0m  [100/106], [94mLoss[0m : 2.31728

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20732
[1mStep[0m  [10/106], [94mLoss[0m : 2.43065
[1mStep[0m  [20/106], [94mLoss[0m : 2.63112
[1mStep[0m  [30/106], [94mLoss[0m : 2.25024
[1mStep[0m  [40/106], [94mLoss[0m : 2.11036
[1mStep[0m  [50/106], [94mLoss[0m : 2.50540
[1mStep[0m  [60/106], [94mLoss[0m : 2.50696
[1mStep[0m  [70/106], [94mLoss[0m : 2.25278
[1mStep[0m  [80/106], [94mLoss[0m : 2.56916
[1mStep[0m  [90/106], [94mLoss[0m : 2.43189
[1mStep[0m  [100/106], [94mLoss[0m : 2.14363

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.352, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67854
[1mStep[0m  [10/106], [94mLoss[0m : 2.19518
[1mStep[0m  [20/106], [94mLoss[0m : 2.55633
[1mStep[0m  [30/106], [94mLoss[0m : 2.38483
[1mStep[0m  [40/106], [94mLoss[0m : 2.26589
[1mStep[0m  [50/106], [94mLoss[0m : 2.27958
[1mStep[0m  [60/106], [94mLoss[0m : 2.34738
[1mStep[0m  [70/106], [94mLoss[0m : 2.59774
[1mStep[0m  [80/106], [94mLoss[0m : 2.61043
[1mStep[0m  [90/106], [94mLoss[0m : 2.77577
[1mStep[0m  [100/106], [94mLoss[0m : 2.60919

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21184
[1mStep[0m  [10/106], [94mLoss[0m : 2.45305
[1mStep[0m  [20/106], [94mLoss[0m : 2.31111
[1mStep[0m  [30/106], [94mLoss[0m : 2.68850
[1mStep[0m  [40/106], [94mLoss[0m : 2.53381
[1mStep[0m  [50/106], [94mLoss[0m : 2.52232
[1mStep[0m  [60/106], [94mLoss[0m : 2.64243
[1mStep[0m  [70/106], [94mLoss[0m : 2.35248
[1mStep[0m  [80/106], [94mLoss[0m : 2.48894
[1mStep[0m  [90/106], [94mLoss[0m : 2.46680
[1mStep[0m  [100/106], [94mLoss[0m : 1.99475

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.355, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34354
[1mStep[0m  [10/106], [94mLoss[0m : 2.36875
[1mStep[0m  [20/106], [94mLoss[0m : 2.83704
[1mStep[0m  [30/106], [94mLoss[0m : 2.39075
[1mStep[0m  [40/106], [94mLoss[0m : 2.33132
[1mStep[0m  [50/106], [94mLoss[0m : 2.63084
[1mStep[0m  [60/106], [94mLoss[0m : 2.24070
[1mStep[0m  [70/106], [94mLoss[0m : 2.59576
[1mStep[0m  [80/106], [94mLoss[0m : 2.69304
[1mStep[0m  [90/106], [94mLoss[0m : 2.20188
[1mStep[0m  [100/106], [94mLoss[0m : 2.34260

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.342
====================================

Phase 1 - Evaluation MAE:  2.3416700678051643
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.15059
[1mStep[0m  [10/106], [94mLoss[0m : 2.34651
[1mStep[0m  [20/106], [94mLoss[0m : 2.84198
[1mStep[0m  [30/106], [94mLoss[0m : 2.53469
[1mStep[0m  [40/106], [94mLoss[0m : 2.27907
[1mStep[0m  [50/106], [94mLoss[0m : 2.27083
[1mStep[0m  [60/106], [94mLoss[0m : 2.50598
[1mStep[0m  [70/106], [94mLoss[0m : 2.54306
[1mStep[0m  [80/106], [94mLoss[0m : 2.28512
[1mStep[0m  [90/106], [94mLoss[0m : 2.68314
[1mStep[0m  [100/106], [94mLoss[0m : 2.45977

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61036
[1mStep[0m  [10/106], [94mLoss[0m : 2.52464
[1mStep[0m  [20/106], [94mLoss[0m : 2.71444
[1mStep[0m  [30/106], [94mLoss[0m : 2.65224
[1mStep[0m  [40/106], [94mLoss[0m : 2.25149
[1mStep[0m  [50/106], [94mLoss[0m : 2.49085
[1mStep[0m  [60/106], [94mLoss[0m : 2.69626
[1mStep[0m  [70/106], [94mLoss[0m : 2.48989
[1mStep[0m  [80/106], [94mLoss[0m : 2.59653
[1mStep[0m  [90/106], [94mLoss[0m : 2.48609
[1mStep[0m  [100/106], [94mLoss[0m : 2.37414

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.691, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31452
[1mStep[0m  [10/106], [94mLoss[0m : 2.37605
[1mStep[0m  [20/106], [94mLoss[0m : 2.59815
[1mStep[0m  [30/106], [94mLoss[0m : 2.49725
[1mStep[0m  [40/106], [94mLoss[0m : 2.44265
[1mStep[0m  [50/106], [94mLoss[0m : 2.63075
[1mStep[0m  [60/106], [94mLoss[0m : 2.63057
[1mStep[0m  [70/106], [94mLoss[0m : 2.55069
[1mStep[0m  [80/106], [94mLoss[0m : 2.31917
[1mStep[0m  [90/106], [94mLoss[0m : 2.74286
[1mStep[0m  [100/106], [94mLoss[0m : 2.50866

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.752, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42778
[1mStep[0m  [10/106], [94mLoss[0m : 2.38953
[1mStep[0m  [20/106], [94mLoss[0m : 2.41422
[1mStep[0m  [30/106], [94mLoss[0m : 2.41452
[1mStep[0m  [40/106], [94mLoss[0m : 2.32674
[1mStep[0m  [50/106], [94mLoss[0m : 2.68420
[1mStep[0m  [60/106], [94mLoss[0m : 2.59273
[1mStep[0m  [70/106], [94mLoss[0m : 2.98847
[1mStep[0m  [80/106], [94mLoss[0m : 2.66742
[1mStep[0m  [90/106], [94mLoss[0m : 2.18542
[1mStep[0m  [100/106], [94mLoss[0m : 2.34071

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41018
[1mStep[0m  [10/106], [94mLoss[0m : 2.36669
[1mStep[0m  [20/106], [94mLoss[0m : 2.32564
[1mStep[0m  [30/106], [94mLoss[0m : 2.48545
[1mStep[0m  [40/106], [94mLoss[0m : 2.26211
[1mStep[0m  [50/106], [94mLoss[0m : 2.47499
[1mStep[0m  [60/106], [94mLoss[0m : 2.50090
[1mStep[0m  [70/106], [94mLoss[0m : 2.15669
[1mStep[0m  [80/106], [94mLoss[0m : 2.26016
[1mStep[0m  [90/106], [94mLoss[0m : 2.59645
[1mStep[0m  [100/106], [94mLoss[0m : 2.47238

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.497, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47705
[1mStep[0m  [10/106], [94mLoss[0m : 2.39195
[1mStep[0m  [20/106], [94mLoss[0m : 2.21340
[1mStep[0m  [30/106], [94mLoss[0m : 2.59290
[1mStep[0m  [40/106], [94mLoss[0m : 2.53342
[1mStep[0m  [50/106], [94mLoss[0m : 2.04327
[1mStep[0m  [60/106], [94mLoss[0m : 2.34550
[1mStep[0m  [70/106], [94mLoss[0m : 2.16516
[1mStep[0m  [80/106], [94mLoss[0m : 2.25555
[1mStep[0m  [90/106], [94mLoss[0m : 2.37662
[1mStep[0m  [100/106], [94mLoss[0m : 2.32521

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08508
[1mStep[0m  [10/106], [94mLoss[0m : 2.22100
[1mStep[0m  [20/106], [94mLoss[0m : 2.46134
[1mStep[0m  [30/106], [94mLoss[0m : 2.29226
[1mStep[0m  [40/106], [94mLoss[0m : 2.32863
[1mStep[0m  [50/106], [94mLoss[0m : 2.26751
[1mStep[0m  [60/106], [94mLoss[0m : 2.38378
[1mStep[0m  [70/106], [94mLoss[0m : 2.22428
[1mStep[0m  [80/106], [94mLoss[0m : 2.59198
[1mStep[0m  [90/106], [94mLoss[0m : 2.22360
[1mStep[0m  [100/106], [94mLoss[0m : 2.36291

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.613, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38930
[1mStep[0m  [10/106], [94mLoss[0m : 2.17555
[1mStep[0m  [20/106], [94mLoss[0m : 2.34271
[1mStep[0m  [30/106], [94mLoss[0m : 2.51901
[1mStep[0m  [40/106], [94mLoss[0m : 2.38434
[1mStep[0m  [50/106], [94mLoss[0m : 1.78003
[1mStep[0m  [60/106], [94mLoss[0m : 2.01717
[1mStep[0m  [70/106], [94mLoss[0m : 2.42615
[1mStep[0m  [80/106], [94mLoss[0m : 2.30366
[1mStep[0m  [90/106], [94mLoss[0m : 2.20599
[1mStep[0m  [100/106], [94mLoss[0m : 2.41468

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.475, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90378
[1mStep[0m  [10/106], [94mLoss[0m : 2.00246
[1mStep[0m  [20/106], [94mLoss[0m : 2.34827
[1mStep[0m  [30/106], [94mLoss[0m : 2.35165
[1mStep[0m  [40/106], [94mLoss[0m : 2.31351
[1mStep[0m  [50/106], [94mLoss[0m : 2.21704
[1mStep[0m  [60/106], [94mLoss[0m : 2.14223
[1mStep[0m  [70/106], [94mLoss[0m : 2.07103
[1mStep[0m  [80/106], [94mLoss[0m : 2.15203
[1mStep[0m  [90/106], [94mLoss[0m : 2.23544
[1mStep[0m  [100/106], [94mLoss[0m : 1.89738

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05932
[1mStep[0m  [10/106], [94mLoss[0m : 2.13873
[1mStep[0m  [20/106], [94mLoss[0m : 2.43401
[1mStep[0m  [30/106], [94mLoss[0m : 2.25022
[1mStep[0m  [40/106], [94mLoss[0m : 2.39139
[1mStep[0m  [50/106], [94mLoss[0m : 2.34100
[1mStep[0m  [60/106], [94mLoss[0m : 2.12115
[1mStep[0m  [70/106], [94mLoss[0m : 2.06140
[1mStep[0m  [80/106], [94mLoss[0m : 2.12407
[1mStep[0m  [90/106], [94mLoss[0m : 2.35825
[1mStep[0m  [100/106], [94mLoss[0m : 2.70715

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07512
[1mStep[0m  [10/106], [94mLoss[0m : 2.32718
[1mStep[0m  [20/106], [94mLoss[0m : 2.02814
[1mStep[0m  [30/106], [94mLoss[0m : 2.16743
[1mStep[0m  [40/106], [94mLoss[0m : 2.33238
[1mStep[0m  [50/106], [94mLoss[0m : 2.42016
[1mStep[0m  [60/106], [94mLoss[0m : 2.44051
[1mStep[0m  [70/106], [94mLoss[0m : 2.37270
[1mStep[0m  [80/106], [94mLoss[0m : 2.00657
[1mStep[0m  [90/106], [94mLoss[0m : 1.82695
[1mStep[0m  [100/106], [94mLoss[0m : 2.32016

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07234
[1mStep[0m  [10/106], [94mLoss[0m : 2.26890
[1mStep[0m  [20/106], [94mLoss[0m : 2.14173
[1mStep[0m  [30/106], [94mLoss[0m : 1.99095
[1mStep[0m  [40/106], [94mLoss[0m : 2.02982
[1mStep[0m  [50/106], [94mLoss[0m : 1.96540
[1mStep[0m  [60/106], [94mLoss[0m : 2.43539
[1mStep[0m  [70/106], [94mLoss[0m : 2.33295
[1mStep[0m  [80/106], [94mLoss[0m : 2.27559
[1mStep[0m  [90/106], [94mLoss[0m : 2.39876
[1mStep[0m  [100/106], [94mLoss[0m : 1.98211

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24618
[1mStep[0m  [10/106], [94mLoss[0m : 1.73376
[1mStep[0m  [20/106], [94mLoss[0m : 2.02561
[1mStep[0m  [30/106], [94mLoss[0m : 2.38146
[1mStep[0m  [40/106], [94mLoss[0m : 1.89465
[1mStep[0m  [50/106], [94mLoss[0m : 2.17936
[1mStep[0m  [60/106], [94mLoss[0m : 2.12250
[1mStep[0m  [70/106], [94mLoss[0m : 2.52035
[1mStep[0m  [80/106], [94mLoss[0m : 2.47776
[1mStep[0m  [90/106], [94mLoss[0m : 2.07066
[1mStep[0m  [100/106], [94mLoss[0m : 1.88061

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.098, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21127
[1mStep[0m  [10/106], [94mLoss[0m : 2.30420
[1mStep[0m  [20/106], [94mLoss[0m : 2.10868
[1mStep[0m  [30/106], [94mLoss[0m : 2.16922
[1mStep[0m  [40/106], [94mLoss[0m : 1.88288
[1mStep[0m  [50/106], [94mLoss[0m : 1.97701
[1mStep[0m  [60/106], [94mLoss[0m : 2.36251
[1mStep[0m  [70/106], [94mLoss[0m : 1.83293
[1mStep[0m  [80/106], [94mLoss[0m : 2.18968
[1mStep[0m  [90/106], [94mLoss[0m : 2.21660
[1mStep[0m  [100/106], [94mLoss[0m : 1.79103

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76430
[1mStep[0m  [10/106], [94mLoss[0m : 2.08382
[1mStep[0m  [20/106], [94mLoss[0m : 2.01597
[1mStep[0m  [30/106], [94mLoss[0m : 1.86417
[1mStep[0m  [40/106], [94mLoss[0m : 2.18637
[1mStep[0m  [50/106], [94mLoss[0m : 2.13656
[1mStep[0m  [60/106], [94mLoss[0m : 2.27826
[1mStep[0m  [70/106], [94mLoss[0m : 2.07354
[1mStep[0m  [80/106], [94mLoss[0m : 2.26642
[1mStep[0m  [90/106], [94mLoss[0m : 2.19239
[1mStep[0m  [100/106], [94mLoss[0m : 2.01835

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00596
[1mStep[0m  [10/106], [94mLoss[0m : 2.13677
[1mStep[0m  [20/106], [94mLoss[0m : 2.02184
[1mStep[0m  [30/106], [94mLoss[0m : 1.94037
[1mStep[0m  [40/106], [94mLoss[0m : 1.85909
[1mStep[0m  [50/106], [94mLoss[0m : 2.03421
[1mStep[0m  [60/106], [94mLoss[0m : 1.98496
[1mStep[0m  [70/106], [94mLoss[0m : 1.97086
[1mStep[0m  [80/106], [94mLoss[0m : 2.27954
[1mStep[0m  [90/106], [94mLoss[0m : 2.10371
[1mStep[0m  [100/106], [94mLoss[0m : 2.08041

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63693
[1mStep[0m  [10/106], [94mLoss[0m : 2.01773
[1mStep[0m  [20/106], [94mLoss[0m : 2.22208
[1mStep[0m  [30/106], [94mLoss[0m : 2.11938
[1mStep[0m  [40/106], [94mLoss[0m : 1.95545
[1mStep[0m  [50/106], [94mLoss[0m : 1.87271
[1mStep[0m  [60/106], [94mLoss[0m : 1.93288
[1mStep[0m  [70/106], [94mLoss[0m : 1.78690
[1mStep[0m  [80/106], [94mLoss[0m : 2.19823
[1mStep[0m  [90/106], [94mLoss[0m : 2.13810
[1mStep[0m  [100/106], [94mLoss[0m : 1.84815

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07500
[1mStep[0m  [10/106], [94mLoss[0m : 2.11434
[1mStep[0m  [20/106], [94mLoss[0m : 1.77186
[1mStep[0m  [30/106], [94mLoss[0m : 1.95176
[1mStep[0m  [40/106], [94mLoss[0m : 1.82991
[1mStep[0m  [50/106], [94mLoss[0m : 2.15558
[1mStep[0m  [60/106], [94mLoss[0m : 1.84799
[1mStep[0m  [70/106], [94mLoss[0m : 2.09569
[1mStep[0m  [80/106], [94mLoss[0m : 1.89484
[1mStep[0m  [90/106], [94mLoss[0m : 1.69874
[1mStep[0m  [100/106], [94mLoss[0m : 2.04322

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.93125
[1mStep[0m  [10/106], [94mLoss[0m : 1.91896
[1mStep[0m  [20/106], [94mLoss[0m : 1.65288
[1mStep[0m  [30/106], [94mLoss[0m : 2.02545
[1mStep[0m  [40/106], [94mLoss[0m : 1.77697
[1mStep[0m  [50/106], [94mLoss[0m : 1.77714
[1mStep[0m  [60/106], [94mLoss[0m : 2.10876
[1mStep[0m  [70/106], [94mLoss[0m : 2.16165
[1mStep[0m  [80/106], [94mLoss[0m : 2.15871
[1mStep[0m  [90/106], [94mLoss[0m : 1.81790
[1mStep[0m  [100/106], [94mLoss[0m : 1.94025

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70970
[1mStep[0m  [10/106], [94mLoss[0m : 1.82976
[1mStep[0m  [20/106], [94mLoss[0m : 1.78097
[1mStep[0m  [30/106], [94mLoss[0m : 1.70304
[1mStep[0m  [40/106], [94mLoss[0m : 1.97970
[1mStep[0m  [50/106], [94mLoss[0m : 2.44996
[1mStep[0m  [60/106], [94mLoss[0m : 1.88422
[1mStep[0m  [70/106], [94mLoss[0m : 1.95350
[1mStep[0m  [80/106], [94mLoss[0m : 2.10498
[1mStep[0m  [90/106], [94mLoss[0m : 1.86723
[1mStep[0m  [100/106], [94mLoss[0m : 1.91495

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.01710
[1mStep[0m  [10/106], [94mLoss[0m : 1.88202
[1mStep[0m  [20/106], [94mLoss[0m : 1.58685
[1mStep[0m  [30/106], [94mLoss[0m : 1.94028
[1mStep[0m  [40/106], [94mLoss[0m : 1.66874
[1mStep[0m  [50/106], [94mLoss[0m : 1.85128
[1mStep[0m  [60/106], [94mLoss[0m : 2.10313
[1mStep[0m  [70/106], [94mLoss[0m : 1.74352
[1mStep[0m  [80/106], [94mLoss[0m : 1.83864
[1mStep[0m  [90/106], [94mLoss[0m : 1.64415
[1mStep[0m  [100/106], [94mLoss[0m : 1.87040

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70365
[1mStep[0m  [10/106], [94mLoss[0m : 1.78721
[1mStep[0m  [20/106], [94mLoss[0m : 1.92525
[1mStep[0m  [30/106], [94mLoss[0m : 1.93518
[1mStep[0m  [40/106], [94mLoss[0m : 1.92863
[1mStep[0m  [50/106], [94mLoss[0m : 1.70065
[1mStep[0m  [60/106], [94mLoss[0m : 1.78675
[1mStep[0m  [70/106], [94mLoss[0m : 1.96364
[1mStep[0m  [80/106], [94mLoss[0m : 1.78484
[1mStep[0m  [90/106], [94mLoss[0m : 1.74694
[1mStep[0m  [100/106], [94mLoss[0m : 1.81008

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74870
[1mStep[0m  [10/106], [94mLoss[0m : 1.72240
[1mStep[0m  [20/106], [94mLoss[0m : 2.03463
[1mStep[0m  [30/106], [94mLoss[0m : 1.81158
[1mStep[0m  [40/106], [94mLoss[0m : 1.89381
[1mStep[0m  [50/106], [94mLoss[0m : 1.80389
[1mStep[0m  [60/106], [94mLoss[0m : 1.84705
[1mStep[0m  [70/106], [94mLoss[0m : 1.65853
[1mStep[0m  [80/106], [94mLoss[0m : 1.99303
[1mStep[0m  [90/106], [94mLoss[0m : 1.64665
[1mStep[0m  [100/106], [94mLoss[0m : 1.88256

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60480
[1mStep[0m  [10/106], [94mLoss[0m : 2.02029
[1mStep[0m  [20/106], [94mLoss[0m : 1.67227
[1mStep[0m  [30/106], [94mLoss[0m : 1.74703
[1mStep[0m  [40/106], [94mLoss[0m : 1.55735
[1mStep[0m  [50/106], [94mLoss[0m : 1.67235
[1mStep[0m  [60/106], [94mLoss[0m : 1.80138
[1mStep[0m  [70/106], [94mLoss[0m : 1.82209
[1mStep[0m  [80/106], [94mLoss[0m : 1.96057
[1mStep[0m  [90/106], [94mLoss[0m : 1.80993
[1mStep[0m  [100/106], [94mLoss[0m : 1.75301

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.88498
[1mStep[0m  [10/106], [94mLoss[0m : 1.99622
[1mStep[0m  [20/106], [94mLoss[0m : 1.83324
[1mStep[0m  [30/106], [94mLoss[0m : 1.74112
[1mStep[0m  [40/106], [94mLoss[0m : 1.90224
[1mStep[0m  [50/106], [94mLoss[0m : 1.70091
[1mStep[0m  [60/106], [94mLoss[0m : 1.80205
[1mStep[0m  [70/106], [94mLoss[0m : 1.76844
[1mStep[0m  [80/106], [94mLoss[0m : 1.73256
[1mStep[0m  [90/106], [94mLoss[0m : 1.77424
[1mStep[0m  [100/106], [94mLoss[0m : 1.58029

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53450
[1mStep[0m  [10/106], [94mLoss[0m : 1.77963
[1mStep[0m  [20/106], [94mLoss[0m : 2.09545
[1mStep[0m  [30/106], [94mLoss[0m : 1.88233
[1mStep[0m  [40/106], [94mLoss[0m : 1.77124
[1mStep[0m  [50/106], [94mLoss[0m : 1.95782
[1mStep[0m  [60/106], [94mLoss[0m : 1.74951
[1mStep[0m  [70/106], [94mLoss[0m : 1.58493
[1mStep[0m  [80/106], [94mLoss[0m : 1.82821
[1mStep[0m  [90/106], [94mLoss[0m : 1.91025
[1mStep[0m  [100/106], [94mLoss[0m : 1.48348

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55519
[1mStep[0m  [10/106], [94mLoss[0m : 2.05158
[1mStep[0m  [20/106], [94mLoss[0m : 1.90367
[1mStep[0m  [30/106], [94mLoss[0m : 1.76576
[1mStep[0m  [40/106], [94mLoss[0m : 1.66056
[1mStep[0m  [50/106], [94mLoss[0m : 1.62984
[1mStep[0m  [60/106], [94mLoss[0m : 1.84481
[1mStep[0m  [70/106], [94mLoss[0m : 1.58626
[1mStep[0m  [80/106], [94mLoss[0m : 1.85619
[1mStep[0m  [90/106], [94mLoss[0m : 1.57138
[1mStep[0m  [100/106], [94mLoss[0m : 1.70901

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71790
[1mStep[0m  [10/106], [94mLoss[0m : 1.62702
[1mStep[0m  [20/106], [94mLoss[0m : 1.56078
[1mStep[0m  [30/106], [94mLoss[0m : 1.64234
[1mStep[0m  [40/106], [94mLoss[0m : 1.91606
[1mStep[0m  [50/106], [94mLoss[0m : 1.82161
[1mStep[0m  [60/106], [94mLoss[0m : 1.68843
[1mStep[0m  [70/106], [94mLoss[0m : 1.88402
[1mStep[0m  [80/106], [94mLoss[0m : 1.66075
[1mStep[0m  [90/106], [94mLoss[0m : 1.72838
[1mStep[0m  [100/106], [94mLoss[0m : 1.51869

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57176
[1mStep[0m  [10/106], [94mLoss[0m : 1.57079
[1mStep[0m  [20/106], [94mLoss[0m : 1.65699
[1mStep[0m  [30/106], [94mLoss[0m : 1.56597
[1mStep[0m  [40/106], [94mLoss[0m : 1.41104
[1mStep[0m  [50/106], [94mLoss[0m : 1.52961
[1mStep[0m  [60/106], [94mLoss[0m : 1.74443
[1mStep[0m  [70/106], [94mLoss[0m : 1.62986
[1mStep[0m  [80/106], [94mLoss[0m : 1.66648
[1mStep[0m  [90/106], [94mLoss[0m : 1.67727
[1mStep[0m  [100/106], [94mLoss[0m : 1.70814

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.423, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54982
[1mStep[0m  [10/106], [94mLoss[0m : 1.72221
[1mStep[0m  [20/106], [94mLoss[0m : 1.50848
[1mStep[0m  [30/106], [94mLoss[0m : 1.60192
[1mStep[0m  [40/106], [94mLoss[0m : 1.49924
[1mStep[0m  [50/106], [94mLoss[0m : 1.50631
[1mStep[0m  [60/106], [94mLoss[0m : 1.59436
[1mStep[0m  [70/106], [94mLoss[0m : 1.63174
[1mStep[0m  [80/106], [94mLoss[0m : 1.68312
[1mStep[0m  [90/106], [94mLoss[0m : 1.83886
[1mStep[0m  [100/106], [94mLoss[0m : 1.66821

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.635, [92mTest[0m: 2.455, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.443
====================================

Phase 2 - Evaluation MAE:  2.4425475462427677
MAE score P1        2.34167
MAE score P2       2.442548
loss               1.634789
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 10.91694
[1mStep[0m  [10/106], [94mLoss[0m : 10.60271
[1mStep[0m  [20/106], [94mLoss[0m : 9.72434
[1mStep[0m  [30/106], [94mLoss[0m : 9.00000
[1mStep[0m  [40/106], [94mLoss[0m : 7.95534
[1mStep[0m  [50/106], [94mLoss[0m : 7.53767
[1mStep[0m  [60/106], [94mLoss[0m : 6.41747
[1mStep[0m  [70/106], [94mLoss[0m : 6.62133
[1mStep[0m  [80/106], [94mLoss[0m : 5.69997
[1mStep[0m  [90/106], [94mLoss[0m : 4.62718
[1mStep[0m  [100/106], [94mLoss[0m : 4.46405

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.339, [92mTest[0m: 11.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.76711
[1mStep[0m  [10/106], [94mLoss[0m : 4.19846
[1mStep[0m  [20/106], [94mLoss[0m : 3.62011
[1mStep[0m  [30/106], [94mLoss[0m : 2.94013
[1mStep[0m  [40/106], [94mLoss[0m : 3.35247
[1mStep[0m  [50/106], [94mLoss[0m : 2.42856
[1mStep[0m  [60/106], [94mLoss[0m : 3.08701
[1mStep[0m  [70/106], [94mLoss[0m : 2.84305
[1mStep[0m  [80/106], [94mLoss[0m : 2.99882
[1mStep[0m  [90/106], [94mLoss[0m : 2.31966
[1mStep[0m  [100/106], [94mLoss[0m : 2.77465

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.028, [92mTest[0m: 3.872, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50163
[1mStep[0m  [10/106], [94mLoss[0m : 2.59823
[1mStep[0m  [20/106], [94mLoss[0m : 2.78863
[1mStep[0m  [30/106], [94mLoss[0m : 2.65077
[1mStep[0m  [40/106], [94mLoss[0m : 3.18761
[1mStep[0m  [50/106], [94mLoss[0m : 2.71538
[1mStep[0m  [60/106], [94mLoss[0m : 2.58213
[1mStep[0m  [70/106], [94mLoss[0m : 2.71980
[1mStep[0m  [80/106], [94mLoss[0m : 2.51902
[1mStep[0m  [90/106], [94mLoss[0m : 2.25011
[1mStep[0m  [100/106], [94mLoss[0m : 2.65890

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.511, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73603
[1mStep[0m  [10/106], [94mLoss[0m : 2.81691
[1mStep[0m  [20/106], [94mLoss[0m : 2.64836
[1mStep[0m  [30/106], [94mLoss[0m : 2.47144
[1mStep[0m  [40/106], [94mLoss[0m : 2.49732
[1mStep[0m  [50/106], [94mLoss[0m : 2.46235
[1mStep[0m  [60/106], [94mLoss[0m : 2.74759
[1mStep[0m  [70/106], [94mLoss[0m : 2.95590
[1mStep[0m  [80/106], [94mLoss[0m : 2.51717
[1mStep[0m  [90/106], [94mLoss[0m : 2.74700
[1mStep[0m  [100/106], [94mLoss[0m : 2.71982

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52304
[1mStep[0m  [10/106], [94mLoss[0m : 2.59851
[1mStep[0m  [20/106], [94mLoss[0m : 2.39551
[1mStep[0m  [30/106], [94mLoss[0m : 2.61138
[1mStep[0m  [40/106], [94mLoss[0m : 2.46248
[1mStep[0m  [50/106], [94mLoss[0m : 2.47431
[1mStep[0m  [60/106], [94mLoss[0m : 2.90526
[1mStep[0m  [70/106], [94mLoss[0m : 2.54772
[1mStep[0m  [80/106], [94mLoss[0m : 2.48764
[1mStep[0m  [90/106], [94mLoss[0m : 2.39052
[1mStep[0m  [100/106], [94mLoss[0m : 2.37213

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.86594
[1mStep[0m  [10/106], [94mLoss[0m : 2.41640
[1mStep[0m  [20/106], [94mLoss[0m : 2.56199
[1mStep[0m  [30/106], [94mLoss[0m : 2.65729
[1mStep[0m  [40/106], [94mLoss[0m : 2.55807
[1mStep[0m  [50/106], [94mLoss[0m : 2.59790
[1mStep[0m  [60/106], [94mLoss[0m : 2.33875
[1mStep[0m  [70/106], [94mLoss[0m : 2.57191
[1mStep[0m  [80/106], [94mLoss[0m : 2.74069
[1mStep[0m  [90/106], [94mLoss[0m : 2.93437
[1mStep[0m  [100/106], [94mLoss[0m : 2.54338

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61471
[1mStep[0m  [10/106], [94mLoss[0m : 2.76045
[1mStep[0m  [20/106], [94mLoss[0m : 2.55348
[1mStep[0m  [30/106], [94mLoss[0m : 2.55006
[1mStep[0m  [40/106], [94mLoss[0m : 2.37226
[1mStep[0m  [50/106], [94mLoss[0m : 2.76430
[1mStep[0m  [60/106], [94mLoss[0m : 2.83754
[1mStep[0m  [70/106], [94mLoss[0m : 2.40873
[1mStep[0m  [80/106], [94mLoss[0m : 2.61197
[1mStep[0m  [90/106], [94mLoss[0m : 2.60444
[1mStep[0m  [100/106], [94mLoss[0m : 2.39903

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53413
[1mStep[0m  [10/106], [94mLoss[0m : 3.10268
[1mStep[0m  [20/106], [94mLoss[0m : 2.54899
[1mStep[0m  [30/106], [94mLoss[0m : 2.62170
[1mStep[0m  [40/106], [94mLoss[0m : 2.61156
[1mStep[0m  [50/106], [94mLoss[0m : 2.47258
[1mStep[0m  [60/106], [94mLoss[0m : 2.33957
[1mStep[0m  [70/106], [94mLoss[0m : 2.44337
[1mStep[0m  [80/106], [94mLoss[0m : 2.29364
[1mStep[0m  [90/106], [94mLoss[0m : 2.69400
[1mStep[0m  [100/106], [94mLoss[0m : 2.62455

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85520
[1mStep[0m  [10/106], [94mLoss[0m : 2.77934
[1mStep[0m  [20/106], [94mLoss[0m : 2.56725
[1mStep[0m  [30/106], [94mLoss[0m : 2.41633
[1mStep[0m  [40/106], [94mLoss[0m : 2.17312
[1mStep[0m  [50/106], [94mLoss[0m : 2.44849
[1mStep[0m  [60/106], [94mLoss[0m : 2.27612
[1mStep[0m  [70/106], [94mLoss[0m : 2.45669
[1mStep[0m  [80/106], [94mLoss[0m : 2.80666
[1mStep[0m  [90/106], [94mLoss[0m : 2.53031
[1mStep[0m  [100/106], [94mLoss[0m : 2.35072

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.442, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28091
[1mStep[0m  [10/106], [94mLoss[0m : 2.50433
[1mStep[0m  [20/106], [94mLoss[0m : 2.42319
[1mStep[0m  [30/106], [94mLoss[0m : 2.55704
[1mStep[0m  [40/106], [94mLoss[0m : 2.53062
[1mStep[0m  [50/106], [94mLoss[0m : 2.52635
[1mStep[0m  [60/106], [94mLoss[0m : 2.45830
[1mStep[0m  [70/106], [94mLoss[0m : 2.61326
[1mStep[0m  [80/106], [94mLoss[0m : 2.54196
[1mStep[0m  [90/106], [94mLoss[0m : 2.44691
[1mStep[0m  [100/106], [94mLoss[0m : 2.41254

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60079
[1mStep[0m  [10/106], [94mLoss[0m : 2.50192
[1mStep[0m  [20/106], [94mLoss[0m : 2.62520
[1mStep[0m  [30/106], [94mLoss[0m : 2.52325
[1mStep[0m  [40/106], [94mLoss[0m : 2.30358
[1mStep[0m  [50/106], [94mLoss[0m : 2.79120
[1mStep[0m  [60/106], [94mLoss[0m : 2.45823
[1mStep[0m  [70/106], [94mLoss[0m : 2.68210
[1mStep[0m  [80/106], [94mLoss[0m : 2.67529
[1mStep[0m  [90/106], [94mLoss[0m : 2.52138
[1mStep[0m  [100/106], [94mLoss[0m : 2.60113

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.441, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74567
[1mStep[0m  [10/106], [94mLoss[0m : 2.47336
[1mStep[0m  [20/106], [94mLoss[0m : 2.50427
[1mStep[0m  [30/106], [94mLoss[0m : 2.53880
[1mStep[0m  [40/106], [94mLoss[0m : 2.41920
[1mStep[0m  [50/106], [94mLoss[0m : 2.73023
[1mStep[0m  [60/106], [94mLoss[0m : 2.42584
[1mStep[0m  [70/106], [94mLoss[0m : 2.35865
[1mStep[0m  [80/106], [94mLoss[0m : 2.43041
[1mStep[0m  [90/106], [94mLoss[0m : 2.39665
[1mStep[0m  [100/106], [94mLoss[0m : 2.25133

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.95771
[1mStep[0m  [10/106], [94mLoss[0m : 2.63999
[1mStep[0m  [20/106], [94mLoss[0m : 2.55338
[1mStep[0m  [30/106], [94mLoss[0m : 2.67502
[1mStep[0m  [40/106], [94mLoss[0m : 2.64416
[1mStep[0m  [50/106], [94mLoss[0m : 2.70308
[1mStep[0m  [60/106], [94mLoss[0m : 2.56297
[1mStep[0m  [70/106], [94mLoss[0m : 2.53819
[1mStep[0m  [80/106], [94mLoss[0m : 2.37816
[1mStep[0m  [90/106], [94mLoss[0m : 2.64192
[1mStep[0m  [100/106], [94mLoss[0m : 2.66560

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13568
[1mStep[0m  [10/106], [94mLoss[0m : 2.60918
[1mStep[0m  [20/106], [94mLoss[0m : 2.53983
[1mStep[0m  [30/106], [94mLoss[0m : 2.39300
[1mStep[0m  [40/106], [94mLoss[0m : 2.53972
[1mStep[0m  [50/106], [94mLoss[0m : 2.68976
[1mStep[0m  [60/106], [94mLoss[0m : 2.69706
[1mStep[0m  [70/106], [94mLoss[0m : 2.66019
[1mStep[0m  [80/106], [94mLoss[0m : 2.86170
[1mStep[0m  [90/106], [94mLoss[0m : 2.21029
[1mStep[0m  [100/106], [94mLoss[0m : 2.43714

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13881
[1mStep[0m  [10/106], [94mLoss[0m : 2.89234
[1mStep[0m  [20/106], [94mLoss[0m : 2.72593
[1mStep[0m  [30/106], [94mLoss[0m : 2.58479
[1mStep[0m  [40/106], [94mLoss[0m : 2.66322
[1mStep[0m  [50/106], [94mLoss[0m : 2.34535
[1mStep[0m  [60/106], [94mLoss[0m : 2.56526
[1mStep[0m  [70/106], [94mLoss[0m : 2.63572
[1mStep[0m  [80/106], [94mLoss[0m : 2.62811
[1mStep[0m  [90/106], [94mLoss[0m : 2.68856
[1mStep[0m  [100/106], [94mLoss[0m : 2.40498

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17196
[1mStep[0m  [10/106], [94mLoss[0m : 2.63139
[1mStep[0m  [20/106], [94mLoss[0m : 2.64149
[1mStep[0m  [30/106], [94mLoss[0m : 2.54713
[1mStep[0m  [40/106], [94mLoss[0m : 2.27607
[1mStep[0m  [50/106], [94mLoss[0m : 2.84361
[1mStep[0m  [60/106], [94mLoss[0m : 2.32786
[1mStep[0m  [70/106], [94mLoss[0m : 2.47370
[1mStep[0m  [80/106], [94mLoss[0m : 2.75508
[1mStep[0m  [90/106], [94mLoss[0m : 2.55143
[1mStep[0m  [100/106], [94mLoss[0m : 2.28425

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66726
[1mStep[0m  [10/106], [94mLoss[0m : 2.75792
[1mStep[0m  [20/106], [94mLoss[0m : 2.33449
[1mStep[0m  [30/106], [94mLoss[0m : 2.60991
[1mStep[0m  [40/106], [94mLoss[0m : 2.64526
[1mStep[0m  [50/106], [94mLoss[0m : 2.42160
[1mStep[0m  [60/106], [94mLoss[0m : 2.54074
[1mStep[0m  [70/106], [94mLoss[0m : 2.64861
[1mStep[0m  [80/106], [94mLoss[0m : 2.48881
[1mStep[0m  [90/106], [94mLoss[0m : 2.43361
[1mStep[0m  [100/106], [94mLoss[0m : 2.66580

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84601
[1mStep[0m  [10/106], [94mLoss[0m : 2.36323
[1mStep[0m  [20/106], [94mLoss[0m : 2.63211
[1mStep[0m  [30/106], [94mLoss[0m : 2.31853
[1mStep[0m  [40/106], [94mLoss[0m : 2.54051
[1mStep[0m  [50/106], [94mLoss[0m : 2.74307
[1mStep[0m  [60/106], [94mLoss[0m : 2.71659
[1mStep[0m  [70/106], [94mLoss[0m : 2.49514
[1mStep[0m  [80/106], [94mLoss[0m : 2.37325
[1mStep[0m  [90/106], [94mLoss[0m : 2.63979
[1mStep[0m  [100/106], [94mLoss[0m : 2.61253

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.98828
[1mStep[0m  [10/106], [94mLoss[0m : 2.45339
[1mStep[0m  [20/106], [94mLoss[0m : 2.67131
[1mStep[0m  [30/106], [94mLoss[0m : 2.64674
[1mStep[0m  [40/106], [94mLoss[0m : 2.50966
[1mStep[0m  [50/106], [94mLoss[0m : 2.56004
[1mStep[0m  [60/106], [94mLoss[0m : 2.53233
[1mStep[0m  [70/106], [94mLoss[0m : 2.87510
[1mStep[0m  [80/106], [94mLoss[0m : 2.31964
[1mStep[0m  [90/106], [94mLoss[0m : 2.60219
[1mStep[0m  [100/106], [94mLoss[0m : 2.39759

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62801
[1mStep[0m  [10/106], [94mLoss[0m : 2.53902
[1mStep[0m  [20/106], [94mLoss[0m : 2.38793
[1mStep[0m  [30/106], [94mLoss[0m : 2.71328
[1mStep[0m  [40/106], [94mLoss[0m : 2.49150
[1mStep[0m  [50/106], [94mLoss[0m : 2.52455
[1mStep[0m  [60/106], [94mLoss[0m : 3.07062
[1mStep[0m  [70/106], [94mLoss[0m : 2.48514
[1mStep[0m  [80/106], [94mLoss[0m : 2.47481
[1mStep[0m  [90/106], [94mLoss[0m : 2.65375
[1mStep[0m  [100/106], [94mLoss[0m : 2.44052

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38434
[1mStep[0m  [10/106], [94mLoss[0m : 2.92676
[1mStep[0m  [20/106], [94mLoss[0m : 2.56524
[1mStep[0m  [30/106], [94mLoss[0m : 2.26692
[1mStep[0m  [40/106], [94mLoss[0m : 2.50187
[1mStep[0m  [50/106], [94mLoss[0m : 2.48624
[1mStep[0m  [60/106], [94mLoss[0m : 2.64302
[1mStep[0m  [70/106], [94mLoss[0m : 2.39810
[1mStep[0m  [80/106], [94mLoss[0m : 2.48989
[1mStep[0m  [90/106], [94mLoss[0m : 2.66391
[1mStep[0m  [100/106], [94mLoss[0m : 2.41706

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37056
[1mStep[0m  [10/106], [94mLoss[0m : 2.38921
[1mStep[0m  [20/106], [94mLoss[0m : 2.38243
[1mStep[0m  [30/106], [94mLoss[0m : 2.59688
[1mStep[0m  [40/106], [94mLoss[0m : 2.24284
[1mStep[0m  [50/106], [94mLoss[0m : 2.47183
[1mStep[0m  [60/106], [94mLoss[0m : 2.67659
[1mStep[0m  [70/106], [94mLoss[0m : 2.28450
[1mStep[0m  [80/106], [94mLoss[0m : 2.26698
[1mStep[0m  [90/106], [94mLoss[0m : 2.44957
[1mStep[0m  [100/106], [94mLoss[0m : 2.59496

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58733
[1mStep[0m  [10/106], [94mLoss[0m : 2.54698
[1mStep[0m  [20/106], [94mLoss[0m : 2.33256
[1mStep[0m  [30/106], [94mLoss[0m : 2.33918
[1mStep[0m  [40/106], [94mLoss[0m : 2.56641
[1mStep[0m  [50/106], [94mLoss[0m : 2.88180
[1mStep[0m  [60/106], [94mLoss[0m : 2.67758
[1mStep[0m  [70/106], [94mLoss[0m : 2.60515
[1mStep[0m  [80/106], [94mLoss[0m : 2.60753
[1mStep[0m  [90/106], [94mLoss[0m : 2.48948
[1mStep[0m  [100/106], [94mLoss[0m : 2.53808

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70721
[1mStep[0m  [10/106], [94mLoss[0m : 2.89266
[1mStep[0m  [20/106], [94mLoss[0m : 2.55554
[1mStep[0m  [30/106], [94mLoss[0m : 2.78279
[1mStep[0m  [40/106], [94mLoss[0m : 2.36750
[1mStep[0m  [50/106], [94mLoss[0m : 2.45977
[1mStep[0m  [60/106], [94mLoss[0m : 2.44083
[1mStep[0m  [70/106], [94mLoss[0m : 2.19701
[1mStep[0m  [80/106], [94mLoss[0m : 2.66107
[1mStep[0m  [90/106], [94mLoss[0m : 2.49340
[1mStep[0m  [100/106], [94mLoss[0m : 2.75433

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55966
[1mStep[0m  [10/106], [94mLoss[0m : 2.47257
[1mStep[0m  [20/106], [94mLoss[0m : 2.76206
[1mStep[0m  [30/106], [94mLoss[0m : 2.39240
[1mStep[0m  [40/106], [94mLoss[0m : 2.51449
[1mStep[0m  [50/106], [94mLoss[0m : 2.26429
[1mStep[0m  [60/106], [94mLoss[0m : 2.67246
[1mStep[0m  [70/106], [94mLoss[0m : 2.37911
[1mStep[0m  [80/106], [94mLoss[0m : 2.47305
[1mStep[0m  [90/106], [94mLoss[0m : 2.24011
[1mStep[0m  [100/106], [94mLoss[0m : 2.68364

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59933
[1mStep[0m  [10/106], [94mLoss[0m : 2.49293
[1mStep[0m  [20/106], [94mLoss[0m : 2.25730
[1mStep[0m  [30/106], [94mLoss[0m : 2.37606
[1mStep[0m  [40/106], [94mLoss[0m : 2.59504
[1mStep[0m  [50/106], [94mLoss[0m : 2.51296
[1mStep[0m  [60/106], [94mLoss[0m : 2.63339
[1mStep[0m  [70/106], [94mLoss[0m : 2.56551
[1mStep[0m  [80/106], [94mLoss[0m : 2.88551
[1mStep[0m  [90/106], [94mLoss[0m : 2.63233
[1mStep[0m  [100/106], [94mLoss[0m : 2.61675

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66604
[1mStep[0m  [10/106], [94mLoss[0m : 2.48595
[1mStep[0m  [20/106], [94mLoss[0m : 2.41692
[1mStep[0m  [30/106], [94mLoss[0m : 2.41413
[1mStep[0m  [40/106], [94mLoss[0m : 2.71718
[1mStep[0m  [50/106], [94mLoss[0m : 2.75742
[1mStep[0m  [60/106], [94mLoss[0m : 2.24012
[1mStep[0m  [70/106], [94mLoss[0m : 2.60668
[1mStep[0m  [80/106], [94mLoss[0m : 2.46337
[1mStep[0m  [90/106], [94mLoss[0m : 2.47988
[1mStep[0m  [100/106], [94mLoss[0m : 2.68947

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72101
[1mStep[0m  [10/106], [94mLoss[0m : 2.56063
[1mStep[0m  [20/106], [94mLoss[0m : 2.34101
[1mStep[0m  [30/106], [94mLoss[0m : 2.37453
[1mStep[0m  [40/106], [94mLoss[0m : 2.65415
[1mStep[0m  [50/106], [94mLoss[0m : 2.41952
[1mStep[0m  [60/106], [94mLoss[0m : 2.58670
[1mStep[0m  [70/106], [94mLoss[0m : 2.70495
[1mStep[0m  [80/106], [94mLoss[0m : 2.33465
[1mStep[0m  [90/106], [94mLoss[0m : 2.92183
[1mStep[0m  [100/106], [94mLoss[0m : 2.49272

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39932
[1mStep[0m  [10/106], [94mLoss[0m : 2.59863
[1mStep[0m  [20/106], [94mLoss[0m : 2.70563
[1mStep[0m  [30/106], [94mLoss[0m : 2.59406
[1mStep[0m  [40/106], [94mLoss[0m : 2.23327
[1mStep[0m  [50/106], [94mLoss[0m : 2.40940
[1mStep[0m  [60/106], [94mLoss[0m : 2.49864
[1mStep[0m  [70/106], [94mLoss[0m : 2.43961
[1mStep[0m  [80/106], [94mLoss[0m : 2.93956
[1mStep[0m  [90/106], [94mLoss[0m : 2.99920
[1mStep[0m  [100/106], [94mLoss[0m : 2.50411

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49893
[1mStep[0m  [10/106], [94mLoss[0m : 2.48977
[1mStep[0m  [20/106], [94mLoss[0m : 2.45854
[1mStep[0m  [30/106], [94mLoss[0m : 2.50893
[1mStep[0m  [40/106], [94mLoss[0m : 2.49467
[1mStep[0m  [50/106], [94mLoss[0m : 2.49366
[1mStep[0m  [60/106], [94mLoss[0m : 2.60262
[1mStep[0m  [70/106], [94mLoss[0m : 2.57841
[1mStep[0m  [80/106], [94mLoss[0m : 2.43500
[1mStep[0m  [90/106], [94mLoss[0m : 2.42567
[1mStep[0m  [100/106], [94mLoss[0m : 2.25263

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.416, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.408
====================================

Phase 1 - Evaluation MAE:  2.4076146539652123
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.64343
[1mStep[0m  [10/106], [94mLoss[0m : 2.50933
[1mStep[0m  [20/106], [94mLoss[0m : 2.42509
[1mStep[0m  [30/106], [94mLoss[0m : 2.72885
[1mStep[0m  [40/106], [94mLoss[0m : 2.62586
[1mStep[0m  [50/106], [94mLoss[0m : 2.32304
[1mStep[0m  [60/106], [94mLoss[0m : 2.33587
[1mStep[0m  [70/106], [94mLoss[0m : 2.78582
[1mStep[0m  [80/106], [94mLoss[0m : 2.28062
[1mStep[0m  [90/106], [94mLoss[0m : 2.54477
[1mStep[0m  [100/106], [94mLoss[0m : 2.72428

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64993
[1mStep[0m  [10/106], [94mLoss[0m : 2.49228
[1mStep[0m  [20/106], [94mLoss[0m : 2.88130
[1mStep[0m  [30/106], [94mLoss[0m : 2.56514
[1mStep[0m  [40/106], [94mLoss[0m : 2.46473
[1mStep[0m  [50/106], [94mLoss[0m : 2.51307
[1mStep[0m  [60/106], [94mLoss[0m : 2.72812
[1mStep[0m  [70/106], [94mLoss[0m : 2.00908
[1mStep[0m  [80/106], [94mLoss[0m : 2.52168
[1mStep[0m  [90/106], [94mLoss[0m : 2.50662
[1mStep[0m  [100/106], [94mLoss[0m : 2.38082

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.436, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42171
[1mStep[0m  [10/106], [94mLoss[0m : 2.79855
[1mStep[0m  [20/106], [94mLoss[0m : 2.75812
[1mStep[0m  [30/106], [94mLoss[0m : 3.02481
[1mStep[0m  [40/106], [94mLoss[0m : 2.55409
[1mStep[0m  [50/106], [94mLoss[0m : 2.71256
[1mStep[0m  [60/106], [94mLoss[0m : 2.37364
[1mStep[0m  [70/106], [94mLoss[0m : 2.29839
[1mStep[0m  [80/106], [94mLoss[0m : 2.56191
[1mStep[0m  [90/106], [94mLoss[0m : 2.62560
[1mStep[0m  [100/106], [94mLoss[0m : 2.39052

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51024
[1mStep[0m  [10/106], [94mLoss[0m : 2.15625
[1mStep[0m  [20/106], [94mLoss[0m : 2.27396
[1mStep[0m  [30/106], [94mLoss[0m : 2.66603
[1mStep[0m  [40/106], [94mLoss[0m : 2.73376
[1mStep[0m  [50/106], [94mLoss[0m : 2.76354
[1mStep[0m  [60/106], [94mLoss[0m : 2.68252
[1mStep[0m  [70/106], [94mLoss[0m : 2.24686
[1mStep[0m  [80/106], [94mLoss[0m : 2.38736
[1mStep[0m  [90/106], [94mLoss[0m : 2.66421
[1mStep[0m  [100/106], [94mLoss[0m : 2.30107

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48044
[1mStep[0m  [10/106], [94mLoss[0m : 2.39919
[1mStep[0m  [20/106], [94mLoss[0m : 2.50798
[1mStep[0m  [30/106], [94mLoss[0m : 2.68627
[1mStep[0m  [40/106], [94mLoss[0m : 2.42518
[1mStep[0m  [50/106], [94mLoss[0m : 2.55556
[1mStep[0m  [60/106], [94mLoss[0m : 2.62946
[1mStep[0m  [70/106], [94mLoss[0m : 2.60921
[1mStep[0m  [80/106], [94mLoss[0m : 2.30195
[1mStep[0m  [90/106], [94mLoss[0m : 2.45591
[1mStep[0m  [100/106], [94mLoss[0m : 2.80545

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.541, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18690
[1mStep[0m  [10/106], [94mLoss[0m : 2.34719
[1mStep[0m  [20/106], [94mLoss[0m : 2.11109
[1mStep[0m  [30/106], [94mLoss[0m : 2.41160
[1mStep[0m  [40/106], [94mLoss[0m : 2.36419
[1mStep[0m  [50/106], [94mLoss[0m : 2.60158
[1mStep[0m  [60/106], [94mLoss[0m : 2.40312
[1mStep[0m  [70/106], [94mLoss[0m : 2.41035
[1mStep[0m  [80/106], [94mLoss[0m : 2.41333
[1mStep[0m  [90/106], [94mLoss[0m : 2.49668
[1mStep[0m  [100/106], [94mLoss[0m : 2.47534

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.564, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59650
[1mStep[0m  [10/106], [94mLoss[0m : 2.18573
[1mStep[0m  [20/106], [94mLoss[0m : 2.21897
[1mStep[0m  [30/106], [94mLoss[0m : 2.43361
[1mStep[0m  [40/106], [94mLoss[0m : 2.52115
[1mStep[0m  [50/106], [94mLoss[0m : 2.16856
[1mStep[0m  [60/106], [94mLoss[0m : 2.37004
[1mStep[0m  [70/106], [94mLoss[0m : 2.24574
[1mStep[0m  [80/106], [94mLoss[0m : 2.59763
[1mStep[0m  [90/106], [94mLoss[0m : 2.43947
[1mStep[0m  [100/106], [94mLoss[0m : 2.55858

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.552, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39523
[1mStep[0m  [10/106], [94mLoss[0m : 2.41122
[1mStep[0m  [20/106], [94mLoss[0m : 2.28344
[1mStep[0m  [30/106], [94mLoss[0m : 2.51352
[1mStep[0m  [40/106], [94mLoss[0m : 2.27295
[1mStep[0m  [50/106], [94mLoss[0m : 1.97757
[1mStep[0m  [60/106], [94mLoss[0m : 2.31503
[1mStep[0m  [70/106], [94mLoss[0m : 2.74241
[1mStep[0m  [80/106], [94mLoss[0m : 2.41986
[1mStep[0m  [90/106], [94mLoss[0m : 2.41713
[1mStep[0m  [100/106], [94mLoss[0m : 2.45188

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.561, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05438
[1mStep[0m  [10/106], [94mLoss[0m : 2.20739
[1mStep[0m  [20/106], [94mLoss[0m : 2.34019
[1mStep[0m  [30/106], [94mLoss[0m : 2.19987
[1mStep[0m  [40/106], [94mLoss[0m : 2.56646
[1mStep[0m  [50/106], [94mLoss[0m : 2.29479
[1mStep[0m  [60/106], [94mLoss[0m : 2.43176
[1mStep[0m  [70/106], [94mLoss[0m : 2.32170
[1mStep[0m  [80/106], [94mLoss[0m : 2.28307
[1mStep[0m  [90/106], [94mLoss[0m : 2.56296
[1mStep[0m  [100/106], [94mLoss[0m : 2.12814

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.550, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19139
[1mStep[0m  [10/106], [94mLoss[0m : 2.31448
[1mStep[0m  [20/106], [94mLoss[0m : 2.37069
[1mStep[0m  [30/106], [94mLoss[0m : 2.11639
[1mStep[0m  [40/106], [94mLoss[0m : 2.15103
[1mStep[0m  [50/106], [94mLoss[0m : 2.47138
[1mStep[0m  [60/106], [94mLoss[0m : 2.34436
[1mStep[0m  [70/106], [94mLoss[0m : 2.47855
[1mStep[0m  [80/106], [94mLoss[0m : 2.04701
[1mStep[0m  [90/106], [94mLoss[0m : 2.61199
[1mStep[0m  [100/106], [94mLoss[0m : 2.33138

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25577
[1mStep[0m  [10/106], [94mLoss[0m : 2.13289
[1mStep[0m  [20/106], [94mLoss[0m : 2.30102
[1mStep[0m  [30/106], [94mLoss[0m : 2.37987
[1mStep[0m  [40/106], [94mLoss[0m : 2.49658
[1mStep[0m  [50/106], [94mLoss[0m : 2.49136
[1mStep[0m  [60/106], [94mLoss[0m : 2.56743
[1mStep[0m  [70/106], [94mLoss[0m : 2.42289
[1mStep[0m  [80/106], [94mLoss[0m : 2.24286
[1mStep[0m  [90/106], [94mLoss[0m : 2.10416
[1mStep[0m  [100/106], [94mLoss[0m : 2.20211

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45042
[1mStep[0m  [10/106], [94mLoss[0m : 2.32774
[1mStep[0m  [20/106], [94mLoss[0m : 2.43741
[1mStep[0m  [30/106], [94mLoss[0m : 2.39011
[1mStep[0m  [40/106], [94mLoss[0m : 2.43418
[1mStep[0m  [50/106], [94mLoss[0m : 2.01425
[1mStep[0m  [60/106], [94mLoss[0m : 2.31544
[1mStep[0m  [70/106], [94mLoss[0m : 2.41155
[1mStep[0m  [80/106], [94mLoss[0m : 2.16516
[1mStep[0m  [90/106], [94mLoss[0m : 2.09330
[1mStep[0m  [100/106], [94mLoss[0m : 2.44114

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.567, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21637
[1mStep[0m  [10/106], [94mLoss[0m : 2.20984
[1mStep[0m  [20/106], [94mLoss[0m : 2.27346
[1mStep[0m  [30/106], [94mLoss[0m : 2.01828
[1mStep[0m  [40/106], [94mLoss[0m : 2.47876
[1mStep[0m  [50/106], [94mLoss[0m : 2.16568
[1mStep[0m  [60/106], [94mLoss[0m : 1.92325
[1mStep[0m  [70/106], [94mLoss[0m : 2.20237
[1mStep[0m  [80/106], [94mLoss[0m : 2.13297
[1mStep[0m  [90/106], [94mLoss[0m : 2.49196
[1mStep[0m  [100/106], [94mLoss[0m : 2.08145

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.492, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75290
[1mStep[0m  [10/106], [94mLoss[0m : 2.17225
[1mStep[0m  [20/106], [94mLoss[0m : 2.17538
[1mStep[0m  [30/106], [94mLoss[0m : 2.30449
[1mStep[0m  [40/106], [94mLoss[0m : 2.10301
[1mStep[0m  [50/106], [94mLoss[0m : 2.13722
[1mStep[0m  [60/106], [94mLoss[0m : 2.16786
[1mStep[0m  [70/106], [94mLoss[0m : 1.93455
[1mStep[0m  [80/106], [94mLoss[0m : 2.28272
[1mStep[0m  [90/106], [94mLoss[0m : 2.12593
[1mStep[0m  [100/106], [94mLoss[0m : 2.14271

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57513
[1mStep[0m  [10/106], [94mLoss[0m : 2.21223
[1mStep[0m  [20/106], [94mLoss[0m : 2.18423
[1mStep[0m  [30/106], [94mLoss[0m : 2.07350
[1mStep[0m  [40/106], [94mLoss[0m : 2.22248
[1mStep[0m  [50/106], [94mLoss[0m : 2.23618
[1mStep[0m  [60/106], [94mLoss[0m : 2.01873
[1mStep[0m  [70/106], [94mLoss[0m : 2.09946
[1mStep[0m  [80/106], [94mLoss[0m : 1.95287
[1mStep[0m  [90/106], [94mLoss[0m : 2.04365
[1mStep[0m  [100/106], [94mLoss[0m : 1.88732

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.533, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06175
[1mStep[0m  [10/106], [94mLoss[0m : 2.02925
[1mStep[0m  [20/106], [94mLoss[0m : 2.31867
[1mStep[0m  [30/106], [94mLoss[0m : 1.93580
[1mStep[0m  [40/106], [94mLoss[0m : 1.77054
[1mStep[0m  [50/106], [94mLoss[0m : 2.20786
[1mStep[0m  [60/106], [94mLoss[0m : 1.98967
[1mStep[0m  [70/106], [94mLoss[0m : 2.23096
[1mStep[0m  [80/106], [94mLoss[0m : 1.77523
[1mStep[0m  [90/106], [94mLoss[0m : 2.22893
[1mStep[0m  [100/106], [94mLoss[0m : 1.94585

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95970
[1mStep[0m  [10/106], [94mLoss[0m : 1.80258
[1mStep[0m  [20/106], [94mLoss[0m : 1.94316
[1mStep[0m  [30/106], [94mLoss[0m : 2.17036
[1mStep[0m  [40/106], [94mLoss[0m : 2.14192
[1mStep[0m  [50/106], [94mLoss[0m : 2.04468
[1mStep[0m  [60/106], [94mLoss[0m : 2.24655
[1mStep[0m  [70/106], [94mLoss[0m : 2.14270
[1mStep[0m  [80/106], [94mLoss[0m : 2.33074
[1mStep[0m  [90/106], [94mLoss[0m : 1.94436
[1mStep[0m  [100/106], [94mLoss[0m : 1.82056

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87428
[1mStep[0m  [10/106], [94mLoss[0m : 1.92109
[1mStep[0m  [20/106], [94mLoss[0m : 2.26614
[1mStep[0m  [30/106], [94mLoss[0m : 2.06497
[1mStep[0m  [40/106], [94mLoss[0m : 2.18713
[1mStep[0m  [50/106], [94mLoss[0m : 1.82621
[1mStep[0m  [60/106], [94mLoss[0m : 2.04622
[1mStep[0m  [70/106], [94mLoss[0m : 2.44609
[1mStep[0m  [80/106], [94mLoss[0m : 1.95058
[1mStep[0m  [90/106], [94mLoss[0m : 2.35746
[1mStep[0m  [100/106], [94mLoss[0m : 1.99025

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.052, [92mTest[0m: 2.495, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98023
[1mStep[0m  [10/106], [94mLoss[0m : 1.87366
[1mStep[0m  [20/106], [94mLoss[0m : 2.05501
[1mStep[0m  [30/106], [94mLoss[0m : 1.92542
[1mStep[0m  [40/106], [94mLoss[0m : 2.11338
[1mStep[0m  [50/106], [94mLoss[0m : 2.07253
[1mStep[0m  [60/106], [94mLoss[0m : 1.81676
[1mStep[0m  [70/106], [94mLoss[0m : 2.13842
[1mStep[0m  [80/106], [94mLoss[0m : 1.94598
[1mStep[0m  [90/106], [94mLoss[0m : 2.07186
[1mStep[0m  [100/106], [94mLoss[0m : 1.61705

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.587, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15035
[1mStep[0m  [10/106], [94mLoss[0m : 1.88984
[1mStep[0m  [20/106], [94mLoss[0m : 2.04924
[1mStep[0m  [30/106], [94mLoss[0m : 2.23167
[1mStep[0m  [40/106], [94mLoss[0m : 2.25032
[1mStep[0m  [50/106], [94mLoss[0m : 1.94256
[1mStep[0m  [60/106], [94mLoss[0m : 2.00619
[1mStep[0m  [70/106], [94mLoss[0m : 1.98304
[1mStep[0m  [80/106], [94mLoss[0m : 1.99314
[1mStep[0m  [90/106], [94mLoss[0m : 2.01852
[1mStep[0m  [100/106], [94mLoss[0m : 1.83049

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.425, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95499
[1mStep[0m  [10/106], [94mLoss[0m : 2.23118
[1mStep[0m  [20/106], [94mLoss[0m : 2.21030
[1mStep[0m  [30/106], [94mLoss[0m : 1.89334
[1mStep[0m  [40/106], [94mLoss[0m : 2.02549
[1mStep[0m  [50/106], [94mLoss[0m : 1.90745
[1mStep[0m  [60/106], [94mLoss[0m : 1.87049
[1mStep[0m  [70/106], [94mLoss[0m : 1.85074
[1mStep[0m  [80/106], [94mLoss[0m : 1.96654
[1mStep[0m  [90/106], [94mLoss[0m : 1.93208
[1mStep[0m  [100/106], [94mLoss[0m : 1.88673

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.68222
[1mStep[0m  [10/106], [94mLoss[0m : 2.06758
[1mStep[0m  [20/106], [94mLoss[0m : 1.85527
[1mStep[0m  [30/106], [94mLoss[0m : 1.85342
[1mStep[0m  [40/106], [94mLoss[0m : 1.96119
[1mStep[0m  [50/106], [94mLoss[0m : 1.95770
[1mStep[0m  [60/106], [94mLoss[0m : 2.05204
[1mStep[0m  [70/106], [94mLoss[0m : 1.78110
[1mStep[0m  [80/106], [94mLoss[0m : 1.84815
[1mStep[0m  [90/106], [94mLoss[0m : 1.86820
[1mStep[0m  [100/106], [94mLoss[0m : 1.92610

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.434, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77310
[1mStep[0m  [10/106], [94mLoss[0m : 1.75474
[1mStep[0m  [20/106], [94mLoss[0m : 2.08076
[1mStep[0m  [30/106], [94mLoss[0m : 1.80919
[1mStep[0m  [40/106], [94mLoss[0m : 1.83050
[1mStep[0m  [50/106], [94mLoss[0m : 1.78327
[1mStep[0m  [60/106], [94mLoss[0m : 1.88370
[1mStep[0m  [70/106], [94mLoss[0m : 1.65650
[1mStep[0m  [80/106], [94mLoss[0m : 2.01201
[1mStep[0m  [90/106], [94mLoss[0m : 1.74894
[1mStep[0m  [100/106], [94mLoss[0m : 2.02944

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.574, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52039
[1mStep[0m  [10/106], [94mLoss[0m : 1.81562
[1mStep[0m  [20/106], [94mLoss[0m : 2.15821
[1mStep[0m  [30/106], [94mLoss[0m : 1.73715
[1mStep[0m  [40/106], [94mLoss[0m : 1.69047
[1mStep[0m  [50/106], [94mLoss[0m : 1.74962
[1mStep[0m  [60/106], [94mLoss[0m : 1.73201
[1mStep[0m  [70/106], [94mLoss[0m : 1.95460
[1mStep[0m  [80/106], [94mLoss[0m : 2.00949
[1mStep[0m  [90/106], [94mLoss[0m : 1.97627
[1mStep[0m  [100/106], [94mLoss[0m : 1.82482

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90666
[1mStep[0m  [10/106], [94mLoss[0m : 1.56734
[1mStep[0m  [20/106], [94mLoss[0m : 1.98606
[1mStep[0m  [30/106], [94mLoss[0m : 2.06761
[1mStep[0m  [40/106], [94mLoss[0m : 2.01799
[1mStep[0m  [50/106], [94mLoss[0m : 1.82675
[1mStep[0m  [60/106], [94mLoss[0m : 1.92724
[1mStep[0m  [70/106], [94mLoss[0m : 1.74296
[1mStep[0m  [80/106], [94mLoss[0m : 1.52511
[1mStep[0m  [90/106], [94mLoss[0m : 2.13728
[1mStep[0m  [100/106], [94mLoss[0m : 2.12757

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78023
[1mStep[0m  [10/106], [94mLoss[0m : 1.69188
[1mStep[0m  [20/106], [94mLoss[0m : 1.59363
[1mStep[0m  [30/106], [94mLoss[0m : 1.91944
[1mStep[0m  [40/106], [94mLoss[0m : 1.84311
[1mStep[0m  [50/106], [94mLoss[0m : 1.64509
[1mStep[0m  [60/106], [94mLoss[0m : 1.75452
[1mStep[0m  [70/106], [94mLoss[0m : 1.71254
[1mStep[0m  [80/106], [94mLoss[0m : 2.02739
[1mStep[0m  [90/106], [94mLoss[0m : 1.57499
[1mStep[0m  [100/106], [94mLoss[0m : 1.82300

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.535, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79613
[1mStep[0m  [10/106], [94mLoss[0m : 1.64506
[1mStep[0m  [20/106], [94mLoss[0m : 2.10429
[1mStep[0m  [30/106], [94mLoss[0m : 1.74196
[1mStep[0m  [40/106], [94mLoss[0m : 1.85070
[1mStep[0m  [50/106], [94mLoss[0m : 1.92444
[1mStep[0m  [60/106], [94mLoss[0m : 1.68624
[1mStep[0m  [70/106], [94mLoss[0m : 1.95315
[1mStep[0m  [80/106], [94mLoss[0m : 1.98462
[1mStep[0m  [90/106], [94mLoss[0m : 2.10193
[1mStep[0m  [100/106], [94mLoss[0m : 1.81699

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64416
[1mStep[0m  [10/106], [94mLoss[0m : 1.67510
[1mStep[0m  [20/106], [94mLoss[0m : 1.64709
[1mStep[0m  [30/106], [94mLoss[0m : 1.82219
[1mStep[0m  [40/106], [94mLoss[0m : 1.61537
[1mStep[0m  [50/106], [94mLoss[0m : 1.62084
[1mStep[0m  [60/106], [94mLoss[0m : 1.81193
[1mStep[0m  [70/106], [94mLoss[0m : 1.52342
[1mStep[0m  [80/106], [94mLoss[0m : 1.80070
[1mStep[0m  [90/106], [94mLoss[0m : 2.16361
[1mStep[0m  [100/106], [94mLoss[0m : 1.58517

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.435, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74456
[1mStep[0m  [10/106], [94mLoss[0m : 1.53942
[1mStep[0m  [20/106], [94mLoss[0m : 1.83265
[1mStep[0m  [30/106], [94mLoss[0m : 1.72510
[1mStep[0m  [40/106], [94mLoss[0m : 1.73603
[1mStep[0m  [50/106], [94mLoss[0m : 2.00120
[1mStep[0m  [60/106], [94mLoss[0m : 1.76459
[1mStep[0m  [70/106], [94mLoss[0m : 1.56822
[1mStep[0m  [80/106], [94mLoss[0m : 1.90880
[1mStep[0m  [90/106], [94mLoss[0m : 1.81860
[1mStep[0m  [100/106], [94mLoss[0m : 1.64232

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.477, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77733
[1mStep[0m  [10/106], [94mLoss[0m : 1.69679
[1mStep[0m  [20/106], [94mLoss[0m : 1.62444
[1mStep[0m  [30/106], [94mLoss[0m : 1.75088
[1mStep[0m  [40/106], [94mLoss[0m : 1.64641
[1mStep[0m  [50/106], [94mLoss[0m : 1.73874
[1mStep[0m  [60/106], [94mLoss[0m : 1.69868
[1mStep[0m  [70/106], [94mLoss[0m : 1.78688
[1mStep[0m  [80/106], [94mLoss[0m : 1.63622
[1mStep[0m  [90/106], [94mLoss[0m : 1.77100
[1mStep[0m  [100/106], [94mLoss[0m : 1.48632

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.634
====================================

Phase 2 - Evaluation MAE:  2.6339414884459296
MAE score P1       2.407615
MAE score P2       2.633941
loss               1.759852
learning_rate       0.00505
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay           0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.22572
[1mStep[0m  [10/106], [94mLoss[0m : 11.02638
[1mStep[0m  [20/106], [94mLoss[0m : 10.14524
[1mStep[0m  [30/106], [94mLoss[0m : 9.61249
[1mStep[0m  [40/106], [94mLoss[0m : 8.32015
[1mStep[0m  [50/106], [94mLoss[0m : 8.22296
[1mStep[0m  [60/106], [94mLoss[0m : 7.08388
[1mStep[0m  [70/106], [94mLoss[0m : 5.79532
[1mStep[0m  [80/106], [94mLoss[0m : 4.66104
[1mStep[0m  [90/106], [94mLoss[0m : 3.68648
[1mStep[0m  [100/106], [94mLoss[0m : 3.07680

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.298, [92mTest[0m: 11.018, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68607
[1mStep[0m  [10/106], [94mLoss[0m : 3.02832
[1mStep[0m  [20/106], [94mLoss[0m : 2.46870
[1mStep[0m  [30/106], [94mLoss[0m : 2.68370
[1mStep[0m  [40/106], [94mLoss[0m : 2.45589
[1mStep[0m  [50/106], [94mLoss[0m : 2.69207
[1mStep[0m  [60/106], [94mLoss[0m : 2.49591
[1mStep[0m  [70/106], [94mLoss[0m : 2.70976
[1mStep[0m  [80/106], [94mLoss[0m : 2.49176
[1mStep[0m  [90/106], [94mLoss[0m : 2.36633
[1mStep[0m  [100/106], [94mLoss[0m : 2.62020

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.682, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40698
[1mStep[0m  [10/106], [94mLoss[0m : 2.57420
[1mStep[0m  [20/106], [94mLoss[0m : 2.86751
[1mStep[0m  [30/106], [94mLoss[0m : 2.42406
[1mStep[0m  [40/106], [94mLoss[0m : 2.42462
[1mStep[0m  [50/106], [94mLoss[0m : 2.39931
[1mStep[0m  [60/106], [94mLoss[0m : 2.58931
[1mStep[0m  [70/106], [94mLoss[0m : 2.79588
[1mStep[0m  [80/106], [94mLoss[0m : 2.46784
[1mStep[0m  [90/106], [94mLoss[0m : 2.56788
[1mStep[0m  [100/106], [94mLoss[0m : 2.25371

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44867
[1mStep[0m  [10/106], [94mLoss[0m : 2.72054
[1mStep[0m  [20/106], [94mLoss[0m : 2.23054
[1mStep[0m  [30/106], [94mLoss[0m : 2.33916
[1mStep[0m  [40/106], [94mLoss[0m : 2.44165
[1mStep[0m  [50/106], [94mLoss[0m : 2.49045
[1mStep[0m  [60/106], [94mLoss[0m : 2.46215
[1mStep[0m  [70/106], [94mLoss[0m : 2.54175
[1mStep[0m  [80/106], [94mLoss[0m : 2.27627
[1mStep[0m  [90/106], [94mLoss[0m : 2.47612
[1mStep[0m  [100/106], [94mLoss[0m : 2.38830

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32507
[1mStep[0m  [10/106], [94mLoss[0m : 2.28243
[1mStep[0m  [20/106], [94mLoss[0m : 2.51154
[1mStep[0m  [30/106], [94mLoss[0m : 2.28056
[1mStep[0m  [40/106], [94mLoss[0m : 2.63460
[1mStep[0m  [50/106], [94mLoss[0m : 2.36202
[1mStep[0m  [60/106], [94mLoss[0m : 2.07780
[1mStep[0m  [70/106], [94mLoss[0m : 2.76012
[1mStep[0m  [80/106], [94mLoss[0m : 2.50560
[1mStep[0m  [90/106], [94mLoss[0m : 2.50278
[1mStep[0m  [100/106], [94mLoss[0m : 2.65118

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78758
[1mStep[0m  [10/106], [94mLoss[0m : 2.63870
[1mStep[0m  [20/106], [94mLoss[0m : 2.35314
[1mStep[0m  [30/106], [94mLoss[0m : 2.45513
[1mStep[0m  [40/106], [94mLoss[0m : 2.38111
[1mStep[0m  [50/106], [94mLoss[0m : 2.54810
[1mStep[0m  [60/106], [94mLoss[0m : 2.44298
[1mStep[0m  [70/106], [94mLoss[0m : 2.36619
[1mStep[0m  [80/106], [94mLoss[0m : 2.46009
[1mStep[0m  [90/106], [94mLoss[0m : 2.28816
[1mStep[0m  [100/106], [94mLoss[0m : 2.32175

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37668
[1mStep[0m  [10/106], [94mLoss[0m : 2.49879
[1mStep[0m  [20/106], [94mLoss[0m : 2.53964
[1mStep[0m  [30/106], [94mLoss[0m : 2.19577
[1mStep[0m  [40/106], [94mLoss[0m : 2.38971
[1mStep[0m  [50/106], [94mLoss[0m : 2.42959
[1mStep[0m  [60/106], [94mLoss[0m : 2.39018
[1mStep[0m  [70/106], [94mLoss[0m : 2.42304
[1mStep[0m  [80/106], [94mLoss[0m : 2.64921
[1mStep[0m  [90/106], [94mLoss[0m : 2.64044
[1mStep[0m  [100/106], [94mLoss[0m : 2.19896

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66807
[1mStep[0m  [10/106], [94mLoss[0m : 2.72551
[1mStep[0m  [20/106], [94mLoss[0m : 2.52925
[1mStep[0m  [30/106], [94mLoss[0m : 2.58983
[1mStep[0m  [40/106], [94mLoss[0m : 2.42529
[1mStep[0m  [50/106], [94mLoss[0m : 2.56569
[1mStep[0m  [60/106], [94mLoss[0m : 2.60667
[1mStep[0m  [70/106], [94mLoss[0m : 2.86098
[1mStep[0m  [80/106], [94mLoss[0m : 2.53963
[1mStep[0m  [90/106], [94mLoss[0m : 2.48596
[1mStep[0m  [100/106], [94mLoss[0m : 2.45246

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75806
[1mStep[0m  [10/106], [94mLoss[0m : 2.40504
[1mStep[0m  [20/106], [94mLoss[0m : 2.34133
[1mStep[0m  [30/106], [94mLoss[0m : 2.33612
[1mStep[0m  [40/106], [94mLoss[0m : 2.52486
[1mStep[0m  [50/106], [94mLoss[0m : 2.69227
[1mStep[0m  [60/106], [94mLoss[0m : 2.43441
[1mStep[0m  [70/106], [94mLoss[0m : 2.52556
[1mStep[0m  [80/106], [94mLoss[0m : 2.64436
[1mStep[0m  [90/106], [94mLoss[0m : 2.51930
[1mStep[0m  [100/106], [94mLoss[0m : 2.37297

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52164
[1mStep[0m  [10/106], [94mLoss[0m : 2.96199
[1mStep[0m  [20/106], [94mLoss[0m : 2.12530
[1mStep[0m  [30/106], [94mLoss[0m : 2.20018
[1mStep[0m  [40/106], [94mLoss[0m : 2.54976
[1mStep[0m  [50/106], [94mLoss[0m : 2.06584
[1mStep[0m  [60/106], [94mLoss[0m : 2.45585
[1mStep[0m  [70/106], [94mLoss[0m : 2.48220
[1mStep[0m  [80/106], [94mLoss[0m : 2.40904
[1mStep[0m  [90/106], [94mLoss[0m : 2.79768
[1mStep[0m  [100/106], [94mLoss[0m : 2.27365

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51965
[1mStep[0m  [10/106], [94mLoss[0m : 2.78737
[1mStep[0m  [20/106], [94mLoss[0m : 2.39734
[1mStep[0m  [30/106], [94mLoss[0m : 2.75544
[1mStep[0m  [40/106], [94mLoss[0m : 2.63819
[1mStep[0m  [50/106], [94mLoss[0m : 2.32156
[1mStep[0m  [60/106], [94mLoss[0m : 2.38804
[1mStep[0m  [70/106], [94mLoss[0m : 2.43857
[1mStep[0m  [80/106], [94mLoss[0m : 2.86651
[1mStep[0m  [90/106], [94mLoss[0m : 2.59825
[1mStep[0m  [100/106], [94mLoss[0m : 2.38159

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.387, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65381
[1mStep[0m  [10/106], [94mLoss[0m : 2.36860
[1mStep[0m  [20/106], [94mLoss[0m : 2.25972
[1mStep[0m  [30/106], [94mLoss[0m : 2.35065
[1mStep[0m  [40/106], [94mLoss[0m : 2.73165
[1mStep[0m  [50/106], [94mLoss[0m : 2.49093
[1mStep[0m  [60/106], [94mLoss[0m : 2.35746
[1mStep[0m  [70/106], [94mLoss[0m : 2.77283
[1mStep[0m  [80/106], [94mLoss[0m : 2.32925
[1mStep[0m  [90/106], [94mLoss[0m : 2.45843
[1mStep[0m  [100/106], [94mLoss[0m : 2.45783

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48783
[1mStep[0m  [10/106], [94mLoss[0m : 2.41012
[1mStep[0m  [20/106], [94mLoss[0m : 2.55172
[1mStep[0m  [30/106], [94mLoss[0m : 2.48460
[1mStep[0m  [40/106], [94mLoss[0m : 2.34288
[1mStep[0m  [50/106], [94mLoss[0m : 2.31966
[1mStep[0m  [60/106], [94mLoss[0m : 2.36924
[1mStep[0m  [70/106], [94mLoss[0m : 2.54798
[1mStep[0m  [80/106], [94mLoss[0m : 2.49981
[1mStep[0m  [90/106], [94mLoss[0m : 2.34602
[1mStep[0m  [100/106], [94mLoss[0m : 2.36750

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18633
[1mStep[0m  [10/106], [94mLoss[0m : 2.60786
[1mStep[0m  [20/106], [94mLoss[0m : 2.26077
[1mStep[0m  [30/106], [94mLoss[0m : 2.34982
[1mStep[0m  [40/106], [94mLoss[0m : 2.32186
[1mStep[0m  [50/106], [94mLoss[0m : 2.33736
[1mStep[0m  [60/106], [94mLoss[0m : 2.32680
[1mStep[0m  [70/106], [94mLoss[0m : 2.38766
[1mStep[0m  [80/106], [94mLoss[0m : 2.27900
[1mStep[0m  [90/106], [94mLoss[0m : 2.36393
[1mStep[0m  [100/106], [94mLoss[0m : 2.58776

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56392
[1mStep[0m  [10/106], [94mLoss[0m : 2.77402
[1mStep[0m  [20/106], [94mLoss[0m : 2.55811
[1mStep[0m  [30/106], [94mLoss[0m : 2.48728
[1mStep[0m  [40/106], [94mLoss[0m : 2.19356
[1mStep[0m  [50/106], [94mLoss[0m : 2.49635
[1mStep[0m  [60/106], [94mLoss[0m : 2.26519
[1mStep[0m  [70/106], [94mLoss[0m : 2.73201
[1mStep[0m  [80/106], [94mLoss[0m : 2.30585
[1mStep[0m  [90/106], [94mLoss[0m : 2.66220
[1mStep[0m  [100/106], [94mLoss[0m : 2.26389

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62695
[1mStep[0m  [10/106], [94mLoss[0m : 2.73189
[1mStep[0m  [20/106], [94mLoss[0m : 2.44509
[1mStep[0m  [30/106], [94mLoss[0m : 2.60431
[1mStep[0m  [40/106], [94mLoss[0m : 2.55791
[1mStep[0m  [50/106], [94mLoss[0m : 2.41376
[1mStep[0m  [60/106], [94mLoss[0m : 2.38464
[1mStep[0m  [70/106], [94mLoss[0m : 2.32899
[1mStep[0m  [80/106], [94mLoss[0m : 2.49368
[1mStep[0m  [90/106], [94mLoss[0m : 2.64276
[1mStep[0m  [100/106], [94mLoss[0m : 2.44068

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62908
[1mStep[0m  [10/106], [94mLoss[0m : 2.42359
[1mStep[0m  [20/106], [94mLoss[0m : 2.35437
[1mStep[0m  [30/106], [94mLoss[0m : 2.71464
[1mStep[0m  [40/106], [94mLoss[0m : 2.55862
[1mStep[0m  [50/106], [94mLoss[0m : 2.30127
[1mStep[0m  [60/106], [94mLoss[0m : 2.48867
[1mStep[0m  [70/106], [94mLoss[0m : 2.46978
[1mStep[0m  [80/106], [94mLoss[0m : 2.26012
[1mStep[0m  [90/106], [94mLoss[0m : 2.77205
[1mStep[0m  [100/106], [94mLoss[0m : 2.35556

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28492
[1mStep[0m  [10/106], [94mLoss[0m : 2.37104
[1mStep[0m  [20/106], [94mLoss[0m : 2.42623
[1mStep[0m  [30/106], [94mLoss[0m : 2.58909
[1mStep[0m  [40/106], [94mLoss[0m : 2.57717
[1mStep[0m  [50/106], [94mLoss[0m : 2.46347
[1mStep[0m  [60/106], [94mLoss[0m : 2.59681
[1mStep[0m  [70/106], [94mLoss[0m : 2.75545
[1mStep[0m  [80/106], [94mLoss[0m : 2.51440
[1mStep[0m  [90/106], [94mLoss[0m : 2.54558
[1mStep[0m  [100/106], [94mLoss[0m : 2.37579

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41430
[1mStep[0m  [10/106], [94mLoss[0m : 2.25260
[1mStep[0m  [20/106], [94mLoss[0m : 2.70923
[1mStep[0m  [30/106], [94mLoss[0m : 2.30537
[1mStep[0m  [40/106], [94mLoss[0m : 2.50106
[1mStep[0m  [50/106], [94mLoss[0m : 2.04944
[1mStep[0m  [60/106], [94mLoss[0m : 2.30253
[1mStep[0m  [70/106], [94mLoss[0m : 2.21348
[1mStep[0m  [80/106], [94mLoss[0m : 2.36420
[1mStep[0m  [90/106], [94mLoss[0m : 2.41965
[1mStep[0m  [100/106], [94mLoss[0m : 2.20164

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30437
[1mStep[0m  [10/106], [94mLoss[0m : 2.37564
[1mStep[0m  [20/106], [94mLoss[0m : 2.21864
[1mStep[0m  [30/106], [94mLoss[0m : 2.54440
[1mStep[0m  [40/106], [94mLoss[0m : 2.47531
[1mStep[0m  [50/106], [94mLoss[0m : 2.69458
[1mStep[0m  [60/106], [94mLoss[0m : 2.74614
[1mStep[0m  [70/106], [94mLoss[0m : 2.44251
[1mStep[0m  [80/106], [94mLoss[0m : 2.33207
[1mStep[0m  [90/106], [94mLoss[0m : 2.68738
[1mStep[0m  [100/106], [94mLoss[0m : 2.63609

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41659
[1mStep[0m  [10/106], [94mLoss[0m : 2.45052
[1mStep[0m  [20/106], [94mLoss[0m : 2.78030
[1mStep[0m  [30/106], [94mLoss[0m : 2.05019
[1mStep[0m  [40/106], [94mLoss[0m : 2.54846
[1mStep[0m  [50/106], [94mLoss[0m : 2.46078
[1mStep[0m  [60/106], [94mLoss[0m : 2.24376
[1mStep[0m  [70/106], [94mLoss[0m : 2.21221
[1mStep[0m  [80/106], [94mLoss[0m : 2.21391
[1mStep[0m  [90/106], [94mLoss[0m : 2.41285
[1mStep[0m  [100/106], [94mLoss[0m : 2.53832

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.390, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53084
[1mStep[0m  [10/106], [94mLoss[0m : 2.26960
[1mStep[0m  [20/106], [94mLoss[0m : 2.51988
[1mStep[0m  [30/106], [94mLoss[0m : 2.57857
[1mStep[0m  [40/106], [94mLoss[0m : 2.67350
[1mStep[0m  [50/106], [94mLoss[0m : 2.47619
[1mStep[0m  [60/106], [94mLoss[0m : 2.48509
[1mStep[0m  [70/106], [94mLoss[0m : 2.68150
[1mStep[0m  [80/106], [94mLoss[0m : 2.43030
[1mStep[0m  [90/106], [94mLoss[0m : 2.27141
[1mStep[0m  [100/106], [94mLoss[0m : 2.59204

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55141
[1mStep[0m  [10/106], [94mLoss[0m : 2.61893
[1mStep[0m  [20/106], [94mLoss[0m : 2.35754
[1mStep[0m  [30/106], [94mLoss[0m : 2.35075
[1mStep[0m  [40/106], [94mLoss[0m : 2.22989
[1mStep[0m  [50/106], [94mLoss[0m : 2.36205
[1mStep[0m  [60/106], [94mLoss[0m : 2.38484
[1mStep[0m  [70/106], [94mLoss[0m : 2.37811
[1mStep[0m  [80/106], [94mLoss[0m : 2.95899
[1mStep[0m  [90/106], [94mLoss[0m : 2.40158
[1mStep[0m  [100/106], [94mLoss[0m : 2.13092

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.393, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26851
[1mStep[0m  [10/106], [94mLoss[0m : 2.15694
[1mStep[0m  [20/106], [94mLoss[0m : 2.38714
[1mStep[0m  [30/106], [94mLoss[0m : 2.44725
[1mStep[0m  [40/106], [94mLoss[0m : 2.17784
[1mStep[0m  [50/106], [94mLoss[0m : 2.16618
[1mStep[0m  [60/106], [94mLoss[0m : 2.29750
[1mStep[0m  [70/106], [94mLoss[0m : 2.35220
[1mStep[0m  [80/106], [94mLoss[0m : 2.36011
[1mStep[0m  [90/106], [94mLoss[0m : 2.46667
[1mStep[0m  [100/106], [94mLoss[0m : 2.44815

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47377
[1mStep[0m  [10/106], [94mLoss[0m : 2.31748
[1mStep[0m  [20/106], [94mLoss[0m : 2.49995
[1mStep[0m  [30/106], [94mLoss[0m : 2.49284
[1mStep[0m  [40/106], [94mLoss[0m : 2.57852
[1mStep[0m  [50/106], [94mLoss[0m : 2.21405
[1mStep[0m  [60/106], [94mLoss[0m : 2.62230
[1mStep[0m  [70/106], [94mLoss[0m : 2.62131
[1mStep[0m  [80/106], [94mLoss[0m : 2.74023
[1mStep[0m  [90/106], [94mLoss[0m : 2.54303
[1mStep[0m  [100/106], [94mLoss[0m : 2.62477

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27734
[1mStep[0m  [10/106], [94mLoss[0m : 2.32638
[1mStep[0m  [20/106], [94mLoss[0m : 2.55579
[1mStep[0m  [30/106], [94mLoss[0m : 2.69905
[1mStep[0m  [40/106], [94mLoss[0m : 2.58479
[1mStep[0m  [50/106], [94mLoss[0m : 2.54642
[1mStep[0m  [60/106], [94mLoss[0m : 2.38874
[1mStep[0m  [70/106], [94mLoss[0m : 2.56015
[1mStep[0m  [80/106], [94mLoss[0m : 2.32257
[1mStep[0m  [90/106], [94mLoss[0m : 2.12601
[1mStep[0m  [100/106], [94mLoss[0m : 2.29521

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44655
[1mStep[0m  [10/106], [94mLoss[0m : 2.25885
[1mStep[0m  [20/106], [94mLoss[0m : 2.50567
[1mStep[0m  [30/106], [94mLoss[0m : 2.04339
[1mStep[0m  [40/106], [94mLoss[0m : 2.52134
[1mStep[0m  [50/106], [94mLoss[0m : 2.20808
[1mStep[0m  [60/106], [94mLoss[0m : 2.66116
[1mStep[0m  [70/106], [94mLoss[0m : 2.43028
[1mStep[0m  [80/106], [94mLoss[0m : 2.41651
[1mStep[0m  [90/106], [94mLoss[0m : 2.46938
[1mStep[0m  [100/106], [94mLoss[0m : 2.49343

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35925
[1mStep[0m  [10/106], [94mLoss[0m : 2.50339
[1mStep[0m  [20/106], [94mLoss[0m : 2.32742
[1mStep[0m  [30/106], [94mLoss[0m : 2.60976
[1mStep[0m  [40/106], [94mLoss[0m : 2.29086
[1mStep[0m  [50/106], [94mLoss[0m : 2.56510
[1mStep[0m  [60/106], [94mLoss[0m : 2.51888
[1mStep[0m  [70/106], [94mLoss[0m : 2.41559
[1mStep[0m  [80/106], [94mLoss[0m : 2.38384
[1mStep[0m  [90/106], [94mLoss[0m : 2.58858
[1mStep[0m  [100/106], [94mLoss[0m : 2.26991

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.402, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42237
[1mStep[0m  [10/106], [94mLoss[0m : 2.47943
[1mStep[0m  [20/106], [94mLoss[0m : 2.48966
[1mStep[0m  [30/106], [94mLoss[0m : 2.26632
[1mStep[0m  [40/106], [94mLoss[0m : 2.20917
[1mStep[0m  [50/106], [94mLoss[0m : 2.34514
[1mStep[0m  [60/106], [94mLoss[0m : 2.66949
[1mStep[0m  [70/106], [94mLoss[0m : 2.25513
[1mStep[0m  [80/106], [94mLoss[0m : 2.88160
[1mStep[0m  [90/106], [94mLoss[0m : 2.48024
[1mStep[0m  [100/106], [94mLoss[0m : 2.27118

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.397, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34092
[1mStep[0m  [10/106], [94mLoss[0m : 2.65980
[1mStep[0m  [20/106], [94mLoss[0m : 2.37504
[1mStep[0m  [30/106], [94mLoss[0m : 2.51542
[1mStep[0m  [40/106], [94mLoss[0m : 2.27832
[1mStep[0m  [50/106], [94mLoss[0m : 2.33794
[1mStep[0m  [60/106], [94mLoss[0m : 2.48290
[1mStep[0m  [70/106], [94mLoss[0m : 2.11116
[1mStep[0m  [80/106], [94mLoss[0m : 2.35949
[1mStep[0m  [90/106], [94mLoss[0m : 2.50841
[1mStep[0m  [100/106], [94mLoss[0m : 2.40375

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.395
====================================

Phase 1 - Evaluation MAE:  2.3945976923096857
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.71358
[1mStep[0m  [10/106], [94mLoss[0m : 2.56942
[1mStep[0m  [20/106], [94mLoss[0m : 2.67048
[1mStep[0m  [30/106], [94mLoss[0m : 2.57307
[1mStep[0m  [40/106], [94mLoss[0m : 2.69646
[1mStep[0m  [50/106], [94mLoss[0m : 2.58974
[1mStep[0m  [60/106], [94mLoss[0m : 2.29039
[1mStep[0m  [70/106], [94mLoss[0m : 2.29893
[1mStep[0m  [80/106], [94mLoss[0m : 2.30300
[1mStep[0m  [90/106], [94mLoss[0m : 2.45310
[1mStep[0m  [100/106], [94mLoss[0m : 2.33384

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45632
[1mStep[0m  [10/106], [94mLoss[0m : 2.40249
[1mStep[0m  [20/106], [94mLoss[0m : 2.29364
[1mStep[0m  [30/106], [94mLoss[0m : 2.39796
[1mStep[0m  [40/106], [94mLoss[0m : 2.29927
[1mStep[0m  [50/106], [94mLoss[0m : 2.31199
[1mStep[0m  [60/106], [94mLoss[0m : 2.10988
[1mStep[0m  [70/106], [94mLoss[0m : 2.54071
[1mStep[0m  [80/106], [94mLoss[0m : 2.37681
[1mStep[0m  [90/106], [94mLoss[0m : 2.26637
[1mStep[0m  [100/106], [94mLoss[0m : 2.64920

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21197
[1mStep[0m  [10/106], [94mLoss[0m : 2.16083
[1mStep[0m  [20/106], [94mLoss[0m : 2.31306
[1mStep[0m  [30/106], [94mLoss[0m : 2.54411
[1mStep[0m  [40/106], [94mLoss[0m : 2.38140
[1mStep[0m  [50/106], [94mLoss[0m : 2.44869
[1mStep[0m  [60/106], [94mLoss[0m : 2.25932
[1mStep[0m  [70/106], [94mLoss[0m : 2.41383
[1mStep[0m  [80/106], [94mLoss[0m : 2.27830
[1mStep[0m  [90/106], [94mLoss[0m : 2.25124
[1mStep[0m  [100/106], [94mLoss[0m : 2.15763

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05602
[1mStep[0m  [10/106], [94mLoss[0m : 2.08056
[1mStep[0m  [20/106], [94mLoss[0m : 1.94719
[1mStep[0m  [30/106], [94mLoss[0m : 2.06509
[1mStep[0m  [40/106], [94mLoss[0m : 2.61123
[1mStep[0m  [50/106], [94mLoss[0m : 2.44217
[1mStep[0m  [60/106], [94mLoss[0m : 2.46627
[1mStep[0m  [70/106], [94mLoss[0m : 2.07888
[1mStep[0m  [80/106], [94mLoss[0m : 2.37775
[1mStep[0m  [90/106], [94mLoss[0m : 2.34737
[1mStep[0m  [100/106], [94mLoss[0m : 2.16743

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13746
[1mStep[0m  [10/106], [94mLoss[0m : 1.77113
[1mStep[0m  [20/106], [94mLoss[0m : 2.13162
[1mStep[0m  [30/106], [94mLoss[0m : 2.05697
[1mStep[0m  [40/106], [94mLoss[0m : 2.31629
[1mStep[0m  [50/106], [94mLoss[0m : 2.41566
[1mStep[0m  [60/106], [94mLoss[0m : 2.13648
[1mStep[0m  [70/106], [94mLoss[0m : 2.23166
[1mStep[0m  [80/106], [94mLoss[0m : 2.19862
[1mStep[0m  [90/106], [94mLoss[0m : 2.31433
[1mStep[0m  [100/106], [94mLoss[0m : 2.24391

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31697
[1mStep[0m  [10/106], [94mLoss[0m : 1.99919
[1mStep[0m  [20/106], [94mLoss[0m : 2.16952
[1mStep[0m  [30/106], [94mLoss[0m : 1.92251
[1mStep[0m  [40/106], [94mLoss[0m : 2.11183
[1mStep[0m  [50/106], [94mLoss[0m : 2.10822
[1mStep[0m  [60/106], [94mLoss[0m : 2.23272
[1mStep[0m  [70/106], [94mLoss[0m : 2.04457
[1mStep[0m  [80/106], [94mLoss[0m : 2.31097
[1mStep[0m  [90/106], [94mLoss[0m : 2.46135
[1mStep[0m  [100/106], [94mLoss[0m : 2.08091

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91545
[1mStep[0m  [10/106], [94mLoss[0m : 1.97953
[1mStep[0m  [20/106], [94mLoss[0m : 2.03833
[1mStep[0m  [30/106], [94mLoss[0m : 2.05747
[1mStep[0m  [40/106], [94mLoss[0m : 2.10506
[1mStep[0m  [50/106], [94mLoss[0m : 2.05797
[1mStep[0m  [60/106], [94mLoss[0m : 2.19585
[1mStep[0m  [70/106], [94mLoss[0m : 1.92372
[1mStep[0m  [80/106], [94mLoss[0m : 2.19096
[1mStep[0m  [90/106], [94mLoss[0m : 1.96965
[1mStep[0m  [100/106], [94mLoss[0m : 2.05299

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87026
[1mStep[0m  [10/106], [94mLoss[0m : 2.27983
[1mStep[0m  [20/106], [94mLoss[0m : 1.78747
[1mStep[0m  [30/106], [94mLoss[0m : 1.95115
[1mStep[0m  [40/106], [94mLoss[0m : 1.91545
[1mStep[0m  [50/106], [94mLoss[0m : 2.38836
[1mStep[0m  [60/106], [94mLoss[0m : 1.87939
[1mStep[0m  [70/106], [94mLoss[0m : 2.06173
[1mStep[0m  [80/106], [94mLoss[0m : 2.06256
[1mStep[0m  [90/106], [94mLoss[0m : 2.10364
[1mStep[0m  [100/106], [94mLoss[0m : 2.23861

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.98581
[1mStep[0m  [10/106], [94mLoss[0m : 2.09980
[1mStep[0m  [20/106], [94mLoss[0m : 1.93700
[1mStep[0m  [30/106], [94mLoss[0m : 1.97445
[1mStep[0m  [40/106], [94mLoss[0m : 1.97635
[1mStep[0m  [50/106], [94mLoss[0m : 1.96954
[1mStep[0m  [60/106], [94mLoss[0m : 2.17620
[1mStep[0m  [70/106], [94mLoss[0m : 1.90940
[1mStep[0m  [80/106], [94mLoss[0m : 2.01673
[1mStep[0m  [90/106], [94mLoss[0m : 2.11785
[1mStep[0m  [100/106], [94mLoss[0m : 2.02871

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47625
[1mStep[0m  [10/106], [94mLoss[0m : 1.76401
[1mStep[0m  [20/106], [94mLoss[0m : 2.07661
[1mStep[0m  [30/106], [94mLoss[0m : 2.01677
[1mStep[0m  [40/106], [94mLoss[0m : 1.96003
[1mStep[0m  [50/106], [94mLoss[0m : 2.08227
[1mStep[0m  [60/106], [94mLoss[0m : 1.82458
[1mStep[0m  [70/106], [94mLoss[0m : 2.03854
[1mStep[0m  [80/106], [94mLoss[0m : 2.18182
[1mStep[0m  [90/106], [94mLoss[0m : 2.32426
[1mStep[0m  [100/106], [94mLoss[0m : 2.09487

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94350
[1mStep[0m  [10/106], [94mLoss[0m : 1.97084
[1mStep[0m  [20/106], [94mLoss[0m : 2.04657
[1mStep[0m  [30/106], [94mLoss[0m : 2.20338
[1mStep[0m  [40/106], [94mLoss[0m : 1.83227
[1mStep[0m  [50/106], [94mLoss[0m : 1.82442
[1mStep[0m  [60/106], [94mLoss[0m : 1.70964
[1mStep[0m  [70/106], [94mLoss[0m : 2.03716
[1mStep[0m  [80/106], [94mLoss[0m : 1.81333
[1mStep[0m  [90/106], [94mLoss[0m : 1.96635
[1mStep[0m  [100/106], [94mLoss[0m : 2.09876

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72758
[1mStep[0m  [10/106], [94mLoss[0m : 1.60305
[1mStep[0m  [20/106], [94mLoss[0m : 2.11312
[1mStep[0m  [30/106], [94mLoss[0m : 1.94429
[1mStep[0m  [40/106], [94mLoss[0m : 2.23190
[1mStep[0m  [50/106], [94mLoss[0m : 1.95343
[1mStep[0m  [60/106], [94mLoss[0m : 1.84984
[1mStep[0m  [70/106], [94mLoss[0m : 2.08906
[1mStep[0m  [80/106], [94mLoss[0m : 2.03154
[1mStep[0m  [90/106], [94mLoss[0m : 2.07247
[1mStep[0m  [100/106], [94mLoss[0m : 2.22680

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70658
[1mStep[0m  [10/106], [94mLoss[0m : 1.93294
[1mStep[0m  [20/106], [94mLoss[0m : 1.86650
[1mStep[0m  [30/106], [94mLoss[0m : 1.79643
[1mStep[0m  [40/106], [94mLoss[0m : 1.99834
[1mStep[0m  [50/106], [94mLoss[0m : 1.97267
[1mStep[0m  [60/106], [94mLoss[0m : 1.98428
[1mStep[0m  [70/106], [94mLoss[0m : 1.96706
[1mStep[0m  [80/106], [94mLoss[0m : 1.97651
[1mStep[0m  [90/106], [94mLoss[0m : 2.12895
[1mStep[0m  [100/106], [94mLoss[0m : 1.93769

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85690
[1mStep[0m  [10/106], [94mLoss[0m : 1.79913
[1mStep[0m  [20/106], [94mLoss[0m : 1.79825
[1mStep[0m  [30/106], [94mLoss[0m : 2.21469
[1mStep[0m  [40/106], [94mLoss[0m : 2.04162
[1mStep[0m  [50/106], [94mLoss[0m : 1.85083
[1mStep[0m  [60/106], [94mLoss[0m : 1.64146
[1mStep[0m  [70/106], [94mLoss[0m : 1.70934
[1mStep[0m  [80/106], [94mLoss[0m : 2.08607
[1mStep[0m  [90/106], [94mLoss[0m : 1.83788
[1mStep[0m  [100/106], [94mLoss[0m : 2.22621

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85194
[1mStep[0m  [10/106], [94mLoss[0m : 1.56624
[1mStep[0m  [20/106], [94mLoss[0m : 1.72964
[1mStep[0m  [30/106], [94mLoss[0m : 2.15751
[1mStep[0m  [40/106], [94mLoss[0m : 1.68543
[1mStep[0m  [50/106], [94mLoss[0m : 2.14871
[1mStep[0m  [60/106], [94mLoss[0m : 2.11188
[1mStep[0m  [70/106], [94mLoss[0m : 2.00504
[1mStep[0m  [80/106], [94mLoss[0m : 1.91001
[1mStep[0m  [90/106], [94mLoss[0m : 2.02609
[1mStep[0m  [100/106], [94mLoss[0m : 1.99572

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00012
[1mStep[0m  [10/106], [94mLoss[0m : 1.82747
[1mStep[0m  [20/106], [94mLoss[0m : 1.89290
[1mStep[0m  [30/106], [94mLoss[0m : 1.77511
[1mStep[0m  [40/106], [94mLoss[0m : 1.71829
[1mStep[0m  [50/106], [94mLoss[0m : 2.23972
[1mStep[0m  [60/106], [94mLoss[0m : 1.93854
[1mStep[0m  [70/106], [94mLoss[0m : 2.03199
[1mStep[0m  [80/106], [94mLoss[0m : 1.78013
[1mStep[0m  [90/106], [94mLoss[0m : 2.08266
[1mStep[0m  [100/106], [94mLoss[0m : 2.08568

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.887, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.51418
[1mStep[0m  [10/106], [94mLoss[0m : 1.84236
[1mStep[0m  [20/106], [94mLoss[0m : 1.64173
[1mStep[0m  [30/106], [94mLoss[0m : 1.74828
[1mStep[0m  [40/106], [94mLoss[0m : 1.91898
[1mStep[0m  [50/106], [94mLoss[0m : 2.07175
[1mStep[0m  [60/106], [94mLoss[0m : 1.95312
[1mStep[0m  [70/106], [94mLoss[0m : 1.70956
[1mStep[0m  [80/106], [94mLoss[0m : 1.66235
[1mStep[0m  [90/106], [94mLoss[0m : 1.88655
[1mStep[0m  [100/106], [94mLoss[0m : 2.16409

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61845
[1mStep[0m  [10/106], [94mLoss[0m : 1.94292
[1mStep[0m  [20/106], [94mLoss[0m : 1.77751
[1mStep[0m  [30/106], [94mLoss[0m : 1.65074
[1mStep[0m  [40/106], [94mLoss[0m : 1.97091
[1mStep[0m  [50/106], [94mLoss[0m : 1.63501
[1mStep[0m  [60/106], [94mLoss[0m : 2.07240
[1mStep[0m  [70/106], [94mLoss[0m : 1.87184
[1mStep[0m  [80/106], [94mLoss[0m : 1.43196
[1mStep[0m  [90/106], [94mLoss[0m : 2.03880
[1mStep[0m  [100/106], [94mLoss[0m : 1.80993

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82748
[1mStep[0m  [10/106], [94mLoss[0m : 1.40077
[1mStep[0m  [20/106], [94mLoss[0m : 1.63917
[1mStep[0m  [30/106], [94mLoss[0m : 1.82996
[1mStep[0m  [40/106], [94mLoss[0m : 1.78407
[1mStep[0m  [50/106], [94mLoss[0m : 1.94929
[1mStep[0m  [60/106], [94mLoss[0m : 1.87606
[1mStep[0m  [70/106], [94mLoss[0m : 1.91798
[1mStep[0m  [80/106], [94mLoss[0m : 1.82182
[1mStep[0m  [90/106], [94mLoss[0m : 1.70303
[1mStep[0m  [100/106], [94mLoss[0m : 1.72591

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.73656
[1mStep[0m  [10/106], [94mLoss[0m : 2.08532
[1mStep[0m  [20/106], [94mLoss[0m : 1.57771
[1mStep[0m  [30/106], [94mLoss[0m : 1.82875
[1mStep[0m  [40/106], [94mLoss[0m : 1.69470
[1mStep[0m  [50/106], [94mLoss[0m : 1.71703
[1mStep[0m  [60/106], [94mLoss[0m : 1.98939
[1mStep[0m  [70/106], [94mLoss[0m : 2.11217
[1mStep[0m  [80/106], [94mLoss[0m : 1.75566
[1mStep[0m  [90/106], [94mLoss[0m : 1.77459
[1mStep[0m  [100/106], [94mLoss[0m : 1.72965

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45616
[1mStep[0m  [10/106], [94mLoss[0m : 1.78711
[1mStep[0m  [20/106], [94mLoss[0m : 1.87673
[1mStep[0m  [30/106], [94mLoss[0m : 1.75463
[1mStep[0m  [40/106], [94mLoss[0m : 1.68680
[1mStep[0m  [50/106], [94mLoss[0m : 1.91233
[1mStep[0m  [60/106], [94mLoss[0m : 1.75044
[1mStep[0m  [70/106], [94mLoss[0m : 1.63090
[1mStep[0m  [80/106], [94mLoss[0m : 1.71418
[1mStep[0m  [90/106], [94mLoss[0m : 1.82380
[1mStep[0m  [100/106], [94mLoss[0m : 1.85547

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89873
[1mStep[0m  [10/106], [94mLoss[0m : 1.73807
[1mStep[0m  [20/106], [94mLoss[0m : 1.73588
[1mStep[0m  [30/106], [94mLoss[0m : 1.70975
[1mStep[0m  [40/106], [94mLoss[0m : 1.54713
[1mStep[0m  [50/106], [94mLoss[0m : 1.79013
[1mStep[0m  [60/106], [94mLoss[0m : 1.79059
[1mStep[0m  [70/106], [94mLoss[0m : 2.15585
[1mStep[0m  [80/106], [94mLoss[0m : 1.98103
[1mStep[0m  [90/106], [94mLoss[0m : 1.91602
[1mStep[0m  [100/106], [94mLoss[0m : 1.91022

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42484
[1mStep[0m  [10/106], [94mLoss[0m : 1.64052
[1mStep[0m  [20/106], [94mLoss[0m : 1.54478
[1mStep[0m  [30/106], [94mLoss[0m : 1.82376
[1mStep[0m  [40/106], [94mLoss[0m : 1.91493
[1mStep[0m  [50/106], [94mLoss[0m : 1.87863
[1mStep[0m  [60/106], [94mLoss[0m : 1.91185
[1mStep[0m  [70/106], [94mLoss[0m : 1.82319
[1mStep[0m  [80/106], [94mLoss[0m : 1.74428
[1mStep[0m  [90/106], [94mLoss[0m : 1.88562
[1mStep[0m  [100/106], [94mLoss[0m : 2.00810

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.58328
[1mStep[0m  [10/106], [94mLoss[0m : 1.57458
[1mStep[0m  [20/106], [94mLoss[0m : 1.60715
[1mStep[0m  [30/106], [94mLoss[0m : 1.34588
[1mStep[0m  [40/106], [94mLoss[0m : 1.67544
[1mStep[0m  [50/106], [94mLoss[0m : 1.63905
[1mStep[0m  [60/106], [94mLoss[0m : 1.78248
[1mStep[0m  [70/106], [94mLoss[0m : 1.57049
[1mStep[0m  [80/106], [94mLoss[0m : 1.83031
[1mStep[0m  [90/106], [94mLoss[0m : 1.82179
[1mStep[0m  [100/106], [94mLoss[0m : 1.67998

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.475, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53153
[1mStep[0m  [10/106], [94mLoss[0m : 1.79887
[1mStep[0m  [20/106], [94mLoss[0m : 1.67424
[1mStep[0m  [30/106], [94mLoss[0m : 1.81153
[1mStep[0m  [40/106], [94mLoss[0m : 1.73857
[1mStep[0m  [50/106], [94mLoss[0m : 2.04120
[1mStep[0m  [60/106], [94mLoss[0m : 1.70582
[1mStep[0m  [70/106], [94mLoss[0m : 1.67545
[1mStep[0m  [80/106], [94mLoss[0m : 2.10177
[1mStep[0m  [90/106], [94mLoss[0m : 1.73672
[1mStep[0m  [100/106], [94mLoss[0m : 1.89107

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.46794
[1mStep[0m  [10/106], [94mLoss[0m : 1.68275
[1mStep[0m  [20/106], [94mLoss[0m : 1.59366
[1mStep[0m  [30/106], [94mLoss[0m : 1.80665
[1mStep[0m  [40/106], [94mLoss[0m : 1.70724
[1mStep[0m  [50/106], [94mLoss[0m : 1.76229
[1mStep[0m  [60/106], [94mLoss[0m : 1.57259
[1mStep[0m  [70/106], [94mLoss[0m : 1.58364
[1mStep[0m  [80/106], [94mLoss[0m : 1.65843
[1mStep[0m  [90/106], [94mLoss[0m : 1.55152
[1mStep[0m  [100/106], [94mLoss[0m : 1.60180

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.712, [92mTest[0m: 2.532, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48403
[1mStep[0m  [10/106], [94mLoss[0m : 1.55402
[1mStep[0m  [20/106], [94mLoss[0m : 1.83312
[1mStep[0m  [30/106], [94mLoss[0m : 1.71616
[1mStep[0m  [40/106], [94mLoss[0m : 1.70018
[1mStep[0m  [50/106], [94mLoss[0m : 1.68171
[1mStep[0m  [60/106], [94mLoss[0m : 2.01515
[1mStep[0m  [70/106], [94mLoss[0m : 1.76425
[1mStep[0m  [80/106], [94mLoss[0m : 1.77806
[1mStep[0m  [90/106], [94mLoss[0m : 1.74731
[1mStep[0m  [100/106], [94mLoss[0m : 1.81448

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.512, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61477
[1mStep[0m  [10/106], [94mLoss[0m : 1.41906
[1mStep[0m  [20/106], [94mLoss[0m : 1.48296
[1mStep[0m  [30/106], [94mLoss[0m : 1.60497
[1mStep[0m  [40/106], [94mLoss[0m : 1.59540
[1mStep[0m  [50/106], [94mLoss[0m : 1.79555
[1mStep[0m  [60/106], [94mLoss[0m : 1.58103
[1mStep[0m  [70/106], [94mLoss[0m : 1.67858
[1mStep[0m  [80/106], [94mLoss[0m : 1.66345
[1mStep[0m  [90/106], [94mLoss[0m : 1.75330
[1mStep[0m  [100/106], [94mLoss[0m : 1.68413

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.541, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56809
[1mStep[0m  [10/106], [94mLoss[0m : 1.78280
[1mStep[0m  [20/106], [94mLoss[0m : 1.60772
[1mStep[0m  [30/106], [94mLoss[0m : 1.89766
[1mStep[0m  [40/106], [94mLoss[0m : 1.78221
[1mStep[0m  [50/106], [94mLoss[0m : 1.64784
[1mStep[0m  [60/106], [94mLoss[0m : 1.78739
[1mStep[0m  [70/106], [94mLoss[0m : 1.55215
[1mStep[0m  [80/106], [94mLoss[0m : 1.76173
[1mStep[0m  [90/106], [94mLoss[0m : 1.87834
[1mStep[0m  [100/106], [94mLoss[0m : 1.70090

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61902
[1mStep[0m  [10/106], [94mLoss[0m : 1.55531
[1mStep[0m  [20/106], [94mLoss[0m : 1.59954
[1mStep[0m  [30/106], [94mLoss[0m : 1.53839
[1mStep[0m  [40/106], [94mLoss[0m : 1.62632
[1mStep[0m  [50/106], [94mLoss[0m : 1.75606
[1mStep[0m  [60/106], [94mLoss[0m : 1.68214
[1mStep[0m  [70/106], [94mLoss[0m : 2.00168
[1mStep[0m  [80/106], [94mLoss[0m : 1.92896
[1mStep[0m  [90/106], [94mLoss[0m : 1.66023
[1mStep[0m  [100/106], [94mLoss[0m : 1.43634

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.5160626825296655
MAE score P1       2.394598
MAE score P2       2.516063
loss               1.656379
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 10.99268
[1mStep[0m  [10/106], [94mLoss[0m : 5.70795
[1mStep[0m  [20/106], [94mLoss[0m : 3.29455
[1mStep[0m  [30/106], [94mLoss[0m : 2.93372
[1mStep[0m  [40/106], [94mLoss[0m : 2.79489
[1mStep[0m  [50/106], [94mLoss[0m : 2.37425
[1mStep[0m  [60/106], [94mLoss[0m : 2.25392
[1mStep[0m  [70/106], [94mLoss[0m : 2.49601
[1mStep[0m  [80/106], [94mLoss[0m : 2.37092
[1mStep[0m  [90/106], [94mLoss[0m : 2.47221
[1mStep[0m  [100/106], [94mLoss[0m : 2.61861

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.316, [92mTest[0m: 10.871, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34128
[1mStep[0m  [10/106], [94mLoss[0m : 2.45680
[1mStep[0m  [20/106], [94mLoss[0m : 2.60132
[1mStep[0m  [30/106], [94mLoss[0m : 2.45794
[1mStep[0m  [40/106], [94mLoss[0m : 2.66820
[1mStep[0m  [50/106], [94mLoss[0m : 2.44303
[1mStep[0m  [60/106], [94mLoss[0m : 2.50782
[1mStep[0m  [70/106], [94mLoss[0m : 2.30141
[1mStep[0m  [80/106], [94mLoss[0m : 2.50785
[1mStep[0m  [90/106], [94mLoss[0m : 2.45373
[1mStep[0m  [100/106], [94mLoss[0m : 2.60439

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38729
[1mStep[0m  [10/106], [94mLoss[0m : 2.21405
[1mStep[0m  [20/106], [94mLoss[0m : 2.55915
[1mStep[0m  [30/106], [94mLoss[0m : 2.43307
[1mStep[0m  [40/106], [94mLoss[0m : 2.63204
[1mStep[0m  [50/106], [94mLoss[0m : 2.27605
[1mStep[0m  [60/106], [94mLoss[0m : 2.37208
[1mStep[0m  [70/106], [94mLoss[0m : 2.65142
[1mStep[0m  [80/106], [94mLoss[0m : 2.44001
[1mStep[0m  [90/106], [94mLoss[0m : 2.23623
[1mStep[0m  [100/106], [94mLoss[0m : 2.40994

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39749
[1mStep[0m  [10/106], [94mLoss[0m : 2.74988
[1mStep[0m  [20/106], [94mLoss[0m : 2.44803
[1mStep[0m  [30/106], [94mLoss[0m : 2.42595
[1mStep[0m  [40/106], [94mLoss[0m : 2.55713
[1mStep[0m  [50/106], [94mLoss[0m : 2.45153
[1mStep[0m  [60/106], [94mLoss[0m : 2.46071
[1mStep[0m  [70/106], [94mLoss[0m : 2.38921
[1mStep[0m  [80/106], [94mLoss[0m : 2.72670
[1mStep[0m  [90/106], [94mLoss[0m : 2.32275
[1mStep[0m  [100/106], [94mLoss[0m : 2.42072

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43838
[1mStep[0m  [10/106], [94mLoss[0m : 2.72423
[1mStep[0m  [20/106], [94mLoss[0m : 2.48111
[1mStep[0m  [30/106], [94mLoss[0m : 2.43432
[1mStep[0m  [40/106], [94mLoss[0m : 2.55157
[1mStep[0m  [50/106], [94mLoss[0m : 2.39424
[1mStep[0m  [60/106], [94mLoss[0m : 2.26569
[1mStep[0m  [70/106], [94mLoss[0m : 2.73969
[1mStep[0m  [80/106], [94mLoss[0m : 2.77867
[1mStep[0m  [90/106], [94mLoss[0m : 2.51899
[1mStep[0m  [100/106], [94mLoss[0m : 2.67368

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20343
[1mStep[0m  [10/106], [94mLoss[0m : 2.38348
[1mStep[0m  [20/106], [94mLoss[0m : 2.69810
[1mStep[0m  [30/106], [94mLoss[0m : 2.50625
[1mStep[0m  [40/106], [94mLoss[0m : 2.53869
[1mStep[0m  [50/106], [94mLoss[0m : 2.63962
[1mStep[0m  [60/106], [94mLoss[0m : 2.20252
[1mStep[0m  [70/106], [94mLoss[0m : 2.43882
[1mStep[0m  [80/106], [94mLoss[0m : 2.46243
[1mStep[0m  [90/106], [94mLoss[0m : 2.24016
[1mStep[0m  [100/106], [94mLoss[0m : 2.60464

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78330
[1mStep[0m  [10/106], [94mLoss[0m : 2.37298
[1mStep[0m  [20/106], [94mLoss[0m : 2.40681
[1mStep[0m  [30/106], [94mLoss[0m : 2.59325
[1mStep[0m  [40/106], [94mLoss[0m : 2.39784
[1mStep[0m  [50/106], [94mLoss[0m : 2.55779
[1mStep[0m  [60/106], [94mLoss[0m : 2.36495
[1mStep[0m  [70/106], [94mLoss[0m : 2.22544
[1mStep[0m  [80/106], [94mLoss[0m : 2.63068
[1mStep[0m  [90/106], [94mLoss[0m : 2.48380
[1mStep[0m  [100/106], [94mLoss[0m : 2.45462

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19395
[1mStep[0m  [10/106], [94mLoss[0m : 2.33722
[1mStep[0m  [20/106], [94mLoss[0m : 2.12399
[1mStep[0m  [30/106], [94mLoss[0m : 2.59034
[1mStep[0m  [40/106], [94mLoss[0m : 2.37785
[1mStep[0m  [50/106], [94mLoss[0m : 2.58494
[1mStep[0m  [60/106], [94mLoss[0m : 2.27922
[1mStep[0m  [70/106], [94mLoss[0m : 2.28490
[1mStep[0m  [80/106], [94mLoss[0m : 2.38105
[1mStep[0m  [90/106], [94mLoss[0m : 2.41598
[1mStep[0m  [100/106], [94mLoss[0m : 2.22396

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34386
[1mStep[0m  [10/106], [94mLoss[0m : 2.61098
[1mStep[0m  [20/106], [94mLoss[0m : 2.45325
[1mStep[0m  [30/106], [94mLoss[0m : 2.62397
[1mStep[0m  [40/106], [94mLoss[0m : 2.39864
[1mStep[0m  [50/106], [94mLoss[0m : 2.12391
[1mStep[0m  [60/106], [94mLoss[0m : 2.35516
[1mStep[0m  [70/106], [94mLoss[0m : 2.42449
[1mStep[0m  [80/106], [94mLoss[0m : 2.44693
[1mStep[0m  [90/106], [94mLoss[0m : 2.82910
[1mStep[0m  [100/106], [94mLoss[0m : 2.53547

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13333
[1mStep[0m  [10/106], [94mLoss[0m : 2.42506
[1mStep[0m  [20/106], [94mLoss[0m : 2.45692
[1mStep[0m  [30/106], [94mLoss[0m : 2.55357
[1mStep[0m  [40/106], [94mLoss[0m : 2.34051
[1mStep[0m  [50/106], [94mLoss[0m : 2.46979
[1mStep[0m  [60/106], [94mLoss[0m : 2.21752
[1mStep[0m  [70/106], [94mLoss[0m : 2.52477
[1mStep[0m  [80/106], [94mLoss[0m : 2.49421
[1mStep[0m  [90/106], [94mLoss[0m : 2.71216
[1mStep[0m  [100/106], [94mLoss[0m : 2.55492

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33271
[1mStep[0m  [10/106], [94mLoss[0m : 2.63576
[1mStep[0m  [20/106], [94mLoss[0m : 2.38162
[1mStep[0m  [30/106], [94mLoss[0m : 2.32233
[1mStep[0m  [40/106], [94mLoss[0m : 2.37737
[1mStep[0m  [50/106], [94mLoss[0m : 2.66615
[1mStep[0m  [60/106], [94mLoss[0m : 2.40424
[1mStep[0m  [70/106], [94mLoss[0m : 2.44396
[1mStep[0m  [80/106], [94mLoss[0m : 2.20885
[1mStep[0m  [90/106], [94mLoss[0m : 2.61065
[1mStep[0m  [100/106], [94mLoss[0m : 2.34878

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63502
[1mStep[0m  [10/106], [94mLoss[0m : 2.37803
[1mStep[0m  [20/106], [94mLoss[0m : 2.06747
[1mStep[0m  [30/106], [94mLoss[0m : 2.41906
[1mStep[0m  [40/106], [94mLoss[0m : 2.40424
[1mStep[0m  [50/106], [94mLoss[0m : 2.81921
[1mStep[0m  [60/106], [94mLoss[0m : 2.55937
[1mStep[0m  [70/106], [94mLoss[0m : 2.36867
[1mStep[0m  [80/106], [94mLoss[0m : 2.61535
[1mStep[0m  [90/106], [94mLoss[0m : 2.69522
[1mStep[0m  [100/106], [94mLoss[0m : 2.77700

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76934
[1mStep[0m  [10/106], [94mLoss[0m : 2.38479
[1mStep[0m  [20/106], [94mLoss[0m : 2.22902
[1mStep[0m  [30/106], [94mLoss[0m : 2.41567
[1mStep[0m  [40/106], [94mLoss[0m : 2.62951
[1mStep[0m  [50/106], [94mLoss[0m : 2.23840
[1mStep[0m  [60/106], [94mLoss[0m : 2.42158
[1mStep[0m  [70/106], [94mLoss[0m : 2.91498
[1mStep[0m  [80/106], [94mLoss[0m : 2.13508
[1mStep[0m  [90/106], [94mLoss[0m : 2.49599
[1mStep[0m  [100/106], [94mLoss[0m : 2.13913

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.81527
[1mStep[0m  [10/106], [94mLoss[0m : 2.67595
[1mStep[0m  [20/106], [94mLoss[0m : 2.49660
[1mStep[0m  [30/106], [94mLoss[0m : 2.52371
[1mStep[0m  [40/106], [94mLoss[0m : 2.34462
[1mStep[0m  [50/106], [94mLoss[0m : 2.32800
[1mStep[0m  [60/106], [94mLoss[0m : 2.39781
[1mStep[0m  [70/106], [94mLoss[0m : 2.42859
[1mStep[0m  [80/106], [94mLoss[0m : 2.67658
[1mStep[0m  [90/106], [94mLoss[0m : 2.17471
[1mStep[0m  [100/106], [94mLoss[0m : 2.50440

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63447
[1mStep[0m  [10/106], [94mLoss[0m : 2.05120
[1mStep[0m  [20/106], [94mLoss[0m : 2.49115
[1mStep[0m  [30/106], [94mLoss[0m : 2.68466
[1mStep[0m  [40/106], [94mLoss[0m : 2.55260
[1mStep[0m  [50/106], [94mLoss[0m : 2.22108
[1mStep[0m  [60/106], [94mLoss[0m : 2.49140
[1mStep[0m  [70/106], [94mLoss[0m : 2.47565
[1mStep[0m  [80/106], [94mLoss[0m : 2.43216
[1mStep[0m  [90/106], [94mLoss[0m : 2.32578
[1mStep[0m  [100/106], [94mLoss[0m : 2.72419

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59496
[1mStep[0m  [10/106], [94mLoss[0m : 2.25729
[1mStep[0m  [20/106], [94mLoss[0m : 2.59325
[1mStep[0m  [30/106], [94mLoss[0m : 2.30273
[1mStep[0m  [40/106], [94mLoss[0m : 2.34816
[1mStep[0m  [50/106], [94mLoss[0m : 2.24052
[1mStep[0m  [60/106], [94mLoss[0m : 2.03075
[1mStep[0m  [70/106], [94mLoss[0m : 2.31354
[1mStep[0m  [80/106], [94mLoss[0m : 2.46034
[1mStep[0m  [90/106], [94mLoss[0m : 2.52776
[1mStep[0m  [100/106], [94mLoss[0m : 2.64231

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36279
[1mStep[0m  [10/106], [94mLoss[0m : 2.36413
[1mStep[0m  [20/106], [94mLoss[0m : 2.46489
[1mStep[0m  [30/106], [94mLoss[0m : 2.23316
[1mStep[0m  [40/106], [94mLoss[0m : 2.42449
[1mStep[0m  [50/106], [94mLoss[0m : 2.17569
[1mStep[0m  [60/106], [94mLoss[0m : 2.34871
[1mStep[0m  [70/106], [94mLoss[0m : 2.30862
[1mStep[0m  [80/106], [94mLoss[0m : 2.42669
[1mStep[0m  [90/106], [94mLoss[0m : 2.44023
[1mStep[0m  [100/106], [94mLoss[0m : 2.36127

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52866
[1mStep[0m  [10/106], [94mLoss[0m : 2.49082
[1mStep[0m  [20/106], [94mLoss[0m : 2.26314
[1mStep[0m  [30/106], [94mLoss[0m : 2.61674
[1mStep[0m  [40/106], [94mLoss[0m : 2.64656
[1mStep[0m  [50/106], [94mLoss[0m : 2.65354
[1mStep[0m  [60/106], [94mLoss[0m : 2.63329
[1mStep[0m  [70/106], [94mLoss[0m : 2.30839
[1mStep[0m  [80/106], [94mLoss[0m : 2.30188
[1mStep[0m  [90/106], [94mLoss[0m : 2.43611
[1mStep[0m  [100/106], [94mLoss[0m : 2.39622

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49307
[1mStep[0m  [10/106], [94mLoss[0m : 2.69094
[1mStep[0m  [20/106], [94mLoss[0m : 2.42659
[1mStep[0m  [30/106], [94mLoss[0m : 2.58458
[1mStep[0m  [40/106], [94mLoss[0m : 2.35928
[1mStep[0m  [50/106], [94mLoss[0m : 2.33416
[1mStep[0m  [60/106], [94mLoss[0m : 2.51362
[1mStep[0m  [70/106], [94mLoss[0m : 2.26326
[1mStep[0m  [80/106], [94mLoss[0m : 2.34692
[1mStep[0m  [90/106], [94mLoss[0m : 2.40065
[1mStep[0m  [100/106], [94mLoss[0m : 2.19811

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51744
[1mStep[0m  [10/106], [94mLoss[0m : 2.52161
[1mStep[0m  [20/106], [94mLoss[0m : 2.19452
[1mStep[0m  [30/106], [94mLoss[0m : 2.44869
[1mStep[0m  [40/106], [94mLoss[0m : 2.45268
[1mStep[0m  [50/106], [94mLoss[0m : 2.44064
[1mStep[0m  [60/106], [94mLoss[0m : 2.29647
[1mStep[0m  [70/106], [94mLoss[0m : 2.94525
[1mStep[0m  [80/106], [94mLoss[0m : 2.39007
[1mStep[0m  [90/106], [94mLoss[0m : 2.13862
[1mStep[0m  [100/106], [94mLoss[0m : 2.49695

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.396, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29252
[1mStep[0m  [10/106], [94mLoss[0m : 2.61990
[1mStep[0m  [20/106], [94mLoss[0m : 2.59537
[1mStep[0m  [30/106], [94mLoss[0m : 2.36302
[1mStep[0m  [40/106], [94mLoss[0m : 2.19084
[1mStep[0m  [50/106], [94mLoss[0m : 2.28483
[1mStep[0m  [60/106], [94mLoss[0m : 2.70401
[1mStep[0m  [70/106], [94mLoss[0m : 2.24728
[1mStep[0m  [80/106], [94mLoss[0m : 2.54492
[1mStep[0m  [90/106], [94mLoss[0m : 2.35377
[1mStep[0m  [100/106], [94mLoss[0m : 2.40004

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41680
[1mStep[0m  [10/106], [94mLoss[0m : 2.40090
[1mStep[0m  [20/106], [94mLoss[0m : 2.35860
[1mStep[0m  [30/106], [94mLoss[0m : 2.35347
[1mStep[0m  [40/106], [94mLoss[0m : 2.69601
[1mStep[0m  [50/106], [94mLoss[0m : 2.62558
[1mStep[0m  [60/106], [94mLoss[0m : 2.55006
[1mStep[0m  [70/106], [94mLoss[0m : 2.66340
[1mStep[0m  [80/106], [94mLoss[0m : 2.44448
[1mStep[0m  [90/106], [94mLoss[0m : 2.45575
[1mStep[0m  [100/106], [94mLoss[0m : 2.31454

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25653
[1mStep[0m  [10/106], [94mLoss[0m : 2.49501
[1mStep[0m  [20/106], [94mLoss[0m : 2.66401
[1mStep[0m  [30/106], [94mLoss[0m : 2.83647
[1mStep[0m  [40/106], [94mLoss[0m : 2.35779
[1mStep[0m  [50/106], [94mLoss[0m : 2.41327
[1mStep[0m  [60/106], [94mLoss[0m : 2.43426
[1mStep[0m  [70/106], [94mLoss[0m : 2.40789
[1mStep[0m  [80/106], [94mLoss[0m : 2.39943
[1mStep[0m  [90/106], [94mLoss[0m : 2.36267
[1mStep[0m  [100/106], [94mLoss[0m : 2.46366

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18609
[1mStep[0m  [10/106], [94mLoss[0m : 2.40822
[1mStep[0m  [20/106], [94mLoss[0m : 2.35552
[1mStep[0m  [30/106], [94mLoss[0m : 2.40876
[1mStep[0m  [40/106], [94mLoss[0m : 2.38266
[1mStep[0m  [50/106], [94mLoss[0m : 2.35410
[1mStep[0m  [60/106], [94mLoss[0m : 2.41291
[1mStep[0m  [70/106], [94mLoss[0m : 2.18136
[1mStep[0m  [80/106], [94mLoss[0m : 2.40951
[1mStep[0m  [90/106], [94mLoss[0m : 2.27711
[1mStep[0m  [100/106], [94mLoss[0m : 2.44798

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.387, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33433
[1mStep[0m  [10/106], [94mLoss[0m : 2.21942
[1mStep[0m  [20/106], [94mLoss[0m : 2.59934
[1mStep[0m  [30/106], [94mLoss[0m : 2.40694
[1mStep[0m  [40/106], [94mLoss[0m : 2.41693
[1mStep[0m  [50/106], [94mLoss[0m : 2.30279
[1mStep[0m  [60/106], [94mLoss[0m : 2.74864
[1mStep[0m  [70/106], [94mLoss[0m : 2.39623
[1mStep[0m  [80/106], [94mLoss[0m : 2.50428
[1mStep[0m  [90/106], [94mLoss[0m : 2.60041
[1mStep[0m  [100/106], [94mLoss[0m : 2.46916

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54216
[1mStep[0m  [10/106], [94mLoss[0m : 2.25758
[1mStep[0m  [20/106], [94mLoss[0m : 2.45515
[1mStep[0m  [30/106], [94mLoss[0m : 2.51578
[1mStep[0m  [40/106], [94mLoss[0m : 2.50722
[1mStep[0m  [50/106], [94mLoss[0m : 2.22149
[1mStep[0m  [60/106], [94mLoss[0m : 2.35875
[1mStep[0m  [70/106], [94mLoss[0m : 2.46401
[1mStep[0m  [80/106], [94mLoss[0m : 2.49716
[1mStep[0m  [90/106], [94mLoss[0m : 2.71975
[1mStep[0m  [100/106], [94mLoss[0m : 2.62005

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.392, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57456
[1mStep[0m  [10/106], [94mLoss[0m : 2.47016
[1mStep[0m  [20/106], [94mLoss[0m : 2.04883
[1mStep[0m  [30/106], [94mLoss[0m : 2.17019
[1mStep[0m  [40/106], [94mLoss[0m : 2.39971
[1mStep[0m  [50/106], [94mLoss[0m : 2.23299
[1mStep[0m  [60/106], [94mLoss[0m : 2.33944
[1mStep[0m  [70/106], [94mLoss[0m : 2.71585
[1mStep[0m  [80/106], [94mLoss[0m : 2.32765
[1mStep[0m  [90/106], [94mLoss[0m : 2.20221
[1mStep[0m  [100/106], [94mLoss[0m : 2.44283

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.388, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37649
[1mStep[0m  [10/106], [94mLoss[0m : 2.61404
[1mStep[0m  [20/106], [94mLoss[0m : 2.38058
[1mStep[0m  [30/106], [94mLoss[0m : 2.82879
[1mStep[0m  [40/106], [94mLoss[0m : 2.28156
[1mStep[0m  [50/106], [94mLoss[0m : 2.32409
[1mStep[0m  [60/106], [94mLoss[0m : 2.25454
[1mStep[0m  [70/106], [94mLoss[0m : 2.45022
[1mStep[0m  [80/106], [94mLoss[0m : 2.13803
[1mStep[0m  [90/106], [94mLoss[0m : 2.35936
[1mStep[0m  [100/106], [94mLoss[0m : 2.59689

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42825
[1mStep[0m  [10/106], [94mLoss[0m : 2.52263
[1mStep[0m  [20/106], [94mLoss[0m : 2.53640
[1mStep[0m  [30/106], [94mLoss[0m : 1.99445
[1mStep[0m  [40/106], [94mLoss[0m : 2.28650
[1mStep[0m  [50/106], [94mLoss[0m : 2.37322
[1mStep[0m  [60/106], [94mLoss[0m : 2.25649
[1mStep[0m  [70/106], [94mLoss[0m : 2.46121
[1mStep[0m  [80/106], [94mLoss[0m : 2.52201
[1mStep[0m  [90/106], [94mLoss[0m : 2.50744
[1mStep[0m  [100/106], [94mLoss[0m : 2.32563

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43631
[1mStep[0m  [10/106], [94mLoss[0m : 2.37509
[1mStep[0m  [20/106], [94mLoss[0m : 2.56253
[1mStep[0m  [30/106], [94mLoss[0m : 2.42774
[1mStep[0m  [40/106], [94mLoss[0m : 2.37947
[1mStep[0m  [50/106], [94mLoss[0m : 2.44723
[1mStep[0m  [60/106], [94mLoss[0m : 2.35367
[1mStep[0m  [70/106], [94mLoss[0m : 2.53454
[1mStep[0m  [80/106], [94mLoss[0m : 2.04681
[1mStep[0m  [90/106], [94mLoss[0m : 2.81139
[1mStep[0m  [100/106], [94mLoss[0m : 2.03850

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.399, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.392
====================================

Phase 1 - Evaluation MAE:  2.3924725280617767
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.28042
[1mStep[0m  [10/106], [94mLoss[0m : 2.18655
[1mStep[0m  [20/106], [94mLoss[0m : 2.45950
[1mStep[0m  [30/106], [94mLoss[0m : 2.38923
[1mStep[0m  [40/106], [94mLoss[0m : 2.28467
[1mStep[0m  [50/106], [94mLoss[0m : 2.34413
[1mStep[0m  [60/106], [94mLoss[0m : 2.48609
[1mStep[0m  [70/106], [94mLoss[0m : 2.44188
[1mStep[0m  [80/106], [94mLoss[0m : 2.62296
[1mStep[0m  [90/106], [94mLoss[0m : 2.35198
[1mStep[0m  [100/106], [94mLoss[0m : 2.39628

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49874
[1mStep[0m  [10/106], [94mLoss[0m : 2.54180
[1mStep[0m  [20/106], [94mLoss[0m : 2.23975
[1mStep[0m  [30/106], [94mLoss[0m : 2.32372
[1mStep[0m  [40/106], [94mLoss[0m : 2.52060
[1mStep[0m  [50/106], [94mLoss[0m : 2.16988
[1mStep[0m  [60/106], [94mLoss[0m : 2.21612
[1mStep[0m  [70/106], [94mLoss[0m : 1.93925
[1mStep[0m  [80/106], [94mLoss[0m : 2.26893
[1mStep[0m  [90/106], [94mLoss[0m : 2.34540
[1mStep[0m  [100/106], [94mLoss[0m : 2.32265

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.514, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38213
[1mStep[0m  [10/106], [94mLoss[0m : 2.00511
[1mStep[0m  [20/106], [94mLoss[0m : 2.44575
[1mStep[0m  [30/106], [94mLoss[0m : 2.38852
[1mStep[0m  [40/106], [94mLoss[0m : 2.28710
[1mStep[0m  [50/106], [94mLoss[0m : 2.31636
[1mStep[0m  [60/106], [94mLoss[0m : 2.32223
[1mStep[0m  [70/106], [94mLoss[0m : 2.29065
[1mStep[0m  [80/106], [94mLoss[0m : 2.41466
[1mStep[0m  [90/106], [94mLoss[0m : 2.43224
[1mStep[0m  [100/106], [94mLoss[0m : 2.08235

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33238
[1mStep[0m  [10/106], [94mLoss[0m : 2.31539
[1mStep[0m  [20/106], [94mLoss[0m : 2.06400
[1mStep[0m  [30/106], [94mLoss[0m : 2.01717
[1mStep[0m  [40/106], [94mLoss[0m : 2.19169
[1mStep[0m  [50/106], [94mLoss[0m : 2.15642
[1mStep[0m  [60/106], [94mLoss[0m : 2.41168
[1mStep[0m  [70/106], [94mLoss[0m : 2.06310
[1mStep[0m  [80/106], [94mLoss[0m : 2.21540
[1mStep[0m  [90/106], [94mLoss[0m : 2.13491
[1mStep[0m  [100/106], [94mLoss[0m : 2.23900

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06417
[1mStep[0m  [10/106], [94mLoss[0m : 1.95109
[1mStep[0m  [20/106], [94mLoss[0m : 2.06891
[1mStep[0m  [30/106], [94mLoss[0m : 1.99065
[1mStep[0m  [40/106], [94mLoss[0m : 2.14324
[1mStep[0m  [50/106], [94mLoss[0m : 1.85380
[1mStep[0m  [60/106], [94mLoss[0m : 2.23631
[1mStep[0m  [70/106], [94mLoss[0m : 1.87369
[1mStep[0m  [80/106], [94mLoss[0m : 2.24753
[1mStep[0m  [90/106], [94mLoss[0m : 2.18429
[1mStep[0m  [100/106], [94mLoss[0m : 2.22451

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.93368
[1mStep[0m  [10/106], [94mLoss[0m : 2.28285
[1mStep[0m  [20/106], [94mLoss[0m : 1.96362
[1mStep[0m  [30/106], [94mLoss[0m : 2.06218
[1mStep[0m  [40/106], [94mLoss[0m : 2.04015
[1mStep[0m  [50/106], [94mLoss[0m : 2.12187
[1mStep[0m  [60/106], [94mLoss[0m : 2.24584
[1mStep[0m  [70/106], [94mLoss[0m : 2.10371
[1mStep[0m  [80/106], [94mLoss[0m : 1.80416
[1mStep[0m  [90/106], [94mLoss[0m : 2.12111
[1mStep[0m  [100/106], [94mLoss[0m : 2.26027

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62665
[1mStep[0m  [10/106], [94mLoss[0m : 1.85020
[1mStep[0m  [20/106], [94mLoss[0m : 1.85281
[1mStep[0m  [30/106], [94mLoss[0m : 1.78428
[1mStep[0m  [40/106], [94mLoss[0m : 1.84077
[1mStep[0m  [50/106], [94mLoss[0m : 1.82493
[1mStep[0m  [60/106], [94mLoss[0m : 2.01091
[1mStep[0m  [70/106], [94mLoss[0m : 2.33013
[1mStep[0m  [80/106], [94mLoss[0m : 2.25707
[1mStep[0m  [90/106], [94mLoss[0m : 2.06329
[1mStep[0m  [100/106], [94mLoss[0m : 1.97813

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83701
[1mStep[0m  [10/106], [94mLoss[0m : 1.92785
[1mStep[0m  [20/106], [94mLoss[0m : 1.91453
[1mStep[0m  [30/106], [94mLoss[0m : 1.86685
[1mStep[0m  [40/106], [94mLoss[0m : 1.79636
[1mStep[0m  [50/106], [94mLoss[0m : 1.86784
[1mStep[0m  [60/106], [94mLoss[0m : 1.88011
[1mStep[0m  [70/106], [94mLoss[0m : 2.35254
[1mStep[0m  [80/106], [94mLoss[0m : 1.94667
[1mStep[0m  [90/106], [94mLoss[0m : 2.15044
[1mStep[0m  [100/106], [94mLoss[0m : 1.94056

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.437, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92347
[1mStep[0m  [10/106], [94mLoss[0m : 1.78060
[1mStep[0m  [20/106], [94mLoss[0m : 1.95423
[1mStep[0m  [30/106], [94mLoss[0m : 2.17571
[1mStep[0m  [40/106], [94mLoss[0m : 1.70350
[1mStep[0m  [50/106], [94mLoss[0m : 1.96874
[1mStep[0m  [60/106], [94mLoss[0m : 1.94654
[1mStep[0m  [70/106], [94mLoss[0m : 2.15655
[1mStep[0m  [80/106], [94mLoss[0m : 1.94152
[1mStep[0m  [90/106], [94mLoss[0m : 1.74540
[1mStep[0m  [100/106], [94mLoss[0m : 2.05621

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89347
[1mStep[0m  [10/106], [94mLoss[0m : 1.63728
[1mStep[0m  [20/106], [94mLoss[0m : 1.74184
[1mStep[0m  [30/106], [94mLoss[0m : 1.85118
[1mStep[0m  [40/106], [94mLoss[0m : 2.01660
[1mStep[0m  [50/106], [94mLoss[0m : 2.18329
[1mStep[0m  [60/106], [94mLoss[0m : 1.80857
[1mStep[0m  [70/106], [94mLoss[0m : 1.96859
[1mStep[0m  [80/106], [94mLoss[0m : 1.86661
[1mStep[0m  [90/106], [94mLoss[0m : 1.75315
[1mStep[0m  [100/106], [94mLoss[0m : 1.85361

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78321
[1mStep[0m  [10/106], [94mLoss[0m : 1.43814
[1mStep[0m  [20/106], [94mLoss[0m : 1.79942
[1mStep[0m  [30/106], [94mLoss[0m : 1.72150
[1mStep[0m  [40/106], [94mLoss[0m : 1.80681
[1mStep[0m  [50/106], [94mLoss[0m : 1.76336
[1mStep[0m  [60/106], [94mLoss[0m : 1.89419
[1mStep[0m  [70/106], [94mLoss[0m : 2.17169
[1mStep[0m  [80/106], [94mLoss[0m : 1.79566
[1mStep[0m  [90/106], [94mLoss[0m : 1.93948
[1mStep[0m  [100/106], [94mLoss[0m : 1.80832

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.480, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48200
[1mStep[0m  [10/106], [94mLoss[0m : 2.01575
[1mStep[0m  [20/106], [94mLoss[0m : 1.65940
[1mStep[0m  [30/106], [94mLoss[0m : 1.84639
[1mStep[0m  [40/106], [94mLoss[0m : 1.78862
[1mStep[0m  [50/106], [94mLoss[0m : 1.71127
[1mStep[0m  [60/106], [94mLoss[0m : 1.57651
[1mStep[0m  [70/106], [94mLoss[0m : 1.54831
[1mStep[0m  [80/106], [94mLoss[0m : 1.85059
[1mStep[0m  [90/106], [94mLoss[0m : 1.94315
[1mStep[0m  [100/106], [94mLoss[0m : 1.78375

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76300
[1mStep[0m  [10/106], [94mLoss[0m : 1.63737
[1mStep[0m  [20/106], [94mLoss[0m : 1.69716
[1mStep[0m  [30/106], [94mLoss[0m : 1.75535
[1mStep[0m  [40/106], [94mLoss[0m : 1.82171
[1mStep[0m  [50/106], [94mLoss[0m : 1.53055
[1mStep[0m  [60/106], [94mLoss[0m : 1.86103
[1mStep[0m  [70/106], [94mLoss[0m : 1.52770
[1mStep[0m  [80/106], [94mLoss[0m : 1.97896
[1mStep[0m  [90/106], [94mLoss[0m : 1.64117
[1mStep[0m  [100/106], [94mLoss[0m : 1.78496

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76676
[1mStep[0m  [10/106], [94mLoss[0m : 1.66743
[1mStep[0m  [20/106], [94mLoss[0m : 1.60770
[1mStep[0m  [30/106], [94mLoss[0m : 1.67955
[1mStep[0m  [40/106], [94mLoss[0m : 1.64184
[1mStep[0m  [50/106], [94mLoss[0m : 1.65144
[1mStep[0m  [60/106], [94mLoss[0m : 1.57682
[1mStep[0m  [70/106], [94mLoss[0m : 1.77792
[1mStep[0m  [80/106], [94mLoss[0m : 1.69054
[1mStep[0m  [90/106], [94mLoss[0m : 1.51283
[1mStep[0m  [100/106], [94mLoss[0m : 1.71127

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.498, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70939
[1mStep[0m  [10/106], [94mLoss[0m : 1.52856
[1mStep[0m  [20/106], [94mLoss[0m : 1.73831
[1mStep[0m  [30/106], [94mLoss[0m : 1.49114
[1mStep[0m  [40/106], [94mLoss[0m : 1.60552
[1mStep[0m  [50/106], [94mLoss[0m : 1.75111
[1mStep[0m  [60/106], [94mLoss[0m : 1.61348
[1mStep[0m  [70/106], [94mLoss[0m : 1.54022
[1mStep[0m  [80/106], [94mLoss[0m : 1.78511
[1mStep[0m  [90/106], [94mLoss[0m : 1.41477
[1mStep[0m  [100/106], [94mLoss[0m : 1.96533

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.655, [92mTest[0m: 2.502, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42280
[1mStep[0m  [10/106], [94mLoss[0m : 1.37811
[1mStep[0m  [20/106], [94mLoss[0m : 1.57491
[1mStep[0m  [30/106], [94mLoss[0m : 1.69843
[1mStep[0m  [40/106], [94mLoss[0m : 1.46434
[1mStep[0m  [50/106], [94mLoss[0m : 1.70591
[1mStep[0m  [60/106], [94mLoss[0m : 1.78545
[1mStep[0m  [70/106], [94mLoss[0m : 1.77016
[1mStep[0m  [80/106], [94mLoss[0m : 1.72612
[1mStep[0m  [90/106], [94mLoss[0m : 1.60012
[1mStep[0m  [100/106], [94mLoss[0m : 1.54918

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65370
[1mStep[0m  [10/106], [94mLoss[0m : 1.47760
[1mStep[0m  [20/106], [94mLoss[0m : 1.57207
[1mStep[0m  [30/106], [94mLoss[0m : 1.71605
[1mStep[0m  [40/106], [94mLoss[0m : 1.40552
[1mStep[0m  [50/106], [94mLoss[0m : 1.80656
[1mStep[0m  [60/106], [94mLoss[0m : 1.61949
[1mStep[0m  [70/106], [94mLoss[0m : 1.85011
[1mStep[0m  [80/106], [94mLoss[0m : 1.36446
[1mStep[0m  [90/106], [94mLoss[0m : 1.47070
[1mStep[0m  [100/106], [94mLoss[0m : 1.77640

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.507, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39496
[1mStep[0m  [10/106], [94mLoss[0m : 1.28431
[1mStep[0m  [20/106], [94mLoss[0m : 1.76693
[1mStep[0m  [30/106], [94mLoss[0m : 1.52668
[1mStep[0m  [40/106], [94mLoss[0m : 1.48778
[1mStep[0m  [50/106], [94mLoss[0m : 1.47732
[1mStep[0m  [60/106], [94mLoss[0m : 1.36454
[1mStep[0m  [70/106], [94mLoss[0m : 1.93448
[1mStep[0m  [80/106], [94mLoss[0m : 1.51302
[1mStep[0m  [90/106], [94mLoss[0m : 1.50280
[1mStep[0m  [100/106], [94mLoss[0m : 1.46354

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.520, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.39189
[1mStep[0m  [10/106], [94mLoss[0m : 1.47111
[1mStep[0m  [20/106], [94mLoss[0m : 1.51230
[1mStep[0m  [30/106], [94mLoss[0m : 1.66258
[1mStep[0m  [40/106], [94mLoss[0m : 1.27072
[1mStep[0m  [50/106], [94mLoss[0m : 1.55496
[1mStep[0m  [60/106], [94mLoss[0m : 1.85519
[1mStep[0m  [70/106], [94mLoss[0m : 1.66134
[1mStep[0m  [80/106], [94mLoss[0m : 1.70559
[1mStep[0m  [90/106], [94mLoss[0m : 1.51004
[1mStep[0m  [100/106], [94mLoss[0m : 1.42427

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.532, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.65680
[1mStep[0m  [10/106], [94mLoss[0m : 1.50912
[1mStep[0m  [20/106], [94mLoss[0m : 1.43447
[1mStep[0m  [30/106], [94mLoss[0m : 1.36180
[1mStep[0m  [40/106], [94mLoss[0m : 1.55699
[1mStep[0m  [50/106], [94mLoss[0m : 1.65511
[1mStep[0m  [60/106], [94mLoss[0m : 1.49908
[1mStep[0m  [70/106], [94mLoss[0m : 1.48513
[1mStep[0m  [80/106], [94mLoss[0m : 1.58909
[1mStep[0m  [90/106], [94mLoss[0m : 1.54258
[1mStep[0m  [100/106], [94mLoss[0m : 1.59673

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.628, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.33611
[1mStep[0m  [10/106], [94mLoss[0m : 1.41467
[1mStep[0m  [20/106], [94mLoss[0m : 1.22607
[1mStep[0m  [30/106], [94mLoss[0m : 1.31970
[1mStep[0m  [40/106], [94mLoss[0m : 1.62691
[1mStep[0m  [50/106], [94mLoss[0m : 1.53643
[1mStep[0m  [60/106], [94mLoss[0m : 1.53883
[1mStep[0m  [70/106], [94mLoss[0m : 1.73742
[1mStep[0m  [80/106], [94mLoss[0m : 1.35458
[1mStep[0m  [90/106], [94mLoss[0m : 1.62532
[1mStep[0m  [100/106], [94mLoss[0m : 1.63629

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.43793
[1mStep[0m  [10/106], [94mLoss[0m : 1.38059
[1mStep[0m  [20/106], [94mLoss[0m : 1.48135
[1mStep[0m  [30/106], [94mLoss[0m : 1.33333
[1mStep[0m  [40/106], [94mLoss[0m : 1.40079
[1mStep[0m  [50/106], [94mLoss[0m : 1.64904
[1mStep[0m  [60/106], [94mLoss[0m : 1.38694
[1mStep[0m  [70/106], [94mLoss[0m : 1.45016
[1mStep[0m  [80/106], [94mLoss[0m : 1.39197
[1mStep[0m  [90/106], [94mLoss[0m : 1.36134
[1mStep[0m  [100/106], [94mLoss[0m : 1.49924

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.454, [92mTest[0m: 2.494, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.42914
[1mStep[0m  [10/106], [94mLoss[0m : 1.39204
[1mStep[0m  [20/106], [94mLoss[0m : 1.37032
[1mStep[0m  [30/106], [94mLoss[0m : 1.38662
[1mStep[0m  [40/106], [94mLoss[0m : 1.74134
[1mStep[0m  [50/106], [94mLoss[0m : 1.38881
[1mStep[0m  [60/106], [94mLoss[0m : 1.40962
[1mStep[0m  [70/106], [94mLoss[0m : 1.31760
[1mStep[0m  [80/106], [94mLoss[0m : 1.52450
[1mStep[0m  [90/106], [94mLoss[0m : 1.37172
[1mStep[0m  [100/106], [94mLoss[0m : 1.40793

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.412, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.31071
[1mStep[0m  [10/106], [94mLoss[0m : 1.27263
[1mStep[0m  [20/106], [94mLoss[0m : 1.21961
[1mStep[0m  [30/106], [94mLoss[0m : 1.35683
[1mStep[0m  [40/106], [94mLoss[0m : 1.42527
[1mStep[0m  [50/106], [94mLoss[0m : 1.45705
[1mStep[0m  [60/106], [94mLoss[0m : 1.35634
[1mStep[0m  [70/106], [94mLoss[0m : 1.64475
[1mStep[0m  [80/106], [94mLoss[0m : 1.49594
[1mStep[0m  [90/106], [94mLoss[0m : 1.32342
[1mStep[0m  [100/106], [94mLoss[0m : 1.42905

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.400, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.495
====================================

Phase 2 - Evaluation MAE:  2.4950385588519977
MAE score P1        2.392473
MAE score P2        2.495039
loss                1.400127
learning_rate        0.00505
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay           0.001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.18208
[1mStep[0m  [10/106], [94mLoss[0m : 9.33091
[1mStep[0m  [20/106], [94mLoss[0m : 8.22409
[1mStep[0m  [30/106], [94mLoss[0m : 7.32875
[1mStep[0m  [40/106], [94mLoss[0m : 6.27317
[1mStep[0m  [50/106], [94mLoss[0m : 4.70472
[1mStep[0m  [60/106], [94mLoss[0m : 3.62527
[1mStep[0m  [70/106], [94mLoss[0m : 3.47068
[1mStep[0m  [80/106], [94mLoss[0m : 2.96084
[1mStep[0m  [90/106], [94mLoss[0m : 2.87825
[1mStep[0m  [100/106], [94mLoss[0m : 2.57687

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.381, [92mTest[0m: 10.934, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34716
[1mStep[0m  [10/106], [94mLoss[0m : 2.76986
[1mStep[0m  [20/106], [94mLoss[0m : 2.62047
[1mStep[0m  [30/106], [94mLoss[0m : 2.77840
[1mStep[0m  [40/106], [94mLoss[0m : 2.38233
[1mStep[0m  [50/106], [94mLoss[0m : 2.37401
[1mStep[0m  [60/106], [94mLoss[0m : 2.74958
[1mStep[0m  [70/106], [94mLoss[0m : 2.71380
[1mStep[0m  [80/106], [94mLoss[0m : 2.85385
[1mStep[0m  [90/106], [94mLoss[0m : 2.55284
[1mStep[0m  [100/106], [94mLoss[0m : 2.76537

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.565, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90369
[1mStep[0m  [10/106], [94mLoss[0m : 2.20170
[1mStep[0m  [20/106], [94mLoss[0m : 3.15865
[1mStep[0m  [30/106], [94mLoss[0m : 2.67222
[1mStep[0m  [40/106], [94mLoss[0m : 2.78181
[1mStep[0m  [50/106], [94mLoss[0m : 2.47716
[1mStep[0m  [60/106], [94mLoss[0m : 2.96911
[1mStep[0m  [70/106], [94mLoss[0m : 2.43046
[1mStep[0m  [80/106], [94mLoss[0m : 2.28016
[1mStep[0m  [90/106], [94mLoss[0m : 2.55777
[1mStep[0m  [100/106], [94mLoss[0m : 2.62063

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32934
[1mStep[0m  [10/106], [94mLoss[0m : 2.38949
[1mStep[0m  [20/106], [94mLoss[0m : 2.12732
[1mStep[0m  [30/106], [94mLoss[0m : 2.55067
[1mStep[0m  [40/106], [94mLoss[0m : 2.42005
[1mStep[0m  [50/106], [94mLoss[0m : 2.32922
[1mStep[0m  [60/106], [94mLoss[0m : 2.71498
[1mStep[0m  [70/106], [94mLoss[0m : 2.56891
[1mStep[0m  [80/106], [94mLoss[0m : 2.59256
[1mStep[0m  [90/106], [94mLoss[0m : 2.53504
[1mStep[0m  [100/106], [94mLoss[0m : 2.15500

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56917
[1mStep[0m  [10/106], [94mLoss[0m : 2.54230
[1mStep[0m  [20/106], [94mLoss[0m : 2.51882
[1mStep[0m  [30/106], [94mLoss[0m : 2.83420
[1mStep[0m  [40/106], [94mLoss[0m : 2.80672
[1mStep[0m  [50/106], [94mLoss[0m : 2.64066
[1mStep[0m  [60/106], [94mLoss[0m : 2.86397
[1mStep[0m  [70/106], [94mLoss[0m : 2.64394
[1mStep[0m  [80/106], [94mLoss[0m : 2.45368
[1mStep[0m  [90/106], [94mLoss[0m : 2.45927
[1mStep[0m  [100/106], [94mLoss[0m : 2.76514

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30263
[1mStep[0m  [10/106], [94mLoss[0m : 2.77601
[1mStep[0m  [20/106], [94mLoss[0m : 2.71955
[1mStep[0m  [30/106], [94mLoss[0m : 2.59084
[1mStep[0m  [40/106], [94mLoss[0m : 2.39396
[1mStep[0m  [50/106], [94mLoss[0m : 2.46578
[1mStep[0m  [60/106], [94mLoss[0m : 2.86245
[1mStep[0m  [70/106], [94mLoss[0m : 2.62211
[1mStep[0m  [80/106], [94mLoss[0m : 2.74947
[1mStep[0m  [90/106], [94mLoss[0m : 2.93347
[1mStep[0m  [100/106], [94mLoss[0m : 2.44538

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70269
[1mStep[0m  [10/106], [94mLoss[0m : 2.83008
[1mStep[0m  [20/106], [94mLoss[0m : 2.51955
[1mStep[0m  [30/106], [94mLoss[0m : 2.39971
[1mStep[0m  [40/106], [94mLoss[0m : 2.67044
[1mStep[0m  [50/106], [94mLoss[0m : 2.67177
[1mStep[0m  [60/106], [94mLoss[0m : 2.30326
[1mStep[0m  [70/106], [94mLoss[0m : 2.57321
[1mStep[0m  [80/106], [94mLoss[0m : 2.39295
[1mStep[0m  [90/106], [94mLoss[0m : 2.62416
[1mStep[0m  [100/106], [94mLoss[0m : 2.77318

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.78684
[1mStep[0m  [10/106], [94mLoss[0m : 2.36820
[1mStep[0m  [20/106], [94mLoss[0m : 2.39947
[1mStep[0m  [30/106], [94mLoss[0m : 2.33235
[1mStep[0m  [40/106], [94mLoss[0m : 2.55859
[1mStep[0m  [50/106], [94mLoss[0m : 2.59052
[1mStep[0m  [60/106], [94mLoss[0m : 2.48386
[1mStep[0m  [70/106], [94mLoss[0m : 3.01472
[1mStep[0m  [80/106], [94mLoss[0m : 2.63384
[1mStep[0m  [90/106], [94mLoss[0m : 2.87212
[1mStep[0m  [100/106], [94mLoss[0m : 2.62400

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.433, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57584
[1mStep[0m  [10/106], [94mLoss[0m : 2.55217
[1mStep[0m  [20/106], [94mLoss[0m : 2.70866
[1mStep[0m  [30/106], [94mLoss[0m : 2.41936
[1mStep[0m  [40/106], [94mLoss[0m : 2.66562
[1mStep[0m  [50/106], [94mLoss[0m : 2.69018
[1mStep[0m  [60/106], [94mLoss[0m : 2.41719
[1mStep[0m  [70/106], [94mLoss[0m : 2.47218
[1mStep[0m  [80/106], [94mLoss[0m : 2.44482
[1mStep[0m  [90/106], [94mLoss[0m : 2.60588
[1mStep[0m  [100/106], [94mLoss[0m : 2.45666

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67045
[1mStep[0m  [10/106], [94mLoss[0m : 2.84362
[1mStep[0m  [20/106], [94mLoss[0m : 2.41844
[1mStep[0m  [30/106], [94mLoss[0m : 2.63690
[1mStep[0m  [40/106], [94mLoss[0m : 2.27314
[1mStep[0m  [50/106], [94mLoss[0m : 2.66002
[1mStep[0m  [60/106], [94mLoss[0m : 2.59100
[1mStep[0m  [70/106], [94mLoss[0m : 2.43743
[1mStep[0m  [80/106], [94mLoss[0m : 2.93085
[1mStep[0m  [90/106], [94mLoss[0m : 2.77268
[1mStep[0m  [100/106], [94mLoss[0m : 2.56309

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32382
[1mStep[0m  [10/106], [94mLoss[0m : 2.40106
[1mStep[0m  [20/106], [94mLoss[0m : 2.46213
[1mStep[0m  [30/106], [94mLoss[0m : 2.87136
[1mStep[0m  [40/106], [94mLoss[0m : 2.45249
[1mStep[0m  [50/106], [94mLoss[0m : 2.48100
[1mStep[0m  [60/106], [94mLoss[0m : 2.84100
[1mStep[0m  [70/106], [94mLoss[0m : 2.55018
[1mStep[0m  [80/106], [94mLoss[0m : 2.41789
[1mStep[0m  [90/106], [94mLoss[0m : 2.42123
[1mStep[0m  [100/106], [94mLoss[0m : 2.36020

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75592
[1mStep[0m  [10/106], [94mLoss[0m : 2.71237
[1mStep[0m  [20/106], [94mLoss[0m : 2.28155
[1mStep[0m  [30/106], [94mLoss[0m : 2.55113
[1mStep[0m  [40/106], [94mLoss[0m : 2.49571
[1mStep[0m  [50/106], [94mLoss[0m : 2.41135
[1mStep[0m  [60/106], [94mLoss[0m : 2.82449
[1mStep[0m  [70/106], [94mLoss[0m : 2.65176
[1mStep[0m  [80/106], [94mLoss[0m : 2.48825
[1mStep[0m  [90/106], [94mLoss[0m : 2.71959
[1mStep[0m  [100/106], [94mLoss[0m : 2.52021

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31427
[1mStep[0m  [10/106], [94mLoss[0m : 2.47358
[1mStep[0m  [20/106], [94mLoss[0m : 2.24968
[1mStep[0m  [30/106], [94mLoss[0m : 2.64489
[1mStep[0m  [40/106], [94mLoss[0m : 2.58398
[1mStep[0m  [50/106], [94mLoss[0m : 2.40396
[1mStep[0m  [60/106], [94mLoss[0m : 2.54476
[1mStep[0m  [70/106], [94mLoss[0m : 2.57390
[1mStep[0m  [80/106], [94mLoss[0m : 2.91483
[1mStep[0m  [90/106], [94mLoss[0m : 2.48533
[1mStep[0m  [100/106], [94mLoss[0m : 2.38844

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.421, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56098
[1mStep[0m  [10/106], [94mLoss[0m : 2.51928
[1mStep[0m  [20/106], [94mLoss[0m : 2.55354
[1mStep[0m  [30/106], [94mLoss[0m : 3.19221
[1mStep[0m  [40/106], [94mLoss[0m : 2.53246
[1mStep[0m  [50/106], [94mLoss[0m : 2.69048
[1mStep[0m  [60/106], [94mLoss[0m : 2.44925
[1mStep[0m  [70/106], [94mLoss[0m : 2.15763
[1mStep[0m  [80/106], [94mLoss[0m : 2.48037
[1mStep[0m  [90/106], [94mLoss[0m : 2.32545
[1mStep[0m  [100/106], [94mLoss[0m : 2.64019

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20119
[1mStep[0m  [10/106], [94mLoss[0m : 2.86742
[1mStep[0m  [20/106], [94mLoss[0m : 2.60467
[1mStep[0m  [30/106], [94mLoss[0m : 2.63265
[1mStep[0m  [40/106], [94mLoss[0m : 2.37738
[1mStep[0m  [50/106], [94mLoss[0m : 2.75257
[1mStep[0m  [60/106], [94mLoss[0m : 2.32885
[1mStep[0m  [70/106], [94mLoss[0m : 2.54246
[1mStep[0m  [80/106], [94mLoss[0m : 2.55502
[1mStep[0m  [90/106], [94mLoss[0m : 2.97981
[1mStep[0m  [100/106], [94mLoss[0m : 2.87054

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74692
[1mStep[0m  [10/106], [94mLoss[0m : 2.57848
[1mStep[0m  [20/106], [94mLoss[0m : 2.60981
[1mStep[0m  [30/106], [94mLoss[0m : 2.68307
[1mStep[0m  [40/106], [94mLoss[0m : 2.69123
[1mStep[0m  [50/106], [94mLoss[0m : 2.29062
[1mStep[0m  [60/106], [94mLoss[0m : 2.64996
[1mStep[0m  [70/106], [94mLoss[0m : 2.56113
[1mStep[0m  [80/106], [94mLoss[0m : 2.38072
[1mStep[0m  [90/106], [94mLoss[0m : 2.46886
[1mStep[0m  [100/106], [94mLoss[0m : 2.90403

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56603
[1mStep[0m  [10/106], [94mLoss[0m : 2.35191
[1mStep[0m  [20/106], [94mLoss[0m : 2.55563
[1mStep[0m  [30/106], [94mLoss[0m : 2.61843
[1mStep[0m  [40/106], [94mLoss[0m : 2.37099
[1mStep[0m  [50/106], [94mLoss[0m : 2.30461
[1mStep[0m  [60/106], [94mLoss[0m : 2.57821
[1mStep[0m  [70/106], [94mLoss[0m : 2.69795
[1mStep[0m  [80/106], [94mLoss[0m : 2.31816
[1mStep[0m  [90/106], [94mLoss[0m : 2.44777
[1mStep[0m  [100/106], [94mLoss[0m : 2.57601

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52208
[1mStep[0m  [10/106], [94mLoss[0m : 2.34507
[1mStep[0m  [20/106], [94mLoss[0m : 2.35806
[1mStep[0m  [30/106], [94mLoss[0m : 2.75662
[1mStep[0m  [40/106], [94mLoss[0m : 2.40280
[1mStep[0m  [50/106], [94mLoss[0m : 2.68099
[1mStep[0m  [60/106], [94mLoss[0m : 2.71548
[1mStep[0m  [70/106], [94mLoss[0m : 2.47128
[1mStep[0m  [80/106], [94mLoss[0m : 2.41176
[1mStep[0m  [90/106], [94mLoss[0m : 2.53861
[1mStep[0m  [100/106], [94mLoss[0m : 2.65115

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75895
[1mStep[0m  [10/106], [94mLoss[0m : 2.55914
[1mStep[0m  [20/106], [94mLoss[0m : 2.49052
[1mStep[0m  [30/106], [94mLoss[0m : 2.42816
[1mStep[0m  [40/106], [94mLoss[0m : 2.64962
[1mStep[0m  [50/106], [94mLoss[0m : 2.70678
[1mStep[0m  [60/106], [94mLoss[0m : 2.57496
[1mStep[0m  [70/106], [94mLoss[0m : 2.55292
[1mStep[0m  [80/106], [94mLoss[0m : 3.01306
[1mStep[0m  [90/106], [94mLoss[0m : 2.22377
[1mStep[0m  [100/106], [94mLoss[0m : 2.47873

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60583
[1mStep[0m  [10/106], [94mLoss[0m : 2.53665
[1mStep[0m  [20/106], [94mLoss[0m : 2.69086
[1mStep[0m  [30/106], [94mLoss[0m : 2.36843
[1mStep[0m  [40/106], [94mLoss[0m : 2.67724
[1mStep[0m  [50/106], [94mLoss[0m : 2.62449
[1mStep[0m  [60/106], [94mLoss[0m : 2.12618
[1mStep[0m  [70/106], [94mLoss[0m : 2.69076
[1mStep[0m  [80/106], [94mLoss[0m : 2.51413
[1mStep[0m  [90/106], [94mLoss[0m : 2.80892
[1mStep[0m  [100/106], [94mLoss[0m : 2.67266

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49438
[1mStep[0m  [10/106], [94mLoss[0m : 2.65464
[1mStep[0m  [20/106], [94mLoss[0m : 2.59241
[1mStep[0m  [30/106], [94mLoss[0m : 2.69652
[1mStep[0m  [40/106], [94mLoss[0m : 2.99433
[1mStep[0m  [50/106], [94mLoss[0m : 2.68381
[1mStep[0m  [60/106], [94mLoss[0m : 2.44156
[1mStep[0m  [70/106], [94mLoss[0m : 2.56445
[1mStep[0m  [80/106], [94mLoss[0m : 2.63383
[1mStep[0m  [90/106], [94mLoss[0m : 2.70511
[1mStep[0m  [100/106], [94mLoss[0m : 2.83929

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.418, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60628
[1mStep[0m  [10/106], [94mLoss[0m : 2.65393
[1mStep[0m  [20/106], [94mLoss[0m : 2.57518
[1mStep[0m  [30/106], [94mLoss[0m : 2.31394
[1mStep[0m  [40/106], [94mLoss[0m : 2.59982
[1mStep[0m  [50/106], [94mLoss[0m : 2.46855
[1mStep[0m  [60/106], [94mLoss[0m : 2.54398
[1mStep[0m  [70/106], [94mLoss[0m : 2.56323
[1mStep[0m  [80/106], [94mLoss[0m : 2.52541
[1mStep[0m  [90/106], [94mLoss[0m : 2.43879
[1mStep[0m  [100/106], [94mLoss[0m : 2.73406

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56221
[1mStep[0m  [10/106], [94mLoss[0m : 2.26145
[1mStep[0m  [20/106], [94mLoss[0m : 2.57667
[1mStep[0m  [30/106], [94mLoss[0m : 2.69131
[1mStep[0m  [40/106], [94mLoss[0m : 2.63127
[1mStep[0m  [50/106], [94mLoss[0m : 2.58183
[1mStep[0m  [60/106], [94mLoss[0m : 2.74481
[1mStep[0m  [70/106], [94mLoss[0m : 2.28994
[1mStep[0m  [80/106], [94mLoss[0m : 2.71420
[1mStep[0m  [90/106], [94mLoss[0m : 2.69666
[1mStep[0m  [100/106], [94mLoss[0m : 2.49499

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35567
[1mStep[0m  [10/106], [94mLoss[0m : 2.52359
[1mStep[0m  [20/106], [94mLoss[0m : 2.54589
[1mStep[0m  [30/106], [94mLoss[0m : 2.49022
[1mStep[0m  [40/106], [94mLoss[0m : 2.58375
[1mStep[0m  [50/106], [94mLoss[0m : 2.13834
[1mStep[0m  [60/106], [94mLoss[0m : 2.32902
[1mStep[0m  [70/106], [94mLoss[0m : 2.88860
[1mStep[0m  [80/106], [94mLoss[0m : 2.42691
[1mStep[0m  [90/106], [94mLoss[0m : 2.78945
[1mStep[0m  [100/106], [94mLoss[0m : 2.89415

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50917
[1mStep[0m  [10/106], [94mLoss[0m : 3.04635
[1mStep[0m  [20/106], [94mLoss[0m : 2.36659
[1mStep[0m  [30/106], [94mLoss[0m : 2.45318
[1mStep[0m  [40/106], [94mLoss[0m : 2.50139
[1mStep[0m  [50/106], [94mLoss[0m : 2.50078
[1mStep[0m  [60/106], [94mLoss[0m : 2.58449
[1mStep[0m  [70/106], [94mLoss[0m : 2.57641
[1mStep[0m  [80/106], [94mLoss[0m : 2.66604
[1mStep[0m  [90/106], [94mLoss[0m : 2.22444
[1mStep[0m  [100/106], [94mLoss[0m : 2.44765

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.82253
[1mStep[0m  [10/106], [94mLoss[0m : 2.36617
[1mStep[0m  [20/106], [94mLoss[0m : 2.59778
[1mStep[0m  [30/106], [94mLoss[0m : 2.64077
[1mStep[0m  [40/106], [94mLoss[0m : 2.64897
[1mStep[0m  [50/106], [94mLoss[0m : 2.49440
[1mStep[0m  [60/106], [94mLoss[0m : 2.60607
[1mStep[0m  [70/106], [94mLoss[0m : 2.69146
[1mStep[0m  [80/106], [94mLoss[0m : 2.18517
[1mStep[0m  [90/106], [94mLoss[0m : 2.21211
[1mStep[0m  [100/106], [94mLoss[0m : 2.49368

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.408, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44644
[1mStep[0m  [10/106], [94mLoss[0m : 2.91960
[1mStep[0m  [20/106], [94mLoss[0m : 2.27852
[1mStep[0m  [30/106], [94mLoss[0m : 2.22856
[1mStep[0m  [40/106], [94mLoss[0m : 2.61080
[1mStep[0m  [50/106], [94mLoss[0m : 2.58834
[1mStep[0m  [60/106], [94mLoss[0m : 2.32012
[1mStep[0m  [70/106], [94mLoss[0m : 2.59702
[1mStep[0m  [80/106], [94mLoss[0m : 2.59259
[1mStep[0m  [90/106], [94mLoss[0m : 2.41189
[1mStep[0m  [100/106], [94mLoss[0m : 2.25682

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40471
[1mStep[0m  [10/106], [94mLoss[0m : 2.64627
[1mStep[0m  [20/106], [94mLoss[0m : 2.32598
[1mStep[0m  [30/106], [94mLoss[0m : 2.44252
[1mStep[0m  [40/106], [94mLoss[0m : 2.32722
[1mStep[0m  [50/106], [94mLoss[0m : 2.67983
[1mStep[0m  [60/106], [94mLoss[0m : 2.32186
[1mStep[0m  [70/106], [94mLoss[0m : 3.02777
[1mStep[0m  [80/106], [94mLoss[0m : 2.46729
[1mStep[0m  [90/106], [94mLoss[0m : 2.49642
[1mStep[0m  [100/106], [94mLoss[0m : 2.71784

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.405, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66053
[1mStep[0m  [10/106], [94mLoss[0m : 2.81007
[1mStep[0m  [20/106], [94mLoss[0m : 2.51881
[1mStep[0m  [30/106], [94mLoss[0m : 2.51720
[1mStep[0m  [40/106], [94mLoss[0m : 2.41787
[1mStep[0m  [50/106], [94mLoss[0m : 2.37142
[1mStep[0m  [60/106], [94mLoss[0m : 2.69405
[1mStep[0m  [70/106], [94mLoss[0m : 2.64662
[1mStep[0m  [80/106], [94mLoss[0m : 2.54385
[1mStep[0m  [90/106], [94mLoss[0m : 2.50997
[1mStep[0m  [100/106], [94mLoss[0m : 2.45468

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.404, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54493
[1mStep[0m  [10/106], [94mLoss[0m : 2.59887
[1mStep[0m  [20/106], [94mLoss[0m : 2.27734
[1mStep[0m  [30/106], [94mLoss[0m : 2.74213
[1mStep[0m  [40/106], [94mLoss[0m : 2.75444
[1mStep[0m  [50/106], [94mLoss[0m : 2.22409
[1mStep[0m  [60/106], [94mLoss[0m : 2.49313
[1mStep[0m  [70/106], [94mLoss[0m : 2.11693
[1mStep[0m  [80/106], [94mLoss[0m : 2.45797
[1mStep[0m  [90/106], [94mLoss[0m : 2.20103
[1mStep[0m  [100/106], [94mLoss[0m : 2.25393

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.406, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.401
====================================

Phase 1 - Evaluation MAE:  2.4006086205536463
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.29133
[1mStep[0m  [10/106], [94mLoss[0m : 2.62618
[1mStep[0m  [20/106], [94mLoss[0m : 2.32063
[1mStep[0m  [30/106], [94mLoss[0m : 2.40363
[1mStep[0m  [40/106], [94mLoss[0m : 2.71993
[1mStep[0m  [50/106], [94mLoss[0m : 2.24151
[1mStep[0m  [60/106], [94mLoss[0m : 2.66330
[1mStep[0m  [70/106], [94mLoss[0m : 2.37707
[1mStep[0m  [80/106], [94mLoss[0m : 2.49795
[1mStep[0m  [90/106], [94mLoss[0m : 2.39880
[1mStep[0m  [100/106], [94mLoss[0m : 2.49541

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47744
[1mStep[0m  [10/106], [94mLoss[0m : 2.29698
[1mStep[0m  [20/106], [94mLoss[0m : 2.62083
[1mStep[0m  [30/106], [94mLoss[0m : 2.75405
[1mStep[0m  [40/106], [94mLoss[0m : 2.20470
[1mStep[0m  [50/106], [94mLoss[0m : 2.83246
[1mStep[0m  [60/106], [94mLoss[0m : 2.64944
[1mStep[0m  [70/106], [94mLoss[0m : 2.33726
[1mStep[0m  [80/106], [94mLoss[0m : 2.75438
[1mStep[0m  [90/106], [94mLoss[0m : 2.44198
[1mStep[0m  [100/106], [94mLoss[0m : 2.37700

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.639, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24012
[1mStep[0m  [10/106], [94mLoss[0m : 2.42393
[1mStep[0m  [20/106], [94mLoss[0m : 2.38898
[1mStep[0m  [30/106], [94mLoss[0m : 2.63483
[1mStep[0m  [40/106], [94mLoss[0m : 2.27603
[1mStep[0m  [50/106], [94mLoss[0m : 2.94875
[1mStep[0m  [60/106], [94mLoss[0m : 2.50959
[1mStep[0m  [70/106], [94mLoss[0m : 2.28631
[1mStep[0m  [80/106], [94mLoss[0m : 2.78734
[1mStep[0m  [90/106], [94mLoss[0m : 2.55348
[1mStep[0m  [100/106], [94mLoss[0m : 2.55257

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.582, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39723
[1mStep[0m  [10/106], [94mLoss[0m : 2.33640
[1mStep[0m  [20/106], [94mLoss[0m : 2.49331
[1mStep[0m  [30/106], [94mLoss[0m : 2.50717
[1mStep[0m  [40/106], [94mLoss[0m : 2.20040
[1mStep[0m  [50/106], [94mLoss[0m : 2.21414
[1mStep[0m  [60/106], [94mLoss[0m : 2.01734
[1mStep[0m  [70/106], [94mLoss[0m : 2.42247
[1mStep[0m  [80/106], [94mLoss[0m : 2.55000
[1mStep[0m  [90/106], [94mLoss[0m : 2.72817
[1mStep[0m  [100/106], [94mLoss[0m : 2.35632

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.614, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52869
[1mStep[0m  [10/106], [94mLoss[0m : 2.36269
[1mStep[0m  [20/106], [94mLoss[0m : 2.37957
[1mStep[0m  [30/106], [94mLoss[0m : 2.28801
[1mStep[0m  [40/106], [94mLoss[0m : 2.33394
[1mStep[0m  [50/106], [94mLoss[0m : 2.34924
[1mStep[0m  [60/106], [94mLoss[0m : 2.42466
[1mStep[0m  [70/106], [94mLoss[0m : 2.34789
[1mStep[0m  [80/106], [94mLoss[0m : 2.30214
[1mStep[0m  [90/106], [94mLoss[0m : 2.34657
[1mStep[0m  [100/106], [94mLoss[0m : 2.29157

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.551, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19971
[1mStep[0m  [10/106], [94mLoss[0m : 2.37989
[1mStep[0m  [20/106], [94mLoss[0m : 2.65309
[1mStep[0m  [30/106], [94mLoss[0m : 2.56363
[1mStep[0m  [40/106], [94mLoss[0m : 2.24059
[1mStep[0m  [50/106], [94mLoss[0m : 2.77773
[1mStep[0m  [60/106], [94mLoss[0m : 2.54492
[1mStep[0m  [70/106], [94mLoss[0m : 2.42319
[1mStep[0m  [80/106], [94mLoss[0m : 2.16157
[1mStep[0m  [90/106], [94mLoss[0m : 2.52884
[1mStep[0m  [100/106], [94mLoss[0m : 2.61264

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.605, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29491
[1mStep[0m  [10/106], [94mLoss[0m : 2.37269
[1mStep[0m  [20/106], [94mLoss[0m : 2.05587
[1mStep[0m  [30/106], [94mLoss[0m : 2.21579
[1mStep[0m  [40/106], [94mLoss[0m : 2.13458
[1mStep[0m  [50/106], [94mLoss[0m : 2.30307
[1mStep[0m  [60/106], [94mLoss[0m : 2.41267
[1mStep[0m  [70/106], [94mLoss[0m : 2.18752
[1mStep[0m  [80/106], [94mLoss[0m : 2.49168
[1mStep[0m  [90/106], [94mLoss[0m : 2.39554
[1mStep[0m  [100/106], [94mLoss[0m : 2.56888

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.496, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35872
[1mStep[0m  [10/106], [94mLoss[0m : 2.34115
[1mStep[0m  [20/106], [94mLoss[0m : 2.37679
[1mStep[0m  [30/106], [94mLoss[0m : 2.14516
[1mStep[0m  [40/106], [94mLoss[0m : 2.35323
[1mStep[0m  [50/106], [94mLoss[0m : 2.01222
[1mStep[0m  [60/106], [94mLoss[0m : 2.11486
[1mStep[0m  [70/106], [94mLoss[0m : 2.46399
[1mStep[0m  [80/106], [94mLoss[0m : 2.42506
[1mStep[0m  [90/106], [94mLoss[0m : 2.45900
[1mStep[0m  [100/106], [94mLoss[0m : 2.34725

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30003
[1mStep[0m  [10/106], [94mLoss[0m : 2.36873
[1mStep[0m  [20/106], [94mLoss[0m : 2.10630
[1mStep[0m  [30/106], [94mLoss[0m : 1.98315
[1mStep[0m  [40/106], [94mLoss[0m : 2.23696
[1mStep[0m  [50/106], [94mLoss[0m : 2.20584
[1mStep[0m  [60/106], [94mLoss[0m : 2.33529
[1mStep[0m  [70/106], [94mLoss[0m : 2.11898
[1mStep[0m  [80/106], [94mLoss[0m : 2.13785
[1mStep[0m  [90/106], [94mLoss[0m : 2.14024
[1mStep[0m  [100/106], [94mLoss[0m : 2.16399

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.553, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20419
[1mStep[0m  [10/106], [94mLoss[0m : 1.92601
[1mStep[0m  [20/106], [94mLoss[0m : 2.20466
[1mStep[0m  [30/106], [94mLoss[0m : 2.18153
[1mStep[0m  [40/106], [94mLoss[0m : 2.38489
[1mStep[0m  [50/106], [94mLoss[0m : 1.83930
[1mStep[0m  [60/106], [94mLoss[0m : 2.00436
[1mStep[0m  [70/106], [94mLoss[0m : 2.01434
[1mStep[0m  [80/106], [94mLoss[0m : 2.50650
[1mStep[0m  [90/106], [94mLoss[0m : 2.30542
[1mStep[0m  [100/106], [94mLoss[0m : 2.01678

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.165, [92mTest[0m: 2.629, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90950
[1mStep[0m  [10/106], [94mLoss[0m : 1.95605
[1mStep[0m  [20/106], [94mLoss[0m : 1.88862
[1mStep[0m  [30/106], [94mLoss[0m : 2.12574
[1mStep[0m  [40/106], [94mLoss[0m : 2.23988
[1mStep[0m  [50/106], [94mLoss[0m : 2.26951
[1mStep[0m  [60/106], [94mLoss[0m : 2.26693
[1mStep[0m  [70/106], [94mLoss[0m : 2.09813
[1mStep[0m  [80/106], [94mLoss[0m : 2.02454
[1mStep[0m  [90/106], [94mLoss[0m : 2.31631
[1mStep[0m  [100/106], [94mLoss[0m : 2.26972

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.627, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96991
[1mStep[0m  [10/106], [94mLoss[0m : 2.05208
[1mStep[0m  [20/106], [94mLoss[0m : 2.22243
[1mStep[0m  [30/106], [94mLoss[0m : 2.07850
[1mStep[0m  [40/106], [94mLoss[0m : 2.19315
[1mStep[0m  [50/106], [94mLoss[0m : 1.94925
[1mStep[0m  [60/106], [94mLoss[0m : 2.16706
[1mStep[0m  [70/106], [94mLoss[0m : 1.94784
[1mStep[0m  [80/106], [94mLoss[0m : 1.89273
[1mStep[0m  [90/106], [94mLoss[0m : 2.28706
[1mStep[0m  [100/106], [94mLoss[0m : 2.37995

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31938
[1mStep[0m  [10/106], [94mLoss[0m : 1.83469
[1mStep[0m  [20/106], [94mLoss[0m : 2.01269
[1mStep[0m  [30/106], [94mLoss[0m : 1.97955
[1mStep[0m  [40/106], [94mLoss[0m : 2.27296
[1mStep[0m  [50/106], [94mLoss[0m : 1.84569
[1mStep[0m  [60/106], [94mLoss[0m : 1.87635
[1mStep[0m  [70/106], [94mLoss[0m : 1.71506
[1mStep[0m  [80/106], [94mLoss[0m : 1.98728
[1mStep[0m  [90/106], [94mLoss[0m : 2.00769
[1mStep[0m  [100/106], [94mLoss[0m : 2.18183

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70447
[1mStep[0m  [10/106], [94mLoss[0m : 1.71702
[1mStep[0m  [20/106], [94mLoss[0m : 1.90924
[1mStep[0m  [30/106], [94mLoss[0m : 1.67453
[1mStep[0m  [40/106], [94mLoss[0m : 1.92770
[1mStep[0m  [50/106], [94mLoss[0m : 1.88112
[1mStep[0m  [60/106], [94mLoss[0m : 2.19319
[1mStep[0m  [70/106], [94mLoss[0m : 2.20198
[1mStep[0m  [80/106], [94mLoss[0m : 2.21188
[1mStep[0m  [90/106], [94mLoss[0m : 1.87205
[1mStep[0m  [100/106], [94mLoss[0m : 1.85884

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.535, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92919
[1mStep[0m  [10/106], [94mLoss[0m : 1.87575
[1mStep[0m  [20/106], [94mLoss[0m : 1.97284
[1mStep[0m  [30/106], [94mLoss[0m : 2.05512
[1mStep[0m  [40/106], [94mLoss[0m : 2.01984
[1mStep[0m  [50/106], [94mLoss[0m : 2.04173
[1mStep[0m  [60/106], [94mLoss[0m : 1.80940
[1mStep[0m  [70/106], [94mLoss[0m : 1.77180
[1mStep[0m  [80/106], [94mLoss[0m : 2.18996
[1mStep[0m  [90/106], [94mLoss[0m : 2.08873
[1mStep[0m  [100/106], [94mLoss[0m : 2.03559

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.459, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74859
[1mStep[0m  [10/106], [94mLoss[0m : 1.65417
[1mStep[0m  [20/106], [94mLoss[0m : 2.07271
[1mStep[0m  [30/106], [94mLoss[0m : 1.83462
[1mStep[0m  [40/106], [94mLoss[0m : 1.97650
[1mStep[0m  [50/106], [94mLoss[0m : 1.95688
[1mStep[0m  [60/106], [94mLoss[0m : 2.03448
[1mStep[0m  [70/106], [94mLoss[0m : 1.63472
[1mStep[0m  [80/106], [94mLoss[0m : 2.30558
[1mStep[0m  [90/106], [94mLoss[0m : 1.86929
[1mStep[0m  [100/106], [94mLoss[0m : 1.99516

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.85793
[1mStep[0m  [10/106], [94mLoss[0m : 1.77816
[1mStep[0m  [20/106], [94mLoss[0m : 1.77554
[1mStep[0m  [30/106], [94mLoss[0m : 1.69801
[1mStep[0m  [40/106], [94mLoss[0m : 1.81247
[1mStep[0m  [50/106], [94mLoss[0m : 2.09508
[1mStep[0m  [60/106], [94mLoss[0m : 2.01198
[1mStep[0m  [70/106], [94mLoss[0m : 1.78203
[1mStep[0m  [80/106], [94mLoss[0m : 1.94809
[1mStep[0m  [90/106], [94mLoss[0m : 1.81238
[1mStep[0m  [100/106], [94mLoss[0m : 1.80465

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02393
[1mStep[0m  [10/106], [94mLoss[0m : 1.69558
[1mStep[0m  [20/106], [94mLoss[0m : 2.06169
[1mStep[0m  [30/106], [94mLoss[0m : 1.99327
[1mStep[0m  [40/106], [94mLoss[0m : 1.94754
[1mStep[0m  [50/106], [94mLoss[0m : 1.82664
[1mStep[0m  [60/106], [94mLoss[0m : 1.93862
[1mStep[0m  [70/106], [94mLoss[0m : 2.08568
[1mStep[0m  [80/106], [94mLoss[0m : 1.91124
[1mStep[0m  [90/106], [94mLoss[0m : 1.72150
[1mStep[0m  [100/106], [94mLoss[0m : 1.90913

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.584, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77945
[1mStep[0m  [10/106], [94mLoss[0m : 1.61241
[1mStep[0m  [20/106], [94mLoss[0m : 1.80184
[1mStep[0m  [30/106], [94mLoss[0m : 1.86481
[1mStep[0m  [40/106], [94mLoss[0m : 1.64521
[1mStep[0m  [50/106], [94mLoss[0m : 2.12134
[1mStep[0m  [60/106], [94mLoss[0m : 1.76027
[1mStep[0m  [70/106], [94mLoss[0m : 2.19248
[1mStep[0m  [80/106], [94mLoss[0m : 1.78548
[1mStep[0m  [90/106], [94mLoss[0m : 2.05171
[1mStep[0m  [100/106], [94mLoss[0m : 1.73249

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.604, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70322
[1mStep[0m  [10/106], [94mLoss[0m : 1.89960
[1mStep[0m  [20/106], [94mLoss[0m : 1.82646
[1mStep[0m  [30/106], [94mLoss[0m : 1.86252
[1mStep[0m  [40/106], [94mLoss[0m : 1.70380
[1mStep[0m  [50/106], [94mLoss[0m : 1.81512
[1mStep[0m  [60/106], [94mLoss[0m : 1.85423
[1mStep[0m  [70/106], [94mLoss[0m : 1.81691
[1mStep[0m  [80/106], [94mLoss[0m : 1.84575
[1mStep[0m  [90/106], [94mLoss[0m : 1.82371
[1mStep[0m  [100/106], [94mLoss[0m : 1.64511

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.495, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.94732
[1mStep[0m  [10/106], [94mLoss[0m : 1.66472
[1mStep[0m  [20/106], [94mLoss[0m : 1.74672
[1mStep[0m  [30/106], [94mLoss[0m : 1.53610
[1mStep[0m  [40/106], [94mLoss[0m : 1.75946
[1mStep[0m  [50/106], [94mLoss[0m : 1.80704
[1mStep[0m  [60/106], [94mLoss[0m : 1.53683
[1mStep[0m  [70/106], [94mLoss[0m : 1.58477
[1mStep[0m  [80/106], [94mLoss[0m : 1.82296
[1mStep[0m  [90/106], [94mLoss[0m : 1.71143
[1mStep[0m  [100/106], [94mLoss[0m : 1.76988

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.80970
[1mStep[0m  [10/106], [94mLoss[0m : 1.55623
[1mStep[0m  [20/106], [94mLoss[0m : 1.57768
[1mStep[0m  [30/106], [94mLoss[0m : 1.80264
[1mStep[0m  [40/106], [94mLoss[0m : 1.86528
[1mStep[0m  [50/106], [94mLoss[0m : 1.94995
[1mStep[0m  [60/106], [94mLoss[0m : 1.82339
[1mStep[0m  [70/106], [94mLoss[0m : 1.65367
[1mStep[0m  [80/106], [94mLoss[0m : 1.84137
[1mStep[0m  [90/106], [94mLoss[0m : 1.80322
[1mStep[0m  [100/106], [94mLoss[0m : 2.01900

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.64531
[1mStep[0m  [10/106], [94mLoss[0m : 1.80807
[1mStep[0m  [20/106], [94mLoss[0m : 1.52710
[1mStep[0m  [30/106], [94mLoss[0m : 1.33836
[1mStep[0m  [40/106], [94mLoss[0m : 1.61169
[1mStep[0m  [50/106], [94mLoss[0m : 1.78624
[1mStep[0m  [60/106], [94mLoss[0m : 1.94058
[1mStep[0m  [70/106], [94mLoss[0m : 2.00896
[1mStep[0m  [80/106], [94mLoss[0m : 1.89318
[1mStep[0m  [90/106], [94mLoss[0m : 1.90862
[1mStep[0m  [100/106], [94mLoss[0m : 1.77671

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.594, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63396
[1mStep[0m  [10/106], [94mLoss[0m : 1.68455
[1mStep[0m  [20/106], [94mLoss[0m : 1.63537
[1mStep[0m  [30/106], [94mLoss[0m : 1.75490
[1mStep[0m  [40/106], [94mLoss[0m : 1.70465
[1mStep[0m  [50/106], [94mLoss[0m : 1.35987
[1mStep[0m  [60/106], [94mLoss[0m : 1.50221
[1mStep[0m  [70/106], [94mLoss[0m : 1.80343
[1mStep[0m  [80/106], [94mLoss[0m : 1.81114
[1mStep[0m  [90/106], [94mLoss[0m : 1.55816
[1mStep[0m  [100/106], [94mLoss[0m : 1.86694

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.459, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.61519
[1mStep[0m  [10/106], [94mLoss[0m : 1.86120
[1mStep[0m  [20/106], [94mLoss[0m : 1.54939
[1mStep[0m  [30/106], [94mLoss[0m : 2.02580
[1mStep[0m  [40/106], [94mLoss[0m : 1.66603
[1mStep[0m  [50/106], [94mLoss[0m : 1.68447
[1mStep[0m  [60/106], [94mLoss[0m : 1.65051
[1mStep[0m  [70/106], [94mLoss[0m : 1.71769
[1mStep[0m  [80/106], [94mLoss[0m : 1.71389
[1mStep[0m  [90/106], [94mLoss[0m : 1.63872
[1mStep[0m  [100/106], [94mLoss[0m : 1.82915

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.593, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.53094
[1mStep[0m  [10/106], [94mLoss[0m : 1.52209
[1mStep[0m  [20/106], [94mLoss[0m : 1.59181
[1mStep[0m  [30/106], [94mLoss[0m : 1.43384
[1mStep[0m  [40/106], [94mLoss[0m : 1.76052
[1mStep[0m  [50/106], [94mLoss[0m : 1.91252
[1mStep[0m  [60/106], [94mLoss[0m : 1.79055
[1mStep[0m  [70/106], [94mLoss[0m : 1.72529
[1mStep[0m  [80/106], [94mLoss[0m : 1.76735
[1mStep[0m  [90/106], [94mLoss[0m : 1.62778
[1mStep[0m  [100/106], [94mLoss[0m : 1.59188

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76227
[1mStep[0m  [10/106], [94mLoss[0m : 1.59684
[1mStep[0m  [20/106], [94mLoss[0m : 1.65382
[1mStep[0m  [30/106], [94mLoss[0m : 1.81305
[1mStep[0m  [40/106], [94mLoss[0m : 1.74185
[1mStep[0m  [50/106], [94mLoss[0m : 1.78838
[1mStep[0m  [60/106], [94mLoss[0m : 1.76418
[1mStep[0m  [70/106], [94mLoss[0m : 1.57108
[1mStep[0m  [80/106], [94mLoss[0m : 1.66680
[1mStep[0m  [90/106], [94mLoss[0m : 1.61800
[1mStep[0m  [100/106], [94mLoss[0m : 1.88615

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.576, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.57923
[1mStep[0m  [10/106], [94mLoss[0m : 1.48274
[1mStep[0m  [20/106], [94mLoss[0m : 1.60194
[1mStep[0m  [30/106], [94mLoss[0m : 1.43838
[1mStep[0m  [40/106], [94mLoss[0m : 1.65461
[1mStep[0m  [50/106], [94mLoss[0m : 1.70841
[1mStep[0m  [60/106], [94mLoss[0m : 1.51816
[1mStep[0m  [70/106], [94mLoss[0m : 1.47873
[1mStep[0m  [80/106], [94mLoss[0m : 1.65494
[1mStep[0m  [90/106], [94mLoss[0m : 1.83291
[1mStep[0m  [100/106], [94mLoss[0m : 1.78520

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72592
[1mStep[0m  [10/106], [94mLoss[0m : 1.72000
[1mStep[0m  [20/106], [94mLoss[0m : 1.53161
[1mStep[0m  [30/106], [94mLoss[0m : 1.72468
[1mStep[0m  [40/106], [94mLoss[0m : 1.47076
[1mStep[0m  [50/106], [94mLoss[0m : 1.59888
[1mStep[0m  [60/106], [94mLoss[0m : 1.77955
[1mStep[0m  [70/106], [94mLoss[0m : 1.57962
[1mStep[0m  [80/106], [94mLoss[0m : 1.44754
[1mStep[0m  [90/106], [94mLoss[0m : 1.65297
[1mStep[0m  [100/106], [94mLoss[0m : 1.82367

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.605, [92mTest[0m: 2.684, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66225
[1mStep[0m  [10/106], [94mLoss[0m : 1.61500
[1mStep[0m  [20/106], [94mLoss[0m : 1.61809
[1mStep[0m  [30/106], [94mLoss[0m : 1.59199
[1mStep[0m  [40/106], [94mLoss[0m : 1.66005
[1mStep[0m  [50/106], [94mLoss[0m : 1.70402
[1mStep[0m  [60/106], [94mLoss[0m : 1.66991
[1mStep[0m  [70/106], [94mLoss[0m : 1.72903
[1mStep[0m  [80/106], [94mLoss[0m : 1.70288
[1mStep[0m  [90/106], [94mLoss[0m : 1.40209
[1mStep[0m  [100/106], [94mLoss[0m : 1.71431

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.470, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.502
====================================

Phase 2 - Evaluation MAE:  2.502355256170597
MAE score P1       2.400609
MAE score P2       2.502355
loss               1.600209
learning_rate       0.00505
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.77705
[1mStep[0m  [5/53], [94mLoss[0m : 7.38409
[1mStep[0m  [10/53], [94mLoss[0m : 3.98785
[1mStep[0m  [15/53], [94mLoss[0m : 3.08990
[1mStep[0m  [20/53], [94mLoss[0m : 2.75763
[1mStep[0m  [25/53], [94mLoss[0m : 2.66206
[1mStep[0m  [30/53], [94mLoss[0m : 2.64859
[1mStep[0m  [35/53], [94mLoss[0m : 2.75230
[1mStep[0m  [40/53], [94mLoss[0m : 2.71259
[1mStep[0m  [45/53], [94mLoss[0m : 2.31104
[1mStep[0m  [50/53], [94mLoss[0m : 2.49674

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.764, [92mTest[0m: 10.738, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65804
[1mStep[0m  [5/53], [94mLoss[0m : 2.31799
[1mStep[0m  [10/53], [94mLoss[0m : 2.70496
[1mStep[0m  [15/53], [94mLoss[0m : 2.45828
[1mStep[0m  [20/53], [94mLoss[0m : 2.43970
[1mStep[0m  [25/53], [94mLoss[0m : 2.61037
[1mStep[0m  [30/53], [94mLoss[0m : 2.42225
[1mStep[0m  [35/53], [94mLoss[0m : 2.52135
[1mStep[0m  [40/53], [94mLoss[0m : 2.56840
[1mStep[0m  [45/53], [94mLoss[0m : 2.27843
[1mStep[0m  [50/53], [94mLoss[0m : 2.54019

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41754
[1mStep[0m  [5/53], [94mLoss[0m : 2.59092
[1mStep[0m  [10/53], [94mLoss[0m : 2.61545
[1mStep[0m  [15/53], [94mLoss[0m : 2.59070
[1mStep[0m  [20/53], [94mLoss[0m : 2.52247
[1mStep[0m  [25/53], [94mLoss[0m : 2.60087
[1mStep[0m  [30/53], [94mLoss[0m : 2.66450
[1mStep[0m  [35/53], [94mLoss[0m : 2.33182
[1mStep[0m  [40/53], [94mLoss[0m : 2.58579
[1mStep[0m  [45/53], [94mLoss[0m : 2.66513
[1mStep[0m  [50/53], [94mLoss[0m : 2.60894

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57050
[1mStep[0m  [5/53], [94mLoss[0m : 2.51879
[1mStep[0m  [10/53], [94mLoss[0m : 2.49197
[1mStep[0m  [15/53], [94mLoss[0m : 2.38595
[1mStep[0m  [20/53], [94mLoss[0m : 2.50946
[1mStep[0m  [25/53], [94mLoss[0m : 2.41190
[1mStep[0m  [30/53], [94mLoss[0m : 2.39485
[1mStep[0m  [35/53], [94mLoss[0m : 2.37537
[1mStep[0m  [40/53], [94mLoss[0m : 2.38014
[1mStep[0m  [45/53], [94mLoss[0m : 2.30033
[1mStep[0m  [50/53], [94mLoss[0m : 2.52315

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36279
[1mStep[0m  [5/53], [94mLoss[0m : 2.39828
[1mStep[0m  [10/53], [94mLoss[0m : 2.59892
[1mStep[0m  [15/53], [94mLoss[0m : 2.48204
[1mStep[0m  [20/53], [94mLoss[0m : 2.25594
[1mStep[0m  [25/53], [94mLoss[0m : 2.22071
[1mStep[0m  [30/53], [94mLoss[0m : 2.54838
[1mStep[0m  [35/53], [94mLoss[0m : 2.61395
[1mStep[0m  [40/53], [94mLoss[0m : 2.36521
[1mStep[0m  [45/53], [94mLoss[0m : 2.46844
[1mStep[0m  [50/53], [94mLoss[0m : 2.33559

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57246
[1mStep[0m  [5/53], [94mLoss[0m : 2.51181
[1mStep[0m  [10/53], [94mLoss[0m : 2.28509
[1mStep[0m  [15/53], [94mLoss[0m : 2.40604
[1mStep[0m  [20/53], [94mLoss[0m : 2.52628
[1mStep[0m  [25/53], [94mLoss[0m : 2.30654
[1mStep[0m  [30/53], [94mLoss[0m : 2.54132
[1mStep[0m  [35/53], [94mLoss[0m : 2.47234
[1mStep[0m  [40/53], [94mLoss[0m : 2.44981
[1mStep[0m  [45/53], [94mLoss[0m : 2.38109
[1mStep[0m  [50/53], [94mLoss[0m : 2.59792

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.454, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36267
[1mStep[0m  [5/53], [94mLoss[0m : 2.32973
[1mStep[0m  [10/53], [94mLoss[0m : 2.30299
[1mStep[0m  [15/53], [94mLoss[0m : 2.39654
[1mStep[0m  [20/53], [94mLoss[0m : 2.49113
[1mStep[0m  [25/53], [94mLoss[0m : 2.44673
[1mStep[0m  [30/53], [94mLoss[0m : 2.40030
[1mStep[0m  [35/53], [94mLoss[0m : 2.38329
[1mStep[0m  [40/53], [94mLoss[0m : 2.61264
[1mStep[0m  [45/53], [94mLoss[0m : 2.50912
[1mStep[0m  [50/53], [94mLoss[0m : 2.56388

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09739
[1mStep[0m  [5/53], [94mLoss[0m : 2.54862
[1mStep[0m  [10/53], [94mLoss[0m : 2.28760
[1mStep[0m  [15/53], [94mLoss[0m : 2.57903
[1mStep[0m  [20/53], [94mLoss[0m : 2.36814
[1mStep[0m  [25/53], [94mLoss[0m : 2.46584
[1mStep[0m  [30/53], [94mLoss[0m : 2.54078
[1mStep[0m  [35/53], [94mLoss[0m : 2.50908
[1mStep[0m  [40/53], [94mLoss[0m : 2.39079
[1mStep[0m  [45/53], [94mLoss[0m : 2.49994
[1mStep[0m  [50/53], [94mLoss[0m : 2.50014

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42055
[1mStep[0m  [5/53], [94mLoss[0m : 2.48852
[1mStep[0m  [10/53], [94mLoss[0m : 2.36867
[1mStep[0m  [15/53], [94mLoss[0m : 2.42255
[1mStep[0m  [20/53], [94mLoss[0m : 2.42712
[1mStep[0m  [25/53], [94mLoss[0m : 2.41681
[1mStep[0m  [30/53], [94mLoss[0m : 2.44934
[1mStep[0m  [35/53], [94mLoss[0m : 2.51366
[1mStep[0m  [40/53], [94mLoss[0m : 2.43918
[1mStep[0m  [45/53], [94mLoss[0m : 2.34947
[1mStep[0m  [50/53], [94mLoss[0m : 2.61952

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50067
[1mStep[0m  [5/53], [94mLoss[0m : 2.31825
[1mStep[0m  [10/53], [94mLoss[0m : 2.49063
[1mStep[0m  [15/53], [94mLoss[0m : 2.50664
[1mStep[0m  [20/53], [94mLoss[0m : 2.33573
[1mStep[0m  [25/53], [94mLoss[0m : 2.54618
[1mStep[0m  [30/53], [94mLoss[0m : 2.21031
[1mStep[0m  [35/53], [94mLoss[0m : 2.58830
[1mStep[0m  [40/53], [94mLoss[0m : 2.39731
[1mStep[0m  [45/53], [94mLoss[0m : 2.51383
[1mStep[0m  [50/53], [94mLoss[0m : 2.41850

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18215
[1mStep[0m  [5/53], [94mLoss[0m : 2.36660
[1mStep[0m  [10/53], [94mLoss[0m : 2.53398
[1mStep[0m  [15/53], [94mLoss[0m : 2.52654
[1mStep[0m  [20/53], [94mLoss[0m : 2.41405
[1mStep[0m  [25/53], [94mLoss[0m : 2.46100
[1mStep[0m  [30/53], [94mLoss[0m : 2.28978
[1mStep[0m  [35/53], [94mLoss[0m : 2.35188
[1mStep[0m  [40/53], [94mLoss[0m : 2.54441
[1mStep[0m  [45/53], [94mLoss[0m : 2.42168
[1mStep[0m  [50/53], [94mLoss[0m : 2.54045

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40691
[1mStep[0m  [5/53], [94mLoss[0m : 2.46088
[1mStep[0m  [10/53], [94mLoss[0m : 2.28800
[1mStep[0m  [15/53], [94mLoss[0m : 2.49484
[1mStep[0m  [20/53], [94mLoss[0m : 2.38511
[1mStep[0m  [25/53], [94mLoss[0m : 2.56487
[1mStep[0m  [30/53], [94mLoss[0m : 2.47095
[1mStep[0m  [35/53], [94mLoss[0m : 2.37570
[1mStep[0m  [40/53], [94mLoss[0m : 2.41992
[1mStep[0m  [45/53], [94mLoss[0m : 2.37112
[1mStep[0m  [50/53], [94mLoss[0m : 2.37974

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57509
[1mStep[0m  [5/53], [94mLoss[0m : 2.35261
[1mStep[0m  [10/53], [94mLoss[0m : 2.56723
[1mStep[0m  [15/53], [94mLoss[0m : 2.43533
[1mStep[0m  [20/53], [94mLoss[0m : 2.38077
[1mStep[0m  [25/53], [94mLoss[0m : 2.52132
[1mStep[0m  [30/53], [94mLoss[0m : 2.57694
[1mStep[0m  [35/53], [94mLoss[0m : 2.18449
[1mStep[0m  [40/53], [94mLoss[0m : 2.55889
[1mStep[0m  [45/53], [94mLoss[0m : 2.72685
[1mStep[0m  [50/53], [94mLoss[0m : 2.36375

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33744
[1mStep[0m  [5/53], [94mLoss[0m : 2.53767
[1mStep[0m  [10/53], [94mLoss[0m : 2.53476
[1mStep[0m  [15/53], [94mLoss[0m : 2.34725
[1mStep[0m  [20/53], [94mLoss[0m : 2.41465
[1mStep[0m  [25/53], [94mLoss[0m : 2.56264
[1mStep[0m  [30/53], [94mLoss[0m : 2.44206
[1mStep[0m  [35/53], [94mLoss[0m : 2.59635
[1mStep[0m  [40/53], [94mLoss[0m : 2.52345
[1mStep[0m  [45/53], [94mLoss[0m : 2.26451
[1mStep[0m  [50/53], [94mLoss[0m : 2.32084

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27593
[1mStep[0m  [5/53], [94mLoss[0m : 2.64701
[1mStep[0m  [10/53], [94mLoss[0m : 2.31011
[1mStep[0m  [15/53], [94mLoss[0m : 2.69752
[1mStep[0m  [20/53], [94mLoss[0m : 2.39816
[1mStep[0m  [25/53], [94mLoss[0m : 2.59558
[1mStep[0m  [30/53], [94mLoss[0m : 2.45312
[1mStep[0m  [35/53], [94mLoss[0m : 2.38681
[1mStep[0m  [40/53], [94mLoss[0m : 2.55487
[1mStep[0m  [45/53], [94mLoss[0m : 2.39749
[1mStep[0m  [50/53], [94mLoss[0m : 2.54421

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53007
[1mStep[0m  [5/53], [94mLoss[0m : 2.33802
[1mStep[0m  [10/53], [94mLoss[0m : 2.43932
[1mStep[0m  [15/53], [94mLoss[0m : 2.46857
[1mStep[0m  [20/53], [94mLoss[0m : 2.40537
[1mStep[0m  [25/53], [94mLoss[0m : 2.45207
[1mStep[0m  [30/53], [94mLoss[0m : 2.28356
[1mStep[0m  [35/53], [94mLoss[0m : 2.48154
[1mStep[0m  [40/53], [94mLoss[0m : 2.55314
[1mStep[0m  [45/53], [94mLoss[0m : 2.31577
[1mStep[0m  [50/53], [94mLoss[0m : 2.64057

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.424, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61252
[1mStep[0m  [5/53], [94mLoss[0m : 2.51951
[1mStep[0m  [10/53], [94mLoss[0m : 2.36206
[1mStep[0m  [15/53], [94mLoss[0m : 2.27495
[1mStep[0m  [20/53], [94mLoss[0m : 2.22533
[1mStep[0m  [25/53], [94mLoss[0m : 2.65733
[1mStep[0m  [30/53], [94mLoss[0m : 2.60777
[1mStep[0m  [35/53], [94mLoss[0m : 2.37498
[1mStep[0m  [40/53], [94mLoss[0m : 2.48789
[1mStep[0m  [45/53], [94mLoss[0m : 2.29677
[1mStep[0m  [50/53], [94mLoss[0m : 2.56304

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40109
[1mStep[0m  [5/53], [94mLoss[0m : 2.43588
[1mStep[0m  [10/53], [94mLoss[0m : 2.47853
[1mStep[0m  [15/53], [94mLoss[0m : 2.39925
[1mStep[0m  [20/53], [94mLoss[0m : 2.66983
[1mStep[0m  [25/53], [94mLoss[0m : 2.42329
[1mStep[0m  [30/53], [94mLoss[0m : 2.53140
[1mStep[0m  [35/53], [94mLoss[0m : 2.51029
[1mStep[0m  [40/53], [94mLoss[0m : 2.32083
[1mStep[0m  [45/53], [94mLoss[0m : 2.34211
[1mStep[0m  [50/53], [94mLoss[0m : 2.20576

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.431, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.22752
[1mStep[0m  [5/53], [94mLoss[0m : 2.44361
[1mStep[0m  [10/53], [94mLoss[0m : 2.43852
[1mStep[0m  [15/53], [94mLoss[0m : 2.42620
[1mStep[0m  [20/53], [94mLoss[0m : 2.24477
[1mStep[0m  [25/53], [94mLoss[0m : 2.48343
[1mStep[0m  [30/53], [94mLoss[0m : 2.46339
[1mStep[0m  [35/53], [94mLoss[0m : 2.45570
[1mStep[0m  [40/53], [94mLoss[0m : 2.47206
[1mStep[0m  [45/53], [94mLoss[0m : 2.22478
[1mStep[0m  [50/53], [94mLoss[0m : 2.52370

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45055
[1mStep[0m  [5/53], [94mLoss[0m : 2.66409
[1mStep[0m  [10/53], [94mLoss[0m : 2.41866
[1mStep[0m  [15/53], [94mLoss[0m : 2.55842
[1mStep[0m  [20/53], [94mLoss[0m : 2.51674
[1mStep[0m  [25/53], [94mLoss[0m : 2.49919
[1mStep[0m  [30/53], [94mLoss[0m : 2.47411
[1mStep[0m  [35/53], [94mLoss[0m : 2.47160
[1mStep[0m  [40/53], [94mLoss[0m : 2.28556
[1mStep[0m  [45/53], [94mLoss[0m : 2.47287
[1mStep[0m  [50/53], [94mLoss[0m : 2.40295

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.415, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56514
[1mStep[0m  [5/53], [94mLoss[0m : 2.45411
[1mStep[0m  [10/53], [94mLoss[0m : 2.44441
[1mStep[0m  [15/53], [94mLoss[0m : 2.27450
[1mStep[0m  [20/53], [94mLoss[0m : 2.61785
[1mStep[0m  [25/53], [94mLoss[0m : 2.62943
[1mStep[0m  [30/53], [94mLoss[0m : 2.35132
[1mStep[0m  [35/53], [94mLoss[0m : 2.38449
[1mStep[0m  [40/53], [94mLoss[0m : 2.27882
[1mStep[0m  [45/53], [94mLoss[0m : 2.48817
[1mStep[0m  [50/53], [94mLoss[0m : 2.25406

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.414, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32004
[1mStep[0m  [5/53], [94mLoss[0m : 2.22635
[1mStep[0m  [10/53], [94mLoss[0m : 2.27740
[1mStep[0m  [15/53], [94mLoss[0m : 2.42878
[1mStep[0m  [20/53], [94mLoss[0m : 2.52072
[1mStep[0m  [25/53], [94mLoss[0m : 2.38769
[1mStep[0m  [30/53], [94mLoss[0m : 2.29131
[1mStep[0m  [35/53], [94mLoss[0m : 2.31570
[1mStep[0m  [40/53], [94mLoss[0m : 2.36051
[1mStep[0m  [45/53], [94mLoss[0m : 2.56230
[1mStep[0m  [50/53], [94mLoss[0m : 2.36362

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.410, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.34410
[1mStep[0m  [5/53], [94mLoss[0m : 2.40609
[1mStep[0m  [10/53], [94mLoss[0m : 2.42501
[1mStep[0m  [15/53], [94mLoss[0m : 2.40345
[1mStep[0m  [20/53], [94mLoss[0m : 2.32719
[1mStep[0m  [25/53], [94mLoss[0m : 2.29701
[1mStep[0m  [30/53], [94mLoss[0m : 2.15565
[1mStep[0m  [35/53], [94mLoss[0m : 2.43245
[1mStep[0m  [40/53], [94mLoss[0m : 2.33086
[1mStep[0m  [45/53], [94mLoss[0m : 2.35085
[1mStep[0m  [50/53], [94mLoss[0m : 2.36711

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.411, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31154
[1mStep[0m  [5/53], [94mLoss[0m : 2.52450
[1mStep[0m  [10/53], [94mLoss[0m : 2.33205
[1mStep[0m  [15/53], [94mLoss[0m : 2.29141
[1mStep[0m  [20/53], [94mLoss[0m : 2.40581
[1mStep[0m  [25/53], [94mLoss[0m : 2.53456
[1mStep[0m  [30/53], [94mLoss[0m : 2.53408
[1mStep[0m  [35/53], [94mLoss[0m : 2.60292
[1mStep[0m  [40/53], [94mLoss[0m : 2.42276
[1mStep[0m  [45/53], [94mLoss[0m : 2.36814
[1mStep[0m  [50/53], [94mLoss[0m : 2.49231

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.431, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.33830
[1mStep[0m  [5/53], [94mLoss[0m : 2.56389
[1mStep[0m  [10/53], [94mLoss[0m : 2.49094
[1mStep[0m  [15/53], [94mLoss[0m : 2.34232
[1mStep[0m  [20/53], [94mLoss[0m : 2.32735
[1mStep[0m  [25/53], [94mLoss[0m : 2.51104
[1mStep[0m  [30/53], [94mLoss[0m : 2.49039
[1mStep[0m  [35/53], [94mLoss[0m : 2.16979
[1mStep[0m  [40/53], [94mLoss[0m : 2.38102
[1mStep[0m  [45/53], [94mLoss[0m : 2.57594
[1mStep[0m  [50/53], [94mLoss[0m : 2.55672

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48592
[1mStep[0m  [5/53], [94mLoss[0m : 2.30143
[1mStep[0m  [10/53], [94mLoss[0m : 2.37241
[1mStep[0m  [15/53], [94mLoss[0m : 2.41215
[1mStep[0m  [20/53], [94mLoss[0m : 2.71714
[1mStep[0m  [25/53], [94mLoss[0m : 2.38989
[1mStep[0m  [30/53], [94mLoss[0m : 2.54752
[1mStep[0m  [35/53], [94mLoss[0m : 2.44056
[1mStep[0m  [40/53], [94mLoss[0m : 2.51792
[1mStep[0m  [45/53], [94mLoss[0m : 2.39203
[1mStep[0m  [50/53], [94mLoss[0m : 2.21186

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42341
[1mStep[0m  [5/53], [94mLoss[0m : 2.40243
[1mStep[0m  [10/53], [94mLoss[0m : 2.56189
[1mStep[0m  [15/53], [94mLoss[0m : 2.31503
[1mStep[0m  [20/53], [94mLoss[0m : 2.40918
[1mStep[0m  [25/53], [94mLoss[0m : 2.35507
[1mStep[0m  [30/53], [94mLoss[0m : 2.48130
[1mStep[0m  [35/53], [94mLoss[0m : 2.38070
[1mStep[0m  [40/53], [94mLoss[0m : 2.58854
[1mStep[0m  [45/53], [94mLoss[0m : 2.22884
[1mStep[0m  [50/53], [94mLoss[0m : 2.51554

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58385
[1mStep[0m  [5/53], [94mLoss[0m : 2.34930
[1mStep[0m  [10/53], [94mLoss[0m : 2.38706
[1mStep[0m  [15/53], [94mLoss[0m : 2.39572
[1mStep[0m  [20/53], [94mLoss[0m : 2.50039
[1mStep[0m  [25/53], [94mLoss[0m : 2.51462
[1mStep[0m  [30/53], [94mLoss[0m : 2.34283
[1mStep[0m  [35/53], [94mLoss[0m : 2.32081
[1mStep[0m  [40/53], [94mLoss[0m : 2.43555
[1mStep[0m  [45/53], [94mLoss[0m : 2.41722
[1mStep[0m  [50/53], [94mLoss[0m : 2.37913

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.407, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40495
[1mStep[0m  [5/53], [94mLoss[0m : 2.25062
[1mStep[0m  [10/53], [94mLoss[0m : 2.61883
[1mStep[0m  [15/53], [94mLoss[0m : 2.42420
[1mStep[0m  [20/53], [94mLoss[0m : 2.33156
[1mStep[0m  [25/53], [94mLoss[0m : 2.45348
[1mStep[0m  [30/53], [94mLoss[0m : 2.38246
[1mStep[0m  [35/53], [94mLoss[0m : 2.42368
[1mStep[0m  [40/53], [94mLoss[0m : 2.36368
[1mStep[0m  [45/53], [94mLoss[0m : 2.21891
[1mStep[0m  [50/53], [94mLoss[0m : 2.60968

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37519
[1mStep[0m  [5/53], [94mLoss[0m : 2.31825
[1mStep[0m  [10/53], [94mLoss[0m : 2.30127
[1mStep[0m  [15/53], [94mLoss[0m : 2.58634
[1mStep[0m  [20/53], [94mLoss[0m : 2.69944
[1mStep[0m  [25/53], [94mLoss[0m : 2.38489
[1mStep[0m  [30/53], [94mLoss[0m : 2.48158
[1mStep[0m  [35/53], [94mLoss[0m : 2.56018
[1mStep[0m  [40/53], [94mLoss[0m : 2.52399
[1mStep[0m  [45/53], [94mLoss[0m : 2.38191
[1mStep[0m  [50/53], [94mLoss[0m : 2.47101

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.415
====================================

Phase 1 - Evaluation MAE:  2.4151849196507382
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.41553
[1mStep[0m  [5/53], [94mLoss[0m : 2.58927
[1mStep[0m  [10/53], [94mLoss[0m : 2.41612
[1mStep[0m  [15/53], [94mLoss[0m : 2.29748
[1mStep[0m  [20/53], [94mLoss[0m : 2.34963
[1mStep[0m  [25/53], [94mLoss[0m : 2.46244
[1mStep[0m  [30/53], [94mLoss[0m : 2.25249
[1mStep[0m  [35/53], [94mLoss[0m : 2.44864
[1mStep[0m  [40/53], [94mLoss[0m : 2.51925
[1mStep[0m  [45/53], [94mLoss[0m : 2.19787
[1mStep[0m  [50/53], [94mLoss[0m : 2.17831

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.50218
[1mStep[0m  [5/53], [94mLoss[0m : 2.27580
[1mStep[0m  [10/53], [94mLoss[0m : 2.30978
[1mStep[0m  [15/53], [94mLoss[0m : 2.26109
[1mStep[0m  [20/53], [94mLoss[0m : 2.31030
[1mStep[0m  [25/53], [94mLoss[0m : 2.47183
[1mStep[0m  [30/53], [94mLoss[0m : 2.39613
[1mStep[0m  [35/53], [94mLoss[0m : 2.19083
[1mStep[0m  [40/53], [94mLoss[0m : 2.34242
[1mStep[0m  [45/53], [94mLoss[0m : 2.37438
[1mStep[0m  [50/53], [94mLoss[0m : 2.40595

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.407, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18918
[1mStep[0m  [5/53], [94mLoss[0m : 2.34623
[1mStep[0m  [10/53], [94mLoss[0m : 2.50686
[1mStep[0m  [15/53], [94mLoss[0m : 2.30730
[1mStep[0m  [20/53], [94mLoss[0m : 2.26594
[1mStep[0m  [25/53], [94mLoss[0m : 2.34403
[1mStep[0m  [30/53], [94mLoss[0m : 2.43750
[1mStep[0m  [35/53], [94mLoss[0m : 2.53837
[1mStep[0m  [40/53], [94mLoss[0m : 2.33919
[1mStep[0m  [45/53], [94mLoss[0m : 2.21036
[1mStep[0m  [50/53], [94mLoss[0m : 2.32171

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.580, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41016
[1mStep[0m  [5/53], [94mLoss[0m : 2.33990
[1mStep[0m  [10/53], [94mLoss[0m : 2.38544
[1mStep[0m  [15/53], [94mLoss[0m : 2.32274
[1mStep[0m  [20/53], [94mLoss[0m : 2.34644
[1mStep[0m  [25/53], [94mLoss[0m : 2.20263
[1mStep[0m  [30/53], [94mLoss[0m : 2.25233
[1mStep[0m  [35/53], [94mLoss[0m : 2.21902
[1mStep[0m  [40/53], [94mLoss[0m : 2.39062
[1mStep[0m  [45/53], [94mLoss[0m : 2.30497
[1mStep[0m  [50/53], [94mLoss[0m : 2.29504

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.570, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.32033
[1mStep[0m  [5/53], [94mLoss[0m : 2.24950
[1mStep[0m  [10/53], [94mLoss[0m : 2.41637
[1mStep[0m  [15/53], [94mLoss[0m : 2.37259
[1mStep[0m  [20/53], [94mLoss[0m : 2.25086
[1mStep[0m  [25/53], [94mLoss[0m : 2.10929
[1mStep[0m  [30/53], [94mLoss[0m : 2.24278
[1mStep[0m  [35/53], [94mLoss[0m : 2.33169
[1mStep[0m  [40/53], [94mLoss[0m : 2.34405
[1mStep[0m  [45/53], [94mLoss[0m : 2.25071
[1mStep[0m  [50/53], [94mLoss[0m : 2.28101

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.620, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41034
[1mStep[0m  [5/53], [94mLoss[0m : 2.07172
[1mStep[0m  [10/53], [94mLoss[0m : 2.34061
[1mStep[0m  [15/53], [94mLoss[0m : 2.32468
[1mStep[0m  [20/53], [94mLoss[0m : 2.15720
[1mStep[0m  [25/53], [94mLoss[0m : 2.41045
[1mStep[0m  [30/53], [94mLoss[0m : 2.09558
[1mStep[0m  [35/53], [94mLoss[0m : 2.31721
[1mStep[0m  [40/53], [94mLoss[0m : 2.15383
[1mStep[0m  [45/53], [94mLoss[0m : 2.21613
[1mStep[0m  [50/53], [94mLoss[0m : 2.41855

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.640, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.09444
[1mStep[0m  [5/53], [94mLoss[0m : 2.11329
[1mStep[0m  [10/53], [94mLoss[0m : 2.17443
[1mStep[0m  [15/53], [94mLoss[0m : 2.26099
[1mStep[0m  [20/53], [94mLoss[0m : 2.32043
[1mStep[0m  [25/53], [94mLoss[0m : 2.24323
[1mStep[0m  [30/53], [94mLoss[0m : 2.25332
[1mStep[0m  [35/53], [94mLoss[0m : 2.39109
[1mStep[0m  [40/53], [94mLoss[0m : 2.35482
[1mStep[0m  [45/53], [94mLoss[0m : 2.15956
[1mStep[0m  [50/53], [94mLoss[0m : 2.11442

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.251, [92mTest[0m: 2.623, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43721
[1mStep[0m  [5/53], [94mLoss[0m : 2.14078
[1mStep[0m  [10/53], [94mLoss[0m : 2.16780
[1mStep[0m  [15/53], [94mLoss[0m : 2.25122
[1mStep[0m  [20/53], [94mLoss[0m : 2.12329
[1mStep[0m  [25/53], [94mLoss[0m : 2.14153
[1mStep[0m  [30/53], [94mLoss[0m : 2.24279
[1mStep[0m  [35/53], [94mLoss[0m : 2.21569
[1mStep[0m  [40/53], [94mLoss[0m : 2.09856
[1mStep[0m  [45/53], [94mLoss[0m : 2.19528
[1mStep[0m  [50/53], [94mLoss[0m : 2.20071

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.603, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.30523
[1mStep[0m  [5/53], [94mLoss[0m : 2.04926
[1mStep[0m  [10/53], [94mLoss[0m : 2.14658
[1mStep[0m  [15/53], [94mLoss[0m : 2.02885
[1mStep[0m  [20/53], [94mLoss[0m : 2.12008
[1mStep[0m  [25/53], [94mLoss[0m : 2.16101
[1mStep[0m  [30/53], [94mLoss[0m : 2.22037
[1mStep[0m  [35/53], [94mLoss[0m : 2.22570
[1mStep[0m  [40/53], [94mLoss[0m : 2.29609
[1mStep[0m  [45/53], [94mLoss[0m : 2.10119
[1mStep[0m  [50/53], [94mLoss[0m : 2.26572

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.588, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.18108
[1mStep[0m  [5/53], [94mLoss[0m : 2.15863
[1mStep[0m  [10/53], [94mLoss[0m : 2.23959
[1mStep[0m  [15/53], [94mLoss[0m : 2.31951
[1mStep[0m  [20/53], [94mLoss[0m : 2.07781
[1mStep[0m  [25/53], [94mLoss[0m : 2.03846
[1mStep[0m  [30/53], [94mLoss[0m : 2.16545
[1mStep[0m  [35/53], [94mLoss[0m : 2.10880
[1mStep[0m  [40/53], [94mLoss[0m : 2.03039
[1mStep[0m  [45/53], [94mLoss[0m : 2.34311
[1mStep[0m  [50/53], [94mLoss[0m : 1.88995

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.591, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.04593
[1mStep[0m  [5/53], [94mLoss[0m : 1.87776
[1mStep[0m  [10/53], [94mLoss[0m : 2.04653
[1mStep[0m  [15/53], [94mLoss[0m : 2.07432
[1mStep[0m  [20/53], [94mLoss[0m : 2.14952
[1mStep[0m  [25/53], [94mLoss[0m : 2.01052
[1mStep[0m  [30/53], [94mLoss[0m : 2.00351
[1mStep[0m  [35/53], [94mLoss[0m : 2.42511
[1mStep[0m  [40/53], [94mLoss[0m : 2.06602
[1mStep[0m  [45/53], [94mLoss[0m : 2.06662
[1mStep[0m  [50/53], [94mLoss[0m : 2.15246

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.554, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.12258
[1mStep[0m  [5/53], [94mLoss[0m : 2.13562
[1mStep[0m  [10/53], [94mLoss[0m : 1.98004
[1mStep[0m  [15/53], [94mLoss[0m : 1.98014
[1mStep[0m  [20/53], [94mLoss[0m : 1.85805
[1mStep[0m  [25/53], [94mLoss[0m : 1.95553
[1mStep[0m  [30/53], [94mLoss[0m : 2.14919
[1mStep[0m  [35/53], [94mLoss[0m : 2.14170
[1mStep[0m  [40/53], [94mLoss[0m : 2.11243
[1mStep[0m  [45/53], [94mLoss[0m : 2.07138
[1mStep[0m  [50/53], [94mLoss[0m : 1.88315

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.531, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.89000
[1mStep[0m  [5/53], [94mLoss[0m : 2.07156
[1mStep[0m  [10/53], [94mLoss[0m : 2.05267
[1mStep[0m  [15/53], [94mLoss[0m : 2.04916
[1mStep[0m  [20/53], [94mLoss[0m : 2.03529
[1mStep[0m  [25/53], [94mLoss[0m : 1.92352
[1mStep[0m  [30/53], [94mLoss[0m : 2.04901
[1mStep[0m  [35/53], [94mLoss[0m : 2.06525
[1mStep[0m  [40/53], [94mLoss[0m : 2.05536
[1mStep[0m  [45/53], [94mLoss[0m : 1.85389
[1mStep[0m  [50/53], [94mLoss[0m : 2.16307

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.06241
[1mStep[0m  [5/53], [94mLoss[0m : 1.98414
[1mStep[0m  [10/53], [94mLoss[0m : 2.10638
[1mStep[0m  [15/53], [94mLoss[0m : 1.94035
[1mStep[0m  [20/53], [94mLoss[0m : 2.04769
[1mStep[0m  [25/53], [94mLoss[0m : 2.10775
[1mStep[0m  [30/53], [94mLoss[0m : 1.90516
[1mStep[0m  [35/53], [94mLoss[0m : 2.06269
[1mStep[0m  [40/53], [94mLoss[0m : 2.00978
[1mStep[0m  [45/53], [94mLoss[0m : 1.91726
[1mStep[0m  [50/53], [94mLoss[0m : 1.91448

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.93041
[1mStep[0m  [5/53], [94mLoss[0m : 1.98999
[1mStep[0m  [10/53], [94mLoss[0m : 1.84673
[1mStep[0m  [15/53], [94mLoss[0m : 2.02900
[1mStep[0m  [20/53], [94mLoss[0m : 1.83833
[1mStep[0m  [25/53], [94mLoss[0m : 1.74177
[1mStep[0m  [30/53], [94mLoss[0m : 1.91888
[1mStep[0m  [35/53], [94mLoss[0m : 2.12890
[1mStep[0m  [40/53], [94mLoss[0m : 1.97224
[1mStep[0m  [45/53], [94mLoss[0m : 2.08274
[1mStep[0m  [50/53], [94mLoss[0m : 1.82364

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.78441
[1mStep[0m  [5/53], [94mLoss[0m : 1.93996
[1mStep[0m  [10/53], [94mLoss[0m : 1.90237
[1mStep[0m  [15/53], [94mLoss[0m : 2.00242
[1mStep[0m  [20/53], [94mLoss[0m : 1.82833
[1mStep[0m  [25/53], [94mLoss[0m : 1.88714
[1mStep[0m  [30/53], [94mLoss[0m : 1.92141
[1mStep[0m  [35/53], [94mLoss[0m : 1.74893
[1mStep[0m  [40/53], [94mLoss[0m : 1.74599
[1mStep[0m  [45/53], [94mLoss[0m : 2.03502
[1mStep[0m  [50/53], [94mLoss[0m : 1.87151

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.83501
[1mStep[0m  [5/53], [94mLoss[0m : 1.81803
[1mStep[0m  [10/53], [94mLoss[0m : 1.80968
[1mStep[0m  [15/53], [94mLoss[0m : 1.98055
[1mStep[0m  [20/53], [94mLoss[0m : 1.88436
[1mStep[0m  [25/53], [94mLoss[0m : 1.74475
[1mStep[0m  [30/53], [94mLoss[0m : 1.76659
[1mStep[0m  [35/53], [94mLoss[0m : 2.08138
[1mStep[0m  [40/53], [94mLoss[0m : 1.79557
[1mStep[0m  [45/53], [94mLoss[0m : 2.02015
[1mStep[0m  [50/53], [94mLoss[0m : 1.86954

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.427, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.73800
[1mStep[0m  [5/53], [94mLoss[0m : 1.71800
[1mStep[0m  [10/53], [94mLoss[0m : 1.69052
[1mStep[0m  [15/53], [94mLoss[0m : 1.97612
[1mStep[0m  [20/53], [94mLoss[0m : 1.74270
[1mStep[0m  [25/53], [94mLoss[0m : 1.96229
[1mStep[0m  [30/53], [94mLoss[0m : 1.95131
[1mStep[0m  [35/53], [94mLoss[0m : 1.87395
[1mStep[0m  [40/53], [94mLoss[0m : 1.84927
[1mStep[0m  [45/53], [94mLoss[0m : 1.93142
[1mStep[0m  [50/53], [94mLoss[0m : 1.91393

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75922
[1mStep[0m  [5/53], [94mLoss[0m : 1.65967
[1mStep[0m  [10/53], [94mLoss[0m : 1.84886
[1mStep[0m  [15/53], [94mLoss[0m : 1.91967
[1mStep[0m  [20/53], [94mLoss[0m : 1.96669
[1mStep[0m  [25/53], [94mLoss[0m : 1.84404
[1mStep[0m  [30/53], [94mLoss[0m : 1.72454
[1mStep[0m  [35/53], [94mLoss[0m : 1.74634
[1mStep[0m  [40/53], [94mLoss[0m : 1.96391
[1mStep[0m  [45/53], [94mLoss[0m : 1.95458
[1mStep[0m  [50/53], [94mLoss[0m : 1.76828

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.67668
[1mStep[0m  [5/53], [94mLoss[0m : 1.59328
[1mStep[0m  [10/53], [94mLoss[0m : 1.64525
[1mStep[0m  [15/53], [94mLoss[0m : 1.71046
[1mStep[0m  [20/53], [94mLoss[0m : 1.64338
[1mStep[0m  [25/53], [94mLoss[0m : 1.75905
[1mStep[0m  [30/53], [94mLoss[0m : 1.75122
[1mStep[0m  [35/53], [94mLoss[0m : 1.84565
[1mStep[0m  [40/53], [94mLoss[0m : 1.66080
[1mStep[0m  [45/53], [94mLoss[0m : 1.77845
[1mStep[0m  [50/53], [94mLoss[0m : 1.70380

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.443, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.65194
[1mStep[0m  [5/53], [94mLoss[0m : 1.66329
[1mStep[0m  [10/53], [94mLoss[0m : 1.62848
[1mStep[0m  [15/53], [94mLoss[0m : 1.71466
[1mStep[0m  [20/53], [94mLoss[0m : 1.61829
[1mStep[0m  [25/53], [94mLoss[0m : 1.62349
[1mStep[0m  [30/53], [94mLoss[0m : 1.48123
[1mStep[0m  [35/53], [94mLoss[0m : 1.72373
[1mStep[0m  [40/53], [94mLoss[0m : 1.74497
[1mStep[0m  [45/53], [94mLoss[0m : 1.69500
[1mStep[0m  [50/53], [94mLoss[0m : 1.90551

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.49748
[1mStep[0m  [5/53], [94mLoss[0m : 1.71134
[1mStep[0m  [10/53], [94mLoss[0m : 1.45361
[1mStep[0m  [15/53], [94mLoss[0m : 1.67358
[1mStep[0m  [20/53], [94mLoss[0m : 1.71006
[1mStep[0m  [25/53], [94mLoss[0m : 1.80077
[1mStep[0m  [30/53], [94mLoss[0m : 1.68714
[1mStep[0m  [35/53], [94mLoss[0m : 1.71259
[1mStep[0m  [40/53], [94mLoss[0m : 1.65324
[1mStep[0m  [45/53], [94mLoss[0m : 1.68503
[1mStep[0m  [50/53], [94mLoss[0m : 1.59437

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.419, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.75184
[1mStep[0m  [5/53], [94mLoss[0m : 1.70329
[1mStep[0m  [10/53], [94mLoss[0m : 1.37894
[1mStep[0m  [15/53], [94mLoss[0m : 1.66233
[1mStep[0m  [20/53], [94mLoss[0m : 1.87313
[1mStep[0m  [25/53], [94mLoss[0m : 1.64097
[1mStep[0m  [30/53], [94mLoss[0m : 1.51374
[1mStep[0m  [35/53], [94mLoss[0m : 1.62704
[1mStep[0m  [40/53], [94mLoss[0m : 1.56435
[1mStep[0m  [45/53], [94mLoss[0m : 1.64233
[1mStep[0m  [50/53], [94mLoss[0m : 1.67639

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.412, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.74007
[1mStep[0m  [5/53], [94mLoss[0m : 1.45119
[1mStep[0m  [10/53], [94mLoss[0m : 1.73549
[1mStep[0m  [15/53], [94mLoss[0m : 1.75321
[1mStep[0m  [20/53], [94mLoss[0m : 1.81998
[1mStep[0m  [25/53], [94mLoss[0m : 1.77662
[1mStep[0m  [30/53], [94mLoss[0m : 1.64865
[1mStep[0m  [35/53], [94mLoss[0m : 1.62675
[1mStep[0m  [40/53], [94mLoss[0m : 1.73133
[1mStep[0m  [45/53], [94mLoss[0m : 1.66518
[1mStep[0m  [50/53], [94mLoss[0m : 1.57834

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.60324
[1mStep[0m  [5/53], [94mLoss[0m : 1.57317
[1mStep[0m  [10/53], [94mLoss[0m : 1.55725
[1mStep[0m  [15/53], [94mLoss[0m : 1.50610
[1mStep[0m  [20/53], [94mLoss[0m : 1.65985
[1mStep[0m  [25/53], [94mLoss[0m : 1.69610
[1mStep[0m  [30/53], [94mLoss[0m : 1.38617
[1mStep[0m  [35/53], [94mLoss[0m : 1.56785
[1mStep[0m  [40/53], [94mLoss[0m : 1.36026
[1mStep[0m  [45/53], [94mLoss[0m : 1.74433
[1mStep[0m  [50/53], [94mLoss[0m : 1.56053

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.435, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.57485
[1mStep[0m  [5/53], [94mLoss[0m : 1.47267
[1mStep[0m  [10/53], [94mLoss[0m : 1.60935
[1mStep[0m  [15/53], [94mLoss[0m : 1.53467
[1mStep[0m  [20/53], [94mLoss[0m : 1.52987
[1mStep[0m  [25/53], [94mLoss[0m : 1.69474
[1mStep[0m  [30/53], [94mLoss[0m : 1.56506
[1mStep[0m  [35/53], [94mLoss[0m : 1.44868
[1mStep[0m  [40/53], [94mLoss[0m : 1.80137
[1mStep[0m  [45/53], [94mLoss[0m : 1.42034
[1mStep[0m  [50/53], [94mLoss[0m : 1.71573

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.457, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.44717
[1mStep[0m  [5/53], [94mLoss[0m : 1.52070
[1mStep[0m  [10/53], [94mLoss[0m : 1.40270
[1mStep[0m  [15/53], [94mLoss[0m : 1.56755
[1mStep[0m  [20/53], [94mLoss[0m : 1.36973
[1mStep[0m  [25/53], [94mLoss[0m : 1.52913
[1mStep[0m  [30/53], [94mLoss[0m : 1.65774
[1mStep[0m  [35/53], [94mLoss[0m : 1.40044
[1mStep[0m  [40/53], [94mLoss[0m : 1.60085
[1mStep[0m  [45/53], [94mLoss[0m : 1.63098
[1mStep[0m  [50/53], [94mLoss[0m : 1.49114

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.451, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.63154
[1mStep[0m  [5/53], [94mLoss[0m : 1.54070
[1mStep[0m  [10/53], [94mLoss[0m : 1.58858
[1mStep[0m  [15/53], [94mLoss[0m : 1.51481
[1mStep[0m  [20/53], [94mLoss[0m : 1.57406
[1mStep[0m  [25/53], [94mLoss[0m : 1.52254
[1mStep[0m  [30/53], [94mLoss[0m : 1.53828
[1mStep[0m  [35/53], [94mLoss[0m : 1.55826
[1mStep[0m  [40/53], [94mLoss[0m : 1.54695
[1mStep[0m  [45/53], [94mLoss[0m : 1.56800
[1mStep[0m  [50/53], [94mLoss[0m : 1.50375

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.29260
[1mStep[0m  [5/53], [94mLoss[0m : 1.41822
[1mStep[0m  [10/53], [94mLoss[0m : 1.32557
[1mStep[0m  [15/53], [94mLoss[0m : 1.45323
[1mStep[0m  [20/53], [94mLoss[0m : 1.41703
[1mStep[0m  [25/53], [94mLoss[0m : 1.62108
[1mStep[0m  [30/53], [94mLoss[0m : 1.57326
[1mStep[0m  [35/53], [94mLoss[0m : 1.44553
[1mStep[0m  [40/53], [94mLoss[0m : 1.61544
[1mStep[0m  [45/53], [94mLoss[0m : 1.44921
[1mStep[0m  [50/53], [94mLoss[0m : 1.68521

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 1.46857
[1mStep[0m  [5/53], [94mLoss[0m : 1.47808
[1mStep[0m  [10/53], [94mLoss[0m : 1.58074
[1mStep[0m  [15/53], [94mLoss[0m : 1.39710
[1mStep[0m  [20/53], [94mLoss[0m : 1.49830
[1mStep[0m  [25/53], [94mLoss[0m : 1.34575
[1mStep[0m  [30/53], [94mLoss[0m : 1.49630
[1mStep[0m  [35/53], [94mLoss[0m : 1.33657
[1mStep[0m  [40/53], [94mLoss[0m : 1.62663
[1mStep[0m  [45/53], [94mLoss[0m : 1.47599
[1mStep[0m  [50/53], [94mLoss[0m : 1.75578

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.449, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.540
====================================

Phase 2 - Evaluation MAE:  2.539873141508836
MAE score P1      2.415185
MAE score P2      2.539873
loss              1.470199
learning_rate      0.00505
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.95424
[1mStep[0m  [5/53], [94mLoss[0m : 10.65779
[1mStep[0m  [10/53], [94mLoss[0m : 11.04135
[1mStep[0m  [15/53], [94mLoss[0m : 10.73554
[1mStep[0m  [20/53], [94mLoss[0m : 10.71543
[1mStep[0m  [25/53], [94mLoss[0m : 10.76576
[1mStep[0m  [30/53], [94mLoss[0m : 10.61005
[1mStep[0m  [35/53], [94mLoss[0m : 10.57864
[1mStep[0m  [40/53], [94mLoss[0m : 10.49709
[1mStep[0m  [45/53], [94mLoss[0m : 10.22326
[1mStep[0m  [50/53], [94mLoss[0m : 10.01096

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.615, [92mTest[0m: 11.078, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.39197
[1mStep[0m  [5/53], [94mLoss[0m : 10.27404
[1mStep[0m  [10/53], [94mLoss[0m : 10.66368
[1mStep[0m  [15/53], [94mLoss[0m : 10.31837
[1mStep[0m  [20/53], [94mLoss[0m : 10.13905
[1mStep[0m  [25/53], [94mLoss[0m : 9.96083
[1mStep[0m  [30/53], [94mLoss[0m : 9.84460
[1mStep[0m  [35/53], [94mLoss[0m : 9.96161
[1mStep[0m  [40/53], [94mLoss[0m : 10.26815
[1mStep[0m  [45/53], [94mLoss[0m : 10.15821
[1mStep[0m  [50/53], [94mLoss[0m : 10.09564

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.128, [92mTest[0m: 10.318, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.44130
[1mStep[0m  [5/53], [94mLoss[0m : 9.44914
[1mStep[0m  [10/53], [94mLoss[0m : 9.53988
[1mStep[0m  [15/53], [94mLoss[0m : 9.72753
[1mStep[0m  [20/53], [94mLoss[0m : 9.20600
[1mStep[0m  [25/53], [94mLoss[0m : 9.52157
[1mStep[0m  [30/53], [94mLoss[0m : 9.82974
[1mStep[0m  [35/53], [94mLoss[0m : 9.47161
[1mStep[0m  [40/53], [94mLoss[0m : 9.54402
[1mStep[0m  [45/53], [94mLoss[0m : 9.54019
[1mStep[0m  [50/53], [94mLoss[0m : 9.55180

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.592, [92mTest[0m: 9.681, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.04398
[1mStep[0m  [5/53], [94mLoss[0m : 9.36992
[1mStep[0m  [10/53], [94mLoss[0m : 8.99331
[1mStep[0m  [15/53], [94mLoss[0m : 9.39661
[1mStep[0m  [20/53], [94mLoss[0m : 9.11944
[1mStep[0m  [25/53], [94mLoss[0m : 8.91376
[1mStep[0m  [30/53], [94mLoss[0m : 8.43612
[1mStep[0m  [35/53], [94mLoss[0m : 8.94390
[1mStep[0m  [40/53], [94mLoss[0m : 8.55566
[1mStep[0m  [45/53], [94mLoss[0m : 8.72958
[1mStep[0m  [50/53], [94mLoss[0m : 8.49036

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.955, [92mTest[0m: 8.990, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.32239
[1mStep[0m  [5/53], [94mLoss[0m : 8.42908
[1mStep[0m  [10/53], [94mLoss[0m : 8.43727
[1mStep[0m  [15/53], [94mLoss[0m : 8.31426
[1mStep[0m  [20/53], [94mLoss[0m : 8.07863
[1mStep[0m  [25/53], [94mLoss[0m : 7.77219
[1mStep[0m  [30/53], [94mLoss[0m : 8.01091
[1mStep[0m  [35/53], [94mLoss[0m : 7.74851
[1mStep[0m  [40/53], [94mLoss[0m : 7.89131
[1mStep[0m  [45/53], [94mLoss[0m : 7.56462
[1mStep[0m  [50/53], [94mLoss[0m : 7.83463

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.183, [92mTest[0m: 8.125, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.81666
[1mStep[0m  [5/53], [94mLoss[0m : 7.88753
[1mStep[0m  [10/53], [94mLoss[0m : 7.89301
[1mStep[0m  [15/53], [94mLoss[0m : 7.43805
[1mStep[0m  [20/53], [94mLoss[0m : 7.71514
[1mStep[0m  [25/53], [94mLoss[0m : 7.40911
[1mStep[0m  [30/53], [94mLoss[0m : 7.36847
[1mStep[0m  [35/53], [94mLoss[0m : 7.11021
[1mStep[0m  [40/53], [94mLoss[0m : 7.16525
[1mStep[0m  [45/53], [94mLoss[0m : 7.11368
[1mStep[0m  [50/53], [94mLoss[0m : 6.95252

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.360, [92mTest[0m: 7.171, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.87312
[1mStep[0m  [5/53], [94mLoss[0m : 7.21811
[1mStep[0m  [10/53], [94mLoss[0m : 7.17726
[1mStep[0m  [15/53], [94mLoss[0m : 7.07909
[1mStep[0m  [20/53], [94mLoss[0m : 6.66193
[1mStep[0m  [25/53], [94mLoss[0m : 6.46421
[1mStep[0m  [30/53], [94mLoss[0m : 6.55556
[1mStep[0m  [35/53], [94mLoss[0m : 6.53253
[1mStep[0m  [40/53], [94mLoss[0m : 6.45310
[1mStep[0m  [45/53], [94mLoss[0m : 6.22507
[1mStep[0m  [50/53], [94mLoss[0m : 6.29811

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.665, [92mTest[0m: 6.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.28878
[1mStep[0m  [5/53], [94mLoss[0m : 6.35988
[1mStep[0m  [10/53], [94mLoss[0m : 6.45067
[1mStep[0m  [15/53], [94mLoss[0m : 5.96229
[1mStep[0m  [20/53], [94mLoss[0m : 6.28023
[1mStep[0m  [25/53], [94mLoss[0m : 6.53984
[1mStep[0m  [30/53], [94mLoss[0m : 5.99817
[1mStep[0m  [35/53], [94mLoss[0m : 6.07638
[1mStep[0m  [40/53], [94mLoss[0m : 5.54831
[1mStep[0m  [45/53], [94mLoss[0m : 6.23948
[1mStep[0m  [50/53], [94mLoss[0m : 5.81396

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.093, [92mTest[0m: 5.630, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.69576
[1mStep[0m  [5/53], [94mLoss[0m : 5.18269
[1mStep[0m  [10/53], [94mLoss[0m : 5.88419
[1mStep[0m  [15/53], [94mLoss[0m : 5.84186
[1mStep[0m  [20/53], [94mLoss[0m : 5.96796
[1mStep[0m  [25/53], [94mLoss[0m : 5.51354
[1mStep[0m  [30/53], [94mLoss[0m : 5.68367
[1mStep[0m  [35/53], [94mLoss[0m : 5.33181
[1mStep[0m  [40/53], [94mLoss[0m : 5.81376
[1mStep[0m  [45/53], [94mLoss[0m : 5.62904
[1mStep[0m  [50/53], [94mLoss[0m : 5.67976

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.590, [92mTest[0m: 4.976, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.37584
[1mStep[0m  [5/53], [94mLoss[0m : 5.39552
[1mStep[0m  [10/53], [94mLoss[0m : 5.01934
[1mStep[0m  [15/53], [94mLoss[0m : 4.99423
[1mStep[0m  [20/53], [94mLoss[0m : 5.20781
[1mStep[0m  [25/53], [94mLoss[0m : 4.73944
[1mStep[0m  [30/53], [94mLoss[0m : 5.52630
[1mStep[0m  [35/53], [94mLoss[0m : 5.31001
[1mStep[0m  [40/53], [94mLoss[0m : 4.97123
[1mStep[0m  [45/53], [94mLoss[0m : 5.20658
[1mStep[0m  [50/53], [94mLoss[0m : 4.63961

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.094, [92mTest[0m: 4.645, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.43278
[1mStep[0m  [5/53], [94mLoss[0m : 5.05668
[1mStep[0m  [10/53], [94mLoss[0m : 4.61680
[1mStep[0m  [15/53], [94mLoss[0m : 4.28258
[1mStep[0m  [20/53], [94mLoss[0m : 5.01650
[1mStep[0m  [25/53], [94mLoss[0m : 4.60843
[1mStep[0m  [30/53], [94mLoss[0m : 4.43587
[1mStep[0m  [35/53], [94mLoss[0m : 4.73843
[1mStep[0m  [40/53], [94mLoss[0m : 4.78473
[1mStep[0m  [45/53], [94mLoss[0m : 4.15925
[1mStep[0m  [50/53], [94mLoss[0m : 4.20265

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.557, [92mTest[0m: 4.000, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.19788
[1mStep[0m  [5/53], [94mLoss[0m : 3.77305
[1mStep[0m  [10/53], [94mLoss[0m : 4.30337
[1mStep[0m  [15/53], [94mLoss[0m : 3.98151
[1mStep[0m  [20/53], [94mLoss[0m : 3.76612
[1mStep[0m  [25/53], [94mLoss[0m : 4.22060
[1mStep[0m  [30/53], [94mLoss[0m : 4.25014
[1mStep[0m  [35/53], [94mLoss[0m : 3.71566
[1mStep[0m  [40/53], [94mLoss[0m : 3.78927
[1mStep[0m  [45/53], [94mLoss[0m : 4.06382
[1mStep[0m  [50/53], [94mLoss[0m : 3.29219

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.065, [92mTest[0m: 3.673, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.52893
[1mStep[0m  [5/53], [94mLoss[0m : 3.81916
[1mStep[0m  [10/53], [94mLoss[0m : 3.30135
[1mStep[0m  [15/53], [94mLoss[0m : 3.45743
[1mStep[0m  [20/53], [94mLoss[0m : 3.61736
[1mStep[0m  [25/53], [94mLoss[0m : 3.68747
[1mStep[0m  [30/53], [94mLoss[0m : 3.58452
[1mStep[0m  [35/53], [94mLoss[0m : 3.64127
[1mStep[0m  [40/53], [94mLoss[0m : 2.99507
[1mStep[0m  [45/53], [94mLoss[0m : 3.36097
[1mStep[0m  [50/53], [94mLoss[0m : 3.16038

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.591, [92mTest[0m: 3.193, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.67144
[1mStep[0m  [5/53], [94mLoss[0m : 3.33630
[1mStep[0m  [10/53], [94mLoss[0m : 3.29339
[1mStep[0m  [15/53], [94mLoss[0m : 3.26100
[1mStep[0m  [20/53], [94mLoss[0m : 3.29094
[1mStep[0m  [25/53], [94mLoss[0m : 3.39168
[1mStep[0m  [30/53], [94mLoss[0m : 3.15474
[1mStep[0m  [35/53], [94mLoss[0m : 3.44726
[1mStep[0m  [40/53], [94mLoss[0m : 3.38107
[1mStep[0m  [45/53], [94mLoss[0m : 2.92110
[1mStep[0m  [50/53], [94mLoss[0m : 2.84217

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.222, [92mTest[0m: 2.869, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.92930
[1mStep[0m  [5/53], [94mLoss[0m : 3.01601
[1mStep[0m  [10/53], [94mLoss[0m : 2.70045
[1mStep[0m  [15/53], [94mLoss[0m : 3.18006
[1mStep[0m  [20/53], [94mLoss[0m : 2.87689
[1mStep[0m  [25/53], [94mLoss[0m : 2.99455
[1mStep[0m  [30/53], [94mLoss[0m : 2.83709
[1mStep[0m  [35/53], [94mLoss[0m : 2.94335
[1mStep[0m  [40/53], [94mLoss[0m : 3.00133
[1mStep[0m  [45/53], [94mLoss[0m : 2.67862
[1mStep[0m  [50/53], [94mLoss[0m : 3.15639

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.992, [92mTest[0m: 2.654, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.93085
[1mStep[0m  [5/53], [94mLoss[0m : 2.82701
[1mStep[0m  [10/53], [94mLoss[0m : 2.85732
[1mStep[0m  [15/53], [94mLoss[0m : 2.80249
[1mStep[0m  [20/53], [94mLoss[0m : 2.74182
[1mStep[0m  [25/53], [94mLoss[0m : 3.10795
[1mStep[0m  [30/53], [94mLoss[0m : 2.91701
[1mStep[0m  [35/53], [94mLoss[0m : 2.99123
[1mStep[0m  [40/53], [94mLoss[0m : 2.77318
[1mStep[0m  [45/53], [94mLoss[0m : 2.76685
[1mStep[0m  [50/53], [94mLoss[0m : 2.71792

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.844, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.05988
[1mStep[0m  [5/53], [94mLoss[0m : 3.00700
[1mStep[0m  [10/53], [94mLoss[0m : 2.89561
[1mStep[0m  [15/53], [94mLoss[0m : 2.80246
[1mStep[0m  [20/53], [94mLoss[0m : 2.94761
[1mStep[0m  [25/53], [94mLoss[0m : 2.73681
[1mStep[0m  [30/53], [94mLoss[0m : 3.10538
[1mStep[0m  [35/53], [94mLoss[0m : 2.75133
[1mStep[0m  [40/53], [94mLoss[0m : 2.81778
[1mStep[0m  [45/53], [94mLoss[0m : 2.77478
[1mStep[0m  [50/53], [94mLoss[0m : 2.70538

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.476, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.76378
[1mStep[0m  [5/53], [94mLoss[0m : 2.80911
[1mStep[0m  [10/53], [94mLoss[0m : 2.82937
[1mStep[0m  [15/53], [94mLoss[0m : 2.96182
[1mStep[0m  [20/53], [94mLoss[0m : 2.82936
[1mStep[0m  [25/53], [94mLoss[0m : 2.78123
[1mStep[0m  [30/53], [94mLoss[0m : 2.50164
[1mStep[0m  [35/53], [94mLoss[0m : 2.60485
[1mStep[0m  [40/53], [94mLoss[0m : 2.81312
[1mStep[0m  [45/53], [94mLoss[0m : 2.68833
[1mStep[0m  [50/53], [94mLoss[0m : 2.87615

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.769, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.06464
[1mStep[0m  [5/53], [94mLoss[0m : 2.61942
[1mStep[0m  [10/53], [94mLoss[0m : 2.61862
[1mStep[0m  [15/53], [94mLoss[0m : 2.51135
[1mStep[0m  [20/53], [94mLoss[0m : 2.47073
[1mStep[0m  [25/53], [94mLoss[0m : 2.68143
[1mStep[0m  [30/53], [94mLoss[0m : 2.72478
[1mStep[0m  [35/53], [94mLoss[0m : 2.68909
[1mStep[0m  [40/53], [94mLoss[0m : 2.90720
[1mStep[0m  [45/53], [94mLoss[0m : 2.72978
[1mStep[0m  [50/53], [94mLoss[0m : 2.82536

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.94483
[1mStep[0m  [5/53], [94mLoss[0m : 2.63054
[1mStep[0m  [10/53], [94mLoss[0m : 2.67581
[1mStep[0m  [15/53], [94mLoss[0m : 2.78154
[1mStep[0m  [20/53], [94mLoss[0m : 2.70584
[1mStep[0m  [25/53], [94mLoss[0m : 2.34397
[1mStep[0m  [30/53], [94mLoss[0m : 2.76144
[1mStep[0m  [35/53], [94mLoss[0m : 2.89344
[1mStep[0m  [40/53], [94mLoss[0m : 2.73117
[1mStep[0m  [45/53], [94mLoss[0m : 2.81183
[1mStep[0m  [50/53], [94mLoss[0m : 2.55141

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.720, [92mTest[0m: 2.429, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.82173
[1mStep[0m  [5/53], [94mLoss[0m : 2.62797
[1mStep[0m  [10/53], [94mLoss[0m : 2.53693
[1mStep[0m  [15/53], [94mLoss[0m : 2.76446
[1mStep[0m  [20/53], [94mLoss[0m : 2.87685
[1mStep[0m  [25/53], [94mLoss[0m : 2.75181
[1mStep[0m  [30/53], [94mLoss[0m : 2.67145
[1mStep[0m  [35/53], [94mLoss[0m : 2.67369
[1mStep[0m  [40/53], [94mLoss[0m : 2.66562
[1mStep[0m  [45/53], [94mLoss[0m : 3.01746
[1mStep[0m  [50/53], [94mLoss[0m : 2.57856

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.742, [92mTest[0m: 2.424, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67128
[1mStep[0m  [5/53], [94mLoss[0m : 2.48692
[1mStep[0m  [10/53], [94mLoss[0m : 2.40712
[1mStep[0m  [15/53], [94mLoss[0m : 2.65161
[1mStep[0m  [20/53], [94mLoss[0m : 2.90808
[1mStep[0m  [25/53], [94mLoss[0m : 2.78851
[1mStep[0m  [30/53], [94mLoss[0m : 2.90310
[1mStep[0m  [35/53], [94mLoss[0m : 2.55645
[1mStep[0m  [40/53], [94mLoss[0m : 2.59931
[1mStep[0m  [45/53], [94mLoss[0m : 2.81739
[1mStep[0m  [50/53], [94mLoss[0m : 2.94129

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.428, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.93346
[1mStep[0m  [5/53], [94mLoss[0m : 2.86696
[1mStep[0m  [10/53], [94mLoss[0m : 2.76069
[1mStep[0m  [15/53], [94mLoss[0m : 2.69770
[1mStep[0m  [20/53], [94mLoss[0m : 2.56370
[1mStep[0m  [25/53], [94mLoss[0m : 2.47174
[1mStep[0m  [30/53], [94mLoss[0m : 2.78931
[1mStep[0m  [35/53], [94mLoss[0m : 2.75991
[1mStep[0m  [40/53], [94mLoss[0m : 2.58845
[1mStep[0m  [45/53], [94mLoss[0m : 2.90075
[1mStep[0m  [50/53], [94mLoss[0m : 2.51289

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.422, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86740
[1mStep[0m  [5/53], [94mLoss[0m : 2.80038
[1mStep[0m  [10/53], [94mLoss[0m : 3.12567
[1mStep[0m  [15/53], [94mLoss[0m : 2.76302
[1mStep[0m  [20/53], [94mLoss[0m : 2.74631
[1mStep[0m  [25/53], [94mLoss[0m : 2.68277
[1mStep[0m  [30/53], [94mLoss[0m : 2.71641
[1mStep[0m  [35/53], [94mLoss[0m : 2.70808
[1mStep[0m  [40/53], [94mLoss[0m : 2.66697
[1mStep[0m  [45/53], [94mLoss[0m : 2.75466
[1mStep[0m  [50/53], [94mLoss[0m : 2.72307

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.59396
[1mStep[0m  [5/53], [94mLoss[0m : 2.86224
[1mStep[0m  [10/53], [94mLoss[0m : 2.90343
[1mStep[0m  [15/53], [94mLoss[0m : 2.78257
[1mStep[0m  [20/53], [94mLoss[0m : 2.49778
[1mStep[0m  [25/53], [94mLoss[0m : 2.60591
[1mStep[0m  [30/53], [94mLoss[0m : 2.59074
[1mStep[0m  [35/53], [94mLoss[0m : 2.67360
[1mStep[0m  [40/53], [94mLoss[0m : 2.60937
[1mStep[0m  [45/53], [94mLoss[0m : 2.87881
[1mStep[0m  [50/53], [94mLoss[0m : 2.70282

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.430, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55729
[1mStep[0m  [5/53], [94mLoss[0m : 2.56175
[1mStep[0m  [10/53], [94mLoss[0m : 2.58661
[1mStep[0m  [15/53], [94mLoss[0m : 2.66972
[1mStep[0m  [20/53], [94mLoss[0m : 2.56470
[1mStep[0m  [25/53], [94mLoss[0m : 2.74094
[1mStep[0m  [30/53], [94mLoss[0m : 2.60728
[1mStep[0m  [35/53], [94mLoss[0m : 2.33777
[1mStep[0m  [40/53], [94mLoss[0m : 2.64087
[1mStep[0m  [45/53], [94mLoss[0m : 2.66441
[1mStep[0m  [50/53], [94mLoss[0m : 2.58875

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56962
[1mStep[0m  [5/53], [94mLoss[0m : 2.50688
[1mStep[0m  [10/53], [94mLoss[0m : 2.48186
[1mStep[0m  [15/53], [94mLoss[0m : 2.76628
[1mStep[0m  [20/53], [94mLoss[0m : 2.31001
[1mStep[0m  [25/53], [94mLoss[0m : 2.83570
[1mStep[0m  [30/53], [94mLoss[0m : 2.81566
[1mStep[0m  [35/53], [94mLoss[0m : 2.71402
[1mStep[0m  [40/53], [94mLoss[0m : 2.68728
[1mStep[0m  [45/53], [94mLoss[0m : 2.53781
[1mStep[0m  [50/53], [94mLoss[0m : 2.84705

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.76988
[1mStep[0m  [5/53], [94mLoss[0m : 2.47927
[1mStep[0m  [10/53], [94mLoss[0m : 2.77103
[1mStep[0m  [15/53], [94mLoss[0m : 2.50595
[1mStep[0m  [20/53], [94mLoss[0m : 2.88045
[1mStep[0m  [25/53], [94mLoss[0m : 2.71858
[1mStep[0m  [30/53], [94mLoss[0m : 2.93973
[1mStep[0m  [35/53], [94mLoss[0m : 2.59741
[1mStep[0m  [40/53], [94mLoss[0m : 2.63924
[1mStep[0m  [45/53], [94mLoss[0m : 2.77816
[1mStep[0m  [50/53], [94mLoss[0m : 2.68335

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72396
[1mStep[0m  [5/53], [94mLoss[0m : 2.46797
[1mStep[0m  [10/53], [94mLoss[0m : 2.51849
[1mStep[0m  [15/53], [94mLoss[0m : 2.72722
[1mStep[0m  [20/53], [94mLoss[0m : 2.95632
[1mStep[0m  [25/53], [94mLoss[0m : 2.65196
[1mStep[0m  [30/53], [94mLoss[0m : 2.74768
[1mStep[0m  [35/53], [94mLoss[0m : 2.58838
[1mStep[0m  [40/53], [94mLoss[0m : 2.55153
[1mStep[0m  [45/53], [94mLoss[0m : 2.61063
[1mStep[0m  [50/53], [94mLoss[0m : 2.77251

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.400, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51040
[1mStep[0m  [5/53], [94mLoss[0m : 2.62506
[1mStep[0m  [10/53], [94mLoss[0m : 2.58033
[1mStep[0m  [15/53], [94mLoss[0m : 2.53336
[1mStep[0m  [20/53], [94mLoss[0m : 2.50631
[1mStep[0m  [25/53], [94mLoss[0m : 2.61838
[1mStep[0m  [30/53], [94mLoss[0m : 2.89009
[1mStep[0m  [35/53], [94mLoss[0m : 2.86888
[1mStep[0m  [40/53], [94mLoss[0m : 2.53451
[1mStep[0m  [45/53], [94mLoss[0m : 2.67934
[1mStep[0m  [50/53], [94mLoss[0m : 2.88984

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.413, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.406
====================================

Phase 1 - Evaluation MAE:  2.405573707360488
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 2.66437
[1mStep[0m  [5/53], [94mLoss[0m : 2.60323
[1mStep[0m  [10/53], [94mLoss[0m : 2.71239
[1mStep[0m  [15/53], [94mLoss[0m : 2.58327
[1mStep[0m  [20/53], [94mLoss[0m : 2.71787
[1mStep[0m  [25/53], [94mLoss[0m : 2.52351
[1mStep[0m  [30/53], [94mLoss[0m : 2.62437
[1mStep[0m  [35/53], [94mLoss[0m : 2.82355
[1mStep[0m  [40/53], [94mLoss[0m : 2.72906
[1mStep[0m  [45/53], [94mLoss[0m : 2.76098
[1mStep[0m  [50/53], [94mLoss[0m : 2.72187

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.414, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77385
[1mStep[0m  [5/53], [94mLoss[0m : 2.66433
[1mStep[0m  [10/53], [94mLoss[0m : 2.76887
[1mStep[0m  [15/53], [94mLoss[0m : 2.55135
[1mStep[0m  [20/53], [94mLoss[0m : 2.58662
[1mStep[0m  [25/53], [94mLoss[0m : 2.90519
[1mStep[0m  [30/53], [94mLoss[0m : 2.63863
[1mStep[0m  [35/53], [94mLoss[0m : 2.53185
[1mStep[0m  [40/53], [94mLoss[0m : 2.85073
[1mStep[0m  [45/53], [94mLoss[0m : 2.82438
[1mStep[0m  [50/53], [94mLoss[0m : 2.82005

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70697
[1mStep[0m  [5/53], [94mLoss[0m : 2.66896
[1mStep[0m  [10/53], [94mLoss[0m : 2.64451
[1mStep[0m  [15/53], [94mLoss[0m : 2.62024
[1mStep[0m  [20/53], [94mLoss[0m : 2.81034
[1mStep[0m  [25/53], [94mLoss[0m : 2.66389
[1mStep[0m  [30/53], [94mLoss[0m : 2.43960
[1mStep[0m  [35/53], [94mLoss[0m : 2.62182
[1mStep[0m  [40/53], [94mLoss[0m : 2.78790
[1mStep[0m  [45/53], [94mLoss[0m : 2.51697
[1mStep[0m  [50/53], [94mLoss[0m : 2.86955

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.501, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45406
[1mStep[0m  [5/53], [94mLoss[0m : 2.81037
[1mStep[0m  [10/53], [94mLoss[0m : 2.65295
[1mStep[0m  [15/53], [94mLoss[0m : 2.57622
[1mStep[0m  [20/53], [94mLoss[0m : 2.92439
[1mStep[0m  [25/53], [94mLoss[0m : 2.88456
[1mStep[0m  [30/53], [94mLoss[0m : 2.58700
[1mStep[0m  [35/53], [94mLoss[0m : 2.43442
[1mStep[0m  [40/53], [94mLoss[0m : 2.53054
[1mStep[0m  [45/53], [94mLoss[0m : 2.58388
[1mStep[0m  [50/53], [94mLoss[0m : 2.60993

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40334
[1mStep[0m  [5/53], [94mLoss[0m : 2.77724
[1mStep[0m  [10/53], [94mLoss[0m : 2.56760
[1mStep[0m  [15/53], [94mLoss[0m : 2.55586
[1mStep[0m  [20/53], [94mLoss[0m : 2.60073
[1mStep[0m  [25/53], [94mLoss[0m : 2.53891
[1mStep[0m  [30/53], [94mLoss[0m : 2.47248
[1mStep[0m  [35/53], [94mLoss[0m : 2.47772
[1mStep[0m  [40/53], [94mLoss[0m : 2.79139
[1mStep[0m  [45/53], [94mLoss[0m : 2.65305
[1mStep[0m  [50/53], [94mLoss[0m : 2.65462

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36644
[1mStep[0m  [5/53], [94mLoss[0m : 2.42513
[1mStep[0m  [10/53], [94mLoss[0m : 2.56080
[1mStep[0m  [15/53], [94mLoss[0m : 2.68548
[1mStep[0m  [20/53], [94mLoss[0m : 2.59886
[1mStep[0m  [25/53], [94mLoss[0m : 2.64414
[1mStep[0m  [30/53], [94mLoss[0m : 2.51114
[1mStep[0m  [35/53], [94mLoss[0m : 2.72873
[1mStep[0m  [40/53], [94mLoss[0m : 2.67341
[1mStep[0m  [45/53], [94mLoss[0m : 2.24569
[1mStep[0m  [50/53], [94mLoss[0m : 2.88676

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.69537
[1mStep[0m  [5/53], [94mLoss[0m : 2.47663
[1mStep[0m  [10/53], [94mLoss[0m : 2.70458
[1mStep[0m  [15/53], [94mLoss[0m : 2.47359
[1mStep[0m  [20/53], [94mLoss[0m : 2.49677
[1mStep[0m  [25/53], [94mLoss[0m : 2.55664
[1mStep[0m  [30/53], [94mLoss[0m : 2.35924
[1mStep[0m  [35/53], [94mLoss[0m : 2.50582
[1mStep[0m  [40/53], [94mLoss[0m : 2.82078
[1mStep[0m  [45/53], [94mLoss[0m : 2.68260
[1mStep[0m  [50/53], [94mLoss[0m : 2.52677

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56481
[1mStep[0m  [5/53], [94mLoss[0m : 2.47050
[1mStep[0m  [10/53], [94mLoss[0m : 2.29835
[1mStep[0m  [15/53], [94mLoss[0m : 2.27023
[1mStep[0m  [20/53], [94mLoss[0m : 2.46667
[1mStep[0m  [25/53], [94mLoss[0m : 2.37119
[1mStep[0m  [30/53], [94mLoss[0m : 2.64914
[1mStep[0m  [35/53], [94mLoss[0m : 2.64625
[1mStep[0m  [40/53], [94mLoss[0m : 2.66048
[1mStep[0m  [45/53], [94mLoss[0m : 2.54629
[1mStep[0m  [50/53], [94mLoss[0m : 2.53656

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61536
[1mStep[0m  [5/53], [94mLoss[0m : 2.73619
[1mStep[0m  [10/53], [94mLoss[0m : 2.39726
[1mStep[0m  [15/53], [94mLoss[0m : 2.59239
[1mStep[0m  [20/53], [94mLoss[0m : 2.78427
[1mStep[0m  [25/53], [94mLoss[0m : 2.50617
[1mStep[0m  [30/53], [94mLoss[0m : 2.60172
[1mStep[0m  [35/53], [94mLoss[0m : 2.53819
[1mStep[0m  [40/53], [94mLoss[0m : 2.40382
[1mStep[0m  [45/53], [94mLoss[0m : 2.56125
[1mStep[0m  [50/53], [94mLoss[0m : 2.66658

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64356
[1mStep[0m  [5/53], [94mLoss[0m : 2.43302
[1mStep[0m  [10/53], [94mLoss[0m : 2.20255
[1mStep[0m  [15/53], [94mLoss[0m : 2.46268
[1mStep[0m  [20/53], [94mLoss[0m : 2.31227
[1mStep[0m  [25/53], [94mLoss[0m : 2.43004
[1mStep[0m  [30/53], [94mLoss[0m : 2.20014
[1mStep[0m  [35/53], [94mLoss[0m : 2.28254
[1mStep[0m  [40/53], [94mLoss[0m : 2.45715
[1mStep[0m  [45/53], [94mLoss[0m : 2.38512
[1mStep[0m  [50/53], [94mLoss[0m : 2.52776

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.396, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47303
[1mStep[0m  [5/53], [94mLoss[0m : 2.66948
[1mStep[0m  [10/53], [94mLoss[0m : 2.63018
[1mStep[0m  [15/53], [94mLoss[0m : 2.48119
[1mStep[0m  [20/53], [94mLoss[0m : 2.67027
[1mStep[0m  [25/53], [94mLoss[0m : 2.43120
[1mStep[0m  [30/53], [94mLoss[0m : 2.53137
[1mStep[0m  [35/53], [94mLoss[0m : 2.28599
[1mStep[0m  [40/53], [94mLoss[0m : 2.42059
[1mStep[0m  [45/53], [94mLoss[0m : 2.71245
[1mStep[0m  [50/53], [94mLoss[0m : 2.33551

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38110
[1mStep[0m  [5/53], [94mLoss[0m : 2.25257
[1mStep[0m  [10/53], [94mLoss[0m : 2.19805
[1mStep[0m  [15/53], [94mLoss[0m : 2.44869
[1mStep[0m  [20/53], [94mLoss[0m : 2.69911
[1mStep[0m  [25/53], [94mLoss[0m : 2.47672
[1mStep[0m  [30/53], [94mLoss[0m : 2.44542
[1mStep[0m  [35/53], [94mLoss[0m : 2.55721
[1mStep[0m  [40/53], [94mLoss[0m : 2.53734
[1mStep[0m  [45/53], [94mLoss[0m : 2.47281
[1mStep[0m  [50/53], [94mLoss[0m : 2.23431

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.391, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48311
[1mStep[0m  [5/53], [94mLoss[0m : 2.33994
[1mStep[0m  [10/53], [94mLoss[0m : 2.22919
[1mStep[0m  [15/53], [94mLoss[0m : 2.53978
[1mStep[0m  [20/53], [94mLoss[0m : 2.29476
[1mStep[0m  [25/53], [94mLoss[0m : 2.41590
[1mStep[0m  [30/53], [94mLoss[0m : 2.25085
[1mStep[0m  [35/53], [94mLoss[0m : 2.68323
[1mStep[0m  [40/53], [94mLoss[0m : 2.46057
[1mStep[0m  [45/53], [94mLoss[0m : 2.54156
[1mStep[0m  [50/53], [94mLoss[0m : 2.17421

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.39694
[1mStep[0m  [5/53], [94mLoss[0m : 2.23309
[1mStep[0m  [10/53], [94mLoss[0m : 2.33709
[1mStep[0m  [15/53], [94mLoss[0m : 2.30054
[1mStep[0m  [20/53], [94mLoss[0m : 2.20155
[1mStep[0m  [25/53], [94mLoss[0m : 2.40380
[1mStep[0m  [30/53], [94mLoss[0m : 2.46151
[1mStep[0m  [35/53], [94mLoss[0m : 2.39989
[1mStep[0m  [40/53], [94mLoss[0m : 2.42781
[1mStep[0m  [45/53], [94mLoss[0m : 2.27712
[1mStep[0m  [50/53], [94mLoss[0m : 2.45297

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.27100
[1mStep[0m  [5/53], [94mLoss[0m : 2.64198
[1mStep[0m  [10/53], [94mLoss[0m : 2.23452
[1mStep[0m  [15/53], [94mLoss[0m : 2.49696
[1mStep[0m  [20/53], [94mLoss[0m : 2.23492
[1mStep[0m  [25/53], [94mLoss[0m : 2.27178
[1mStep[0m  [30/53], [94mLoss[0m : 2.22683
[1mStep[0m  [35/53], [94mLoss[0m : 2.37758
[1mStep[0m  [40/53], [94mLoss[0m : 2.48769
[1mStep[0m  [45/53], [94mLoss[0m : 2.21894
[1mStep[0m  [50/53], [94mLoss[0m : 2.46493

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.31684
[1mStep[0m  [5/53], [94mLoss[0m : 2.34924
[1mStep[0m  [10/53], [94mLoss[0m : 2.42535
[1mStep[0m  [15/53], [94mLoss[0m : 2.32949
