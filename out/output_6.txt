no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  6
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 11.55385
[1mStep[0m  [33/339], [94mLoss[0m : 9.98833
[1mStep[0m  [66/339], [94mLoss[0m : 7.58287
[1mStep[0m  [99/339], [94mLoss[0m : 5.49667
[1mStep[0m  [132/339], [94mLoss[0m : 4.75483
[1mStep[0m  [165/339], [94mLoss[0m : 3.29687
[1mStep[0m  [198/339], [94mLoss[0m : 2.31514
[1mStep[0m  [231/339], [94mLoss[0m : 2.37282
[1mStep[0m  [264/339], [94mLoss[0m : 2.29173
[1mStep[0m  [297/339], [94mLoss[0m : 3.20470
[1mStep[0m  [330/339], [94mLoss[0m : 2.99795

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.706, [92mTest[0m: 10.956, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69842
[1mStep[0m  [33/339], [94mLoss[0m : 2.52433
[1mStep[0m  [66/339], [94mLoss[0m : 2.69630
[1mStep[0m  [99/339], [94mLoss[0m : 2.50443
[1mStep[0m  [132/339], [94mLoss[0m : 2.00578
[1mStep[0m  [165/339], [94mLoss[0m : 2.80145
[1mStep[0m  [198/339], [94mLoss[0m : 3.02619
[1mStep[0m  [231/339], [94mLoss[0m : 1.90594
[1mStep[0m  [264/339], [94mLoss[0m : 2.20332
[1mStep[0m  [297/339], [94mLoss[0m : 3.02673
[1mStep[0m  [330/339], [94mLoss[0m : 3.10530

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.736, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47471
[1mStep[0m  [33/339], [94mLoss[0m : 2.47047
[1mStep[0m  [66/339], [94mLoss[0m : 2.73790
[1mStep[0m  [99/339], [94mLoss[0m : 2.73802
[1mStep[0m  [132/339], [94mLoss[0m : 2.17549
[1mStep[0m  [165/339], [94mLoss[0m : 2.84214
[1mStep[0m  [198/339], [94mLoss[0m : 2.67117
[1mStep[0m  [231/339], [94mLoss[0m : 2.85930
[1mStep[0m  [264/339], [94mLoss[0m : 2.49766
[1mStep[0m  [297/339], [94mLoss[0m : 2.30069
[1mStep[0m  [330/339], [94mLoss[0m : 1.98378

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.405, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58614
[1mStep[0m  [33/339], [94mLoss[0m : 2.51740
[1mStep[0m  [66/339], [94mLoss[0m : 2.93059
[1mStep[0m  [99/339], [94mLoss[0m : 2.52574
[1mStep[0m  [132/339], [94mLoss[0m : 2.73822
[1mStep[0m  [165/339], [94mLoss[0m : 2.95411
[1mStep[0m  [198/339], [94mLoss[0m : 2.67749
[1mStep[0m  [231/339], [94mLoss[0m : 3.11710
[1mStep[0m  [264/339], [94mLoss[0m : 2.90650
[1mStep[0m  [297/339], [94mLoss[0m : 2.88691
[1mStep[0m  [330/339], [94mLoss[0m : 2.18877

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94229
[1mStep[0m  [33/339], [94mLoss[0m : 2.66354
[1mStep[0m  [66/339], [94mLoss[0m : 3.94386
[1mStep[0m  [99/339], [94mLoss[0m : 3.09456
[1mStep[0m  [132/339], [94mLoss[0m : 2.66098
[1mStep[0m  [165/339], [94mLoss[0m : 2.52023
[1mStep[0m  [198/339], [94mLoss[0m : 3.05493
[1mStep[0m  [231/339], [94mLoss[0m : 2.16700
[1mStep[0m  [264/339], [94mLoss[0m : 2.83265
[1mStep[0m  [297/339], [94mLoss[0m : 2.97734
[1mStep[0m  [330/339], [94mLoss[0m : 2.44470

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73974
[1mStep[0m  [33/339], [94mLoss[0m : 2.34098
[1mStep[0m  [66/339], [94mLoss[0m : 1.88156
[1mStep[0m  [99/339], [94mLoss[0m : 2.35071
[1mStep[0m  [132/339], [94mLoss[0m : 2.63808
[1mStep[0m  [165/339], [94mLoss[0m : 2.64068
[1mStep[0m  [198/339], [94mLoss[0m : 3.03272
[1mStep[0m  [231/339], [94mLoss[0m : 2.96556
[1mStep[0m  [264/339], [94mLoss[0m : 3.15881
[1mStep[0m  [297/339], [94mLoss[0m : 2.92041
[1mStep[0m  [330/339], [94mLoss[0m : 2.68154

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59057
[1mStep[0m  [33/339], [94mLoss[0m : 2.29415
[1mStep[0m  [66/339], [94mLoss[0m : 2.39714
[1mStep[0m  [99/339], [94mLoss[0m : 2.27078
[1mStep[0m  [132/339], [94mLoss[0m : 2.61847
[1mStep[0m  [165/339], [94mLoss[0m : 2.50172
[1mStep[0m  [198/339], [94mLoss[0m : 2.65187
[1mStep[0m  [231/339], [94mLoss[0m : 2.34421
[1mStep[0m  [264/339], [94mLoss[0m : 2.32404
[1mStep[0m  [297/339], [94mLoss[0m : 1.81582
[1mStep[0m  [330/339], [94mLoss[0m : 3.17045

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83209
[1mStep[0m  [33/339], [94mLoss[0m : 2.16116
[1mStep[0m  [66/339], [94mLoss[0m : 2.34967
[1mStep[0m  [99/339], [94mLoss[0m : 2.47920
[1mStep[0m  [132/339], [94mLoss[0m : 2.51598
[1mStep[0m  [165/339], [94mLoss[0m : 3.04510
[1mStep[0m  [198/339], [94mLoss[0m : 2.34672
[1mStep[0m  [231/339], [94mLoss[0m : 3.55986
[1mStep[0m  [264/339], [94mLoss[0m : 2.70965
[1mStep[0m  [297/339], [94mLoss[0m : 1.96172
[1mStep[0m  [330/339], [94mLoss[0m : 2.24278

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45826
[1mStep[0m  [33/339], [94mLoss[0m : 2.80263
[1mStep[0m  [66/339], [94mLoss[0m : 2.50052
[1mStep[0m  [99/339], [94mLoss[0m : 2.55143
[1mStep[0m  [132/339], [94mLoss[0m : 2.09149
[1mStep[0m  [165/339], [94mLoss[0m : 2.18285
[1mStep[0m  [198/339], [94mLoss[0m : 2.78710
[1mStep[0m  [231/339], [94mLoss[0m : 1.97969
[1mStep[0m  [264/339], [94mLoss[0m : 2.99505
[1mStep[0m  [297/339], [94mLoss[0m : 2.25782
[1mStep[0m  [330/339], [94mLoss[0m : 3.20140

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07065
[1mStep[0m  [33/339], [94mLoss[0m : 2.95761
[1mStep[0m  [66/339], [94mLoss[0m : 2.44978
[1mStep[0m  [99/339], [94mLoss[0m : 2.76521
[1mStep[0m  [132/339], [94mLoss[0m : 2.53944
[1mStep[0m  [165/339], [94mLoss[0m : 2.53357
[1mStep[0m  [198/339], [94mLoss[0m : 2.39738
[1mStep[0m  [231/339], [94mLoss[0m : 2.51364
[1mStep[0m  [264/339], [94mLoss[0m : 2.30302
[1mStep[0m  [297/339], [94mLoss[0m : 2.21031
[1mStep[0m  [330/339], [94mLoss[0m : 2.47364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78075
[1mStep[0m  [33/339], [94mLoss[0m : 1.66733
[1mStep[0m  [66/339], [94mLoss[0m : 2.02268
[1mStep[0m  [99/339], [94mLoss[0m : 2.71482
[1mStep[0m  [132/339], [94mLoss[0m : 2.36461
[1mStep[0m  [165/339], [94mLoss[0m : 2.32497
[1mStep[0m  [198/339], [94mLoss[0m : 1.92240
[1mStep[0m  [231/339], [94mLoss[0m : 2.31730
[1mStep[0m  [264/339], [94mLoss[0m : 2.48917
[1mStep[0m  [297/339], [94mLoss[0m : 2.19826
[1mStep[0m  [330/339], [94mLoss[0m : 3.20443

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54251
[1mStep[0m  [33/339], [94mLoss[0m : 3.12258
[1mStep[0m  [66/339], [94mLoss[0m : 2.02786
[1mStep[0m  [99/339], [94mLoss[0m : 3.30447
[1mStep[0m  [132/339], [94mLoss[0m : 2.94968
[1mStep[0m  [165/339], [94mLoss[0m : 2.78324
[1mStep[0m  [198/339], [94mLoss[0m : 2.31459
[1mStep[0m  [231/339], [94mLoss[0m : 2.67713
[1mStep[0m  [264/339], [94mLoss[0m : 2.75427
[1mStep[0m  [297/339], [94mLoss[0m : 1.50673
[1mStep[0m  [330/339], [94mLoss[0m : 1.78232

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.350, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38115
[1mStep[0m  [33/339], [94mLoss[0m : 2.66869
[1mStep[0m  [66/339], [94mLoss[0m : 2.58231
[1mStep[0m  [99/339], [94mLoss[0m : 2.12166
[1mStep[0m  [132/339], [94mLoss[0m : 2.28452
[1mStep[0m  [165/339], [94mLoss[0m : 2.31402
[1mStep[0m  [198/339], [94mLoss[0m : 2.40712
[1mStep[0m  [231/339], [94mLoss[0m : 2.47427
[1mStep[0m  [264/339], [94mLoss[0m : 2.89132
[1mStep[0m  [297/339], [94mLoss[0m : 2.22862
[1mStep[0m  [330/339], [94mLoss[0m : 2.42050

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91134
[1mStep[0m  [33/339], [94mLoss[0m : 2.74740
[1mStep[0m  [66/339], [94mLoss[0m : 2.17982
[1mStep[0m  [99/339], [94mLoss[0m : 2.38817
[1mStep[0m  [132/339], [94mLoss[0m : 1.98510
[1mStep[0m  [165/339], [94mLoss[0m : 2.56397
[1mStep[0m  [198/339], [94mLoss[0m : 1.50910
[1mStep[0m  [231/339], [94mLoss[0m : 2.55280
[1mStep[0m  [264/339], [94mLoss[0m : 1.88455
[1mStep[0m  [297/339], [94mLoss[0m : 2.68711
[1mStep[0m  [330/339], [94mLoss[0m : 1.98671

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82204
[1mStep[0m  [33/339], [94mLoss[0m : 2.52174
[1mStep[0m  [66/339], [94mLoss[0m : 2.99476
[1mStep[0m  [99/339], [94mLoss[0m : 2.65714
[1mStep[0m  [132/339], [94mLoss[0m : 2.66201
[1mStep[0m  [165/339], [94mLoss[0m : 2.12115
[1mStep[0m  [198/339], [94mLoss[0m : 2.08859
[1mStep[0m  [231/339], [94mLoss[0m : 2.12295
[1mStep[0m  [264/339], [94mLoss[0m : 2.06707
[1mStep[0m  [297/339], [94mLoss[0m : 1.82600
[1mStep[0m  [330/339], [94mLoss[0m : 3.08026

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61183
[1mStep[0m  [33/339], [94mLoss[0m : 2.94934
[1mStep[0m  [66/339], [94mLoss[0m : 2.48216
[1mStep[0m  [99/339], [94mLoss[0m : 3.08805
[1mStep[0m  [132/339], [94mLoss[0m : 2.48998
[1mStep[0m  [165/339], [94mLoss[0m : 2.35302
[1mStep[0m  [198/339], [94mLoss[0m : 3.68166
[1mStep[0m  [231/339], [94mLoss[0m : 2.76614
[1mStep[0m  [264/339], [94mLoss[0m : 2.85968
[1mStep[0m  [297/339], [94mLoss[0m : 2.06419
[1mStep[0m  [330/339], [94mLoss[0m : 2.56270

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71945
[1mStep[0m  [33/339], [94mLoss[0m : 2.31273
[1mStep[0m  [66/339], [94mLoss[0m : 2.38690
[1mStep[0m  [99/339], [94mLoss[0m : 2.00867
[1mStep[0m  [132/339], [94mLoss[0m : 2.58548
[1mStep[0m  [165/339], [94mLoss[0m : 2.01181
[1mStep[0m  [198/339], [94mLoss[0m : 2.46625
[1mStep[0m  [231/339], [94mLoss[0m : 2.45727
[1mStep[0m  [264/339], [94mLoss[0m : 2.19000
[1mStep[0m  [297/339], [94mLoss[0m : 2.71405
[1mStep[0m  [330/339], [94mLoss[0m : 2.41534

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35992
[1mStep[0m  [33/339], [94mLoss[0m : 2.49571
[1mStep[0m  [66/339], [94mLoss[0m : 2.51013
[1mStep[0m  [99/339], [94mLoss[0m : 2.64083
[1mStep[0m  [132/339], [94mLoss[0m : 2.27011
[1mStep[0m  [165/339], [94mLoss[0m : 1.93400
[1mStep[0m  [198/339], [94mLoss[0m : 2.43392
[1mStep[0m  [231/339], [94mLoss[0m : 2.35880
[1mStep[0m  [264/339], [94mLoss[0m : 2.58154
[1mStep[0m  [297/339], [94mLoss[0m : 2.16986
[1mStep[0m  [330/339], [94mLoss[0m : 2.75853

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59860
[1mStep[0m  [33/339], [94mLoss[0m : 2.21159
[1mStep[0m  [66/339], [94mLoss[0m : 2.57440
[1mStep[0m  [99/339], [94mLoss[0m : 2.44712
[1mStep[0m  [132/339], [94mLoss[0m : 2.20448
[1mStep[0m  [165/339], [94mLoss[0m : 2.44196
[1mStep[0m  [198/339], [94mLoss[0m : 2.07982
[1mStep[0m  [231/339], [94mLoss[0m : 1.85102
[1mStep[0m  [264/339], [94mLoss[0m : 1.97703
[1mStep[0m  [297/339], [94mLoss[0m : 2.59941
[1mStep[0m  [330/339], [94mLoss[0m : 2.83311

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84778
[1mStep[0m  [33/339], [94mLoss[0m : 2.58307
[1mStep[0m  [66/339], [94mLoss[0m : 2.53160
[1mStep[0m  [99/339], [94mLoss[0m : 2.32570
[1mStep[0m  [132/339], [94mLoss[0m : 2.08450
[1mStep[0m  [165/339], [94mLoss[0m : 1.81414
[1mStep[0m  [198/339], [94mLoss[0m : 3.14423
[1mStep[0m  [231/339], [94mLoss[0m : 2.35638
[1mStep[0m  [264/339], [94mLoss[0m : 2.10832
[1mStep[0m  [297/339], [94mLoss[0m : 2.22879
[1mStep[0m  [330/339], [94mLoss[0m : 2.89692

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11804
[1mStep[0m  [33/339], [94mLoss[0m : 2.72600
[1mStep[0m  [66/339], [94mLoss[0m : 2.31928
[1mStep[0m  [99/339], [94mLoss[0m : 2.25548
[1mStep[0m  [132/339], [94mLoss[0m : 1.95629
[1mStep[0m  [165/339], [94mLoss[0m : 2.26316
[1mStep[0m  [198/339], [94mLoss[0m : 2.47255
[1mStep[0m  [231/339], [94mLoss[0m : 1.90673
[1mStep[0m  [264/339], [94mLoss[0m : 2.65278
[1mStep[0m  [297/339], [94mLoss[0m : 2.25636
[1mStep[0m  [330/339], [94mLoss[0m : 2.16263

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.359, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12544
[1mStep[0m  [33/339], [94mLoss[0m : 2.04029
[1mStep[0m  [66/339], [94mLoss[0m : 1.75540
[1mStep[0m  [99/339], [94mLoss[0m : 2.64281
[1mStep[0m  [132/339], [94mLoss[0m : 2.62337
[1mStep[0m  [165/339], [94mLoss[0m : 2.55191
[1mStep[0m  [198/339], [94mLoss[0m : 2.16494
[1mStep[0m  [231/339], [94mLoss[0m : 3.41702
[1mStep[0m  [264/339], [94mLoss[0m : 2.15687
[1mStep[0m  [297/339], [94mLoss[0m : 1.86444
[1mStep[0m  [330/339], [94mLoss[0m : 2.63945

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64490
[1mStep[0m  [33/339], [94mLoss[0m : 2.34065
[1mStep[0m  [66/339], [94mLoss[0m : 2.43868
[1mStep[0m  [99/339], [94mLoss[0m : 2.18574
[1mStep[0m  [132/339], [94mLoss[0m : 2.59662
[1mStep[0m  [165/339], [94mLoss[0m : 1.66101
[1mStep[0m  [198/339], [94mLoss[0m : 1.62874
[1mStep[0m  [231/339], [94mLoss[0m : 2.05204
[1mStep[0m  [264/339], [94mLoss[0m : 2.55907
[1mStep[0m  [297/339], [94mLoss[0m : 2.10007
[1mStep[0m  [330/339], [94mLoss[0m : 2.27276

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17978
[1mStep[0m  [33/339], [94mLoss[0m : 2.12892
[1mStep[0m  [66/339], [94mLoss[0m : 2.40398
[1mStep[0m  [99/339], [94mLoss[0m : 2.75984
[1mStep[0m  [132/339], [94mLoss[0m : 2.24473
[1mStep[0m  [165/339], [94mLoss[0m : 2.18768
[1mStep[0m  [198/339], [94mLoss[0m : 2.37220
[1mStep[0m  [231/339], [94mLoss[0m : 1.86313
[1mStep[0m  [264/339], [94mLoss[0m : 2.06870
[1mStep[0m  [297/339], [94mLoss[0m : 1.69944
[1mStep[0m  [330/339], [94mLoss[0m : 2.21084

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.359, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84158
[1mStep[0m  [33/339], [94mLoss[0m : 1.84918
[1mStep[0m  [66/339], [94mLoss[0m : 1.91060
[1mStep[0m  [99/339], [94mLoss[0m : 2.24299
[1mStep[0m  [132/339], [94mLoss[0m : 2.69338
[1mStep[0m  [165/339], [94mLoss[0m : 2.39217
[1mStep[0m  [198/339], [94mLoss[0m : 1.88290
[1mStep[0m  [231/339], [94mLoss[0m : 2.33810
[1mStep[0m  [264/339], [94mLoss[0m : 3.15574
[1mStep[0m  [297/339], [94mLoss[0m : 2.29982
[1mStep[0m  [330/339], [94mLoss[0m : 2.12852

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.02966
[1mStep[0m  [33/339], [94mLoss[0m : 2.42327
[1mStep[0m  [66/339], [94mLoss[0m : 2.58289
[1mStep[0m  [99/339], [94mLoss[0m : 2.85771
[1mStep[0m  [132/339], [94mLoss[0m : 2.06168
[1mStep[0m  [165/339], [94mLoss[0m : 2.35149
[1mStep[0m  [198/339], [94mLoss[0m : 2.57121
[1mStep[0m  [231/339], [94mLoss[0m : 2.13567
[1mStep[0m  [264/339], [94mLoss[0m : 2.62711
[1mStep[0m  [297/339], [94mLoss[0m : 2.45743
[1mStep[0m  [330/339], [94mLoss[0m : 1.99519

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28960
[1mStep[0m  [33/339], [94mLoss[0m : 3.75299
[1mStep[0m  [66/339], [94mLoss[0m : 1.79305
[1mStep[0m  [99/339], [94mLoss[0m : 2.10307
[1mStep[0m  [132/339], [94mLoss[0m : 2.09578
[1mStep[0m  [165/339], [94mLoss[0m : 2.54071
[1mStep[0m  [198/339], [94mLoss[0m : 2.21980
[1mStep[0m  [231/339], [94mLoss[0m : 2.49662
[1mStep[0m  [264/339], [94mLoss[0m : 2.25835
[1mStep[0m  [297/339], [94mLoss[0m : 2.48367
[1mStep[0m  [330/339], [94mLoss[0m : 2.12241

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86572
[1mStep[0m  [33/339], [94mLoss[0m : 2.39899
[1mStep[0m  [66/339], [94mLoss[0m : 2.44767
[1mStep[0m  [99/339], [94mLoss[0m : 2.77250
[1mStep[0m  [132/339], [94mLoss[0m : 2.40952
[1mStep[0m  [165/339], [94mLoss[0m : 2.04912
[1mStep[0m  [198/339], [94mLoss[0m : 1.73710
[1mStep[0m  [231/339], [94mLoss[0m : 2.39672
[1mStep[0m  [264/339], [94mLoss[0m : 2.38068
[1mStep[0m  [297/339], [94mLoss[0m : 2.16909
[1mStep[0m  [330/339], [94mLoss[0m : 2.46342

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.374, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64716
[1mStep[0m  [33/339], [94mLoss[0m : 2.46821
[1mStep[0m  [66/339], [94mLoss[0m : 2.15832
[1mStep[0m  [99/339], [94mLoss[0m : 2.70024
[1mStep[0m  [132/339], [94mLoss[0m : 2.42657
[1mStep[0m  [165/339], [94mLoss[0m : 2.64487
[1mStep[0m  [198/339], [94mLoss[0m : 2.42264
[1mStep[0m  [231/339], [94mLoss[0m : 2.47617
[1mStep[0m  [264/339], [94mLoss[0m : 3.05979
[1mStep[0m  [297/339], [94mLoss[0m : 2.35911
[1mStep[0m  [330/339], [94mLoss[0m : 2.14919

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08972
[1mStep[0m  [33/339], [94mLoss[0m : 1.88217
[1mStep[0m  [66/339], [94mLoss[0m : 2.33932
[1mStep[0m  [99/339], [94mLoss[0m : 2.42169
[1mStep[0m  [132/339], [94mLoss[0m : 2.72604
[1mStep[0m  [165/339], [94mLoss[0m : 2.60090
[1mStep[0m  [198/339], [94mLoss[0m : 1.71537
[1mStep[0m  [231/339], [94mLoss[0m : 2.02163
[1mStep[0m  [264/339], [94mLoss[0m : 2.64755
[1mStep[0m  [297/339], [94mLoss[0m : 2.31815
[1mStep[0m  [330/339], [94mLoss[0m : 2.98391

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.319
====================================

Phase 1 - Evaluation MAE:  2.319351167805427
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 1.89108
[1mStep[0m  [33/339], [94mLoss[0m : 2.79036
[1mStep[0m  [66/339], [94mLoss[0m : 2.03741
[1mStep[0m  [99/339], [94mLoss[0m : 2.94617
[1mStep[0m  [132/339], [94mLoss[0m : 2.56040
[1mStep[0m  [165/339], [94mLoss[0m : 2.95207
[1mStep[0m  [198/339], [94mLoss[0m : 2.14967
[1mStep[0m  [231/339], [94mLoss[0m : 2.55301
[1mStep[0m  [264/339], [94mLoss[0m : 2.60629
[1mStep[0m  [297/339], [94mLoss[0m : 2.16916
[1mStep[0m  [330/339], [94mLoss[0m : 2.55194

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.319, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82777
[1mStep[0m  [33/339], [94mLoss[0m : 2.28745
[1mStep[0m  [66/339], [94mLoss[0m : 1.71220
[1mStep[0m  [99/339], [94mLoss[0m : 2.61898
[1mStep[0m  [132/339], [94mLoss[0m : 2.31959
[1mStep[0m  [165/339], [94mLoss[0m : 2.54235
[1mStep[0m  [198/339], [94mLoss[0m : 3.05680
[1mStep[0m  [231/339], [94mLoss[0m : 2.83376
[1mStep[0m  [264/339], [94mLoss[0m : 2.72045
[1mStep[0m  [297/339], [94mLoss[0m : 2.90941
[1mStep[0m  [330/339], [94mLoss[0m : 2.46560

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07428
[1mStep[0m  [33/339], [94mLoss[0m : 2.44940
[1mStep[0m  [66/339], [94mLoss[0m : 2.09213
[1mStep[0m  [99/339], [94mLoss[0m : 1.80580
[1mStep[0m  [132/339], [94mLoss[0m : 1.87165
[1mStep[0m  [165/339], [94mLoss[0m : 2.76365
[1mStep[0m  [198/339], [94mLoss[0m : 2.01224
[1mStep[0m  [231/339], [94mLoss[0m : 1.60619
[1mStep[0m  [264/339], [94mLoss[0m : 2.59622
[1mStep[0m  [297/339], [94mLoss[0m : 2.48929
[1mStep[0m  [330/339], [94mLoss[0m : 3.16580

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.493, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26363
[1mStep[0m  [33/339], [94mLoss[0m : 2.43443
[1mStep[0m  [66/339], [94mLoss[0m : 2.36227
[1mStep[0m  [99/339], [94mLoss[0m : 2.43669
[1mStep[0m  [132/339], [94mLoss[0m : 2.75168
[1mStep[0m  [165/339], [94mLoss[0m : 2.22511
[1mStep[0m  [198/339], [94mLoss[0m : 1.78324
[1mStep[0m  [231/339], [94mLoss[0m : 2.94842
[1mStep[0m  [264/339], [94mLoss[0m : 2.06882
[1mStep[0m  [297/339], [94mLoss[0m : 2.07564
[1mStep[0m  [330/339], [94mLoss[0m : 2.73000

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.463, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00650
[1mStep[0m  [33/339], [94mLoss[0m : 2.87312
[1mStep[0m  [66/339], [94mLoss[0m : 1.88290
[1mStep[0m  [99/339], [94mLoss[0m : 2.30329
[1mStep[0m  [132/339], [94mLoss[0m : 2.06790
[1mStep[0m  [165/339], [94mLoss[0m : 2.50997
[1mStep[0m  [198/339], [94mLoss[0m : 2.41851
[1mStep[0m  [231/339], [94mLoss[0m : 2.53259
[1mStep[0m  [264/339], [94mLoss[0m : 2.39692
[1mStep[0m  [297/339], [94mLoss[0m : 2.02664
[1mStep[0m  [330/339], [94mLoss[0m : 2.56073

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.545, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40287
[1mStep[0m  [33/339], [94mLoss[0m : 2.94824
[1mStep[0m  [66/339], [94mLoss[0m : 1.87437
[1mStep[0m  [99/339], [94mLoss[0m : 2.27981
[1mStep[0m  [132/339], [94mLoss[0m : 2.89386
[1mStep[0m  [165/339], [94mLoss[0m : 2.36453
[1mStep[0m  [198/339], [94mLoss[0m : 2.18846
[1mStep[0m  [231/339], [94mLoss[0m : 2.20370
[1mStep[0m  [264/339], [94mLoss[0m : 2.69548
[1mStep[0m  [297/339], [94mLoss[0m : 2.59971
[1mStep[0m  [330/339], [94mLoss[0m : 1.99493

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.257, [92mTest[0m: 2.597, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19741
[1mStep[0m  [33/339], [94mLoss[0m : 2.11430
[1mStep[0m  [66/339], [94mLoss[0m : 2.43146
[1mStep[0m  [99/339], [94mLoss[0m : 1.88101
[1mStep[0m  [132/339], [94mLoss[0m : 1.71049
[1mStep[0m  [165/339], [94mLoss[0m : 2.40756
[1mStep[0m  [198/339], [94mLoss[0m : 2.22839
[1mStep[0m  [231/339], [94mLoss[0m : 2.38468
[1mStep[0m  [264/339], [94mLoss[0m : 1.72245
[1mStep[0m  [297/339], [94mLoss[0m : 2.34402
[1mStep[0m  [330/339], [94mLoss[0m : 2.05954

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.512, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25341
[1mStep[0m  [33/339], [94mLoss[0m : 1.96786
[1mStep[0m  [66/339], [94mLoss[0m : 2.28670
[1mStep[0m  [99/339], [94mLoss[0m : 2.15673
[1mStep[0m  [132/339], [94mLoss[0m : 1.61631
[1mStep[0m  [165/339], [94mLoss[0m : 2.13822
[1mStep[0m  [198/339], [94mLoss[0m : 2.60978
[1mStep[0m  [231/339], [94mLoss[0m : 1.96578
[1mStep[0m  [264/339], [94mLoss[0m : 2.44421
[1mStep[0m  [297/339], [94mLoss[0m : 1.94104
[1mStep[0m  [330/339], [94mLoss[0m : 2.81980

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.482, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70561
[1mStep[0m  [33/339], [94mLoss[0m : 1.84403
[1mStep[0m  [66/339], [94mLoss[0m : 1.65976
[1mStep[0m  [99/339], [94mLoss[0m : 2.06618
[1mStep[0m  [132/339], [94mLoss[0m : 2.17060
[1mStep[0m  [165/339], [94mLoss[0m : 2.02622
[1mStep[0m  [198/339], [94mLoss[0m : 2.22922
[1mStep[0m  [231/339], [94mLoss[0m : 1.92267
[1mStep[0m  [264/339], [94mLoss[0m : 3.21338
[1mStep[0m  [297/339], [94mLoss[0m : 2.51105
[1mStep[0m  [330/339], [94mLoss[0m : 1.72061

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96166
[1mStep[0m  [33/339], [94mLoss[0m : 2.02920
[1mStep[0m  [66/339], [94mLoss[0m : 2.19638
[1mStep[0m  [99/339], [94mLoss[0m : 1.75042
[1mStep[0m  [132/339], [94mLoss[0m : 1.88742
[1mStep[0m  [165/339], [94mLoss[0m : 2.66771
[1mStep[0m  [198/339], [94mLoss[0m : 1.91083
[1mStep[0m  [231/339], [94mLoss[0m : 2.11926
[1mStep[0m  [264/339], [94mLoss[0m : 1.77919
[1mStep[0m  [297/339], [94mLoss[0m : 2.35650
[1mStep[0m  [330/339], [94mLoss[0m : 1.70003

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68791
[1mStep[0m  [33/339], [94mLoss[0m : 1.66205
[1mStep[0m  [66/339], [94mLoss[0m : 2.60358
[1mStep[0m  [99/339], [94mLoss[0m : 1.82788
[1mStep[0m  [132/339], [94mLoss[0m : 2.74189
[1mStep[0m  [165/339], [94mLoss[0m : 1.60034
[1mStep[0m  [198/339], [94mLoss[0m : 1.94548
[1mStep[0m  [231/339], [94mLoss[0m : 2.38403
[1mStep[0m  [264/339], [94mLoss[0m : 1.97829
[1mStep[0m  [297/339], [94mLoss[0m : 3.09261
[1mStep[0m  [330/339], [94mLoss[0m : 2.17056

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.489, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74114
[1mStep[0m  [33/339], [94mLoss[0m : 2.11783
[1mStep[0m  [66/339], [94mLoss[0m : 1.65504
[1mStep[0m  [99/339], [94mLoss[0m : 1.53549
[1mStep[0m  [132/339], [94mLoss[0m : 2.30343
[1mStep[0m  [165/339], [94mLoss[0m : 1.56418
[1mStep[0m  [198/339], [94mLoss[0m : 2.49812
[1mStep[0m  [231/339], [94mLoss[0m : 2.60199
[1mStep[0m  [264/339], [94mLoss[0m : 1.65635
[1mStep[0m  [297/339], [94mLoss[0m : 2.09070
[1mStep[0m  [330/339], [94mLoss[0m : 2.06349

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62996
[1mStep[0m  [33/339], [94mLoss[0m : 1.47925
[1mStep[0m  [66/339], [94mLoss[0m : 2.20199
[1mStep[0m  [99/339], [94mLoss[0m : 2.44169
[1mStep[0m  [132/339], [94mLoss[0m : 2.17506
[1mStep[0m  [165/339], [94mLoss[0m : 2.43756
[1mStep[0m  [198/339], [94mLoss[0m : 2.73564
[1mStep[0m  [231/339], [94mLoss[0m : 2.05764
[1mStep[0m  [264/339], [94mLoss[0m : 2.21250
[1mStep[0m  [297/339], [94mLoss[0m : 1.97080
[1mStep[0m  [330/339], [94mLoss[0m : 1.90758

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90955
[1mStep[0m  [33/339], [94mLoss[0m : 2.26176
[1mStep[0m  [66/339], [94mLoss[0m : 1.82241
[1mStep[0m  [99/339], [94mLoss[0m : 2.13384
[1mStep[0m  [132/339], [94mLoss[0m : 1.43853
[1mStep[0m  [165/339], [94mLoss[0m : 1.69440
[1mStep[0m  [198/339], [94mLoss[0m : 1.97681
[1mStep[0m  [231/339], [94mLoss[0m : 2.18682
[1mStep[0m  [264/339], [94mLoss[0m : 1.78470
[1mStep[0m  [297/339], [94mLoss[0m : 1.54391
[1mStep[0m  [330/339], [94mLoss[0m : 2.18736

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.539, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77712
[1mStep[0m  [33/339], [94mLoss[0m : 1.96326
[1mStep[0m  [66/339], [94mLoss[0m : 1.99793
[1mStep[0m  [99/339], [94mLoss[0m : 1.99784
[1mStep[0m  [132/339], [94mLoss[0m : 1.68986
[1mStep[0m  [165/339], [94mLoss[0m : 1.83622
[1mStep[0m  [198/339], [94mLoss[0m : 2.18708
[1mStep[0m  [231/339], [94mLoss[0m : 1.64354
[1mStep[0m  [264/339], [94mLoss[0m : 1.54567
[1mStep[0m  [297/339], [94mLoss[0m : 1.69002
[1mStep[0m  [330/339], [94mLoss[0m : 1.65950

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.530, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95907
[1mStep[0m  [33/339], [94mLoss[0m : 1.76593
[1mStep[0m  [66/339], [94mLoss[0m : 1.53335
[1mStep[0m  [99/339], [94mLoss[0m : 1.49488
[1mStep[0m  [132/339], [94mLoss[0m : 1.88421
[1mStep[0m  [165/339], [94mLoss[0m : 1.62876
[1mStep[0m  [198/339], [94mLoss[0m : 1.68741
[1mStep[0m  [231/339], [94mLoss[0m : 2.03437
[1mStep[0m  [264/339], [94mLoss[0m : 2.59130
[1mStep[0m  [297/339], [94mLoss[0m : 2.08700
[1mStep[0m  [330/339], [94mLoss[0m : 2.26918

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85478
[1mStep[0m  [33/339], [94mLoss[0m : 2.13265
[1mStep[0m  [66/339], [94mLoss[0m : 1.81564
[1mStep[0m  [99/339], [94mLoss[0m : 1.64710
[1mStep[0m  [132/339], [94mLoss[0m : 1.83307
[1mStep[0m  [165/339], [94mLoss[0m : 1.65625
[1mStep[0m  [198/339], [94mLoss[0m : 2.06720
[1mStep[0m  [231/339], [94mLoss[0m : 1.85456
[1mStep[0m  [264/339], [94mLoss[0m : 1.90030
[1mStep[0m  [297/339], [94mLoss[0m : 1.71041
[1mStep[0m  [330/339], [94mLoss[0m : 1.85753

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47103
[1mStep[0m  [33/339], [94mLoss[0m : 2.02354
[1mStep[0m  [66/339], [94mLoss[0m : 1.80093
[1mStep[0m  [99/339], [94mLoss[0m : 1.96692
[1mStep[0m  [132/339], [94mLoss[0m : 2.03000
[1mStep[0m  [165/339], [94mLoss[0m : 1.85299
[1mStep[0m  [198/339], [94mLoss[0m : 1.73472
[1mStep[0m  [231/339], [94mLoss[0m : 1.55299
[1mStep[0m  [264/339], [94mLoss[0m : 1.83217
[1mStep[0m  [297/339], [94mLoss[0m : 2.29239
[1mStep[0m  [330/339], [94mLoss[0m : 1.47998

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.537, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92192
[1mStep[0m  [33/339], [94mLoss[0m : 1.68022
[1mStep[0m  [66/339], [94mLoss[0m : 1.47919
[1mStep[0m  [99/339], [94mLoss[0m : 2.13823
[1mStep[0m  [132/339], [94mLoss[0m : 1.90014
[1mStep[0m  [165/339], [94mLoss[0m : 1.70952
[1mStep[0m  [198/339], [94mLoss[0m : 1.50192
[1mStep[0m  [231/339], [94mLoss[0m : 1.57710
[1mStep[0m  [264/339], [94mLoss[0m : 2.07380
[1mStep[0m  [297/339], [94mLoss[0m : 1.67345
[1mStep[0m  [330/339], [94mLoss[0m : 2.68792

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.532, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.34273
[1mStep[0m  [33/339], [94mLoss[0m : 1.73090
[1mStep[0m  [66/339], [94mLoss[0m : 1.92915
[1mStep[0m  [99/339], [94mLoss[0m : 1.46296
[1mStep[0m  [132/339], [94mLoss[0m : 1.52183
[1mStep[0m  [165/339], [94mLoss[0m : 1.79697
[1mStep[0m  [198/339], [94mLoss[0m : 1.44837
[1mStep[0m  [231/339], [94mLoss[0m : 1.95865
[1mStep[0m  [264/339], [94mLoss[0m : 1.68282
[1mStep[0m  [297/339], [94mLoss[0m : 1.59213
[1mStep[0m  [330/339], [94mLoss[0m : 1.76236

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.568, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67400
[1mStep[0m  [33/339], [94mLoss[0m : 1.87268
[1mStep[0m  [66/339], [94mLoss[0m : 1.32596
[1mStep[0m  [99/339], [94mLoss[0m : 1.56981
[1mStep[0m  [132/339], [94mLoss[0m : 1.60905
[1mStep[0m  [165/339], [94mLoss[0m : 2.22685
[1mStep[0m  [198/339], [94mLoss[0m : 1.72767
[1mStep[0m  [231/339], [94mLoss[0m : 1.48699
[1mStep[0m  [264/339], [94mLoss[0m : 1.66481
[1mStep[0m  [297/339], [94mLoss[0m : 1.80670
[1mStep[0m  [330/339], [94mLoss[0m : 2.06121

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.520, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.49893
[1mStep[0m  [33/339], [94mLoss[0m : 1.48105
[1mStep[0m  [66/339], [94mLoss[0m : 1.64267
[1mStep[0m  [99/339], [94mLoss[0m : 1.91223
[1mStep[0m  [132/339], [94mLoss[0m : 1.80815
[1mStep[0m  [165/339], [94mLoss[0m : 1.41967
[1mStep[0m  [198/339], [94mLoss[0m : 1.94237
[1mStep[0m  [231/339], [94mLoss[0m : 2.03951
[1mStep[0m  [264/339], [94mLoss[0m : 2.22793
[1mStep[0m  [297/339], [94mLoss[0m : 1.50698
[1mStep[0m  [330/339], [94mLoss[0m : 1.34451

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51823
[1mStep[0m  [33/339], [94mLoss[0m : 2.22752
[1mStep[0m  [66/339], [94mLoss[0m : 1.39368
[1mStep[0m  [99/339], [94mLoss[0m : 1.36515
[1mStep[0m  [132/339], [94mLoss[0m : 1.88843
[1mStep[0m  [165/339], [94mLoss[0m : 1.64983
[1mStep[0m  [198/339], [94mLoss[0m : 1.77015
[1mStep[0m  [231/339], [94mLoss[0m : 2.17685
[1mStep[0m  [264/339], [94mLoss[0m : 1.42640
[1mStep[0m  [297/339], [94mLoss[0m : 1.99653
[1mStep[0m  [330/339], [94mLoss[0m : 1.72602

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.550, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53450
[1mStep[0m  [33/339], [94mLoss[0m : 2.44211
[1mStep[0m  [66/339], [94mLoss[0m : 1.98279
[1mStep[0m  [99/339], [94mLoss[0m : 1.74442
[1mStep[0m  [132/339], [94mLoss[0m : 1.56302
[1mStep[0m  [165/339], [94mLoss[0m : 1.78309
[1mStep[0m  [198/339], [94mLoss[0m : 1.87649
[1mStep[0m  [231/339], [94mLoss[0m : 2.35938
[1mStep[0m  [264/339], [94mLoss[0m : 1.76999
[1mStep[0m  [297/339], [94mLoss[0m : 1.74593
[1mStep[0m  [330/339], [94mLoss[0m : 1.69633

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39201
[1mStep[0m  [33/339], [94mLoss[0m : 2.27886
[1mStep[0m  [66/339], [94mLoss[0m : 1.78260
[1mStep[0m  [99/339], [94mLoss[0m : 1.16885
[1mStep[0m  [132/339], [94mLoss[0m : 1.85189
[1mStep[0m  [165/339], [94mLoss[0m : 1.66553
[1mStep[0m  [198/339], [94mLoss[0m : 1.76444
[1mStep[0m  [231/339], [94mLoss[0m : 1.17957
[1mStep[0m  [264/339], [94mLoss[0m : 1.66392
[1mStep[0m  [297/339], [94mLoss[0m : 1.91628
[1mStep[0m  [330/339], [94mLoss[0m : 1.76559

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.628, [92mTest[0m: 2.468, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12274
[1mStep[0m  [33/339], [94mLoss[0m : 1.40105
[1mStep[0m  [66/339], [94mLoss[0m : 1.46362
[1mStep[0m  [99/339], [94mLoss[0m : 1.73429
[1mStep[0m  [132/339], [94mLoss[0m : 1.24264
[1mStep[0m  [165/339], [94mLoss[0m : 1.52619
[1mStep[0m  [198/339], [94mLoss[0m : 1.60546
[1mStep[0m  [231/339], [94mLoss[0m : 1.23788
[1mStep[0m  [264/339], [94mLoss[0m : 1.75600
[1mStep[0m  [297/339], [94mLoss[0m : 1.89479
[1mStep[0m  [330/339], [94mLoss[0m : 2.03911

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.503, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.18134
[1mStep[0m  [33/339], [94mLoss[0m : 1.62074
[1mStep[0m  [66/339], [94mLoss[0m : 1.30709
[1mStep[0m  [99/339], [94mLoss[0m : 1.19678
[1mStep[0m  [132/339], [94mLoss[0m : 1.28745
[1mStep[0m  [165/339], [94mLoss[0m : 1.82475
[1mStep[0m  [198/339], [94mLoss[0m : 1.84552
[1mStep[0m  [231/339], [94mLoss[0m : 1.13404
[1mStep[0m  [264/339], [94mLoss[0m : 1.65228
[1mStep[0m  [297/339], [94mLoss[0m : 1.73414
[1mStep[0m  [330/339], [94mLoss[0m : 1.70863

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37486
[1mStep[0m  [33/339], [94mLoss[0m : 1.43317
[1mStep[0m  [66/339], [94mLoss[0m : 1.66373
[1mStep[0m  [99/339], [94mLoss[0m : 1.70396
[1mStep[0m  [132/339], [94mLoss[0m : 1.55638
[1mStep[0m  [165/339], [94mLoss[0m : 1.40728
[1mStep[0m  [198/339], [94mLoss[0m : 1.37845
[1mStep[0m  [231/339], [94mLoss[0m : 1.44877
[1mStep[0m  [264/339], [94mLoss[0m : 1.82684
[1mStep[0m  [297/339], [94mLoss[0m : 1.32241
[1mStep[0m  [330/339], [94mLoss[0m : 1.93274

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.544, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05632
[1mStep[0m  [33/339], [94mLoss[0m : 1.21937
[1mStep[0m  [66/339], [94mLoss[0m : 1.44295
[1mStep[0m  [99/339], [94mLoss[0m : 1.92171
[1mStep[0m  [132/339], [94mLoss[0m : 1.84369
[1mStep[0m  [165/339], [94mLoss[0m : 1.81840
[1mStep[0m  [198/339], [94mLoss[0m : 1.35827
[1mStep[0m  [231/339], [94mLoss[0m : 1.17793
[1mStep[0m  [264/339], [94mLoss[0m : 1.62509
[1mStep[0m  [297/339], [94mLoss[0m : 1.45100
[1mStep[0m  [330/339], [94mLoss[0m : 1.83644

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.516, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92377
[1mStep[0m  [33/339], [94mLoss[0m : 1.49485
[1mStep[0m  [66/339], [94mLoss[0m : 1.49769
[1mStep[0m  [99/339], [94mLoss[0m : 1.46904
[1mStep[0m  [132/339], [94mLoss[0m : 1.61297
[1mStep[0m  [165/339], [94mLoss[0m : 1.49881
[1mStep[0m  [198/339], [94mLoss[0m : 1.45164
[1mStep[0m  [231/339], [94mLoss[0m : 1.48360
[1mStep[0m  [264/339], [94mLoss[0m : 1.63541
[1mStep[0m  [297/339], [94mLoss[0m : 1.40861
[1mStep[0m  [330/339], [94mLoss[0m : 1.30420

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.518
====================================

Phase 2 - Evaluation MAE:  2.518247076895385
MAE score P1       2.319351
MAE score P2       2.518247
loss               1.562869
learning_rate       0.00505
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.60081
[1mStep[0m  [2/21], [94mLoss[0m : 10.49060
[1mStep[0m  [4/21], [94mLoss[0m : 10.22862
[1mStep[0m  [6/21], [94mLoss[0m : 10.09657
[1mStep[0m  [8/21], [94mLoss[0m : 9.79409
[1mStep[0m  [10/21], [94mLoss[0m : 9.05756
[1mStep[0m  [12/21], [94mLoss[0m : 8.36403
[1mStep[0m  [14/21], [94mLoss[0m : 8.51389
[1mStep[0m  [16/21], [94mLoss[0m : 7.60959
[1mStep[0m  [18/21], [94mLoss[0m : 7.10680
[1mStep[0m  [20/21], [94mLoss[0m : 6.63006

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.971, [92mTest[0m: 10.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.01055
[1mStep[0m  [2/21], [94mLoss[0m : 5.43388
[1mStep[0m  [4/21], [94mLoss[0m : 4.77242
[1mStep[0m  [6/21], [94mLoss[0m : 4.41019
[1mStep[0m  [8/21], [94mLoss[0m : 4.07232
[1mStep[0m  [10/21], [94mLoss[0m : 3.72288
[1mStep[0m  [12/21], [94mLoss[0m : 3.31915
[1mStep[0m  [14/21], [94mLoss[0m : 3.13632
[1mStep[0m  [16/21], [94mLoss[0m : 3.06178
[1mStep[0m  [18/21], [94mLoss[0m : 3.04386
[1mStep[0m  [20/21], [94mLoss[0m : 2.88178

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.999, [92mTest[0m: 6.093, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80054
[1mStep[0m  [2/21], [94mLoss[0m : 2.70331
[1mStep[0m  [4/21], [94mLoss[0m : 2.68808
[1mStep[0m  [6/21], [94mLoss[0m : 2.85628
[1mStep[0m  [8/21], [94mLoss[0m : 2.85020
[1mStep[0m  [10/21], [94mLoss[0m : 2.93199
[1mStep[0m  [12/21], [94mLoss[0m : 2.66205
[1mStep[0m  [14/21], [94mLoss[0m : 2.69583
[1mStep[0m  [16/21], [94mLoss[0m : 2.62660
[1mStep[0m  [18/21], [94mLoss[0m : 2.64208
[1mStep[0m  [20/21], [94mLoss[0m : 2.49435

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.683, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61163
[1mStep[0m  [2/21], [94mLoss[0m : 2.53751
[1mStep[0m  [4/21], [94mLoss[0m : 2.75246
[1mStep[0m  [6/21], [94mLoss[0m : 2.69970
[1mStep[0m  [8/21], [94mLoss[0m : 2.52768
[1mStep[0m  [10/21], [94mLoss[0m : 2.46411
[1mStep[0m  [12/21], [94mLoss[0m : 2.68619
[1mStep[0m  [14/21], [94mLoss[0m : 2.73317
[1mStep[0m  [16/21], [94mLoss[0m : 2.54985
[1mStep[0m  [18/21], [94mLoss[0m : 2.54613
[1mStep[0m  [20/21], [94mLoss[0m : 2.59704

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.535, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58731
[1mStep[0m  [2/21], [94mLoss[0m : 2.54184
[1mStep[0m  [4/21], [94mLoss[0m : 2.55460
[1mStep[0m  [6/21], [94mLoss[0m : 2.60883
[1mStep[0m  [8/21], [94mLoss[0m : 2.67250
[1mStep[0m  [10/21], [94mLoss[0m : 2.54492
[1mStep[0m  [12/21], [94mLoss[0m : 2.61578
[1mStep[0m  [14/21], [94mLoss[0m : 2.55453
[1mStep[0m  [16/21], [94mLoss[0m : 2.59982
[1mStep[0m  [18/21], [94mLoss[0m : 2.76500
[1mStep[0m  [20/21], [94mLoss[0m : 2.67698

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76829
[1mStep[0m  [2/21], [94mLoss[0m : 2.77284
[1mStep[0m  [4/21], [94mLoss[0m : 2.52260
[1mStep[0m  [6/21], [94mLoss[0m : 2.63805
[1mStep[0m  [8/21], [94mLoss[0m : 2.54390
[1mStep[0m  [10/21], [94mLoss[0m : 2.58802
[1mStep[0m  [12/21], [94mLoss[0m : 2.52171
[1mStep[0m  [14/21], [94mLoss[0m : 2.57643
[1mStep[0m  [16/21], [94mLoss[0m : 2.51867
[1mStep[0m  [18/21], [94mLoss[0m : 2.63442
[1mStep[0m  [20/21], [94mLoss[0m : 2.57284

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51131
[1mStep[0m  [2/21], [94mLoss[0m : 2.73913
[1mStep[0m  [4/21], [94mLoss[0m : 2.73450
[1mStep[0m  [6/21], [94mLoss[0m : 2.73608
[1mStep[0m  [8/21], [94mLoss[0m : 2.60121
[1mStep[0m  [10/21], [94mLoss[0m : 2.51922
[1mStep[0m  [12/21], [94mLoss[0m : 2.50016
[1mStep[0m  [14/21], [94mLoss[0m : 2.65148
[1mStep[0m  [16/21], [94mLoss[0m : 2.69034
[1mStep[0m  [18/21], [94mLoss[0m : 2.52879
[1mStep[0m  [20/21], [94mLoss[0m : 2.57527

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52966
[1mStep[0m  [2/21], [94mLoss[0m : 2.59281
[1mStep[0m  [4/21], [94mLoss[0m : 2.55083
[1mStep[0m  [6/21], [94mLoss[0m : 2.63868
[1mStep[0m  [8/21], [94mLoss[0m : 2.72267
[1mStep[0m  [10/21], [94mLoss[0m : 2.49416
[1mStep[0m  [12/21], [94mLoss[0m : 2.58566
[1mStep[0m  [14/21], [94mLoss[0m : 2.58681
[1mStep[0m  [16/21], [94mLoss[0m : 2.38298
[1mStep[0m  [18/21], [94mLoss[0m : 2.61185
[1mStep[0m  [20/21], [94mLoss[0m : 2.53257

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53831
[1mStep[0m  [2/21], [94mLoss[0m : 2.59654
[1mStep[0m  [4/21], [94mLoss[0m : 2.57904
[1mStep[0m  [6/21], [94mLoss[0m : 2.56222
[1mStep[0m  [8/21], [94mLoss[0m : 2.47125
[1mStep[0m  [10/21], [94mLoss[0m : 2.60313
[1mStep[0m  [12/21], [94mLoss[0m : 2.62877
[1mStep[0m  [14/21], [94mLoss[0m : 2.54446
[1mStep[0m  [16/21], [94mLoss[0m : 2.53577
[1mStep[0m  [18/21], [94mLoss[0m : 2.57138
[1mStep[0m  [20/21], [94mLoss[0m : 2.51456

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46020
[1mStep[0m  [2/21], [94mLoss[0m : 2.67233
[1mStep[0m  [4/21], [94mLoss[0m : 2.64868
[1mStep[0m  [6/21], [94mLoss[0m : 2.56388
[1mStep[0m  [8/21], [94mLoss[0m : 2.58455
[1mStep[0m  [10/21], [94mLoss[0m : 2.53600
[1mStep[0m  [12/21], [94mLoss[0m : 2.59247
[1mStep[0m  [14/21], [94mLoss[0m : 2.60741
[1mStep[0m  [16/21], [94mLoss[0m : 2.48950
[1mStep[0m  [18/21], [94mLoss[0m : 2.59266
[1mStep[0m  [20/21], [94mLoss[0m : 2.57528

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54429
[1mStep[0m  [2/21], [94mLoss[0m : 2.64336
[1mStep[0m  [4/21], [94mLoss[0m : 2.61629
[1mStep[0m  [6/21], [94mLoss[0m : 2.58737
[1mStep[0m  [8/21], [94mLoss[0m : 2.53502
[1mStep[0m  [10/21], [94mLoss[0m : 2.38567
[1mStep[0m  [12/21], [94mLoss[0m : 2.64481
[1mStep[0m  [14/21], [94mLoss[0m : 2.61104
[1mStep[0m  [16/21], [94mLoss[0m : 2.61556
[1mStep[0m  [18/21], [94mLoss[0m : 2.42418
[1mStep[0m  [20/21], [94mLoss[0m : 2.61829

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67102
[1mStep[0m  [2/21], [94mLoss[0m : 2.56288
[1mStep[0m  [4/21], [94mLoss[0m : 2.59427
[1mStep[0m  [6/21], [94mLoss[0m : 2.41024
[1mStep[0m  [8/21], [94mLoss[0m : 2.58686
[1mStep[0m  [10/21], [94mLoss[0m : 2.50815
[1mStep[0m  [12/21], [94mLoss[0m : 2.54520
[1mStep[0m  [14/21], [94mLoss[0m : 2.43551
[1mStep[0m  [16/21], [94mLoss[0m : 2.58217
[1mStep[0m  [18/21], [94mLoss[0m : 2.54884
[1mStep[0m  [20/21], [94mLoss[0m : 2.62522

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48583
[1mStep[0m  [2/21], [94mLoss[0m : 2.67815
[1mStep[0m  [4/21], [94mLoss[0m : 2.50278
[1mStep[0m  [6/21], [94mLoss[0m : 2.57891
[1mStep[0m  [8/21], [94mLoss[0m : 2.63157
[1mStep[0m  [10/21], [94mLoss[0m : 2.51646
[1mStep[0m  [12/21], [94mLoss[0m : 2.51959
[1mStep[0m  [14/21], [94mLoss[0m : 2.67661
[1mStep[0m  [16/21], [94mLoss[0m : 2.67912
[1mStep[0m  [18/21], [94mLoss[0m : 2.65935
[1mStep[0m  [20/21], [94mLoss[0m : 2.45807

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71139
[1mStep[0m  [2/21], [94mLoss[0m : 2.52462
[1mStep[0m  [4/21], [94mLoss[0m : 2.60617
[1mStep[0m  [6/21], [94mLoss[0m : 2.62506
[1mStep[0m  [8/21], [94mLoss[0m : 2.35223
[1mStep[0m  [10/21], [94mLoss[0m : 2.60833
[1mStep[0m  [12/21], [94mLoss[0m : 2.57171
[1mStep[0m  [14/21], [94mLoss[0m : 2.52726
[1mStep[0m  [16/21], [94mLoss[0m : 2.58930
[1mStep[0m  [18/21], [94mLoss[0m : 2.82569
[1mStep[0m  [20/21], [94mLoss[0m : 2.55600

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.51546
[1mStep[0m  [2/21], [94mLoss[0m : 2.74694
[1mStep[0m  [4/21], [94mLoss[0m : 2.58074
[1mStep[0m  [6/21], [94mLoss[0m : 2.57091
[1mStep[0m  [8/21], [94mLoss[0m : 2.41783
[1mStep[0m  [10/21], [94mLoss[0m : 2.43913
[1mStep[0m  [12/21], [94mLoss[0m : 2.44909
[1mStep[0m  [14/21], [94mLoss[0m : 2.53812
[1mStep[0m  [16/21], [94mLoss[0m : 2.55804
[1mStep[0m  [18/21], [94mLoss[0m : 2.54718
[1mStep[0m  [20/21], [94mLoss[0m : 2.61200

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60192
[1mStep[0m  [2/21], [94mLoss[0m : 2.54764
[1mStep[0m  [4/21], [94mLoss[0m : 2.33935
[1mStep[0m  [6/21], [94mLoss[0m : 2.42347
[1mStep[0m  [8/21], [94mLoss[0m : 2.45626
[1mStep[0m  [10/21], [94mLoss[0m : 2.52801
[1mStep[0m  [12/21], [94mLoss[0m : 2.73488
[1mStep[0m  [14/21], [94mLoss[0m : 2.30803
[1mStep[0m  [16/21], [94mLoss[0m : 2.70889
[1mStep[0m  [18/21], [94mLoss[0m : 2.41463
[1mStep[0m  [20/21], [94mLoss[0m : 2.59341

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61627
[1mStep[0m  [2/21], [94mLoss[0m : 2.68370
[1mStep[0m  [4/21], [94mLoss[0m : 2.47182
[1mStep[0m  [6/21], [94mLoss[0m : 2.47511
[1mStep[0m  [8/21], [94mLoss[0m : 2.46506
[1mStep[0m  [10/21], [94mLoss[0m : 2.51520
[1mStep[0m  [12/21], [94mLoss[0m : 2.52261
[1mStep[0m  [14/21], [94mLoss[0m : 2.57454
[1mStep[0m  [16/21], [94mLoss[0m : 2.61566
[1mStep[0m  [18/21], [94mLoss[0m : 2.50788
[1mStep[0m  [20/21], [94mLoss[0m : 2.55113

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61868
[1mStep[0m  [2/21], [94mLoss[0m : 2.54407
[1mStep[0m  [4/21], [94mLoss[0m : 2.66522
[1mStep[0m  [6/21], [94mLoss[0m : 2.61498
[1mStep[0m  [8/21], [94mLoss[0m : 2.48242
[1mStep[0m  [10/21], [94mLoss[0m : 2.51483
[1mStep[0m  [12/21], [94mLoss[0m : 2.53822
[1mStep[0m  [14/21], [94mLoss[0m : 2.55747
[1mStep[0m  [16/21], [94mLoss[0m : 2.52599
[1mStep[0m  [18/21], [94mLoss[0m : 2.54999
[1mStep[0m  [20/21], [94mLoss[0m : 2.49491

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61828
[1mStep[0m  [2/21], [94mLoss[0m : 2.66108
[1mStep[0m  [4/21], [94mLoss[0m : 2.67939
[1mStep[0m  [6/21], [94mLoss[0m : 2.46820
[1mStep[0m  [8/21], [94mLoss[0m : 2.47691
[1mStep[0m  [10/21], [94mLoss[0m : 2.37485
[1mStep[0m  [12/21], [94mLoss[0m : 2.53484
[1mStep[0m  [14/21], [94mLoss[0m : 2.51101
[1mStep[0m  [16/21], [94mLoss[0m : 2.54898
[1mStep[0m  [18/21], [94mLoss[0m : 2.49328
[1mStep[0m  [20/21], [94mLoss[0m : 2.36089

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50152
[1mStep[0m  [2/21], [94mLoss[0m : 2.42588
[1mStep[0m  [4/21], [94mLoss[0m : 2.59870
[1mStep[0m  [6/21], [94mLoss[0m : 2.56612
[1mStep[0m  [8/21], [94mLoss[0m : 2.61440
[1mStep[0m  [10/21], [94mLoss[0m : 2.57545
[1mStep[0m  [12/21], [94mLoss[0m : 2.50063
[1mStep[0m  [14/21], [94mLoss[0m : 2.48091
[1mStep[0m  [16/21], [94mLoss[0m : 2.58402
[1mStep[0m  [18/21], [94mLoss[0m : 2.36978
[1mStep[0m  [20/21], [94mLoss[0m : 2.54754

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65957
[1mStep[0m  [2/21], [94mLoss[0m : 2.50221
[1mStep[0m  [4/21], [94mLoss[0m : 2.59977
[1mStep[0m  [6/21], [94mLoss[0m : 2.48496
[1mStep[0m  [8/21], [94mLoss[0m : 2.34484
[1mStep[0m  [10/21], [94mLoss[0m : 2.51118
[1mStep[0m  [12/21], [94mLoss[0m : 2.62679
[1mStep[0m  [14/21], [94mLoss[0m : 2.53150
[1mStep[0m  [16/21], [94mLoss[0m : 2.55260
[1mStep[0m  [18/21], [94mLoss[0m : 2.57683
[1mStep[0m  [20/21], [94mLoss[0m : 2.45890

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61313
[1mStep[0m  [2/21], [94mLoss[0m : 2.51914
[1mStep[0m  [4/21], [94mLoss[0m : 2.54551
[1mStep[0m  [6/21], [94mLoss[0m : 2.54782
[1mStep[0m  [8/21], [94mLoss[0m : 2.41517
[1mStep[0m  [10/21], [94mLoss[0m : 2.55945
[1mStep[0m  [12/21], [94mLoss[0m : 2.40519
[1mStep[0m  [14/21], [94mLoss[0m : 2.55753
[1mStep[0m  [16/21], [94mLoss[0m : 2.73741
[1mStep[0m  [18/21], [94mLoss[0m : 2.60741
[1mStep[0m  [20/21], [94mLoss[0m : 2.51831

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60678
[1mStep[0m  [2/21], [94mLoss[0m : 2.50766
[1mStep[0m  [4/21], [94mLoss[0m : 2.56888
[1mStep[0m  [6/21], [94mLoss[0m : 2.52823
[1mStep[0m  [8/21], [94mLoss[0m : 2.69232
[1mStep[0m  [10/21], [94mLoss[0m : 2.63838
[1mStep[0m  [12/21], [94mLoss[0m : 2.57016
[1mStep[0m  [14/21], [94mLoss[0m : 2.42488
[1mStep[0m  [16/21], [94mLoss[0m : 2.56427
[1mStep[0m  [18/21], [94mLoss[0m : 2.70240
[1mStep[0m  [20/21], [94mLoss[0m : 2.31307

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49107
[1mStep[0m  [2/21], [94mLoss[0m : 2.51888
[1mStep[0m  [4/21], [94mLoss[0m : 2.56289
[1mStep[0m  [6/21], [94mLoss[0m : 2.58769
[1mStep[0m  [8/21], [94mLoss[0m : 2.69048
[1mStep[0m  [10/21], [94mLoss[0m : 2.57191
[1mStep[0m  [12/21], [94mLoss[0m : 2.62293
[1mStep[0m  [14/21], [94mLoss[0m : 2.52231
[1mStep[0m  [16/21], [94mLoss[0m : 2.56101
[1mStep[0m  [18/21], [94mLoss[0m : 2.68083
[1mStep[0m  [20/21], [94mLoss[0m : 2.53072

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44181
[1mStep[0m  [2/21], [94mLoss[0m : 2.56390
[1mStep[0m  [4/21], [94mLoss[0m : 2.50862
[1mStep[0m  [6/21], [94mLoss[0m : 2.48710
[1mStep[0m  [8/21], [94mLoss[0m : 2.59223
[1mStep[0m  [10/21], [94mLoss[0m : 2.61570
[1mStep[0m  [12/21], [94mLoss[0m : 2.58193
[1mStep[0m  [14/21], [94mLoss[0m : 2.61845
[1mStep[0m  [16/21], [94mLoss[0m : 2.51904
[1mStep[0m  [18/21], [94mLoss[0m : 2.46317
[1mStep[0m  [20/21], [94mLoss[0m : 2.55389

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54059
[1mStep[0m  [2/21], [94mLoss[0m : 2.56920
[1mStep[0m  [4/21], [94mLoss[0m : 2.56588
[1mStep[0m  [6/21], [94mLoss[0m : 2.63648
[1mStep[0m  [8/21], [94mLoss[0m : 2.51083
[1mStep[0m  [10/21], [94mLoss[0m : 2.54469
[1mStep[0m  [12/21], [94mLoss[0m : 2.70669
[1mStep[0m  [14/21], [94mLoss[0m : 2.50965
[1mStep[0m  [16/21], [94mLoss[0m : 2.56939
[1mStep[0m  [18/21], [94mLoss[0m : 2.32626
[1mStep[0m  [20/21], [94mLoss[0m : 2.45716

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38441
[1mStep[0m  [2/21], [94mLoss[0m : 2.65531
[1mStep[0m  [4/21], [94mLoss[0m : 2.55842
[1mStep[0m  [6/21], [94mLoss[0m : 2.61882
[1mStep[0m  [8/21], [94mLoss[0m : 2.57586
[1mStep[0m  [10/21], [94mLoss[0m : 2.43229
[1mStep[0m  [12/21], [94mLoss[0m : 2.53979
[1mStep[0m  [14/21], [94mLoss[0m : 2.54766
[1mStep[0m  [16/21], [94mLoss[0m : 2.42034
[1mStep[0m  [18/21], [94mLoss[0m : 2.58232
[1mStep[0m  [20/21], [94mLoss[0m : 2.56518

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47289
[1mStep[0m  [2/21], [94mLoss[0m : 2.36982
[1mStep[0m  [4/21], [94mLoss[0m : 2.55763
[1mStep[0m  [6/21], [94mLoss[0m : 2.72107
[1mStep[0m  [8/21], [94mLoss[0m : 2.42519
[1mStep[0m  [10/21], [94mLoss[0m : 2.52458
[1mStep[0m  [12/21], [94mLoss[0m : 2.59393
[1mStep[0m  [14/21], [94mLoss[0m : 2.56720
[1mStep[0m  [16/21], [94mLoss[0m : 2.42847
[1mStep[0m  [18/21], [94mLoss[0m : 2.32974
[1mStep[0m  [20/21], [94mLoss[0m : 2.57104

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50659
[1mStep[0m  [2/21], [94mLoss[0m : 2.60303
[1mStep[0m  [4/21], [94mLoss[0m : 2.40399
[1mStep[0m  [6/21], [94mLoss[0m : 2.46907
[1mStep[0m  [8/21], [94mLoss[0m : 2.46383
[1mStep[0m  [10/21], [94mLoss[0m : 2.38590
[1mStep[0m  [12/21], [94mLoss[0m : 2.60358
[1mStep[0m  [14/21], [94mLoss[0m : 2.50225
[1mStep[0m  [16/21], [94mLoss[0m : 2.54313
[1mStep[0m  [18/21], [94mLoss[0m : 2.57917
[1mStep[0m  [20/21], [94mLoss[0m : 2.41480

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71463
[1mStep[0m  [2/21], [94mLoss[0m : 2.64930
[1mStep[0m  [4/21], [94mLoss[0m : 2.58422
[1mStep[0m  [6/21], [94mLoss[0m : 2.53025
[1mStep[0m  [8/21], [94mLoss[0m : 2.38581
[1mStep[0m  [10/21], [94mLoss[0m : 2.40563
[1mStep[0m  [12/21], [94mLoss[0m : 2.55952
[1mStep[0m  [14/21], [94mLoss[0m : 2.63310
[1mStep[0m  [16/21], [94mLoss[0m : 2.58562
[1mStep[0m  [18/21], [94mLoss[0m : 2.39726
[1mStep[0m  [20/21], [94mLoss[0m : 2.43696

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.336
====================================

Phase 1 - Evaluation MAE:  2.336465631212507
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.53107
[1mStep[0m  [2/21], [94mLoss[0m : 2.42832
[1mStep[0m  [4/21], [94mLoss[0m : 2.42060
[1mStep[0m  [6/21], [94mLoss[0m : 2.50166
[1mStep[0m  [8/21], [94mLoss[0m : 2.53875
[1mStep[0m  [10/21], [94mLoss[0m : 2.43041
[1mStep[0m  [12/21], [94mLoss[0m : 2.67117
[1mStep[0m  [14/21], [94mLoss[0m : 2.56173
[1mStep[0m  [16/21], [94mLoss[0m : 2.77161
[1mStep[0m  [18/21], [94mLoss[0m : 2.37945
[1mStep[0m  [20/21], [94mLoss[0m : 2.55366

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52268
[1mStep[0m  [2/21], [94mLoss[0m : 2.56189
[1mStep[0m  [4/21], [94mLoss[0m : 2.57456
[1mStep[0m  [6/21], [94mLoss[0m : 2.66498
[1mStep[0m  [8/21], [94mLoss[0m : 2.52246
[1mStep[0m  [10/21], [94mLoss[0m : 2.42540
[1mStep[0m  [12/21], [94mLoss[0m : 2.43451
[1mStep[0m  [14/21], [94mLoss[0m : 2.45705
[1mStep[0m  [16/21], [94mLoss[0m : 2.49255
[1mStep[0m  [18/21], [94mLoss[0m : 2.42711
[1mStep[0m  [20/21], [94mLoss[0m : 2.32726

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52777
[1mStep[0m  [2/21], [94mLoss[0m : 2.43737
[1mStep[0m  [4/21], [94mLoss[0m : 2.34547
[1mStep[0m  [6/21], [94mLoss[0m : 2.39976
[1mStep[0m  [8/21], [94mLoss[0m : 2.44513
[1mStep[0m  [10/21], [94mLoss[0m : 2.59363
[1mStep[0m  [12/21], [94mLoss[0m : 2.38442
[1mStep[0m  [14/21], [94mLoss[0m : 2.54800
[1mStep[0m  [16/21], [94mLoss[0m : 2.46492
[1mStep[0m  [18/21], [94mLoss[0m : 2.47885
[1mStep[0m  [20/21], [94mLoss[0m : 2.45966

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54653
[1mStep[0m  [2/21], [94mLoss[0m : 2.44471
[1mStep[0m  [4/21], [94mLoss[0m : 2.44629
[1mStep[0m  [6/21], [94mLoss[0m : 2.48128
[1mStep[0m  [8/21], [94mLoss[0m : 2.35050
[1mStep[0m  [10/21], [94mLoss[0m : 2.44579
[1mStep[0m  [12/21], [94mLoss[0m : 2.50256
[1mStep[0m  [14/21], [94mLoss[0m : 2.42798
[1mStep[0m  [16/21], [94mLoss[0m : 2.59900
[1mStep[0m  [18/21], [94mLoss[0m : 2.65092
[1mStep[0m  [20/21], [94mLoss[0m : 2.32127

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.484, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47943
[1mStep[0m  [2/21], [94mLoss[0m : 2.43164
[1mStep[0m  [4/21], [94mLoss[0m : 2.43300
[1mStep[0m  [6/21], [94mLoss[0m : 2.54656
[1mStep[0m  [8/21], [94mLoss[0m : 2.46570
[1mStep[0m  [10/21], [94mLoss[0m : 2.31591
[1mStep[0m  [12/21], [94mLoss[0m : 2.42693
[1mStep[0m  [14/21], [94mLoss[0m : 2.53450
[1mStep[0m  [16/21], [94mLoss[0m : 2.38418
[1mStep[0m  [18/21], [94mLoss[0m : 2.45726
[1mStep[0m  [20/21], [94mLoss[0m : 2.56809

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.665, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.32177
[1mStep[0m  [2/21], [94mLoss[0m : 2.33442
[1mStep[0m  [4/21], [94mLoss[0m : 2.38951
[1mStep[0m  [6/21], [94mLoss[0m : 2.52483
[1mStep[0m  [8/21], [94mLoss[0m : 2.40552
[1mStep[0m  [10/21], [94mLoss[0m : 2.37666
[1mStep[0m  [12/21], [94mLoss[0m : 2.51191
[1mStep[0m  [14/21], [94mLoss[0m : 2.48788
[1mStep[0m  [16/21], [94mLoss[0m : 2.38677
[1mStep[0m  [18/21], [94mLoss[0m : 2.39012
[1mStep[0m  [20/21], [94mLoss[0m : 2.19184

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.760, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.20586
[1mStep[0m  [2/21], [94mLoss[0m : 2.38264
[1mStep[0m  [4/21], [94mLoss[0m : 2.35241
[1mStep[0m  [6/21], [94mLoss[0m : 2.44590
[1mStep[0m  [8/21], [94mLoss[0m : 2.56551
[1mStep[0m  [10/21], [94mLoss[0m : 2.31480
[1mStep[0m  [12/21], [94mLoss[0m : 2.45418
[1mStep[0m  [14/21], [94mLoss[0m : 2.41022
[1mStep[0m  [16/21], [94mLoss[0m : 2.37466
[1mStep[0m  [18/21], [94mLoss[0m : 2.37686
[1mStep[0m  [20/21], [94mLoss[0m : 2.29160

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.712, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39541
[1mStep[0m  [2/21], [94mLoss[0m : 2.22262
[1mStep[0m  [4/21], [94mLoss[0m : 2.36056
[1mStep[0m  [6/21], [94mLoss[0m : 2.39877
[1mStep[0m  [8/21], [94mLoss[0m : 2.39761
[1mStep[0m  [10/21], [94mLoss[0m : 2.27409
[1mStep[0m  [12/21], [94mLoss[0m : 2.44659
[1mStep[0m  [14/21], [94mLoss[0m : 2.32003
[1mStep[0m  [16/21], [94mLoss[0m : 2.37953
[1mStep[0m  [18/21], [94mLoss[0m : 2.42170
[1mStep[0m  [20/21], [94mLoss[0m : 2.37873

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.752, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.38665
[1mStep[0m  [2/21], [94mLoss[0m : 2.36687
[1mStep[0m  [4/21], [94mLoss[0m : 2.33419
[1mStep[0m  [6/21], [94mLoss[0m : 2.32418
[1mStep[0m  [8/21], [94mLoss[0m : 2.28797
[1mStep[0m  [10/21], [94mLoss[0m : 2.41984
[1mStep[0m  [12/21], [94mLoss[0m : 2.37803
[1mStep[0m  [14/21], [94mLoss[0m : 2.29328
[1mStep[0m  [16/21], [94mLoss[0m : 2.34208
[1mStep[0m  [18/21], [94mLoss[0m : 2.57886
[1mStep[0m  [20/21], [94mLoss[0m : 2.36035

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.662, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23648
[1mStep[0m  [2/21], [94mLoss[0m : 2.26859
[1mStep[0m  [4/21], [94mLoss[0m : 2.30403
[1mStep[0m  [6/21], [94mLoss[0m : 2.24380
[1mStep[0m  [8/21], [94mLoss[0m : 2.21371
[1mStep[0m  [10/21], [94mLoss[0m : 2.30111
[1mStep[0m  [12/21], [94mLoss[0m : 2.28254
[1mStep[0m  [14/21], [94mLoss[0m : 2.26192
[1mStep[0m  [16/21], [94mLoss[0m : 2.32939
[1mStep[0m  [18/21], [94mLoss[0m : 2.24714
[1mStep[0m  [20/21], [94mLoss[0m : 2.32045

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.613, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.24560
[1mStep[0m  [2/21], [94mLoss[0m : 2.33390
[1mStep[0m  [4/21], [94mLoss[0m : 2.23576
[1mStep[0m  [6/21], [94mLoss[0m : 2.15788
[1mStep[0m  [8/21], [94mLoss[0m : 2.27127
[1mStep[0m  [10/21], [94mLoss[0m : 2.19734
[1mStep[0m  [12/21], [94mLoss[0m : 2.29403
[1mStep[0m  [14/21], [94mLoss[0m : 2.24085
[1mStep[0m  [16/21], [94mLoss[0m : 2.10829
[1mStep[0m  [18/21], [94mLoss[0m : 2.30527
[1mStep[0m  [20/21], [94mLoss[0m : 2.20714

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.701, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.16424
[1mStep[0m  [2/21], [94mLoss[0m : 2.14047
[1mStep[0m  [4/21], [94mLoss[0m : 2.23785
[1mStep[0m  [6/21], [94mLoss[0m : 2.22664
[1mStep[0m  [8/21], [94mLoss[0m : 2.16709
[1mStep[0m  [10/21], [94mLoss[0m : 2.18735
[1mStep[0m  [12/21], [94mLoss[0m : 2.27552
[1mStep[0m  [14/21], [94mLoss[0m : 2.15246
[1mStep[0m  [16/21], [94mLoss[0m : 2.16874
[1mStep[0m  [18/21], [94mLoss[0m : 2.26297
[1mStep[0m  [20/21], [94mLoss[0m : 1.98445

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.591, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.23394
[1mStep[0m  [2/21], [94mLoss[0m : 2.12708
[1mStep[0m  [4/21], [94mLoss[0m : 2.07919
[1mStep[0m  [6/21], [94mLoss[0m : 2.16041
[1mStep[0m  [8/21], [94mLoss[0m : 2.24591
[1mStep[0m  [10/21], [94mLoss[0m : 2.25437
[1mStep[0m  [12/21], [94mLoss[0m : 2.09410
[1mStep[0m  [14/21], [94mLoss[0m : 2.22965
[1mStep[0m  [16/21], [94mLoss[0m : 2.10096
[1mStep[0m  [18/21], [94mLoss[0m : 2.20708
[1mStep[0m  [20/21], [94mLoss[0m : 2.13682

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.628, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.05292
[1mStep[0m  [2/21], [94mLoss[0m : 2.25195
[1mStep[0m  [4/21], [94mLoss[0m : 2.15916
[1mStep[0m  [6/21], [94mLoss[0m : 2.09636
[1mStep[0m  [8/21], [94mLoss[0m : 2.17845
[1mStep[0m  [10/21], [94mLoss[0m : 2.24181
[1mStep[0m  [12/21], [94mLoss[0m : 2.10547
[1mStep[0m  [14/21], [94mLoss[0m : 1.98787
[1mStep[0m  [16/21], [94mLoss[0m : 2.16239
[1mStep[0m  [18/21], [94mLoss[0m : 2.15581
[1mStep[0m  [20/21], [94mLoss[0m : 2.12875

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.685, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.13102
[1mStep[0m  [2/21], [94mLoss[0m : 2.07704
[1mStep[0m  [4/21], [94mLoss[0m : 2.22984
[1mStep[0m  [6/21], [94mLoss[0m : 2.09229
[1mStep[0m  [8/21], [94mLoss[0m : 2.07817
[1mStep[0m  [10/21], [94mLoss[0m : 2.18317
[1mStep[0m  [12/21], [94mLoss[0m : 2.11701
[1mStep[0m  [14/21], [94mLoss[0m : 2.11663
[1mStep[0m  [16/21], [94mLoss[0m : 2.05893
[1mStep[0m  [18/21], [94mLoss[0m : 2.11031
[1mStep[0m  [20/21], [94mLoss[0m : 2.10115

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.590, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.10816
[1mStep[0m  [2/21], [94mLoss[0m : 2.07910
[1mStep[0m  [4/21], [94mLoss[0m : 2.08885
[1mStep[0m  [6/21], [94mLoss[0m : 2.18574
[1mStep[0m  [8/21], [94mLoss[0m : 2.02060
[1mStep[0m  [10/21], [94mLoss[0m : 2.15427
[1mStep[0m  [12/21], [94mLoss[0m : 2.12034
[1mStep[0m  [14/21], [94mLoss[0m : 2.15189
[1mStep[0m  [16/21], [94mLoss[0m : 2.08421
[1mStep[0m  [18/21], [94mLoss[0m : 1.97150
[1mStep[0m  [20/21], [94mLoss[0m : 2.06785

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.635, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.91502
[1mStep[0m  [2/21], [94mLoss[0m : 2.01135
[1mStep[0m  [4/21], [94mLoss[0m : 2.02634
[1mStep[0m  [6/21], [94mLoss[0m : 2.09843
[1mStep[0m  [8/21], [94mLoss[0m : 1.97858
[1mStep[0m  [10/21], [94mLoss[0m : 1.99612
[1mStep[0m  [12/21], [94mLoss[0m : 2.09775
[1mStep[0m  [14/21], [94mLoss[0m : 2.03815
[1mStep[0m  [16/21], [94mLoss[0m : 1.92263
[1mStep[0m  [18/21], [94mLoss[0m : 2.01843
[1mStep[0m  [20/21], [94mLoss[0m : 2.06831

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.553, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.06857
[1mStep[0m  [2/21], [94mLoss[0m : 1.99351
[1mStep[0m  [4/21], [94mLoss[0m : 2.01635
[1mStep[0m  [6/21], [94mLoss[0m : 1.93428
[1mStep[0m  [8/21], [94mLoss[0m : 1.96326
[1mStep[0m  [10/21], [94mLoss[0m : 2.04725
[1mStep[0m  [12/21], [94mLoss[0m : 2.02828
[1mStep[0m  [14/21], [94mLoss[0m : 2.03602
[1mStep[0m  [16/21], [94mLoss[0m : 1.88152
[1mStep[0m  [18/21], [94mLoss[0m : 2.06647
[1mStep[0m  [20/21], [94mLoss[0m : 1.98389

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.604, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90823
[1mStep[0m  [2/21], [94mLoss[0m : 1.80108
[1mStep[0m  [4/21], [94mLoss[0m : 2.02517
[1mStep[0m  [6/21], [94mLoss[0m : 1.90326
[1mStep[0m  [8/21], [94mLoss[0m : 1.91379
[1mStep[0m  [10/21], [94mLoss[0m : 1.96300
[1mStep[0m  [12/21], [94mLoss[0m : 1.79907
[1mStep[0m  [14/21], [94mLoss[0m : 1.89635
[1mStep[0m  [16/21], [94mLoss[0m : 1.98458
[1mStep[0m  [18/21], [94mLoss[0m : 1.90221
[1mStep[0m  [20/21], [94mLoss[0m : 1.89382

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.597, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.90150
[1mStep[0m  [2/21], [94mLoss[0m : 1.85553
[1mStep[0m  [4/21], [94mLoss[0m : 2.03177
[1mStep[0m  [6/21], [94mLoss[0m : 1.89829
[1mStep[0m  [8/21], [94mLoss[0m : 1.78974
[1mStep[0m  [10/21], [94mLoss[0m : 1.80867
[1mStep[0m  [12/21], [94mLoss[0m : 1.89605
[1mStep[0m  [14/21], [94mLoss[0m : 1.82717
[1mStep[0m  [16/21], [94mLoss[0m : 1.81571
[1mStep[0m  [18/21], [94mLoss[0m : 1.92677
[1mStep[0m  [20/21], [94mLoss[0m : 1.87129

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.912, [92mTest[0m: 2.585, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.92249
[1mStep[0m  [2/21], [94mLoss[0m : 1.91329
[1mStep[0m  [4/21], [94mLoss[0m : 1.88895
[1mStep[0m  [6/21], [94mLoss[0m : 1.82093
[1mStep[0m  [8/21], [94mLoss[0m : 1.78305
[1mStep[0m  [10/21], [94mLoss[0m : 1.90696
[1mStep[0m  [12/21], [94mLoss[0m : 1.96149
[1mStep[0m  [14/21], [94mLoss[0m : 1.86670
[1mStep[0m  [16/21], [94mLoss[0m : 1.82884
[1mStep[0m  [18/21], [94mLoss[0m : 1.95592
[1mStep[0m  [20/21], [94mLoss[0m : 2.05551

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.78842
[1mStep[0m  [2/21], [94mLoss[0m : 1.83621
[1mStep[0m  [4/21], [94mLoss[0m : 1.66303
[1mStep[0m  [6/21], [94mLoss[0m : 1.78930
[1mStep[0m  [8/21], [94mLoss[0m : 1.87958
[1mStep[0m  [10/21], [94mLoss[0m : 1.81693
[1mStep[0m  [12/21], [94mLoss[0m : 1.85162
[1mStep[0m  [14/21], [94mLoss[0m : 1.85654
[1mStep[0m  [16/21], [94mLoss[0m : 1.98412
[1mStep[0m  [18/21], [94mLoss[0m : 1.90609
[1mStep[0m  [20/21], [94mLoss[0m : 1.85759

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.83702
[1mStep[0m  [2/21], [94mLoss[0m : 1.89548
[1mStep[0m  [4/21], [94mLoss[0m : 1.82861
[1mStep[0m  [6/21], [94mLoss[0m : 1.70447
[1mStep[0m  [8/21], [94mLoss[0m : 1.84187
[1mStep[0m  [10/21], [94mLoss[0m : 1.85981
[1mStep[0m  [12/21], [94mLoss[0m : 1.74665
[1mStep[0m  [14/21], [94mLoss[0m : 1.82754
[1mStep[0m  [16/21], [94mLoss[0m : 1.82865
[1mStep[0m  [18/21], [94mLoss[0m : 1.70109
[1mStep[0m  [20/21], [94mLoss[0m : 1.93520

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.86363
[1mStep[0m  [2/21], [94mLoss[0m : 1.74430
[1mStep[0m  [4/21], [94mLoss[0m : 1.69697
[1mStep[0m  [6/21], [94mLoss[0m : 1.79101
[1mStep[0m  [8/21], [94mLoss[0m : 1.77209
[1mStep[0m  [10/21], [94mLoss[0m : 1.77778
[1mStep[0m  [12/21], [94mLoss[0m : 1.80158
[1mStep[0m  [14/21], [94mLoss[0m : 1.81178
[1mStep[0m  [16/21], [94mLoss[0m : 1.82675
[1mStep[0m  [18/21], [94mLoss[0m : 1.84235
[1mStep[0m  [20/21], [94mLoss[0m : 1.91867

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.580, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.71838
[1mStep[0m  [2/21], [94mLoss[0m : 1.72102
[1mStep[0m  [4/21], [94mLoss[0m : 1.88218
[1mStep[0m  [6/21], [94mLoss[0m : 1.72127
[1mStep[0m  [8/21], [94mLoss[0m : 1.85091
[1mStep[0m  [10/21], [94mLoss[0m : 1.74280
[1mStep[0m  [12/21], [94mLoss[0m : 1.77246
[1mStep[0m  [14/21], [94mLoss[0m : 1.75429
[1mStep[0m  [16/21], [94mLoss[0m : 1.84254
[1mStep[0m  [18/21], [94mLoss[0m : 1.69530
[1mStep[0m  [20/21], [94mLoss[0m : 1.76130

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.526, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.72625
[1mStep[0m  [2/21], [94mLoss[0m : 1.78881
[1mStep[0m  [4/21], [94mLoss[0m : 1.79519
[1mStep[0m  [6/21], [94mLoss[0m : 1.81834
[1mStep[0m  [8/21], [94mLoss[0m : 1.72050
[1mStep[0m  [10/21], [94mLoss[0m : 1.65026
[1mStep[0m  [12/21], [94mLoss[0m : 1.64687
[1mStep[0m  [14/21], [94mLoss[0m : 1.75759
[1mStep[0m  [16/21], [94mLoss[0m : 1.80302
[1mStep[0m  [18/21], [94mLoss[0m : 1.92355
[1mStep[0m  [20/21], [94mLoss[0m : 1.81990

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.74684
[1mStep[0m  [2/21], [94mLoss[0m : 1.73302
[1mStep[0m  [4/21], [94mLoss[0m : 1.75850
[1mStep[0m  [6/21], [94mLoss[0m : 1.61723
[1mStep[0m  [8/21], [94mLoss[0m : 1.69791
[1mStep[0m  [10/21], [94mLoss[0m : 1.69384
[1mStep[0m  [12/21], [94mLoss[0m : 1.76334
[1mStep[0m  [14/21], [94mLoss[0m : 1.74954
[1mStep[0m  [16/21], [94mLoss[0m : 1.86613
[1mStep[0m  [18/21], [94mLoss[0m : 1.62810
[1mStep[0m  [20/21], [94mLoss[0m : 1.86174

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.666, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.69394
[1mStep[0m  [2/21], [94mLoss[0m : 1.68200
[1mStep[0m  [4/21], [94mLoss[0m : 1.75890
[1mStep[0m  [6/21], [94mLoss[0m : 1.66813
[1mStep[0m  [8/21], [94mLoss[0m : 1.69723
[1mStep[0m  [10/21], [94mLoss[0m : 1.56374
[1mStep[0m  [12/21], [94mLoss[0m : 1.65554
[1mStep[0m  [14/21], [94mLoss[0m : 1.73415
[1mStep[0m  [16/21], [94mLoss[0m : 1.67641
[1mStep[0m  [18/21], [94mLoss[0m : 1.74120
[1mStep[0m  [20/21], [94mLoss[0m : 1.72738

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.627, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.56733
[1mStep[0m  [2/21], [94mLoss[0m : 1.58243
[1mStep[0m  [4/21], [94mLoss[0m : 1.66125
[1mStep[0m  [6/21], [94mLoss[0m : 1.75273
[1mStep[0m  [8/21], [94mLoss[0m : 1.67898
[1mStep[0m  [10/21], [94mLoss[0m : 1.57455
[1mStep[0m  [12/21], [94mLoss[0m : 1.63995
[1mStep[0m  [14/21], [94mLoss[0m : 1.63952
[1mStep[0m  [16/21], [94mLoss[0m : 1.74933
[1mStep[0m  [18/21], [94mLoss[0m : 1.70948
[1mStep[0m  [20/21], [94mLoss[0m : 1.61841

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.522, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 1.62498
[1mStep[0m  [2/21], [94mLoss[0m : 1.62606
[1mStep[0m  [4/21], [94mLoss[0m : 1.65894
[1mStep[0m  [6/21], [94mLoss[0m : 1.66848
[1mStep[0m  [8/21], [94mLoss[0m : 1.53316
[1mStep[0m  [10/21], [94mLoss[0m : 1.68502
[1mStep[0m  [12/21], [94mLoss[0m : 1.68995
[1mStep[0m  [14/21], [94mLoss[0m : 1.60780
[1mStep[0m  [16/21], [94mLoss[0m : 1.63848
[1mStep[0m  [18/21], [94mLoss[0m : 1.55326
[1mStep[0m  [20/21], [94mLoss[0m : 1.57599

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.584, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.616
====================================

Phase 2 - Evaluation MAE:  2.6164278302873885
MAE score P1       2.336466
MAE score P2       2.616428
loss               1.649929
learning_rate      0.002575
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay          0.001
Name: 1, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.47388
[1mStep[0m  [33/339], [94mLoss[0m : 5.93891
[1mStep[0m  [66/339], [94mLoss[0m : 3.40771
[1mStep[0m  [99/339], [94mLoss[0m : 3.87022
[1mStep[0m  [132/339], [94mLoss[0m : 2.73605
[1mStep[0m  [165/339], [94mLoss[0m : 2.95742
[1mStep[0m  [198/339], [94mLoss[0m : 2.92612
[1mStep[0m  [231/339], [94mLoss[0m : 3.26760
[1mStep[0m  [264/339], [94mLoss[0m : 2.66356
[1mStep[0m  [297/339], [94mLoss[0m : 2.62760
[1mStep[0m  [330/339], [94mLoss[0m : 2.86803

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.421, [92mTest[0m: 10.925, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95142
[1mStep[0m  [33/339], [94mLoss[0m : 2.27048
[1mStep[0m  [66/339], [94mLoss[0m : 3.04918
[1mStep[0m  [99/339], [94mLoss[0m : 2.89274
[1mStep[0m  [132/339], [94mLoss[0m : 2.43442
[1mStep[0m  [165/339], [94mLoss[0m : 2.37411
[1mStep[0m  [198/339], [94mLoss[0m : 2.90801
[1mStep[0m  [231/339], [94mLoss[0m : 3.11561
[1mStep[0m  [264/339], [94mLoss[0m : 2.42914
[1mStep[0m  [297/339], [94mLoss[0m : 2.48956
[1mStep[0m  [330/339], [94mLoss[0m : 2.87638

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26187
[1mStep[0m  [33/339], [94mLoss[0m : 1.91825
[1mStep[0m  [66/339], [94mLoss[0m : 2.70609
[1mStep[0m  [99/339], [94mLoss[0m : 2.51670
[1mStep[0m  [132/339], [94mLoss[0m : 2.52548
[1mStep[0m  [165/339], [94mLoss[0m : 3.06656
[1mStep[0m  [198/339], [94mLoss[0m : 2.48832
[1mStep[0m  [231/339], [94mLoss[0m : 2.98698
[1mStep[0m  [264/339], [94mLoss[0m : 2.78487
[1mStep[0m  [297/339], [94mLoss[0m : 3.13310
[1mStep[0m  [330/339], [94mLoss[0m : 2.48040

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23494
[1mStep[0m  [33/339], [94mLoss[0m : 2.49452
[1mStep[0m  [66/339], [94mLoss[0m : 2.44965
[1mStep[0m  [99/339], [94mLoss[0m : 2.70756
[1mStep[0m  [132/339], [94mLoss[0m : 1.99966
[1mStep[0m  [165/339], [94mLoss[0m : 2.31849
[1mStep[0m  [198/339], [94mLoss[0m : 3.04578
[1mStep[0m  [231/339], [94mLoss[0m : 2.55463
[1mStep[0m  [264/339], [94mLoss[0m : 2.39550
[1mStep[0m  [297/339], [94mLoss[0m : 2.19355
[1mStep[0m  [330/339], [94mLoss[0m : 3.19647

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.98599
[1mStep[0m  [33/339], [94mLoss[0m : 2.78870
[1mStep[0m  [66/339], [94mLoss[0m : 2.38600
[1mStep[0m  [99/339], [94mLoss[0m : 2.13142
[1mStep[0m  [132/339], [94mLoss[0m : 2.14452
[1mStep[0m  [165/339], [94mLoss[0m : 2.65813
[1mStep[0m  [198/339], [94mLoss[0m : 2.32353
[1mStep[0m  [231/339], [94mLoss[0m : 2.39435
[1mStep[0m  [264/339], [94mLoss[0m : 2.46386
[1mStep[0m  [297/339], [94mLoss[0m : 2.33130
[1mStep[0m  [330/339], [94mLoss[0m : 2.31328

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73119
[1mStep[0m  [33/339], [94mLoss[0m : 2.61911
[1mStep[0m  [66/339], [94mLoss[0m : 2.17635
[1mStep[0m  [99/339], [94mLoss[0m : 2.76250
[1mStep[0m  [132/339], [94mLoss[0m : 2.88355
[1mStep[0m  [165/339], [94mLoss[0m : 2.53193
[1mStep[0m  [198/339], [94mLoss[0m : 2.49922
[1mStep[0m  [231/339], [94mLoss[0m : 2.64577
[1mStep[0m  [264/339], [94mLoss[0m : 2.52338
[1mStep[0m  [297/339], [94mLoss[0m : 2.31671
[1mStep[0m  [330/339], [94mLoss[0m : 2.50914

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61827
[1mStep[0m  [33/339], [94mLoss[0m : 3.04099
[1mStep[0m  [66/339], [94mLoss[0m : 2.29282
[1mStep[0m  [99/339], [94mLoss[0m : 2.22202
[1mStep[0m  [132/339], [94mLoss[0m : 2.64302
[1mStep[0m  [165/339], [94mLoss[0m : 2.03479
[1mStep[0m  [198/339], [94mLoss[0m : 2.66138
[1mStep[0m  [231/339], [94mLoss[0m : 2.83801
[1mStep[0m  [264/339], [94mLoss[0m : 2.61234
[1mStep[0m  [297/339], [94mLoss[0m : 2.37076
[1mStep[0m  [330/339], [94mLoss[0m : 3.07868

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64426
[1mStep[0m  [33/339], [94mLoss[0m : 2.37003
[1mStep[0m  [66/339], [94mLoss[0m : 2.80561
[1mStep[0m  [99/339], [94mLoss[0m : 2.45534
[1mStep[0m  [132/339], [94mLoss[0m : 2.89063
[1mStep[0m  [165/339], [94mLoss[0m : 2.21071
[1mStep[0m  [198/339], [94mLoss[0m : 3.36934
[1mStep[0m  [231/339], [94mLoss[0m : 2.59682
[1mStep[0m  [264/339], [94mLoss[0m : 2.19130
[1mStep[0m  [297/339], [94mLoss[0m : 3.08411
[1mStep[0m  [330/339], [94mLoss[0m : 3.26156

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48869
[1mStep[0m  [33/339], [94mLoss[0m : 2.01715
[1mStep[0m  [66/339], [94mLoss[0m : 2.50125
[1mStep[0m  [99/339], [94mLoss[0m : 1.70657
[1mStep[0m  [132/339], [94mLoss[0m : 2.92522
[1mStep[0m  [165/339], [94mLoss[0m : 2.82164
[1mStep[0m  [198/339], [94mLoss[0m : 2.86315
[1mStep[0m  [231/339], [94mLoss[0m : 2.78445
[1mStep[0m  [264/339], [94mLoss[0m : 2.06081
[1mStep[0m  [297/339], [94mLoss[0m : 2.38947
[1mStep[0m  [330/339], [94mLoss[0m : 2.78178

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.30909
[1mStep[0m  [33/339], [94mLoss[0m : 1.74200
[1mStep[0m  [66/339], [94mLoss[0m : 2.22284
[1mStep[0m  [99/339], [94mLoss[0m : 2.32543
[1mStep[0m  [132/339], [94mLoss[0m : 1.94293
[1mStep[0m  [165/339], [94mLoss[0m : 2.86403
[1mStep[0m  [198/339], [94mLoss[0m : 2.13389
[1mStep[0m  [231/339], [94mLoss[0m : 2.42636
[1mStep[0m  [264/339], [94mLoss[0m : 2.83030
[1mStep[0m  [297/339], [94mLoss[0m : 2.87906
[1mStep[0m  [330/339], [94mLoss[0m : 2.61474

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.307, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01476
[1mStep[0m  [33/339], [94mLoss[0m : 2.41458
[1mStep[0m  [66/339], [94mLoss[0m : 2.25958
[1mStep[0m  [99/339], [94mLoss[0m : 2.59881
[1mStep[0m  [132/339], [94mLoss[0m : 2.93668
[1mStep[0m  [165/339], [94mLoss[0m : 2.38198
[1mStep[0m  [198/339], [94mLoss[0m : 2.38645
[1mStep[0m  [231/339], [94mLoss[0m : 2.78978
[1mStep[0m  [264/339], [94mLoss[0m : 2.58438
[1mStep[0m  [297/339], [94mLoss[0m : 2.52797
[1mStep[0m  [330/339], [94mLoss[0m : 2.62716

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26005
[1mStep[0m  [33/339], [94mLoss[0m : 1.75960
[1mStep[0m  [66/339], [94mLoss[0m : 2.04235
[1mStep[0m  [99/339], [94mLoss[0m : 2.19408
[1mStep[0m  [132/339], [94mLoss[0m : 2.42057
[1mStep[0m  [165/339], [94mLoss[0m : 2.10263
[1mStep[0m  [198/339], [94mLoss[0m : 2.66392
[1mStep[0m  [231/339], [94mLoss[0m : 1.91205
[1mStep[0m  [264/339], [94mLoss[0m : 2.22093
[1mStep[0m  [297/339], [94mLoss[0m : 2.41386
[1mStep[0m  [330/339], [94mLoss[0m : 2.36836

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92407
[1mStep[0m  [33/339], [94mLoss[0m : 2.12939
[1mStep[0m  [66/339], [94mLoss[0m : 2.33224
[1mStep[0m  [99/339], [94mLoss[0m : 2.70944
[1mStep[0m  [132/339], [94mLoss[0m : 2.56518
[1mStep[0m  [165/339], [94mLoss[0m : 2.16377
[1mStep[0m  [198/339], [94mLoss[0m : 2.41201
[1mStep[0m  [231/339], [94mLoss[0m : 2.52738
[1mStep[0m  [264/339], [94mLoss[0m : 2.12354
[1mStep[0m  [297/339], [94mLoss[0m : 2.33022
[1mStep[0m  [330/339], [94mLoss[0m : 1.94936

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36627
[1mStep[0m  [33/339], [94mLoss[0m : 2.43633
[1mStep[0m  [66/339], [94mLoss[0m : 2.38190
[1mStep[0m  [99/339], [94mLoss[0m : 2.17941
[1mStep[0m  [132/339], [94mLoss[0m : 2.65560
[1mStep[0m  [165/339], [94mLoss[0m : 3.01850
[1mStep[0m  [198/339], [94mLoss[0m : 1.93030
[1mStep[0m  [231/339], [94mLoss[0m : 2.37941
[1mStep[0m  [264/339], [94mLoss[0m : 3.33785
[1mStep[0m  [297/339], [94mLoss[0m : 1.78575
[1mStep[0m  [330/339], [94mLoss[0m : 2.15844

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.306, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87063
[1mStep[0m  [33/339], [94mLoss[0m : 2.44831
[1mStep[0m  [66/339], [94mLoss[0m : 2.28457
[1mStep[0m  [99/339], [94mLoss[0m : 2.34744
[1mStep[0m  [132/339], [94mLoss[0m : 2.97213
[1mStep[0m  [165/339], [94mLoss[0m : 2.16906
[1mStep[0m  [198/339], [94mLoss[0m : 1.81075
[1mStep[0m  [231/339], [94mLoss[0m : 2.40345
[1mStep[0m  [264/339], [94mLoss[0m : 2.30680
[1mStep[0m  [297/339], [94mLoss[0m : 2.42890
[1mStep[0m  [330/339], [94mLoss[0m : 1.83776

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28831
[1mStep[0m  [33/339], [94mLoss[0m : 3.04608
[1mStep[0m  [66/339], [94mLoss[0m : 2.19630
[1mStep[0m  [99/339], [94mLoss[0m : 2.10473
[1mStep[0m  [132/339], [94mLoss[0m : 2.16403
[1mStep[0m  [165/339], [94mLoss[0m : 2.66909
[1mStep[0m  [198/339], [94mLoss[0m : 2.62645
[1mStep[0m  [231/339], [94mLoss[0m : 2.56547
[1mStep[0m  [264/339], [94mLoss[0m : 2.40181
[1mStep[0m  [297/339], [94mLoss[0m : 1.75205
[1mStep[0m  [330/339], [94mLoss[0m : 1.95644

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.89003
[1mStep[0m  [33/339], [94mLoss[0m : 2.57199
[1mStep[0m  [66/339], [94mLoss[0m : 2.87260
[1mStep[0m  [99/339], [94mLoss[0m : 2.76468
[1mStep[0m  [132/339], [94mLoss[0m : 2.36775
[1mStep[0m  [165/339], [94mLoss[0m : 3.04321
[1mStep[0m  [198/339], [94mLoss[0m : 2.67181
[1mStep[0m  [231/339], [94mLoss[0m : 2.15485
[1mStep[0m  [264/339], [94mLoss[0m : 2.95184
[1mStep[0m  [297/339], [94mLoss[0m : 1.76054
[1mStep[0m  [330/339], [94mLoss[0m : 2.50539

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.319, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35576
[1mStep[0m  [33/339], [94mLoss[0m : 2.43124
[1mStep[0m  [66/339], [94mLoss[0m : 2.68453
[1mStep[0m  [99/339], [94mLoss[0m : 2.34965
[1mStep[0m  [132/339], [94mLoss[0m : 2.72795
[1mStep[0m  [165/339], [94mLoss[0m : 2.37482
[1mStep[0m  [198/339], [94mLoss[0m : 1.92047
[1mStep[0m  [231/339], [94mLoss[0m : 2.74872
[1mStep[0m  [264/339], [94mLoss[0m : 2.52631
[1mStep[0m  [297/339], [94mLoss[0m : 2.17144
[1mStep[0m  [330/339], [94mLoss[0m : 2.11321

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09440
[1mStep[0m  [33/339], [94mLoss[0m : 2.96171
[1mStep[0m  [66/339], [94mLoss[0m : 2.84794
[1mStep[0m  [99/339], [94mLoss[0m : 2.37698
[1mStep[0m  [132/339], [94mLoss[0m : 1.47267
[1mStep[0m  [165/339], [94mLoss[0m : 3.05589
[1mStep[0m  [198/339], [94mLoss[0m : 1.89074
[1mStep[0m  [231/339], [94mLoss[0m : 3.03492
[1mStep[0m  [264/339], [94mLoss[0m : 2.15422
[1mStep[0m  [297/339], [94mLoss[0m : 2.83341
[1mStep[0m  [330/339], [94mLoss[0m : 2.79162

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.310, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47672
[1mStep[0m  [33/339], [94mLoss[0m : 2.66166
[1mStep[0m  [66/339], [94mLoss[0m : 2.70353
[1mStep[0m  [99/339], [94mLoss[0m : 2.57427
[1mStep[0m  [132/339], [94mLoss[0m : 2.63021
[1mStep[0m  [165/339], [94mLoss[0m : 2.44888
[1mStep[0m  [198/339], [94mLoss[0m : 2.50012
[1mStep[0m  [231/339], [94mLoss[0m : 2.48737
[1mStep[0m  [264/339], [94mLoss[0m : 1.90961
[1mStep[0m  [297/339], [94mLoss[0m : 2.79335
[1mStep[0m  [330/339], [94mLoss[0m : 2.74405

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.307, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63404
[1mStep[0m  [33/339], [94mLoss[0m : 2.08534
[1mStep[0m  [66/339], [94mLoss[0m : 2.85971
[1mStep[0m  [99/339], [94mLoss[0m : 2.59739
[1mStep[0m  [132/339], [94mLoss[0m : 1.88667
[1mStep[0m  [165/339], [94mLoss[0m : 2.20954
[1mStep[0m  [198/339], [94mLoss[0m : 2.38795
[1mStep[0m  [231/339], [94mLoss[0m : 2.83832
[1mStep[0m  [264/339], [94mLoss[0m : 2.11681
[1mStep[0m  [297/339], [94mLoss[0m : 1.74203
[1mStep[0m  [330/339], [94mLoss[0m : 3.62926

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.299, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77550
[1mStep[0m  [33/339], [94mLoss[0m : 3.41130
[1mStep[0m  [66/339], [94mLoss[0m : 2.20178
[1mStep[0m  [99/339], [94mLoss[0m : 2.28723
[1mStep[0m  [132/339], [94mLoss[0m : 2.39337
[1mStep[0m  [165/339], [94mLoss[0m : 2.15857
[1mStep[0m  [198/339], [94mLoss[0m : 2.45471
[1mStep[0m  [231/339], [94mLoss[0m : 2.23121
[1mStep[0m  [264/339], [94mLoss[0m : 2.69619
[1mStep[0m  [297/339], [94mLoss[0m : 2.29658
[1mStep[0m  [330/339], [94mLoss[0m : 2.93834

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.303, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.98587
[1mStep[0m  [33/339], [94mLoss[0m : 2.62829
[1mStep[0m  [66/339], [94mLoss[0m : 2.72421
[1mStep[0m  [99/339], [94mLoss[0m : 2.00747
[1mStep[0m  [132/339], [94mLoss[0m : 2.26843
[1mStep[0m  [165/339], [94mLoss[0m : 2.35772
[1mStep[0m  [198/339], [94mLoss[0m : 2.28192
[1mStep[0m  [231/339], [94mLoss[0m : 2.64199
[1mStep[0m  [264/339], [94mLoss[0m : 2.53436
[1mStep[0m  [297/339], [94mLoss[0m : 2.62468
[1mStep[0m  [330/339], [94mLoss[0m : 3.17548

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.298, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78854
[1mStep[0m  [33/339], [94mLoss[0m : 2.70630
[1mStep[0m  [66/339], [94mLoss[0m : 2.78556
[1mStep[0m  [99/339], [94mLoss[0m : 2.76027
[1mStep[0m  [132/339], [94mLoss[0m : 2.79871
[1mStep[0m  [165/339], [94mLoss[0m : 2.36011
[1mStep[0m  [198/339], [94mLoss[0m : 2.30256
[1mStep[0m  [231/339], [94mLoss[0m : 2.07070
[1mStep[0m  [264/339], [94mLoss[0m : 2.19026
[1mStep[0m  [297/339], [94mLoss[0m : 2.23188
[1mStep[0m  [330/339], [94mLoss[0m : 2.22757

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.282, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33107
[1mStep[0m  [33/339], [94mLoss[0m : 2.90496
[1mStep[0m  [66/339], [94mLoss[0m : 2.72356
[1mStep[0m  [99/339], [94mLoss[0m : 2.51813
[1mStep[0m  [132/339], [94mLoss[0m : 2.39893
[1mStep[0m  [165/339], [94mLoss[0m : 2.24808
[1mStep[0m  [198/339], [94mLoss[0m : 2.31443
[1mStep[0m  [231/339], [94mLoss[0m : 2.25006
[1mStep[0m  [264/339], [94mLoss[0m : 2.73322
[1mStep[0m  [297/339], [94mLoss[0m : 2.70545
[1mStep[0m  [330/339], [94mLoss[0m : 2.10796

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.311, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40513
[1mStep[0m  [33/339], [94mLoss[0m : 2.38526
[1mStep[0m  [66/339], [94mLoss[0m : 2.10275
[1mStep[0m  [99/339], [94mLoss[0m : 2.17713
[1mStep[0m  [132/339], [94mLoss[0m : 2.41913
[1mStep[0m  [165/339], [94mLoss[0m : 2.20016
[1mStep[0m  [198/339], [94mLoss[0m : 2.88888
[1mStep[0m  [231/339], [94mLoss[0m : 2.49716
[1mStep[0m  [264/339], [94mLoss[0m : 2.32832
[1mStep[0m  [297/339], [94mLoss[0m : 2.48990
[1mStep[0m  [330/339], [94mLoss[0m : 1.76608

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.301, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37121
[1mStep[0m  [33/339], [94mLoss[0m : 2.12193
[1mStep[0m  [66/339], [94mLoss[0m : 2.26581
[1mStep[0m  [99/339], [94mLoss[0m : 2.59472
[1mStep[0m  [132/339], [94mLoss[0m : 1.81637
[1mStep[0m  [165/339], [94mLoss[0m : 2.00903
[1mStep[0m  [198/339], [94mLoss[0m : 2.53236
[1mStep[0m  [231/339], [94mLoss[0m : 2.16022
[1mStep[0m  [264/339], [94mLoss[0m : 3.20137
[1mStep[0m  [297/339], [94mLoss[0m : 2.00076
[1mStep[0m  [330/339], [94mLoss[0m : 2.06346

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.311, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21966
[1mStep[0m  [33/339], [94mLoss[0m : 3.00101
[1mStep[0m  [66/339], [94mLoss[0m : 2.69672
[1mStep[0m  [99/339], [94mLoss[0m : 2.19764
[1mStep[0m  [132/339], [94mLoss[0m : 2.81689
[1mStep[0m  [165/339], [94mLoss[0m : 2.14064
[1mStep[0m  [198/339], [94mLoss[0m : 2.05428
[1mStep[0m  [231/339], [94mLoss[0m : 2.24460
[1mStep[0m  [264/339], [94mLoss[0m : 2.16257
[1mStep[0m  [297/339], [94mLoss[0m : 1.85269
[1mStep[0m  [330/339], [94mLoss[0m : 2.51570

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.288, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10700
[1mStep[0m  [33/339], [94mLoss[0m : 1.81576
[1mStep[0m  [66/339], [94mLoss[0m : 2.43336
[1mStep[0m  [99/339], [94mLoss[0m : 2.54365
[1mStep[0m  [132/339], [94mLoss[0m : 2.43998
[1mStep[0m  [165/339], [94mLoss[0m : 2.14041
[1mStep[0m  [198/339], [94mLoss[0m : 2.25132
[1mStep[0m  [231/339], [94mLoss[0m : 2.22998
[1mStep[0m  [264/339], [94mLoss[0m : 1.76800
[1mStep[0m  [297/339], [94mLoss[0m : 2.09248
[1mStep[0m  [330/339], [94mLoss[0m : 2.84466

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.279, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68091
[1mStep[0m  [33/339], [94mLoss[0m : 2.42037
[1mStep[0m  [66/339], [94mLoss[0m : 2.57657
[1mStep[0m  [99/339], [94mLoss[0m : 3.15501
[1mStep[0m  [132/339], [94mLoss[0m : 2.28456
[1mStep[0m  [165/339], [94mLoss[0m : 2.49858
[1mStep[0m  [198/339], [94mLoss[0m : 2.19975
[1mStep[0m  [231/339], [94mLoss[0m : 3.14343
[1mStep[0m  [264/339], [94mLoss[0m : 2.63428
[1mStep[0m  [297/339], [94mLoss[0m : 3.25773
[1mStep[0m  [330/339], [94mLoss[0m : 3.05599

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.295, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.294
====================================

Phase 1 - Evaluation MAE:  2.293978695320872
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.36644
[1mStep[0m  [33/339], [94mLoss[0m : 2.45192
[1mStep[0m  [66/339], [94mLoss[0m : 1.93431
[1mStep[0m  [99/339], [94mLoss[0m : 2.25611
[1mStep[0m  [132/339], [94mLoss[0m : 2.36362
[1mStep[0m  [165/339], [94mLoss[0m : 2.57290
[1mStep[0m  [198/339], [94mLoss[0m : 1.87744
[1mStep[0m  [231/339], [94mLoss[0m : 2.09667
[1mStep[0m  [264/339], [94mLoss[0m : 2.98547
[1mStep[0m  [297/339], [94mLoss[0m : 2.66797
[1mStep[0m  [330/339], [94mLoss[0m : 2.09308

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.295, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54797
[1mStep[0m  [33/339], [94mLoss[0m : 1.70591
[1mStep[0m  [66/339], [94mLoss[0m : 2.26452
[1mStep[0m  [99/339], [94mLoss[0m : 2.52023
[1mStep[0m  [132/339], [94mLoss[0m : 2.38123
[1mStep[0m  [165/339], [94mLoss[0m : 2.60668
[1mStep[0m  [198/339], [94mLoss[0m : 2.38190
[1mStep[0m  [231/339], [94mLoss[0m : 2.42477
[1mStep[0m  [264/339], [94mLoss[0m : 2.59332
[1mStep[0m  [297/339], [94mLoss[0m : 2.94926
[1mStep[0m  [330/339], [94mLoss[0m : 2.14374

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.719, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58023
[1mStep[0m  [33/339], [94mLoss[0m : 1.69936
[1mStep[0m  [66/339], [94mLoss[0m : 2.75174
[1mStep[0m  [99/339], [94mLoss[0m : 2.44438
[1mStep[0m  [132/339], [94mLoss[0m : 2.66656
[1mStep[0m  [165/339], [94mLoss[0m : 2.65461
[1mStep[0m  [198/339], [94mLoss[0m : 2.39641
[1mStep[0m  [231/339], [94mLoss[0m : 2.04500
[1mStep[0m  [264/339], [94mLoss[0m : 2.44610
[1mStep[0m  [297/339], [94mLoss[0m : 2.45737
[1mStep[0m  [330/339], [94mLoss[0m : 1.98592

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34146
[1mStep[0m  [33/339], [94mLoss[0m : 1.78419
[1mStep[0m  [66/339], [94mLoss[0m : 2.43990
[1mStep[0m  [99/339], [94mLoss[0m : 2.07783
[1mStep[0m  [132/339], [94mLoss[0m : 2.55813
[1mStep[0m  [165/339], [94mLoss[0m : 1.98743
[1mStep[0m  [198/339], [94mLoss[0m : 2.23309
[1mStep[0m  [231/339], [94mLoss[0m : 2.09579
[1mStep[0m  [264/339], [94mLoss[0m : 3.06644
[1mStep[0m  [297/339], [94mLoss[0m : 2.17931
[1mStep[0m  [330/339], [94mLoss[0m : 2.43831

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50252
[1mStep[0m  [33/339], [94mLoss[0m : 1.79837
[1mStep[0m  [66/339], [94mLoss[0m : 1.46050
[1mStep[0m  [99/339], [94mLoss[0m : 1.79809
[1mStep[0m  [132/339], [94mLoss[0m : 2.80114
[1mStep[0m  [165/339], [94mLoss[0m : 1.95280
[1mStep[0m  [198/339], [94mLoss[0m : 2.78920
[1mStep[0m  [231/339], [94mLoss[0m : 2.29437
[1mStep[0m  [264/339], [94mLoss[0m : 2.18076
[1mStep[0m  [297/339], [94mLoss[0m : 2.25947
[1mStep[0m  [330/339], [94mLoss[0m : 2.23426

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.187, [92mTest[0m: 2.515, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98442
[1mStep[0m  [33/339], [94mLoss[0m : 1.71506
[1mStep[0m  [66/339], [94mLoss[0m : 2.19918
[1mStep[0m  [99/339], [94mLoss[0m : 2.20959
[1mStep[0m  [132/339], [94mLoss[0m : 1.71711
[1mStep[0m  [165/339], [94mLoss[0m : 2.14036
[1mStep[0m  [198/339], [94mLoss[0m : 2.24466
[1mStep[0m  [231/339], [94mLoss[0m : 2.12105
[1mStep[0m  [264/339], [94mLoss[0m : 2.06066
[1mStep[0m  [297/339], [94mLoss[0m : 2.35591
[1mStep[0m  [330/339], [94mLoss[0m : 2.30102

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.152, [92mTest[0m: 2.506, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26870
[1mStep[0m  [33/339], [94mLoss[0m : 2.12120
[1mStep[0m  [66/339], [94mLoss[0m : 1.66040
[1mStep[0m  [99/339], [94mLoss[0m : 1.83817
[1mStep[0m  [132/339], [94mLoss[0m : 2.31617
[1mStep[0m  [165/339], [94mLoss[0m : 2.11469
[1mStep[0m  [198/339], [94mLoss[0m : 2.12123
[1mStep[0m  [231/339], [94mLoss[0m : 1.57840
[1mStep[0m  [264/339], [94mLoss[0m : 2.55082
[1mStep[0m  [297/339], [94mLoss[0m : 2.64034
[1mStep[0m  [330/339], [94mLoss[0m : 2.94780

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11423
[1mStep[0m  [33/339], [94mLoss[0m : 1.80942
[1mStep[0m  [66/339], [94mLoss[0m : 2.01157
[1mStep[0m  [99/339], [94mLoss[0m : 1.85752
[1mStep[0m  [132/339], [94mLoss[0m : 2.04883
[1mStep[0m  [165/339], [94mLoss[0m : 1.93454
[1mStep[0m  [198/339], [94mLoss[0m : 2.17729
[1mStep[0m  [231/339], [94mLoss[0m : 1.89212
[1mStep[0m  [264/339], [94mLoss[0m : 1.93668
[1mStep[0m  [297/339], [94mLoss[0m : 1.93239
[1mStep[0m  [330/339], [94mLoss[0m : 2.61579

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.406, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18766
[1mStep[0m  [33/339], [94mLoss[0m : 1.34367
[1mStep[0m  [66/339], [94mLoss[0m : 2.06138
[1mStep[0m  [99/339], [94mLoss[0m : 1.66939
[1mStep[0m  [132/339], [94mLoss[0m : 2.20467
[1mStep[0m  [165/339], [94mLoss[0m : 1.42248
[1mStep[0m  [198/339], [94mLoss[0m : 2.02303
[1mStep[0m  [231/339], [94mLoss[0m : 2.55201
[1mStep[0m  [264/339], [94mLoss[0m : 1.96756
[1mStep[0m  [297/339], [94mLoss[0m : 2.01930
[1mStep[0m  [330/339], [94mLoss[0m : 1.49310

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.398, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35856
[1mStep[0m  [33/339], [94mLoss[0m : 1.80021
[1mStep[0m  [66/339], [94mLoss[0m : 1.61854
[1mStep[0m  [99/339], [94mLoss[0m : 1.95617
[1mStep[0m  [132/339], [94mLoss[0m : 2.03408
[1mStep[0m  [165/339], [94mLoss[0m : 2.19571
[1mStep[0m  [198/339], [94mLoss[0m : 1.83910
[1mStep[0m  [231/339], [94mLoss[0m : 1.71058
[1mStep[0m  [264/339], [94mLoss[0m : 1.97663
[1mStep[0m  [297/339], [94mLoss[0m : 1.95592
[1mStep[0m  [330/339], [94mLoss[0m : 2.36736

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.953, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93300
[1mStep[0m  [33/339], [94mLoss[0m : 1.72742
[1mStep[0m  [66/339], [94mLoss[0m : 1.89886
[1mStep[0m  [99/339], [94mLoss[0m : 1.39189
[1mStep[0m  [132/339], [94mLoss[0m : 2.20030
[1mStep[0m  [165/339], [94mLoss[0m : 1.75001
[1mStep[0m  [198/339], [94mLoss[0m : 2.05323
[1mStep[0m  [231/339], [94mLoss[0m : 1.96256
[1mStep[0m  [264/339], [94mLoss[0m : 2.19250
[1mStep[0m  [297/339], [94mLoss[0m : 1.49127
[1mStep[0m  [330/339], [94mLoss[0m : 2.16108

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34471
[1mStep[0m  [33/339], [94mLoss[0m : 2.28170
[1mStep[0m  [66/339], [94mLoss[0m : 2.26015
[1mStep[0m  [99/339], [94mLoss[0m : 1.87234
[1mStep[0m  [132/339], [94mLoss[0m : 2.15354
[1mStep[0m  [165/339], [94mLoss[0m : 1.96385
[1mStep[0m  [198/339], [94mLoss[0m : 1.74516
[1mStep[0m  [231/339], [94mLoss[0m : 1.89198
[1mStep[0m  [264/339], [94mLoss[0m : 1.85956
[1mStep[0m  [297/339], [94mLoss[0m : 1.84464
[1mStep[0m  [330/339], [94mLoss[0m : 1.97941

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33480
[1mStep[0m  [33/339], [94mLoss[0m : 1.85110
[1mStep[0m  [66/339], [94mLoss[0m : 1.81937
[1mStep[0m  [99/339], [94mLoss[0m : 1.87877
[1mStep[0m  [132/339], [94mLoss[0m : 2.24795
[1mStep[0m  [165/339], [94mLoss[0m : 1.81560
[1mStep[0m  [198/339], [94mLoss[0m : 1.71527
[1mStep[0m  [231/339], [94mLoss[0m : 2.08879
[1mStep[0m  [264/339], [94mLoss[0m : 2.37933
[1mStep[0m  [297/339], [94mLoss[0m : 2.01800
[1mStep[0m  [330/339], [94mLoss[0m : 1.79569

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75903
[1mStep[0m  [33/339], [94mLoss[0m : 1.24294
[1mStep[0m  [66/339], [94mLoss[0m : 1.73923
[1mStep[0m  [99/339], [94mLoss[0m : 1.78887
[1mStep[0m  [132/339], [94mLoss[0m : 1.65825
[1mStep[0m  [165/339], [94mLoss[0m : 1.70678
[1mStep[0m  [198/339], [94mLoss[0m : 2.12931
[1mStep[0m  [231/339], [94mLoss[0m : 1.70910
[1mStep[0m  [264/339], [94mLoss[0m : 1.67589
[1mStep[0m  [297/339], [94mLoss[0m : 1.91668
[1mStep[0m  [330/339], [94mLoss[0m : 1.68719

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83568
[1mStep[0m  [33/339], [94mLoss[0m : 1.31770
[1mStep[0m  [66/339], [94mLoss[0m : 1.54712
[1mStep[0m  [99/339], [94mLoss[0m : 1.46095
[1mStep[0m  [132/339], [94mLoss[0m : 1.73935
[1mStep[0m  [165/339], [94mLoss[0m : 1.38657
[1mStep[0m  [198/339], [94mLoss[0m : 1.81660
[1mStep[0m  [231/339], [94mLoss[0m : 1.84552
[1mStep[0m  [264/339], [94mLoss[0m : 1.68858
[1mStep[0m  [297/339], [94mLoss[0m : 1.99971
[1mStep[0m  [330/339], [94mLoss[0m : 1.87867

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.450, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18085
[1mStep[0m  [33/339], [94mLoss[0m : 1.72365
[1mStep[0m  [66/339], [94mLoss[0m : 1.27101
[1mStep[0m  [99/339], [94mLoss[0m : 1.85837
[1mStep[0m  [132/339], [94mLoss[0m : 1.59749
[1mStep[0m  [165/339], [94mLoss[0m : 1.60538
[1mStep[0m  [198/339], [94mLoss[0m : 1.97335
[1mStep[0m  [231/339], [94mLoss[0m : 1.94998
[1mStep[0m  [264/339], [94mLoss[0m : 1.65012
[1mStep[0m  [297/339], [94mLoss[0m : 1.82447
[1mStep[0m  [330/339], [94mLoss[0m : 2.34939

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.510, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08670
[1mStep[0m  [33/339], [94mLoss[0m : 1.24231
[1mStep[0m  [66/339], [94mLoss[0m : 1.72672
[1mStep[0m  [99/339], [94mLoss[0m : 1.55972
[1mStep[0m  [132/339], [94mLoss[0m : 1.85067
[1mStep[0m  [165/339], [94mLoss[0m : 1.77572
[1mStep[0m  [198/339], [94mLoss[0m : 1.92486
[1mStep[0m  [231/339], [94mLoss[0m : 1.22225
[1mStep[0m  [264/339], [94mLoss[0m : 1.50997
[1mStep[0m  [297/339], [94mLoss[0m : 1.72478
[1mStep[0m  [330/339], [94mLoss[0m : 1.55985

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.468, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24417
[1mStep[0m  [33/339], [94mLoss[0m : 2.07727
[1mStep[0m  [66/339], [94mLoss[0m : 1.64261
[1mStep[0m  [99/339], [94mLoss[0m : 1.68102
[1mStep[0m  [132/339], [94mLoss[0m : 1.60210
[1mStep[0m  [165/339], [94mLoss[0m : 1.86098
[1mStep[0m  [198/339], [94mLoss[0m : 1.55947
[1mStep[0m  [231/339], [94mLoss[0m : 2.14258
[1mStep[0m  [264/339], [94mLoss[0m : 1.57651
[1mStep[0m  [297/339], [94mLoss[0m : 1.52410
[1mStep[0m  [330/339], [94mLoss[0m : 1.47990

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88275
[1mStep[0m  [33/339], [94mLoss[0m : 1.33419
[1mStep[0m  [66/339], [94mLoss[0m : 1.54076
[1mStep[0m  [99/339], [94mLoss[0m : 1.61783
[1mStep[0m  [132/339], [94mLoss[0m : 1.68015
[1mStep[0m  [165/339], [94mLoss[0m : 1.68983
[1mStep[0m  [198/339], [94mLoss[0m : 1.79532
[1mStep[0m  [231/339], [94mLoss[0m : 1.69038
[1mStep[0m  [264/339], [94mLoss[0m : 1.53125
[1mStep[0m  [297/339], [94mLoss[0m : 2.16402
[1mStep[0m  [330/339], [94mLoss[0m : 1.87402

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.559, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66084
[1mStep[0m  [33/339], [94mLoss[0m : 1.64684
[1mStep[0m  [66/339], [94mLoss[0m : 1.18640
[1mStep[0m  [99/339], [94mLoss[0m : 1.58265
[1mStep[0m  [132/339], [94mLoss[0m : 1.64062
[1mStep[0m  [165/339], [94mLoss[0m : 1.41722
[1mStep[0m  [198/339], [94mLoss[0m : 1.80100
[1mStep[0m  [231/339], [94mLoss[0m : 1.65491
[1mStep[0m  [264/339], [94mLoss[0m : 1.46658
[1mStep[0m  [297/339], [94mLoss[0m : 1.37943
[1mStep[0m  [330/339], [94mLoss[0m : 1.84895

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.646, [92mTest[0m: 2.491, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.18147
[1mStep[0m  [33/339], [94mLoss[0m : 1.34929
[1mStep[0m  [66/339], [94mLoss[0m : 1.86483
[1mStep[0m  [99/339], [94mLoss[0m : 1.60242
[1mStep[0m  [132/339], [94mLoss[0m : 2.01832
[1mStep[0m  [165/339], [94mLoss[0m : 1.52327
[1mStep[0m  [198/339], [94mLoss[0m : 1.57853
[1mStep[0m  [231/339], [94mLoss[0m : 1.55313
[1mStep[0m  [264/339], [94mLoss[0m : 1.26525
[1mStep[0m  [297/339], [94mLoss[0m : 1.69293
[1mStep[0m  [330/339], [94mLoss[0m : 1.32354

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.426, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35344
[1mStep[0m  [33/339], [94mLoss[0m : 1.42053
[1mStep[0m  [66/339], [94mLoss[0m : 1.36864
[1mStep[0m  [99/339], [94mLoss[0m : 1.58235
[1mStep[0m  [132/339], [94mLoss[0m : 1.59754
[1mStep[0m  [165/339], [94mLoss[0m : 1.98765
[1mStep[0m  [198/339], [94mLoss[0m : 1.25674
[1mStep[0m  [231/339], [94mLoss[0m : 1.45462
[1mStep[0m  [264/339], [94mLoss[0m : 1.76172
[1mStep[0m  [297/339], [94mLoss[0m : 1.30681
[1mStep[0m  [330/339], [94mLoss[0m : 2.12358

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.448, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.29918
[1mStep[0m  [33/339], [94mLoss[0m : 1.50050
[1mStep[0m  [66/339], [94mLoss[0m : 1.95777
[1mStep[0m  [99/339], [94mLoss[0m : 1.76902
[1mStep[0m  [132/339], [94mLoss[0m : 1.66409
[1mStep[0m  [165/339], [94mLoss[0m : 2.19663
[1mStep[0m  [198/339], [94mLoss[0m : 1.56560
[1mStep[0m  [231/339], [94mLoss[0m : 1.80524
[1mStep[0m  [264/339], [94mLoss[0m : 1.47860
[1mStep[0m  [297/339], [94mLoss[0m : 1.54376
[1mStep[0m  [330/339], [94mLoss[0m : 1.18790

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39615
[1mStep[0m  [33/339], [94mLoss[0m : 1.79124
[1mStep[0m  [66/339], [94mLoss[0m : 2.24492
[1mStep[0m  [99/339], [94mLoss[0m : 1.50473
[1mStep[0m  [132/339], [94mLoss[0m : 1.77399
[1mStep[0m  [165/339], [94mLoss[0m : 1.55887
[1mStep[0m  [198/339], [94mLoss[0m : 1.49116
[1mStep[0m  [231/339], [94mLoss[0m : 1.36282
[1mStep[0m  [264/339], [94mLoss[0m : 1.65705
[1mStep[0m  [297/339], [94mLoss[0m : 1.52006
[1mStep[0m  [330/339], [94mLoss[0m : 1.47029

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74631
[1mStep[0m  [33/339], [94mLoss[0m : 2.00303
[1mStep[0m  [66/339], [94mLoss[0m : 1.38723
[1mStep[0m  [99/339], [94mLoss[0m : 1.35130
[1mStep[0m  [132/339], [94mLoss[0m : 1.31607
[1mStep[0m  [165/339], [94mLoss[0m : 1.60297
[1mStep[0m  [198/339], [94mLoss[0m : 1.66202
[1mStep[0m  [231/339], [94mLoss[0m : 1.52339
[1mStep[0m  [264/339], [94mLoss[0m : 1.33541
[1mStep[0m  [297/339], [94mLoss[0m : 1.31890
[1mStep[0m  [330/339], [94mLoss[0m : 1.30997

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28298
[1mStep[0m  [33/339], [94mLoss[0m : 1.90543
[1mStep[0m  [66/339], [94mLoss[0m : 1.79174
[1mStep[0m  [99/339], [94mLoss[0m : 1.16085
[1mStep[0m  [132/339], [94mLoss[0m : 1.39603
[1mStep[0m  [165/339], [94mLoss[0m : 1.87298
[1mStep[0m  [198/339], [94mLoss[0m : 1.82017
[1mStep[0m  [231/339], [94mLoss[0m : 1.36698
[1mStep[0m  [264/339], [94mLoss[0m : 1.29171
[1mStep[0m  [297/339], [94mLoss[0m : 1.44999
[1mStep[0m  [330/339], [94mLoss[0m : 1.26768

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.26783
[1mStep[0m  [33/339], [94mLoss[0m : 1.27109
[1mStep[0m  [66/339], [94mLoss[0m : 2.02255
[1mStep[0m  [99/339], [94mLoss[0m : 1.59675
[1mStep[0m  [132/339], [94mLoss[0m : 1.59541
[1mStep[0m  [165/339], [94mLoss[0m : 1.87104
[1mStep[0m  [198/339], [94mLoss[0m : 1.18194
[1mStep[0m  [231/339], [94mLoss[0m : 1.16961
[1mStep[0m  [264/339], [94mLoss[0m : 1.46471
[1mStep[0m  [297/339], [94mLoss[0m : 1.44480
[1mStep[0m  [330/339], [94mLoss[0m : 1.81191

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.23648
[1mStep[0m  [33/339], [94mLoss[0m : 1.52308
[1mStep[0m  [66/339], [94mLoss[0m : 1.75736
[1mStep[0m  [99/339], [94mLoss[0m : 1.99878
[1mStep[0m  [132/339], [94mLoss[0m : 1.84829
[1mStep[0m  [165/339], [94mLoss[0m : 1.76212
[1mStep[0m  [198/339], [94mLoss[0m : 2.03402
[1mStep[0m  [231/339], [94mLoss[0m : 1.38633
[1mStep[0m  [264/339], [94mLoss[0m : 1.44638
[1mStep[0m  [297/339], [94mLoss[0m : 1.64638
[1mStep[0m  [330/339], [94mLoss[0m : 1.38973

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.488, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95161
[1mStep[0m  [33/339], [94mLoss[0m : 1.46409
[1mStep[0m  [66/339], [94mLoss[0m : 1.22526
[1mStep[0m  [99/339], [94mLoss[0m : 1.65958
[1mStep[0m  [132/339], [94mLoss[0m : 1.40875
[1mStep[0m  [165/339], [94mLoss[0m : 1.82576
[1mStep[0m  [198/339], [94mLoss[0m : 1.31792
[1mStep[0m  [231/339], [94mLoss[0m : 1.81712
[1mStep[0m  [264/339], [94mLoss[0m : 1.70562
[1mStep[0m  [297/339], [94mLoss[0m : 1.70143
[1mStep[0m  [330/339], [94mLoss[0m : 1.38395

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55910
[1mStep[0m  [33/339], [94mLoss[0m : 1.45054
[1mStep[0m  [66/339], [94mLoss[0m : 1.41817
[1mStep[0m  [99/339], [94mLoss[0m : 1.36556
[1mStep[0m  [132/339], [94mLoss[0m : 1.65305
[1mStep[0m  [165/339], [94mLoss[0m : 1.38302
[1mStep[0m  [198/339], [94mLoss[0m : 1.86286
[1mStep[0m  [231/339], [94mLoss[0m : 1.64872
[1mStep[0m  [264/339], [94mLoss[0m : 1.31466
[1mStep[0m  [297/339], [94mLoss[0m : 1.27341
[1mStep[0m  [330/339], [94mLoss[0m : 1.27129

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.507, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.501
====================================

Phase 2 - Evaluation MAE:  2.5009829111858806
MAE score P1      2.293979
MAE score P2      2.500983
loss              1.410719
learning_rate      0.00505
batch_size              32
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.75281
[1mStep[0m  [33/339], [94mLoss[0m : 2.36751
[1mStep[0m  [66/339], [94mLoss[0m : 2.58876
[1mStep[0m  [99/339], [94mLoss[0m : 3.12697
[1mStep[0m  [132/339], [94mLoss[0m : 2.98111
[1mStep[0m  [165/339], [94mLoss[0m : 3.17338
[1mStep[0m  [198/339], [94mLoss[0m : 3.12003
[1mStep[0m  [231/339], [94mLoss[0m : 2.61970
[1mStep[0m  [264/339], [94mLoss[0m : 2.89366
[1mStep[0m  [297/339], [94mLoss[0m : 1.88263
[1mStep[0m  [330/339], [94mLoss[0m : 2.46247

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.179, [92mTest[0m: 10.969, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70631
[1mStep[0m  [33/339], [94mLoss[0m : 2.91165
[1mStep[0m  [66/339], [94mLoss[0m : 2.16502
[1mStep[0m  [99/339], [94mLoss[0m : 2.70991
[1mStep[0m  [132/339], [94mLoss[0m : 2.57027
[1mStep[0m  [165/339], [94mLoss[0m : 2.72929
[1mStep[0m  [198/339], [94mLoss[0m : 3.13722
[1mStep[0m  [231/339], [94mLoss[0m : 3.18119
[1mStep[0m  [264/339], [94mLoss[0m : 2.62600
[1mStep[0m  [297/339], [94mLoss[0m : 2.50299
[1mStep[0m  [330/339], [94mLoss[0m : 2.70632

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17491
[1mStep[0m  [33/339], [94mLoss[0m : 2.31967
[1mStep[0m  [66/339], [94mLoss[0m : 2.17248
[1mStep[0m  [99/339], [94mLoss[0m : 3.05896
[1mStep[0m  [132/339], [94mLoss[0m : 2.31858
[1mStep[0m  [165/339], [94mLoss[0m : 1.97742
[1mStep[0m  [198/339], [94mLoss[0m : 2.67865
[1mStep[0m  [231/339], [94mLoss[0m : 2.78489
[1mStep[0m  [264/339], [94mLoss[0m : 2.56845
[1mStep[0m  [297/339], [94mLoss[0m : 2.41692
[1mStep[0m  [330/339], [94mLoss[0m : 2.46371

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88209
[1mStep[0m  [33/339], [94mLoss[0m : 2.85586
[1mStep[0m  [66/339], [94mLoss[0m : 2.91658
[1mStep[0m  [99/339], [94mLoss[0m : 3.06098
[1mStep[0m  [132/339], [94mLoss[0m : 2.35383
[1mStep[0m  [165/339], [94mLoss[0m : 2.16194
[1mStep[0m  [198/339], [94mLoss[0m : 3.10087
[1mStep[0m  [231/339], [94mLoss[0m : 2.61827
[1mStep[0m  [264/339], [94mLoss[0m : 2.28967
[1mStep[0m  [297/339], [94mLoss[0m : 2.27555
[1mStep[0m  [330/339], [94mLoss[0m : 2.41151

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.379, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16233
[1mStep[0m  [33/339], [94mLoss[0m : 2.36263
[1mStep[0m  [66/339], [94mLoss[0m : 3.88278
[1mStep[0m  [99/339], [94mLoss[0m : 1.87005
[1mStep[0m  [132/339], [94mLoss[0m : 2.42466
[1mStep[0m  [165/339], [94mLoss[0m : 2.47177
[1mStep[0m  [198/339], [94mLoss[0m : 1.87266
[1mStep[0m  [231/339], [94mLoss[0m : 2.86282
[1mStep[0m  [264/339], [94mLoss[0m : 2.02774
[1mStep[0m  [297/339], [94mLoss[0m : 2.84309
[1mStep[0m  [330/339], [94mLoss[0m : 2.82512

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62245
[1mStep[0m  [33/339], [94mLoss[0m : 2.63149
[1mStep[0m  [66/339], [94mLoss[0m : 2.43019
[1mStep[0m  [99/339], [94mLoss[0m : 2.63143
[1mStep[0m  [132/339], [94mLoss[0m : 2.30422
[1mStep[0m  [165/339], [94mLoss[0m : 2.03036
[1mStep[0m  [198/339], [94mLoss[0m : 2.00489
[1mStep[0m  [231/339], [94mLoss[0m : 2.41823
[1mStep[0m  [264/339], [94mLoss[0m : 2.57721
[1mStep[0m  [297/339], [94mLoss[0m : 2.47641
[1mStep[0m  [330/339], [94mLoss[0m : 2.62848

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83131
[1mStep[0m  [33/339], [94mLoss[0m : 3.04227
[1mStep[0m  [66/339], [94mLoss[0m : 2.09815
[1mStep[0m  [99/339], [94mLoss[0m : 2.63760
[1mStep[0m  [132/339], [94mLoss[0m : 2.69922
[1mStep[0m  [165/339], [94mLoss[0m : 1.82275
[1mStep[0m  [198/339], [94mLoss[0m : 3.30752
[1mStep[0m  [231/339], [94mLoss[0m : 2.24025
[1mStep[0m  [264/339], [94mLoss[0m : 2.57588
[1mStep[0m  [297/339], [94mLoss[0m : 2.55454
[1mStep[0m  [330/339], [94mLoss[0m : 2.24396

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28568
[1mStep[0m  [33/339], [94mLoss[0m : 2.50607
[1mStep[0m  [66/339], [94mLoss[0m : 2.48177
[1mStep[0m  [99/339], [94mLoss[0m : 2.02383
[1mStep[0m  [132/339], [94mLoss[0m : 2.48785
[1mStep[0m  [165/339], [94mLoss[0m : 2.19436
[1mStep[0m  [198/339], [94mLoss[0m : 3.16301
[1mStep[0m  [231/339], [94mLoss[0m : 2.01572
[1mStep[0m  [264/339], [94mLoss[0m : 2.32962
[1mStep[0m  [297/339], [94mLoss[0m : 2.58602
[1mStep[0m  [330/339], [94mLoss[0m : 3.22492

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.90311
[1mStep[0m  [33/339], [94mLoss[0m : 2.39492
[1mStep[0m  [66/339], [94mLoss[0m : 2.03936
[1mStep[0m  [99/339], [94mLoss[0m : 2.49799
[1mStep[0m  [132/339], [94mLoss[0m : 2.86568
[1mStep[0m  [165/339], [94mLoss[0m : 1.82937
[1mStep[0m  [198/339], [94mLoss[0m : 2.24277
[1mStep[0m  [231/339], [94mLoss[0m : 2.37808
[1mStep[0m  [264/339], [94mLoss[0m : 2.91698
[1mStep[0m  [297/339], [94mLoss[0m : 2.12087
[1mStep[0m  [330/339], [94mLoss[0m : 2.39270

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22910
[1mStep[0m  [33/339], [94mLoss[0m : 2.85255
[1mStep[0m  [66/339], [94mLoss[0m : 2.22335
[1mStep[0m  [99/339], [94mLoss[0m : 2.09489
[1mStep[0m  [132/339], [94mLoss[0m : 2.23067
[1mStep[0m  [165/339], [94mLoss[0m : 2.50579
[1mStep[0m  [198/339], [94mLoss[0m : 2.52501
[1mStep[0m  [231/339], [94mLoss[0m : 2.62525
[1mStep[0m  [264/339], [94mLoss[0m : 2.23729
[1mStep[0m  [297/339], [94mLoss[0m : 2.20376
[1mStep[0m  [330/339], [94mLoss[0m : 1.82844

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94568
[1mStep[0m  [33/339], [94mLoss[0m : 2.14277
[1mStep[0m  [66/339], [94mLoss[0m : 1.66098
[1mStep[0m  [99/339], [94mLoss[0m : 2.10538
[1mStep[0m  [132/339], [94mLoss[0m : 2.52914
[1mStep[0m  [165/339], [94mLoss[0m : 1.94267
[1mStep[0m  [198/339], [94mLoss[0m : 2.12185
[1mStep[0m  [231/339], [94mLoss[0m : 2.28605
[1mStep[0m  [264/339], [94mLoss[0m : 2.58392
[1mStep[0m  [297/339], [94mLoss[0m : 1.95247
[1mStep[0m  [330/339], [94mLoss[0m : 2.50678

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.340, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97642
[1mStep[0m  [33/339], [94mLoss[0m : 2.06694
[1mStep[0m  [66/339], [94mLoss[0m : 2.71335
[1mStep[0m  [99/339], [94mLoss[0m : 2.09201
[1mStep[0m  [132/339], [94mLoss[0m : 2.16418
[1mStep[0m  [165/339], [94mLoss[0m : 2.46140
[1mStep[0m  [198/339], [94mLoss[0m : 2.52050
[1mStep[0m  [231/339], [94mLoss[0m : 2.23044
[1mStep[0m  [264/339], [94mLoss[0m : 2.67805
[1mStep[0m  [297/339], [94mLoss[0m : 2.72583
[1mStep[0m  [330/339], [94mLoss[0m : 2.28833

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.92774
[1mStep[0m  [33/339], [94mLoss[0m : 2.43044
[1mStep[0m  [66/339], [94mLoss[0m : 2.23036
[1mStep[0m  [99/339], [94mLoss[0m : 2.60289
[1mStep[0m  [132/339], [94mLoss[0m : 2.30620
[1mStep[0m  [165/339], [94mLoss[0m : 1.90311
[1mStep[0m  [198/339], [94mLoss[0m : 2.37804
[1mStep[0m  [231/339], [94mLoss[0m : 2.52450
[1mStep[0m  [264/339], [94mLoss[0m : 2.41159
[1mStep[0m  [297/339], [94mLoss[0m : 1.89074
[1mStep[0m  [330/339], [94mLoss[0m : 1.93185

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42577
[1mStep[0m  [33/339], [94mLoss[0m : 2.75257
[1mStep[0m  [66/339], [94mLoss[0m : 2.56934
[1mStep[0m  [99/339], [94mLoss[0m : 1.77726
[1mStep[0m  [132/339], [94mLoss[0m : 2.08596
[1mStep[0m  [165/339], [94mLoss[0m : 2.60191
[1mStep[0m  [198/339], [94mLoss[0m : 3.14996
[1mStep[0m  [231/339], [94mLoss[0m : 2.88882
[1mStep[0m  [264/339], [94mLoss[0m : 2.73310
[1mStep[0m  [297/339], [94mLoss[0m : 2.19661
[1mStep[0m  [330/339], [94mLoss[0m : 3.08079

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71982
[1mStep[0m  [33/339], [94mLoss[0m : 2.02580
[1mStep[0m  [66/339], [94mLoss[0m : 2.31599
[1mStep[0m  [99/339], [94mLoss[0m : 2.42670
[1mStep[0m  [132/339], [94mLoss[0m : 2.72877
[1mStep[0m  [165/339], [94mLoss[0m : 1.85677
[1mStep[0m  [198/339], [94mLoss[0m : 2.06498
[1mStep[0m  [231/339], [94mLoss[0m : 2.53884
[1mStep[0m  [264/339], [94mLoss[0m : 2.28499
[1mStep[0m  [297/339], [94mLoss[0m : 2.33214
[1mStep[0m  [330/339], [94mLoss[0m : 2.36121

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15848
[1mStep[0m  [33/339], [94mLoss[0m : 2.22549
[1mStep[0m  [66/339], [94mLoss[0m : 2.15099
[1mStep[0m  [99/339], [94mLoss[0m : 1.98508
[1mStep[0m  [132/339], [94mLoss[0m : 2.70786
[1mStep[0m  [165/339], [94mLoss[0m : 2.22838
[1mStep[0m  [198/339], [94mLoss[0m : 2.31506
[1mStep[0m  [231/339], [94mLoss[0m : 1.98695
[1mStep[0m  [264/339], [94mLoss[0m : 2.09457
[1mStep[0m  [297/339], [94mLoss[0m : 2.67503
[1mStep[0m  [330/339], [94mLoss[0m : 2.49526

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98426
[1mStep[0m  [33/339], [94mLoss[0m : 2.46587
[1mStep[0m  [66/339], [94mLoss[0m : 2.57472
[1mStep[0m  [99/339], [94mLoss[0m : 2.26786
[1mStep[0m  [132/339], [94mLoss[0m : 1.71698
[1mStep[0m  [165/339], [94mLoss[0m : 2.03266
[1mStep[0m  [198/339], [94mLoss[0m : 2.61268
[1mStep[0m  [231/339], [94mLoss[0m : 2.13178
[1mStep[0m  [264/339], [94mLoss[0m : 2.37287
[1mStep[0m  [297/339], [94mLoss[0m : 1.99326
[1mStep[0m  [330/339], [94mLoss[0m : 1.75477

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.323, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85993
[1mStep[0m  [33/339], [94mLoss[0m : 2.66049
[1mStep[0m  [66/339], [94mLoss[0m : 3.28132
[1mStep[0m  [99/339], [94mLoss[0m : 2.29129
[1mStep[0m  [132/339], [94mLoss[0m : 2.73603
[1mStep[0m  [165/339], [94mLoss[0m : 2.32526
[1mStep[0m  [198/339], [94mLoss[0m : 2.81549
[1mStep[0m  [231/339], [94mLoss[0m : 2.66337
[1mStep[0m  [264/339], [94mLoss[0m : 2.51983
[1mStep[0m  [297/339], [94mLoss[0m : 2.35070
[1mStep[0m  [330/339], [94mLoss[0m : 1.90033

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01763
[1mStep[0m  [33/339], [94mLoss[0m : 2.49541
[1mStep[0m  [66/339], [94mLoss[0m : 2.16693
[1mStep[0m  [99/339], [94mLoss[0m : 2.47730
[1mStep[0m  [132/339], [94mLoss[0m : 3.09349
[1mStep[0m  [165/339], [94mLoss[0m : 2.09350
[1mStep[0m  [198/339], [94mLoss[0m : 1.81251
[1mStep[0m  [231/339], [94mLoss[0m : 1.80706
[1mStep[0m  [264/339], [94mLoss[0m : 2.29887
[1mStep[0m  [297/339], [94mLoss[0m : 2.47000
[1mStep[0m  [330/339], [94mLoss[0m : 2.40575

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.303, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99921
[1mStep[0m  [33/339], [94mLoss[0m : 1.96623
[1mStep[0m  [66/339], [94mLoss[0m : 2.08870
[1mStep[0m  [99/339], [94mLoss[0m : 2.40605
[1mStep[0m  [132/339], [94mLoss[0m : 1.78901
[1mStep[0m  [165/339], [94mLoss[0m : 2.76167
[1mStep[0m  [198/339], [94mLoss[0m : 2.37397
[1mStep[0m  [231/339], [94mLoss[0m : 2.91505
[1mStep[0m  [264/339], [94mLoss[0m : 2.51595
[1mStep[0m  [297/339], [94mLoss[0m : 2.23592
[1mStep[0m  [330/339], [94mLoss[0m : 2.35726

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42148
[1mStep[0m  [33/339], [94mLoss[0m : 2.19247
[1mStep[0m  [66/339], [94mLoss[0m : 2.74312
[1mStep[0m  [99/339], [94mLoss[0m : 2.17247
[1mStep[0m  [132/339], [94mLoss[0m : 3.14790
[1mStep[0m  [165/339], [94mLoss[0m : 2.86859
[1mStep[0m  [198/339], [94mLoss[0m : 2.25112
[1mStep[0m  [231/339], [94mLoss[0m : 2.41323
[1mStep[0m  [264/339], [94mLoss[0m : 2.38216
[1mStep[0m  [297/339], [94mLoss[0m : 2.42012
[1mStep[0m  [330/339], [94mLoss[0m : 2.25228

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.312, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37230
[1mStep[0m  [33/339], [94mLoss[0m : 1.94523
[1mStep[0m  [66/339], [94mLoss[0m : 1.80874
[1mStep[0m  [99/339], [94mLoss[0m : 3.19337
[1mStep[0m  [132/339], [94mLoss[0m : 2.15609
[1mStep[0m  [165/339], [94mLoss[0m : 2.32175
[1mStep[0m  [198/339], [94mLoss[0m : 2.26325
[1mStep[0m  [231/339], [94mLoss[0m : 2.01426
[1mStep[0m  [264/339], [94mLoss[0m : 1.83850
[1mStep[0m  [297/339], [94mLoss[0m : 2.17332
[1mStep[0m  [330/339], [94mLoss[0m : 2.00127

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.310, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71382
[1mStep[0m  [33/339], [94mLoss[0m : 2.41254
[1mStep[0m  [66/339], [94mLoss[0m : 2.48401
[1mStep[0m  [99/339], [94mLoss[0m : 2.31867
[1mStep[0m  [132/339], [94mLoss[0m : 2.03574
[1mStep[0m  [165/339], [94mLoss[0m : 2.12004
[1mStep[0m  [198/339], [94mLoss[0m : 2.47086
[1mStep[0m  [231/339], [94mLoss[0m : 2.36861
[1mStep[0m  [264/339], [94mLoss[0m : 1.98700
[1mStep[0m  [297/339], [94mLoss[0m : 2.04930
[1mStep[0m  [330/339], [94mLoss[0m : 2.43167

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.366, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.02205
[1mStep[0m  [33/339], [94mLoss[0m : 2.91636
[1mStep[0m  [66/339], [94mLoss[0m : 2.99993
[1mStep[0m  [99/339], [94mLoss[0m : 2.14386
[1mStep[0m  [132/339], [94mLoss[0m : 2.25467
[1mStep[0m  [165/339], [94mLoss[0m : 2.59848
[1mStep[0m  [198/339], [94mLoss[0m : 2.40382
[1mStep[0m  [231/339], [94mLoss[0m : 2.40504
[1mStep[0m  [264/339], [94mLoss[0m : 2.22989
[1mStep[0m  [297/339], [94mLoss[0m : 2.04188
[1mStep[0m  [330/339], [94mLoss[0m : 2.03096

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.365, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82729
[1mStep[0m  [33/339], [94mLoss[0m : 2.04461
[1mStep[0m  [66/339], [94mLoss[0m : 2.08156
[1mStep[0m  [99/339], [94mLoss[0m : 2.27877
[1mStep[0m  [132/339], [94mLoss[0m : 1.61574
[1mStep[0m  [165/339], [94mLoss[0m : 2.44922
[1mStep[0m  [198/339], [94mLoss[0m : 3.09173
[1mStep[0m  [231/339], [94mLoss[0m : 2.35565
[1mStep[0m  [264/339], [94mLoss[0m : 1.98985
[1mStep[0m  [297/339], [94mLoss[0m : 2.12880
[1mStep[0m  [330/339], [94mLoss[0m : 2.26124

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98498
[1mStep[0m  [33/339], [94mLoss[0m : 1.75357
[1mStep[0m  [66/339], [94mLoss[0m : 2.38031
[1mStep[0m  [99/339], [94mLoss[0m : 2.55004
[1mStep[0m  [132/339], [94mLoss[0m : 2.17331
[1mStep[0m  [165/339], [94mLoss[0m : 2.28148
[1mStep[0m  [198/339], [94mLoss[0m : 2.55094
[1mStep[0m  [231/339], [94mLoss[0m : 2.17888
[1mStep[0m  [264/339], [94mLoss[0m : 2.66455
[1mStep[0m  [297/339], [94mLoss[0m : 2.50780
[1mStep[0m  [330/339], [94mLoss[0m : 2.16237

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04254
[1mStep[0m  [33/339], [94mLoss[0m : 2.28368
[1mStep[0m  [66/339], [94mLoss[0m : 3.05525
[1mStep[0m  [99/339], [94mLoss[0m : 2.06163
[1mStep[0m  [132/339], [94mLoss[0m : 2.29065
[1mStep[0m  [165/339], [94mLoss[0m : 2.03843
[1mStep[0m  [198/339], [94mLoss[0m : 2.36356
[1mStep[0m  [231/339], [94mLoss[0m : 2.17424
[1mStep[0m  [264/339], [94mLoss[0m : 2.01950
[1mStep[0m  [297/339], [94mLoss[0m : 2.29734
[1mStep[0m  [330/339], [94mLoss[0m : 2.11513

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08929
[1mStep[0m  [33/339], [94mLoss[0m : 2.52079
[1mStep[0m  [66/339], [94mLoss[0m : 2.26110
[1mStep[0m  [99/339], [94mLoss[0m : 2.36425
[1mStep[0m  [132/339], [94mLoss[0m : 2.20817
[1mStep[0m  [165/339], [94mLoss[0m : 2.65467
[1mStep[0m  [198/339], [94mLoss[0m : 2.06696
[1mStep[0m  [231/339], [94mLoss[0m : 1.61124
[1mStep[0m  [264/339], [94mLoss[0m : 2.44286
[1mStep[0m  [297/339], [94mLoss[0m : 2.49976
[1mStep[0m  [330/339], [94mLoss[0m : 1.81809

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.356, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11358
[1mStep[0m  [33/339], [94mLoss[0m : 2.44015
[1mStep[0m  [66/339], [94mLoss[0m : 2.38352
[1mStep[0m  [99/339], [94mLoss[0m : 2.11253
[1mStep[0m  [132/339], [94mLoss[0m : 2.40574
[1mStep[0m  [165/339], [94mLoss[0m : 2.65228
[1mStep[0m  [198/339], [94mLoss[0m : 2.52259
[1mStep[0m  [231/339], [94mLoss[0m : 2.05756
[1mStep[0m  [264/339], [94mLoss[0m : 2.35150
[1mStep[0m  [297/339], [94mLoss[0m : 2.00870
[1mStep[0m  [330/339], [94mLoss[0m : 1.94791

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10608
[1mStep[0m  [33/339], [94mLoss[0m : 2.25154
[1mStep[0m  [66/339], [94mLoss[0m : 1.71783
[1mStep[0m  [99/339], [94mLoss[0m : 2.21172
[1mStep[0m  [132/339], [94mLoss[0m : 2.36439
[1mStep[0m  [165/339], [94mLoss[0m : 2.67653
[1mStep[0m  [198/339], [94mLoss[0m : 1.43891
[1mStep[0m  [231/339], [94mLoss[0m : 2.06497
[1mStep[0m  [264/339], [94mLoss[0m : 1.69536
[1mStep[0m  [297/339], [94mLoss[0m : 2.49859
[1mStep[0m  [330/339], [94mLoss[0m : 2.41280

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.398, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.347
====================================

Phase 1 - Evaluation MAE:  2.346843011611331
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.27006
[1mStep[0m  [33/339], [94mLoss[0m : 1.91566
[1mStep[0m  [66/339], [94mLoss[0m : 2.17874
[1mStep[0m  [99/339], [94mLoss[0m : 2.13440
[1mStep[0m  [132/339], [94mLoss[0m : 2.40737
[1mStep[0m  [165/339], [94mLoss[0m : 1.93849
[1mStep[0m  [198/339], [94mLoss[0m : 2.11916
[1mStep[0m  [231/339], [94mLoss[0m : 2.94769
[1mStep[0m  [264/339], [94mLoss[0m : 2.60969
[1mStep[0m  [297/339], [94mLoss[0m : 2.23027
[1mStep[0m  [330/339], [94mLoss[0m : 2.11079

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43162
[1mStep[0m  [33/339], [94mLoss[0m : 2.35610
[1mStep[0m  [66/339], [94mLoss[0m : 2.58545
[1mStep[0m  [99/339], [94mLoss[0m : 2.46800
[1mStep[0m  [132/339], [94mLoss[0m : 2.37134
[1mStep[0m  [165/339], [94mLoss[0m : 1.93730
[1mStep[0m  [198/339], [94mLoss[0m : 2.76873
[1mStep[0m  [231/339], [94mLoss[0m : 2.59052
[1mStep[0m  [264/339], [94mLoss[0m : 2.46137
[1mStep[0m  [297/339], [94mLoss[0m : 1.90874
[1mStep[0m  [330/339], [94mLoss[0m : 2.30492

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.373, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43047
[1mStep[0m  [33/339], [94mLoss[0m : 2.35948
[1mStep[0m  [66/339], [94mLoss[0m : 1.68876
[1mStep[0m  [99/339], [94mLoss[0m : 2.25085
[1mStep[0m  [132/339], [94mLoss[0m : 1.95833
[1mStep[0m  [165/339], [94mLoss[0m : 2.60139
[1mStep[0m  [198/339], [94mLoss[0m : 1.90722
[1mStep[0m  [231/339], [94mLoss[0m : 2.48282
[1mStep[0m  [264/339], [94mLoss[0m : 2.31169
[1mStep[0m  [297/339], [94mLoss[0m : 2.03259
[1mStep[0m  [330/339], [94mLoss[0m : 2.28412

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.370, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56043
[1mStep[0m  [33/339], [94mLoss[0m : 1.59292
[1mStep[0m  [66/339], [94mLoss[0m : 1.95155
[1mStep[0m  [99/339], [94mLoss[0m : 2.66427
[1mStep[0m  [132/339], [94mLoss[0m : 2.00693
[1mStep[0m  [165/339], [94mLoss[0m : 2.92227
[1mStep[0m  [198/339], [94mLoss[0m : 2.03018
[1mStep[0m  [231/339], [94mLoss[0m : 2.83647
[1mStep[0m  [264/339], [94mLoss[0m : 2.01054
[1mStep[0m  [297/339], [94mLoss[0m : 2.69534
[1mStep[0m  [330/339], [94mLoss[0m : 2.37765

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20276
[1mStep[0m  [33/339], [94mLoss[0m : 2.18256
[1mStep[0m  [66/339], [94mLoss[0m : 2.52000
[1mStep[0m  [99/339], [94mLoss[0m : 2.15813
[1mStep[0m  [132/339], [94mLoss[0m : 2.31897
[1mStep[0m  [165/339], [94mLoss[0m : 2.14823
[1mStep[0m  [198/339], [94mLoss[0m : 2.22901
[1mStep[0m  [231/339], [94mLoss[0m : 1.72373
[1mStep[0m  [264/339], [94mLoss[0m : 2.58249
[1mStep[0m  [297/339], [94mLoss[0m : 2.58493
[1mStep[0m  [330/339], [94mLoss[0m : 2.10798

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79278
[1mStep[0m  [33/339], [94mLoss[0m : 2.15037
[1mStep[0m  [66/339], [94mLoss[0m : 2.73092
[1mStep[0m  [99/339], [94mLoss[0m : 1.81228
[1mStep[0m  [132/339], [94mLoss[0m : 2.46320
[1mStep[0m  [165/339], [94mLoss[0m : 2.48456
[1mStep[0m  [198/339], [94mLoss[0m : 2.19087
[1mStep[0m  [231/339], [94mLoss[0m : 2.13769
[1mStep[0m  [264/339], [94mLoss[0m : 2.57556
[1mStep[0m  [297/339], [94mLoss[0m : 2.18388
[1mStep[0m  [330/339], [94mLoss[0m : 2.39058

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.404, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94324
[1mStep[0m  [33/339], [94mLoss[0m : 2.21657
[1mStep[0m  [66/339], [94mLoss[0m : 1.97748
[1mStep[0m  [99/339], [94mLoss[0m : 2.17961
[1mStep[0m  [132/339], [94mLoss[0m : 1.72906
[1mStep[0m  [165/339], [94mLoss[0m : 1.92610
[1mStep[0m  [198/339], [94mLoss[0m : 1.78466
[1mStep[0m  [231/339], [94mLoss[0m : 1.80871
[1mStep[0m  [264/339], [94mLoss[0m : 2.26562
[1mStep[0m  [297/339], [94mLoss[0m : 2.34008
[1mStep[0m  [330/339], [94mLoss[0m : 2.22595

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05657
[1mStep[0m  [33/339], [94mLoss[0m : 1.82563
[1mStep[0m  [66/339], [94mLoss[0m : 2.10515
[1mStep[0m  [99/339], [94mLoss[0m : 1.83240
[1mStep[0m  [132/339], [94mLoss[0m : 2.14330
[1mStep[0m  [165/339], [94mLoss[0m : 2.49904
[1mStep[0m  [198/339], [94mLoss[0m : 1.84158
[1mStep[0m  [231/339], [94mLoss[0m : 2.29313
[1mStep[0m  [264/339], [94mLoss[0m : 1.75160
[1mStep[0m  [297/339], [94mLoss[0m : 1.68174
[1mStep[0m  [330/339], [94mLoss[0m : 2.14781

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76371
[1mStep[0m  [33/339], [94mLoss[0m : 1.98354
[1mStep[0m  [66/339], [94mLoss[0m : 2.07410
[1mStep[0m  [99/339], [94mLoss[0m : 1.48742
[1mStep[0m  [132/339], [94mLoss[0m : 1.70930
[1mStep[0m  [165/339], [94mLoss[0m : 1.64023
[1mStep[0m  [198/339], [94mLoss[0m : 1.94151
[1mStep[0m  [231/339], [94mLoss[0m : 2.16902
[1mStep[0m  [264/339], [94mLoss[0m : 2.52443
[1mStep[0m  [297/339], [94mLoss[0m : 1.36866
[1mStep[0m  [330/339], [94mLoss[0m : 2.53313

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.434, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35268
[1mStep[0m  [33/339], [94mLoss[0m : 1.67111
[1mStep[0m  [66/339], [94mLoss[0m : 1.91298
[1mStep[0m  [99/339], [94mLoss[0m : 2.36175
[1mStep[0m  [132/339], [94mLoss[0m : 2.88943
[1mStep[0m  [165/339], [94mLoss[0m : 1.66918
[1mStep[0m  [198/339], [94mLoss[0m : 2.10614
[1mStep[0m  [231/339], [94mLoss[0m : 2.08293
[1mStep[0m  [264/339], [94mLoss[0m : 2.85268
[1mStep[0m  [297/339], [94mLoss[0m : 2.09964
[1mStep[0m  [330/339], [94mLoss[0m : 1.80366

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55116
[1mStep[0m  [33/339], [94mLoss[0m : 1.49374
[1mStep[0m  [66/339], [94mLoss[0m : 2.25061
[1mStep[0m  [99/339], [94mLoss[0m : 1.90573
[1mStep[0m  [132/339], [94mLoss[0m : 2.31845
[1mStep[0m  [165/339], [94mLoss[0m : 1.50712
[1mStep[0m  [198/339], [94mLoss[0m : 2.47294
[1mStep[0m  [231/339], [94mLoss[0m : 1.79877
[1mStep[0m  [264/339], [94mLoss[0m : 2.26098
[1mStep[0m  [297/339], [94mLoss[0m : 1.75224
[1mStep[0m  [330/339], [94mLoss[0m : 2.01885

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07013
[1mStep[0m  [33/339], [94mLoss[0m : 2.24048
[1mStep[0m  [66/339], [94mLoss[0m : 1.97088
[1mStep[0m  [99/339], [94mLoss[0m : 1.70496
[1mStep[0m  [132/339], [94mLoss[0m : 1.87514
[1mStep[0m  [165/339], [94mLoss[0m : 2.79494
[1mStep[0m  [198/339], [94mLoss[0m : 2.05333
[1mStep[0m  [231/339], [94mLoss[0m : 1.90900
[1mStep[0m  [264/339], [94mLoss[0m : 1.77639
[1mStep[0m  [297/339], [94mLoss[0m : 2.27560
[1mStep[0m  [330/339], [94mLoss[0m : 1.84619

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.401, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62916
[1mStep[0m  [33/339], [94mLoss[0m : 1.90270
[1mStep[0m  [66/339], [94mLoss[0m : 2.45951
[1mStep[0m  [99/339], [94mLoss[0m : 1.76792
[1mStep[0m  [132/339], [94mLoss[0m : 1.56271
[1mStep[0m  [165/339], [94mLoss[0m : 2.16614
[1mStep[0m  [198/339], [94mLoss[0m : 1.98282
[1mStep[0m  [231/339], [94mLoss[0m : 2.03513
[1mStep[0m  [264/339], [94mLoss[0m : 2.67629
[1mStep[0m  [297/339], [94mLoss[0m : 1.61340
[1mStep[0m  [330/339], [94mLoss[0m : 2.09362

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.479, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60757
[1mStep[0m  [33/339], [94mLoss[0m : 1.68505
[1mStep[0m  [66/339], [94mLoss[0m : 2.07137
[1mStep[0m  [99/339], [94mLoss[0m : 1.95573
[1mStep[0m  [132/339], [94mLoss[0m : 1.77257
[1mStep[0m  [165/339], [94mLoss[0m : 1.75174
[1mStep[0m  [198/339], [94mLoss[0m : 2.78195
[1mStep[0m  [231/339], [94mLoss[0m : 1.37173
[1mStep[0m  [264/339], [94mLoss[0m : 2.10662
[1mStep[0m  [297/339], [94mLoss[0m : 1.53345
[1mStep[0m  [330/339], [94mLoss[0m : 1.90568

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35518
[1mStep[0m  [33/339], [94mLoss[0m : 1.67506
[1mStep[0m  [66/339], [94mLoss[0m : 1.23159
[1mStep[0m  [99/339], [94mLoss[0m : 1.63170
[1mStep[0m  [132/339], [94mLoss[0m : 1.93175
[1mStep[0m  [165/339], [94mLoss[0m : 2.03074
[1mStep[0m  [198/339], [94mLoss[0m : 1.44481
[1mStep[0m  [231/339], [94mLoss[0m : 1.50911
[1mStep[0m  [264/339], [94mLoss[0m : 2.10215
[1mStep[0m  [297/339], [94mLoss[0m : 1.73271
[1mStep[0m  [330/339], [94mLoss[0m : 2.05474

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.504, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40772
[1mStep[0m  [33/339], [94mLoss[0m : 1.56344
[1mStep[0m  [66/339], [94mLoss[0m : 1.71892
[1mStep[0m  [99/339], [94mLoss[0m : 1.72585
[1mStep[0m  [132/339], [94mLoss[0m : 1.62848
[1mStep[0m  [165/339], [94mLoss[0m : 1.89124
[1mStep[0m  [198/339], [94mLoss[0m : 1.94042
[1mStep[0m  [231/339], [94mLoss[0m : 1.58476
[1mStep[0m  [264/339], [94mLoss[0m : 1.94288
[1mStep[0m  [297/339], [94mLoss[0m : 1.71458
[1mStep[0m  [330/339], [94mLoss[0m : 1.69641

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61393
[1mStep[0m  [33/339], [94mLoss[0m : 1.62917
[1mStep[0m  [66/339], [94mLoss[0m : 1.72682
[1mStep[0m  [99/339], [94mLoss[0m : 1.26065
[1mStep[0m  [132/339], [94mLoss[0m : 2.03417
[1mStep[0m  [165/339], [94mLoss[0m : 1.82001
[1mStep[0m  [198/339], [94mLoss[0m : 1.41069
[1mStep[0m  [231/339], [94mLoss[0m : 1.21960
[1mStep[0m  [264/339], [94mLoss[0m : 1.85226
[1mStep[0m  [297/339], [94mLoss[0m : 1.80732
[1mStep[0m  [330/339], [94mLoss[0m : 2.19219

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81458
[1mStep[0m  [33/339], [94mLoss[0m : 1.31097
[1mStep[0m  [66/339], [94mLoss[0m : 1.75868
[1mStep[0m  [99/339], [94mLoss[0m : 1.32090
[1mStep[0m  [132/339], [94mLoss[0m : 1.67002
[1mStep[0m  [165/339], [94mLoss[0m : 1.59688
[1mStep[0m  [198/339], [94mLoss[0m : 1.73944
[1mStep[0m  [231/339], [94mLoss[0m : 1.61626
[1mStep[0m  [264/339], [94mLoss[0m : 2.19668
[1mStep[0m  [297/339], [94mLoss[0m : 1.86563
[1mStep[0m  [330/339], [94mLoss[0m : 1.46514

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65837
[1mStep[0m  [33/339], [94mLoss[0m : 2.06108
[1mStep[0m  [66/339], [94mLoss[0m : 1.62850
[1mStep[0m  [99/339], [94mLoss[0m : 1.57737
[1mStep[0m  [132/339], [94mLoss[0m : 1.85742
[1mStep[0m  [165/339], [94mLoss[0m : 1.79204
[1mStep[0m  [198/339], [94mLoss[0m : 1.35251
[1mStep[0m  [231/339], [94mLoss[0m : 1.91283
[1mStep[0m  [264/339], [94mLoss[0m : 1.74208
[1mStep[0m  [297/339], [94mLoss[0m : 1.50048
[1mStep[0m  [330/339], [94mLoss[0m : 1.33874

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01262
[1mStep[0m  [33/339], [94mLoss[0m : 1.34990
[1mStep[0m  [66/339], [94mLoss[0m : 1.63330
[1mStep[0m  [99/339], [94mLoss[0m : 1.73885
[1mStep[0m  [132/339], [94mLoss[0m : 1.82852
[1mStep[0m  [165/339], [94mLoss[0m : 1.66789
[1mStep[0m  [198/339], [94mLoss[0m : 1.49451
[1mStep[0m  [231/339], [94mLoss[0m : 1.73033
[1mStep[0m  [264/339], [94mLoss[0m : 1.70795
[1mStep[0m  [297/339], [94mLoss[0m : 1.62560
[1mStep[0m  [330/339], [94mLoss[0m : 1.68192

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.453, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70651
[1mStep[0m  [33/339], [94mLoss[0m : 2.07532
[1mStep[0m  [66/339], [94mLoss[0m : 1.74013
[1mStep[0m  [99/339], [94mLoss[0m : 1.64002
[1mStep[0m  [132/339], [94mLoss[0m : 2.09053
[1mStep[0m  [165/339], [94mLoss[0m : 1.50137
[1mStep[0m  [198/339], [94mLoss[0m : 1.59152
[1mStep[0m  [231/339], [94mLoss[0m : 1.94910
[1mStep[0m  [264/339], [94mLoss[0m : 1.37207
[1mStep[0m  [297/339], [94mLoss[0m : 1.51430
[1mStep[0m  [330/339], [94mLoss[0m : 1.76687

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.515, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42437
[1mStep[0m  [33/339], [94mLoss[0m : 1.50581
[1mStep[0m  [66/339], [94mLoss[0m : 2.22661
[1mStep[0m  [99/339], [94mLoss[0m : 1.55266
[1mStep[0m  [132/339], [94mLoss[0m : 2.20506
[1mStep[0m  [165/339], [94mLoss[0m : 1.25563
[1mStep[0m  [198/339], [94mLoss[0m : 1.89801
[1mStep[0m  [231/339], [94mLoss[0m : 1.42297
[1mStep[0m  [264/339], [94mLoss[0m : 1.60339
[1mStep[0m  [297/339], [94mLoss[0m : 1.16509
[1mStep[0m  [330/339], [94mLoss[0m : 1.87267

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60611
[1mStep[0m  [33/339], [94mLoss[0m : 1.50050
[1mStep[0m  [66/339], [94mLoss[0m : 1.55564
[1mStep[0m  [99/339], [94mLoss[0m : 2.18644
[1mStep[0m  [132/339], [94mLoss[0m : 1.20815
[1mStep[0m  [165/339], [94mLoss[0m : 1.91844
[1mStep[0m  [198/339], [94mLoss[0m : 1.91915
[1mStep[0m  [231/339], [94mLoss[0m : 1.44456
[1mStep[0m  [264/339], [94mLoss[0m : 1.79825
[1mStep[0m  [297/339], [94mLoss[0m : 1.55903
[1mStep[0m  [330/339], [94mLoss[0m : 1.47353

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.523, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88143
[1mStep[0m  [33/339], [94mLoss[0m : 1.84252
[1mStep[0m  [66/339], [94mLoss[0m : 2.28738
[1mStep[0m  [99/339], [94mLoss[0m : 1.32095
[1mStep[0m  [132/339], [94mLoss[0m : 1.24052
[1mStep[0m  [165/339], [94mLoss[0m : 1.35941
[1mStep[0m  [198/339], [94mLoss[0m : 1.45715
[1mStep[0m  [231/339], [94mLoss[0m : 1.70676
[1mStep[0m  [264/339], [94mLoss[0m : 1.21816
[1mStep[0m  [297/339], [94mLoss[0m : 1.74345
[1mStep[0m  [330/339], [94mLoss[0m : 1.61990

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.573, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40151
[1mStep[0m  [33/339], [94mLoss[0m : 1.63145
[1mStep[0m  [66/339], [94mLoss[0m : 1.30294
[1mStep[0m  [99/339], [94mLoss[0m : 1.56204
[1mStep[0m  [132/339], [94mLoss[0m : 1.66758
[1mStep[0m  [165/339], [94mLoss[0m : 1.34447
[1mStep[0m  [198/339], [94mLoss[0m : 1.61338
[1mStep[0m  [231/339], [94mLoss[0m : 1.83265
[1mStep[0m  [264/339], [94mLoss[0m : 1.42346
[1mStep[0m  [297/339], [94mLoss[0m : 1.49994
[1mStep[0m  [330/339], [94mLoss[0m : 1.54746

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.569, [92mTest[0m: 2.553, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41589
[1mStep[0m  [33/339], [94mLoss[0m : 1.70348
[1mStep[0m  [66/339], [94mLoss[0m : 1.17319
[1mStep[0m  [99/339], [94mLoss[0m : 1.48156
[1mStep[0m  [132/339], [94mLoss[0m : 1.44764
[1mStep[0m  [165/339], [94mLoss[0m : 1.76798
[1mStep[0m  [198/339], [94mLoss[0m : 2.02056
[1mStep[0m  [231/339], [94mLoss[0m : 1.21692
[1mStep[0m  [264/339], [94mLoss[0m : 1.44699
[1mStep[0m  [297/339], [94mLoss[0m : 1.47039
[1mStep[0m  [330/339], [94mLoss[0m : 1.59445

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.550, [92mTest[0m: 2.526, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93441
[1mStep[0m  [33/339], [94mLoss[0m : 1.83466
[1mStep[0m  [66/339], [94mLoss[0m : 1.48154
[1mStep[0m  [99/339], [94mLoss[0m : 1.46742
[1mStep[0m  [132/339], [94mLoss[0m : 1.51410
[1mStep[0m  [165/339], [94mLoss[0m : 1.72413
[1mStep[0m  [198/339], [94mLoss[0m : 2.06181
[1mStep[0m  [231/339], [94mLoss[0m : 1.27584
[1mStep[0m  [264/339], [94mLoss[0m : 1.47911
[1mStep[0m  [297/339], [94mLoss[0m : 1.66539
[1mStep[0m  [330/339], [94mLoss[0m : 1.25057

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.543, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35494
[1mStep[0m  [33/339], [94mLoss[0m : 1.06362
[1mStep[0m  [66/339], [94mLoss[0m : 1.26993
[1mStep[0m  [99/339], [94mLoss[0m : 1.25373
[1mStep[0m  [132/339], [94mLoss[0m : 1.29374
[1mStep[0m  [165/339], [94mLoss[0m : 1.62212
[1mStep[0m  [198/339], [94mLoss[0m : 1.42009
[1mStep[0m  [231/339], [94mLoss[0m : 1.24218
[1mStep[0m  [264/339], [94mLoss[0m : 1.55858
[1mStep[0m  [297/339], [94mLoss[0m : 1.42759
[1mStep[0m  [330/339], [94mLoss[0m : 1.68199

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.603, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57409
[1mStep[0m  [33/339], [94mLoss[0m : 1.22892
[1mStep[0m  [66/339], [94mLoss[0m : 0.83818
[1mStep[0m  [99/339], [94mLoss[0m : 1.46722
[1mStep[0m  [132/339], [94mLoss[0m : 1.35064
[1mStep[0m  [165/339], [94mLoss[0m : 1.63790
[1mStep[0m  [198/339], [94mLoss[0m : 1.59446
[1mStep[0m  [231/339], [94mLoss[0m : 1.29552
[1mStep[0m  [264/339], [94mLoss[0m : 1.94060
[1mStep[0m  [297/339], [94mLoss[0m : 1.34523
[1mStep[0m  [330/339], [94mLoss[0m : 1.47095

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.560, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39364
[1mStep[0m  [33/339], [94mLoss[0m : 1.56279
[1mStep[0m  [66/339], [94mLoss[0m : 1.63041
[1mStep[0m  [99/339], [94mLoss[0m : 1.52646
[1mStep[0m  [132/339], [94mLoss[0m : 1.13487
[1mStep[0m  [165/339], [94mLoss[0m : 1.37673
[1mStep[0m  [198/339], [94mLoss[0m : 1.35058
[1mStep[0m  [231/339], [94mLoss[0m : 1.34615
[1mStep[0m  [264/339], [94mLoss[0m : 1.27850
[1mStep[0m  [297/339], [94mLoss[0m : 1.56980
[1mStep[0m  [330/339], [94mLoss[0m : 1.69107

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.477, [92mTest[0m: 2.552, [96mlr[0m: 0.004545
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.584
====================================

Phase 2 - Evaluation MAE:  2.5838086932106354
MAE score P1       2.346843
MAE score P2       2.583809
loss               1.477184
learning_rate       0.00505
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.61944
[1mStep[0m  [2/21], [94mLoss[0m : 11.07647
[1mStep[0m  [4/21], [94mLoss[0m : 10.94822
[1mStep[0m  [6/21], [94mLoss[0m : 10.66479
[1mStep[0m  [8/21], [94mLoss[0m : 10.90671
[1mStep[0m  [10/21], [94mLoss[0m : 10.65646
[1mStep[0m  [12/21], [94mLoss[0m : 10.76066
[1mStep[0m  [14/21], [94mLoss[0m : 10.72032
[1mStep[0m  [16/21], [94mLoss[0m : 10.87140
[1mStep[0m  [18/21], [94mLoss[0m : 10.61875
[1mStep[0m  [20/21], [94mLoss[0m : 10.80751

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.861, [92mTest[0m: 10.857, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78223
[1mStep[0m  [2/21], [94mLoss[0m : 10.90316
[1mStep[0m  [4/21], [94mLoss[0m : 11.02660
[1mStep[0m  [6/21], [94mLoss[0m : 10.56065
[1mStep[0m  [8/21], [94mLoss[0m : 10.84408
[1mStep[0m  [10/21], [94mLoss[0m : 10.51655
[1mStep[0m  [12/21], [94mLoss[0m : 10.72714
[1mStep[0m  [14/21], [94mLoss[0m : 10.48664
[1mStep[0m  [16/21], [94mLoss[0m : 10.62486
[1mStep[0m  [18/21], [94mLoss[0m : 10.70075
[1mStep[0m  [20/21], [94mLoss[0m : 10.97066

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.734, [92mTest[0m: 10.720, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50553
[1mStep[0m  [2/21], [94mLoss[0m : 10.90669
[1mStep[0m  [4/21], [94mLoss[0m : 10.66549
[1mStep[0m  [6/21], [94mLoss[0m : 10.66235
[1mStep[0m  [8/21], [94mLoss[0m : 10.44546
[1mStep[0m  [10/21], [94mLoss[0m : 10.38588
[1mStep[0m  [12/21], [94mLoss[0m : 10.39215
[1mStep[0m  [14/21], [94mLoss[0m : 10.68776
[1mStep[0m  [16/21], [94mLoss[0m : 10.73666
[1mStep[0m  [18/21], [94mLoss[0m : 10.67390
[1mStep[0m  [20/21], [94mLoss[0m : 10.77531

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.609, [92mTest[0m: 10.589, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51311
[1mStep[0m  [2/21], [94mLoss[0m : 10.51295
[1mStep[0m  [4/21], [94mLoss[0m : 10.57566
[1mStep[0m  [6/21], [94mLoss[0m : 10.39479
[1mStep[0m  [8/21], [94mLoss[0m : 10.39693
[1mStep[0m  [10/21], [94mLoss[0m : 10.74266
[1mStep[0m  [12/21], [94mLoss[0m : 10.60514
[1mStep[0m  [14/21], [94mLoss[0m : 10.57571
[1mStep[0m  [16/21], [94mLoss[0m : 10.64237
[1mStep[0m  [18/21], [94mLoss[0m : 10.38540
[1mStep[0m  [20/21], [94mLoss[0m : 10.46760

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.476, [92mTest[0m: 10.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.52318
[1mStep[0m  [2/21], [94mLoss[0m : 10.42067
[1mStep[0m  [4/21], [94mLoss[0m : 10.74262
[1mStep[0m  [6/21], [94mLoss[0m : 10.12462
[1mStep[0m  [8/21], [94mLoss[0m : 10.45727
[1mStep[0m  [10/21], [94mLoss[0m : 10.56550
[1mStep[0m  [12/21], [94mLoss[0m : 10.34818
[1mStep[0m  [14/21], [94mLoss[0m : 10.29630
[1mStep[0m  [16/21], [94mLoss[0m : 10.07726
[1mStep[0m  [18/21], [94mLoss[0m : 10.35034
[1mStep[0m  [20/21], [94mLoss[0m : 10.31399

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.344, [92mTest[0m: 10.268, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47919
[1mStep[0m  [2/21], [94mLoss[0m : 10.23904
[1mStep[0m  [4/21], [94mLoss[0m : 10.18715
[1mStep[0m  [6/21], [94mLoss[0m : 10.36378
[1mStep[0m  [8/21], [94mLoss[0m : 10.34310
[1mStep[0m  [10/21], [94mLoss[0m : 10.10490
[1mStep[0m  [12/21], [94mLoss[0m : 10.12719
[1mStep[0m  [14/21], [94mLoss[0m : 10.31498
[1mStep[0m  [16/21], [94mLoss[0m : 9.94644
[1mStep[0m  [18/21], [94mLoss[0m : 10.22361
[1mStep[0m  [20/21], [94mLoss[0m : 9.97613

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.206, [92mTest[0m: 10.110, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.16681
[1mStep[0m  [2/21], [94mLoss[0m : 10.17498
[1mStep[0m  [4/21], [94mLoss[0m : 10.16212
[1mStep[0m  [6/21], [94mLoss[0m : 10.37456
[1mStep[0m  [8/21], [94mLoss[0m : 10.06253
[1mStep[0m  [10/21], [94mLoss[0m : 10.03415
[1mStep[0m  [12/21], [94mLoss[0m : 10.02168
[1mStep[0m  [14/21], [94mLoss[0m : 10.14408
[1mStep[0m  [16/21], [94mLoss[0m : 9.96482
[1mStep[0m  [18/21], [94mLoss[0m : 10.28025
[1mStep[0m  [20/21], [94mLoss[0m : 10.04816

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.068, [92mTest[0m: 9.950, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.94663
[1mStep[0m  [2/21], [94mLoss[0m : 9.93396
[1mStep[0m  [4/21], [94mLoss[0m : 9.66527
[1mStep[0m  [6/21], [94mLoss[0m : 10.14078
[1mStep[0m  [8/21], [94mLoss[0m : 9.99545
[1mStep[0m  [10/21], [94mLoss[0m : 9.73067
[1mStep[0m  [12/21], [94mLoss[0m : 10.23307
[1mStep[0m  [14/21], [94mLoss[0m : 9.88075
[1mStep[0m  [16/21], [94mLoss[0m : 9.89828
[1mStep[0m  [18/21], [94mLoss[0m : 10.23852
[1mStep[0m  [20/21], [94mLoss[0m : 9.95572

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.930, [92mTest[0m: 9.805, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.98421
[1mStep[0m  [2/21], [94mLoss[0m : 9.96906
[1mStep[0m  [4/21], [94mLoss[0m : 9.66270
[1mStep[0m  [6/21], [94mLoss[0m : 9.83999
[1mStep[0m  [8/21], [94mLoss[0m : 9.64120
[1mStep[0m  [10/21], [94mLoss[0m : 9.66920
[1mStep[0m  [12/21], [94mLoss[0m : 9.60733
[1mStep[0m  [14/21], [94mLoss[0m : 9.85016
[1mStep[0m  [16/21], [94mLoss[0m : 9.54819
[1mStep[0m  [18/21], [94mLoss[0m : 9.86500
[1mStep[0m  [20/21], [94mLoss[0m : 9.55355

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.778, [92mTest[0m: 9.610, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.76909
[1mStep[0m  [2/21], [94mLoss[0m : 9.63407
[1mStep[0m  [4/21], [94mLoss[0m : 9.68939
[1mStep[0m  [6/21], [94mLoss[0m : 9.89585
[1mStep[0m  [8/21], [94mLoss[0m : 9.62701
[1mStep[0m  [10/21], [94mLoss[0m : 9.57658
[1mStep[0m  [12/21], [94mLoss[0m : 9.52943
[1mStep[0m  [14/21], [94mLoss[0m : 9.55720
[1mStep[0m  [16/21], [94mLoss[0m : 9.60578
[1mStep[0m  [18/21], [94mLoss[0m : 9.33744
[1mStep[0m  [20/21], [94mLoss[0m : 9.76000

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.637, [92mTest[0m: 9.449, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.87512
[1mStep[0m  [2/21], [94mLoss[0m : 9.43474
[1mStep[0m  [4/21], [94mLoss[0m : 9.18624
[1mStep[0m  [6/21], [94mLoss[0m : 9.27347
[1mStep[0m  [8/21], [94mLoss[0m : 9.76933
[1mStep[0m  [10/21], [94mLoss[0m : 9.38611
[1mStep[0m  [12/21], [94mLoss[0m : 9.29714
[1mStep[0m  [14/21], [94mLoss[0m : 9.57340
[1mStep[0m  [16/21], [94mLoss[0m : 9.37214
[1mStep[0m  [18/21], [94mLoss[0m : 9.35993
[1mStep[0m  [20/21], [94mLoss[0m : 9.23755

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.472, [92mTest[0m: 9.238, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.37592
[1mStep[0m  [2/21], [94mLoss[0m : 9.23906
[1mStep[0m  [4/21], [94mLoss[0m : 9.27128
[1mStep[0m  [6/21], [94mLoss[0m : 9.25531
[1mStep[0m  [8/21], [94mLoss[0m : 9.63613
[1mStep[0m  [10/21], [94mLoss[0m : 9.23939
[1mStep[0m  [12/21], [94mLoss[0m : 9.28885
[1mStep[0m  [14/21], [94mLoss[0m : 9.28308
[1mStep[0m  [16/21], [94mLoss[0m : 9.30732
[1mStep[0m  [18/21], [94mLoss[0m : 8.96520
[1mStep[0m  [20/21], [94mLoss[0m : 9.07884

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.298, [92mTest[0m: 9.044, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.11567
[1mStep[0m  [2/21], [94mLoss[0m : 9.00222
[1mStep[0m  [4/21], [94mLoss[0m : 9.27128
[1mStep[0m  [6/21], [94mLoss[0m : 9.20834
[1mStep[0m  [8/21], [94mLoss[0m : 9.16187
[1mStep[0m  [10/21], [94mLoss[0m : 9.28988
[1mStep[0m  [12/21], [94mLoss[0m : 9.45494
[1mStep[0m  [14/21], [94mLoss[0m : 9.10991
[1mStep[0m  [16/21], [94mLoss[0m : 9.11587
[1mStep[0m  [18/21], [94mLoss[0m : 8.81861
[1mStep[0m  [20/21], [94mLoss[0m : 9.10869

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.120, [92mTest[0m: 8.823, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.98079
[1mStep[0m  [2/21], [94mLoss[0m : 8.89900
[1mStep[0m  [4/21], [94mLoss[0m : 8.85030
[1mStep[0m  [6/21], [94mLoss[0m : 8.81646
[1mStep[0m  [8/21], [94mLoss[0m : 8.97435
[1mStep[0m  [10/21], [94mLoss[0m : 8.74026
[1mStep[0m  [12/21], [94mLoss[0m : 8.62462
[1mStep[0m  [14/21], [94mLoss[0m : 8.97607
[1mStep[0m  [16/21], [94mLoss[0m : 9.07959
[1mStep[0m  [18/21], [94mLoss[0m : 8.78634
[1mStep[0m  [20/21], [94mLoss[0m : 8.74146

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.929, [92mTest[0m: 8.613, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.79318
[1mStep[0m  [2/21], [94mLoss[0m : 8.91127
[1mStep[0m  [4/21], [94mLoss[0m : 8.91542
[1mStep[0m  [6/21], [94mLoss[0m : 8.82322
[1mStep[0m  [8/21], [94mLoss[0m : 8.43127
[1mStep[0m  [10/21], [94mLoss[0m : 8.84269
[1mStep[0m  [12/21], [94mLoss[0m : 8.74668
[1mStep[0m  [14/21], [94mLoss[0m : 8.37118
[1mStep[0m  [16/21], [94mLoss[0m : 8.80311
[1mStep[0m  [18/21], [94mLoss[0m : 8.75630
[1mStep[0m  [20/21], [94mLoss[0m : 8.62806

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.727, [92mTest[0m: 8.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.69979
[1mStep[0m  [2/21], [94mLoss[0m : 8.59856
[1mStep[0m  [4/21], [94mLoss[0m : 8.25849
[1mStep[0m  [6/21], [94mLoss[0m : 8.44772
[1mStep[0m  [8/21], [94mLoss[0m : 8.36637
[1mStep[0m  [10/21], [94mLoss[0m : 8.61835
[1mStep[0m  [12/21], [94mLoss[0m : 8.52401
[1mStep[0m  [14/21], [94mLoss[0m : 8.27611
[1mStep[0m  [16/21], [94mLoss[0m : 8.58691
[1mStep[0m  [18/21], [94mLoss[0m : 8.44320
[1mStep[0m  [20/21], [94mLoss[0m : 8.51536

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.508, [92mTest[0m: 8.155, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.37157
[1mStep[0m  [2/21], [94mLoss[0m : 8.48510
[1mStep[0m  [4/21], [94mLoss[0m : 8.17991
[1mStep[0m  [6/21], [94mLoss[0m : 8.28865
[1mStep[0m  [8/21], [94mLoss[0m : 8.36019
[1mStep[0m  [10/21], [94mLoss[0m : 8.16310
[1mStep[0m  [12/21], [94mLoss[0m : 8.16853
[1mStep[0m  [14/21], [94mLoss[0m : 8.28037
[1mStep[0m  [16/21], [94mLoss[0m : 8.23649
[1mStep[0m  [18/21], [94mLoss[0m : 8.17772
[1mStep[0m  [20/21], [94mLoss[0m : 8.10066

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.276, [92mTest[0m: 7.875, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.87487
[1mStep[0m  [2/21], [94mLoss[0m : 8.34498
[1mStep[0m  [4/21], [94mLoss[0m : 8.08791
[1mStep[0m  [6/21], [94mLoss[0m : 8.01011
[1mStep[0m  [8/21], [94mLoss[0m : 8.17776
[1mStep[0m  [10/21], [94mLoss[0m : 8.04834
[1mStep[0m  [12/21], [94mLoss[0m : 8.11778
[1mStep[0m  [14/21], [94mLoss[0m : 7.98976
[1mStep[0m  [16/21], [94mLoss[0m : 7.96076
[1mStep[0m  [18/21], [94mLoss[0m : 7.97872
[1mStep[0m  [20/21], [94mLoss[0m : 8.12448

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.023, [92mTest[0m: 7.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.88401
[1mStep[0m  [2/21], [94mLoss[0m : 7.60220
[1mStep[0m  [4/21], [94mLoss[0m : 7.63910
[1mStep[0m  [6/21], [94mLoss[0m : 7.66899
[1mStep[0m  [8/21], [94mLoss[0m : 7.89546
[1mStep[0m  [10/21], [94mLoss[0m : 7.72298
[1mStep[0m  [12/21], [94mLoss[0m : 7.84242
[1mStep[0m  [14/21], [94mLoss[0m : 7.88180
[1mStep[0m  [16/21], [94mLoss[0m : 7.50840
[1mStep[0m  [18/21], [94mLoss[0m : 7.43248
[1mStep[0m  [20/21], [94mLoss[0m : 7.58707

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.756, [92mTest[0m: 7.269, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.70916
[1mStep[0m  [2/21], [94mLoss[0m : 7.19094
[1mStep[0m  [4/21], [94mLoss[0m : 7.51601
[1mStep[0m  [6/21], [94mLoss[0m : 7.75050
[1mStep[0m  [8/21], [94mLoss[0m : 7.48009
[1mStep[0m  [10/21], [94mLoss[0m : 7.35977
[1mStep[0m  [12/21], [94mLoss[0m : 7.31243
[1mStep[0m  [14/21], [94mLoss[0m : 7.17858
[1mStep[0m  [16/21], [94mLoss[0m : 7.39212
[1mStep[0m  [18/21], [94mLoss[0m : 7.28214
[1mStep[0m  [20/21], [94mLoss[0m : 7.09481

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.469, [92mTest[0m: 6.946, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.45994
[1mStep[0m  [2/21], [94mLoss[0m : 7.23647
[1mStep[0m  [4/21], [94mLoss[0m : 7.41312
[1mStep[0m  [6/21], [94mLoss[0m : 7.19472
[1mStep[0m  [8/21], [94mLoss[0m : 7.07299
[1mStep[0m  [10/21], [94mLoss[0m : 7.10490
[1mStep[0m  [12/21], [94mLoss[0m : 7.06524
[1mStep[0m  [14/21], [94mLoss[0m : 7.24315
[1mStep[0m  [16/21], [94mLoss[0m : 6.88909
[1mStep[0m  [18/21], [94mLoss[0m : 6.98691
[1mStep[0m  [20/21], [94mLoss[0m : 7.06552

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.196, [92mTest[0m: 6.647, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.98976
[1mStep[0m  [2/21], [94mLoss[0m : 7.11753
[1mStep[0m  [4/21], [94mLoss[0m : 7.12542
[1mStep[0m  [6/21], [94mLoss[0m : 7.02033
[1mStep[0m  [8/21], [94mLoss[0m : 6.93760
[1mStep[0m  [10/21], [94mLoss[0m : 6.94280
[1mStep[0m  [12/21], [94mLoss[0m : 6.96150
[1mStep[0m  [14/21], [94mLoss[0m : 7.12868
[1mStep[0m  [16/21], [94mLoss[0m : 6.85388
[1mStep[0m  [18/21], [94mLoss[0m : 6.65404
[1mStep[0m  [20/21], [94mLoss[0m : 6.94472

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.952, [92mTest[0m: 6.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.07109
[1mStep[0m  [2/21], [94mLoss[0m : 6.88739
[1mStep[0m  [4/21], [94mLoss[0m : 6.76227
[1mStep[0m  [6/21], [94mLoss[0m : 6.69897
[1mStep[0m  [8/21], [94mLoss[0m : 6.63738
[1mStep[0m  [10/21], [94mLoss[0m : 6.85297
[1mStep[0m  [12/21], [94mLoss[0m : 6.78393
[1mStep[0m  [14/21], [94mLoss[0m : 6.42688
[1mStep[0m  [16/21], [94mLoss[0m : 6.57967
[1mStep[0m  [18/21], [94mLoss[0m : 6.92124
[1mStep[0m  [20/21], [94mLoss[0m : 6.62218

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.699, [92mTest[0m: 6.020, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.52817
[1mStep[0m  [2/21], [94mLoss[0m : 6.49359
[1mStep[0m  [4/21], [94mLoss[0m : 6.54427
[1mStep[0m  [6/21], [94mLoss[0m : 6.61718
[1mStep[0m  [8/21], [94mLoss[0m : 6.62280
[1mStep[0m  [10/21], [94mLoss[0m : 6.47662
[1mStep[0m  [12/21], [94mLoss[0m : 6.46726
[1mStep[0m  [14/21], [94mLoss[0m : 6.82297
[1mStep[0m  [16/21], [94mLoss[0m : 6.28656
[1mStep[0m  [18/21], [94mLoss[0m : 6.31929
[1mStep[0m  [20/21], [94mLoss[0m : 6.40491

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.461, [92mTest[0m: 5.771, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.35051
[1mStep[0m  [2/21], [94mLoss[0m : 6.19211
[1mStep[0m  [4/21], [94mLoss[0m : 6.15979
[1mStep[0m  [6/21], [94mLoss[0m : 6.46954
[1mStep[0m  [8/21], [94mLoss[0m : 6.24615
[1mStep[0m  [10/21], [94mLoss[0m : 6.15999
[1mStep[0m  [12/21], [94mLoss[0m : 6.19334
[1mStep[0m  [14/21], [94mLoss[0m : 6.19796
[1mStep[0m  [16/21], [94mLoss[0m : 6.24486
[1mStep[0m  [18/21], [94mLoss[0m : 6.01312
[1mStep[0m  [20/21], [94mLoss[0m : 5.98999

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.232, [92mTest[0m: 5.567, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.04075
[1mStep[0m  [2/21], [94mLoss[0m : 6.27806
[1mStep[0m  [4/21], [94mLoss[0m : 5.99522
[1mStep[0m  [6/21], [94mLoss[0m : 6.38521
[1mStep[0m  [8/21], [94mLoss[0m : 5.86510
[1mStep[0m  [10/21], [94mLoss[0m : 5.66821
[1mStep[0m  [12/21], [94mLoss[0m : 6.19032
[1mStep[0m  [14/21], [94mLoss[0m : 6.22310
[1mStep[0m  [16/21], [94mLoss[0m : 6.05427
[1mStep[0m  [18/21], [94mLoss[0m : 5.91039
[1mStep[0m  [20/21], [94mLoss[0m : 5.74081

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.013, [92mTest[0m: 5.320, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.69158
[1mStep[0m  [2/21], [94mLoss[0m : 5.84608
[1mStep[0m  [4/21], [94mLoss[0m : 5.68522
[1mStep[0m  [6/21], [94mLoss[0m : 5.67262
[1mStep[0m  [8/21], [94mLoss[0m : 6.10960
[1mStep[0m  [10/21], [94mLoss[0m : 5.62960
[1mStep[0m  [12/21], [94mLoss[0m : 5.66232
[1mStep[0m  [14/21], [94mLoss[0m : 5.59557
[1mStep[0m  [16/21], [94mLoss[0m : 5.82655
[1mStep[0m  [18/21], [94mLoss[0m : 5.97565
[1mStep[0m  [20/21], [94mLoss[0m : 5.72098

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.783, [92mTest[0m: 5.108, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.86834
[1mStep[0m  [2/21], [94mLoss[0m : 5.49503
[1mStep[0m  [4/21], [94mLoss[0m : 5.49639
[1mStep[0m  [6/21], [94mLoss[0m : 5.47562
[1mStep[0m  [8/21], [94mLoss[0m : 5.55819
[1mStep[0m  [10/21], [94mLoss[0m : 5.94396
[1mStep[0m  [12/21], [94mLoss[0m : 5.51660
[1mStep[0m  [14/21], [94mLoss[0m : 5.48037
[1mStep[0m  [16/21], [94mLoss[0m : 5.56404
[1mStep[0m  [18/21], [94mLoss[0m : 5.44921
[1mStep[0m  [20/21], [94mLoss[0m : 5.41658

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 5.544, [92mTest[0m: 4.867, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.40066
[1mStep[0m  [2/21], [94mLoss[0m : 5.42733
[1mStep[0m  [4/21], [94mLoss[0m : 5.42049
[1mStep[0m  [6/21], [94mLoss[0m : 5.56749
[1mStep[0m  [8/21], [94mLoss[0m : 5.24285
[1mStep[0m  [10/21], [94mLoss[0m : 5.21124
[1mStep[0m  [12/21], [94mLoss[0m : 5.26761
[1mStep[0m  [14/21], [94mLoss[0m : 5.31093
[1mStep[0m  [16/21], [94mLoss[0m : 5.22930
[1mStep[0m  [18/21], [94mLoss[0m : 5.12035
[1mStep[0m  [20/21], [94mLoss[0m : 5.29163

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 5.342, [92mTest[0m: 4.659, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.26041
[1mStep[0m  [2/21], [94mLoss[0m : 5.33227
[1mStep[0m  [4/21], [94mLoss[0m : 4.99874
[1mStep[0m  [6/21], [94mLoss[0m : 5.10575
[1mStep[0m  [8/21], [94mLoss[0m : 5.26159
[1mStep[0m  [10/21], [94mLoss[0m : 4.99750
[1mStep[0m  [12/21], [94mLoss[0m : 5.12263
[1mStep[0m  [14/21], [94mLoss[0m : 5.13065
[1mStep[0m  [16/21], [94mLoss[0m : 5.20491
[1mStep[0m  [18/21], [94mLoss[0m : 5.10073
[1mStep[0m  [20/21], [94mLoss[0m : 5.10593

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 5.134, [92mTest[0m: 4.435, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.287
====================================

Phase 1 - Evaluation MAE:  4.287116663796561
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 5.09218
[1mStep[0m  [2/21], [94mLoss[0m : 4.91016
[1mStep[0m  [4/21], [94mLoss[0m : 4.98720
[1mStep[0m  [6/21], [94mLoss[0m : 4.77939
[1mStep[0m  [8/21], [94mLoss[0m : 5.01239
[1mStep[0m  [10/21], [94mLoss[0m : 4.74418
[1mStep[0m  [12/21], [94mLoss[0m : 4.94039
[1mStep[0m  [14/21], [94mLoss[0m : 4.54382
[1mStep[0m  [16/21], [94mLoss[0m : 4.92565
[1mStep[0m  [18/21], [94mLoss[0m : 4.74617
[1mStep[0m  [20/21], [94mLoss[0m : 4.90647

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.905, [92mTest[0m: 4.290, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.77794
[1mStep[0m  [2/21], [94mLoss[0m : 4.87492
[1mStep[0m  [4/21], [94mLoss[0m : 4.81238
[1mStep[0m  [6/21], [94mLoss[0m : 4.38779
[1mStep[0m  [8/21], [94mLoss[0m : 4.63781
[1mStep[0m  [10/21], [94mLoss[0m : 4.22440
[1mStep[0m  [12/21], [94mLoss[0m : 4.51817
[1mStep[0m  [14/21], [94mLoss[0m : 4.81918
[1mStep[0m  [16/21], [94mLoss[0m : 4.65061
[1mStep[0m  [18/21], [94mLoss[0m : 4.38540
[1mStep[0m  [20/21], [94mLoss[0m : 4.59636

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.609, [92mTest[0m: 4.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.37093
[1mStep[0m  [2/21], [94mLoss[0m : 4.37062
[1mStep[0m  [4/21], [94mLoss[0m : 4.35513
[1mStep[0m  [6/21], [94mLoss[0m : 4.18860
[1mStep[0m  [8/21], [94mLoss[0m : 4.32294
[1mStep[0m  [10/21], [94mLoss[0m : 4.33509
[1mStep[0m  [12/21], [94mLoss[0m : 4.32562
[1mStep[0m  [14/21], [94mLoss[0m : 4.43972
[1mStep[0m  [16/21], [94mLoss[0m : 4.25758
[1mStep[0m  [18/21], [94mLoss[0m : 4.33769
[1mStep[0m  [20/21], [94mLoss[0m : 4.04084

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.321, [92mTest[0m: 3.892, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.28876
[1mStep[0m  [2/21], [94mLoss[0m : 4.42222
[1mStep[0m  [4/21], [94mLoss[0m : 4.14673
[1mStep[0m  [6/21], [94mLoss[0m : 4.04091
[1mStep[0m  [8/21], [94mLoss[0m : 4.02256
[1mStep[0m  [10/21], [94mLoss[0m : 4.07947
[1mStep[0m  [12/21], [94mLoss[0m : 3.93225
[1mStep[0m  [14/21], [94mLoss[0m : 3.93681
[1mStep[0m  [16/21], [94mLoss[0m : 4.19342
[1mStep[0m  [18/21], [94mLoss[0m : 3.82683
[1mStep[0m  [20/21], [94mLoss[0m : 4.00005

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.105, [92mTest[0m: 3.599, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.95436
[1mStep[0m  [2/21], [94mLoss[0m : 4.12041
[1mStep[0m  [4/21], [94mLoss[0m : 3.75358
[1mStep[0m  [6/21], [94mLoss[0m : 3.78990
[1mStep[0m  [8/21], [94mLoss[0m : 4.19592
[1mStep[0m  [10/21], [94mLoss[0m : 3.77382
[1mStep[0m  [12/21], [94mLoss[0m : 4.21747
[1mStep[0m  [14/21], [94mLoss[0m : 3.91375
[1mStep[0m  [16/21], [94mLoss[0m : 3.69184
[1mStep[0m  [18/21], [94mLoss[0m : 3.88655
[1mStep[0m  [20/21], [94mLoss[0m : 3.80235

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.903, [92mTest[0m: 3.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.91911
[1mStep[0m  [2/21], [94mLoss[0m : 3.59118
[1mStep[0m  [4/21], [94mLoss[0m : 3.74151
[1mStep[0m  [6/21], [94mLoss[0m : 3.70027
[1mStep[0m  [8/21], [94mLoss[0m : 3.84689
[1mStep[0m  [10/21], [94mLoss[0m : 3.71668
[1mStep[0m  [12/21], [94mLoss[0m : 3.53796
[1mStep[0m  [14/21], [94mLoss[0m : 3.78035
[1mStep[0m  [16/21], [94mLoss[0m : 3.48551
[1mStep[0m  [18/21], [94mLoss[0m : 3.68498
[1mStep[0m  [20/21], [94mLoss[0m : 3.86876

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.687, [92mTest[0m: 3.263, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.69245
[1mStep[0m  [2/21], [94mLoss[0m : 3.66599
[1mStep[0m  [4/21], [94mLoss[0m : 3.28614
[1mStep[0m  [6/21], [94mLoss[0m : 3.61658
[1mStep[0m  [8/21], [94mLoss[0m : 3.70045
[1mStep[0m  [10/21], [94mLoss[0m : 3.52371
[1mStep[0m  [12/21], [94mLoss[0m : 3.48508
[1mStep[0m  [14/21], [94mLoss[0m : 3.39688
[1mStep[0m  [16/21], [94mLoss[0m : 3.57172
[1mStep[0m  [18/21], [94mLoss[0m : 3.58067
[1mStep[0m  [20/21], [94mLoss[0m : 3.48387

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.551, [92mTest[0m: 3.094, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.29780
[1mStep[0m  [2/21], [94mLoss[0m : 3.43365
[1mStep[0m  [4/21], [94mLoss[0m : 3.63752
[1mStep[0m  [6/21], [94mLoss[0m : 3.44048
[1mStep[0m  [8/21], [94mLoss[0m : 3.57272
[1mStep[0m  [10/21], [94mLoss[0m : 3.44781
[1mStep[0m  [12/21], [94mLoss[0m : 3.55112
[1mStep[0m  [14/21], [94mLoss[0m : 3.28925
[1mStep[0m  [16/21], [94mLoss[0m : 3.32538
[1mStep[0m  [18/21], [94mLoss[0m : 3.31557
[1mStep[0m  [20/21], [94mLoss[0m : 3.43254

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.401, [92mTest[0m: 3.057, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.27784
[1mStep[0m  [2/21], [94mLoss[0m : 3.25964
[1mStep[0m  [4/21], [94mLoss[0m : 3.49279
[1mStep[0m  [6/21], [94mLoss[0m : 3.44539
[1mStep[0m  [8/21], [94mLoss[0m : 3.29243
[1mStep[0m  [10/21], [94mLoss[0m : 3.22504
[1mStep[0m  [12/21], [94mLoss[0m : 3.21959
[1mStep[0m  [14/21], [94mLoss[0m : 3.27959
[1mStep[0m  [16/21], [94mLoss[0m : 3.24941
[1mStep[0m  [18/21], [94mLoss[0m : 3.39238
[1mStep[0m  [20/21], [94mLoss[0m : 3.01830

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.291, [92mTest[0m: 2.942, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.19804
[1mStep[0m  [2/21], [94mLoss[0m : 3.09262
[1mStep[0m  [4/21], [94mLoss[0m : 3.31908
[1mStep[0m  [6/21], [94mLoss[0m : 3.28539
[1mStep[0m  [8/21], [94mLoss[0m : 3.45163
[1mStep[0m  [10/21], [94mLoss[0m : 3.26162
[1mStep[0m  [12/21], [94mLoss[0m : 3.22487
[1mStep[0m  [14/21], [94mLoss[0m : 3.14877
[1mStep[0m  [16/21], [94mLoss[0m : 3.28673
[1mStep[0m  [18/21], [94mLoss[0m : 3.07137
[1mStep[0m  [20/21], [94mLoss[0m : 2.98679

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.194, [92mTest[0m: 2.880, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.98318
[1mStep[0m  [2/21], [94mLoss[0m : 3.23526
[1mStep[0m  [4/21], [94mLoss[0m : 3.19926
[1mStep[0m  [6/21], [94mLoss[0m : 3.17273
[1mStep[0m  [8/21], [94mLoss[0m : 3.07411
[1mStep[0m  [10/21], [94mLoss[0m : 3.13258
[1mStep[0m  [12/21], [94mLoss[0m : 2.98977
[1mStep[0m  [14/21], [94mLoss[0m : 3.17804
[1mStep[0m  [16/21], [94mLoss[0m : 3.19119
[1mStep[0m  [18/21], [94mLoss[0m : 3.14893
[1mStep[0m  [20/21], [94mLoss[0m : 2.99457

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.106, [92mTest[0m: 2.839, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.11936
[1mStep[0m  [2/21], [94mLoss[0m : 2.93720
[1mStep[0m  [4/21], [94mLoss[0m : 3.07545
[1mStep[0m  [6/21], [94mLoss[0m : 3.04294
[1mStep[0m  [8/21], [94mLoss[0m : 3.10948
[1mStep[0m  [10/21], [94mLoss[0m : 3.14501
[1mStep[0m  [12/21], [94mLoss[0m : 3.15123
[1mStep[0m  [14/21], [94mLoss[0m : 2.83171
[1mStep[0m  [16/21], [94mLoss[0m : 3.04599
[1mStep[0m  [18/21], [94mLoss[0m : 3.01408
[1mStep[0m  [20/21], [94mLoss[0m : 2.99665

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.049, [92mTest[0m: 2.752, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.90067
[1mStep[0m  [2/21], [94mLoss[0m : 2.88634
[1mStep[0m  [4/21], [94mLoss[0m : 2.92226
[1mStep[0m  [6/21], [94mLoss[0m : 2.92791
[1mStep[0m  [8/21], [94mLoss[0m : 2.91909
[1mStep[0m  [10/21], [94mLoss[0m : 3.06639
[1mStep[0m  [12/21], [94mLoss[0m : 2.97842
[1mStep[0m  [14/21], [94mLoss[0m : 2.98964
[1mStep[0m  [16/21], [94mLoss[0m : 2.96483
[1mStep[0m  [18/21], [94mLoss[0m : 2.94469
[1mStep[0m  [20/21], [94mLoss[0m : 3.04214

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.957, [92mTest[0m: 2.723, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.85146
[1mStep[0m  [2/21], [94mLoss[0m : 2.85964
[1mStep[0m  [4/21], [94mLoss[0m : 2.83157
[1mStep[0m  [6/21], [94mLoss[0m : 3.04762
[1mStep[0m  [8/21], [94mLoss[0m : 2.90061
[1mStep[0m  [10/21], [94mLoss[0m : 2.83019
[1mStep[0m  [12/21], [94mLoss[0m : 2.86629
[1mStep[0m  [14/21], [94mLoss[0m : 2.76403
[1mStep[0m  [16/21], [94mLoss[0m : 2.94435
[1mStep[0m  [18/21], [94mLoss[0m : 2.90189
[1mStep[0m  [20/21], [94mLoss[0m : 2.77819

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.899, [92mTest[0m: 2.706, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.88262
[1mStep[0m  [2/21], [94mLoss[0m : 2.83518
[1mStep[0m  [4/21], [94mLoss[0m : 2.87550
[1mStep[0m  [6/21], [94mLoss[0m : 2.85660
[1mStep[0m  [8/21], [94mLoss[0m : 2.91309
[1mStep[0m  [10/21], [94mLoss[0m : 2.83719
[1mStep[0m  [12/21], [94mLoss[0m : 2.87930
[1mStep[0m  [14/21], [94mLoss[0m : 2.68531
[1mStep[0m  [16/21], [94mLoss[0m : 2.75522
[1mStep[0m  [18/21], [94mLoss[0m : 2.85846
[1mStep[0m  [20/21], [94mLoss[0m : 2.94678

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.833, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.99641
[1mStep[0m  [2/21], [94mLoss[0m : 2.75839
[1mStep[0m  [4/21], [94mLoss[0m : 2.56183
[1mStep[0m  [6/21], [94mLoss[0m : 2.67946
[1mStep[0m  [8/21], [94mLoss[0m : 2.78371
[1mStep[0m  [10/21], [94mLoss[0m : 2.87017
[1mStep[0m  [12/21], [94mLoss[0m : 2.75155
[1mStep[0m  [14/21], [94mLoss[0m : 2.90496
[1mStep[0m  [16/21], [94mLoss[0m : 2.88060
[1mStep[0m  [18/21], [94mLoss[0m : 2.87559
[1mStep[0m  [20/21], [94mLoss[0m : 2.84521

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.792, [92mTest[0m: 2.601, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82908
[1mStep[0m  [2/21], [94mLoss[0m : 2.83920
[1mStep[0m  [4/21], [94mLoss[0m : 2.78569
[1mStep[0m  [6/21], [94mLoss[0m : 2.74408
[1mStep[0m  [8/21], [94mLoss[0m : 2.95444
[1mStep[0m  [10/21], [94mLoss[0m : 2.72460
[1mStep[0m  [12/21], [94mLoss[0m : 2.72104
[1mStep[0m  [14/21], [94mLoss[0m : 2.77620
[1mStep[0m  [16/21], [94mLoss[0m : 2.86685
[1mStep[0m  [18/21], [94mLoss[0m : 2.81128
[1mStep[0m  [20/21], [94mLoss[0m : 2.68852

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.767, [92mTest[0m: 2.605, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73198
[1mStep[0m  [2/21], [94mLoss[0m : 2.93738
[1mStep[0m  [4/21], [94mLoss[0m : 2.68433
[1mStep[0m  [6/21], [94mLoss[0m : 2.55193
[1mStep[0m  [8/21], [94mLoss[0m : 2.76847
[1mStep[0m  [10/21], [94mLoss[0m : 2.70514
[1mStep[0m  [12/21], [94mLoss[0m : 2.80844
[1mStep[0m  [14/21], [94mLoss[0m : 2.60097
[1mStep[0m  [16/21], [94mLoss[0m : 2.69626
[1mStep[0m  [18/21], [94mLoss[0m : 2.79126
[1mStep[0m  [20/21], [94mLoss[0m : 2.58939

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.734, [92mTest[0m: 2.576, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78830
[1mStep[0m  [2/21], [94mLoss[0m : 2.67033
[1mStep[0m  [4/21], [94mLoss[0m : 2.69080
[1mStep[0m  [6/21], [94mLoss[0m : 2.69133
[1mStep[0m  [8/21], [94mLoss[0m : 2.65664
[1mStep[0m  [10/21], [94mLoss[0m : 2.60511
[1mStep[0m  [12/21], [94mLoss[0m : 2.74333
[1mStep[0m  [14/21], [94mLoss[0m : 2.74085
[1mStep[0m  [16/21], [94mLoss[0m : 2.80849
[1mStep[0m  [18/21], [94mLoss[0m : 2.60330
[1mStep[0m  [20/21], [94mLoss[0m : 2.76502

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.556, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71704
[1mStep[0m  [2/21], [94mLoss[0m : 2.58303
[1mStep[0m  [4/21], [94mLoss[0m : 2.61911
[1mStep[0m  [6/21], [94mLoss[0m : 2.51249
[1mStep[0m  [8/21], [94mLoss[0m : 2.54963
[1mStep[0m  [10/21], [94mLoss[0m : 2.74421
[1mStep[0m  [12/21], [94mLoss[0m : 2.74224
[1mStep[0m  [14/21], [94mLoss[0m : 2.41663
[1mStep[0m  [16/21], [94mLoss[0m : 2.67390
[1mStep[0m  [18/21], [94mLoss[0m : 2.58737
[1mStep[0m  [20/21], [94mLoss[0m : 2.75927

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.571, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73071
[1mStep[0m  [2/21], [94mLoss[0m : 2.65137
[1mStep[0m  [4/21], [94mLoss[0m : 2.64073
[1mStep[0m  [6/21], [94mLoss[0m : 2.51506
[1mStep[0m  [8/21], [94mLoss[0m : 2.77315
[1mStep[0m  [10/21], [94mLoss[0m : 2.65644
[1mStep[0m  [12/21], [94mLoss[0m : 2.55640
[1mStep[0m  [14/21], [94mLoss[0m : 2.74311
[1mStep[0m  [16/21], [94mLoss[0m : 2.68210
[1mStep[0m  [18/21], [94mLoss[0m : 2.58589
[1mStep[0m  [20/21], [94mLoss[0m : 2.65903

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57870
[1mStep[0m  [2/21], [94mLoss[0m : 2.67422
[1mStep[0m  [4/21], [94mLoss[0m : 2.51999
[1mStep[0m  [6/21], [94mLoss[0m : 2.48848
[1mStep[0m  [8/21], [94mLoss[0m : 2.66233
[1mStep[0m  [10/21], [94mLoss[0m : 2.68902
[1mStep[0m  [12/21], [94mLoss[0m : 2.79224
[1mStep[0m  [14/21], [94mLoss[0m : 2.85306
[1mStep[0m  [16/21], [94mLoss[0m : 2.49647
[1mStep[0m  [18/21], [94mLoss[0m : 2.66493
[1mStep[0m  [20/21], [94mLoss[0m : 2.56455

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64771
[1mStep[0m  [2/21], [94mLoss[0m : 2.40191
[1mStep[0m  [4/21], [94mLoss[0m : 2.55232
[1mStep[0m  [6/21], [94mLoss[0m : 2.62718
[1mStep[0m  [8/21], [94mLoss[0m : 2.50682
[1mStep[0m  [10/21], [94mLoss[0m : 2.55863
[1mStep[0m  [12/21], [94mLoss[0m : 2.55603
[1mStep[0m  [14/21], [94mLoss[0m : 2.45869
[1mStep[0m  [16/21], [94mLoss[0m : 2.84222
[1mStep[0m  [18/21], [94mLoss[0m : 2.64707
[1mStep[0m  [20/21], [94mLoss[0m : 2.54083

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.540, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63193
[1mStep[0m  [2/21], [94mLoss[0m : 2.60333
[1mStep[0m  [4/21], [94mLoss[0m : 2.57438
[1mStep[0m  [6/21], [94mLoss[0m : 2.56593
[1mStep[0m  [8/21], [94mLoss[0m : 2.64311
[1mStep[0m  [10/21], [94mLoss[0m : 2.51887
[1mStep[0m  [12/21], [94mLoss[0m : 2.64798
[1mStep[0m  [14/21], [94mLoss[0m : 2.56022
[1mStep[0m  [16/21], [94mLoss[0m : 2.60763
[1mStep[0m  [18/21], [94mLoss[0m : 2.48344
[1mStep[0m  [20/21], [94mLoss[0m : 2.65822

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.533, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75687
[1mStep[0m  [2/21], [94mLoss[0m : 2.56017
[1mStep[0m  [4/21], [94mLoss[0m : 2.61323
[1mStep[0m  [6/21], [94mLoss[0m : 2.53160
[1mStep[0m  [8/21], [94mLoss[0m : 2.46434
[1mStep[0m  [10/21], [94mLoss[0m : 2.48822
[1mStep[0m  [12/21], [94mLoss[0m : 2.71392
[1mStep[0m  [14/21], [94mLoss[0m : 2.42614
[1mStep[0m  [16/21], [94mLoss[0m : 2.43315
[1mStep[0m  [18/21], [94mLoss[0m : 2.80837
[1mStep[0m  [20/21], [94mLoss[0m : 2.57320

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.63303
[1mStep[0m  [2/21], [94mLoss[0m : 2.49455
[1mStep[0m  [4/21], [94mLoss[0m : 2.60614
[1mStep[0m  [6/21], [94mLoss[0m : 2.49980
[1mStep[0m  [8/21], [94mLoss[0m : 2.53564
[1mStep[0m  [10/21], [94mLoss[0m : 2.65727
[1mStep[0m  [12/21], [94mLoss[0m : 2.58047
[1mStep[0m  [14/21], [94mLoss[0m : 2.54698
[1mStep[0m  [16/21], [94mLoss[0m : 2.56720
[1mStep[0m  [18/21], [94mLoss[0m : 2.64497
[1mStep[0m  [20/21], [94mLoss[0m : 2.47579

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42452
[1mStep[0m  [2/21], [94mLoss[0m : 2.45607
[1mStep[0m  [4/21], [94mLoss[0m : 2.50987
[1mStep[0m  [6/21], [94mLoss[0m : 2.66638
[1mStep[0m  [8/21], [94mLoss[0m : 2.60500
[1mStep[0m  [10/21], [94mLoss[0m : 2.55631
[1mStep[0m  [12/21], [94mLoss[0m : 2.65194
[1mStep[0m  [14/21], [94mLoss[0m : 2.64898
[1mStep[0m  [16/21], [94mLoss[0m : 2.62203
[1mStep[0m  [18/21], [94mLoss[0m : 2.65071
[1mStep[0m  [20/21], [94mLoss[0m : 2.36757

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.486, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53268
[1mStep[0m  [2/21], [94mLoss[0m : 2.63739
[1mStep[0m  [4/21], [94mLoss[0m : 2.41740
[1mStep[0m  [6/21], [94mLoss[0m : 2.53085
[1mStep[0m  [8/21], [94mLoss[0m : 2.68242
[1mStep[0m  [10/21], [94mLoss[0m : 2.60185
[1mStep[0m  [12/21], [94mLoss[0m : 2.52066
[1mStep[0m  [14/21], [94mLoss[0m : 2.49337
[1mStep[0m  [16/21], [94mLoss[0m : 2.40506
[1mStep[0m  [18/21], [94mLoss[0m : 2.51248
[1mStep[0m  [20/21], [94mLoss[0m : 2.48785

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70305
[1mStep[0m  [2/21], [94mLoss[0m : 2.39371
[1mStep[0m  [4/21], [94mLoss[0m : 2.54370
[1mStep[0m  [6/21], [94mLoss[0m : 2.45635
[1mStep[0m  [8/21], [94mLoss[0m : 2.48325
[1mStep[0m  [10/21], [94mLoss[0m : 2.53981
[1mStep[0m  [12/21], [94mLoss[0m : 2.47751
[1mStep[0m  [14/21], [94mLoss[0m : 2.57229
[1mStep[0m  [16/21], [94mLoss[0m : 2.56869
[1mStep[0m  [18/21], [94mLoss[0m : 2.47030
[1mStep[0m  [20/21], [94mLoss[0m : 2.40543

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41304
[1mStep[0m  [2/21], [94mLoss[0m : 2.59597
[1mStep[0m  [4/21], [94mLoss[0m : 2.47717
[1mStep[0m  [6/21], [94mLoss[0m : 2.61919
[1mStep[0m  [8/21], [94mLoss[0m : 2.58178
[1mStep[0m  [10/21], [94mLoss[0m : 2.57157
[1mStep[0m  [12/21], [94mLoss[0m : 2.41601
[1mStep[0m  [14/21], [94mLoss[0m : 2.30470
[1mStep[0m  [16/21], [94mLoss[0m : 2.57423
[1mStep[0m  [18/21], [94mLoss[0m : 2.47722
[1mStep[0m  [20/21], [94mLoss[0m : 2.46547

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.476
====================================

Phase 2 - Evaluation MAE:  2.4763829367501393
MAE score P1       4.287117
MAE score P2       2.476383
loss               2.494964
learning_rate      0.002575
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay          0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 11.95307
[1mStep[0m  [33/339], [94mLoss[0m : 2.54724
[1mStep[0m  [66/339], [94mLoss[0m : 3.13530
[1mStep[0m  [99/339], [94mLoss[0m : 2.64301
[1mStep[0m  [132/339], [94mLoss[0m : 2.30073
[1mStep[0m  [165/339], [94mLoss[0m : 2.63691
[1mStep[0m  [198/339], [94mLoss[0m : 3.05664
[1mStep[0m  [231/339], [94mLoss[0m : 2.19217
[1mStep[0m  [264/339], [94mLoss[0m : 2.48796
[1mStep[0m  [297/339], [94mLoss[0m : 2.49579
[1mStep[0m  [330/339], [94mLoss[0m : 2.88051

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.177, [92mTest[0m: 10.883, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80171
[1mStep[0m  [33/339], [94mLoss[0m : 2.34900
[1mStep[0m  [66/339], [94mLoss[0m : 2.41360
[1mStep[0m  [99/339], [94mLoss[0m : 2.76394
[1mStep[0m  [132/339], [94mLoss[0m : 2.32519
[1mStep[0m  [165/339], [94mLoss[0m : 2.39107
[1mStep[0m  [198/339], [94mLoss[0m : 2.12222
[1mStep[0m  [231/339], [94mLoss[0m : 2.61442
[1mStep[0m  [264/339], [94mLoss[0m : 2.87113
[1mStep[0m  [297/339], [94mLoss[0m : 2.85166
[1mStep[0m  [330/339], [94mLoss[0m : 2.22274

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.34525
[1mStep[0m  [33/339], [94mLoss[0m : 1.75033
[1mStep[0m  [66/339], [94mLoss[0m : 2.36167
[1mStep[0m  [99/339], [94mLoss[0m : 1.98514
[1mStep[0m  [132/339], [94mLoss[0m : 2.59738
[1mStep[0m  [165/339], [94mLoss[0m : 2.30887
[1mStep[0m  [198/339], [94mLoss[0m : 3.00466
[1mStep[0m  [231/339], [94mLoss[0m : 3.00193
[1mStep[0m  [264/339], [94mLoss[0m : 2.45807
[1mStep[0m  [297/339], [94mLoss[0m : 2.93540
[1mStep[0m  [330/339], [94mLoss[0m : 2.50608

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36685
[1mStep[0m  [33/339], [94mLoss[0m : 3.01569
[1mStep[0m  [66/339], [94mLoss[0m : 2.60124
[1mStep[0m  [99/339], [94mLoss[0m : 2.94715
[1mStep[0m  [132/339], [94mLoss[0m : 2.49215
[1mStep[0m  [165/339], [94mLoss[0m : 2.44234
[1mStep[0m  [198/339], [94mLoss[0m : 2.79953
[1mStep[0m  [231/339], [94mLoss[0m : 3.24600
[1mStep[0m  [264/339], [94mLoss[0m : 2.55296
[1mStep[0m  [297/339], [94mLoss[0m : 2.77717
[1mStep[0m  [330/339], [94mLoss[0m : 2.38614

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56717
[1mStep[0m  [33/339], [94mLoss[0m : 2.41131
[1mStep[0m  [66/339], [94mLoss[0m : 2.71062
[1mStep[0m  [99/339], [94mLoss[0m : 2.31838
[1mStep[0m  [132/339], [94mLoss[0m : 2.82374
[1mStep[0m  [165/339], [94mLoss[0m : 2.21074
[1mStep[0m  [198/339], [94mLoss[0m : 2.80858
[1mStep[0m  [231/339], [94mLoss[0m : 2.98899
[1mStep[0m  [264/339], [94mLoss[0m : 2.52175
[1mStep[0m  [297/339], [94mLoss[0m : 2.78019
[1mStep[0m  [330/339], [94mLoss[0m : 2.40548

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66382
[1mStep[0m  [33/339], [94mLoss[0m : 2.43120
[1mStep[0m  [66/339], [94mLoss[0m : 3.05667
[1mStep[0m  [99/339], [94mLoss[0m : 2.14086
[1mStep[0m  [132/339], [94mLoss[0m : 2.62334
[1mStep[0m  [165/339], [94mLoss[0m : 2.07264
[1mStep[0m  [198/339], [94mLoss[0m : 2.45262
[1mStep[0m  [231/339], [94mLoss[0m : 1.92344
[1mStep[0m  [264/339], [94mLoss[0m : 2.78923
[1mStep[0m  [297/339], [94mLoss[0m : 2.05647
[1mStep[0m  [330/339], [94mLoss[0m : 2.37048

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23795
[1mStep[0m  [33/339], [94mLoss[0m : 1.92558
[1mStep[0m  [66/339], [94mLoss[0m : 2.71888
[1mStep[0m  [99/339], [94mLoss[0m : 2.44046
[1mStep[0m  [132/339], [94mLoss[0m : 2.59657
[1mStep[0m  [165/339], [94mLoss[0m : 2.19479
[1mStep[0m  [198/339], [94mLoss[0m : 2.04777
[1mStep[0m  [231/339], [94mLoss[0m : 2.99507
[1mStep[0m  [264/339], [94mLoss[0m : 2.32483
[1mStep[0m  [297/339], [94mLoss[0m : 2.59715
[1mStep[0m  [330/339], [94mLoss[0m : 2.75471

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70728
[1mStep[0m  [33/339], [94mLoss[0m : 2.93693
[1mStep[0m  [66/339], [94mLoss[0m : 2.43104
[1mStep[0m  [99/339], [94mLoss[0m : 2.53026
[1mStep[0m  [132/339], [94mLoss[0m : 2.33613
[1mStep[0m  [165/339], [94mLoss[0m : 1.64877
[1mStep[0m  [198/339], [94mLoss[0m : 2.57319
[1mStep[0m  [231/339], [94mLoss[0m : 2.48724
[1mStep[0m  [264/339], [94mLoss[0m : 2.57166
[1mStep[0m  [297/339], [94mLoss[0m : 2.80528
[1mStep[0m  [330/339], [94mLoss[0m : 2.55778

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.352, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38959
[1mStep[0m  [33/339], [94mLoss[0m : 2.65661
[1mStep[0m  [66/339], [94mLoss[0m : 2.20402
[1mStep[0m  [99/339], [94mLoss[0m : 2.38277
[1mStep[0m  [132/339], [94mLoss[0m : 2.57129
[1mStep[0m  [165/339], [94mLoss[0m : 2.48677
[1mStep[0m  [198/339], [94mLoss[0m : 2.97180
[1mStep[0m  [231/339], [94mLoss[0m : 2.62717
[1mStep[0m  [264/339], [94mLoss[0m : 2.22645
[1mStep[0m  [297/339], [94mLoss[0m : 2.57497
[1mStep[0m  [330/339], [94mLoss[0m : 1.80019

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.317, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30451
[1mStep[0m  [33/339], [94mLoss[0m : 2.67536
[1mStep[0m  [66/339], [94mLoss[0m : 2.71903
[1mStep[0m  [99/339], [94mLoss[0m : 2.53960
[1mStep[0m  [132/339], [94mLoss[0m : 2.39304
[1mStep[0m  [165/339], [94mLoss[0m : 2.58798
[1mStep[0m  [198/339], [94mLoss[0m : 2.35621
[1mStep[0m  [231/339], [94mLoss[0m : 2.59059
[1mStep[0m  [264/339], [94mLoss[0m : 2.67459
[1mStep[0m  [297/339], [94mLoss[0m : 2.37121
[1mStep[0m  [330/339], [94mLoss[0m : 1.91207

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44383
[1mStep[0m  [33/339], [94mLoss[0m : 2.31471
[1mStep[0m  [66/339], [94mLoss[0m : 3.06921
[1mStep[0m  [99/339], [94mLoss[0m : 2.53080
[1mStep[0m  [132/339], [94mLoss[0m : 2.21806
[1mStep[0m  [165/339], [94mLoss[0m : 2.69673
[1mStep[0m  [198/339], [94mLoss[0m : 2.71182
[1mStep[0m  [231/339], [94mLoss[0m : 2.28703
[1mStep[0m  [264/339], [94mLoss[0m : 2.45107
[1mStep[0m  [297/339], [94mLoss[0m : 2.45830
[1mStep[0m  [330/339], [94mLoss[0m : 2.58729

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.355, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92836
[1mStep[0m  [33/339], [94mLoss[0m : 2.56200
[1mStep[0m  [66/339], [94mLoss[0m : 2.30560
[1mStep[0m  [99/339], [94mLoss[0m : 2.52350
[1mStep[0m  [132/339], [94mLoss[0m : 2.06325
[1mStep[0m  [165/339], [94mLoss[0m : 2.58900
[1mStep[0m  [198/339], [94mLoss[0m : 2.95440
[1mStep[0m  [231/339], [94mLoss[0m : 2.88356
[1mStep[0m  [264/339], [94mLoss[0m : 2.34328
[1mStep[0m  [297/339], [94mLoss[0m : 2.35006
[1mStep[0m  [330/339], [94mLoss[0m : 2.31244

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.337, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81673
[1mStep[0m  [33/339], [94mLoss[0m : 2.21335
[1mStep[0m  [66/339], [94mLoss[0m : 2.63180
[1mStep[0m  [99/339], [94mLoss[0m : 2.86739
[1mStep[0m  [132/339], [94mLoss[0m : 2.57518
[1mStep[0m  [165/339], [94mLoss[0m : 2.32445
[1mStep[0m  [198/339], [94mLoss[0m : 2.60525
[1mStep[0m  [231/339], [94mLoss[0m : 2.66876
[1mStep[0m  [264/339], [94mLoss[0m : 2.87985
[1mStep[0m  [297/339], [94mLoss[0m : 2.70289
[1mStep[0m  [330/339], [94mLoss[0m : 2.21092

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55319
[1mStep[0m  [33/339], [94mLoss[0m : 3.34939
[1mStep[0m  [66/339], [94mLoss[0m : 2.67865
[1mStep[0m  [99/339], [94mLoss[0m : 2.01809
[1mStep[0m  [132/339], [94mLoss[0m : 2.25210
[1mStep[0m  [165/339], [94mLoss[0m : 2.47475
[1mStep[0m  [198/339], [94mLoss[0m : 2.61175
[1mStep[0m  [231/339], [94mLoss[0m : 2.20872
[1mStep[0m  [264/339], [94mLoss[0m : 2.22831
[1mStep[0m  [297/339], [94mLoss[0m : 1.90622
[1mStep[0m  [330/339], [94mLoss[0m : 2.58605

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.314, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48206
[1mStep[0m  [33/339], [94mLoss[0m : 2.62624
[1mStep[0m  [66/339], [94mLoss[0m : 1.66510
[1mStep[0m  [99/339], [94mLoss[0m : 2.91620
[1mStep[0m  [132/339], [94mLoss[0m : 2.71403
[1mStep[0m  [165/339], [94mLoss[0m : 2.33821
[1mStep[0m  [198/339], [94mLoss[0m : 2.50435
[1mStep[0m  [231/339], [94mLoss[0m : 2.24491
[1mStep[0m  [264/339], [94mLoss[0m : 1.85397
[1mStep[0m  [297/339], [94mLoss[0m : 1.99403
[1mStep[0m  [330/339], [94mLoss[0m : 2.39860

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56053
[1mStep[0m  [33/339], [94mLoss[0m : 3.63690
[1mStep[0m  [66/339], [94mLoss[0m : 1.42899
[1mStep[0m  [99/339], [94mLoss[0m : 2.24605
[1mStep[0m  [132/339], [94mLoss[0m : 2.74527
[1mStep[0m  [165/339], [94mLoss[0m : 2.70755
[1mStep[0m  [198/339], [94mLoss[0m : 2.66096
[1mStep[0m  [231/339], [94mLoss[0m : 2.60749
[1mStep[0m  [264/339], [94mLoss[0m : 2.15520
[1mStep[0m  [297/339], [94mLoss[0m : 2.28674
[1mStep[0m  [330/339], [94mLoss[0m : 2.38920

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.353, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60178
[1mStep[0m  [33/339], [94mLoss[0m : 2.33608
[1mStep[0m  [66/339], [94mLoss[0m : 2.85052
[1mStep[0m  [99/339], [94mLoss[0m : 2.97406
[1mStep[0m  [132/339], [94mLoss[0m : 1.95397
[1mStep[0m  [165/339], [94mLoss[0m : 2.48661
[1mStep[0m  [198/339], [94mLoss[0m : 2.58812
[1mStep[0m  [231/339], [94mLoss[0m : 2.19612
[1mStep[0m  [264/339], [94mLoss[0m : 3.08064
[1mStep[0m  [297/339], [94mLoss[0m : 2.86256
[1mStep[0m  [330/339], [94mLoss[0m : 2.49551

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76729
[1mStep[0m  [33/339], [94mLoss[0m : 2.17615
[1mStep[0m  [66/339], [94mLoss[0m : 2.82450
[1mStep[0m  [99/339], [94mLoss[0m : 2.35591
[1mStep[0m  [132/339], [94mLoss[0m : 2.28921
[1mStep[0m  [165/339], [94mLoss[0m : 3.02280
[1mStep[0m  [198/339], [94mLoss[0m : 2.80235
[1mStep[0m  [231/339], [94mLoss[0m : 3.12049
[1mStep[0m  [264/339], [94mLoss[0m : 2.04039
[1mStep[0m  [297/339], [94mLoss[0m : 2.41799
[1mStep[0m  [330/339], [94mLoss[0m : 3.25950

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.306, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06277
[1mStep[0m  [33/339], [94mLoss[0m : 2.77734
[1mStep[0m  [66/339], [94mLoss[0m : 2.68568
[1mStep[0m  [99/339], [94mLoss[0m : 2.26314
[1mStep[0m  [132/339], [94mLoss[0m : 2.48721
[1mStep[0m  [165/339], [94mLoss[0m : 2.94843
[1mStep[0m  [198/339], [94mLoss[0m : 2.43076
[1mStep[0m  [231/339], [94mLoss[0m : 2.45939
[1mStep[0m  [264/339], [94mLoss[0m : 2.56911
[1mStep[0m  [297/339], [94mLoss[0m : 1.85210
[1mStep[0m  [330/339], [94mLoss[0m : 2.27074

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.317, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13885
[1mStep[0m  [33/339], [94mLoss[0m : 2.94732
[1mStep[0m  [66/339], [94mLoss[0m : 2.63533
[1mStep[0m  [99/339], [94mLoss[0m : 2.85307
[1mStep[0m  [132/339], [94mLoss[0m : 2.11176
[1mStep[0m  [165/339], [94mLoss[0m : 2.46557
[1mStep[0m  [198/339], [94mLoss[0m : 2.78594
[1mStep[0m  [231/339], [94mLoss[0m : 2.49876
[1mStep[0m  [264/339], [94mLoss[0m : 2.21187
[1mStep[0m  [297/339], [94mLoss[0m : 2.23666
[1mStep[0m  [330/339], [94mLoss[0m : 1.78781

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.371, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17443
[1mStep[0m  [33/339], [94mLoss[0m : 2.51418
[1mStep[0m  [66/339], [94mLoss[0m : 2.45716
[1mStep[0m  [99/339], [94mLoss[0m : 2.69662
[1mStep[0m  [132/339], [94mLoss[0m : 2.04149
[1mStep[0m  [165/339], [94mLoss[0m : 2.71302
[1mStep[0m  [198/339], [94mLoss[0m : 2.60937
[1mStep[0m  [231/339], [94mLoss[0m : 2.70716
[1mStep[0m  [264/339], [94mLoss[0m : 2.63521
[1mStep[0m  [297/339], [94mLoss[0m : 2.83555
[1mStep[0m  [330/339], [94mLoss[0m : 2.88868

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67638
[1mStep[0m  [33/339], [94mLoss[0m : 2.58331
[1mStep[0m  [66/339], [94mLoss[0m : 3.08421
[1mStep[0m  [99/339], [94mLoss[0m : 2.18140
[1mStep[0m  [132/339], [94mLoss[0m : 2.32445
[1mStep[0m  [165/339], [94mLoss[0m : 2.33527
[1mStep[0m  [198/339], [94mLoss[0m : 2.25747
[1mStep[0m  [231/339], [94mLoss[0m : 2.42806
[1mStep[0m  [264/339], [94mLoss[0m : 2.03872
[1mStep[0m  [297/339], [94mLoss[0m : 2.55520
[1mStep[0m  [330/339], [94mLoss[0m : 2.85895

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.342, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31176
[1mStep[0m  [33/339], [94mLoss[0m : 2.13548
[1mStep[0m  [66/339], [94mLoss[0m : 2.75368
[1mStep[0m  [99/339], [94mLoss[0m : 2.04992
[1mStep[0m  [132/339], [94mLoss[0m : 2.76793
[1mStep[0m  [165/339], [94mLoss[0m : 2.59121
[1mStep[0m  [198/339], [94mLoss[0m : 1.93591
[1mStep[0m  [231/339], [94mLoss[0m : 2.12628
[1mStep[0m  [264/339], [94mLoss[0m : 2.94936
[1mStep[0m  [297/339], [94mLoss[0m : 2.66075
[1mStep[0m  [330/339], [94mLoss[0m : 2.23924

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37081
[1mStep[0m  [33/339], [94mLoss[0m : 2.79543
[1mStep[0m  [66/339], [94mLoss[0m : 2.49463
[1mStep[0m  [99/339], [94mLoss[0m : 1.81707
[1mStep[0m  [132/339], [94mLoss[0m : 2.58206
[1mStep[0m  [165/339], [94mLoss[0m : 2.33544
[1mStep[0m  [198/339], [94mLoss[0m : 1.93693
[1mStep[0m  [231/339], [94mLoss[0m : 2.71938
[1mStep[0m  [264/339], [94mLoss[0m : 1.99881
[1mStep[0m  [297/339], [94mLoss[0m : 2.66530
[1mStep[0m  [330/339], [94mLoss[0m : 2.31454

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.366, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88590
[1mStep[0m  [33/339], [94mLoss[0m : 2.03333
[1mStep[0m  [66/339], [94mLoss[0m : 2.60681
[1mStep[0m  [99/339], [94mLoss[0m : 2.80989
[1mStep[0m  [132/339], [94mLoss[0m : 2.66951
[1mStep[0m  [165/339], [94mLoss[0m : 3.60486
[1mStep[0m  [198/339], [94mLoss[0m : 2.77554
[1mStep[0m  [231/339], [94mLoss[0m : 3.71079
[1mStep[0m  [264/339], [94mLoss[0m : 2.46448
[1mStep[0m  [297/339], [94mLoss[0m : 2.18038
[1mStep[0m  [330/339], [94mLoss[0m : 3.25441

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43527
[1mStep[0m  [33/339], [94mLoss[0m : 1.97155
[1mStep[0m  [66/339], [94mLoss[0m : 2.82378
[1mStep[0m  [99/339], [94mLoss[0m : 2.81888
[1mStep[0m  [132/339], [94mLoss[0m : 2.40199
[1mStep[0m  [165/339], [94mLoss[0m : 2.18596
[1mStep[0m  [198/339], [94mLoss[0m : 2.85633
[1mStep[0m  [231/339], [94mLoss[0m : 2.78370
[1mStep[0m  [264/339], [94mLoss[0m : 2.17614
[1mStep[0m  [297/339], [94mLoss[0m : 2.06810
[1mStep[0m  [330/339], [94mLoss[0m : 2.15386

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.370, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52729
[1mStep[0m  [33/339], [94mLoss[0m : 2.57607
[1mStep[0m  [66/339], [94mLoss[0m : 2.34956
[1mStep[0m  [99/339], [94mLoss[0m : 2.41930
[1mStep[0m  [132/339], [94mLoss[0m : 2.49281
[1mStep[0m  [165/339], [94mLoss[0m : 3.04211
[1mStep[0m  [198/339], [94mLoss[0m : 2.72185
[1mStep[0m  [231/339], [94mLoss[0m : 2.10301
[1mStep[0m  [264/339], [94mLoss[0m : 2.41568
[1mStep[0m  [297/339], [94mLoss[0m : 1.94052
[1mStep[0m  [330/339], [94mLoss[0m : 2.53763

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.293, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17071
[1mStep[0m  [33/339], [94mLoss[0m : 3.02580
[1mStep[0m  [66/339], [94mLoss[0m : 2.20087
[1mStep[0m  [99/339], [94mLoss[0m : 2.09181
[1mStep[0m  [132/339], [94mLoss[0m : 2.66088
[1mStep[0m  [165/339], [94mLoss[0m : 2.28252
[1mStep[0m  [198/339], [94mLoss[0m : 2.58594
[1mStep[0m  [231/339], [94mLoss[0m : 2.05852
[1mStep[0m  [264/339], [94mLoss[0m : 2.70793
[1mStep[0m  [297/339], [94mLoss[0m : 1.98407
[1mStep[0m  [330/339], [94mLoss[0m : 2.93789

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.286, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12692
[1mStep[0m  [33/339], [94mLoss[0m : 2.69328
[1mStep[0m  [66/339], [94mLoss[0m : 2.44437
[1mStep[0m  [99/339], [94mLoss[0m : 2.49636
[1mStep[0m  [132/339], [94mLoss[0m : 1.71889
[1mStep[0m  [165/339], [94mLoss[0m : 2.07082
[1mStep[0m  [198/339], [94mLoss[0m : 2.58814
[1mStep[0m  [231/339], [94mLoss[0m : 2.55311
[1mStep[0m  [264/339], [94mLoss[0m : 2.07040
[1mStep[0m  [297/339], [94mLoss[0m : 2.47957
[1mStep[0m  [330/339], [94mLoss[0m : 2.21498

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.300, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22740
[1mStep[0m  [33/339], [94mLoss[0m : 2.17950
[1mStep[0m  [66/339], [94mLoss[0m : 2.36845
[1mStep[0m  [99/339], [94mLoss[0m : 3.10838
[1mStep[0m  [132/339], [94mLoss[0m : 2.85019
[1mStep[0m  [165/339], [94mLoss[0m : 2.60214
[1mStep[0m  [198/339], [94mLoss[0m : 2.51966
[1mStep[0m  [231/339], [94mLoss[0m : 1.90714
[1mStep[0m  [264/339], [94mLoss[0m : 2.68081
[1mStep[0m  [297/339], [94mLoss[0m : 2.49668
[1mStep[0m  [330/339], [94mLoss[0m : 1.75710

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.323
====================================

Phase 1 - Evaluation MAE:  2.323268669896421
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 1.95980
[1mStep[0m  [33/339], [94mLoss[0m : 3.03572
[1mStep[0m  [66/339], [94mLoss[0m : 2.87931
[1mStep[0m  [99/339], [94mLoss[0m : 2.48748
[1mStep[0m  [132/339], [94mLoss[0m : 3.15318
[1mStep[0m  [165/339], [94mLoss[0m : 2.45435
[1mStep[0m  [198/339], [94mLoss[0m : 1.75411
[1mStep[0m  [231/339], [94mLoss[0m : 2.84653
[1mStep[0m  [264/339], [94mLoss[0m : 3.10497
[1mStep[0m  [297/339], [94mLoss[0m : 2.22276
[1mStep[0m  [330/339], [94mLoss[0m : 2.37302

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44533
[1mStep[0m  [33/339], [94mLoss[0m : 2.75202
[1mStep[0m  [66/339], [94mLoss[0m : 2.53023
[1mStep[0m  [99/339], [94mLoss[0m : 2.82937
[1mStep[0m  [132/339], [94mLoss[0m : 2.10869
[1mStep[0m  [165/339], [94mLoss[0m : 2.89564
[1mStep[0m  [198/339], [94mLoss[0m : 2.57832
[1mStep[0m  [231/339], [94mLoss[0m : 2.58669
[1mStep[0m  [264/339], [94mLoss[0m : 2.43593
[1mStep[0m  [297/339], [94mLoss[0m : 2.70209
[1mStep[0m  [330/339], [94mLoss[0m : 3.18940

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39173
[1mStep[0m  [33/339], [94mLoss[0m : 1.72656
[1mStep[0m  [66/339], [94mLoss[0m : 2.24053
[1mStep[0m  [99/339], [94mLoss[0m : 2.34085
[1mStep[0m  [132/339], [94mLoss[0m : 1.89502
[1mStep[0m  [165/339], [94mLoss[0m : 2.68516
[1mStep[0m  [198/339], [94mLoss[0m : 2.44003
[1mStep[0m  [231/339], [94mLoss[0m : 2.00664
[1mStep[0m  [264/339], [94mLoss[0m : 2.11730
[1mStep[0m  [297/339], [94mLoss[0m : 1.98897
[1mStep[0m  [330/339], [94mLoss[0m : 2.39348

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65519
[1mStep[0m  [33/339], [94mLoss[0m : 2.88071
[1mStep[0m  [66/339], [94mLoss[0m : 3.18917
[1mStep[0m  [99/339], [94mLoss[0m : 2.21025
[1mStep[0m  [132/339], [94mLoss[0m : 1.68295
[1mStep[0m  [165/339], [94mLoss[0m : 2.08771
[1mStep[0m  [198/339], [94mLoss[0m : 3.16988
[1mStep[0m  [231/339], [94mLoss[0m : 2.28179
[1mStep[0m  [264/339], [94mLoss[0m : 2.35929
[1mStep[0m  [297/339], [94mLoss[0m : 2.04037
[1mStep[0m  [330/339], [94mLoss[0m : 2.72575

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25584
[1mStep[0m  [33/339], [94mLoss[0m : 1.85236
[1mStep[0m  [66/339], [94mLoss[0m : 2.37591
[1mStep[0m  [99/339], [94mLoss[0m : 3.14090
[1mStep[0m  [132/339], [94mLoss[0m : 2.15440
[1mStep[0m  [165/339], [94mLoss[0m : 2.84188
[1mStep[0m  [198/339], [94mLoss[0m : 2.06375
[1mStep[0m  [231/339], [94mLoss[0m : 2.32545
[1mStep[0m  [264/339], [94mLoss[0m : 2.21581
[1mStep[0m  [297/339], [94mLoss[0m : 1.90654
[1mStep[0m  [330/339], [94mLoss[0m : 2.66963

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26776
[1mStep[0m  [33/339], [94mLoss[0m : 2.01973
[1mStep[0m  [66/339], [94mLoss[0m : 2.22760
[1mStep[0m  [99/339], [94mLoss[0m : 2.33323
[1mStep[0m  [132/339], [94mLoss[0m : 1.95792
[1mStep[0m  [165/339], [94mLoss[0m : 2.48845
[1mStep[0m  [198/339], [94mLoss[0m : 2.54408
[1mStep[0m  [231/339], [94mLoss[0m : 2.45731
[1mStep[0m  [264/339], [94mLoss[0m : 2.59187
[1mStep[0m  [297/339], [94mLoss[0m : 1.61316
[1mStep[0m  [330/339], [94mLoss[0m : 2.82376

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05070
[1mStep[0m  [33/339], [94mLoss[0m : 2.60442
[1mStep[0m  [66/339], [94mLoss[0m : 1.98904
[1mStep[0m  [99/339], [94mLoss[0m : 1.73977
[1mStep[0m  [132/339], [94mLoss[0m : 2.02834
[1mStep[0m  [165/339], [94mLoss[0m : 2.41395
[1mStep[0m  [198/339], [94mLoss[0m : 2.23362
[1mStep[0m  [231/339], [94mLoss[0m : 2.76888
[1mStep[0m  [264/339], [94mLoss[0m : 2.37027
[1mStep[0m  [297/339], [94mLoss[0m : 2.57590
[1mStep[0m  [330/339], [94mLoss[0m : 2.75862

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15327
[1mStep[0m  [33/339], [94mLoss[0m : 2.12244
[1mStep[0m  [66/339], [94mLoss[0m : 3.56064
[1mStep[0m  [99/339], [94mLoss[0m : 2.75659
[1mStep[0m  [132/339], [94mLoss[0m : 2.30008
[1mStep[0m  [165/339], [94mLoss[0m : 2.22540
[1mStep[0m  [198/339], [94mLoss[0m : 2.20412
[1mStep[0m  [231/339], [94mLoss[0m : 2.27139
[1mStep[0m  [264/339], [94mLoss[0m : 2.78467
[1mStep[0m  [297/339], [94mLoss[0m : 2.48569
[1mStep[0m  [330/339], [94mLoss[0m : 2.19270

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13458
[1mStep[0m  [33/339], [94mLoss[0m : 2.38028
[1mStep[0m  [66/339], [94mLoss[0m : 2.69840
[1mStep[0m  [99/339], [94mLoss[0m : 1.84801
[1mStep[0m  [132/339], [94mLoss[0m : 2.82453
[1mStep[0m  [165/339], [94mLoss[0m : 3.44589
[1mStep[0m  [198/339], [94mLoss[0m : 1.75321
[1mStep[0m  [231/339], [94mLoss[0m : 2.43672
[1mStep[0m  [264/339], [94mLoss[0m : 2.42352
[1mStep[0m  [297/339], [94mLoss[0m : 1.84482
[1mStep[0m  [330/339], [94mLoss[0m : 2.27565

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.389, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26563
[1mStep[0m  [33/339], [94mLoss[0m : 2.56889
[1mStep[0m  [66/339], [94mLoss[0m : 3.03676
[1mStep[0m  [99/339], [94mLoss[0m : 2.27758
[1mStep[0m  [132/339], [94mLoss[0m : 2.19547
[1mStep[0m  [165/339], [94mLoss[0m : 2.20254
[1mStep[0m  [198/339], [94mLoss[0m : 2.42249
[1mStep[0m  [231/339], [94mLoss[0m : 2.34938
[1mStep[0m  [264/339], [94mLoss[0m : 2.08488
[1mStep[0m  [297/339], [94mLoss[0m : 2.19756
[1mStep[0m  [330/339], [94mLoss[0m : 2.86444

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30493
[1mStep[0m  [33/339], [94mLoss[0m : 2.46686
[1mStep[0m  [66/339], [94mLoss[0m : 1.85238
[1mStep[0m  [99/339], [94mLoss[0m : 2.28839
[1mStep[0m  [132/339], [94mLoss[0m : 2.24534
[1mStep[0m  [165/339], [94mLoss[0m : 2.29320
[1mStep[0m  [198/339], [94mLoss[0m : 2.11175
[1mStep[0m  [231/339], [94mLoss[0m : 2.75920
[1mStep[0m  [264/339], [94mLoss[0m : 2.79529
[1mStep[0m  [297/339], [94mLoss[0m : 2.12115
[1mStep[0m  [330/339], [94mLoss[0m : 2.64733

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03292
[1mStep[0m  [33/339], [94mLoss[0m : 1.89831
[1mStep[0m  [66/339], [94mLoss[0m : 1.89065
[1mStep[0m  [99/339], [94mLoss[0m : 1.68444
[1mStep[0m  [132/339], [94mLoss[0m : 2.32552
[1mStep[0m  [165/339], [94mLoss[0m : 2.73533
[1mStep[0m  [198/339], [94mLoss[0m : 2.57857
[1mStep[0m  [231/339], [94mLoss[0m : 2.12949
[1mStep[0m  [264/339], [94mLoss[0m : 2.37613
[1mStep[0m  [297/339], [94mLoss[0m : 2.38726
[1mStep[0m  [330/339], [94mLoss[0m : 2.50844

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.429, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35749
[1mStep[0m  [33/339], [94mLoss[0m : 3.28704
[1mStep[0m  [66/339], [94mLoss[0m : 2.21787
[1mStep[0m  [99/339], [94mLoss[0m : 2.24487
[1mStep[0m  [132/339], [94mLoss[0m : 2.22635
[1mStep[0m  [165/339], [94mLoss[0m : 2.20268
[1mStep[0m  [198/339], [94mLoss[0m : 2.04447
[1mStep[0m  [231/339], [94mLoss[0m : 2.12686
[1mStep[0m  [264/339], [94mLoss[0m : 1.99950
[1mStep[0m  [297/339], [94mLoss[0m : 2.29016
[1mStep[0m  [330/339], [94mLoss[0m : 3.25933

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24080
[1mStep[0m  [33/339], [94mLoss[0m : 2.12728
[1mStep[0m  [66/339], [94mLoss[0m : 2.41605
[1mStep[0m  [99/339], [94mLoss[0m : 2.16459
[1mStep[0m  [132/339], [94mLoss[0m : 2.36226
[1mStep[0m  [165/339], [94mLoss[0m : 1.81373
[1mStep[0m  [198/339], [94mLoss[0m : 3.03990
[1mStep[0m  [231/339], [94mLoss[0m : 1.72261
[1mStep[0m  [264/339], [94mLoss[0m : 3.01730
[1mStep[0m  [297/339], [94mLoss[0m : 2.51171
[1mStep[0m  [330/339], [94mLoss[0m : 2.55918

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06063
[1mStep[0m  [33/339], [94mLoss[0m : 1.91853
[1mStep[0m  [66/339], [94mLoss[0m : 2.14400
[1mStep[0m  [99/339], [94mLoss[0m : 2.17833
[1mStep[0m  [132/339], [94mLoss[0m : 2.57014
[1mStep[0m  [165/339], [94mLoss[0m : 2.63765
[1mStep[0m  [198/339], [94mLoss[0m : 2.22919
[1mStep[0m  [231/339], [94mLoss[0m : 2.64692
[1mStep[0m  [264/339], [94mLoss[0m : 2.36414
[1mStep[0m  [297/339], [94mLoss[0m : 2.07672
[1mStep[0m  [330/339], [94mLoss[0m : 1.91871

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.54405
[1mStep[0m  [33/339], [94mLoss[0m : 2.39075
[1mStep[0m  [66/339], [94mLoss[0m : 2.23361
[1mStep[0m  [99/339], [94mLoss[0m : 2.47895
[1mStep[0m  [132/339], [94mLoss[0m : 2.46472
[1mStep[0m  [165/339], [94mLoss[0m : 2.39946
[1mStep[0m  [198/339], [94mLoss[0m : 1.72516
[1mStep[0m  [231/339], [94mLoss[0m : 1.67932
[1mStep[0m  [264/339], [94mLoss[0m : 2.45317
[1mStep[0m  [297/339], [94mLoss[0m : 2.55236
[1mStep[0m  [330/339], [94mLoss[0m : 2.18406

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85853
[1mStep[0m  [33/339], [94mLoss[0m : 2.21492
[1mStep[0m  [66/339], [94mLoss[0m : 1.87164
[1mStep[0m  [99/339], [94mLoss[0m : 2.50911
[1mStep[0m  [132/339], [94mLoss[0m : 2.61560
[1mStep[0m  [165/339], [94mLoss[0m : 1.75975
[1mStep[0m  [198/339], [94mLoss[0m : 2.71067
[1mStep[0m  [231/339], [94mLoss[0m : 2.69635
[1mStep[0m  [264/339], [94mLoss[0m : 2.10322
[1mStep[0m  [297/339], [94mLoss[0m : 2.52380
[1mStep[0m  [330/339], [94mLoss[0m : 2.72698

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.409, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68733
[1mStep[0m  [33/339], [94mLoss[0m : 2.39467
[1mStep[0m  [66/339], [94mLoss[0m : 2.16791
[1mStep[0m  [99/339], [94mLoss[0m : 2.22852
[1mStep[0m  [132/339], [94mLoss[0m : 2.11882
[1mStep[0m  [165/339], [94mLoss[0m : 2.24511
[1mStep[0m  [198/339], [94mLoss[0m : 2.39054
[1mStep[0m  [231/339], [94mLoss[0m : 2.24924
[1mStep[0m  [264/339], [94mLoss[0m : 2.29996
[1mStep[0m  [297/339], [94mLoss[0m : 2.41703
[1mStep[0m  [330/339], [94mLoss[0m : 2.33856

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05439
[1mStep[0m  [33/339], [94mLoss[0m : 1.89560
[1mStep[0m  [66/339], [94mLoss[0m : 2.28369
[1mStep[0m  [99/339], [94mLoss[0m : 2.77845
[1mStep[0m  [132/339], [94mLoss[0m : 2.44149
[1mStep[0m  [165/339], [94mLoss[0m : 2.29613
[1mStep[0m  [198/339], [94mLoss[0m : 2.42501
[1mStep[0m  [231/339], [94mLoss[0m : 2.30437
[1mStep[0m  [264/339], [94mLoss[0m : 2.60103
[1mStep[0m  [297/339], [94mLoss[0m : 3.07436
[1mStep[0m  [330/339], [94mLoss[0m : 2.31972

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91519
[1mStep[0m  [33/339], [94mLoss[0m : 2.01751
[1mStep[0m  [66/339], [94mLoss[0m : 2.02605
[1mStep[0m  [99/339], [94mLoss[0m : 2.34499
[1mStep[0m  [132/339], [94mLoss[0m : 2.09549
[1mStep[0m  [165/339], [94mLoss[0m : 2.76511
[1mStep[0m  [198/339], [94mLoss[0m : 1.72584
[1mStep[0m  [231/339], [94mLoss[0m : 2.11362
[1mStep[0m  [264/339], [94mLoss[0m : 2.35399
[1mStep[0m  [297/339], [94mLoss[0m : 2.37036
[1mStep[0m  [330/339], [94mLoss[0m : 2.47921

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.409, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65892
[1mStep[0m  [33/339], [94mLoss[0m : 1.85954
[1mStep[0m  [66/339], [94mLoss[0m : 2.02592
[1mStep[0m  [99/339], [94mLoss[0m : 2.42589
[1mStep[0m  [132/339], [94mLoss[0m : 2.16999
[1mStep[0m  [165/339], [94mLoss[0m : 2.61052
[1mStep[0m  [198/339], [94mLoss[0m : 2.51035
[1mStep[0m  [231/339], [94mLoss[0m : 1.70099
[1mStep[0m  [264/339], [94mLoss[0m : 1.88712
[1mStep[0m  [297/339], [94mLoss[0m : 2.67289
[1mStep[0m  [330/339], [94mLoss[0m : 2.47921

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.386, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.05185
[1mStep[0m  [33/339], [94mLoss[0m : 2.38718
[1mStep[0m  [66/339], [94mLoss[0m : 1.96765
[1mStep[0m  [99/339], [94mLoss[0m : 2.43262
[1mStep[0m  [132/339], [94mLoss[0m : 2.27546
[1mStep[0m  [165/339], [94mLoss[0m : 2.23111
[1mStep[0m  [198/339], [94mLoss[0m : 1.97860
[1mStep[0m  [231/339], [94mLoss[0m : 3.15831
[1mStep[0m  [264/339], [94mLoss[0m : 2.13603
[1mStep[0m  [297/339], [94mLoss[0m : 2.05030
[1mStep[0m  [330/339], [94mLoss[0m : 2.28629

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.469, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93800
[1mStep[0m  [33/339], [94mLoss[0m : 1.94127
[1mStep[0m  [66/339], [94mLoss[0m : 2.71405
[1mStep[0m  [99/339], [94mLoss[0m : 1.72925
[1mStep[0m  [132/339], [94mLoss[0m : 2.44157
[1mStep[0m  [165/339], [94mLoss[0m : 2.63359
[1mStep[0m  [198/339], [94mLoss[0m : 3.18146
[1mStep[0m  [231/339], [94mLoss[0m : 2.31769
[1mStep[0m  [264/339], [94mLoss[0m : 1.94600
[1mStep[0m  [297/339], [94mLoss[0m : 2.11012
[1mStep[0m  [330/339], [94mLoss[0m : 2.02130

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.479, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.99814
[1mStep[0m  [33/339], [94mLoss[0m : 2.20500
[1mStep[0m  [66/339], [94mLoss[0m : 2.67134
[1mStep[0m  [99/339], [94mLoss[0m : 2.64777
[1mStep[0m  [132/339], [94mLoss[0m : 2.23969
[1mStep[0m  [165/339], [94mLoss[0m : 2.39467
[1mStep[0m  [198/339], [94mLoss[0m : 2.55817
[1mStep[0m  [231/339], [94mLoss[0m : 2.49908
[1mStep[0m  [264/339], [94mLoss[0m : 2.23164
[1mStep[0m  [297/339], [94mLoss[0m : 2.71232
[1mStep[0m  [330/339], [94mLoss[0m : 2.32984

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.421, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20817
[1mStep[0m  [33/339], [94mLoss[0m : 2.04259
[1mStep[0m  [66/339], [94mLoss[0m : 2.66061
[1mStep[0m  [99/339], [94mLoss[0m : 1.75181
[1mStep[0m  [132/339], [94mLoss[0m : 2.55564
[1mStep[0m  [165/339], [94mLoss[0m : 1.64106
[1mStep[0m  [198/339], [94mLoss[0m : 2.81684
[1mStep[0m  [231/339], [94mLoss[0m : 2.12745
[1mStep[0m  [264/339], [94mLoss[0m : 2.28891
[1mStep[0m  [297/339], [94mLoss[0m : 2.12294
[1mStep[0m  [330/339], [94mLoss[0m : 1.83121

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.394, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42699
[1mStep[0m  [33/339], [94mLoss[0m : 1.83762
[1mStep[0m  [66/339], [94mLoss[0m : 2.57493
[1mStep[0m  [99/339], [94mLoss[0m : 1.98486
[1mStep[0m  [132/339], [94mLoss[0m : 2.29432
[1mStep[0m  [165/339], [94mLoss[0m : 1.86515
[1mStep[0m  [198/339], [94mLoss[0m : 1.77486
[1mStep[0m  [231/339], [94mLoss[0m : 2.60000
[1mStep[0m  [264/339], [94mLoss[0m : 2.91142
[1mStep[0m  [297/339], [94mLoss[0m : 1.93511
[1mStep[0m  [330/339], [94mLoss[0m : 3.17985

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.467, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20413
[1mStep[0m  [33/339], [94mLoss[0m : 2.03770
[1mStep[0m  [66/339], [94mLoss[0m : 2.17850
[1mStep[0m  [99/339], [94mLoss[0m : 2.44103
[1mStep[0m  [132/339], [94mLoss[0m : 2.15565
[1mStep[0m  [165/339], [94mLoss[0m : 2.43198
[1mStep[0m  [198/339], [94mLoss[0m : 3.01540
[1mStep[0m  [231/339], [94mLoss[0m : 2.08892
[1mStep[0m  [264/339], [94mLoss[0m : 2.13403
[1mStep[0m  [297/339], [94mLoss[0m : 3.33201
[1mStep[0m  [330/339], [94mLoss[0m : 2.31364

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.436, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05436
[1mStep[0m  [33/339], [94mLoss[0m : 1.86575
[1mStep[0m  [66/339], [94mLoss[0m : 2.98117
[1mStep[0m  [99/339], [94mLoss[0m : 2.27622
[1mStep[0m  [132/339], [94mLoss[0m : 2.13533
[1mStep[0m  [165/339], [94mLoss[0m : 1.94757
[1mStep[0m  [198/339], [94mLoss[0m : 2.13228
[1mStep[0m  [231/339], [94mLoss[0m : 2.97183
[1mStep[0m  [264/339], [94mLoss[0m : 2.57965
[1mStep[0m  [297/339], [94mLoss[0m : 2.41334
[1mStep[0m  [330/339], [94mLoss[0m : 1.84750

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.452, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23723
[1mStep[0m  [33/339], [94mLoss[0m : 1.84531
[1mStep[0m  [66/339], [94mLoss[0m : 2.09428
[1mStep[0m  [99/339], [94mLoss[0m : 2.36406
[1mStep[0m  [132/339], [94mLoss[0m : 2.39551
[1mStep[0m  [165/339], [94mLoss[0m : 2.50301
[1mStep[0m  [198/339], [94mLoss[0m : 2.30286
[1mStep[0m  [231/339], [94mLoss[0m : 1.97149
[1mStep[0m  [264/339], [94mLoss[0m : 1.96576
[1mStep[0m  [297/339], [94mLoss[0m : 2.20179
[1mStep[0m  [330/339], [94mLoss[0m : 3.18726

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.403, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18326
[1mStep[0m  [33/339], [94mLoss[0m : 2.01783
[1mStep[0m  [66/339], [94mLoss[0m : 2.00747
[1mStep[0m  [99/339], [94mLoss[0m : 2.93758
[1mStep[0m  [132/339], [94mLoss[0m : 2.49437
[1mStep[0m  [165/339], [94mLoss[0m : 2.33807
[1mStep[0m  [198/339], [94mLoss[0m : 1.97276
[1mStep[0m  [231/339], [94mLoss[0m : 2.18986
[1mStep[0m  [264/339], [94mLoss[0m : 2.31450
[1mStep[0m  [297/339], [94mLoss[0m : 2.08998
[1mStep[0m  [330/339], [94mLoss[0m : 2.43709

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.433, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.414
====================================

Phase 2 - Evaluation MAE:  2.4143687708187946
MAE score P1       2.323269
MAE score P2       2.414369
loss               2.307938
learning_rate       0.00505
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.64658
[1mStep[0m  [2/21], [94mLoss[0m : 10.81395
[1mStep[0m  [4/21], [94mLoss[0m : 10.91122
[1mStep[0m  [6/21], [94mLoss[0m : 10.52186
[1mStep[0m  [8/21], [94mLoss[0m : 10.47529
[1mStep[0m  [10/21], [94mLoss[0m : 10.36384
[1mStep[0m  [12/21], [94mLoss[0m : 10.19092
[1mStep[0m  [14/21], [94mLoss[0m : 9.93295
[1mStep[0m  [16/21], [94mLoss[0m : 9.90536
[1mStep[0m  [18/21], [94mLoss[0m : 9.81717
[1mStep[0m  [20/21], [94mLoss[0m : 10.10028

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.341, [92mTest[0m: 10.799, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12322
[1mStep[0m  [2/21], [94mLoss[0m : 9.98693
[1mStep[0m  [4/21], [94mLoss[0m : 9.47737
[1mStep[0m  [6/21], [94mLoss[0m : 9.53304
[1mStep[0m  [8/21], [94mLoss[0m : 9.51058
[1mStep[0m  [10/21], [94mLoss[0m : 9.48338
[1mStep[0m  [12/21], [94mLoss[0m : 8.99854
[1mStep[0m  [14/21], [94mLoss[0m : 9.01492
[1mStep[0m  [16/21], [94mLoss[0m : 9.27943
[1mStep[0m  [18/21], [94mLoss[0m : 9.21385
[1mStep[0m  [20/21], [94mLoss[0m : 8.77003

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.417, [92mTest[0m: 10.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.98956
[1mStep[0m  [2/21], [94mLoss[0m : 8.83420
[1mStep[0m  [4/21], [94mLoss[0m : 8.88973
[1mStep[0m  [6/21], [94mLoss[0m : 8.53133
[1mStep[0m  [8/21], [94mLoss[0m : 8.40320
[1mStep[0m  [10/21], [94mLoss[0m : 8.41609
[1mStep[0m  [12/21], [94mLoss[0m : 8.46251
[1mStep[0m  [14/21], [94mLoss[0m : 8.22001
[1mStep[0m  [16/21], [94mLoss[0m : 7.88924
[1mStep[0m  [18/21], [94mLoss[0m : 8.37104
[1mStep[0m  [20/21], [94mLoss[0m : 8.34468

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.478, [92mTest[0m: 9.580, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.92531
[1mStep[0m  [2/21], [94mLoss[0m : 8.11101
[1mStep[0m  [4/21], [94mLoss[0m : 7.76411
[1mStep[0m  [6/21], [94mLoss[0m : 7.69616
[1mStep[0m  [8/21], [94mLoss[0m : 7.46025
[1mStep[0m  [10/21], [94mLoss[0m : 7.42670
[1mStep[0m  [12/21], [94mLoss[0m : 7.32757
[1mStep[0m  [14/21], [94mLoss[0m : 7.49592
[1mStep[0m  [16/21], [94mLoss[0m : 7.44438
[1mStep[0m  [18/21], [94mLoss[0m : 6.96245
[1mStep[0m  [20/21], [94mLoss[0m : 7.07511

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.497, [92mTest[0m: 8.865, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.97051
[1mStep[0m  [2/21], [94mLoss[0m : 6.68020
[1mStep[0m  [4/21], [94mLoss[0m : 6.65179
[1mStep[0m  [6/21], [94mLoss[0m : 7.02281
[1mStep[0m  [8/21], [94mLoss[0m : 6.54870
[1mStep[0m  [10/21], [94mLoss[0m : 6.57439
[1mStep[0m  [12/21], [94mLoss[0m : 6.37456
[1mStep[0m  [14/21], [94mLoss[0m : 6.51068
[1mStep[0m  [16/21], [94mLoss[0m : 6.28193
[1mStep[0m  [18/21], [94mLoss[0m : 6.03387
[1mStep[0m  [20/21], [94mLoss[0m : 6.20335

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.502, [92mTest[0m: 8.145, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.88847
[1mStep[0m  [2/21], [94mLoss[0m : 5.88429
[1mStep[0m  [4/21], [94mLoss[0m : 6.14634
[1mStep[0m  [6/21], [94mLoss[0m : 5.69669
[1mStep[0m  [8/21], [94mLoss[0m : 5.55043
[1mStep[0m  [10/21], [94mLoss[0m : 5.62672
[1mStep[0m  [12/21], [94mLoss[0m : 5.27605
[1mStep[0m  [14/21], [94mLoss[0m : 5.27814
[1mStep[0m  [16/21], [94mLoss[0m : 5.33034
[1mStep[0m  [18/21], [94mLoss[0m : 5.20982
[1mStep[0m  [20/21], [94mLoss[0m : 4.80425

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.510, [92mTest[0m: 7.308, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.01218
[1mStep[0m  [2/21], [94mLoss[0m : 4.83971
[1mStep[0m  [4/21], [94mLoss[0m : 4.96794
[1mStep[0m  [6/21], [94mLoss[0m : 4.90832
[1mStep[0m  [8/21], [94mLoss[0m : 4.85622
[1mStep[0m  [10/21], [94mLoss[0m : 4.66765
[1mStep[0m  [12/21], [94mLoss[0m : 4.84527
[1mStep[0m  [14/21], [94mLoss[0m : 4.37567
[1mStep[0m  [16/21], [94mLoss[0m : 4.51519
[1mStep[0m  [18/21], [94mLoss[0m : 4.24413
[1mStep[0m  [20/21], [94mLoss[0m : 4.05434

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.630, [92mTest[0m: 6.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.28691
[1mStep[0m  [2/21], [94mLoss[0m : 4.43069
[1mStep[0m  [4/21], [94mLoss[0m : 4.03147
[1mStep[0m  [6/21], [94mLoss[0m : 3.92791
[1mStep[0m  [8/21], [94mLoss[0m : 3.77347
[1mStep[0m  [10/21], [94mLoss[0m : 3.91338
[1mStep[0m  [12/21], [94mLoss[0m : 3.97766
[1mStep[0m  [14/21], [94mLoss[0m : 3.83523
[1mStep[0m  [16/21], [94mLoss[0m : 3.82665
[1mStep[0m  [18/21], [94mLoss[0m : 3.93908
[1mStep[0m  [20/21], [94mLoss[0m : 3.74496

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.953, [92mTest[0m: 5.378, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 7 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.584
====================================

Phase 1 - Evaluation MAE:  4.584482397351946
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 3.64000
[1mStep[0m  [2/21], [94mLoss[0m : 3.78909
[1mStep[0m  [4/21], [94mLoss[0m : 3.71640
[1mStep[0m  [6/21], [94mLoss[0m : 3.57217
[1mStep[0m  [8/21], [94mLoss[0m : 3.56475
[1mStep[0m  [10/21], [94mLoss[0m : 3.55211
[1mStep[0m  [12/21], [94mLoss[0m : 3.55804
[1mStep[0m  [14/21], [94mLoss[0m : 3.47136
[1mStep[0m  [16/21], [94mLoss[0m : 3.47575
[1mStep[0m  [18/21], [94mLoss[0m : 3.24953
[1mStep[0m  [20/21], [94mLoss[0m : 3.26287

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.509, [92mTest[0m: 4.596, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.410
====================================

Phase 2 - Evaluation MAE:  5.409912041255406
MAE score P1       4.584482
MAE score P2       5.409912
loss                3.50874
learning_rate      0.002575
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 9.80710
[1mStep[0m  [33/339], [94mLoss[0m : 2.42747
[1mStep[0m  [66/339], [94mLoss[0m : 2.24000
[1mStep[0m  [99/339], [94mLoss[0m : 2.33571
[1mStep[0m  [132/339], [94mLoss[0m : 2.25248
[1mStep[0m  [165/339], [94mLoss[0m : 3.05001
[1mStep[0m  [198/339], [94mLoss[0m : 1.82683
[1mStep[0m  [231/339], [94mLoss[0m : 1.96042
[1mStep[0m  [264/339], [94mLoss[0m : 2.59678
[1mStep[0m  [297/339], [94mLoss[0m : 2.15828
[1mStep[0m  [330/339], [94mLoss[0m : 2.54155

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.860, [92mTest[0m: 10.882, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44200
[1mStep[0m  [33/339], [94mLoss[0m : 3.16010
[1mStep[0m  [66/339], [94mLoss[0m : 2.86072
[1mStep[0m  [99/339], [94mLoss[0m : 2.63756
[1mStep[0m  [132/339], [94mLoss[0m : 2.50597
[1mStep[0m  [165/339], [94mLoss[0m : 2.61448
[1mStep[0m  [198/339], [94mLoss[0m : 2.58233
[1mStep[0m  [231/339], [94mLoss[0m : 2.19132
[1mStep[0m  [264/339], [94mLoss[0m : 2.20730
[1mStep[0m  [297/339], [94mLoss[0m : 2.69845
[1mStep[0m  [330/339], [94mLoss[0m : 2.55030

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.625, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94661
[1mStep[0m  [33/339], [94mLoss[0m : 2.17326
[1mStep[0m  [66/339], [94mLoss[0m : 2.01326
[1mStep[0m  [99/339], [94mLoss[0m : 2.45687
[1mStep[0m  [132/339], [94mLoss[0m : 3.29937
[1mStep[0m  [165/339], [94mLoss[0m : 2.77863
[1mStep[0m  [198/339], [94mLoss[0m : 2.30400
[1mStep[0m  [231/339], [94mLoss[0m : 2.83275
[1mStep[0m  [264/339], [94mLoss[0m : 2.66899
[1mStep[0m  [297/339], [94mLoss[0m : 2.84993
[1mStep[0m  [330/339], [94mLoss[0m : 2.77445

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.534, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72201
[1mStep[0m  [33/339], [94mLoss[0m : 2.28282
[1mStep[0m  [66/339], [94mLoss[0m : 2.89659
[1mStep[0m  [99/339], [94mLoss[0m : 2.24332
[1mStep[0m  [132/339], [94mLoss[0m : 2.92111
[1mStep[0m  [165/339], [94mLoss[0m : 1.97834
[1mStep[0m  [198/339], [94mLoss[0m : 2.52734
[1mStep[0m  [231/339], [94mLoss[0m : 2.74927
[1mStep[0m  [264/339], [94mLoss[0m : 2.61901
[1mStep[0m  [297/339], [94mLoss[0m : 2.15532
[1mStep[0m  [330/339], [94mLoss[0m : 1.99546

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.488, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52316
[1mStep[0m  [33/339], [94mLoss[0m : 2.64752
[1mStep[0m  [66/339], [94mLoss[0m : 1.93343
[1mStep[0m  [99/339], [94mLoss[0m : 3.05980
[1mStep[0m  [132/339], [94mLoss[0m : 2.59779
[1mStep[0m  [165/339], [94mLoss[0m : 3.07443
[1mStep[0m  [198/339], [94mLoss[0m : 2.49902
[1mStep[0m  [231/339], [94mLoss[0m : 2.75559
[1mStep[0m  [264/339], [94mLoss[0m : 2.66255
[1mStep[0m  [297/339], [94mLoss[0m : 2.47319
[1mStep[0m  [330/339], [94mLoss[0m : 2.23686

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88639
[1mStep[0m  [33/339], [94mLoss[0m : 2.54293
[1mStep[0m  [66/339], [94mLoss[0m : 2.43920
[1mStep[0m  [99/339], [94mLoss[0m : 2.27461
[1mStep[0m  [132/339], [94mLoss[0m : 2.35658
[1mStep[0m  [165/339], [94mLoss[0m : 2.58641
[1mStep[0m  [198/339], [94mLoss[0m : 2.57124
[1mStep[0m  [231/339], [94mLoss[0m : 2.18652
[1mStep[0m  [264/339], [94mLoss[0m : 2.54910
[1mStep[0m  [297/339], [94mLoss[0m : 3.28926
[1mStep[0m  [330/339], [94mLoss[0m : 2.63441

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55533
[1mStep[0m  [33/339], [94mLoss[0m : 1.83748
[1mStep[0m  [66/339], [94mLoss[0m : 2.05738
[1mStep[0m  [99/339], [94mLoss[0m : 2.34912
[1mStep[0m  [132/339], [94mLoss[0m : 1.78806
[1mStep[0m  [165/339], [94mLoss[0m : 2.33761
[1mStep[0m  [198/339], [94mLoss[0m : 2.06258
[1mStep[0m  [231/339], [94mLoss[0m : 2.55006
[1mStep[0m  [264/339], [94mLoss[0m : 2.40962
[1mStep[0m  [297/339], [94mLoss[0m : 2.16939
[1mStep[0m  [330/339], [94mLoss[0m : 2.17895

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.464, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44606
[1mStep[0m  [33/339], [94mLoss[0m : 2.45741
[1mStep[0m  [66/339], [94mLoss[0m : 2.13448
[1mStep[0m  [99/339], [94mLoss[0m : 2.36234
[1mStep[0m  [132/339], [94mLoss[0m : 2.59906
[1mStep[0m  [165/339], [94mLoss[0m : 2.39997
[1mStep[0m  [198/339], [94mLoss[0m : 2.72602
[1mStep[0m  [231/339], [94mLoss[0m : 2.45447
[1mStep[0m  [264/339], [94mLoss[0m : 3.32793
[1mStep[0m  [297/339], [94mLoss[0m : 2.63622
[1mStep[0m  [330/339], [94mLoss[0m : 1.85343

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54000
[1mStep[0m  [33/339], [94mLoss[0m : 2.20763
[1mStep[0m  [66/339], [94mLoss[0m : 2.25732
[1mStep[0m  [99/339], [94mLoss[0m : 2.51782
[1mStep[0m  [132/339], [94mLoss[0m : 2.08883
[1mStep[0m  [165/339], [94mLoss[0m : 2.55743
[1mStep[0m  [198/339], [94mLoss[0m : 1.98848
[1mStep[0m  [231/339], [94mLoss[0m : 2.61973
[1mStep[0m  [264/339], [94mLoss[0m : 2.27978
[1mStep[0m  [297/339], [94mLoss[0m : 2.34202
[1mStep[0m  [330/339], [94mLoss[0m : 2.64354

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99739
[1mStep[0m  [33/339], [94mLoss[0m : 2.72506
[1mStep[0m  [66/339], [94mLoss[0m : 2.63118
[1mStep[0m  [99/339], [94mLoss[0m : 2.55468
[1mStep[0m  [132/339], [94mLoss[0m : 2.36434
[1mStep[0m  [165/339], [94mLoss[0m : 2.59784
[1mStep[0m  [198/339], [94mLoss[0m : 2.43598
[1mStep[0m  [231/339], [94mLoss[0m : 2.44814
[1mStep[0m  [264/339], [94mLoss[0m : 1.78843
[1mStep[0m  [297/339], [94mLoss[0m : 2.79283
[1mStep[0m  [330/339], [94mLoss[0m : 2.47684

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.359, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30879
[1mStep[0m  [33/339], [94mLoss[0m : 2.24683
[1mStep[0m  [66/339], [94mLoss[0m : 2.09260
[1mStep[0m  [99/339], [94mLoss[0m : 2.84901
[1mStep[0m  [132/339], [94mLoss[0m : 2.81841
[1mStep[0m  [165/339], [94mLoss[0m : 2.53679
[1mStep[0m  [198/339], [94mLoss[0m : 2.63141
[1mStep[0m  [231/339], [94mLoss[0m : 3.32600
[1mStep[0m  [264/339], [94mLoss[0m : 2.36265
[1mStep[0m  [297/339], [94mLoss[0m : 2.02973
[1mStep[0m  [330/339], [94mLoss[0m : 2.79852

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77205
[1mStep[0m  [33/339], [94mLoss[0m : 3.40615
[1mStep[0m  [66/339], [94mLoss[0m : 2.33129
[1mStep[0m  [99/339], [94mLoss[0m : 3.10449
[1mStep[0m  [132/339], [94mLoss[0m : 2.63034
[1mStep[0m  [165/339], [94mLoss[0m : 2.06400
[1mStep[0m  [198/339], [94mLoss[0m : 2.20006
[1mStep[0m  [231/339], [94mLoss[0m : 1.86066
[1mStep[0m  [264/339], [94mLoss[0m : 2.48215
[1mStep[0m  [297/339], [94mLoss[0m : 2.37885
[1mStep[0m  [330/339], [94mLoss[0m : 2.21639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93550
[1mStep[0m  [33/339], [94mLoss[0m : 2.02832
[1mStep[0m  [66/339], [94mLoss[0m : 2.26206
[1mStep[0m  [99/339], [94mLoss[0m : 2.42562
[1mStep[0m  [132/339], [94mLoss[0m : 2.04929
[1mStep[0m  [165/339], [94mLoss[0m : 2.54844
[1mStep[0m  [198/339], [94mLoss[0m : 2.56439
[1mStep[0m  [231/339], [94mLoss[0m : 2.10894
[1mStep[0m  [264/339], [94mLoss[0m : 2.19296
[1mStep[0m  [297/339], [94mLoss[0m : 2.24397
[1mStep[0m  [330/339], [94mLoss[0m : 2.33078

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85797
[1mStep[0m  [33/339], [94mLoss[0m : 2.50722
[1mStep[0m  [66/339], [94mLoss[0m : 2.00429
[1mStep[0m  [99/339], [94mLoss[0m : 1.95685
[1mStep[0m  [132/339], [94mLoss[0m : 2.42604
[1mStep[0m  [165/339], [94mLoss[0m : 1.68854
[1mStep[0m  [198/339], [94mLoss[0m : 2.37912
[1mStep[0m  [231/339], [94mLoss[0m : 2.51118
[1mStep[0m  [264/339], [94mLoss[0m : 2.63832
[1mStep[0m  [297/339], [94mLoss[0m : 2.70922
[1mStep[0m  [330/339], [94mLoss[0m : 2.61784

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.312, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84703
[1mStep[0m  [33/339], [94mLoss[0m : 2.55681
[1mStep[0m  [66/339], [94mLoss[0m : 2.14289
[1mStep[0m  [99/339], [94mLoss[0m : 2.80529
[1mStep[0m  [132/339], [94mLoss[0m : 2.25553
[1mStep[0m  [165/339], [94mLoss[0m : 2.22577
[1mStep[0m  [198/339], [94mLoss[0m : 2.37371
[1mStep[0m  [231/339], [94mLoss[0m : 1.99048
[1mStep[0m  [264/339], [94mLoss[0m : 2.34996
[1mStep[0m  [297/339], [94mLoss[0m : 2.17765
[1mStep[0m  [330/339], [94mLoss[0m : 2.16272

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.314, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60132
[1mStep[0m  [33/339], [94mLoss[0m : 2.24917
[1mStep[0m  [66/339], [94mLoss[0m : 2.46859
[1mStep[0m  [99/339], [94mLoss[0m : 2.54270
[1mStep[0m  [132/339], [94mLoss[0m : 2.27886
[1mStep[0m  [165/339], [94mLoss[0m : 2.16329
[1mStep[0m  [198/339], [94mLoss[0m : 1.81108
[1mStep[0m  [231/339], [94mLoss[0m : 2.54582
[1mStep[0m  [264/339], [94mLoss[0m : 2.14910
[1mStep[0m  [297/339], [94mLoss[0m : 1.71337
[1mStep[0m  [330/339], [94mLoss[0m : 2.73213

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22072
[1mStep[0m  [33/339], [94mLoss[0m : 2.87608
[1mStep[0m  [66/339], [94mLoss[0m : 2.46760
[1mStep[0m  [99/339], [94mLoss[0m : 2.62711
[1mStep[0m  [132/339], [94mLoss[0m : 2.16380
[1mStep[0m  [165/339], [94mLoss[0m : 2.53244
[1mStep[0m  [198/339], [94mLoss[0m : 2.69004
[1mStep[0m  [231/339], [94mLoss[0m : 2.22813
[1mStep[0m  [264/339], [94mLoss[0m : 1.96642
[1mStep[0m  [297/339], [94mLoss[0m : 2.20547
[1mStep[0m  [330/339], [94mLoss[0m : 2.96268

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.309, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77704
[1mStep[0m  [33/339], [94mLoss[0m : 2.43460
[1mStep[0m  [66/339], [94mLoss[0m : 2.14040
[1mStep[0m  [99/339], [94mLoss[0m : 2.50904
[1mStep[0m  [132/339], [94mLoss[0m : 2.63324
[1mStep[0m  [165/339], [94mLoss[0m : 3.04313
[1mStep[0m  [198/339], [94mLoss[0m : 2.29022
[1mStep[0m  [231/339], [94mLoss[0m : 1.88127
[1mStep[0m  [264/339], [94mLoss[0m : 2.57897
[1mStep[0m  [297/339], [94mLoss[0m : 2.52536
[1mStep[0m  [330/339], [94mLoss[0m : 2.49085

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.309, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47216
[1mStep[0m  [33/339], [94mLoss[0m : 2.28987
[1mStep[0m  [66/339], [94mLoss[0m : 2.67561
[1mStep[0m  [99/339], [94mLoss[0m : 2.33088
[1mStep[0m  [132/339], [94mLoss[0m : 2.38140
[1mStep[0m  [165/339], [94mLoss[0m : 2.11020
[1mStep[0m  [198/339], [94mLoss[0m : 2.72030
[1mStep[0m  [231/339], [94mLoss[0m : 1.86519
[1mStep[0m  [264/339], [94mLoss[0m : 1.91037
[1mStep[0m  [297/339], [94mLoss[0m : 2.00683
[1mStep[0m  [330/339], [94mLoss[0m : 2.17652

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09264
[1mStep[0m  [33/339], [94mLoss[0m : 2.47953
[1mStep[0m  [66/339], [94mLoss[0m : 2.13947
[1mStep[0m  [99/339], [94mLoss[0m : 2.58352
[1mStep[0m  [132/339], [94mLoss[0m : 2.66737
[1mStep[0m  [165/339], [94mLoss[0m : 2.41868
[1mStep[0m  [198/339], [94mLoss[0m : 3.21383
[1mStep[0m  [231/339], [94mLoss[0m : 2.62081
[1mStep[0m  [264/339], [94mLoss[0m : 2.94348
[1mStep[0m  [297/339], [94mLoss[0m : 2.38376
[1mStep[0m  [330/339], [94mLoss[0m : 1.73263

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.313, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64747
[1mStep[0m  [33/339], [94mLoss[0m : 2.24892
[1mStep[0m  [66/339], [94mLoss[0m : 2.39850
[1mStep[0m  [99/339], [94mLoss[0m : 2.66261
[1mStep[0m  [132/339], [94mLoss[0m : 2.68428
[1mStep[0m  [165/339], [94mLoss[0m : 2.26178
[1mStep[0m  [198/339], [94mLoss[0m : 2.53008
[1mStep[0m  [231/339], [94mLoss[0m : 2.53056
[1mStep[0m  [264/339], [94mLoss[0m : 2.27818
[1mStep[0m  [297/339], [94mLoss[0m : 2.74773
[1mStep[0m  [330/339], [94mLoss[0m : 3.00820

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.301, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64191
[1mStep[0m  [33/339], [94mLoss[0m : 2.46199
[1mStep[0m  [66/339], [94mLoss[0m : 2.09442
[1mStep[0m  [99/339], [94mLoss[0m : 2.40024
[1mStep[0m  [132/339], [94mLoss[0m : 1.69584
[1mStep[0m  [165/339], [94mLoss[0m : 2.48128
[1mStep[0m  [198/339], [94mLoss[0m : 2.43772
[1mStep[0m  [231/339], [94mLoss[0m : 2.20570
[1mStep[0m  [264/339], [94mLoss[0m : 2.73429
[1mStep[0m  [297/339], [94mLoss[0m : 2.35083
[1mStep[0m  [330/339], [94mLoss[0m : 2.33934

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98066
[1mStep[0m  [33/339], [94mLoss[0m : 2.24213
[1mStep[0m  [66/339], [94mLoss[0m : 2.38776
[1mStep[0m  [99/339], [94mLoss[0m : 2.54810
[1mStep[0m  [132/339], [94mLoss[0m : 2.44376
[1mStep[0m  [165/339], [94mLoss[0m : 1.68511
[1mStep[0m  [198/339], [94mLoss[0m : 1.98221
[1mStep[0m  [231/339], [94mLoss[0m : 2.24542
[1mStep[0m  [264/339], [94mLoss[0m : 2.05800
[1mStep[0m  [297/339], [94mLoss[0m : 1.48998
[1mStep[0m  [330/339], [94mLoss[0m : 1.42669

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93924
[1mStep[0m  [33/339], [94mLoss[0m : 2.44125
[1mStep[0m  [66/339], [94mLoss[0m : 2.31824
[1mStep[0m  [99/339], [94mLoss[0m : 2.40595
[1mStep[0m  [132/339], [94mLoss[0m : 2.25763
[1mStep[0m  [165/339], [94mLoss[0m : 1.93421
[1mStep[0m  [198/339], [94mLoss[0m : 2.56279
[1mStep[0m  [231/339], [94mLoss[0m : 2.93015
[1mStep[0m  [264/339], [94mLoss[0m : 2.43872
[1mStep[0m  [297/339], [94mLoss[0m : 1.77420
[1mStep[0m  [330/339], [94mLoss[0m : 2.44535

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.312, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24873
[1mStep[0m  [33/339], [94mLoss[0m : 1.74700
[1mStep[0m  [66/339], [94mLoss[0m : 2.06048
[1mStep[0m  [99/339], [94mLoss[0m : 2.55254
[1mStep[0m  [132/339], [94mLoss[0m : 2.75112
[1mStep[0m  [165/339], [94mLoss[0m : 2.15656
[1mStep[0m  [198/339], [94mLoss[0m : 1.84856
[1mStep[0m  [231/339], [94mLoss[0m : 2.25366
[1mStep[0m  [264/339], [94mLoss[0m : 2.61548
[1mStep[0m  [297/339], [94mLoss[0m : 2.19458
[1mStep[0m  [330/339], [94mLoss[0m : 2.16263

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.322, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13025
[1mStep[0m  [33/339], [94mLoss[0m : 1.44830
[1mStep[0m  [66/339], [94mLoss[0m : 1.68182
[1mStep[0m  [99/339], [94mLoss[0m : 2.21744
[1mStep[0m  [132/339], [94mLoss[0m : 2.31543
[1mStep[0m  [165/339], [94mLoss[0m : 1.65902
[1mStep[0m  [198/339], [94mLoss[0m : 1.95546
[1mStep[0m  [231/339], [94mLoss[0m : 2.52974
[1mStep[0m  [264/339], [94mLoss[0m : 2.20881
[1mStep[0m  [297/339], [94mLoss[0m : 1.92352
[1mStep[0m  [330/339], [94mLoss[0m : 2.23579

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.348, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67720
[1mStep[0m  [33/339], [94mLoss[0m : 2.61904
[1mStep[0m  [66/339], [94mLoss[0m : 2.75275
[1mStep[0m  [99/339], [94mLoss[0m : 2.10005
[1mStep[0m  [132/339], [94mLoss[0m : 2.20573
[1mStep[0m  [165/339], [94mLoss[0m : 2.30227
[1mStep[0m  [198/339], [94mLoss[0m : 2.41220
[1mStep[0m  [231/339], [94mLoss[0m : 2.94052
[1mStep[0m  [264/339], [94mLoss[0m : 2.61789
[1mStep[0m  [297/339], [94mLoss[0m : 1.93845
[1mStep[0m  [330/339], [94mLoss[0m : 2.35330

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.307, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33182
[1mStep[0m  [33/339], [94mLoss[0m : 2.84303
[1mStep[0m  [66/339], [94mLoss[0m : 2.31440
[1mStep[0m  [99/339], [94mLoss[0m : 2.34909
[1mStep[0m  [132/339], [94mLoss[0m : 2.17831
[1mStep[0m  [165/339], [94mLoss[0m : 2.42528
[1mStep[0m  [198/339], [94mLoss[0m : 2.67699
[1mStep[0m  [231/339], [94mLoss[0m : 2.35771
[1mStep[0m  [264/339], [94mLoss[0m : 2.65215
[1mStep[0m  [297/339], [94mLoss[0m : 2.28248
[1mStep[0m  [330/339], [94mLoss[0m : 2.10938

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.353, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26552
[1mStep[0m  [33/339], [94mLoss[0m : 2.04976
[1mStep[0m  [66/339], [94mLoss[0m : 3.27485
[1mStep[0m  [99/339], [94mLoss[0m : 1.61313
[1mStep[0m  [132/339], [94mLoss[0m : 3.10170
[1mStep[0m  [165/339], [94mLoss[0m : 2.15970
[1mStep[0m  [198/339], [94mLoss[0m : 2.24562
[1mStep[0m  [231/339], [94mLoss[0m : 2.65712
[1mStep[0m  [264/339], [94mLoss[0m : 2.30437
[1mStep[0m  [297/339], [94mLoss[0m : 2.04390
[1mStep[0m  [330/339], [94mLoss[0m : 1.91152

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.295, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97227
[1mStep[0m  [33/339], [94mLoss[0m : 2.67594
[1mStep[0m  [66/339], [94mLoss[0m : 2.57963
[1mStep[0m  [99/339], [94mLoss[0m : 2.24703
[1mStep[0m  [132/339], [94mLoss[0m : 2.63155
[1mStep[0m  [165/339], [94mLoss[0m : 2.23106
[1mStep[0m  [198/339], [94mLoss[0m : 1.97371
[1mStep[0m  [231/339], [94mLoss[0m : 2.44235
[1mStep[0m  [264/339], [94mLoss[0m : 2.25585
[1mStep[0m  [297/339], [94mLoss[0m : 2.11016
[1mStep[0m  [330/339], [94mLoss[0m : 2.00941

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.291
====================================

Phase 1 - Evaluation MAE:  2.2909405790599044
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.73209
[1mStep[0m  [33/339], [94mLoss[0m : 2.66616
[1mStep[0m  [66/339], [94mLoss[0m : 2.92822
[1mStep[0m  [99/339], [94mLoss[0m : 2.43239
[1mStep[0m  [132/339], [94mLoss[0m : 2.46559
[1mStep[0m  [165/339], [94mLoss[0m : 2.58591
[1mStep[0m  [198/339], [94mLoss[0m : 2.72842
[1mStep[0m  [231/339], [94mLoss[0m : 2.82963
[1mStep[0m  [264/339], [94mLoss[0m : 2.06169
[1mStep[0m  [297/339], [94mLoss[0m : 2.23364
[1mStep[0m  [330/339], [94mLoss[0m : 2.58435

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.290, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35173
[1mStep[0m  [33/339], [94mLoss[0m : 2.10169
[1mStep[0m  [66/339], [94mLoss[0m : 2.30010
[1mStep[0m  [99/339], [94mLoss[0m : 2.49462
[1mStep[0m  [132/339], [94mLoss[0m : 2.44843
[1mStep[0m  [165/339], [94mLoss[0m : 2.37068
[1mStep[0m  [198/339], [94mLoss[0m : 2.27876
[1mStep[0m  [231/339], [94mLoss[0m : 2.05359
[1mStep[0m  [264/339], [94mLoss[0m : 2.24315
[1mStep[0m  [297/339], [94mLoss[0m : 2.12055
[1mStep[0m  [330/339], [94mLoss[0m : 2.60511

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.588, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08625
[1mStep[0m  [33/339], [94mLoss[0m : 2.16815
[1mStep[0m  [66/339], [94mLoss[0m : 2.56923
[1mStep[0m  [99/339], [94mLoss[0m : 3.02214
[1mStep[0m  [132/339], [94mLoss[0m : 2.18392
[1mStep[0m  [165/339], [94mLoss[0m : 1.97373
[1mStep[0m  [198/339], [94mLoss[0m : 2.10398
[1mStep[0m  [231/339], [94mLoss[0m : 2.17281
[1mStep[0m  [264/339], [94mLoss[0m : 1.74380
[1mStep[0m  [297/339], [94mLoss[0m : 2.07437
[1mStep[0m  [330/339], [94mLoss[0m : 2.13473

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42925
[1mStep[0m  [33/339], [94mLoss[0m : 2.45831
[1mStep[0m  [66/339], [94mLoss[0m : 2.03180
[1mStep[0m  [99/339], [94mLoss[0m : 1.89056
[1mStep[0m  [132/339], [94mLoss[0m : 2.25162
[1mStep[0m  [165/339], [94mLoss[0m : 2.37039
[1mStep[0m  [198/339], [94mLoss[0m : 2.58419
[1mStep[0m  [231/339], [94mLoss[0m : 1.79630
[1mStep[0m  [264/339], [94mLoss[0m : 2.51460
[1mStep[0m  [297/339], [94mLoss[0m : 1.75693
[1mStep[0m  [330/339], [94mLoss[0m : 2.49985

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.582, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77522
[1mStep[0m  [33/339], [94mLoss[0m : 1.95578
[1mStep[0m  [66/339], [94mLoss[0m : 1.99991
[1mStep[0m  [99/339], [94mLoss[0m : 1.77802
[1mStep[0m  [132/339], [94mLoss[0m : 1.85404
[1mStep[0m  [165/339], [94mLoss[0m : 2.58521
[1mStep[0m  [198/339], [94mLoss[0m : 1.72158
[1mStep[0m  [231/339], [94mLoss[0m : 1.89073
[1mStep[0m  [264/339], [94mLoss[0m : 2.38392
[1mStep[0m  [297/339], [94mLoss[0m : 2.33027
[1mStep[0m  [330/339], [94mLoss[0m : 2.06556

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.440, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08922
[1mStep[0m  [33/339], [94mLoss[0m : 2.35686
[1mStep[0m  [66/339], [94mLoss[0m : 2.13402
[1mStep[0m  [99/339], [94mLoss[0m : 1.77211
[1mStep[0m  [132/339], [94mLoss[0m : 2.08731
[1mStep[0m  [165/339], [94mLoss[0m : 1.82307
[1mStep[0m  [198/339], [94mLoss[0m : 3.01102
[1mStep[0m  [231/339], [94mLoss[0m : 1.83225
[1mStep[0m  [264/339], [94mLoss[0m : 1.61343
[1mStep[0m  [297/339], [94mLoss[0m : 2.39165
[1mStep[0m  [330/339], [94mLoss[0m : 1.83239

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.137, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12249
[1mStep[0m  [33/339], [94mLoss[0m : 2.80924
[1mStep[0m  [66/339], [94mLoss[0m : 1.94376
[1mStep[0m  [99/339], [94mLoss[0m : 2.49079
[1mStep[0m  [132/339], [94mLoss[0m : 1.57592
[1mStep[0m  [165/339], [94mLoss[0m : 1.86265
[1mStep[0m  [198/339], [94mLoss[0m : 1.79582
[1mStep[0m  [231/339], [94mLoss[0m : 1.90467
[1mStep[0m  [264/339], [94mLoss[0m : 2.28295
[1mStep[0m  [297/339], [94mLoss[0m : 1.81882
[1mStep[0m  [330/339], [94mLoss[0m : 1.93996

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.411, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50104
[1mStep[0m  [33/339], [94mLoss[0m : 2.15423
[1mStep[0m  [66/339], [94mLoss[0m : 2.15401
[1mStep[0m  [99/339], [94mLoss[0m : 1.99736
[1mStep[0m  [132/339], [94mLoss[0m : 1.96412
[1mStep[0m  [165/339], [94mLoss[0m : 2.44151
[1mStep[0m  [198/339], [94mLoss[0m : 1.97783
[1mStep[0m  [231/339], [94mLoss[0m : 2.24057
[1mStep[0m  [264/339], [94mLoss[0m : 2.44097
[1mStep[0m  [297/339], [94mLoss[0m : 2.29534
[1mStep[0m  [330/339], [94mLoss[0m : 2.64460

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.495, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11304
[1mStep[0m  [33/339], [94mLoss[0m : 1.68907
[1mStep[0m  [66/339], [94mLoss[0m : 1.84417
[1mStep[0m  [99/339], [94mLoss[0m : 1.78747
[1mStep[0m  [132/339], [94mLoss[0m : 1.82048
[1mStep[0m  [165/339], [94mLoss[0m : 1.92639
[1mStep[0m  [198/339], [94mLoss[0m : 2.25474
[1mStep[0m  [231/339], [94mLoss[0m : 2.06139
[1mStep[0m  [264/339], [94mLoss[0m : 1.69883
[1mStep[0m  [297/339], [94mLoss[0m : 2.23169
[1mStep[0m  [330/339], [94mLoss[0m : 2.50772

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65319
[1mStep[0m  [33/339], [94mLoss[0m : 2.04845
[1mStep[0m  [66/339], [94mLoss[0m : 2.01020
[1mStep[0m  [99/339], [94mLoss[0m : 1.95669
[1mStep[0m  [132/339], [94mLoss[0m : 1.67071
[1mStep[0m  [165/339], [94mLoss[0m : 2.01399
[1mStep[0m  [198/339], [94mLoss[0m : 2.36142
[1mStep[0m  [231/339], [94mLoss[0m : 1.79913
[1mStep[0m  [264/339], [94mLoss[0m : 2.15273
[1mStep[0m  [297/339], [94mLoss[0m : 1.44969
[1mStep[0m  [330/339], [94mLoss[0m : 2.19741

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77799
[1mStep[0m  [33/339], [94mLoss[0m : 1.99738
[1mStep[0m  [66/339], [94mLoss[0m : 2.10273
[1mStep[0m  [99/339], [94mLoss[0m : 2.14105
[1mStep[0m  [132/339], [94mLoss[0m : 2.15299
[1mStep[0m  [165/339], [94mLoss[0m : 2.37314
[1mStep[0m  [198/339], [94mLoss[0m : 2.07140
[1mStep[0m  [231/339], [94mLoss[0m : 1.61742
[1mStep[0m  [264/339], [94mLoss[0m : 2.06660
[1mStep[0m  [297/339], [94mLoss[0m : 2.43761
[1mStep[0m  [330/339], [94mLoss[0m : 1.78930

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.456, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.44868
[1mStep[0m  [33/339], [94mLoss[0m : 1.90643
[1mStep[0m  [66/339], [94mLoss[0m : 2.30148
[1mStep[0m  [99/339], [94mLoss[0m : 1.75604
[1mStep[0m  [132/339], [94mLoss[0m : 1.45308
[1mStep[0m  [165/339], [94mLoss[0m : 1.98707
[1mStep[0m  [198/339], [94mLoss[0m : 1.94498
[1mStep[0m  [231/339], [94mLoss[0m : 1.41861
[1mStep[0m  [264/339], [94mLoss[0m : 2.10812
[1mStep[0m  [297/339], [94mLoss[0m : 2.00154
[1mStep[0m  [330/339], [94mLoss[0m : 1.95033

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.451, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43208
[1mStep[0m  [33/339], [94mLoss[0m : 2.18597
[1mStep[0m  [66/339], [94mLoss[0m : 1.48013
[1mStep[0m  [99/339], [94mLoss[0m : 1.98857
[1mStep[0m  [132/339], [94mLoss[0m : 1.73341
[1mStep[0m  [165/339], [94mLoss[0m : 1.55255
[1mStep[0m  [198/339], [94mLoss[0m : 2.13055
[1mStep[0m  [231/339], [94mLoss[0m : 2.09634
[1mStep[0m  [264/339], [94mLoss[0m : 1.51123
[1mStep[0m  [297/339], [94mLoss[0m : 1.80297
[1mStep[0m  [330/339], [94mLoss[0m : 1.75595

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91191
[1mStep[0m  [33/339], [94mLoss[0m : 1.76282
[1mStep[0m  [66/339], [94mLoss[0m : 1.99540
[1mStep[0m  [99/339], [94mLoss[0m : 1.59720
[1mStep[0m  [132/339], [94mLoss[0m : 1.82826
[1mStep[0m  [165/339], [94mLoss[0m : 2.25099
[1mStep[0m  [198/339], [94mLoss[0m : 2.51270
[1mStep[0m  [231/339], [94mLoss[0m : 2.86164
[1mStep[0m  [264/339], [94mLoss[0m : 1.41908
[1mStep[0m  [297/339], [94mLoss[0m : 1.92408
[1mStep[0m  [330/339], [94mLoss[0m : 1.31059

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.524, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45196
[1mStep[0m  [33/339], [94mLoss[0m : 1.98479
[1mStep[0m  [66/339], [94mLoss[0m : 1.46444
[1mStep[0m  [99/339], [94mLoss[0m : 2.08565
[1mStep[0m  [132/339], [94mLoss[0m : 1.62769
[1mStep[0m  [165/339], [94mLoss[0m : 1.37633
[1mStep[0m  [198/339], [94mLoss[0m : 1.73962
[1mStep[0m  [231/339], [94mLoss[0m : 1.44881
[1mStep[0m  [264/339], [94mLoss[0m : 1.94066
[1mStep[0m  [297/339], [94mLoss[0m : 1.80461
[1mStep[0m  [330/339], [94mLoss[0m : 1.62235

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.872, [92mTest[0m: 2.473, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00828
[1mStep[0m  [33/339], [94mLoss[0m : 1.70639
[1mStep[0m  [66/339], [94mLoss[0m : 1.70106
[1mStep[0m  [99/339], [94mLoss[0m : 1.98796
[1mStep[0m  [132/339], [94mLoss[0m : 1.71200
[1mStep[0m  [165/339], [94mLoss[0m : 1.85878
[1mStep[0m  [198/339], [94mLoss[0m : 1.71807
[1mStep[0m  [231/339], [94mLoss[0m : 2.01916
[1mStep[0m  [264/339], [94mLoss[0m : 1.88126
[1mStep[0m  [297/339], [94mLoss[0m : 1.74152
[1mStep[0m  [330/339], [94mLoss[0m : 1.61721

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20783
[1mStep[0m  [33/339], [94mLoss[0m : 2.15982
[1mStep[0m  [66/339], [94mLoss[0m : 1.46700
[1mStep[0m  [99/339], [94mLoss[0m : 2.01106
[1mStep[0m  [132/339], [94mLoss[0m : 2.59510
[1mStep[0m  [165/339], [94mLoss[0m : 1.71882
[1mStep[0m  [198/339], [94mLoss[0m : 1.93220
[1mStep[0m  [231/339], [94mLoss[0m : 1.50271
[1mStep[0m  [264/339], [94mLoss[0m : 1.99689
[1mStep[0m  [297/339], [94mLoss[0m : 1.59921
[1mStep[0m  [330/339], [94mLoss[0m : 1.65195

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.494, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03848
[1mStep[0m  [33/339], [94mLoss[0m : 2.24270
[1mStep[0m  [66/339], [94mLoss[0m : 2.36014
[1mStep[0m  [99/339], [94mLoss[0m : 1.42895
[1mStep[0m  [132/339], [94mLoss[0m : 1.73619
[1mStep[0m  [165/339], [94mLoss[0m : 1.67041
[1mStep[0m  [198/339], [94mLoss[0m : 1.90180
[1mStep[0m  [231/339], [94mLoss[0m : 1.52087
[1mStep[0m  [264/339], [94mLoss[0m : 1.68616
[1mStep[0m  [297/339], [94mLoss[0m : 2.13353
[1mStep[0m  [330/339], [94mLoss[0m : 1.72883

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16782
[1mStep[0m  [33/339], [94mLoss[0m : 1.92663
[1mStep[0m  [66/339], [94mLoss[0m : 1.95147
[1mStep[0m  [99/339], [94mLoss[0m : 1.69238
[1mStep[0m  [132/339], [94mLoss[0m : 1.42360
[1mStep[0m  [165/339], [94mLoss[0m : 1.84595
[1mStep[0m  [198/339], [94mLoss[0m : 1.55266
[1mStep[0m  [231/339], [94mLoss[0m : 1.91468
[1mStep[0m  [264/339], [94mLoss[0m : 1.91396
[1mStep[0m  [297/339], [94mLoss[0m : 1.70347
[1mStep[0m  [330/339], [94mLoss[0m : 1.85907

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85741
[1mStep[0m  [33/339], [94mLoss[0m : 1.28527
[1mStep[0m  [66/339], [94mLoss[0m : 1.70783
[1mStep[0m  [99/339], [94mLoss[0m : 1.51914
[1mStep[0m  [132/339], [94mLoss[0m : 1.63153
[1mStep[0m  [165/339], [94mLoss[0m : 1.73571
[1mStep[0m  [198/339], [94mLoss[0m : 1.71335
[1mStep[0m  [231/339], [94mLoss[0m : 2.03473
[1mStep[0m  [264/339], [94mLoss[0m : 1.56493
[1mStep[0m  [297/339], [94mLoss[0m : 1.42356
[1mStep[0m  [330/339], [94mLoss[0m : 1.98814

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.787, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40246
[1mStep[0m  [33/339], [94mLoss[0m : 1.39213
[1mStep[0m  [66/339], [94mLoss[0m : 1.76701
[1mStep[0m  [99/339], [94mLoss[0m : 1.93962
[1mStep[0m  [132/339], [94mLoss[0m : 1.24042
[1mStep[0m  [165/339], [94mLoss[0m : 2.42254
[1mStep[0m  [198/339], [94mLoss[0m : 1.32658
[1mStep[0m  [231/339], [94mLoss[0m : 2.05909
[1mStep[0m  [264/339], [94mLoss[0m : 1.45177
[1mStep[0m  [297/339], [94mLoss[0m : 2.62120
[1mStep[0m  [330/339], [94mLoss[0m : 1.68289

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78424
[1mStep[0m  [33/339], [94mLoss[0m : 2.60115
[1mStep[0m  [66/339], [94mLoss[0m : 1.41898
[1mStep[0m  [99/339], [94mLoss[0m : 1.20562
[1mStep[0m  [132/339], [94mLoss[0m : 1.55051
[1mStep[0m  [165/339], [94mLoss[0m : 1.74521
[1mStep[0m  [198/339], [94mLoss[0m : 1.45843
[1mStep[0m  [231/339], [94mLoss[0m : 1.87946
[1mStep[0m  [264/339], [94mLoss[0m : 1.27747
[1mStep[0m  [297/339], [94mLoss[0m : 1.78540
[1mStep[0m  [330/339], [94mLoss[0m : 2.27953

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46597
[1mStep[0m  [33/339], [94mLoss[0m : 1.89367
[1mStep[0m  [66/339], [94mLoss[0m : 1.94273
[1mStep[0m  [99/339], [94mLoss[0m : 0.99225
[1mStep[0m  [132/339], [94mLoss[0m : 2.06150
[1mStep[0m  [165/339], [94mLoss[0m : 2.20082
[1mStep[0m  [198/339], [94mLoss[0m : 1.43889
[1mStep[0m  [231/339], [94mLoss[0m : 1.56689
[1mStep[0m  [264/339], [94mLoss[0m : 1.79263
[1mStep[0m  [297/339], [94mLoss[0m : 1.86561
[1mStep[0m  [330/339], [94mLoss[0m : 1.70295

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.510, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72597
[1mStep[0m  [33/339], [94mLoss[0m : 1.81157
[1mStep[0m  [66/339], [94mLoss[0m : 1.09950
[1mStep[0m  [99/339], [94mLoss[0m : 1.29584
[1mStep[0m  [132/339], [94mLoss[0m : 1.88470
[1mStep[0m  [165/339], [94mLoss[0m : 1.94172
[1mStep[0m  [198/339], [94mLoss[0m : 1.79748
[1mStep[0m  [231/339], [94mLoss[0m : 1.78642
[1mStep[0m  [264/339], [94mLoss[0m : 1.65795
[1mStep[0m  [297/339], [94mLoss[0m : 2.09149
[1mStep[0m  [330/339], [94mLoss[0m : 1.31449

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.33258
[1mStep[0m  [33/339], [94mLoss[0m : 1.60287
[1mStep[0m  [66/339], [94mLoss[0m : 1.44111
[1mStep[0m  [99/339], [94mLoss[0m : 1.49721
[1mStep[0m  [132/339], [94mLoss[0m : 1.57225
[1mStep[0m  [165/339], [94mLoss[0m : 1.32854
[1mStep[0m  [198/339], [94mLoss[0m : 1.30606
[1mStep[0m  [231/339], [94mLoss[0m : 1.94789
[1mStep[0m  [264/339], [94mLoss[0m : 1.61508
[1mStep[0m  [297/339], [94mLoss[0m : 2.09279
[1mStep[0m  [330/339], [94mLoss[0m : 1.81025

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.480, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85248
[1mStep[0m  [33/339], [94mLoss[0m : 1.69994
[1mStep[0m  [66/339], [94mLoss[0m : 1.42027
[1mStep[0m  [99/339], [94mLoss[0m : 1.17998
[1mStep[0m  [132/339], [94mLoss[0m : 1.73926
[1mStep[0m  [165/339], [94mLoss[0m : 1.56734
[1mStep[0m  [198/339], [94mLoss[0m : 2.13708
[1mStep[0m  [231/339], [94mLoss[0m : 1.60121
[1mStep[0m  [264/339], [94mLoss[0m : 2.01666
[1mStep[0m  [297/339], [94mLoss[0m : 1.63940
[1mStep[0m  [330/339], [94mLoss[0m : 2.11717

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.474, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.44219
[1mStep[0m  [33/339], [94mLoss[0m : 2.27764
[1mStep[0m  [66/339], [94mLoss[0m : 1.79643
[1mStep[0m  [99/339], [94mLoss[0m : 1.58713
[1mStep[0m  [132/339], [94mLoss[0m : 1.78315
[1mStep[0m  [165/339], [94mLoss[0m : 1.60739
[1mStep[0m  [198/339], [94mLoss[0m : 2.17724
[1mStep[0m  [231/339], [94mLoss[0m : 1.67882
[1mStep[0m  [264/339], [94mLoss[0m : 1.68664
[1mStep[0m  [297/339], [94mLoss[0m : 2.06276
[1mStep[0m  [330/339], [94mLoss[0m : 1.81773

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.502, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73984
[1mStep[0m  [33/339], [94mLoss[0m : 1.41872
[1mStep[0m  [66/339], [94mLoss[0m : 1.68808
[1mStep[0m  [99/339], [94mLoss[0m : 2.08686
[1mStep[0m  [132/339], [94mLoss[0m : 1.41453
[1mStep[0m  [165/339], [94mLoss[0m : 2.08944
[1mStep[0m  [198/339], [94mLoss[0m : 1.66692
[1mStep[0m  [231/339], [94mLoss[0m : 1.52673
[1mStep[0m  [264/339], [94mLoss[0m : 1.55216
[1mStep[0m  [297/339], [94mLoss[0m : 1.77586
[1mStep[0m  [330/339], [94mLoss[0m : 2.04457

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.481, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.25498
[1mStep[0m  [33/339], [94mLoss[0m : 1.72636
[1mStep[0m  [66/339], [94mLoss[0m : 1.50028
[1mStep[0m  [99/339], [94mLoss[0m : 1.27896
[1mStep[0m  [132/339], [94mLoss[0m : 1.80704
[1mStep[0m  [165/339], [94mLoss[0m : 1.33415
[1mStep[0m  [198/339], [94mLoss[0m : 1.81997
[1mStep[0m  [231/339], [94mLoss[0m : 1.78750
[1mStep[0m  [264/339], [94mLoss[0m : 1.93888
[1mStep[0m  [297/339], [94mLoss[0m : 1.71654
[1mStep[0m  [330/339], [94mLoss[0m : 1.67247

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.484, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.36016
[1mStep[0m  [33/339], [94mLoss[0m : 1.60896
[1mStep[0m  [66/339], [94mLoss[0m : 1.69327
[1mStep[0m  [99/339], [94mLoss[0m : 1.50264
[1mStep[0m  [132/339], [94mLoss[0m : 1.71667
[1mStep[0m  [165/339], [94mLoss[0m : 1.51083
[1mStep[0m  [198/339], [94mLoss[0m : 1.99738
[1mStep[0m  [231/339], [94mLoss[0m : 1.57804
[1mStep[0m  [264/339], [94mLoss[0m : 1.32439
[1mStep[0m  [297/339], [94mLoss[0m : 1.75322
[1mStep[0m  [330/339], [94mLoss[0m : 1.35206

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.499, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.486
====================================

Phase 2 - Evaluation MAE:  2.4864854960314995
MAE score P1      2.290941
MAE score P2      2.486485
loss              1.636499
learning_rate      0.00505
batch_size              32
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay          0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.74852
[1mStep[0m  [33/339], [94mLoss[0m : 2.58694
[1mStep[0m  [66/339], [94mLoss[0m : 2.80900
[1mStep[0m  [99/339], [94mLoss[0m : 2.48908
[1mStep[0m  [132/339], [94mLoss[0m : 2.39708
[1mStep[0m  [165/339], [94mLoss[0m : 2.93028
[1mStep[0m  [198/339], [94mLoss[0m : 2.18159
[1mStep[0m  [231/339], [94mLoss[0m : 2.21089
[1mStep[0m  [264/339], [94mLoss[0m : 2.24115
[1mStep[0m  [297/339], [94mLoss[0m : 2.30909
[1mStep[0m  [330/339], [94mLoss[0m : 2.72290

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.755, [92mTest[0m: 10.750, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38445
[1mStep[0m  [33/339], [94mLoss[0m : 2.87987
[1mStep[0m  [66/339], [94mLoss[0m : 2.49799
[1mStep[0m  [99/339], [94mLoss[0m : 2.88288
[1mStep[0m  [132/339], [94mLoss[0m : 2.58233
[1mStep[0m  [165/339], [94mLoss[0m : 2.32039
[1mStep[0m  [198/339], [94mLoss[0m : 2.16823
[1mStep[0m  [231/339], [94mLoss[0m : 2.59498
[1mStep[0m  [264/339], [94mLoss[0m : 2.24972
[1mStep[0m  [297/339], [94mLoss[0m : 2.95421
[1mStep[0m  [330/339], [94mLoss[0m : 2.30453

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93413
[1mStep[0m  [33/339], [94mLoss[0m : 2.85478
[1mStep[0m  [66/339], [94mLoss[0m : 1.94315
[1mStep[0m  [99/339], [94mLoss[0m : 2.23342
[1mStep[0m  [132/339], [94mLoss[0m : 2.60972
[1mStep[0m  [165/339], [94mLoss[0m : 2.50361
[1mStep[0m  [198/339], [94mLoss[0m : 2.75072
[1mStep[0m  [231/339], [94mLoss[0m : 2.87480
[1mStep[0m  [264/339], [94mLoss[0m : 2.40367
[1mStep[0m  [297/339], [94mLoss[0m : 2.49038
[1mStep[0m  [330/339], [94mLoss[0m : 1.97994

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41701
[1mStep[0m  [33/339], [94mLoss[0m : 2.14932
[1mStep[0m  [66/339], [94mLoss[0m : 2.41471
[1mStep[0m  [99/339], [94mLoss[0m : 2.76881
[1mStep[0m  [132/339], [94mLoss[0m : 3.15587
[1mStep[0m  [165/339], [94mLoss[0m : 2.81863
[1mStep[0m  [198/339], [94mLoss[0m : 3.33779
[1mStep[0m  [231/339], [94mLoss[0m : 1.60954
[1mStep[0m  [264/339], [94mLoss[0m : 2.26318
[1mStep[0m  [297/339], [94mLoss[0m : 1.79560
[1mStep[0m  [330/339], [94mLoss[0m : 2.48544

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55360
[1mStep[0m  [33/339], [94mLoss[0m : 2.82908
[1mStep[0m  [66/339], [94mLoss[0m : 2.45544
[1mStep[0m  [99/339], [94mLoss[0m : 1.77646
[1mStep[0m  [132/339], [94mLoss[0m : 2.46948
[1mStep[0m  [165/339], [94mLoss[0m : 2.21175
[1mStep[0m  [198/339], [94mLoss[0m : 2.18586
[1mStep[0m  [231/339], [94mLoss[0m : 2.14274
[1mStep[0m  [264/339], [94mLoss[0m : 2.73688
[1mStep[0m  [297/339], [94mLoss[0m : 2.87053
[1mStep[0m  [330/339], [94mLoss[0m : 2.39084

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43481
[1mStep[0m  [33/339], [94mLoss[0m : 3.15925
[1mStep[0m  [66/339], [94mLoss[0m : 2.60544
[1mStep[0m  [99/339], [94mLoss[0m : 1.97394
[1mStep[0m  [132/339], [94mLoss[0m : 2.51121
[1mStep[0m  [165/339], [94mLoss[0m : 2.93231
[1mStep[0m  [198/339], [94mLoss[0m : 2.69315
[1mStep[0m  [231/339], [94mLoss[0m : 2.56463
[1mStep[0m  [264/339], [94mLoss[0m : 2.57819
[1mStep[0m  [297/339], [94mLoss[0m : 2.34652
[1mStep[0m  [330/339], [94mLoss[0m : 2.09518

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49813
[1mStep[0m  [33/339], [94mLoss[0m : 2.11124
[1mStep[0m  [66/339], [94mLoss[0m : 2.22235
[1mStep[0m  [99/339], [94mLoss[0m : 2.31446
[1mStep[0m  [132/339], [94mLoss[0m : 1.96617
[1mStep[0m  [165/339], [94mLoss[0m : 2.57677
[1mStep[0m  [198/339], [94mLoss[0m : 1.93643
[1mStep[0m  [231/339], [94mLoss[0m : 3.08163
[1mStep[0m  [264/339], [94mLoss[0m : 2.22121
[1mStep[0m  [297/339], [94mLoss[0m : 2.99972
[1mStep[0m  [330/339], [94mLoss[0m : 1.77162

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14839
[1mStep[0m  [33/339], [94mLoss[0m : 2.70667
[1mStep[0m  [66/339], [94mLoss[0m : 1.92495
[1mStep[0m  [99/339], [94mLoss[0m : 2.54509
[1mStep[0m  [132/339], [94mLoss[0m : 2.31486
[1mStep[0m  [165/339], [94mLoss[0m : 2.33591
[1mStep[0m  [198/339], [94mLoss[0m : 2.19893
[1mStep[0m  [231/339], [94mLoss[0m : 2.45230
[1mStep[0m  [264/339], [94mLoss[0m : 3.05664
[1mStep[0m  [297/339], [94mLoss[0m : 2.66770
[1mStep[0m  [330/339], [94mLoss[0m : 2.72549

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65658
[1mStep[0m  [33/339], [94mLoss[0m : 2.71288
[1mStep[0m  [66/339], [94mLoss[0m : 2.19381
[1mStep[0m  [99/339], [94mLoss[0m : 2.30796
[1mStep[0m  [132/339], [94mLoss[0m : 1.95706
[1mStep[0m  [165/339], [94mLoss[0m : 2.84087
[1mStep[0m  [198/339], [94mLoss[0m : 2.44126
[1mStep[0m  [231/339], [94mLoss[0m : 2.32361
[1mStep[0m  [264/339], [94mLoss[0m : 2.21782
[1mStep[0m  [297/339], [94mLoss[0m : 2.04395
[1mStep[0m  [330/339], [94mLoss[0m : 2.45364

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54850
[1mStep[0m  [33/339], [94mLoss[0m : 2.71726
[1mStep[0m  [66/339], [94mLoss[0m : 2.43622
[1mStep[0m  [99/339], [94mLoss[0m : 2.98581
[1mStep[0m  [132/339], [94mLoss[0m : 2.33397
[1mStep[0m  [165/339], [94mLoss[0m : 3.05376
[1mStep[0m  [198/339], [94mLoss[0m : 2.52020
[1mStep[0m  [231/339], [94mLoss[0m : 1.96651
[1mStep[0m  [264/339], [94mLoss[0m : 2.55694
[1mStep[0m  [297/339], [94mLoss[0m : 2.50877
[1mStep[0m  [330/339], [94mLoss[0m : 2.19880

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66715
[1mStep[0m  [33/339], [94mLoss[0m : 2.58753
[1mStep[0m  [66/339], [94mLoss[0m : 2.21070
[1mStep[0m  [99/339], [94mLoss[0m : 1.81603
[1mStep[0m  [132/339], [94mLoss[0m : 2.81005
[1mStep[0m  [165/339], [94mLoss[0m : 2.82964
[1mStep[0m  [198/339], [94mLoss[0m : 2.59823
[1mStep[0m  [231/339], [94mLoss[0m : 2.71672
[1mStep[0m  [264/339], [94mLoss[0m : 2.37907
[1mStep[0m  [297/339], [94mLoss[0m : 2.78002
[1mStep[0m  [330/339], [94mLoss[0m : 1.95855

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32940
[1mStep[0m  [33/339], [94mLoss[0m : 1.71640
[1mStep[0m  [66/339], [94mLoss[0m : 2.39018
[1mStep[0m  [99/339], [94mLoss[0m : 2.86870
[1mStep[0m  [132/339], [94mLoss[0m : 2.31352
[1mStep[0m  [165/339], [94mLoss[0m : 2.58941
[1mStep[0m  [198/339], [94mLoss[0m : 1.98166
[1mStep[0m  [231/339], [94mLoss[0m : 2.57060
[1mStep[0m  [264/339], [94mLoss[0m : 2.19865
[1mStep[0m  [297/339], [94mLoss[0m : 2.49948
[1mStep[0m  [330/339], [94mLoss[0m : 2.29480

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88754
[1mStep[0m  [33/339], [94mLoss[0m : 2.46120
[1mStep[0m  [66/339], [94mLoss[0m : 2.83653
[1mStep[0m  [99/339], [94mLoss[0m : 2.47637
[1mStep[0m  [132/339], [94mLoss[0m : 2.45884
[1mStep[0m  [165/339], [94mLoss[0m : 2.01905
[1mStep[0m  [198/339], [94mLoss[0m : 2.50390
[1mStep[0m  [231/339], [94mLoss[0m : 2.62311
[1mStep[0m  [264/339], [94mLoss[0m : 2.67547
[1mStep[0m  [297/339], [94mLoss[0m : 2.29584
[1mStep[0m  [330/339], [94mLoss[0m : 2.00261

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32602
[1mStep[0m  [33/339], [94mLoss[0m : 2.71878
[1mStep[0m  [66/339], [94mLoss[0m : 2.29685
[1mStep[0m  [99/339], [94mLoss[0m : 2.36812
[1mStep[0m  [132/339], [94mLoss[0m : 1.87025
[1mStep[0m  [165/339], [94mLoss[0m : 2.30975
[1mStep[0m  [198/339], [94mLoss[0m : 2.96375
[1mStep[0m  [231/339], [94mLoss[0m : 2.34866
[1mStep[0m  [264/339], [94mLoss[0m : 2.77688
[1mStep[0m  [297/339], [94mLoss[0m : 2.40180
[1mStep[0m  [330/339], [94mLoss[0m : 2.21703

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42889
[1mStep[0m  [33/339], [94mLoss[0m : 2.33187
[1mStep[0m  [66/339], [94mLoss[0m : 2.42186
[1mStep[0m  [99/339], [94mLoss[0m : 2.05611
[1mStep[0m  [132/339], [94mLoss[0m : 2.25243
[1mStep[0m  [165/339], [94mLoss[0m : 2.63744
[1mStep[0m  [198/339], [94mLoss[0m : 2.66369
[1mStep[0m  [231/339], [94mLoss[0m : 2.32154
[1mStep[0m  [264/339], [94mLoss[0m : 2.43778
[1mStep[0m  [297/339], [94mLoss[0m : 2.10244
[1mStep[0m  [330/339], [94mLoss[0m : 2.64679

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12096
[1mStep[0m  [33/339], [94mLoss[0m : 2.99745
[1mStep[0m  [66/339], [94mLoss[0m : 2.78352
[1mStep[0m  [99/339], [94mLoss[0m : 2.61979
[1mStep[0m  [132/339], [94mLoss[0m : 1.91253
[1mStep[0m  [165/339], [94mLoss[0m : 2.82128
[1mStep[0m  [198/339], [94mLoss[0m : 2.88700
[1mStep[0m  [231/339], [94mLoss[0m : 2.04926
[1mStep[0m  [264/339], [94mLoss[0m : 2.33789
[1mStep[0m  [297/339], [94mLoss[0m : 2.01699
[1mStep[0m  [330/339], [94mLoss[0m : 2.55090

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61918
[1mStep[0m  [33/339], [94mLoss[0m : 2.17394
[1mStep[0m  [66/339], [94mLoss[0m : 2.70955
[1mStep[0m  [99/339], [94mLoss[0m : 3.15938
[1mStep[0m  [132/339], [94mLoss[0m : 2.09847
[1mStep[0m  [165/339], [94mLoss[0m : 2.91488
[1mStep[0m  [198/339], [94mLoss[0m : 3.25810
[1mStep[0m  [231/339], [94mLoss[0m : 2.73480
[1mStep[0m  [264/339], [94mLoss[0m : 2.45043
[1mStep[0m  [297/339], [94mLoss[0m : 3.08968
[1mStep[0m  [330/339], [94mLoss[0m : 2.41197

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12331
[1mStep[0m  [33/339], [94mLoss[0m : 1.76622
[1mStep[0m  [66/339], [94mLoss[0m : 2.27425
[1mStep[0m  [99/339], [94mLoss[0m : 2.34027
[1mStep[0m  [132/339], [94mLoss[0m : 2.14629
[1mStep[0m  [165/339], [94mLoss[0m : 2.22402
[1mStep[0m  [198/339], [94mLoss[0m : 2.61648
[1mStep[0m  [231/339], [94mLoss[0m : 2.15800
[1mStep[0m  [264/339], [94mLoss[0m : 2.82152
[1mStep[0m  [297/339], [94mLoss[0m : 2.22009
[1mStep[0m  [330/339], [94mLoss[0m : 3.06390

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.333, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41153
[1mStep[0m  [33/339], [94mLoss[0m : 2.62062
[1mStep[0m  [66/339], [94mLoss[0m : 2.32557
[1mStep[0m  [99/339], [94mLoss[0m : 2.47896
[1mStep[0m  [132/339], [94mLoss[0m : 1.81635
[1mStep[0m  [165/339], [94mLoss[0m : 2.26579
[1mStep[0m  [198/339], [94mLoss[0m : 2.74093
[1mStep[0m  [231/339], [94mLoss[0m : 2.46815
[1mStep[0m  [264/339], [94mLoss[0m : 2.12887
[1mStep[0m  [297/339], [94mLoss[0m : 2.34521
[1mStep[0m  [330/339], [94mLoss[0m : 2.78775

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.362, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36470
[1mStep[0m  [33/339], [94mLoss[0m : 2.43054
[1mStep[0m  [66/339], [94mLoss[0m : 2.35057
[1mStep[0m  [99/339], [94mLoss[0m : 2.89063
[1mStep[0m  [132/339], [94mLoss[0m : 2.85015
[1mStep[0m  [165/339], [94mLoss[0m : 2.35044
[1mStep[0m  [198/339], [94mLoss[0m : 2.81466
[1mStep[0m  [231/339], [94mLoss[0m : 2.51426
[1mStep[0m  [264/339], [94mLoss[0m : 2.22081
[1mStep[0m  [297/339], [94mLoss[0m : 3.25821
[1mStep[0m  [330/339], [94mLoss[0m : 2.23491

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46014
[1mStep[0m  [33/339], [94mLoss[0m : 2.29059
[1mStep[0m  [66/339], [94mLoss[0m : 2.73039
[1mStep[0m  [99/339], [94mLoss[0m : 3.40389
[1mStep[0m  [132/339], [94mLoss[0m : 1.95513
[1mStep[0m  [165/339], [94mLoss[0m : 2.63747
[1mStep[0m  [198/339], [94mLoss[0m : 2.38869
[1mStep[0m  [231/339], [94mLoss[0m : 1.95139
[1mStep[0m  [264/339], [94mLoss[0m : 2.17162
[1mStep[0m  [297/339], [94mLoss[0m : 1.75091
[1mStep[0m  [330/339], [94mLoss[0m : 2.65820

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22245
[1mStep[0m  [33/339], [94mLoss[0m : 2.42498
[1mStep[0m  [66/339], [94mLoss[0m : 2.69820
[1mStep[0m  [99/339], [94mLoss[0m : 2.07255
[1mStep[0m  [132/339], [94mLoss[0m : 2.72923
[1mStep[0m  [165/339], [94mLoss[0m : 2.15229
[1mStep[0m  [198/339], [94mLoss[0m : 2.32809
[1mStep[0m  [231/339], [94mLoss[0m : 2.38938
[1mStep[0m  [264/339], [94mLoss[0m : 3.05014
[1mStep[0m  [297/339], [94mLoss[0m : 2.45484
[1mStep[0m  [330/339], [94mLoss[0m : 2.36701

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17876
[1mStep[0m  [33/339], [94mLoss[0m : 2.51330
[1mStep[0m  [66/339], [94mLoss[0m : 2.30228
[1mStep[0m  [99/339], [94mLoss[0m : 2.43370
[1mStep[0m  [132/339], [94mLoss[0m : 1.95154
[1mStep[0m  [165/339], [94mLoss[0m : 3.23271
[1mStep[0m  [198/339], [94mLoss[0m : 2.55585
[1mStep[0m  [231/339], [94mLoss[0m : 2.41353
[1mStep[0m  [264/339], [94mLoss[0m : 2.08728
[1mStep[0m  [297/339], [94mLoss[0m : 2.46787
[1mStep[0m  [330/339], [94mLoss[0m : 2.43069

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37983
[1mStep[0m  [33/339], [94mLoss[0m : 2.27565
[1mStep[0m  [66/339], [94mLoss[0m : 2.17758
[1mStep[0m  [99/339], [94mLoss[0m : 2.47517
[1mStep[0m  [132/339], [94mLoss[0m : 2.40787
[1mStep[0m  [165/339], [94mLoss[0m : 1.97533
[1mStep[0m  [198/339], [94mLoss[0m : 2.88718
[1mStep[0m  [231/339], [94mLoss[0m : 2.23103
[1mStep[0m  [264/339], [94mLoss[0m : 1.96349
[1mStep[0m  [297/339], [94mLoss[0m : 2.77712
[1mStep[0m  [330/339], [94mLoss[0m : 2.35190

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92364
[1mStep[0m  [33/339], [94mLoss[0m : 2.63192
[1mStep[0m  [66/339], [94mLoss[0m : 2.19826
[1mStep[0m  [99/339], [94mLoss[0m : 2.28606
[1mStep[0m  [132/339], [94mLoss[0m : 2.79974
[1mStep[0m  [165/339], [94mLoss[0m : 2.59470
[1mStep[0m  [198/339], [94mLoss[0m : 2.02351
[1mStep[0m  [231/339], [94mLoss[0m : 2.59460
[1mStep[0m  [264/339], [94mLoss[0m : 1.87795
[1mStep[0m  [297/339], [94mLoss[0m : 2.44058
[1mStep[0m  [330/339], [94mLoss[0m : 2.46168

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.99066
[1mStep[0m  [33/339], [94mLoss[0m : 1.93687
[1mStep[0m  [66/339], [94mLoss[0m : 2.67247
[1mStep[0m  [99/339], [94mLoss[0m : 3.39489
[1mStep[0m  [132/339], [94mLoss[0m : 2.59129
[1mStep[0m  [165/339], [94mLoss[0m : 2.70050
[1mStep[0m  [198/339], [94mLoss[0m : 2.05349
[1mStep[0m  [231/339], [94mLoss[0m : 2.73130
[1mStep[0m  [264/339], [94mLoss[0m : 2.37456
[1mStep[0m  [297/339], [94mLoss[0m : 2.48045
[1mStep[0m  [330/339], [94mLoss[0m : 2.26296

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02374
[1mStep[0m  [33/339], [94mLoss[0m : 2.36262
[1mStep[0m  [66/339], [94mLoss[0m : 2.63757
[1mStep[0m  [99/339], [94mLoss[0m : 2.57313
[1mStep[0m  [132/339], [94mLoss[0m : 2.06871
[1mStep[0m  [165/339], [94mLoss[0m : 2.38967
[1mStep[0m  [198/339], [94mLoss[0m : 2.57175
[1mStep[0m  [231/339], [94mLoss[0m : 2.60088
[1mStep[0m  [264/339], [94mLoss[0m : 2.48052
[1mStep[0m  [297/339], [94mLoss[0m : 3.05731
[1mStep[0m  [330/339], [94mLoss[0m : 1.95754

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.344, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70323
[1mStep[0m  [33/339], [94mLoss[0m : 3.06246
[1mStep[0m  [66/339], [94mLoss[0m : 2.07902
[1mStep[0m  [99/339], [94mLoss[0m : 2.78245
[1mStep[0m  [132/339], [94mLoss[0m : 2.28265
[1mStep[0m  [165/339], [94mLoss[0m : 2.76645
[1mStep[0m  [198/339], [94mLoss[0m : 2.86626
[1mStep[0m  [231/339], [94mLoss[0m : 2.35518
[1mStep[0m  [264/339], [94mLoss[0m : 2.11093
[1mStep[0m  [297/339], [94mLoss[0m : 2.42127
[1mStep[0m  [330/339], [94mLoss[0m : 2.62198

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68067
[1mStep[0m  [33/339], [94mLoss[0m : 2.23071
[1mStep[0m  [66/339], [94mLoss[0m : 2.47085
[1mStep[0m  [99/339], [94mLoss[0m : 2.05698
[1mStep[0m  [132/339], [94mLoss[0m : 2.44864
[1mStep[0m  [165/339], [94mLoss[0m : 2.47162
[1mStep[0m  [198/339], [94mLoss[0m : 2.36808
[1mStep[0m  [231/339], [94mLoss[0m : 3.04137
[1mStep[0m  [264/339], [94mLoss[0m : 2.08165
[1mStep[0m  [297/339], [94mLoss[0m : 2.71589
[1mStep[0m  [330/339], [94mLoss[0m : 1.95245

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78656
[1mStep[0m  [33/339], [94mLoss[0m : 2.27940
[1mStep[0m  [66/339], [94mLoss[0m : 2.27819
[1mStep[0m  [99/339], [94mLoss[0m : 2.62263
[1mStep[0m  [132/339], [94mLoss[0m : 2.22960
[1mStep[0m  [165/339], [94mLoss[0m : 2.23714
[1mStep[0m  [198/339], [94mLoss[0m : 2.32799
[1mStep[0m  [231/339], [94mLoss[0m : 2.14538
[1mStep[0m  [264/339], [94mLoss[0m : 2.47082
[1mStep[0m  [297/339], [94mLoss[0m : 2.77087
[1mStep[0m  [330/339], [94mLoss[0m : 2.74277

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.360, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.364
====================================

Phase 1 - Evaluation MAE:  2.3643440894321004
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.29504
[1mStep[0m  [33/339], [94mLoss[0m : 2.38462
[1mStep[0m  [66/339], [94mLoss[0m : 2.51703
[1mStep[0m  [99/339], [94mLoss[0m : 2.39268
[1mStep[0m  [132/339], [94mLoss[0m : 2.82361
[1mStep[0m  [165/339], [94mLoss[0m : 2.47971
[1mStep[0m  [198/339], [94mLoss[0m : 2.92135
[1mStep[0m  [231/339], [94mLoss[0m : 2.40432
[1mStep[0m  [264/339], [94mLoss[0m : 2.38220
[1mStep[0m  [297/339], [94mLoss[0m : 1.94787
[1mStep[0m  [330/339], [94mLoss[0m : 2.30808

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38301
[1mStep[0m  [33/339], [94mLoss[0m : 2.26378
[1mStep[0m  [66/339], [94mLoss[0m : 2.24500
[1mStep[0m  [99/339], [94mLoss[0m : 1.47093
[1mStep[0m  [132/339], [94mLoss[0m : 1.90649
[1mStep[0m  [165/339], [94mLoss[0m : 2.93456
[1mStep[0m  [198/339], [94mLoss[0m : 2.31682
[1mStep[0m  [231/339], [94mLoss[0m : 2.09251
[1mStep[0m  [264/339], [94mLoss[0m : 2.22779
[1mStep[0m  [297/339], [94mLoss[0m : 2.25163
[1mStep[0m  [330/339], [94mLoss[0m : 2.29533

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.452, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92483
[1mStep[0m  [33/339], [94mLoss[0m : 2.14100
[1mStep[0m  [66/339], [94mLoss[0m : 2.93380
[1mStep[0m  [99/339], [94mLoss[0m : 2.89260
[1mStep[0m  [132/339], [94mLoss[0m : 1.97884
[1mStep[0m  [165/339], [94mLoss[0m : 1.93097
[1mStep[0m  [198/339], [94mLoss[0m : 1.97366
[1mStep[0m  [231/339], [94mLoss[0m : 2.37085
[1mStep[0m  [264/339], [94mLoss[0m : 2.80312
[1mStep[0m  [297/339], [94mLoss[0m : 1.71757
[1mStep[0m  [330/339], [94mLoss[0m : 2.85597

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.385, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96638
[1mStep[0m  [33/339], [94mLoss[0m : 2.34707
[1mStep[0m  [66/339], [94mLoss[0m : 2.37014
[1mStep[0m  [99/339], [94mLoss[0m : 2.32889
[1mStep[0m  [132/339], [94mLoss[0m : 1.89786
[1mStep[0m  [165/339], [94mLoss[0m : 2.40835
[1mStep[0m  [198/339], [94mLoss[0m : 2.62451
[1mStep[0m  [231/339], [94mLoss[0m : 2.15096
[1mStep[0m  [264/339], [94mLoss[0m : 2.48848
[1mStep[0m  [297/339], [94mLoss[0m : 2.52996
[1mStep[0m  [330/339], [94mLoss[0m : 2.51574

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01793
[1mStep[0m  [33/339], [94mLoss[0m : 1.78450
[1mStep[0m  [66/339], [94mLoss[0m : 2.40728
[1mStep[0m  [99/339], [94mLoss[0m : 1.73008
[1mStep[0m  [132/339], [94mLoss[0m : 2.09420
[1mStep[0m  [165/339], [94mLoss[0m : 2.11115
[1mStep[0m  [198/339], [94mLoss[0m : 2.69025
[1mStep[0m  [231/339], [94mLoss[0m : 2.15887
[1mStep[0m  [264/339], [94mLoss[0m : 2.52033
[1mStep[0m  [297/339], [94mLoss[0m : 2.51347
[1mStep[0m  [330/339], [94mLoss[0m : 2.54160

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81881
[1mStep[0m  [33/339], [94mLoss[0m : 2.45089
[1mStep[0m  [66/339], [94mLoss[0m : 2.48860
[1mStep[0m  [99/339], [94mLoss[0m : 2.18520
[1mStep[0m  [132/339], [94mLoss[0m : 1.84587
[1mStep[0m  [165/339], [94mLoss[0m : 1.85505
[1mStep[0m  [198/339], [94mLoss[0m : 2.00524
[1mStep[0m  [231/339], [94mLoss[0m : 2.30290
[1mStep[0m  [264/339], [94mLoss[0m : 2.16207
[1mStep[0m  [297/339], [94mLoss[0m : 2.47271
[1mStep[0m  [330/339], [94mLoss[0m : 2.13032

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.381, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61070
[1mStep[0m  [33/339], [94mLoss[0m : 2.05024
[1mStep[0m  [66/339], [94mLoss[0m : 2.49945
[1mStep[0m  [99/339], [94mLoss[0m : 2.05010
[1mStep[0m  [132/339], [94mLoss[0m : 2.18351
[1mStep[0m  [165/339], [94mLoss[0m : 2.77855
[1mStep[0m  [198/339], [94mLoss[0m : 1.73659
[1mStep[0m  [231/339], [94mLoss[0m : 1.92029
[1mStep[0m  [264/339], [94mLoss[0m : 2.60072
[1mStep[0m  [297/339], [94mLoss[0m : 2.15938
[1mStep[0m  [330/339], [94mLoss[0m : 2.17043

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.420, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67497
[1mStep[0m  [33/339], [94mLoss[0m : 2.39393
[1mStep[0m  [66/339], [94mLoss[0m : 1.99929
[1mStep[0m  [99/339], [94mLoss[0m : 1.80755
[1mStep[0m  [132/339], [94mLoss[0m : 2.34405
[1mStep[0m  [165/339], [94mLoss[0m : 2.30115
[1mStep[0m  [198/339], [94mLoss[0m : 1.94174
[1mStep[0m  [231/339], [94mLoss[0m : 2.28812
[1mStep[0m  [264/339], [94mLoss[0m : 3.22841
[1mStep[0m  [297/339], [94mLoss[0m : 2.22529
[1mStep[0m  [330/339], [94mLoss[0m : 2.26152

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.415, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72228
[1mStep[0m  [33/339], [94mLoss[0m : 2.22617
[1mStep[0m  [66/339], [94mLoss[0m : 1.58835
[1mStep[0m  [99/339], [94mLoss[0m : 2.24469
[1mStep[0m  [132/339], [94mLoss[0m : 2.09308
[1mStep[0m  [165/339], [94mLoss[0m : 2.13478
[1mStep[0m  [198/339], [94mLoss[0m : 2.07103
[1mStep[0m  [231/339], [94mLoss[0m : 2.07651
[1mStep[0m  [264/339], [94mLoss[0m : 1.70356
[1mStep[0m  [297/339], [94mLoss[0m : 2.61852
[1mStep[0m  [330/339], [94mLoss[0m : 2.08290

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60341
[1mStep[0m  [33/339], [94mLoss[0m : 1.75282
[1mStep[0m  [66/339], [94mLoss[0m : 1.74652
[1mStep[0m  [99/339], [94mLoss[0m : 2.11298
[1mStep[0m  [132/339], [94mLoss[0m : 2.24237
[1mStep[0m  [165/339], [94mLoss[0m : 2.54748
[1mStep[0m  [198/339], [94mLoss[0m : 1.69579
[1mStep[0m  [231/339], [94mLoss[0m : 2.09844
[1mStep[0m  [264/339], [94mLoss[0m : 2.19211
[1mStep[0m  [297/339], [94mLoss[0m : 2.03346
[1mStep[0m  [330/339], [94mLoss[0m : 1.98253

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98878
[1mStep[0m  [33/339], [94mLoss[0m : 1.97764
[1mStep[0m  [66/339], [94mLoss[0m : 1.61009
[1mStep[0m  [99/339], [94mLoss[0m : 2.07384
[1mStep[0m  [132/339], [94mLoss[0m : 2.05092
[1mStep[0m  [165/339], [94mLoss[0m : 2.25238
[1mStep[0m  [198/339], [94mLoss[0m : 1.95591
[1mStep[0m  [231/339], [94mLoss[0m : 1.85781
[1mStep[0m  [264/339], [94mLoss[0m : 1.58562
[1mStep[0m  [297/339], [94mLoss[0m : 2.04179
[1mStep[0m  [330/339], [94mLoss[0m : 2.21924

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.055, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84764
[1mStep[0m  [33/339], [94mLoss[0m : 2.40224
[1mStep[0m  [66/339], [94mLoss[0m : 1.95034
[1mStep[0m  [99/339], [94mLoss[0m : 2.58952
[1mStep[0m  [132/339], [94mLoss[0m : 1.88954
[1mStep[0m  [165/339], [94mLoss[0m : 2.43166
[1mStep[0m  [198/339], [94mLoss[0m : 1.83976
[1mStep[0m  [231/339], [94mLoss[0m : 2.47191
[1mStep[0m  [264/339], [94mLoss[0m : 1.77724
[1mStep[0m  [297/339], [94mLoss[0m : 1.84225
[1mStep[0m  [330/339], [94mLoss[0m : 1.84889

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.040, [92mTest[0m: 2.417, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89222
[1mStep[0m  [33/339], [94mLoss[0m : 2.07600
[1mStep[0m  [66/339], [94mLoss[0m : 1.72686
[1mStep[0m  [99/339], [94mLoss[0m : 1.77844
[1mStep[0m  [132/339], [94mLoss[0m : 1.86343
[1mStep[0m  [165/339], [94mLoss[0m : 2.02683
[1mStep[0m  [198/339], [94mLoss[0m : 2.18329
[1mStep[0m  [231/339], [94mLoss[0m : 1.85515
[1mStep[0m  [264/339], [94mLoss[0m : 2.63873
[1mStep[0m  [297/339], [94mLoss[0m : 2.33217
[1mStep[0m  [330/339], [94mLoss[0m : 1.78240

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32769
[1mStep[0m  [33/339], [94mLoss[0m : 1.37083
[1mStep[0m  [66/339], [94mLoss[0m : 1.95203
[1mStep[0m  [99/339], [94mLoss[0m : 2.04818
[1mStep[0m  [132/339], [94mLoss[0m : 2.06394
[1mStep[0m  [165/339], [94mLoss[0m : 1.73109
[1mStep[0m  [198/339], [94mLoss[0m : 1.88601
[1mStep[0m  [231/339], [94mLoss[0m : 2.27077
[1mStep[0m  [264/339], [94mLoss[0m : 1.86097
[1mStep[0m  [297/339], [94mLoss[0m : 1.65972
[1mStep[0m  [330/339], [94mLoss[0m : 2.60893

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.435, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85026
[1mStep[0m  [33/339], [94mLoss[0m : 1.59546
[1mStep[0m  [66/339], [94mLoss[0m : 1.91825
[1mStep[0m  [99/339], [94mLoss[0m : 2.17992
[1mStep[0m  [132/339], [94mLoss[0m : 1.84566
[1mStep[0m  [165/339], [94mLoss[0m : 1.73628
[1mStep[0m  [198/339], [94mLoss[0m : 2.24568
[1mStep[0m  [231/339], [94mLoss[0m : 2.43739
[1mStep[0m  [264/339], [94mLoss[0m : 2.11413
[1mStep[0m  [297/339], [94mLoss[0m : 2.53580
[1mStep[0m  [330/339], [94mLoss[0m : 2.14483

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.971, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12793
[1mStep[0m  [33/339], [94mLoss[0m : 1.86286
[1mStep[0m  [66/339], [94mLoss[0m : 2.29540
[1mStep[0m  [99/339], [94mLoss[0m : 1.92894
[1mStep[0m  [132/339], [94mLoss[0m : 1.57250
[1mStep[0m  [165/339], [94mLoss[0m : 2.28344
[1mStep[0m  [198/339], [94mLoss[0m : 1.83234
[1mStep[0m  [231/339], [94mLoss[0m : 1.93781
[1mStep[0m  [264/339], [94mLoss[0m : 2.33389
[1mStep[0m  [297/339], [94mLoss[0m : 1.74220
[1mStep[0m  [330/339], [94mLoss[0m : 2.56382

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83231
[1mStep[0m  [33/339], [94mLoss[0m : 2.27600
[1mStep[0m  [66/339], [94mLoss[0m : 1.92850
[1mStep[0m  [99/339], [94mLoss[0m : 1.72960
[1mStep[0m  [132/339], [94mLoss[0m : 1.83249
[1mStep[0m  [165/339], [94mLoss[0m : 2.22553
[1mStep[0m  [198/339], [94mLoss[0m : 1.91285
[1mStep[0m  [231/339], [94mLoss[0m : 1.98962
[1mStep[0m  [264/339], [94mLoss[0m : 1.90684
[1mStep[0m  [297/339], [94mLoss[0m : 2.09796
[1mStep[0m  [330/339], [94mLoss[0m : 1.34074

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.438, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93939
[1mStep[0m  [33/339], [94mLoss[0m : 1.86117
[1mStep[0m  [66/339], [94mLoss[0m : 1.54963
[1mStep[0m  [99/339], [94mLoss[0m : 2.20422
[1mStep[0m  [132/339], [94mLoss[0m : 2.07001
[1mStep[0m  [165/339], [94mLoss[0m : 2.36655
[1mStep[0m  [198/339], [94mLoss[0m : 2.32501
[1mStep[0m  [231/339], [94mLoss[0m : 2.42000
[1mStep[0m  [264/339], [94mLoss[0m : 2.28593
[1mStep[0m  [297/339], [94mLoss[0m : 1.81150
[1mStep[0m  [330/339], [94mLoss[0m : 1.96607

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.462, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93156
[1mStep[0m  [33/339], [94mLoss[0m : 1.29267
[1mStep[0m  [66/339], [94mLoss[0m : 1.38600
[1mStep[0m  [99/339], [94mLoss[0m : 1.93642
[1mStep[0m  [132/339], [94mLoss[0m : 1.69053
[1mStep[0m  [165/339], [94mLoss[0m : 2.00665
[1mStep[0m  [198/339], [94mLoss[0m : 2.02244
[1mStep[0m  [231/339], [94mLoss[0m : 1.33831
[1mStep[0m  [264/339], [94mLoss[0m : 1.50404
[1mStep[0m  [297/339], [94mLoss[0m : 1.78193
[1mStep[0m  [330/339], [94mLoss[0m : 2.39757

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59955
[1mStep[0m  [33/339], [94mLoss[0m : 2.26476
[1mStep[0m  [66/339], [94mLoss[0m : 2.02457
[1mStep[0m  [99/339], [94mLoss[0m : 1.88653
[1mStep[0m  [132/339], [94mLoss[0m : 1.81515
[1mStep[0m  [165/339], [94mLoss[0m : 2.64830
[1mStep[0m  [198/339], [94mLoss[0m : 2.29280
[1mStep[0m  [231/339], [94mLoss[0m : 1.68764
[1mStep[0m  [264/339], [94mLoss[0m : 2.12444
[1mStep[0m  [297/339], [94mLoss[0m : 1.81369
[1mStep[0m  [330/339], [94mLoss[0m : 2.07675

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.489, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90409
[1mStep[0m  [33/339], [94mLoss[0m : 2.01270
[1mStep[0m  [66/339], [94mLoss[0m : 1.50673
[1mStep[0m  [99/339], [94mLoss[0m : 1.65195
[1mStep[0m  [132/339], [94mLoss[0m : 1.48629
[1mStep[0m  [165/339], [94mLoss[0m : 1.64024
[1mStep[0m  [198/339], [94mLoss[0m : 2.41625
[1mStep[0m  [231/339], [94mLoss[0m : 1.69762
[1mStep[0m  [264/339], [94mLoss[0m : 2.14605
[1mStep[0m  [297/339], [94mLoss[0m : 1.90761
[1mStep[0m  [330/339], [94mLoss[0m : 2.26152

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.478, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92420
[1mStep[0m  [33/339], [94mLoss[0m : 1.72211
[1mStep[0m  [66/339], [94mLoss[0m : 1.75182
[1mStep[0m  [99/339], [94mLoss[0m : 1.27645
[1mStep[0m  [132/339], [94mLoss[0m : 2.07268
[1mStep[0m  [165/339], [94mLoss[0m : 1.88837
[1mStep[0m  [198/339], [94mLoss[0m : 2.09517
[1mStep[0m  [231/339], [94mLoss[0m : 1.74140
[1mStep[0m  [264/339], [94mLoss[0m : 1.75918
[1mStep[0m  [297/339], [94mLoss[0m : 2.36421
[1mStep[0m  [330/339], [94mLoss[0m : 1.79256

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.432, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.54118
[1mStep[0m  [33/339], [94mLoss[0m : 1.27520
[1mStep[0m  [66/339], [94mLoss[0m : 1.85498
[1mStep[0m  [99/339], [94mLoss[0m : 2.09494
[1mStep[0m  [132/339], [94mLoss[0m : 1.29483
[1mStep[0m  [165/339], [94mLoss[0m : 2.04901
[1mStep[0m  [198/339], [94mLoss[0m : 2.07400
[1mStep[0m  [231/339], [94mLoss[0m : 2.54413
[1mStep[0m  [264/339], [94mLoss[0m : 2.31139
[1mStep[0m  [297/339], [94mLoss[0m : 1.72145
[1mStep[0m  [330/339], [94mLoss[0m : 1.96133

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59689
[1mStep[0m  [33/339], [94mLoss[0m : 2.06253
[1mStep[0m  [66/339], [94mLoss[0m : 1.56880
[1mStep[0m  [99/339], [94mLoss[0m : 2.71094
[1mStep[0m  [132/339], [94mLoss[0m : 1.99880
[1mStep[0m  [165/339], [94mLoss[0m : 2.03976
[1mStep[0m  [198/339], [94mLoss[0m : 1.91241
[1mStep[0m  [231/339], [94mLoss[0m : 1.91834
[1mStep[0m  [264/339], [94mLoss[0m : 2.09895
[1mStep[0m  [297/339], [94mLoss[0m : 1.64573
[1mStep[0m  [330/339], [94mLoss[0m : 1.65710

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.31413
[1mStep[0m  [33/339], [94mLoss[0m : 2.28117
[1mStep[0m  [66/339], [94mLoss[0m : 2.00706
[1mStep[0m  [99/339], [94mLoss[0m : 1.69086
[1mStep[0m  [132/339], [94mLoss[0m : 1.37747
[1mStep[0m  [165/339], [94mLoss[0m : 2.34988
[1mStep[0m  [198/339], [94mLoss[0m : 1.86124
[1mStep[0m  [231/339], [94mLoss[0m : 2.37762
[1mStep[0m  [264/339], [94mLoss[0m : 1.71918
[1mStep[0m  [297/339], [94mLoss[0m : 1.60191
[1mStep[0m  [330/339], [94mLoss[0m : 1.64384

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.486, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57767
[1mStep[0m  [33/339], [94mLoss[0m : 1.54382
[1mStep[0m  [66/339], [94mLoss[0m : 1.82635
[1mStep[0m  [99/339], [94mLoss[0m : 3.43289
[1mStep[0m  [132/339], [94mLoss[0m : 2.05542
[1mStep[0m  [165/339], [94mLoss[0m : 1.34591
[1mStep[0m  [198/339], [94mLoss[0m : 1.57062
[1mStep[0m  [231/339], [94mLoss[0m : 2.39064
[1mStep[0m  [264/339], [94mLoss[0m : 2.14504
[1mStep[0m  [297/339], [94mLoss[0m : 1.90194
[1mStep[0m  [330/339], [94mLoss[0m : 1.66828

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.453, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62825
[1mStep[0m  [33/339], [94mLoss[0m : 1.66624
[1mStep[0m  [66/339], [94mLoss[0m : 1.52780
[1mStep[0m  [99/339], [94mLoss[0m : 1.95930
[1mStep[0m  [132/339], [94mLoss[0m : 1.93173
[1mStep[0m  [165/339], [94mLoss[0m : 1.98313
[1mStep[0m  [198/339], [94mLoss[0m : 1.84238
[1mStep[0m  [231/339], [94mLoss[0m : 1.53854
[1mStep[0m  [264/339], [94mLoss[0m : 2.05403
[1mStep[0m  [297/339], [94mLoss[0m : 1.61491
[1mStep[0m  [330/339], [94mLoss[0m : 1.65941

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.476, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84595
[1mStep[0m  [33/339], [94mLoss[0m : 1.75187
[1mStep[0m  [66/339], [94mLoss[0m : 1.84525
[1mStep[0m  [99/339], [94mLoss[0m : 2.44866
[1mStep[0m  [132/339], [94mLoss[0m : 1.69651
[1mStep[0m  [165/339], [94mLoss[0m : 1.48733
[1mStep[0m  [198/339], [94mLoss[0m : 2.10715
[1mStep[0m  [231/339], [94mLoss[0m : 1.64169
[1mStep[0m  [264/339], [94mLoss[0m : 1.83060
[1mStep[0m  [297/339], [94mLoss[0m : 2.24613
[1mStep[0m  [330/339], [94mLoss[0m : 1.64643

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82864
[1mStep[0m  [33/339], [94mLoss[0m : 1.51154
[1mStep[0m  [66/339], [94mLoss[0m : 1.71847
[1mStep[0m  [99/339], [94mLoss[0m : 1.37921
[1mStep[0m  [132/339], [94mLoss[0m : 1.40811
[1mStep[0m  [165/339], [94mLoss[0m : 2.01936
[1mStep[0m  [198/339], [94mLoss[0m : 1.43933
[1mStep[0m  [231/339], [94mLoss[0m : 1.81834
[1mStep[0m  [264/339], [94mLoss[0m : 2.03180
[1mStep[0m  [297/339], [94mLoss[0m : 2.50986
[1mStep[0m  [330/339], [94mLoss[0m : 2.44722

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.519, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64661
[1mStep[0m  [33/339], [94mLoss[0m : 1.29476
[1mStep[0m  [66/339], [94mLoss[0m : 1.57637
[1mStep[0m  [99/339], [94mLoss[0m : 1.94584
[1mStep[0m  [132/339], [94mLoss[0m : 1.75505
[1mStep[0m  [165/339], [94mLoss[0m : 1.81977
[1mStep[0m  [198/339], [94mLoss[0m : 1.75648
[1mStep[0m  [231/339], [94mLoss[0m : 2.07957
[1mStep[0m  [264/339], [94mLoss[0m : 1.81078
[1mStep[0m  [297/339], [94mLoss[0m : 1.39531
[1mStep[0m  [330/339], [94mLoss[0m : 2.10295

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.472, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.479
====================================

Phase 2 - Evaluation MAE:  2.4794737606976938
MAE score P1      2.364344
MAE score P2      2.479474
loss              1.802109
learning_rate      0.00505
batch_size              32
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 11.86947
[1mStep[0m  [33/339], [94mLoss[0m : 8.11731
[1mStep[0m  [66/339], [94mLoss[0m : 6.48230
[1mStep[0m  [99/339], [94mLoss[0m : 3.54224
[1mStep[0m  [132/339], [94mLoss[0m : 2.46769
[1mStep[0m  [165/339], [94mLoss[0m : 2.92050
[1mStep[0m  [198/339], [94mLoss[0m : 2.15796
[1mStep[0m  [231/339], [94mLoss[0m : 2.86228
[1mStep[0m  [264/339], [94mLoss[0m : 2.30047
[1mStep[0m  [297/339], [94mLoss[0m : 2.39161
[1mStep[0m  [330/339], [94mLoss[0m : 2.67287

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.134, [92mTest[0m: 11.042, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.06689
[1mStep[0m  [33/339], [94mLoss[0m : 2.68979
[1mStep[0m  [66/339], [94mLoss[0m : 2.81160
[1mStep[0m  [99/339], [94mLoss[0m : 2.25927
[1mStep[0m  [132/339], [94mLoss[0m : 2.94169
[1mStep[0m  [165/339], [94mLoss[0m : 2.48222
[1mStep[0m  [198/339], [94mLoss[0m : 2.82830
[1mStep[0m  [231/339], [94mLoss[0m : 2.50676
[1mStep[0m  [264/339], [94mLoss[0m : 2.49325
[1mStep[0m  [297/339], [94mLoss[0m : 2.48519
[1mStep[0m  [330/339], [94mLoss[0m : 3.48982

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.460, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70734
[1mStep[0m  [33/339], [94mLoss[0m : 2.98961
[1mStep[0m  [66/339], [94mLoss[0m : 2.66139
[1mStep[0m  [99/339], [94mLoss[0m : 2.56975
[1mStep[0m  [132/339], [94mLoss[0m : 1.93607
[1mStep[0m  [165/339], [94mLoss[0m : 2.40382
[1mStep[0m  [198/339], [94mLoss[0m : 2.63778
[1mStep[0m  [231/339], [94mLoss[0m : 2.57606
[1mStep[0m  [264/339], [94mLoss[0m : 1.93017
[1mStep[0m  [297/339], [94mLoss[0m : 2.36992
[1mStep[0m  [330/339], [94mLoss[0m : 2.17539

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07461
[1mStep[0m  [33/339], [94mLoss[0m : 2.63914
[1mStep[0m  [66/339], [94mLoss[0m : 2.36594
[1mStep[0m  [99/339], [94mLoss[0m : 2.25838
[1mStep[0m  [132/339], [94mLoss[0m : 2.32592
[1mStep[0m  [165/339], [94mLoss[0m : 1.94338
[1mStep[0m  [198/339], [94mLoss[0m : 2.70532
[1mStep[0m  [231/339], [94mLoss[0m : 2.83379
[1mStep[0m  [264/339], [94mLoss[0m : 2.69179
[1mStep[0m  [297/339], [94mLoss[0m : 2.91031
[1mStep[0m  [330/339], [94mLoss[0m : 2.24784

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.360, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01725
[1mStep[0m  [33/339], [94mLoss[0m : 2.59317
[1mStep[0m  [66/339], [94mLoss[0m : 2.85028
[1mStep[0m  [99/339], [94mLoss[0m : 2.11908
[1mStep[0m  [132/339], [94mLoss[0m : 2.39542
[1mStep[0m  [165/339], [94mLoss[0m : 2.39319
[1mStep[0m  [198/339], [94mLoss[0m : 2.34985
[1mStep[0m  [231/339], [94mLoss[0m : 2.34088
[1mStep[0m  [264/339], [94mLoss[0m : 2.96650
[1mStep[0m  [297/339], [94mLoss[0m : 2.54882
[1mStep[0m  [330/339], [94mLoss[0m : 2.52369

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49367
[1mStep[0m  [33/339], [94mLoss[0m : 2.21042
[1mStep[0m  [66/339], [94mLoss[0m : 2.22156
[1mStep[0m  [99/339], [94mLoss[0m : 3.07103
[1mStep[0m  [132/339], [94mLoss[0m : 2.63927
[1mStep[0m  [165/339], [94mLoss[0m : 2.29513
[1mStep[0m  [198/339], [94mLoss[0m : 2.31278
[1mStep[0m  [231/339], [94mLoss[0m : 1.93225
[1mStep[0m  [264/339], [94mLoss[0m : 2.67742
[1mStep[0m  [297/339], [94mLoss[0m : 2.77502
[1mStep[0m  [330/339], [94mLoss[0m : 2.70370

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.347, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04465
[1mStep[0m  [33/339], [94mLoss[0m : 1.95664
[1mStep[0m  [66/339], [94mLoss[0m : 2.32667
[1mStep[0m  [99/339], [94mLoss[0m : 2.49793
[1mStep[0m  [132/339], [94mLoss[0m : 2.97058
[1mStep[0m  [165/339], [94mLoss[0m : 3.09722
[1mStep[0m  [198/339], [94mLoss[0m : 2.53996
[1mStep[0m  [231/339], [94mLoss[0m : 2.99116
[1mStep[0m  [264/339], [94mLoss[0m : 2.37618
[1mStep[0m  [297/339], [94mLoss[0m : 2.69378
[1mStep[0m  [330/339], [94mLoss[0m : 2.57711

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25760
[1mStep[0m  [33/339], [94mLoss[0m : 2.57993
[1mStep[0m  [66/339], [94mLoss[0m : 2.87041
[1mStep[0m  [99/339], [94mLoss[0m : 2.73741
[1mStep[0m  [132/339], [94mLoss[0m : 2.52028
[1mStep[0m  [165/339], [94mLoss[0m : 2.29479
[1mStep[0m  [198/339], [94mLoss[0m : 2.78828
[1mStep[0m  [231/339], [94mLoss[0m : 2.07062
[1mStep[0m  [264/339], [94mLoss[0m : 2.71925
[1mStep[0m  [297/339], [94mLoss[0m : 2.03246
[1mStep[0m  [330/339], [94mLoss[0m : 2.34379

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98239
[1mStep[0m  [33/339], [94mLoss[0m : 2.17078
[1mStep[0m  [66/339], [94mLoss[0m : 2.26138
[1mStep[0m  [99/339], [94mLoss[0m : 2.86945
[1mStep[0m  [132/339], [94mLoss[0m : 2.33372
[1mStep[0m  [165/339], [94mLoss[0m : 2.48927
[1mStep[0m  [198/339], [94mLoss[0m : 2.37490
[1mStep[0m  [231/339], [94mLoss[0m : 1.64800
[1mStep[0m  [264/339], [94mLoss[0m : 2.28823
[1mStep[0m  [297/339], [94mLoss[0m : 2.14901
[1mStep[0m  [330/339], [94mLoss[0m : 2.75640

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.368, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64654
[1mStep[0m  [33/339], [94mLoss[0m : 2.38269
[1mStep[0m  [66/339], [94mLoss[0m : 2.60575
[1mStep[0m  [99/339], [94mLoss[0m : 1.91723
[1mStep[0m  [132/339], [94mLoss[0m : 1.96954
[1mStep[0m  [165/339], [94mLoss[0m : 2.93537
[1mStep[0m  [198/339], [94mLoss[0m : 2.18360
[1mStep[0m  [231/339], [94mLoss[0m : 2.37581
[1mStep[0m  [264/339], [94mLoss[0m : 2.51399
[1mStep[0m  [297/339], [94mLoss[0m : 2.69368
[1mStep[0m  [330/339], [94mLoss[0m : 2.38131

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.358, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40632
[1mStep[0m  [33/339], [94mLoss[0m : 2.35286
[1mStep[0m  [66/339], [94mLoss[0m : 2.05045
[1mStep[0m  [99/339], [94mLoss[0m : 2.62973
[1mStep[0m  [132/339], [94mLoss[0m : 2.82089
[1mStep[0m  [165/339], [94mLoss[0m : 1.77983
[1mStep[0m  [198/339], [94mLoss[0m : 2.40217
[1mStep[0m  [231/339], [94mLoss[0m : 2.12384
[1mStep[0m  [264/339], [94mLoss[0m : 2.04376
[1mStep[0m  [297/339], [94mLoss[0m : 2.72211
[1mStep[0m  [330/339], [94mLoss[0m : 2.42755

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22998
[1mStep[0m  [33/339], [94mLoss[0m : 2.47165
[1mStep[0m  [66/339], [94mLoss[0m : 2.09763
[1mStep[0m  [99/339], [94mLoss[0m : 3.10804
[1mStep[0m  [132/339], [94mLoss[0m : 2.38788
[1mStep[0m  [165/339], [94mLoss[0m : 2.22490
[1mStep[0m  [198/339], [94mLoss[0m : 2.21700
[1mStep[0m  [231/339], [94mLoss[0m : 2.40753
[1mStep[0m  [264/339], [94mLoss[0m : 2.41846
[1mStep[0m  [297/339], [94mLoss[0m : 3.06290
[1mStep[0m  [330/339], [94mLoss[0m : 2.92316

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.349, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66255
[1mStep[0m  [33/339], [94mLoss[0m : 2.21096
[1mStep[0m  [66/339], [94mLoss[0m : 2.38901
[1mStep[0m  [99/339], [94mLoss[0m : 2.21542
[1mStep[0m  [132/339], [94mLoss[0m : 2.42760
[1mStep[0m  [165/339], [94mLoss[0m : 2.57429
[1mStep[0m  [198/339], [94mLoss[0m : 2.23473
[1mStep[0m  [231/339], [94mLoss[0m : 2.80855
[1mStep[0m  [264/339], [94mLoss[0m : 1.88616
[1mStep[0m  [297/339], [94mLoss[0m : 2.92685
[1mStep[0m  [330/339], [94mLoss[0m : 2.32376

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23177
[1mStep[0m  [33/339], [94mLoss[0m : 2.15116
[1mStep[0m  [66/339], [94mLoss[0m : 2.12914
[1mStep[0m  [99/339], [94mLoss[0m : 2.40213
[1mStep[0m  [132/339], [94mLoss[0m : 2.26668
[1mStep[0m  [165/339], [94mLoss[0m : 2.34861
[1mStep[0m  [198/339], [94mLoss[0m : 2.71151
[1mStep[0m  [231/339], [94mLoss[0m : 2.45132
[1mStep[0m  [264/339], [94mLoss[0m : 1.90397
[1mStep[0m  [297/339], [94mLoss[0m : 2.52278
[1mStep[0m  [330/339], [94mLoss[0m : 2.74685

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26802
[1mStep[0m  [33/339], [94mLoss[0m : 2.01216
[1mStep[0m  [66/339], [94mLoss[0m : 2.59002
[1mStep[0m  [99/339], [94mLoss[0m : 2.64223
[1mStep[0m  [132/339], [94mLoss[0m : 2.56428
[1mStep[0m  [165/339], [94mLoss[0m : 2.46197
[1mStep[0m  [198/339], [94mLoss[0m : 2.10524
[1mStep[0m  [231/339], [94mLoss[0m : 2.18070
[1mStep[0m  [264/339], [94mLoss[0m : 2.16381
[1mStep[0m  [297/339], [94mLoss[0m : 2.97448
[1mStep[0m  [330/339], [94mLoss[0m : 2.70499

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63158
[1mStep[0m  [33/339], [94mLoss[0m : 2.47349
[1mStep[0m  [66/339], [94mLoss[0m : 2.67818
[1mStep[0m  [99/339], [94mLoss[0m : 2.23733
[1mStep[0m  [132/339], [94mLoss[0m : 2.70147
[1mStep[0m  [165/339], [94mLoss[0m : 1.97493
[1mStep[0m  [198/339], [94mLoss[0m : 1.93318
[1mStep[0m  [231/339], [94mLoss[0m : 2.58083
[1mStep[0m  [264/339], [94mLoss[0m : 2.33198
[1mStep[0m  [297/339], [94mLoss[0m : 2.77877
[1mStep[0m  [330/339], [94mLoss[0m : 2.62654

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38945
[1mStep[0m  [33/339], [94mLoss[0m : 3.00081
[1mStep[0m  [66/339], [94mLoss[0m : 1.97703
[1mStep[0m  [99/339], [94mLoss[0m : 2.15719
[1mStep[0m  [132/339], [94mLoss[0m : 2.29710
[1mStep[0m  [165/339], [94mLoss[0m : 2.29703
[1mStep[0m  [198/339], [94mLoss[0m : 2.35308
[1mStep[0m  [231/339], [94mLoss[0m : 2.95195
[1mStep[0m  [264/339], [94mLoss[0m : 2.46083
[1mStep[0m  [297/339], [94mLoss[0m : 2.54597
[1mStep[0m  [330/339], [94mLoss[0m : 2.71002

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.383, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14598
[1mStep[0m  [33/339], [94mLoss[0m : 2.18962
[1mStep[0m  [66/339], [94mLoss[0m : 2.81856
[1mStep[0m  [99/339], [94mLoss[0m : 2.28908
[1mStep[0m  [132/339], [94mLoss[0m : 2.49892
[1mStep[0m  [165/339], [94mLoss[0m : 1.83017
[1mStep[0m  [198/339], [94mLoss[0m : 2.35034
[1mStep[0m  [231/339], [94mLoss[0m : 2.04897
[1mStep[0m  [264/339], [94mLoss[0m : 2.31281
[1mStep[0m  [297/339], [94mLoss[0m : 2.27781
[1mStep[0m  [330/339], [94mLoss[0m : 2.41856

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80894
[1mStep[0m  [33/339], [94mLoss[0m : 2.70146
[1mStep[0m  [66/339], [94mLoss[0m : 1.84748
[1mStep[0m  [99/339], [94mLoss[0m : 1.97945
[1mStep[0m  [132/339], [94mLoss[0m : 2.05565
[1mStep[0m  [165/339], [94mLoss[0m : 2.57559
[1mStep[0m  [198/339], [94mLoss[0m : 1.92523
[1mStep[0m  [231/339], [94mLoss[0m : 1.86872
[1mStep[0m  [264/339], [94mLoss[0m : 2.51677
[1mStep[0m  [297/339], [94mLoss[0m : 2.61367
[1mStep[0m  [330/339], [94mLoss[0m : 1.83816

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.364, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.07640
[1mStep[0m  [33/339], [94mLoss[0m : 2.54426
[1mStep[0m  [66/339], [94mLoss[0m : 2.43208
[1mStep[0m  [99/339], [94mLoss[0m : 2.35710
[1mStep[0m  [132/339], [94mLoss[0m : 2.32399
[1mStep[0m  [165/339], [94mLoss[0m : 2.50613
[1mStep[0m  [198/339], [94mLoss[0m : 2.67892
[1mStep[0m  [231/339], [94mLoss[0m : 2.50916
[1mStep[0m  [264/339], [94mLoss[0m : 2.05427
[1mStep[0m  [297/339], [94mLoss[0m : 2.33011
[1mStep[0m  [330/339], [94mLoss[0m : 2.43214

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16000
[1mStep[0m  [33/339], [94mLoss[0m : 2.73659
[1mStep[0m  [66/339], [94mLoss[0m : 2.75996
[1mStep[0m  [99/339], [94mLoss[0m : 2.17461
[1mStep[0m  [132/339], [94mLoss[0m : 2.14575
[1mStep[0m  [165/339], [94mLoss[0m : 2.56043
[1mStep[0m  [198/339], [94mLoss[0m : 1.86687
[1mStep[0m  [231/339], [94mLoss[0m : 2.61930
[1mStep[0m  [264/339], [94mLoss[0m : 2.31705
[1mStep[0m  [297/339], [94mLoss[0m : 2.23411
[1mStep[0m  [330/339], [94mLoss[0m : 2.28148

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.373, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26368
[1mStep[0m  [33/339], [94mLoss[0m : 2.05488
[1mStep[0m  [66/339], [94mLoss[0m : 2.70674
[1mStep[0m  [99/339], [94mLoss[0m : 2.75960
[1mStep[0m  [132/339], [94mLoss[0m : 2.78853
[1mStep[0m  [165/339], [94mLoss[0m : 2.49331
[1mStep[0m  [198/339], [94mLoss[0m : 2.59450
[1mStep[0m  [231/339], [94mLoss[0m : 1.80052
[1mStep[0m  [264/339], [94mLoss[0m : 2.31194
[1mStep[0m  [297/339], [94mLoss[0m : 2.78351
[1mStep[0m  [330/339], [94mLoss[0m : 2.39263

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10052
[1mStep[0m  [33/339], [94mLoss[0m : 1.72952
[1mStep[0m  [66/339], [94mLoss[0m : 2.05023
[1mStep[0m  [99/339], [94mLoss[0m : 1.99206
[1mStep[0m  [132/339], [94mLoss[0m : 2.11807
[1mStep[0m  [165/339], [94mLoss[0m : 2.43204
[1mStep[0m  [198/339], [94mLoss[0m : 2.34046
[1mStep[0m  [231/339], [94mLoss[0m : 3.05959
[1mStep[0m  [264/339], [94mLoss[0m : 2.34073
[1mStep[0m  [297/339], [94mLoss[0m : 2.08035
[1mStep[0m  [330/339], [94mLoss[0m : 2.26940

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02310
[1mStep[0m  [33/339], [94mLoss[0m : 1.98851
[1mStep[0m  [66/339], [94mLoss[0m : 2.28389
[1mStep[0m  [99/339], [94mLoss[0m : 2.01787
[1mStep[0m  [132/339], [94mLoss[0m : 2.43195
[1mStep[0m  [165/339], [94mLoss[0m : 1.94087
[1mStep[0m  [198/339], [94mLoss[0m : 2.70672
[1mStep[0m  [231/339], [94mLoss[0m : 1.94195
[1mStep[0m  [264/339], [94mLoss[0m : 2.15532
[1mStep[0m  [297/339], [94mLoss[0m : 2.10117
[1mStep[0m  [330/339], [94mLoss[0m : 2.35518

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.334, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27115
[1mStep[0m  [33/339], [94mLoss[0m : 3.30643
[1mStep[0m  [66/339], [94mLoss[0m : 2.04271
[1mStep[0m  [99/339], [94mLoss[0m : 2.20166
[1mStep[0m  [132/339], [94mLoss[0m : 2.76616
[1mStep[0m  [165/339], [94mLoss[0m : 2.37828
[1mStep[0m  [198/339], [94mLoss[0m : 2.84313
[1mStep[0m  [231/339], [94mLoss[0m : 1.79481
[1mStep[0m  [264/339], [94mLoss[0m : 1.92345
[1mStep[0m  [297/339], [94mLoss[0m : 2.33614
[1mStep[0m  [330/339], [94mLoss[0m : 2.38193

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.392, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84655
[1mStep[0m  [33/339], [94mLoss[0m : 1.84862
[1mStep[0m  [66/339], [94mLoss[0m : 2.80788
[1mStep[0m  [99/339], [94mLoss[0m : 2.03598
[1mStep[0m  [132/339], [94mLoss[0m : 2.57295
[1mStep[0m  [165/339], [94mLoss[0m : 2.72696
[1mStep[0m  [198/339], [94mLoss[0m : 2.73469
[1mStep[0m  [231/339], [94mLoss[0m : 2.69621
[1mStep[0m  [264/339], [94mLoss[0m : 2.16243
[1mStep[0m  [297/339], [94mLoss[0m : 2.06885
[1mStep[0m  [330/339], [94mLoss[0m : 2.55730

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.358, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87893
[1mStep[0m  [33/339], [94mLoss[0m : 2.25085
[1mStep[0m  [66/339], [94mLoss[0m : 3.09788
[1mStep[0m  [99/339], [94mLoss[0m : 2.24216
[1mStep[0m  [132/339], [94mLoss[0m : 2.15713
[1mStep[0m  [165/339], [94mLoss[0m : 2.00938
[1mStep[0m  [198/339], [94mLoss[0m : 2.55969
[1mStep[0m  [231/339], [94mLoss[0m : 2.42178
[1mStep[0m  [264/339], [94mLoss[0m : 3.11074
[1mStep[0m  [297/339], [94mLoss[0m : 2.67036
[1mStep[0m  [330/339], [94mLoss[0m : 2.19996

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.345, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38934
[1mStep[0m  [33/339], [94mLoss[0m : 2.41737
[1mStep[0m  [66/339], [94mLoss[0m : 2.55309
[1mStep[0m  [99/339], [94mLoss[0m : 2.28218
[1mStep[0m  [132/339], [94mLoss[0m : 2.11999
[1mStep[0m  [165/339], [94mLoss[0m : 2.25853
[1mStep[0m  [198/339], [94mLoss[0m : 2.86822
[1mStep[0m  [231/339], [94mLoss[0m : 2.95437
[1mStep[0m  [264/339], [94mLoss[0m : 2.95308
[1mStep[0m  [297/339], [94mLoss[0m : 2.53888
[1mStep[0m  [330/339], [94mLoss[0m : 2.00476

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08648
[1mStep[0m  [33/339], [94mLoss[0m : 2.82162
[1mStep[0m  [66/339], [94mLoss[0m : 2.12040
[1mStep[0m  [99/339], [94mLoss[0m : 2.50023
[1mStep[0m  [132/339], [94mLoss[0m : 2.98921
[1mStep[0m  [165/339], [94mLoss[0m : 2.39097
[1mStep[0m  [198/339], [94mLoss[0m : 3.13446
[1mStep[0m  [231/339], [94mLoss[0m : 3.08472
[1mStep[0m  [264/339], [94mLoss[0m : 2.57551
[1mStep[0m  [297/339], [94mLoss[0m : 2.27899
[1mStep[0m  [330/339], [94mLoss[0m : 1.94851

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.338, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61894
[1mStep[0m  [33/339], [94mLoss[0m : 2.52430
[1mStep[0m  [66/339], [94mLoss[0m : 2.62148
[1mStep[0m  [99/339], [94mLoss[0m : 2.37301
[1mStep[0m  [132/339], [94mLoss[0m : 2.79288
[1mStep[0m  [165/339], [94mLoss[0m : 2.33599
[1mStep[0m  [198/339], [94mLoss[0m : 2.37565
[1mStep[0m  [231/339], [94mLoss[0m : 2.15717
[1mStep[0m  [264/339], [94mLoss[0m : 2.15602
[1mStep[0m  [297/339], [94mLoss[0m : 2.33770
[1mStep[0m  [330/339], [94mLoss[0m : 2.21627

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.345
====================================

Phase 1 - Evaluation MAE:  2.345009307945724
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.98445
[1mStep[0m  [33/339], [94mLoss[0m : 2.33232
[1mStep[0m  [66/339], [94mLoss[0m : 2.47054
[1mStep[0m  [99/339], [94mLoss[0m : 2.55247
[1mStep[0m  [132/339], [94mLoss[0m : 1.98803
[1mStep[0m  [165/339], [94mLoss[0m : 2.56928
[1mStep[0m  [198/339], [94mLoss[0m : 2.83796
[1mStep[0m  [231/339], [94mLoss[0m : 2.18381
[1mStep[0m  [264/339], [94mLoss[0m : 3.22286
[1mStep[0m  [297/339], [94mLoss[0m : 2.99832
[1mStep[0m  [330/339], [94mLoss[0m : 2.17588

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01512
[1mStep[0m  [33/339], [94mLoss[0m : 1.78264
[1mStep[0m  [66/339], [94mLoss[0m : 2.37187
[1mStep[0m  [99/339], [94mLoss[0m : 2.65788
[1mStep[0m  [132/339], [94mLoss[0m : 2.31148
[1mStep[0m  [165/339], [94mLoss[0m : 2.46615
[1mStep[0m  [198/339], [94mLoss[0m : 2.15571
[1mStep[0m  [231/339], [94mLoss[0m : 2.15555
[1mStep[0m  [264/339], [94mLoss[0m : 2.70267
[1mStep[0m  [297/339], [94mLoss[0m : 2.44195
[1mStep[0m  [330/339], [94mLoss[0m : 2.13434

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.372, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40818
[1mStep[0m  [33/339], [94mLoss[0m : 2.32973
[1mStep[0m  [66/339], [94mLoss[0m : 1.79523
[1mStep[0m  [99/339], [94mLoss[0m : 2.84888
[1mStep[0m  [132/339], [94mLoss[0m : 2.43321
[1mStep[0m  [165/339], [94mLoss[0m : 1.72261
[1mStep[0m  [198/339], [94mLoss[0m : 2.00031
[1mStep[0m  [231/339], [94mLoss[0m : 2.55583
[1mStep[0m  [264/339], [94mLoss[0m : 2.93214
[1mStep[0m  [297/339], [94mLoss[0m : 2.03899
[1mStep[0m  [330/339], [94mLoss[0m : 2.54402

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71163
[1mStep[0m  [33/339], [94mLoss[0m : 2.15335
[1mStep[0m  [66/339], [94mLoss[0m : 1.89461
[1mStep[0m  [99/339], [94mLoss[0m : 2.05252
[1mStep[0m  [132/339], [94mLoss[0m : 2.09117
[1mStep[0m  [165/339], [94mLoss[0m : 2.33985
[1mStep[0m  [198/339], [94mLoss[0m : 2.16893
[1mStep[0m  [231/339], [94mLoss[0m : 2.79362
[1mStep[0m  [264/339], [94mLoss[0m : 2.12642
[1mStep[0m  [297/339], [94mLoss[0m : 2.32078
[1mStep[0m  [330/339], [94mLoss[0m : 2.13786

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.400, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89318
[1mStep[0m  [33/339], [94mLoss[0m : 2.21029
[1mStep[0m  [66/339], [94mLoss[0m : 1.87246
[1mStep[0m  [99/339], [94mLoss[0m : 2.70216
[1mStep[0m  [132/339], [94mLoss[0m : 2.49419
[1mStep[0m  [165/339], [94mLoss[0m : 2.28827
[1mStep[0m  [198/339], [94mLoss[0m : 2.11688
[1mStep[0m  [231/339], [94mLoss[0m : 1.70767
[1mStep[0m  [264/339], [94mLoss[0m : 2.30310
[1mStep[0m  [297/339], [94mLoss[0m : 2.85565
[1mStep[0m  [330/339], [94mLoss[0m : 2.48493

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53338
[1mStep[0m  [33/339], [94mLoss[0m : 2.62505
[1mStep[0m  [66/339], [94mLoss[0m : 1.81486
[1mStep[0m  [99/339], [94mLoss[0m : 1.95120
[1mStep[0m  [132/339], [94mLoss[0m : 1.85696
[1mStep[0m  [165/339], [94mLoss[0m : 2.22449
[1mStep[0m  [198/339], [94mLoss[0m : 2.62278
[1mStep[0m  [231/339], [94mLoss[0m : 2.15568
[1mStep[0m  [264/339], [94mLoss[0m : 2.13537
[1mStep[0m  [297/339], [94mLoss[0m : 2.37587
[1mStep[0m  [330/339], [94mLoss[0m : 2.00853

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01820
[1mStep[0m  [33/339], [94mLoss[0m : 1.88051
[1mStep[0m  [66/339], [94mLoss[0m : 1.97387
[1mStep[0m  [99/339], [94mLoss[0m : 1.51110
[1mStep[0m  [132/339], [94mLoss[0m : 1.59557
[1mStep[0m  [165/339], [94mLoss[0m : 1.90900
[1mStep[0m  [198/339], [94mLoss[0m : 2.58648
[1mStep[0m  [231/339], [94mLoss[0m : 2.55530
[1mStep[0m  [264/339], [94mLoss[0m : 2.12001
[1mStep[0m  [297/339], [94mLoss[0m : 2.25670
[1mStep[0m  [330/339], [94mLoss[0m : 1.75882

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.119, [92mTest[0m: 2.403, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76861
[1mStep[0m  [33/339], [94mLoss[0m : 2.17677
[1mStep[0m  [66/339], [94mLoss[0m : 2.33656
[1mStep[0m  [99/339], [94mLoss[0m : 1.97908
[1mStep[0m  [132/339], [94mLoss[0m : 1.66043
[1mStep[0m  [165/339], [94mLoss[0m : 1.83565
[1mStep[0m  [198/339], [94mLoss[0m : 2.02702
[1mStep[0m  [231/339], [94mLoss[0m : 2.16879
[1mStep[0m  [264/339], [94mLoss[0m : 2.40354
[1mStep[0m  [297/339], [94mLoss[0m : 1.32253
[1mStep[0m  [330/339], [94mLoss[0m : 1.83347

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.390, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92339
[1mStep[0m  [33/339], [94mLoss[0m : 1.60508
[1mStep[0m  [66/339], [94mLoss[0m : 2.00994
[1mStep[0m  [99/339], [94mLoss[0m : 2.74937
[1mStep[0m  [132/339], [94mLoss[0m : 2.39649
[1mStep[0m  [165/339], [94mLoss[0m : 1.87435
[1mStep[0m  [198/339], [94mLoss[0m : 1.80869
[1mStep[0m  [231/339], [94mLoss[0m : 1.91902
[1mStep[0m  [264/339], [94mLoss[0m : 2.34332
[1mStep[0m  [297/339], [94mLoss[0m : 2.42187
[1mStep[0m  [330/339], [94mLoss[0m : 2.25872

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.035, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86928
[1mStep[0m  [33/339], [94mLoss[0m : 2.16104
[1mStep[0m  [66/339], [94mLoss[0m : 2.31087
[1mStep[0m  [99/339], [94mLoss[0m : 2.02186
[1mStep[0m  [132/339], [94mLoss[0m : 2.40762
[1mStep[0m  [165/339], [94mLoss[0m : 2.12125
[1mStep[0m  [198/339], [94mLoss[0m : 2.07504
[1mStep[0m  [231/339], [94mLoss[0m : 1.86565
[1mStep[0m  [264/339], [94mLoss[0m : 2.58221
[1mStep[0m  [297/339], [94mLoss[0m : 2.18478
[1mStep[0m  [330/339], [94mLoss[0m : 1.83904

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33843
[1mStep[0m  [33/339], [94mLoss[0m : 1.78070
[1mStep[0m  [66/339], [94mLoss[0m : 1.80492
[1mStep[0m  [99/339], [94mLoss[0m : 2.03119
[1mStep[0m  [132/339], [94mLoss[0m : 2.56235
[1mStep[0m  [165/339], [94mLoss[0m : 2.04235
[1mStep[0m  [198/339], [94mLoss[0m : 2.41073
[1mStep[0m  [231/339], [94mLoss[0m : 2.21046
[1mStep[0m  [264/339], [94mLoss[0m : 2.29114
[1mStep[0m  [297/339], [94mLoss[0m : 1.72601
[1mStep[0m  [330/339], [94mLoss[0m : 1.52135

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.428, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83377
[1mStep[0m  [33/339], [94mLoss[0m : 2.06268
[1mStep[0m  [66/339], [94mLoss[0m : 1.85964
[1mStep[0m  [99/339], [94mLoss[0m : 2.41957
[1mStep[0m  [132/339], [94mLoss[0m : 1.75733
[1mStep[0m  [165/339], [94mLoss[0m : 2.46306
[1mStep[0m  [198/339], [94mLoss[0m : 2.06732
[1mStep[0m  [231/339], [94mLoss[0m : 2.02092
[1mStep[0m  [264/339], [94mLoss[0m : 1.79957
[1mStep[0m  [297/339], [94mLoss[0m : 2.07219
[1mStep[0m  [330/339], [94mLoss[0m : 1.95522

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40993
[1mStep[0m  [33/339], [94mLoss[0m : 1.94285
[1mStep[0m  [66/339], [94mLoss[0m : 1.96689
[1mStep[0m  [99/339], [94mLoss[0m : 1.77076
[1mStep[0m  [132/339], [94mLoss[0m : 2.43485
[1mStep[0m  [165/339], [94mLoss[0m : 1.70345
[1mStep[0m  [198/339], [94mLoss[0m : 1.79650
[1mStep[0m  [231/339], [94mLoss[0m : 2.23083
[1mStep[0m  [264/339], [94mLoss[0m : 1.91741
[1mStep[0m  [297/339], [94mLoss[0m : 1.94560
[1mStep[0m  [330/339], [94mLoss[0m : 1.80002

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.465, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88419
[1mStep[0m  [33/339], [94mLoss[0m : 1.95551
[1mStep[0m  [66/339], [94mLoss[0m : 2.03861
[1mStep[0m  [99/339], [94mLoss[0m : 2.02946
[1mStep[0m  [132/339], [94mLoss[0m : 2.08625
[1mStep[0m  [165/339], [94mLoss[0m : 1.54909
[1mStep[0m  [198/339], [94mLoss[0m : 1.74330
[1mStep[0m  [231/339], [94mLoss[0m : 3.40467
[1mStep[0m  [264/339], [94mLoss[0m : 1.42769
[1mStep[0m  [297/339], [94mLoss[0m : 2.12756
[1mStep[0m  [330/339], [94mLoss[0m : 1.63287

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82098
[1mStep[0m  [33/339], [94mLoss[0m : 1.91410
[1mStep[0m  [66/339], [94mLoss[0m : 1.27199
[1mStep[0m  [99/339], [94mLoss[0m : 1.66723
[1mStep[0m  [132/339], [94mLoss[0m : 1.92340
[1mStep[0m  [165/339], [94mLoss[0m : 2.21432
[1mStep[0m  [198/339], [94mLoss[0m : 2.57697
[1mStep[0m  [231/339], [94mLoss[0m : 1.64459
[1mStep[0m  [264/339], [94mLoss[0m : 1.70891
[1mStep[0m  [297/339], [94mLoss[0m : 1.75606
[1mStep[0m  [330/339], [94mLoss[0m : 1.43666

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.28803
[1mStep[0m  [33/339], [94mLoss[0m : 1.65818
[1mStep[0m  [66/339], [94mLoss[0m : 1.82234
[1mStep[0m  [99/339], [94mLoss[0m : 1.68765
[1mStep[0m  [132/339], [94mLoss[0m : 1.94123
[1mStep[0m  [165/339], [94mLoss[0m : 1.71588
[1mStep[0m  [198/339], [94mLoss[0m : 1.71481
[1mStep[0m  [231/339], [94mLoss[0m : 2.55715
[1mStep[0m  [264/339], [94mLoss[0m : 1.92703
[1mStep[0m  [297/339], [94mLoss[0m : 1.75128
[1mStep[0m  [330/339], [94mLoss[0m : 1.70669

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47757
[1mStep[0m  [33/339], [94mLoss[0m : 1.86241
[1mStep[0m  [66/339], [94mLoss[0m : 2.06834
[1mStep[0m  [99/339], [94mLoss[0m : 1.80415
[1mStep[0m  [132/339], [94mLoss[0m : 1.89166
[1mStep[0m  [165/339], [94mLoss[0m : 2.11925
[1mStep[0m  [198/339], [94mLoss[0m : 1.53971
[1mStep[0m  [231/339], [94mLoss[0m : 1.77603
[1mStep[0m  [264/339], [94mLoss[0m : 1.77408
[1mStep[0m  [297/339], [94mLoss[0m : 1.84830
[1mStep[0m  [330/339], [94mLoss[0m : 1.56536

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.491, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38328
[1mStep[0m  [33/339], [94mLoss[0m : 1.85549
[1mStep[0m  [66/339], [94mLoss[0m : 1.82686
[1mStep[0m  [99/339], [94mLoss[0m : 1.58994
[1mStep[0m  [132/339], [94mLoss[0m : 2.30320
[1mStep[0m  [165/339], [94mLoss[0m : 1.66550
[1mStep[0m  [198/339], [94mLoss[0m : 1.77622
[1mStep[0m  [231/339], [94mLoss[0m : 2.04983
[1mStep[0m  [264/339], [94mLoss[0m : 1.73265
[1mStep[0m  [297/339], [94mLoss[0m : 1.83345
[1mStep[0m  [330/339], [94mLoss[0m : 1.89065

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.784, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45001
[1mStep[0m  [33/339], [94mLoss[0m : 1.82661
[1mStep[0m  [66/339], [94mLoss[0m : 2.07801
[1mStep[0m  [99/339], [94mLoss[0m : 1.67280
[1mStep[0m  [132/339], [94mLoss[0m : 1.65140
[1mStep[0m  [165/339], [94mLoss[0m : 1.54955
[1mStep[0m  [198/339], [94mLoss[0m : 1.58028
[1mStep[0m  [231/339], [94mLoss[0m : 1.52148
[1mStep[0m  [264/339], [94mLoss[0m : 1.68861
[1mStep[0m  [297/339], [94mLoss[0m : 2.15919
[1mStep[0m  [330/339], [94mLoss[0m : 1.68670

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.453, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77699
[1mStep[0m  [33/339], [94mLoss[0m : 1.41685
[1mStep[0m  [66/339], [94mLoss[0m : 1.46859
[1mStep[0m  [99/339], [94mLoss[0m : 1.83968
[1mStep[0m  [132/339], [94mLoss[0m : 2.25956
[1mStep[0m  [165/339], [94mLoss[0m : 1.83343
[1mStep[0m  [198/339], [94mLoss[0m : 1.43467
[1mStep[0m  [231/339], [94mLoss[0m : 2.23985
[1mStep[0m  [264/339], [94mLoss[0m : 2.29272
[1mStep[0m  [297/339], [94mLoss[0m : 2.13795
[1mStep[0m  [330/339], [94mLoss[0m : 2.13253

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38305
[1mStep[0m  [33/339], [94mLoss[0m : 2.21910
[1mStep[0m  [66/339], [94mLoss[0m : 1.28202
[1mStep[0m  [99/339], [94mLoss[0m : 1.38246
[1mStep[0m  [132/339], [94mLoss[0m : 1.67366
[1mStep[0m  [165/339], [94mLoss[0m : 2.07246
[1mStep[0m  [198/339], [94mLoss[0m : 1.42545
[1mStep[0m  [231/339], [94mLoss[0m : 1.36060
[1mStep[0m  [264/339], [94mLoss[0m : 1.81806
[1mStep[0m  [297/339], [94mLoss[0m : 1.82863
[1mStep[0m  [330/339], [94mLoss[0m : 1.78397

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.498, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91789
[1mStep[0m  [33/339], [94mLoss[0m : 1.17295
[1mStep[0m  [66/339], [94mLoss[0m : 1.75300
[1mStep[0m  [99/339], [94mLoss[0m : 1.65307
[1mStep[0m  [132/339], [94mLoss[0m : 1.56156
[1mStep[0m  [165/339], [94mLoss[0m : 1.55295
[1mStep[0m  [198/339], [94mLoss[0m : 1.48979
[1mStep[0m  [231/339], [94mLoss[0m : 1.87404
[1mStep[0m  [264/339], [94mLoss[0m : 1.49582
[1mStep[0m  [297/339], [94mLoss[0m : 1.27375
[1mStep[0m  [330/339], [94mLoss[0m : 1.58852

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.528, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93492
[1mStep[0m  [33/339], [94mLoss[0m : 1.55548
[1mStep[0m  [66/339], [94mLoss[0m : 1.58878
[1mStep[0m  [99/339], [94mLoss[0m : 1.63267
[1mStep[0m  [132/339], [94mLoss[0m : 1.74185
[1mStep[0m  [165/339], [94mLoss[0m : 1.39286
[1mStep[0m  [198/339], [94mLoss[0m : 1.51842
[1mStep[0m  [231/339], [94mLoss[0m : 1.68622
[1mStep[0m  [264/339], [94mLoss[0m : 1.56528
[1mStep[0m  [297/339], [94mLoss[0m : 1.35713
[1mStep[0m  [330/339], [94mLoss[0m : 1.45541

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.442, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00883
[1mStep[0m  [33/339], [94mLoss[0m : 2.46859
[1mStep[0m  [66/339], [94mLoss[0m : 1.66588
[1mStep[0m  [99/339], [94mLoss[0m : 1.30523
[1mStep[0m  [132/339], [94mLoss[0m : 1.81515
[1mStep[0m  [165/339], [94mLoss[0m : 1.72265
[1mStep[0m  [198/339], [94mLoss[0m : 1.50393
[1mStep[0m  [231/339], [94mLoss[0m : 1.77683
[1mStep[0m  [264/339], [94mLoss[0m : 1.42125
[1mStep[0m  [297/339], [94mLoss[0m : 1.39523
[1mStep[0m  [330/339], [94mLoss[0m : 1.51174

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90271
[1mStep[0m  [33/339], [94mLoss[0m : 1.44769
[1mStep[0m  [66/339], [94mLoss[0m : 1.07545
[1mStep[0m  [99/339], [94mLoss[0m : 1.61049
[1mStep[0m  [132/339], [94mLoss[0m : 1.89101
[1mStep[0m  [165/339], [94mLoss[0m : 2.05127
[1mStep[0m  [198/339], [94mLoss[0m : 1.76080
[1mStep[0m  [231/339], [94mLoss[0m : 1.40157
[1mStep[0m  [264/339], [94mLoss[0m : 1.57141
[1mStep[0m  [297/339], [94mLoss[0m : 1.25454
[1mStep[0m  [330/339], [94mLoss[0m : 2.08189

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.623, [92mTest[0m: 2.465, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.21390
[1mStep[0m  [33/339], [94mLoss[0m : 1.16526
[1mStep[0m  [66/339], [94mLoss[0m : 1.31132
[1mStep[0m  [99/339], [94mLoss[0m : 1.82638
[1mStep[0m  [132/339], [94mLoss[0m : 2.01168
[1mStep[0m  [165/339], [94mLoss[0m : 1.63392
[1mStep[0m  [198/339], [94mLoss[0m : 1.46200
[1mStep[0m  [231/339], [94mLoss[0m : 1.54906
[1mStep[0m  [264/339], [94mLoss[0m : 1.14996
[1mStep[0m  [297/339], [94mLoss[0m : 1.74168
[1mStep[0m  [330/339], [94mLoss[0m : 1.56586

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45665
[1mStep[0m  [33/339], [94mLoss[0m : 2.05023
[1mStep[0m  [66/339], [94mLoss[0m : 1.39746
[1mStep[0m  [99/339], [94mLoss[0m : 1.55486
[1mStep[0m  [132/339], [94mLoss[0m : 1.59789
[1mStep[0m  [165/339], [94mLoss[0m : 1.81367
[1mStep[0m  [198/339], [94mLoss[0m : 1.89905
[1mStep[0m  [231/339], [94mLoss[0m : 1.99055
[1mStep[0m  [264/339], [94mLoss[0m : 2.30726
[1mStep[0m  [297/339], [94mLoss[0m : 2.09386
[1mStep[0m  [330/339], [94mLoss[0m : 1.82866

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.555, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79940
[1mStep[0m  [33/339], [94mLoss[0m : 1.38467
[1mStep[0m  [66/339], [94mLoss[0m : 1.64915
[1mStep[0m  [99/339], [94mLoss[0m : 1.26283
[1mStep[0m  [132/339], [94mLoss[0m : 1.89196
[1mStep[0m  [165/339], [94mLoss[0m : 1.49432
[1mStep[0m  [198/339], [94mLoss[0m : 1.43604
[1mStep[0m  [231/339], [94mLoss[0m : 1.50071
[1mStep[0m  [264/339], [94mLoss[0m : 1.57126
[1mStep[0m  [297/339], [94mLoss[0m : 1.00881
[1mStep[0m  [330/339], [94mLoss[0m : 1.51564

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.536, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.19115
[1mStep[0m  [33/339], [94mLoss[0m : 1.77077
[1mStep[0m  [66/339], [94mLoss[0m : 1.85040
[1mStep[0m  [99/339], [94mLoss[0m : 1.35409
[1mStep[0m  [132/339], [94mLoss[0m : 1.39713
[1mStep[0m  [165/339], [94mLoss[0m : 1.82674
[1mStep[0m  [198/339], [94mLoss[0m : 1.24263
[1mStep[0m  [231/339], [94mLoss[0m : 1.74028
[1mStep[0m  [264/339], [94mLoss[0m : 1.55683
[1mStep[0m  [297/339], [94mLoss[0m : 1.67741
[1mStep[0m  [330/339], [94mLoss[0m : 1.29066

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.558, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81835
[1mStep[0m  [33/339], [94mLoss[0m : 1.49705
[1mStep[0m  [66/339], [94mLoss[0m : 2.26605
[1mStep[0m  [99/339], [94mLoss[0m : 1.76760
[1mStep[0m  [132/339], [94mLoss[0m : 1.29135
[1mStep[0m  [165/339], [94mLoss[0m : 1.65959
[1mStep[0m  [198/339], [94mLoss[0m : 1.57307
[1mStep[0m  [231/339], [94mLoss[0m : 1.58985
[1mStep[0m  [264/339], [94mLoss[0m : 1.72466
[1mStep[0m  [297/339], [94mLoss[0m : 1.33742
[1mStep[0m  [330/339], [94mLoss[0m : 1.61067

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.439, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.540
====================================

Phase 2 - Evaluation MAE:  2.540459736258583
MAE score P1      2.345009
MAE score P2       2.54046
loss              1.542881
learning_rate      0.00505
batch_size              32
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.79791
[1mStep[0m  [33/339], [94mLoss[0m : 2.82589
[1mStep[0m  [66/339], [94mLoss[0m : 2.91492
[1mStep[0m  [99/339], [94mLoss[0m : 2.37495
[1mStep[0m  [132/339], [94mLoss[0m : 2.69747
[1mStep[0m  [165/339], [94mLoss[0m : 2.35480
[1mStep[0m  [198/339], [94mLoss[0m : 2.69856
[1mStep[0m  [231/339], [94mLoss[0m : 2.56506
[1mStep[0m  [264/339], [94mLoss[0m : 2.46187
[1mStep[0m  [297/339], [94mLoss[0m : 2.20465
[1mStep[0m  [330/339], [94mLoss[0m : 2.76246

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.080, [92mTest[0m: 10.960, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.33729
[1mStep[0m  [33/339], [94mLoss[0m : 2.74100
[1mStep[0m  [66/339], [94mLoss[0m : 2.68133
[1mStep[0m  [99/339], [94mLoss[0m : 3.40318
[1mStep[0m  [132/339], [94mLoss[0m : 2.48726
[1mStep[0m  [165/339], [94mLoss[0m : 2.81222
[1mStep[0m  [198/339], [94mLoss[0m : 3.11704
[1mStep[0m  [231/339], [94mLoss[0m : 2.46475
[1mStep[0m  [264/339], [94mLoss[0m : 2.56794
[1mStep[0m  [297/339], [94mLoss[0m : 3.05775
[1mStep[0m  [330/339], [94mLoss[0m : 2.79515

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22271
[1mStep[0m  [33/339], [94mLoss[0m : 2.43174
[1mStep[0m  [66/339], [94mLoss[0m : 2.89381
[1mStep[0m  [99/339], [94mLoss[0m : 2.73826
[1mStep[0m  [132/339], [94mLoss[0m : 3.02186
[1mStep[0m  [165/339], [94mLoss[0m : 2.60088
[1mStep[0m  [198/339], [94mLoss[0m : 2.97751
[1mStep[0m  [231/339], [94mLoss[0m : 2.65364
[1mStep[0m  [264/339], [94mLoss[0m : 2.48054
[1mStep[0m  [297/339], [94mLoss[0m : 2.49397
[1mStep[0m  [330/339], [94mLoss[0m : 2.53946

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62730
[1mStep[0m  [33/339], [94mLoss[0m : 2.49535
[1mStep[0m  [66/339], [94mLoss[0m : 2.80828
[1mStep[0m  [99/339], [94mLoss[0m : 2.48170
[1mStep[0m  [132/339], [94mLoss[0m : 1.83630
[1mStep[0m  [165/339], [94mLoss[0m : 2.78394
[1mStep[0m  [198/339], [94mLoss[0m : 2.59720
[1mStep[0m  [231/339], [94mLoss[0m : 2.33546
[1mStep[0m  [264/339], [94mLoss[0m : 2.19791
[1mStep[0m  [297/339], [94mLoss[0m : 2.60935
[1mStep[0m  [330/339], [94mLoss[0m : 2.67343

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.545, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22564
[1mStep[0m  [33/339], [94mLoss[0m : 2.18779
[1mStep[0m  [66/339], [94mLoss[0m : 2.61541
[1mStep[0m  [99/339], [94mLoss[0m : 2.80154
[1mStep[0m  [132/339], [94mLoss[0m : 1.73593
[1mStep[0m  [165/339], [94mLoss[0m : 2.34799
[1mStep[0m  [198/339], [94mLoss[0m : 2.25038
[1mStep[0m  [231/339], [94mLoss[0m : 2.28893
[1mStep[0m  [264/339], [94mLoss[0m : 2.91502
[1mStep[0m  [297/339], [94mLoss[0m : 2.33928
[1mStep[0m  [330/339], [94mLoss[0m : 2.09192

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.369, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47168
[1mStep[0m  [33/339], [94mLoss[0m : 2.48064
[1mStep[0m  [66/339], [94mLoss[0m : 2.02400
[1mStep[0m  [99/339], [94mLoss[0m : 2.22699
[1mStep[0m  [132/339], [94mLoss[0m : 2.41438
[1mStep[0m  [165/339], [94mLoss[0m : 2.06902
[1mStep[0m  [198/339], [94mLoss[0m : 2.53615
[1mStep[0m  [231/339], [94mLoss[0m : 2.13712
[1mStep[0m  [264/339], [94mLoss[0m : 2.65417
[1mStep[0m  [297/339], [94mLoss[0m : 2.77445
[1mStep[0m  [330/339], [94mLoss[0m : 2.95956

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.348, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40808
[1mStep[0m  [33/339], [94mLoss[0m : 1.93475
[1mStep[0m  [66/339], [94mLoss[0m : 2.00632
[1mStep[0m  [99/339], [94mLoss[0m : 2.45057
[1mStep[0m  [132/339], [94mLoss[0m : 2.17966
[1mStep[0m  [165/339], [94mLoss[0m : 1.96067
[1mStep[0m  [198/339], [94mLoss[0m : 3.15494
[1mStep[0m  [231/339], [94mLoss[0m : 2.08326
[1mStep[0m  [264/339], [94mLoss[0m : 2.55270
[1mStep[0m  [297/339], [94mLoss[0m : 2.26453
[1mStep[0m  [330/339], [94mLoss[0m : 2.77852

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.20049
[1mStep[0m  [33/339], [94mLoss[0m : 2.30566
[1mStep[0m  [66/339], [94mLoss[0m : 2.34387
[1mStep[0m  [99/339], [94mLoss[0m : 2.67968
[1mStep[0m  [132/339], [94mLoss[0m : 2.66458
[1mStep[0m  [165/339], [94mLoss[0m : 3.39062
[1mStep[0m  [198/339], [94mLoss[0m : 2.55472
[1mStep[0m  [231/339], [94mLoss[0m : 2.11045
[1mStep[0m  [264/339], [94mLoss[0m : 2.42157
[1mStep[0m  [297/339], [94mLoss[0m : 2.24384
[1mStep[0m  [330/339], [94mLoss[0m : 2.39425

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.377, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47577
[1mStep[0m  [33/339], [94mLoss[0m : 1.95484
[1mStep[0m  [66/339], [94mLoss[0m : 2.66618
[1mStep[0m  [99/339], [94mLoss[0m : 2.46266
[1mStep[0m  [132/339], [94mLoss[0m : 2.34748
[1mStep[0m  [165/339], [94mLoss[0m : 2.41487
[1mStep[0m  [198/339], [94mLoss[0m : 2.76029
[1mStep[0m  [231/339], [94mLoss[0m : 2.24888
[1mStep[0m  [264/339], [94mLoss[0m : 1.63530
[1mStep[0m  [297/339], [94mLoss[0m : 2.14963
[1mStep[0m  [330/339], [94mLoss[0m : 2.85398

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.338, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69674
[1mStep[0m  [33/339], [94mLoss[0m : 2.40150
[1mStep[0m  [66/339], [94mLoss[0m : 3.30607
[1mStep[0m  [99/339], [94mLoss[0m : 2.54080
[1mStep[0m  [132/339], [94mLoss[0m : 1.99247
[1mStep[0m  [165/339], [94mLoss[0m : 2.89881
[1mStep[0m  [198/339], [94mLoss[0m : 2.05157
[1mStep[0m  [231/339], [94mLoss[0m : 2.60514
[1mStep[0m  [264/339], [94mLoss[0m : 2.42875
[1mStep[0m  [297/339], [94mLoss[0m : 2.06946
[1mStep[0m  [330/339], [94mLoss[0m : 2.49601

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83940
[1mStep[0m  [33/339], [94mLoss[0m : 2.12901
[1mStep[0m  [66/339], [94mLoss[0m : 2.18525
[1mStep[0m  [99/339], [94mLoss[0m : 2.48271
[1mStep[0m  [132/339], [94mLoss[0m : 2.60505
[1mStep[0m  [165/339], [94mLoss[0m : 2.25603
[1mStep[0m  [198/339], [94mLoss[0m : 1.67336
[1mStep[0m  [231/339], [94mLoss[0m : 3.13219
[1mStep[0m  [264/339], [94mLoss[0m : 2.25622
[1mStep[0m  [297/339], [94mLoss[0m : 2.57941
[1mStep[0m  [330/339], [94mLoss[0m : 2.34994

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.346, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42599
[1mStep[0m  [33/339], [94mLoss[0m : 3.10877
[1mStep[0m  [66/339], [94mLoss[0m : 3.00671
[1mStep[0m  [99/339], [94mLoss[0m : 2.57064
[1mStep[0m  [132/339], [94mLoss[0m : 2.94887
[1mStep[0m  [165/339], [94mLoss[0m : 2.03481
[1mStep[0m  [198/339], [94mLoss[0m : 1.92990
[1mStep[0m  [231/339], [94mLoss[0m : 2.01749
[1mStep[0m  [264/339], [94mLoss[0m : 2.58362
[1mStep[0m  [297/339], [94mLoss[0m : 2.60096
[1mStep[0m  [330/339], [94mLoss[0m : 2.57949

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70861
[1mStep[0m  [33/339], [94mLoss[0m : 2.37567
[1mStep[0m  [66/339], [94mLoss[0m : 2.08593
[1mStep[0m  [99/339], [94mLoss[0m : 2.08991
[1mStep[0m  [132/339], [94mLoss[0m : 2.19476
[1mStep[0m  [165/339], [94mLoss[0m : 2.91987
[1mStep[0m  [198/339], [94mLoss[0m : 1.86229
[1mStep[0m  [231/339], [94mLoss[0m : 2.19498
[1mStep[0m  [264/339], [94mLoss[0m : 2.45082
[1mStep[0m  [297/339], [94mLoss[0m : 2.29399
[1mStep[0m  [330/339], [94mLoss[0m : 2.25286

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22067
[1mStep[0m  [33/339], [94mLoss[0m : 2.04980
[1mStep[0m  [66/339], [94mLoss[0m : 2.70960
[1mStep[0m  [99/339], [94mLoss[0m : 3.03226
[1mStep[0m  [132/339], [94mLoss[0m : 2.62642
[1mStep[0m  [165/339], [94mLoss[0m : 2.67364
[1mStep[0m  [198/339], [94mLoss[0m : 2.47706
[1mStep[0m  [231/339], [94mLoss[0m : 1.67290
[1mStep[0m  [264/339], [94mLoss[0m : 3.00541
[1mStep[0m  [297/339], [94mLoss[0m : 2.63360
[1mStep[0m  [330/339], [94mLoss[0m : 2.30485

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77550
[1mStep[0m  [33/339], [94mLoss[0m : 2.40111
[1mStep[0m  [66/339], [94mLoss[0m : 2.49798
[1mStep[0m  [99/339], [94mLoss[0m : 2.29777
[1mStep[0m  [132/339], [94mLoss[0m : 2.58357
[1mStep[0m  [165/339], [94mLoss[0m : 1.93448
[1mStep[0m  [198/339], [94mLoss[0m : 2.75428
[1mStep[0m  [231/339], [94mLoss[0m : 2.55694
[1mStep[0m  [264/339], [94mLoss[0m : 2.34110
[1mStep[0m  [297/339], [94mLoss[0m : 2.72966
[1mStep[0m  [330/339], [94mLoss[0m : 2.72723

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84688
[1mStep[0m  [33/339], [94mLoss[0m : 2.38058
[1mStep[0m  [66/339], [94mLoss[0m : 2.43527
[1mStep[0m  [99/339], [94mLoss[0m : 2.64181
[1mStep[0m  [132/339], [94mLoss[0m : 1.89792
[1mStep[0m  [165/339], [94mLoss[0m : 2.20184
[1mStep[0m  [198/339], [94mLoss[0m : 2.30794
[1mStep[0m  [231/339], [94mLoss[0m : 2.00118
[1mStep[0m  [264/339], [94mLoss[0m : 2.18685
[1mStep[0m  [297/339], [94mLoss[0m : 3.26960
[1mStep[0m  [330/339], [94mLoss[0m : 2.34134

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31982
[1mStep[0m  [33/339], [94mLoss[0m : 2.44972
[1mStep[0m  [66/339], [94mLoss[0m : 2.67233
[1mStep[0m  [99/339], [94mLoss[0m : 2.15086
[1mStep[0m  [132/339], [94mLoss[0m : 2.79303
[1mStep[0m  [165/339], [94mLoss[0m : 2.01338
[1mStep[0m  [198/339], [94mLoss[0m : 2.35657
[1mStep[0m  [231/339], [94mLoss[0m : 2.60255
[1mStep[0m  [264/339], [94mLoss[0m : 2.29943
[1mStep[0m  [297/339], [94mLoss[0m : 2.46425
[1mStep[0m  [330/339], [94mLoss[0m : 2.13170

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.342, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30144
[1mStep[0m  [33/339], [94mLoss[0m : 2.27569
[1mStep[0m  [66/339], [94mLoss[0m : 2.37870
[1mStep[0m  [99/339], [94mLoss[0m : 2.76457
[1mStep[0m  [132/339], [94mLoss[0m : 2.15691
[1mStep[0m  [165/339], [94mLoss[0m : 2.45484
[1mStep[0m  [198/339], [94mLoss[0m : 2.18615
[1mStep[0m  [231/339], [94mLoss[0m : 3.37356
[1mStep[0m  [264/339], [94mLoss[0m : 2.64874
[1mStep[0m  [297/339], [94mLoss[0m : 2.70550
[1mStep[0m  [330/339], [94mLoss[0m : 1.83157

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58820
[1mStep[0m  [33/339], [94mLoss[0m : 2.75328
[1mStep[0m  [66/339], [94mLoss[0m : 2.15577
[1mStep[0m  [99/339], [94mLoss[0m : 2.87416
[1mStep[0m  [132/339], [94mLoss[0m : 2.11666
[1mStep[0m  [165/339], [94mLoss[0m : 2.10676
[1mStep[0m  [198/339], [94mLoss[0m : 2.13766
[1mStep[0m  [231/339], [94mLoss[0m : 2.34503
[1mStep[0m  [264/339], [94mLoss[0m : 2.78405
[1mStep[0m  [297/339], [94mLoss[0m : 1.77158
[1mStep[0m  [330/339], [94mLoss[0m : 1.83086

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97990
[1mStep[0m  [33/339], [94mLoss[0m : 2.68173
[1mStep[0m  [66/339], [94mLoss[0m : 2.26243
[1mStep[0m  [99/339], [94mLoss[0m : 1.75948
[1mStep[0m  [132/339], [94mLoss[0m : 2.61264
[1mStep[0m  [165/339], [94mLoss[0m : 2.11787
[1mStep[0m  [198/339], [94mLoss[0m : 2.31169
[1mStep[0m  [231/339], [94mLoss[0m : 2.94432
[1mStep[0m  [264/339], [94mLoss[0m : 2.90937
[1mStep[0m  [297/339], [94mLoss[0m : 2.76559
[1mStep[0m  [330/339], [94mLoss[0m : 2.55094

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.328, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34700
[1mStep[0m  [33/339], [94mLoss[0m : 2.06355
[1mStep[0m  [66/339], [94mLoss[0m : 2.35315
[1mStep[0m  [99/339], [94mLoss[0m : 2.76686
[1mStep[0m  [132/339], [94mLoss[0m : 2.71701
[1mStep[0m  [165/339], [94mLoss[0m : 2.35415
[1mStep[0m  [198/339], [94mLoss[0m : 1.90096
[1mStep[0m  [231/339], [94mLoss[0m : 2.67719
[1mStep[0m  [264/339], [94mLoss[0m : 2.49079
[1mStep[0m  [297/339], [94mLoss[0m : 2.99701
[1mStep[0m  [330/339], [94mLoss[0m : 2.13690

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.351, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65427
[1mStep[0m  [33/339], [94mLoss[0m : 2.81414
[1mStep[0m  [66/339], [94mLoss[0m : 2.41218
[1mStep[0m  [99/339], [94mLoss[0m : 2.18242
[1mStep[0m  [132/339], [94mLoss[0m : 2.63408
[1mStep[0m  [165/339], [94mLoss[0m : 2.96392
[1mStep[0m  [198/339], [94mLoss[0m : 2.26121
[1mStep[0m  [231/339], [94mLoss[0m : 2.31873
[1mStep[0m  [264/339], [94mLoss[0m : 2.31256
[1mStep[0m  [297/339], [94mLoss[0m : 2.37670
[1mStep[0m  [330/339], [94mLoss[0m : 2.08080

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.297, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02845
[1mStep[0m  [33/339], [94mLoss[0m : 2.80598
[1mStep[0m  [66/339], [94mLoss[0m : 2.99945
[1mStep[0m  [99/339], [94mLoss[0m : 2.08386
[1mStep[0m  [132/339], [94mLoss[0m : 1.90772
[1mStep[0m  [165/339], [94mLoss[0m : 2.49766
[1mStep[0m  [198/339], [94mLoss[0m : 2.63271
[1mStep[0m  [231/339], [94mLoss[0m : 2.10126
[1mStep[0m  [264/339], [94mLoss[0m : 2.85483
[1mStep[0m  [297/339], [94mLoss[0m : 2.54421
[1mStep[0m  [330/339], [94mLoss[0m : 2.23953

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83051
[1mStep[0m  [33/339], [94mLoss[0m : 2.49651
[1mStep[0m  [66/339], [94mLoss[0m : 2.07537
[1mStep[0m  [99/339], [94mLoss[0m : 2.32480
[1mStep[0m  [132/339], [94mLoss[0m : 2.21441
[1mStep[0m  [165/339], [94mLoss[0m : 2.41369
[1mStep[0m  [198/339], [94mLoss[0m : 1.60312
[1mStep[0m  [231/339], [94mLoss[0m : 3.15091
[1mStep[0m  [264/339], [94mLoss[0m : 2.89494
[1mStep[0m  [297/339], [94mLoss[0m : 2.17031
[1mStep[0m  [330/339], [94mLoss[0m : 2.46713

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.309, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76415
[1mStep[0m  [33/339], [94mLoss[0m : 2.02549
[1mStep[0m  [66/339], [94mLoss[0m : 2.58804
[1mStep[0m  [99/339], [94mLoss[0m : 2.36189
[1mStep[0m  [132/339], [94mLoss[0m : 2.22861
[1mStep[0m  [165/339], [94mLoss[0m : 2.90748
[1mStep[0m  [198/339], [94mLoss[0m : 2.62853
[1mStep[0m  [231/339], [94mLoss[0m : 2.74920
[1mStep[0m  [264/339], [94mLoss[0m : 2.18496
[1mStep[0m  [297/339], [94mLoss[0m : 2.22853
[1mStep[0m  [330/339], [94mLoss[0m : 2.22203

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02996
[1mStep[0m  [33/339], [94mLoss[0m : 2.26769
[1mStep[0m  [66/339], [94mLoss[0m : 2.21334
[1mStep[0m  [99/339], [94mLoss[0m : 2.09940
[1mStep[0m  [132/339], [94mLoss[0m : 2.73828
[1mStep[0m  [165/339], [94mLoss[0m : 2.36152
[1mStep[0m  [198/339], [94mLoss[0m : 1.79923
[1mStep[0m  [231/339], [94mLoss[0m : 1.83999
[1mStep[0m  [264/339], [94mLoss[0m : 2.63387
[1mStep[0m  [297/339], [94mLoss[0m : 2.13703
[1mStep[0m  [330/339], [94mLoss[0m : 2.39945

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.308, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81556
[1mStep[0m  [33/339], [94mLoss[0m : 1.77494
[1mStep[0m  [66/339], [94mLoss[0m : 2.36864
[1mStep[0m  [99/339], [94mLoss[0m : 2.36001
[1mStep[0m  [132/339], [94mLoss[0m : 2.29666
[1mStep[0m  [165/339], [94mLoss[0m : 1.92682
[1mStep[0m  [198/339], [94mLoss[0m : 2.02440
[1mStep[0m  [231/339], [94mLoss[0m : 2.34728
[1mStep[0m  [264/339], [94mLoss[0m : 1.59773
[1mStep[0m  [297/339], [94mLoss[0m : 2.09547
[1mStep[0m  [330/339], [94mLoss[0m : 2.87977

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70748
[1mStep[0m  [33/339], [94mLoss[0m : 2.30647
[1mStep[0m  [66/339], [94mLoss[0m : 2.37942
[1mStep[0m  [99/339], [94mLoss[0m : 1.84558
[1mStep[0m  [132/339], [94mLoss[0m : 2.40744
[1mStep[0m  [165/339], [94mLoss[0m : 2.76382
[1mStep[0m  [198/339], [94mLoss[0m : 2.53168
[1mStep[0m  [231/339], [94mLoss[0m : 1.63439
[1mStep[0m  [264/339], [94mLoss[0m : 2.30615
[1mStep[0m  [297/339], [94mLoss[0m : 1.86027
[1mStep[0m  [330/339], [94mLoss[0m : 1.91901

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06470
[1mStep[0m  [33/339], [94mLoss[0m : 2.25003
[1mStep[0m  [66/339], [94mLoss[0m : 2.35499
[1mStep[0m  [99/339], [94mLoss[0m : 2.47488
[1mStep[0m  [132/339], [94mLoss[0m : 2.66179
[1mStep[0m  [165/339], [94mLoss[0m : 2.14462
[1mStep[0m  [198/339], [94mLoss[0m : 2.17778
[1mStep[0m  [231/339], [94mLoss[0m : 1.78162
[1mStep[0m  [264/339], [94mLoss[0m : 1.91357
[1mStep[0m  [297/339], [94mLoss[0m : 1.80222
[1mStep[0m  [330/339], [94mLoss[0m : 2.23622

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.314, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44103
[1mStep[0m  [33/339], [94mLoss[0m : 2.12604
[1mStep[0m  [66/339], [94mLoss[0m : 2.12066
[1mStep[0m  [99/339], [94mLoss[0m : 2.61692
[1mStep[0m  [132/339], [94mLoss[0m : 2.32232
[1mStep[0m  [165/339], [94mLoss[0m : 1.61955
[1mStep[0m  [198/339], [94mLoss[0m : 2.31421
[1mStep[0m  [231/339], [94mLoss[0m : 2.03967
[1mStep[0m  [264/339], [94mLoss[0m : 2.25888
[1mStep[0m  [297/339], [94mLoss[0m : 2.85058
[1mStep[0m  [330/339], [94mLoss[0m : 2.45479

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.335
====================================

Phase 1 - Evaluation MAE:  2.335021158235263
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 1.97505
[1mStep[0m  [33/339], [94mLoss[0m : 1.99059
[1mStep[0m  [66/339], [94mLoss[0m : 2.08821
[1mStep[0m  [99/339], [94mLoss[0m : 2.42747
[1mStep[0m  [132/339], [94mLoss[0m : 2.24397
[1mStep[0m  [165/339], [94mLoss[0m : 2.41998
[1mStep[0m  [198/339], [94mLoss[0m : 1.77657
[1mStep[0m  [231/339], [94mLoss[0m : 2.59010
[1mStep[0m  [264/339], [94mLoss[0m : 2.97458
[1mStep[0m  [297/339], [94mLoss[0m : 2.67284
[1mStep[0m  [330/339], [94mLoss[0m : 2.03954

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41667
[1mStep[0m  [33/339], [94mLoss[0m : 2.52969
[1mStep[0m  [66/339], [94mLoss[0m : 2.38870
[1mStep[0m  [99/339], [94mLoss[0m : 2.43944
[1mStep[0m  [132/339], [94mLoss[0m : 2.78225
[1mStep[0m  [165/339], [94mLoss[0m : 2.88065
[1mStep[0m  [198/339], [94mLoss[0m : 2.10237
[1mStep[0m  [231/339], [94mLoss[0m : 2.71276
[1mStep[0m  [264/339], [94mLoss[0m : 2.50508
[1mStep[0m  [297/339], [94mLoss[0m : 2.35868
[1mStep[0m  [330/339], [94mLoss[0m : 2.21713

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.376, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69256
[1mStep[0m  [33/339], [94mLoss[0m : 3.05723
[1mStep[0m  [66/339], [94mLoss[0m : 1.87874
[1mStep[0m  [99/339], [94mLoss[0m : 2.37755
[1mStep[0m  [132/339], [94mLoss[0m : 2.06736
[1mStep[0m  [165/339], [94mLoss[0m : 2.60183
[1mStep[0m  [198/339], [94mLoss[0m : 2.72098
[1mStep[0m  [231/339], [94mLoss[0m : 1.62316
[1mStep[0m  [264/339], [94mLoss[0m : 1.74336
[1mStep[0m  [297/339], [94mLoss[0m : 2.00985
[1mStep[0m  [330/339], [94mLoss[0m : 2.45512

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23462
[1mStep[0m  [33/339], [94mLoss[0m : 2.30285
[1mStep[0m  [66/339], [94mLoss[0m : 1.82141
[1mStep[0m  [99/339], [94mLoss[0m : 2.34002
[1mStep[0m  [132/339], [94mLoss[0m : 2.01044
[1mStep[0m  [165/339], [94mLoss[0m : 2.00906
[1mStep[0m  [198/339], [94mLoss[0m : 1.95964
[1mStep[0m  [231/339], [94mLoss[0m : 2.60137
[1mStep[0m  [264/339], [94mLoss[0m : 2.51649
[1mStep[0m  [297/339], [94mLoss[0m : 1.91498
[1mStep[0m  [330/339], [94mLoss[0m : 1.79109

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.74331
[1mStep[0m  [33/339], [94mLoss[0m : 2.55377
[1mStep[0m  [66/339], [94mLoss[0m : 2.68374
[1mStep[0m  [99/339], [94mLoss[0m : 1.82813
[1mStep[0m  [132/339], [94mLoss[0m : 2.35797
[1mStep[0m  [165/339], [94mLoss[0m : 2.35105
[1mStep[0m  [198/339], [94mLoss[0m : 2.39085
[1mStep[0m  [231/339], [94mLoss[0m : 1.46473
[1mStep[0m  [264/339], [94mLoss[0m : 2.21175
[1mStep[0m  [297/339], [94mLoss[0m : 2.08333
[1mStep[0m  [330/339], [94mLoss[0m : 2.24768

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.371, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61022
[1mStep[0m  [33/339], [94mLoss[0m : 1.69326
[1mStep[0m  [66/339], [94mLoss[0m : 1.36379
[1mStep[0m  [99/339], [94mLoss[0m : 2.42745
[1mStep[0m  [132/339], [94mLoss[0m : 1.87601
[1mStep[0m  [165/339], [94mLoss[0m : 2.38564
[1mStep[0m  [198/339], [94mLoss[0m : 2.45428
[1mStep[0m  [231/339], [94mLoss[0m : 2.02338
[1mStep[0m  [264/339], [94mLoss[0m : 1.96715
[1mStep[0m  [297/339], [94mLoss[0m : 2.72551
[1mStep[0m  [330/339], [94mLoss[0m : 2.17465

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.366, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92815
[1mStep[0m  [33/339], [94mLoss[0m : 2.15200
[1mStep[0m  [66/339], [94mLoss[0m : 1.87087
[1mStep[0m  [99/339], [94mLoss[0m : 1.83420
[1mStep[0m  [132/339], [94mLoss[0m : 1.71237
[1mStep[0m  [165/339], [94mLoss[0m : 2.65942
[1mStep[0m  [198/339], [94mLoss[0m : 1.99905
[1mStep[0m  [231/339], [94mLoss[0m : 2.34231
[1mStep[0m  [264/339], [94mLoss[0m : 2.12931
[1mStep[0m  [297/339], [94mLoss[0m : 2.02575
[1mStep[0m  [330/339], [94mLoss[0m : 2.12504

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.410, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06466
[1mStep[0m  [33/339], [94mLoss[0m : 1.94942
[1mStep[0m  [66/339], [94mLoss[0m : 2.22323
[1mStep[0m  [99/339], [94mLoss[0m : 2.17439
[1mStep[0m  [132/339], [94mLoss[0m : 1.81393
[1mStep[0m  [165/339], [94mLoss[0m : 2.32636
[1mStep[0m  [198/339], [94mLoss[0m : 1.90144
[1mStep[0m  [231/339], [94mLoss[0m : 2.40801
[1mStep[0m  [264/339], [94mLoss[0m : 2.32801
[1mStep[0m  [297/339], [94mLoss[0m : 2.79682
[1mStep[0m  [330/339], [94mLoss[0m : 2.27221

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.075, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53399
[1mStep[0m  [33/339], [94mLoss[0m : 2.01848
[1mStep[0m  [66/339], [94mLoss[0m : 1.57002
[1mStep[0m  [99/339], [94mLoss[0m : 2.41982
[1mStep[0m  [132/339], [94mLoss[0m : 2.36459
[1mStep[0m  [165/339], [94mLoss[0m : 2.07372
[1mStep[0m  [198/339], [94mLoss[0m : 1.35691
[1mStep[0m  [231/339], [94mLoss[0m : 1.89310
[1mStep[0m  [264/339], [94mLoss[0m : 2.39632
[1mStep[0m  [297/339], [94mLoss[0m : 1.58536
[1mStep[0m  [330/339], [94mLoss[0m : 2.10683

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.445, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19810
[1mStep[0m  [33/339], [94mLoss[0m : 2.30103
[1mStep[0m  [66/339], [94mLoss[0m : 1.83602
[1mStep[0m  [99/339], [94mLoss[0m : 1.79600
[1mStep[0m  [132/339], [94mLoss[0m : 2.23236
[1mStep[0m  [165/339], [94mLoss[0m : 2.16950
[1mStep[0m  [198/339], [94mLoss[0m : 1.65026
[1mStep[0m  [231/339], [94mLoss[0m : 2.17343
[1mStep[0m  [264/339], [94mLoss[0m : 2.33748
[1mStep[0m  [297/339], [94mLoss[0m : 1.85737
[1mStep[0m  [330/339], [94mLoss[0m : 2.45039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09205
[1mStep[0m  [33/339], [94mLoss[0m : 1.54082
[1mStep[0m  [66/339], [94mLoss[0m : 2.42859
[1mStep[0m  [99/339], [94mLoss[0m : 1.54536
[1mStep[0m  [132/339], [94mLoss[0m : 1.94539
[1mStep[0m  [165/339], [94mLoss[0m : 1.60937
[1mStep[0m  [198/339], [94mLoss[0m : 1.74263
[1mStep[0m  [231/339], [94mLoss[0m : 1.90542
[1mStep[0m  [264/339], [94mLoss[0m : 2.41441
[1mStep[0m  [297/339], [94mLoss[0m : 2.33442
[1mStep[0m  [330/339], [94mLoss[0m : 2.74035

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.487, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71435
[1mStep[0m  [33/339], [94mLoss[0m : 1.84298
[1mStep[0m  [66/339], [94mLoss[0m : 2.12542
[1mStep[0m  [99/339], [94mLoss[0m : 1.80393
[1mStep[0m  [132/339], [94mLoss[0m : 2.32137
[1mStep[0m  [165/339], [94mLoss[0m : 2.30914
[1mStep[0m  [198/339], [94mLoss[0m : 1.91421
[1mStep[0m  [231/339], [94mLoss[0m : 1.79041
[1mStep[0m  [264/339], [94mLoss[0m : 2.16582
[1mStep[0m  [297/339], [94mLoss[0m : 1.81053
[1mStep[0m  [330/339], [94mLoss[0m : 2.07462

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.26662
[1mStep[0m  [33/339], [94mLoss[0m : 1.85364
[1mStep[0m  [66/339], [94mLoss[0m : 2.19011
[1mStep[0m  [99/339], [94mLoss[0m : 2.17519
[1mStep[0m  [132/339], [94mLoss[0m : 2.35936
[1mStep[0m  [165/339], [94mLoss[0m : 2.01143
[1mStep[0m  [198/339], [94mLoss[0m : 2.22865
[1mStep[0m  [231/339], [94mLoss[0m : 1.87215
[1mStep[0m  [264/339], [94mLoss[0m : 2.21586
[1mStep[0m  [297/339], [94mLoss[0m : 2.41920
[1mStep[0m  [330/339], [94mLoss[0m : 1.87006

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.444, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56131
[1mStep[0m  [33/339], [94mLoss[0m : 2.05202
[1mStep[0m  [66/339], [94mLoss[0m : 1.94597
[1mStep[0m  [99/339], [94mLoss[0m : 2.03613
[1mStep[0m  [132/339], [94mLoss[0m : 1.32583
[1mStep[0m  [165/339], [94mLoss[0m : 2.38242
[1mStep[0m  [198/339], [94mLoss[0m : 1.67810
[1mStep[0m  [231/339], [94mLoss[0m : 1.85098
[1mStep[0m  [264/339], [94mLoss[0m : 1.81078
[1mStep[0m  [297/339], [94mLoss[0m : 1.47257
[1mStep[0m  [330/339], [94mLoss[0m : 1.96436

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.916, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.31348
[1mStep[0m  [33/339], [94mLoss[0m : 1.85788
[1mStep[0m  [66/339], [94mLoss[0m : 1.81647
[1mStep[0m  [99/339], [94mLoss[0m : 1.80812
[1mStep[0m  [132/339], [94mLoss[0m : 1.75131
[1mStep[0m  [165/339], [94mLoss[0m : 1.80157
[1mStep[0m  [198/339], [94mLoss[0m : 2.31253
[1mStep[0m  [231/339], [94mLoss[0m : 1.45844
[1mStep[0m  [264/339], [94mLoss[0m : 1.52509
[1mStep[0m  [297/339], [94mLoss[0m : 1.90347
[1mStep[0m  [330/339], [94mLoss[0m : 2.42625

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.532, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63919
[1mStep[0m  [33/339], [94mLoss[0m : 2.05953
[1mStep[0m  [66/339], [94mLoss[0m : 1.41886
[1mStep[0m  [99/339], [94mLoss[0m : 1.68909
[1mStep[0m  [132/339], [94mLoss[0m : 2.47869
[1mStep[0m  [165/339], [94mLoss[0m : 1.79094
[1mStep[0m  [198/339], [94mLoss[0m : 2.02506
[1mStep[0m  [231/339], [94mLoss[0m : 1.69494
[1mStep[0m  [264/339], [94mLoss[0m : 1.55633
[1mStep[0m  [297/339], [94mLoss[0m : 1.89266
[1mStep[0m  [330/339], [94mLoss[0m : 2.02058

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.868, [92mTest[0m: 2.478, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63128
[1mStep[0m  [33/339], [94mLoss[0m : 1.84052
[1mStep[0m  [66/339], [94mLoss[0m : 2.13036
[1mStep[0m  [99/339], [94mLoss[0m : 1.46322
[1mStep[0m  [132/339], [94mLoss[0m : 2.39780
[1mStep[0m  [165/339], [94mLoss[0m : 1.66772
[1mStep[0m  [198/339], [94mLoss[0m : 1.68128
[1mStep[0m  [231/339], [94mLoss[0m : 1.86906
[1mStep[0m  [264/339], [94mLoss[0m : 2.07573
[1mStep[0m  [297/339], [94mLoss[0m : 3.33687
[1mStep[0m  [330/339], [94mLoss[0m : 1.71476

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.828, [92mTest[0m: 2.627, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76760
[1mStep[0m  [33/339], [94mLoss[0m : 1.83772
[1mStep[0m  [66/339], [94mLoss[0m : 1.48963
[1mStep[0m  [99/339], [94mLoss[0m : 2.02466
[1mStep[0m  [132/339], [94mLoss[0m : 2.19975
[1mStep[0m  [165/339], [94mLoss[0m : 1.66817
[1mStep[0m  [198/339], [94mLoss[0m : 1.95127
[1mStep[0m  [231/339], [94mLoss[0m : 2.15748
[1mStep[0m  [264/339], [94mLoss[0m : 1.63208
[1mStep[0m  [297/339], [94mLoss[0m : 1.61312
[1mStep[0m  [330/339], [94mLoss[0m : 1.64116

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.521, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.04842
[1mStep[0m  [33/339], [94mLoss[0m : 1.79601
[1mStep[0m  [66/339], [94mLoss[0m : 1.80513
[1mStep[0m  [99/339], [94mLoss[0m : 1.63266
[1mStep[0m  [132/339], [94mLoss[0m : 1.58882
[1mStep[0m  [165/339], [94mLoss[0m : 1.52387
[1mStep[0m  [198/339], [94mLoss[0m : 1.39618
[1mStep[0m  [231/339], [94mLoss[0m : 1.85400
[1mStep[0m  [264/339], [94mLoss[0m : 1.81367
[1mStep[0m  [297/339], [94mLoss[0m : 1.75083
[1mStep[0m  [330/339], [94mLoss[0m : 1.96904

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.528, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87660
[1mStep[0m  [33/339], [94mLoss[0m : 1.43792
[1mStep[0m  [66/339], [94mLoss[0m : 1.61265
[1mStep[0m  [99/339], [94mLoss[0m : 1.68446
[1mStep[0m  [132/339], [94mLoss[0m : 1.68958
[1mStep[0m  [165/339], [94mLoss[0m : 1.93134
[1mStep[0m  [198/339], [94mLoss[0m : 1.43193
[1mStep[0m  [231/339], [94mLoss[0m : 2.04701
[1mStep[0m  [264/339], [94mLoss[0m : 1.40428
[1mStep[0m  [297/339], [94mLoss[0m : 2.65366
[1mStep[0m  [330/339], [94mLoss[0m : 2.08911

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.496, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45209
[1mStep[0m  [33/339], [94mLoss[0m : 1.46000
[1mStep[0m  [66/339], [94mLoss[0m : 2.17980
[1mStep[0m  [99/339], [94mLoss[0m : 1.51427
[1mStep[0m  [132/339], [94mLoss[0m : 2.06196
[1mStep[0m  [165/339], [94mLoss[0m : 2.07385
[1mStep[0m  [198/339], [94mLoss[0m : 1.49631
[1mStep[0m  [231/339], [94mLoss[0m : 1.94399
[1mStep[0m  [264/339], [94mLoss[0m : 1.40247
[1mStep[0m  [297/339], [94mLoss[0m : 1.79793
[1mStep[0m  [330/339], [94mLoss[0m : 1.93181

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.487, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79482
[1mStep[0m  [33/339], [94mLoss[0m : 1.93828
[1mStep[0m  [66/339], [94mLoss[0m : 1.76056
[1mStep[0m  [99/339], [94mLoss[0m : 1.75610
[1mStep[0m  [132/339], [94mLoss[0m : 1.66588
[1mStep[0m  [165/339], [94mLoss[0m : 2.10931
[1mStep[0m  [198/339], [94mLoss[0m : 1.63720
[1mStep[0m  [231/339], [94mLoss[0m : 2.84545
[1mStep[0m  [264/339], [94mLoss[0m : 1.36235
[1mStep[0m  [297/339], [94mLoss[0m : 1.87181
[1mStep[0m  [330/339], [94mLoss[0m : 1.53930

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.701, [92mTest[0m: 2.543, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87147
[1mStep[0m  [33/339], [94mLoss[0m : 1.76150
[1mStep[0m  [66/339], [94mLoss[0m : 1.51415
[1mStep[0m  [99/339], [94mLoss[0m : 1.22222
[1mStep[0m  [132/339], [94mLoss[0m : 1.53440
[1mStep[0m  [165/339], [94mLoss[0m : 1.32418
[1mStep[0m  [198/339], [94mLoss[0m : 1.70504
[1mStep[0m  [231/339], [94mLoss[0m : 1.44954
[1mStep[0m  [264/339], [94mLoss[0m : 1.14871
[1mStep[0m  [297/339], [94mLoss[0m : 1.58609
[1mStep[0m  [330/339], [94mLoss[0m : 1.87766

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.563, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57453
[1mStep[0m  [33/339], [94mLoss[0m : 1.89029
[1mStep[0m  [66/339], [94mLoss[0m : 1.54092
[1mStep[0m  [99/339], [94mLoss[0m : 1.59341
[1mStep[0m  [132/339], [94mLoss[0m : 1.37110
[1mStep[0m  [165/339], [94mLoss[0m : 1.59976
[1mStep[0m  [198/339], [94mLoss[0m : 2.05703
[1mStep[0m  [231/339], [94mLoss[0m : 1.27174
[1mStep[0m  [264/339], [94mLoss[0m : 1.80994
[1mStep[0m  [297/339], [94mLoss[0m : 1.42677
[1mStep[0m  [330/339], [94mLoss[0m : 1.42929

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.547, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.54503
[1mStep[0m  [33/339], [94mLoss[0m : 2.10884
[1mStep[0m  [66/339], [94mLoss[0m : 1.41714
[1mStep[0m  [99/339], [94mLoss[0m : 1.97658
[1mStep[0m  [132/339], [94mLoss[0m : 1.56177
[1mStep[0m  [165/339], [94mLoss[0m : 1.82530
[1mStep[0m  [198/339], [94mLoss[0m : 1.26603
[1mStep[0m  [231/339], [94mLoss[0m : 2.08883
[1mStep[0m  [264/339], [94mLoss[0m : 1.52540
[1mStep[0m  [297/339], [94mLoss[0m : 1.62208
[1mStep[0m  [330/339], [94mLoss[0m : 1.14693

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.513, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53017
[1mStep[0m  [33/339], [94mLoss[0m : 1.94393
[1mStep[0m  [66/339], [94mLoss[0m : 1.35770
[1mStep[0m  [99/339], [94mLoss[0m : 1.19185
[1mStep[0m  [132/339], [94mLoss[0m : 2.30572
[1mStep[0m  [165/339], [94mLoss[0m : 1.59909
[1mStep[0m  [198/339], [94mLoss[0m : 1.93098
[1mStep[0m  [231/339], [94mLoss[0m : 1.85665
[1mStep[0m  [264/339], [94mLoss[0m : 1.26554
[1mStep[0m  [297/339], [94mLoss[0m : 1.21332
[1mStep[0m  [330/339], [94mLoss[0m : 1.67436

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.514, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.50055
[1mStep[0m  [33/339], [94mLoss[0m : 1.46488
[1mStep[0m  [66/339], [94mLoss[0m : 2.15935
[1mStep[0m  [99/339], [94mLoss[0m : 1.46075
[1mStep[0m  [132/339], [94mLoss[0m : 1.29565
[1mStep[0m  [165/339], [94mLoss[0m : 1.62224
[1mStep[0m  [198/339], [94mLoss[0m : 1.52810
[1mStep[0m  [231/339], [94mLoss[0m : 1.64742
[1mStep[0m  [264/339], [94mLoss[0m : 1.61520
[1mStep[0m  [297/339], [94mLoss[0m : 1.41105
[1mStep[0m  [330/339], [94mLoss[0m : 1.98502

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.605, [92mTest[0m: 2.590, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52607
[1mStep[0m  [33/339], [94mLoss[0m : 1.56986
[1mStep[0m  [66/339], [94mLoss[0m : 1.82976
[1mStep[0m  [99/339], [94mLoss[0m : 1.88084
[1mStep[0m  [132/339], [94mLoss[0m : 1.37204
[1mStep[0m  [165/339], [94mLoss[0m : 1.73459
[1mStep[0m  [198/339], [94mLoss[0m : 1.79248
[1mStep[0m  [231/339], [94mLoss[0m : 1.86660
[1mStep[0m  [264/339], [94mLoss[0m : 1.57812
[1mStep[0m  [297/339], [94mLoss[0m : 1.99397
[1mStep[0m  [330/339], [94mLoss[0m : 1.62511

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.554, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62025
[1mStep[0m  [33/339], [94mLoss[0m : 1.81750
[1mStep[0m  [66/339], [94mLoss[0m : 1.42203
[1mStep[0m  [99/339], [94mLoss[0m : 0.99616
[1mStep[0m  [132/339], [94mLoss[0m : 1.76934
[1mStep[0m  [165/339], [94mLoss[0m : 1.23063
[1mStep[0m  [198/339], [94mLoss[0m : 1.49221
[1mStep[0m  [231/339], [94mLoss[0m : 1.54292
[1mStep[0m  [264/339], [94mLoss[0m : 1.41517
[1mStep[0m  [297/339], [94mLoss[0m : 1.30113
[1mStep[0m  [330/339], [94mLoss[0m : 1.54077

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.591, [92mTest[0m: 2.545, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42083
[1mStep[0m  [33/339], [94mLoss[0m : 1.64031
[1mStep[0m  [66/339], [94mLoss[0m : 1.63406
[1mStep[0m  [99/339], [94mLoss[0m : 1.33057
[1mStep[0m  [132/339], [94mLoss[0m : 1.65206
[1mStep[0m  [165/339], [94mLoss[0m : 1.23275
[1mStep[0m  [198/339], [94mLoss[0m : 1.72595
[1mStep[0m  [231/339], [94mLoss[0m : 1.69036
[1mStep[0m  [264/339], [94mLoss[0m : 1.62705
[1mStep[0m  [297/339], [94mLoss[0m : 1.37104
[1mStep[0m  [330/339], [94mLoss[0m : 1.54160

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.554, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.659
====================================

Phase 2 - Evaluation MAE:  2.658742006901091
MAE score P1        2.335021
MAE score P2        2.658742
loss                1.581996
learning_rate        0.00505
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.10867
[1mStep[0m  [33/339], [94mLoss[0m : 4.05910
[1mStep[0m  [66/339], [94mLoss[0m : 2.54157
[1mStep[0m  [99/339], [94mLoss[0m : 1.96990
[1mStep[0m  [132/339], [94mLoss[0m : 2.52213
[1mStep[0m  [165/339], [94mLoss[0m : 3.20177
[1mStep[0m  [198/339], [94mLoss[0m : 2.68963
[1mStep[0m  [231/339], [94mLoss[0m : 2.54596
[1mStep[0m  [264/339], [94mLoss[0m : 2.20416
[1mStep[0m  [297/339], [94mLoss[0m : 3.08439
[1mStep[0m  [330/339], [94mLoss[0m : 2.82760

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.149, [92mTest[0m: 10.862, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18353
[1mStep[0m  [33/339], [94mLoss[0m : 2.71367
[1mStep[0m  [66/339], [94mLoss[0m : 2.38466
[1mStep[0m  [99/339], [94mLoss[0m : 2.15713
[1mStep[0m  [132/339], [94mLoss[0m : 2.51047
[1mStep[0m  [165/339], [94mLoss[0m : 2.12195
[1mStep[0m  [198/339], [94mLoss[0m : 2.81763
[1mStep[0m  [231/339], [94mLoss[0m : 2.00014
[1mStep[0m  [264/339], [94mLoss[0m : 2.58997
[1mStep[0m  [297/339], [94mLoss[0m : 2.47655
[1mStep[0m  [330/339], [94mLoss[0m : 2.33820

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.661, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24197
[1mStep[0m  [33/339], [94mLoss[0m : 2.45812
[1mStep[0m  [66/339], [94mLoss[0m : 2.81189
[1mStep[0m  [99/339], [94mLoss[0m : 2.32631
[1mStep[0m  [132/339], [94mLoss[0m : 2.21424
[1mStep[0m  [165/339], [94mLoss[0m : 2.44910
[1mStep[0m  [198/339], [94mLoss[0m : 2.47425
[1mStep[0m  [231/339], [94mLoss[0m : 2.08414
[1mStep[0m  [264/339], [94mLoss[0m : 2.40225
[1mStep[0m  [297/339], [94mLoss[0m : 2.75181
[1mStep[0m  [330/339], [94mLoss[0m : 3.35242

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62616
[1mStep[0m  [33/339], [94mLoss[0m : 2.59829
[1mStep[0m  [66/339], [94mLoss[0m : 2.43295
[1mStep[0m  [99/339], [94mLoss[0m : 2.54611
[1mStep[0m  [132/339], [94mLoss[0m : 2.48935
[1mStep[0m  [165/339], [94mLoss[0m : 2.40918
[1mStep[0m  [198/339], [94mLoss[0m : 2.48077
[1mStep[0m  [231/339], [94mLoss[0m : 3.09365
[1mStep[0m  [264/339], [94mLoss[0m : 2.70803
[1mStep[0m  [297/339], [94mLoss[0m : 1.93962
[1mStep[0m  [330/339], [94mLoss[0m : 2.00615

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.547, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35269
[1mStep[0m  [33/339], [94mLoss[0m : 3.05990
[1mStep[0m  [66/339], [94mLoss[0m : 2.86623
[1mStep[0m  [99/339], [94mLoss[0m : 2.71334
[1mStep[0m  [132/339], [94mLoss[0m : 1.87105
[1mStep[0m  [165/339], [94mLoss[0m : 3.14192
[1mStep[0m  [198/339], [94mLoss[0m : 1.72555
[1mStep[0m  [231/339], [94mLoss[0m : 2.50524
[1mStep[0m  [264/339], [94mLoss[0m : 2.28829
[1mStep[0m  [297/339], [94mLoss[0m : 3.10951
[1mStep[0m  [330/339], [94mLoss[0m : 2.78556

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.461, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36378
[1mStep[0m  [33/339], [94mLoss[0m : 2.83841
[1mStep[0m  [66/339], [94mLoss[0m : 2.41732
[1mStep[0m  [99/339], [94mLoss[0m : 2.93706
[1mStep[0m  [132/339], [94mLoss[0m : 2.83267
[1mStep[0m  [165/339], [94mLoss[0m : 2.86037
[1mStep[0m  [198/339], [94mLoss[0m : 2.51661
[1mStep[0m  [231/339], [94mLoss[0m : 2.76620
[1mStep[0m  [264/339], [94mLoss[0m : 2.62463
[1mStep[0m  [297/339], [94mLoss[0m : 2.47878
[1mStep[0m  [330/339], [94mLoss[0m : 2.18330

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.432, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88759
[1mStep[0m  [33/339], [94mLoss[0m : 2.23721
[1mStep[0m  [66/339], [94mLoss[0m : 2.90845
[1mStep[0m  [99/339], [94mLoss[0m : 2.40945
[1mStep[0m  [132/339], [94mLoss[0m : 3.20063
[1mStep[0m  [165/339], [94mLoss[0m : 2.80976
[1mStep[0m  [198/339], [94mLoss[0m : 2.41668
[1mStep[0m  [231/339], [94mLoss[0m : 2.38335
[1mStep[0m  [264/339], [94mLoss[0m : 2.61480
[1mStep[0m  [297/339], [94mLoss[0m : 2.56194
[1mStep[0m  [330/339], [94mLoss[0m : 2.35339

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.386, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49240
[1mStep[0m  [33/339], [94mLoss[0m : 1.97334
[1mStep[0m  [66/339], [94mLoss[0m : 2.33458
[1mStep[0m  [99/339], [94mLoss[0m : 2.27749
[1mStep[0m  [132/339], [94mLoss[0m : 3.01892
[1mStep[0m  [165/339], [94mLoss[0m : 3.53306
[1mStep[0m  [198/339], [94mLoss[0m : 2.53472
[1mStep[0m  [231/339], [94mLoss[0m : 2.38623
[1mStep[0m  [264/339], [94mLoss[0m : 2.68294
[1mStep[0m  [297/339], [94mLoss[0m : 2.82920
[1mStep[0m  [330/339], [94mLoss[0m : 2.41706

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.392, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37896
[1mStep[0m  [33/339], [94mLoss[0m : 3.04331
[1mStep[0m  [66/339], [94mLoss[0m : 2.79116
[1mStep[0m  [99/339], [94mLoss[0m : 3.01275
[1mStep[0m  [132/339], [94mLoss[0m : 2.32283
[1mStep[0m  [165/339], [94mLoss[0m : 2.39811
[1mStep[0m  [198/339], [94mLoss[0m : 2.78989
[1mStep[0m  [231/339], [94mLoss[0m : 2.59723
[1mStep[0m  [264/339], [94mLoss[0m : 2.82766
[1mStep[0m  [297/339], [94mLoss[0m : 2.70809
[1mStep[0m  [330/339], [94mLoss[0m : 2.32170

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.418, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62413
[1mStep[0m  [33/339], [94mLoss[0m : 1.85572
[1mStep[0m  [66/339], [94mLoss[0m : 2.38172
[1mStep[0m  [99/339], [94mLoss[0m : 2.09993
[1mStep[0m  [132/339], [94mLoss[0m : 1.89037
[1mStep[0m  [165/339], [94mLoss[0m : 2.62683
[1mStep[0m  [198/339], [94mLoss[0m : 2.54570
[1mStep[0m  [231/339], [94mLoss[0m : 2.67337
[1mStep[0m  [264/339], [94mLoss[0m : 2.71472
[1mStep[0m  [297/339], [94mLoss[0m : 2.23488
[1mStep[0m  [330/339], [94mLoss[0m : 2.64149

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.331, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14402
[1mStep[0m  [33/339], [94mLoss[0m : 1.61716
[1mStep[0m  [66/339], [94mLoss[0m : 3.11641
[1mStep[0m  [99/339], [94mLoss[0m : 2.43275
[1mStep[0m  [132/339], [94mLoss[0m : 2.36785
[1mStep[0m  [165/339], [94mLoss[0m : 2.90482
[1mStep[0m  [198/339], [94mLoss[0m : 3.06666
[1mStep[0m  [231/339], [94mLoss[0m : 2.65955
[1mStep[0m  [264/339], [94mLoss[0m : 3.04662
[1mStep[0m  [297/339], [94mLoss[0m : 2.86972
[1mStep[0m  [330/339], [94mLoss[0m : 2.39309

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.354, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35366
[1mStep[0m  [33/339], [94mLoss[0m : 2.53988
[1mStep[0m  [66/339], [94mLoss[0m : 2.17052
[1mStep[0m  [99/339], [94mLoss[0m : 1.95319
[1mStep[0m  [132/339], [94mLoss[0m : 2.11443
[1mStep[0m  [165/339], [94mLoss[0m : 1.89976
[1mStep[0m  [198/339], [94mLoss[0m : 2.17359
[1mStep[0m  [231/339], [94mLoss[0m : 2.11273
[1mStep[0m  [264/339], [94mLoss[0m : 2.09727
[1mStep[0m  [297/339], [94mLoss[0m : 2.81378
[1mStep[0m  [330/339], [94mLoss[0m : 2.54954

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.394, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51004
[1mStep[0m  [33/339], [94mLoss[0m : 2.07495
[1mStep[0m  [66/339], [94mLoss[0m : 2.22064
[1mStep[0m  [99/339], [94mLoss[0m : 2.11455
[1mStep[0m  [132/339], [94mLoss[0m : 2.44260
[1mStep[0m  [165/339], [94mLoss[0m : 2.73247
[1mStep[0m  [198/339], [94mLoss[0m : 2.63318
[1mStep[0m  [231/339], [94mLoss[0m : 2.23704
[1mStep[0m  [264/339], [94mLoss[0m : 2.44846
[1mStep[0m  [297/339], [94mLoss[0m : 2.78740
[1mStep[0m  [330/339], [94mLoss[0m : 1.96616

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.378, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46014
[1mStep[0m  [33/339], [94mLoss[0m : 2.19127
[1mStep[0m  [66/339], [94mLoss[0m : 2.30010
[1mStep[0m  [99/339], [94mLoss[0m : 2.44169
[1mStep[0m  [132/339], [94mLoss[0m : 2.39330
[1mStep[0m  [165/339], [94mLoss[0m : 2.63770
[1mStep[0m  [198/339], [94mLoss[0m : 2.58393
[1mStep[0m  [231/339], [94mLoss[0m : 1.83189
[1mStep[0m  [264/339], [94mLoss[0m : 3.05747
[1mStep[0m  [297/339], [94mLoss[0m : 3.21369
[1mStep[0m  [330/339], [94mLoss[0m : 3.21376

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.367, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15583
[1mStep[0m  [33/339], [94mLoss[0m : 2.40507
[1mStep[0m  [66/339], [94mLoss[0m : 2.36335
[1mStep[0m  [99/339], [94mLoss[0m : 2.53517
[1mStep[0m  [132/339], [94mLoss[0m : 3.14237
[1mStep[0m  [165/339], [94mLoss[0m : 2.53958
[1mStep[0m  [198/339], [94mLoss[0m : 3.31393
[1mStep[0m  [231/339], [94mLoss[0m : 2.15129
[1mStep[0m  [264/339], [94mLoss[0m : 2.02175
[1mStep[0m  [297/339], [94mLoss[0m : 2.77506
[1mStep[0m  [330/339], [94mLoss[0m : 2.76979

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52446
[1mStep[0m  [33/339], [94mLoss[0m : 2.26853
[1mStep[0m  [66/339], [94mLoss[0m : 2.52577
[1mStep[0m  [99/339], [94mLoss[0m : 2.18264
[1mStep[0m  [132/339], [94mLoss[0m : 2.09110
[1mStep[0m  [165/339], [94mLoss[0m : 2.69301
[1mStep[0m  [198/339], [94mLoss[0m : 1.59980
[1mStep[0m  [231/339], [94mLoss[0m : 1.94869
[1mStep[0m  [264/339], [94mLoss[0m : 2.19226
[1mStep[0m  [297/339], [94mLoss[0m : 2.18274
[1mStep[0m  [330/339], [94mLoss[0m : 2.42685

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52215
[1mStep[0m  [33/339], [94mLoss[0m : 3.01797
[1mStep[0m  [66/339], [94mLoss[0m : 2.82800
[1mStep[0m  [99/339], [94mLoss[0m : 2.27704
[1mStep[0m  [132/339], [94mLoss[0m : 2.45759
[1mStep[0m  [165/339], [94mLoss[0m : 2.53917
[1mStep[0m  [198/339], [94mLoss[0m : 2.02732
[1mStep[0m  [231/339], [94mLoss[0m : 2.14832
[1mStep[0m  [264/339], [94mLoss[0m : 2.52969
[1mStep[0m  [297/339], [94mLoss[0m : 2.35214
[1mStep[0m  [330/339], [94mLoss[0m : 2.10971

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42701
[1mStep[0m  [33/339], [94mLoss[0m : 2.23908
[1mStep[0m  [66/339], [94mLoss[0m : 2.57264
[1mStep[0m  [99/339], [94mLoss[0m : 2.56110
[1mStep[0m  [132/339], [94mLoss[0m : 2.07214
[1mStep[0m  [165/339], [94mLoss[0m : 2.30584
[1mStep[0m  [198/339], [94mLoss[0m : 2.24576
[1mStep[0m  [231/339], [94mLoss[0m : 2.25024
[1mStep[0m  [264/339], [94mLoss[0m : 2.28868
[1mStep[0m  [297/339], [94mLoss[0m : 2.29178
[1mStep[0m  [330/339], [94mLoss[0m : 2.77148

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.374, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43780
[1mStep[0m  [33/339], [94mLoss[0m : 2.41140
[1mStep[0m  [66/339], [94mLoss[0m : 2.60075
[1mStep[0m  [99/339], [94mLoss[0m : 2.40558
[1mStep[0m  [132/339], [94mLoss[0m : 2.62373
[1mStep[0m  [165/339], [94mLoss[0m : 2.15274
[1mStep[0m  [198/339], [94mLoss[0m : 2.59764
[1mStep[0m  [231/339], [94mLoss[0m : 2.31367
[1mStep[0m  [264/339], [94mLoss[0m : 1.80626
[1mStep[0m  [297/339], [94mLoss[0m : 2.76878
[1mStep[0m  [330/339], [94mLoss[0m : 2.50816

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31282
[1mStep[0m  [33/339], [94mLoss[0m : 2.24768
[1mStep[0m  [66/339], [94mLoss[0m : 2.77330
[1mStep[0m  [99/339], [94mLoss[0m : 2.69114
[1mStep[0m  [132/339], [94mLoss[0m : 2.68359
[1mStep[0m  [165/339], [94mLoss[0m : 2.74925
[1mStep[0m  [198/339], [94mLoss[0m : 2.18750
[1mStep[0m  [231/339], [94mLoss[0m : 1.93529
[1mStep[0m  [264/339], [94mLoss[0m : 1.96085
[1mStep[0m  [297/339], [94mLoss[0m : 2.50874
[1mStep[0m  [330/339], [94mLoss[0m : 2.41402

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.337, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34341
[1mStep[0m  [33/339], [94mLoss[0m : 1.98138
[1mStep[0m  [66/339], [94mLoss[0m : 2.50985
[1mStep[0m  [99/339], [94mLoss[0m : 2.54799
[1mStep[0m  [132/339], [94mLoss[0m : 2.57817
[1mStep[0m  [165/339], [94mLoss[0m : 2.47514
[1mStep[0m  [198/339], [94mLoss[0m : 1.99439
[1mStep[0m  [231/339], [94mLoss[0m : 2.69581
[1mStep[0m  [264/339], [94mLoss[0m : 2.79443
[1mStep[0m  [297/339], [94mLoss[0m : 2.21046
[1mStep[0m  [330/339], [94mLoss[0m : 3.10997

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.339, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39037
[1mStep[0m  [33/339], [94mLoss[0m : 2.31291
[1mStep[0m  [66/339], [94mLoss[0m : 2.58305
[1mStep[0m  [99/339], [94mLoss[0m : 2.37922
[1mStep[0m  [132/339], [94mLoss[0m : 2.28831
[1mStep[0m  [165/339], [94mLoss[0m : 2.62857
[1mStep[0m  [198/339], [94mLoss[0m : 2.04917
[1mStep[0m  [231/339], [94mLoss[0m : 2.49269
[1mStep[0m  [264/339], [94mLoss[0m : 2.56173
[1mStep[0m  [297/339], [94mLoss[0m : 2.01023
[1mStep[0m  [330/339], [94mLoss[0m : 2.28460

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.299, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73888
[1mStep[0m  [33/339], [94mLoss[0m : 3.09359
[1mStep[0m  [66/339], [94mLoss[0m : 2.32487
[1mStep[0m  [99/339], [94mLoss[0m : 2.50756
[1mStep[0m  [132/339], [94mLoss[0m : 2.20041
[1mStep[0m  [165/339], [94mLoss[0m : 2.48467
[1mStep[0m  [198/339], [94mLoss[0m : 2.62542
[1mStep[0m  [231/339], [94mLoss[0m : 2.45343
[1mStep[0m  [264/339], [94mLoss[0m : 2.19131
[1mStep[0m  [297/339], [94mLoss[0m : 2.29821
[1mStep[0m  [330/339], [94mLoss[0m : 2.09541

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.291, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80512
[1mStep[0m  [33/339], [94mLoss[0m : 2.11223
[1mStep[0m  [66/339], [94mLoss[0m : 2.33875
[1mStep[0m  [99/339], [94mLoss[0m : 2.83136
[1mStep[0m  [132/339], [94mLoss[0m : 2.19184
[1mStep[0m  [165/339], [94mLoss[0m : 2.15355
[1mStep[0m  [198/339], [94mLoss[0m : 3.36358
[1mStep[0m  [231/339], [94mLoss[0m : 2.24374
[1mStep[0m  [264/339], [94mLoss[0m : 2.30257
[1mStep[0m  [297/339], [94mLoss[0m : 2.67528
[1mStep[0m  [330/339], [94mLoss[0m : 3.34285

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.307, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19739
[1mStep[0m  [33/339], [94mLoss[0m : 2.67660
[1mStep[0m  [66/339], [94mLoss[0m : 1.90059
[1mStep[0m  [99/339], [94mLoss[0m : 2.87543
[1mStep[0m  [132/339], [94mLoss[0m : 2.30934
[1mStep[0m  [165/339], [94mLoss[0m : 2.48638
[1mStep[0m  [198/339], [94mLoss[0m : 2.69716
[1mStep[0m  [231/339], [94mLoss[0m : 2.11835
[1mStep[0m  [264/339], [94mLoss[0m : 2.45205
[1mStep[0m  [297/339], [94mLoss[0m : 2.02300
[1mStep[0m  [330/339], [94mLoss[0m : 1.82873

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33130
[1mStep[0m  [33/339], [94mLoss[0m : 2.47275
[1mStep[0m  [66/339], [94mLoss[0m : 2.52413
[1mStep[0m  [99/339], [94mLoss[0m : 2.14877
[1mStep[0m  [132/339], [94mLoss[0m : 2.64038
[1mStep[0m  [165/339], [94mLoss[0m : 2.13291
[1mStep[0m  [198/339], [94mLoss[0m : 2.79798
[1mStep[0m  [231/339], [94mLoss[0m : 2.78676
[1mStep[0m  [264/339], [94mLoss[0m : 2.42491
[1mStep[0m  [297/339], [94mLoss[0m : 2.85339
[1mStep[0m  [330/339], [94mLoss[0m : 2.20316

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.321, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40199
[1mStep[0m  [33/339], [94mLoss[0m : 2.67746
[1mStep[0m  [66/339], [94mLoss[0m : 3.33177
[1mStep[0m  [99/339], [94mLoss[0m : 2.91643
[1mStep[0m  [132/339], [94mLoss[0m : 1.84325
[1mStep[0m  [165/339], [94mLoss[0m : 1.98963
[1mStep[0m  [198/339], [94mLoss[0m : 2.14723
[1mStep[0m  [231/339], [94mLoss[0m : 2.61490
[1mStep[0m  [264/339], [94mLoss[0m : 2.34062
[1mStep[0m  [297/339], [94mLoss[0m : 2.41927
[1mStep[0m  [330/339], [94mLoss[0m : 2.94354

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44492
[1mStep[0m  [33/339], [94mLoss[0m : 1.94990
[1mStep[0m  [66/339], [94mLoss[0m : 2.36847
[1mStep[0m  [99/339], [94mLoss[0m : 2.31821
[1mStep[0m  [132/339], [94mLoss[0m : 2.36010
[1mStep[0m  [165/339], [94mLoss[0m : 2.45039
[1mStep[0m  [198/339], [94mLoss[0m : 1.94197
[1mStep[0m  [231/339], [94mLoss[0m : 2.25413
[1mStep[0m  [264/339], [94mLoss[0m : 2.31358
[1mStep[0m  [297/339], [94mLoss[0m : 2.26314
[1mStep[0m  [330/339], [94mLoss[0m : 2.28788

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.349, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43074
[1mStep[0m  [33/339], [94mLoss[0m : 2.62666
[1mStep[0m  [66/339], [94mLoss[0m : 3.11400
[1mStep[0m  [99/339], [94mLoss[0m : 2.52692
[1mStep[0m  [132/339], [94mLoss[0m : 2.68068
[1mStep[0m  [165/339], [94mLoss[0m : 2.41293
[1mStep[0m  [198/339], [94mLoss[0m : 2.14192
[1mStep[0m  [231/339], [94mLoss[0m : 2.26217
[1mStep[0m  [264/339], [94mLoss[0m : 2.22012
[1mStep[0m  [297/339], [94mLoss[0m : 2.07227
[1mStep[0m  [330/339], [94mLoss[0m : 2.06819

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.309, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50581
[1mStep[0m  [33/339], [94mLoss[0m : 2.56053
[1mStep[0m  [66/339], [94mLoss[0m : 2.30554
[1mStep[0m  [99/339], [94mLoss[0m : 2.51906
[1mStep[0m  [132/339], [94mLoss[0m : 2.42370
[1mStep[0m  [165/339], [94mLoss[0m : 2.61812
[1mStep[0m  [198/339], [94mLoss[0m : 2.81661
[1mStep[0m  [231/339], [94mLoss[0m : 2.07920
[1mStep[0m  [264/339], [94mLoss[0m : 1.91907
[1mStep[0m  [297/339], [94mLoss[0m : 2.44170
[1mStep[0m  [330/339], [94mLoss[0m : 2.03291

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.336, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.290
====================================

Phase 1 - Evaluation MAE:  2.289976058808048
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.47169
[1mStep[0m  [33/339], [94mLoss[0m : 2.97756
[1mStep[0m  [66/339], [94mLoss[0m : 2.54533
[1mStep[0m  [99/339], [94mLoss[0m : 2.20708
[1mStep[0m  [132/339], [94mLoss[0m : 1.76818
[1mStep[0m  [165/339], [94mLoss[0m : 3.04289
[1mStep[0m  [198/339], [94mLoss[0m : 2.33960
[1mStep[0m  [231/339], [94mLoss[0m : 2.65805
[1mStep[0m  [264/339], [94mLoss[0m : 2.29644
[1mStep[0m  [297/339], [94mLoss[0m : 1.57037
[1mStep[0m  [330/339], [94mLoss[0m : 1.87836

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.290, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71087
[1mStep[0m  [33/339], [94mLoss[0m : 2.09029
[1mStep[0m  [66/339], [94mLoss[0m : 2.75942
[1mStep[0m  [99/339], [94mLoss[0m : 2.59018
[1mStep[0m  [132/339], [94mLoss[0m : 2.47787
[1mStep[0m  [165/339], [94mLoss[0m : 2.65925
[1mStep[0m  [198/339], [94mLoss[0m : 2.51530
[1mStep[0m  [231/339], [94mLoss[0m : 2.98874
[1mStep[0m  [264/339], [94mLoss[0m : 2.17912
[1mStep[0m  [297/339], [94mLoss[0m : 2.35280
[1mStep[0m  [330/339], [94mLoss[0m : 1.57826

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.422, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13776
[1mStep[0m  [33/339], [94mLoss[0m : 2.33741
[1mStep[0m  [66/339], [94mLoss[0m : 3.69312
[1mStep[0m  [99/339], [94mLoss[0m : 2.85442
[1mStep[0m  [132/339], [94mLoss[0m : 1.68588
[1mStep[0m  [165/339], [94mLoss[0m : 2.52273
[1mStep[0m  [198/339], [94mLoss[0m : 2.61723
[1mStep[0m  [231/339], [94mLoss[0m : 2.29677
[1mStep[0m  [264/339], [94mLoss[0m : 1.73839
[1mStep[0m  [297/339], [94mLoss[0m : 2.86458
[1mStep[0m  [330/339], [94mLoss[0m : 2.90553

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42926
[1mStep[0m  [33/339], [94mLoss[0m : 2.69578
[1mStep[0m  [66/339], [94mLoss[0m : 1.87254
[1mStep[0m  [99/339], [94mLoss[0m : 2.01559
[1mStep[0m  [132/339], [94mLoss[0m : 2.99167
[1mStep[0m  [165/339], [94mLoss[0m : 2.21050
[1mStep[0m  [198/339], [94mLoss[0m : 2.34711
[1mStep[0m  [231/339], [94mLoss[0m : 1.94499
[1mStep[0m  [264/339], [94mLoss[0m : 2.71992
[1mStep[0m  [297/339], [94mLoss[0m : 2.84592
[1mStep[0m  [330/339], [94mLoss[0m : 1.97018

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70727
[1mStep[0m  [33/339], [94mLoss[0m : 1.83888
[1mStep[0m  [66/339], [94mLoss[0m : 2.24781
[1mStep[0m  [99/339], [94mLoss[0m : 1.86886
[1mStep[0m  [132/339], [94mLoss[0m : 1.92593
[1mStep[0m  [165/339], [94mLoss[0m : 1.79337
[1mStep[0m  [198/339], [94mLoss[0m : 1.94747
[1mStep[0m  [231/339], [94mLoss[0m : 1.93606
[1mStep[0m  [264/339], [94mLoss[0m : 2.18094
[1mStep[0m  [297/339], [94mLoss[0m : 1.75431
[1mStep[0m  [330/339], [94mLoss[0m : 2.43231

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20858
[1mStep[0m  [33/339], [94mLoss[0m : 2.02280
[1mStep[0m  [66/339], [94mLoss[0m : 1.81506
[1mStep[0m  [99/339], [94mLoss[0m : 1.95632
[1mStep[0m  [132/339], [94mLoss[0m : 2.79997
[1mStep[0m  [165/339], [94mLoss[0m : 1.66730
[1mStep[0m  [198/339], [94mLoss[0m : 2.03732
[1mStep[0m  [231/339], [94mLoss[0m : 1.89895
[1mStep[0m  [264/339], [94mLoss[0m : 2.36735
[1mStep[0m  [297/339], [94mLoss[0m : 2.15646
[1mStep[0m  [330/339], [94mLoss[0m : 1.88406

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.412, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57545
[1mStep[0m  [33/339], [94mLoss[0m : 1.99491
[1mStep[0m  [66/339], [94mLoss[0m : 1.39192
[1mStep[0m  [99/339], [94mLoss[0m : 1.86686
[1mStep[0m  [132/339], [94mLoss[0m : 2.17681
[1mStep[0m  [165/339], [94mLoss[0m : 2.27192
[1mStep[0m  [198/339], [94mLoss[0m : 2.55856
[1mStep[0m  [231/339], [94mLoss[0m : 2.01444
[1mStep[0m  [264/339], [94mLoss[0m : 3.09778
[1mStep[0m  [297/339], [94mLoss[0m : 1.82028
[1mStep[0m  [330/339], [94mLoss[0m : 2.05654

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.388, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31487
[1mStep[0m  [33/339], [94mLoss[0m : 2.21033
[1mStep[0m  [66/339], [94mLoss[0m : 2.31197
[1mStep[0m  [99/339], [94mLoss[0m : 2.34919
[1mStep[0m  [132/339], [94mLoss[0m : 2.27078
[1mStep[0m  [165/339], [94mLoss[0m : 1.93940
[1mStep[0m  [198/339], [94mLoss[0m : 1.81150
[1mStep[0m  [231/339], [94mLoss[0m : 1.45689
[1mStep[0m  [264/339], [94mLoss[0m : 2.34329
[1mStep[0m  [297/339], [94mLoss[0m : 1.88665
[1mStep[0m  [330/339], [94mLoss[0m : 2.17139

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.425, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48802
[1mStep[0m  [33/339], [94mLoss[0m : 2.25145
[1mStep[0m  [66/339], [94mLoss[0m : 1.91475
[1mStep[0m  [99/339], [94mLoss[0m : 2.28450
[1mStep[0m  [132/339], [94mLoss[0m : 2.45873
[1mStep[0m  [165/339], [94mLoss[0m : 2.08035
[1mStep[0m  [198/339], [94mLoss[0m : 1.68178
[1mStep[0m  [231/339], [94mLoss[0m : 1.87210
[1mStep[0m  [264/339], [94mLoss[0m : 1.98315
[1mStep[0m  [297/339], [94mLoss[0m : 1.74535
[1mStep[0m  [330/339], [94mLoss[0m : 2.35565

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.455, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86383
[1mStep[0m  [33/339], [94mLoss[0m : 2.00643
[1mStep[0m  [66/339], [94mLoss[0m : 2.13273
[1mStep[0m  [99/339], [94mLoss[0m : 1.94644
[1mStep[0m  [132/339], [94mLoss[0m : 1.84530
[1mStep[0m  [165/339], [94mLoss[0m : 1.64907
[1mStep[0m  [198/339], [94mLoss[0m : 1.64643
[1mStep[0m  [231/339], [94mLoss[0m : 2.20413
[1mStep[0m  [264/339], [94mLoss[0m : 2.05549
[1mStep[0m  [297/339], [94mLoss[0m : 1.79752
[1mStep[0m  [330/339], [94mLoss[0m : 1.78811

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.430, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58989
[1mStep[0m  [33/339], [94mLoss[0m : 1.66005
[1mStep[0m  [66/339], [94mLoss[0m : 1.46525
[1mStep[0m  [99/339], [94mLoss[0m : 2.37110
[1mStep[0m  [132/339], [94mLoss[0m : 1.79906
[1mStep[0m  [165/339], [94mLoss[0m : 1.94894
[1mStep[0m  [198/339], [94mLoss[0m : 1.94776
[1mStep[0m  [231/339], [94mLoss[0m : 1.65713
[1mStep[0m  [264/339], [94mLoss[0m : 2.17417
[1mStep[0m  [297/339], [94mLoss[0m : 1.73423
[1mStep[0m  [330/339], [94mLoss[0m : 1.73833

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.474, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25106
[1mStep[0m  [33/339], [94mLoss[0m : 1.87919
[1mStep[0m  [66/339], [94mLoss[0m : 1.98317
[1mStep[0m  [99/339], [94mLoss[0m : 2.01357
[1mStep[0m  [132/339], [94mLoss[0m : 1.67573
[1mStep[0m  [165/339], [94mLoss[0m : 1.59350
[1mStep[0m  [198/339], [94mLoss[0m : 1.90846
[1mStep[0m  [231/339], [94mLoss[0m : 1.98226
[1mStep[0m  [264/339], [94mLoss[0m : 1.68541
[1mStep[0m  [297/339], [94mLoss[0m : 1.30630
[1mStep[0m  [330/339], [94mLoss[0m : 2.00376

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.924, [92mTest[0m: 2.522, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70178
[1mStep[0m  [33/339], [94mLoss[0m : 2.16278
[1mStep[0m  [66/339], [94mLoss[0m : 2.40983
[1mStep[0m  [99/339], [94mLoss[0m : 1.61869
[1mStep[0m  [132/339], [94mLoss[0m : 2.21126
[1mStep[0m  [165/339], [94mLoss[0m : 1.67247
[1mStep[0m  [198/339], [94mLoss[0m : 2.24759
[1mStep[0m  [231/339], [94mLoss[0m : 2.09442
[1mStep[0m  [264/339], [94mLoss[0m : 2.29318
[1mStep[0m  [297/339], [94mLoss[0m : 1.76081
[1mStep[0m  [330/339], [94mLoss[0m : 2.05360

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.471, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74888
[1mStep[0m  [33/339], [94mLoss[0m : 2.12000
[1mStep[0m  [66/339], [94mLoss[0m : 1.65783
[1mStep[0m  [99/339], [94mLoss[0m : 1.73911
[1mStep[0m  [132/339], [94mLoss[0m : 1.78886
[1mStep[0m  [165/339], [94mLoss[0m : 1.86489
[1mStep[0m  [198/339], [94mLoss[0m : 1.74061
[1mStep[0m  [231/339], [94mLoss[0m : 2.23301
[1mStep[0m  [264/339], [94mLoss[0m : 1.80224
[1mStep[0m  [297/339], [94mLoss[0m : 1.40444
[1mStep[0m  [330/339], [94mLoss[0m : 1.90022

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.490, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.36727
[1mStep[0m  [33/339], [94mLoss[0m : 1.99952
[1mStep[0m  [66/339], [94mLoss[0m : 1.52236
[1mStep[0m  [99/339], [94mLoss[0m : 1.99739
[1mStep[0m  [132/339], [94mLoss[0m : 1.82996
[1mStep[0m  [165/339], [94mLoss[0m : 1.98110
[1mStep[0m  [198/339], [94mLoss[0m : 2.07963
[1mStep[0m  [231/339], [94mLoss[0m : 1.76772
[1mStep[0m  [264/339], [94mLoss[0m : 1.83442
[1mStep[0m  [297/339], [94mLoss[0m : 1.44867
[1mStep[0m  [330/339], [94mLoss[0m : 1.52587

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.467, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11613
[1mStep[0m  [33/339], [94mLoss[0m : 1.38904
[1mStep[0m  [66/339], [94mLoss[0m : 1.49106
[1mStep[0m  [99/339], [94mLoss[0m : 1.41254
[1mStep[0m  [132/339], [94mLoss[0m : 1.38875
[1mStep[0m  [165/339], [94mLoss[0m : 1.55066
[1mStep[0m  [198/339], [94mLoss[0m : 1.51301
[1mStep[0m  [231/339], [94mLoss[0m : 1.90634
[1mStep[0m  [264/339], [94mLoss[0m : 2.49197
[1mStep[0m  [297/339], [94mLoss[0m : 1.90385
[1mStep[0m  [330/339], [94mLoss[0m : 1.63226

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.472, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43020
[1mStep[0m  [33/339], [94mLoss[0m : 2.03038
[1mStep[0m  [66/339], [94mLoss[0m : 1.59736
[1mStep[0m  [99/339], [94mLoss[0m : 2.21647
[1mStep[0m  [132/339], [94mLoss[0m : 2.34783
[1mStep[0m  [165/339], [94mLoss[0m : 1.77582
[1mStep[0m  [198/339], [94mLoss[0m : 2.37175
[1mStep[0m  [231/339], [94mLoss[0m : 1.76385
[1mStep[0m  [264/339], [94mLoss[0m : 1.62911
[1mStep[0m  [297/339], [94mLoss[0m : 1.58335
[1mStep[0m  [330/339], [94mLoss[0m : 1.91480

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.470, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39285
[1mStep[0m  [33/339], [94mLoss[0m : 1.39305
[1mStep[0m  [66/339], [94mLoss[0m : 1.47542
[1mStep[0m  [99/339], [94mLoss[0m : 1.44724
[1mStep[0m  [132/339], [94mLoss[0m : 1.59560
[1mStep[0m  [165/339], [94mLoss[0m : 1.57570
[1mStep[0m  [198/339], [94mLoss[0m : 1.39852
[1mStep[0m  [231/339], [94mLoss[0m : 1.63303
[1mStep[0m  [264/339], [94mLoss[0m : 1.89516
[1mStep[0m  [297/339], [94mLoss[0m : 1.77445
[1mStep[0m  [330/339], [94mLoss[0m : 2.66320

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.481, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09471
[1mStep[0m  [33/339], [94mLoss[0m : 1.94702
[1mStep[0m  [66/339], [94mLoss[0m : 1.34844
[1mStep[0m  [99/339], [94mLoss[0m : 1.82658
[1mStep[0m  [132/339], [94mLoss[0m : 2.20788
[1mStep[0m  [165/339], [94mLoss[0m : 1.96717
[1mStep[0m  [198/339], [94mLoss[0m : 1.81233
[1mStep[0m  [231/339], [94mLoss[0m : 1.77151
[1mStep[0m  [264/339], [94mLoss[0m : 1.66983
[1mStep[0m  [297/339], [94mLoss[0m : 1.64310
[1mStep[0m  [330/339], [94mLoss[0m : 1.53816

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.772, [92mTest[0m: 2.448, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63532
[1mStep[0m  [33/339], [94mLoss[0m : 2.03991
[1mStep[0m  [66/339], [94mLoss[0m : 1.11628
[1mStep[0m  [99/339], [94mLoss[0m : 1.74478
[1mStep[0m  [132/339], [94mLoss[0m : 1.76506
[1mStep[0m  [165/339], [94mLoss[0m : 1.76404
[1mStep[0m  [198/339], [94mLoss[0m : 1.56743
[1mStep[0m  [231/339], [94mLoss[0m : 1.40840
[1mStep[0m  [264/339], [94mLoss[0m : 1.62773
[1mStep[0m  [297/339], [94mLoss[0m : 2.00705
[1mStep[0m  [330/339], [94mLoss[0m : 1.33736

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.508, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68187
[1mStep[0m  [33/339], [94mLoss[0m : 2.15647
[1mStep[0m  [66/339], [94mLoss[0m : 1.41792
[1mStep[0m  [99/339], [94mLoss[0m : 1.33826
[1mStep[0m  [132/339], [94mLoss[0m : 1.85753
[1mStep[0m  [165/339], [94mLoss[0m : 1.36825
[1mStep[0m  [198/339], [94mLoss[0m : 1.76127
[1mStep[0m  [231/339], [94mLoss[0m : 1.49142
[1mStep[0m  [264/339], [94mLoss[0m : 1.71630
[1mStep[0m  [297/339], [94mLoss[0m : 1.87479
[1mStep[0m  [330/339], [94mLoss[0m : 2.29627

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.521, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26407
[1mStep[0m  [33/339], [94mLoss[0m : 1.71874
[1mStep[0m  [66/339], [94mLoss[0m : 1.78916
[1mStep[0m  [99/339], [94mLoss[0m : 1.56974
[1mStep[0m  [132/339], [94mLoss[0m : 1.45126
[1mStep[0m  [165/339], [94mLoss[0m : 2.21008
[1mStep[0m  [198/339], [94mLoss[0m : 1.84038
[1mStep[0m  [231/339], [94mLoss[0m : 2.03105
[1mStep[0m  [264/339], [94mLoss[0m : 1.67086
[1mStep[0m  [297/339], [94mLoss[0m : 2.09334
[1mStep[0m  [330/339], [94mLoss[0m : 1.90877

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.500, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51343
[1mStep[0m  [33/339], [94mLoss[0m : 1.37086
[1mStep[0m  [66/339], [94mLoss[0m : 1.22322
[1mStep[0m  [99/339], [94mLoss[0m : 1.87316
[1mStep[0m  [132/339], [94mLoss[0m : 1.81541
[1mStep[0m  [165/339], [94mLoss[0m : 1.42547
[1mStep[0m  [198/339], [94mLoss[0m : 1.84923
[1mStep[0m  [231/339], [94mLoss[0m : 1.30948
[1mStep[0m  [264/339], [94mLoss[0m : 1.61838
[1mStep[0m  [297/339], [94mLoss[0m : 1.71076
[1mStep[0m  [330/339], [94mLoss[0m : 1.72021

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.545, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60366
[1mStep[0m  [33/339], [94mLoss[0m : 1.37419
[1mStep[0m  [66/339], [94mLoss[0m : 1.93349
[1mStep[0m  [99/339], [94mLoss[0m : 1.65526
[1mStep[0m  [132/339], [94mLoss[0m : 1.87731
[1mStep[0m  [165/339], [94mLoss[0m : 1.79850
[1mStep[0m  [198/339], [94mLoss[0m : 2.05100
[1mStep[0m  [231/339], [94mLoss[0m : 1.90782
[1mStep[0m  [264/339], [94mLoss[0m : 1.80582
[1mStep[0m  [297/339], [94mLoss[0m : 1.75720
[1mStep[0m  [330/339], [94mLoss[0m : 1.89687

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.530, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41668
[1mStep[0m  [33/339], [94mLoss[0m : 1.51975
[1mStep[0m  [66/339], [94mLoss[0m : 1.75521
[1mStep[0m  [99/339], [94mLoss[0m : 1.19562
[1mStep[0m  [132/339], [94mLoss[0m : 1.56899
[1mStep[0m  [165/339], [94mLoss[0m : 1.63414
[1mStep[0m  [198/339], [94mLoss[0m : 1.73503
[1mStep[0m  [231/339], [94mLoss[0m : 1.59357
[1mStep[0m  [264/339], [94mLoss[0m : 1.86475
[1mStep[0m  [297/339], [94mLoss[0m : 1.46155
[1mStep[0m  [330/339], [94mLoss[0m : 1.40987

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.461, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98063
[1mStep[0m  [33/339], [94mLoss[0m : 1.34887
[1mStep[0m  [66/339], [94mLoss[0m : 1.55229
[1mStep[0m  [99/339], [94mLoss[0m : 2.06058
[1mStep[0m  [132/339], [94mLoss[0m : 1.49406
[1mStep[0m  [165/339], [94mLoss[0m : 1.52309
[1mStep[0m  [198/339], [94mLoss[0m : 1.90599
[1mStep[0m  [231/339], [94mLoss[0m : 1.45409
[1mStep[0m  [264/339], [94mLoss[0m : 1.87098
[1mStep[0m  [297/339], [94mLoss[0m : 1.91750
[1mStep[0m  [330/339], [94mLoss[0m : 1.28380

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.490, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56417
[1mStep[0m  [33/339], [94mLoss[0m : 1.69428
[1mStep[0m  [66/339], [94mLoss[0m : 1.75318
[1mStep[0m  [99/339], [94mLoss[0m : 1.64017
[1mStep[0m  [132/339], [94mLoss[0m : 1.02708
[1mStep[0m  [165/339], [94mLoss[0m : 1.37497
[1mStep[0m  [198/339], [94mLoss[0m : 1.25821
[1mStep[0m  [231/339], [94mLoss[0m : 1.67056
[1mStep[0m  [264/339], [94mLoss[0m : 1.65954
[1mStep[0m  [297/339], [94mLoss[0m : 2.03713
[1mStep[0m  [330/339], [94mLoss[0m : 1.82162

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.462, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93774
[1mStep[0m  [33/339], [94mLoss[0m : 1.19120
[1mStep[0m  [66/339], [94mLoss[0m : 1.81386
[1mStep[0m  [99/339], [94mLoss[0m : 1.45865
[1mStep[0m  [132/339], [94mLoss[0m : 1.60889
[1mStep[0m  [165/339], [94mLoss[0m : 1.68304
[1mStep[0m  [198/339], [94mLoss[0m : 1.78940
[1mStep[0m  [231/339], [94mLoss[0m : 1.62330
[1mStep[0m  [264/339], [94mLoss[0m : 1.71711
[1mStep[0m  [297/339], [94mLoss[0m : 1.59909
[1mStep[0m  [330/339], [94mLoss[0m : 1.88493

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.482, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87281
[1mStep[0m  [33/339], [94mLoss[0m : 1.66932
[1mStep[0m  [66/339], [94mLoss[0m : 1.99848
[1mStep[0m  [99/339], [94mLoss[0m : 1.99286
[1mStep[0m  [132/339], [94mLoss[0m : 1.33240
[1mStep[0m  [165/339], [94mLoss[0m : 1.52323
[1mStep[0m  [198/339], [94mLoss[0m : 2.14551
[1mStep[0m  [231/339], [94mLoss[0m : 1.48211
[1mStep[0m  [264/339], [94mLoss[0m : 1.29646
[1mStep[0m  [297/339], [94mLoss[0m : 1.82259
[1mStep[0m  [330/339], [94mLoss[0m : 1.77979

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.529, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.18737
[1mStep[0m  [33/339], [94mLoss[0m : 2.23696
[1mStep[0m  [66/339], [94mLoss[0m : 1.79390
[1mStep[0m  [99/339], [94mLoss[0m : 1.21190
[1mStep[0m  [132/339], [94mLoss[0m : 1.58136
[1mStep[0m  [165/339], [94mLoss[0m : 1.55210
[1mStep[0m  [198/339], [94mLoss[0m : 1.69546
[1mStep[0m  [231/339], [94mLoss[0m : 2.23891
[1mStep[0m  [264/339], [94mLoss[0m : 1.54681
[1mStep[0m  [297/339], [94mLoss[0m : 1.56939
[1mStep[0m  [330/339], [94mLoss[0m : 1.97205

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.473, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.501
====================================

Phase 2 - Evaluation MAE:  2.500900651501343
MAE score P1      2.289976
MAE score P2      2.500901
loss              1.611925
learning_rate      0.00505
batch_size              32
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay          0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.72960
[1mStep[0m  [2/21], [94mLoss[0m : 10.21285
[1mStep[0m  [4/21], [94mLoss[0m : 10.09087
[1mStep[0m  [6/21], [94mLoss[0m : 10.24925
[1mStep[0m  [8/21], [94mLoss[0m : 9.77388
[1mStep[0m  [10/21], [94mLoss[0m : 9.78744
[1mStep[0m  [12/21], [94mLoss[0m : 9.85979
[1mStep[0m  [14/21], [94mLoss[0m : 9.42354
[1mStep[0m  [16/21], [94mLoss[0m : 9.35308
[1mStep[0m  [18/21], [94mLoss[0m : 9.34556
[1mStep[0m  [20/21], [94mLoss[0m : 9.45005

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.853, [92mTest[0m: 10.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.97854
[1mStep[0m  [2/21], [94mLoss[0m : 9.02511
[1mStep[0m  [4/21], [94mLoss[0m : 8.86818
[1mStep[0m  [6/21], [94mLoss[0m : 8.48878
[1mStep[0m  [8/21], [94mLoss[0m : 8.54182
[1mStep[0m  [10/21], [94mLoss[0m : 8.42950
[1mStep[0m  [12/21], [94mLoss[0m : 8.18226
[1mStep[0m  [14/21], [94mLoss[0m : 8.11347
[1mStep[0m  [16/21], [94mLoss[0m : 7.92619
[1mStep[0m  [18/21], [94mLoss[0m : 7.73331
[1mStep[0m  [20/21], [94mLoss[0m : 7.96016

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.393, [92mTest[0m: 9.095, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.04110
[1mStep[0m  [2/21], [94mLoss[0m : 7.36242
[1mStep[0m  [4/21], [94mLoss[0m : 7.31848
[1mStep[0m  [6/21], [94mLoss[0m : 7.25968
[1mStep[0m  [8/21], [94mLoss[0m : 7.05235
[1mStep[0m  [10/21], [94mLoss[0m : 7.39413
[1mStep[0m  [12/21], [94mLoss[0m : 6.90576
[1mStep[0m  [14/21], [94mLoss[0m : 6.38896
[1mStep[0m  [16/21], [94mLoss[0m : 6.14121
[1mStep[0m  [18/21], [94mLoss[0m : 6.34896
[1mStep[0m  [20/21], [94mLoss[0m : 6.02942

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.938, [92mTest[0m: 7.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.17214
[1mStep[0m  [2/21], [94mLoss[0m : 5.64965
[1mStep[0m  [4/21], [94mLoss[0m : 5.79320
[1mStep[0m  [6/21], [94mLoss[0m : 5.72469
[1mStep[0m  [8/21], [94mLoss[0m : 5.59114
[1mStep[0m  [10/21], [94mLoss[0m : 5.61081
[1mStep[0m  [12/21], [94mLoss[0m : 5.35376
[1mStep[0m  [14/21], [94mLoss[0m : 5.12237
[1mStep[0m  [16/21], [94mLoss[0m : 5.28443
[1mStep[0m  [18/21], [94mLoss[0m : 5.02277
[1mStep[0m  [20/21], [94mLoss[0m : 5.02751

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.501, [92mTest[0m: 6.180, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.83220
[1mStep[0m  [2/21], [94mLoss[0m : 4.47536
[1mStep[0m  [4/21], [94mLoss[0m : 4.49854
[1mStep[0m  [6/21], [94mLoss[0m : 4.64591
[1mStep[0m  [8/21], [94mLoss[0m : 4.36054
[1mStep[0m  [10/21], [94mLoss[0m : 4.30683
[1mStep[0m  [12/21], [94mLoss[0m : 4.32829
[1mStep[0m  [14/21], [94mLoss[0m : 4.14091
[1mStep[0m  [16/21], [94mLoss[0m : 4.25575
[1mStep[0m  [18/21], [94mLoss[0m : 4.09504
[1mStep[0m  [20/21], [94mLoss[0m : 3.94997

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.392, [92mTest[0m: 4.843, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.22420
[1mStep[0m  [2/21], [94mLoss[0m : 4.03886
[1mStep[0m  [4/21], [94mLoss[0m : 4.02874
[1mStep[0m  [6/21], [94mLoss[0m : 3.50137
[1mStep[0m  [8/21], [94mLoss[0m : 3.69320
[1mStep[0m  [10/21], [94mLoss[0m : 3.51247
[1mStep[0m  [12/21], [94mLoss[0m : 3.53045
[1mStep[0m  [14/21], [94mLoss[0m : 3.74762
[1mStep[0m  [16/21], [94mLoss[0m : 3.41663
[1mStep[0m  [18/21], [94mLoss[0m : 3.46132
[1mStep[0m  [20/21], [94mLoss[0m : 3.42538

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.674, [92mTest[0m: 3.893, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.43753
[1mStep[0m  [2/21], [94mLoss[0m : 3.43263
[1mStep[0m  [4/21], [94mLoss[0m : 3.53571
[1mStep[0m  [6/21], [94mLoss[0m : 3.08073
[1mStep[0m  [8/21], [94mLoss[0m : 3.03633
[1mStep[0m  [10/21], [94mLoss[0m : 3.08179
[1mStep[0m  [12/21], [94mLoss[0m : 3.27414
[1mStep[0m  [14/21], [94mLoss[0m : 3.08769
[1mStep[0m  [16/21], [94mLoss[0m : 3.24578
[1mStep[0m  [18/21], [94mLoss[0m : 3.14443
[1mStep[0m  [20/21], [94mLoss[0m : 3.14422

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.239, [92mTest[0m: 3.288, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.39344
[1mStep[0m  [2/21], [94mLoss[0m : 3.20567
[1mStep[0m  [4/21], [94mLoss[0m : 3.03174
[1mStep[0m  [6/21], [94mLoss[0m : 3.20258
[1mStep[0m  [8/21], [94mLoss[0m : 2.78544
[1mStep[0m  [10/21], [94mLoss[0m : 3.06939
[1mStep[0m  [12/21], [94mLoss[0m : 2.84240
[1mStep[0m  [14/21], [94mLoss[0m : 3.08942
[1mStep[0m  [16/21], [94mLoss[0m : 2.79394
[1mStep[0m  [18/21], [94mLoss[0m : 2.91231
[1mStep[0m  [20/21], [94mLoss[0m : 2.73530

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.984, [92mTest[0m: 2.918, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.86717
[1mStep[0m  [2/21], [94mLoss[0m : 3.16483
[1mStep[0m  [4/21], [94mLoss[0m : 2.67586
[1mStep[0m  [6/21], [94mLoss[0m : 2.83483
[1mStep[0m  [8/21], [94mLoss[0m : 2.86496
[1mStep[0m  [10/21], [94mLoss[0m : 2.79999
[1mStep[0m  [12/21], [94mLoss[0m : 2.82155
[1mStep[0m  [14/21], [94mLoss[0m : 2.74899
[1mStep[0m  [16/21], [94mLoss[0m : 2.79255
[1mStep[0m  [18/21], [94mLoss[0m : 2.85434
[1mStep[0m  [20/21], [94mLoss[0m : 2.74760

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.834, [92mTest[0m: 2.676, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72248
[1mStep[0m  [2/21], [94mLoss[0m : 2.65848
[1mStep[0m  [4/21], [94mLoss[0m : 2.56601
[1mStep[0m  [6/21], [94mLoss[0m : 2.74840
[1mStep[0m  [8/21], [94mLoss[0m : 2.62491
[1mStep[0m  [10/21], [94mLoss[0m : 2.67999
[1mStep[0m  [12/21], [94mLoss[0m : 2.63827
[1mStep[0m  [14/21], [94mLoss[0m : 2.68789
[1mStep[0m  [16/21], [94mLoss[0m : 2.66574
[1mStep[0m  [18/21], [94mLoss[0m : 2.75969
[1mStep[0m  [20/21], [94mLoss[0m : 2.63801

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.547, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61185
[1mStep[0m  [2/21], [94mLoss[0m : 2.84293
[1mStep[0m  [4/21], [94mLoss[0m : 2.54783
[1mStep[0m  [6/21], [94mLoss[0m : 2.81784
[1mStep[0m  [8/21], [94mLoss[0m : 2.67746
[1mStep[0m  [10/21], [94mLoss[0m : 2.75129
[1mStep[0m  [12/21], [94mLoss[0m : 2.89676
[1mStep[0m  [14/21], [94mLoss[0m : 2.65476
[1mStep[0m  [16/21], [94mLoss[0m : 2.66189
[1mStep[0m  [18/21], [94mLoss[0m : 2.70748
[1mStep[0m  [20/21], [94mLoss[0m : 2.72593

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77672
[1mStep[0m  [2/21], [94mLoss[0m : 2.41010
[1mStep[0m  [4/21], [94mLoss[0m : 2.76045
[1mStep[0m  [6/21], [94mLoss[0m : 2.73418
[1mStep[0m  [8/21], [94mLoss[0m : 2.56644
[1mStep[0m  [10/21], [94mLoss[0m : 2.46496
[1mStep[0m  [12/21], [94mLoss[0m : 2.64798
[1mStep[0m  [14/21], [94mLoss[0m : 2.68063
[1mStep[0m  [16/21], [94mLoss[0m : 2.77122
[1mStep[0m  [18/21], [94mLoss[0m : 2.82565
[1mStep[0m  [20/21], [94mLoss[0m : 2.57072

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52263
[1mStep[0m  [2/21], [94mLoss[0m : 2.74987
[1mStep[0m  [4/21], [94mLoss[0m : 2.77799
[1mStep[0m  [6/21], [94mLoss[0m : 2.75049
[1mStep[0m  [8/21], [94mLoss[0m : 2.66221
[1mStep[0m  [10/21], [94mLoss[0m : 2.71274
[1mStep[0m  [12/21], [94mLoss[0m : 2.70111
[1mStep[0m  [14/21], [94mLoss[0m : 2.63609
[1mStep[0m  [16/21], [94mLoss[0m : 2.57418
[1mStep[0m  [18/21], [94mLoss[0m : 2.51634
[1mStep[0m  [20/21], [94mLoss[0m : 2.69796

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56788
[1mStep[0m  [2/21], [94mLoss[0m : 2.50984
[1mStep[0m  [4/21], [94mLoss[0m : 2.54945
[1mStep[0m  [6/21], [94mLoss[0m : 2.74503
[1mStep[0m  [8/21], [94mLoss[0m : 2.57280
[1mStep[0m  [10/21], [94mLoss[0m : 2.58867
[1mStep[0m  [12/21], [94mLoss[0m : 2.72866
[1mStep[0m  [14/21], [94mLoss[0m : 2.71819
[1mStep[0m  [16/21], [94mLoss[0m : 2.64955
[1mStep[0m  [18/21], [94mLoss[0m : 2.73818
[1mStep[0m  [20/21], [94mLoss[0m : 2.62276

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67028
[1mStep[0m  [2/21], [94mLoss[0m : 2.50602
[1mStep[0m  [4/21], [94mLoss[0m : 2.67051
[1mStep[0m  [6/21], [94mLoss[0m : 2.52267
[1mStep[0m  [8/21], [94mLoss[0m : 2.53405
[1mStep[0m  [10/21], [94mLoss[0m : 2.79955
[1mStep[0m  [12/21], [94mLoss[0m : 2.85342
[1mStep[0m  [14/21], [94mLoss[0m : 2.58235
[1mStep[0m  [16/21], [94mLoss[0m : 2.67014
[1mStep[0m  [18/21], [94mLoss[0m : 2.66701
[1mStep[0m  [20/21], [94mLoss[0m : 2.53385

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66205
[1mStep[0m  [2/21], [94mLoss[0m : 2.57332
[1mStep[0m  [4/21], [94mLoss[0m : 2.41058
[1mStep[0m  [6/21], [94mLoss[0m : 2.51402
[1mStep[0m  [8/21], [94mLoss[0m : 2.71881
[1mStep[0m  [10/21], [94mLoss[0m : 2.45176
[1mStep[0m  [12/21], [94mLoss[0m : 2.58749
[1mStep[0m  [14/21], [94mLoss[0m : 2.74842
[1mStep[0m  [16/21], [94mLoss[0m : 2.76434
[1mStep[0m  [18/21], [94mLoss[0m : 2.71228
[1mStep[0m  [20/21], [94mLoss[0m : 2.51002

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46578
[1mStep[0m  [2/21], [94mLoss[0m : 2.53737
[1mStep[0m  [4/21], [94mLoss[0m : 2.52550
[1mStep[0m  [6/21], [94mLoss[0m : 2.68964
[1mStep[0m  [8/21], [94mLoss[0m : 2.67196
[1mStep[0m  [10/21], [94mLoss[0m : 2.58504
[1mStep[0m  [12/21], [94mLoss[0m : 2.62771
[1mStep[0m  [14/21], [94mLoss[0m : 2.73028
[1mStep[0m  [16/21], [94mLoss[0m : 2.52414
[1mStep[0m  [18/21], [94mLoss[0m : 2.53913
[1mStep[0m  [20/21], [94mLoss[0m : 2.61454

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.75111
[1mStep[0m  [2/21], [94mLoss[0m : 2.41218
[1mStep[0m  [4/21], [94mLoss[0m : 2.63664
[1mStep[0m  [6/21], [94mLoss[0m : 2.54898
[1mStep[0m  [8/21], [94mLoss[0m : 2.58466
[1mStep[0m  [10/21], [94mLoss[0m : 2.63670
[1mStep[0m  [12/21], [94mLoss[0m : 2.53439
[1mStep[0m  [14/21], [94mLoss[0m : 2.64487
[1mStep[0m  [16/21], [94mLoss[0m : 2.62918
[1mStep[0m  [18/21], [94mLoss[0m : 2.50024
[1mStep[0m  [20/21], [94mLoss[0m : 2.63492

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84123
[1mStep[0m  [2/21], [94mLoss[0m : 2.49547
[1mStep[0m  [4/21], [94mLoss[0m : 2.65820
[1mStep[0m  [6/21], [94mLoss[0m : 2.66340
[1mStep[0m  [8/21], [94mLoss[0m : 2.59292
[1mStep[0m  [10/21], [94mLoss[0m : 2.68455
[1mStep[0m  [12/21], [94mLoss[0m : 2.54985
[1mStep[0m  [14/21], [94mLoss[0m : 2.54387
[1mStep[0m  [16/21], [94mLoss[0m : 2.66478
[1mStep[0m  [18/21], [94mLoss[0m : 2.42836
[1mStep[0m  [20/21], [94mLoss[0m : 2.62173

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70144
[1mStep[0m  [2/21], [94mLoss[0m : 2.51787
[1mStep[0m  [4/21], [94mLoss[0m : 2.46498
[1mStep[0m  [6/21], [94mLoss[0m : 2.68758
[1mStep[0m  [8/21], [94mLoss[0m : 2.70133
[1mStep[0m  [10/21], [94mLoss[0m : 2.52164
[1mStep[0m  [12/21], [94mLoss[0m : 2.43893
[1mStep[0m  [14/21], [94mLoss[0m : 2.53686
[1mStep[0m  [16/21], [94mLoss[0m : 2.66323
[1mStep[0m  [18/21], [94mLoss[0m : 2.70875
[1mStep[0m  [20/21], [94mLoss[0m : 2.63066

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.48282
[1mStep[0m  [2/21], [94mLoss[0m : 2.66226
[1mStep[0m  [4/21], [94mLoss[0m : 2.63500
[1mStep[0m  [6/21], [94mLoss[0m : 2.55994
[1mStep[0m  [8/21], [94mLoss[0m : 2.61461
[1mStep[0m  [10/21], [94mLoss[0m : 2.64083
[1mStep[0m  [12/21], [94mLoss[0m : 2.56648
[1mStep[0m  [14/21], [94mLoss[0m : 2.55496
[1mStep[0m  [16/21], [94mLoss[0m : 2.58177
[1mStep[0m  [18/21], [94mLoss[0m : 2.71723
[1mStep[0m  [20/21], [94mLoss[0m : 2.58341

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61044
[1mStep[0m  [2/21], [94mLoss[0m : 2.61499
[1mStep[0m  [4/21], [94mLoss[0m : 2.58173
[1mStep[0m  [6/21], [94mLoss[0m : 2.59936
[1mStep[0m  [8/21], [94mLoss[0m : 2.68878
[1mStep[0m  [10/21], [94mLoss[0m : 2.55013
[1mStep[0m  [12/21], [94mLoss[0m : 2.64229
[1mStep[0m  [14/21], [94mLoss[0m : 2.48029
[1mStep[0m  [16/21], [94mLoss[0m : 2.68103
[1mStep[0m  [18/21], [94mLoss[0m : 2.67828
[1mStep[0m  [20/21], [94mLoss[0m : 2.69015

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.398, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49615
[1mStep[0m  [2/21], [94mLoss[0m : 2.52742
[1mStep[0m  [4/21], [94mLoss[0m : 2.54093
[1mStep[0m  [6/21], [94mLoss[0m : 2.62832
[1mStep[0m  [8/21], [94mLoss[0m : 2.63866
[1mStep[0m  [10/21], [94mLoss[0m : 2.54538
[1mStep[0m  [12/21], [94mLoss[0m : 2.42906
[1mStep[0m  [14/21], [94mLoss[0m : 2.55844
[1mStep[0m  [16/21], [94mLoss[0m : 2.72923
[1mStep[0m  [18/21], [94mLoss[0m : 2.63299
[1mStep[0m  [20/21], [94mLoss[0m : 2.51498

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.384, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52514
[1mStep[0m  [2/21], [94mLoss[0m : 2.64207
[1mStep[0m  [4/21], [94mLoss[0m : 2.46647
[1mStep[0m  [6/21], [94mLoss[0m : 2.65113
[1mStep[0m  [8/21], [94mLoss[0m : 2.47092
[1mStep[0m  [10/21], [94mLoss[0m : 2.66686
[1mStep[0m  [12/21], [94mLoss[0m : 2.54037
[1mStep[0m  [14/21], [94mLoss[0m : 2.66702
[1mStep[0m  [16/21], [94mLoss[0m : 2.79413
[1mStep[0m  [18/21], [94mLoss[0m : 2.60815
[1mStep[0m  [20/21], [94mLoss[0m : 2.53966

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.57662
[1mStep[0m  [2/21], [94mLoss[0m : 2.50290
[1mStep[0m  [4/21], [94mLoss[0m : 2.71919
[1mStep[0m  [6/21], [94mLoss[0m : 2.60637
[1mStep[0m  [8/21], [94mLoss[0m : 2.76048
[1mStep[0m  [10/21], [94mLoss[0m : 2.60389
[1mStep[0m  [12/21], [94mLoss[0m : 2.63045
[1mStep[0m  [14/21], [94mLoss[0m : 2.55213
[1mStep[0m  [16/21], [94mLoss[0m : 2.48715
[1mStep[0m  [18/21], [94mLoss[0m : 2.66118
[1mStep[0m  [20/21], [94mLoss[0m : 2.41018

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65663
[1mStep[0m  [2/21], [94mLoss[0m : 2.62132
[1mStep[0m  [4/21], [94mLoss[0m : 2.42524
[1mStep[0m  [6/21], [94mLoss[0m : 2.62960
[1mStep[0m  [8/21], [94mLoss[0m : 2.57226
[1mStep[0m  [10/21], [94mLoss[0m : 2.53054
[1mStep[0m  [12/21], [94mLoss[0m : 2.41360
[1mStep[0m  [14/21], [94mLoss[0m : 2.64479
[1mStep[0m  [16/21], [94mLoss[0m : 2.61467
[1mStep[0m  [18/21], [94mLoss[0m : 2.56293
[1mStep[0m  [20/21], [94mLoss[0m : 2.55415

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50432
[1mStep[0m  [2/21], [94mLoss[0m : 2.63067
[1mStep[0m  [4/21], [94mLoss[0m : 2.64607
[1mStep[0m  [6/21], [94mLoss[0m : 2.38503
[1mStep[0m  [8/21], [94mLoss[0m : 2.54290
[1mStep[0m  [10/21], [94mLoss[0m : 2.47620
[1mStep[0m  [12/21], [94mLoss[0m : 2.64309
[1mStep[0m  [14/21], [94mLoss[0m : 2.62678
[1mStep[0m  [16/21], [94mLoss[0m : 2.52335
[1mStep[0m  [18/21], [94mLoss[0m : 2.53545
[1mStep[0m  [20/21], [94mLoss[0m : 2.62400

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.384, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62662
[1mStep[0m  [2/21], [94mLoss[0m : 2.52435
[1mStep[0m  [4/21], [94mLoss[0m : 2.67048
[1mStep[0m  [6/21], [94mLoss[0m : 2.55210
[1mStep[0m  [8/21], [94mLoss[0m : 2.48102
[1mStep[0m  [10/21], [94mLoss[0m : 2.49426
[1mStep[0m  [12/21], [94mLoss[0m : 2.58396
[1mStep[0m  [14/21], [94mLoss[0m : 2.51367
[1mStep[0m  [16/21], [94mLoss[0m : 2.65739
[1mStep[0m  [18/21], [94mLoss[0m : 2.80809
[1mStep[0m  [20/21], [94mLoss[0m : 2.62226

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.378, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61249
[1mStep[0m  [2/21], [94mLoss[0m : 2.49038
[1mStep[0m  [4/21], [94mLoss[0m : 2.65637
[1mStep[0m  [6/21], [94mLoss[0m : 2.54593
[1mStep[0m  [8/21], [94mLoss[0m : 2.54709
[1mStep[0m  [10/21], [94mLoss[0m : 2.56163
[1mStep[0m  [12/21], [94mLoss[0m : 2.52317
[1mStep[0m  [14/21], [94mLoss[0m : 2.72704
[1mStep[0m  [16/21], [94mLoss[0m : 2.55530
[1mStep[0m  [18/21], [94mLoss[0m : 2.59291
[1mStep[0m  [20/21], [94mLoss[0m : 2.71530

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56819
[1mStep[0m  [2/21], [94mLoss[0m : 2.42774
[1mStep[0m  [4/21], [94mLoss[0m : 2.55299
[1mStep[0m  [6/21], [94mLoss[0m : 2.52586
[1mStep[0m  [8/21], [94mLoss[0m : 2.71085
[1mStep[0m  [10/21], [94mLoss[0m : 2.54242
[1mStep[0m  [12/21], [94mLoss[0m : 2.62345
[1mStep[0m  [14/21], [94mLoss[0m : 2.49478
[1mStep[0m  [16/21], [94mLoss[0m : 2.57115
[1mStep[0m  [18/21], [94mLoss[0m : 2.54007
[1mStep[0m  [20/21], [94mLoss[0m : 2.70805

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.367
====================================

Phase 1 - Evaluation MAE:  2.3671458108084544
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 2.49648
[1mStep[0m  [2/21], [94mLoss[0m : 2.60740
[1mStep[0m  [4/21], [94mLoss[0m : 2.62647
[1mStep[0m  [6/21], [94mLoss[0m : 2.52335
[1mStep[0m  [8/21], [94mLoss[0m : 2.59111
[1mStep[0m  [10/21], [94mLoss[0m : 2.63019
[1mStep[0m  [12/21], [94mLoss[0m : 2.71006
[1mStep[0m  [14/21], [94mLoss[0m : 2.63864
[1mStep[0m  [16/21], [94mLoss[0m : 2.61320
[1mStep[0m  [18/21], [94mLoss[0m : 2.54138
[1mStep[0m  [20/21], [94mLoss[0m : 2.57005

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66198
[1mStep[0m  [2/21], [94mLoss[0m : 2.69511
[1mStep[0m  [4/21], [94mLoss[0m : 2.69210
[1mStep[0m  [6/21], [94mLoss[0m : 2.54965
[1mStep[0m  [8/21], [94mLoss[0m : 2.48383
[1mStep[0m  [10/21], [94mLoss[0m : 2.61267
[1mStep[0m  [12/21], [94mLoss[0m : 2.36580
[1mStep[0m  [14/21], [94mLoss[0m : 2.58581
[1mStep[0m  [16/21], [94mLoss[0m : 2.45300
[1mStep[0m  [18/21], [94mLoss[0m : 2.41669
[1mStep[0m  [20/21], [94mLoss[0m : 2.58933

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49954
[1mStep[0m  [2/21], [94mLoss[0m : 2.53171
[1mStep[0m  [4/21], [94mLoss[0m : 2.64283
[1mStep[0m  [6/21], [94mLoss[0m : 2.58470
[1mStep[0m  [8/21], [94mLoss[0m : 2.65514
[1mStep[0m  [10/21], [94mLoss[0m : 2.49443
[1mStep[0m  [12/21], [94mLoss[0m : 2.44759
[1mStep[0m  [14/21], [94mLoss[0m : 2.66990
[1mStep[0m  [16/21], [94mLoss[0m : 2.43520
[1mStep[0m  [18/21], [94mLoss[0m : 2.71557
[1mStep[0m  [20/21], [94mLoss[0m : 2.54557

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68454
[1mStep[0m  [2/21], [94mLoss[0m : 2.64148
[1mStep[0m  [4/21], [94mLoss[0m : 2.40978
[1mStep[0m  [6/21], [94mLoss[0m : 2.61690
[1mStep[0m  [8/21], [94mLoss[0m : 2.49649
[1mStep[0m  [10/21], [94mLoss[0m : 2.58318
[1mStep[0m  [12/21], [94mLoss[0m : 2.58402
[1mStep[0m  [14/21], [94mLoss[0m : 2.61754
[1mStep[0m  [16/21], [94mLoss[0m : 2.65834
[1mStep[0m  [18/21], [94mLoss[0m : 2.66027
[1mStep[0m  [20/21], [94mLoss[0m : 2.36895

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62684
[1mStep[0m  [2/21], [94mLoss[0m : 2.78208
[1mStep[0m  [4/21], [94mLoss[0m : 2.56294
[1mStep[0m  [6/21], [94mLoss[0m : 2.50459
[1mStep[0m  [8/21], [94mLoss[0m : 2.69988
[1mStep[0m  [10/21], [94mLoss[0m : 2.52190
[1mStep[0m  [12/21], [94mLoss[0m : 2.62340
[1mStep[0m  [14/21], [94mLoss[0m : 2.82511
[1mStep[0m  [16/21], [94mLoss[0m : 2.58007
[1mStep[0m  [18/21], [94mLoss[0m : 2.51406
[1mStep[0m  [20/21], [94mLoss[0m : 2.46549

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.41736
[1mStep[0m  [2/21], [94mLoss[0m : 2.33579
[1mStep[0m  [4/21], [94mLoss[0m : 2.49039
[1mStep[0m  [6/21], [94mLoss[0m : 2.68300
[1mStep[0m  [8/21], [94mLoss[0m : 2.54184
[1mStep[0m  [10/21], [94mLoss[0m : 2.75013
[1mStep[0m  [12/21], [94mLoss[0m : 2.44784
[1mStep[0m  [14/21], [94mLoss[0m : 2.57638
[1mStep[0m  [16/21], [94mLoss[0m : 2.80582
[1mStep[0m  [18/21], [94mLoss[0m : 2.44468
[1mStep[0m  [20/21], [94mLoss[0m : 2.59257

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55056
[1mStep[0m  [2/21], [94mLoss[0m : 2.63541
[1mStep[0m  [4/21], [94mLoss[0m : 2.48930
[1mStep[0m  [6/21], [94mLoss[0m : 2.55811
[1mStep[0m  [8/21], [94mLoss[0m : 2.48212
[1mStep[0m  [10/21], [94mLoss[0m : 2.50102
[1mStep[0m  [12/21], [94mLoss[0m : 2.41330
[1mStep[0m  [14/21], [94mLoss[0m : 2.62123
[1mStep[0m  [16/21], [94mLoss[0m : 2.69641
[1mStep[0m  [18/21], [94mLoss[0m : 2.56303
[1mStep[0m  [20/21], [94mLoss[0m : 2.44354

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54482
[1mStep[0m  [2/21], [94mLoss[0m : 2.70564
[1mStep[0m  [4/21], [94mLoss[0m : 2.43079
[1mStep[0m  [6/21], [94mLoss[0m : 2.48563
[1mStep[0m  [8/21], [94mLoss[0m : 2.52691
[1mStep[0m  [10/21], [94mLoss[0m : 2.64053
[1mStep[0m  [12/21], [94mLoss[0m : 2.67689
[1mStep[0m  [14/21], [94mLoss[0m : 2.52502
[1mStep[0m  [16/21], [94mLoss[0m : 2.52352
[1mStep[0m  [18/21], [94mLoss[0m : 2.62842
[1mStep[0m  [20/21], [94mLoss[0m : 2.32737

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61143
[1mStep[0m  [2/21], [94mLoss[0m : 2.57737
[1mStep[0m  [4/21], [94mLoss[0m : 2.67991
[1mStep[0m  [6/21], [94mLoss[0m : 2.63435
[1mStep[0m  [8/21], [94mLoss[0m : 2.61046
[1mStep[0m  [10/21], [94mLoss[0m : 2.60750
[1mStep[0m  [12/21], [94mLoss[0m : 2.42913
[1mStep[0m  [14/21], [94mLoss[0m : 2.59320
[1mStep[0m  [16/21], [94mLoss[0m : 2.51934
[1mStep[0m  [18/21], [94mLoss[0m : 2.46460
[1mStep[0m  [20/21], [94mLoss[0m : 2.44902

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49108
[1mStep[0m  [2/21], [94mLoss[0m : 2.45474
[1mStep[0m  [4/21], [94mLoss[0m : 2.59848
[1mStep[0m  [6/21], [94mLoss[0m : 2.57240
[1mStep[0m  [8/21], [94mLoss[0m : 2.65538
[1mStep[0m  [10/21], [94mLoss[0m : 2.60294
[1mStep[0m  [12/21], [94mLoss[0m : 2.52937
[1mStep[0m  [14/21], [94mLoss[0m : 2.28031
[1mStep[0m  [16/21], [94mLoss[0m : 2.33913
[1mStep[0m  [18/21], [94mLoss[0m : 2.46512
[1mStep[0m  [20/21], [94mLoss[0m : 2.48782

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54322
[1mStep[0m  [2/21], [94mLoss[0m : 2.69249
[1mStep[0m  [4/21], [94mLoss[0m : 2.58928
[1mStep[0m  [6/21], [94mLoss[0m : 2.50013
[1mStep[0m  [8/21], [94mLoss[0m : 2.41529
[1mStep[0m  [10/21], [94mLoss[0m : 2.39124
[1mStep[0m  [12/21], [94mLoss[0m : 2.48656
[1mStep[0m  [14/21], [94mLoss[0m : 2.58432
[1mStep[0m  [16/21], [94mLoss[0m : 2.71039
[1mStep[0m  [18/21], [94mLoss[0m : 2.50732
[1mStep[0m  [20/21], [94mLoss[0m : 2.52107

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52452
[1mStep[0m  [2/21], [94mLoss[0m : 2.51174
[1mStep[0m  [4/21], [94mLoss[0m : 2.55248
[1mStep[0m  [6/21], [94mLoss[0m : 2.51619
[1mStep[0m  [8/21], [94mLoss[0m : 2.51675
[1mStep[0m  [10/21], [94mLoss[0m : 2.59743
[1mStep[0m  [12/21], [94mLoss[0m : 2.52433
[1mStep[0m  [14/21], [94mLoss[0m : 2.39599
[1mStep[0m  [16/21], [94mLoss[0m : 2.48685
[1mStep[0m  [18/21], [94mLoss[0m : 2.48838
[1mStep[0m  [20/21], [94mLoss[0m : 2.63342

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59988
[1mStep[0m  [2/21], [94mLoss[0m : 2.44176
[1mStep[0m  [4/21], [94mLoss[0m : 2.71937
[1mStep[0m  [6/21], [94mLoss[0m : 2.48228
[1mStep[0m  [8/21], [94mLoss[0m : 2.38495
[1mStep[0m  [10/21], [94mLoss[0m : 2.45043
[1mStep[0m  [12/21], [94mLoss[0m : 2.60282
[1mStep[0m  [14/21], [94mLoss[0m : 2.49589
[1mStep[0m  [16/21], [94mLoss[0m : 2.59308
[1mStep[0m  [18/21], [94mLoss[0m : 2.63946
[1mStep[0m  [20/21], [94mLoss[0m : 2.66284

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42813
[1mStep[0m  [2/21], [94mLoss[0m : 2.50978
[1mStep[0m  [4/21], [94mLoss[0m : 2.61284
[1mStep[0m  [6/21], [94mLoss[0m : 2.49113
[1mStep[0m  [8/21], [94mLoss[0m : 2.55810
[1mStep[0m  [10/21], [94mLoss[0m : 2.39380
[1mStep[0m  [12/21], [94mLoss[0m : 2.65760
[1mStep[0m  [14/21], [94mLoss[0m : 2.41429
[1mStep[0m  [16/21], [94mLoss[0m : 2.46720
[1mStep[0m  [18/21], [94mLoss[0m : 2.61197
[1mStep[0m  [20/21], [94mLoss[0m : 2.43332

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46200
[1mStep[0m  [2/21], [94mLoss[0m : 2.69693
[1mStep[0m  [4/21], [94mLoss[0m : 2.68664
[1mStep[0m  [6/21], [94mLoss[0m : 2.51529
[1mStep[0m  [8/21], [94mLoss[0m : 2.53828
[1mStep[0m  [10/21], [94mLoss[0m : 2.35078
[1mStep[0m  [12/21], [94mLoss[0m : 2.52988
[1mStep[0m  [14/21], [94mLoss[0m : 2.45594
[1mStep[0m  [16/21], [94mLoss[0m : 2.55802
[1mStep[0m  [18/21], [94mLoss[0m : 2.35616
[1mStep[0m  [20/21], [94mLoss[0m : 2.60366

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47182
[1mStep[0m  [2/21], [94mLoss[0m : 2.41425
[1mStep[0m  [4/21], [94mLoss[0m : 2.39619
[1mStep[0m  [6/21], [94mLoss[0m : 2.48120
[1mStep[0m  [8/21], [94mLoss[0m : 2.37062
[1mStep[0m  [10/21], [94mLoss[0m : 2.39489
[1mStep[0m  [12/21], [94mLoss[0m : 2.49018
[1mStep[0m  [14/21], [94mLoss[0m : 2.60201
[1mStep[0m  [16/21], [94mLoss[0m : 2.51795
[1mStep[0m  [18/21], [94mLoss[0m : 2.57168
[1mStep[0m  [20/21], [94mLoss[0m : 2.56329

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.327, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.35684
[1mStep[0m  [2/21], [94mLoss[0m : 2.66043
[1mStep[0m  [4/21], [94mLoss[0m : 2.57957
[1mStep[0m  [6/21], [94mLoss[0m : 2.44750
[1mStep[0m  [8/21], [94mLoss[0m : 2.37716
[1mStep[0m  [10/21], [94mLoss[0m : 2.51627
[1mStep[0m  [12/21], [94mLoss[0m : 2.49377
[1mStep[0m  [14/21], [94mLoss[0m : 2.58690
[1mStep[0m  [16/21], [94mLoss[0m : 2.35226
[1mStep[0m  [18/21], [94mLoss[0m : 2.48323
[1mStep[0m  [20/21], [94mLoss[0m : 2.37243

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.320, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59693
[1mStep[0m  [2/21], [94mLoss[0m : 2.31671
[1mStep[0m  [4/21], [94mLoss[0m : 2.38215
[1mStep[0m  [6/21], [94mLoss[0m : 2.44221
[1mStep[0m  [8/21], [94mLoss[0m : 2.42371
[1mStep[0m  [10/21], [94mLoss[0m : 2.50299
[1mStep[0m  [12/21], [94mLoss[0m : 2.59494
[1mStep[0m  [14/21], [94mLoss[0m : 2.49611
[1mStep[0m  [16/21], [94mLoss[0m : 2.31415
[1mStep[0m  [18/21], [94mLoss[0m : 2.40887
[1mStep[0m  [20/21], [94mLoss[0m : 2.44552

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.47252
[1mStep[0m  [2/21], [94mLoss[0m : 2.45615
[1mStep[0m  [4/21], [94mLoss[0m : 2.43465
[1mStep[0m  [6/21], [94mLoss[0m : 2.50703
[1mStep[0m  [8/21], [94mLoss[0m : 2.44671
[1mStep[0m  [10/21], [94mLoss[0m : 2.58293
[1mStep[0m  [12/21], [94mLoss[0m : 2.52752
[1mStep[0m  [14/21], [94mLoss[0m : 2.56550
[1mStep[0m  [16/21], [94mLoss[0m : 2.43097
[1mStep[0m  [18/21], [94mLoss[0m : 2.32305
[1mStep[0m  [20/21], [94mLoss[0m : 2.48370

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.320, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52357
[1mStep[0m  [2/21], [94mLoss[0m : 2.38432
[1mStep[0m  [4/21], [94mLoss[0m : 2.65939
[1mStep[0m  [6/21], [94mLoss[0m : 2.29372
[1mStep[0m  [8/21], [94mLoss[0m : 2.28210
[1mStep[0m  [10/21], [94mLoss[0m : 2.51471
[1mStep[0m  [12/21], [94mLoss[0m : 2.30958
[1mStep[0m  [14/21], [94mLoss[0m : 2.59087
[1mStep[0m  [16/21], [94mLoss[0m : 2.45699
[1mStep[0m  [18/21], [94mLoss[0m : 2.46647
[1mStep[0m  [20/21], [94mLoss[0m : 2.37648

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37851
[1mStep[0m  [2/21], [94mLoss[0m : 2.43349
[1mStep[0m  [4/21], [94mLoss[0m : 2.48199
[1mStep[0m  [6/21], [94mLoss[0m : 2.39588
[1mStep[0m  [8/21], [94mLoss[0m : 2.64980
[1mStep[0m  [10/21], [94mLoss[0m : 2.43692
[1mStep[0m  [12/21], [94mLoss[0m : 2.67725
[1mStep[0m  [14/21], [94mLoss[0m : 2.43229
[1mStep[0m  [16/21], [94mLoss[0m : 2.42829
[1mStep[0m  [18/21], [94mLoss[0m : 2.22491
[1mStep[0m  [20/21], [94mLoss[0m : 2.49045

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.27895
[1mStep[0m  [2/21], [94mLoss[0m : 2.37934
[1mStep[0m  [4/21], [94mLoss[0m : 2.58781
[1mStep[0m  [6/21], [94mLoss[0m : 2.35713
[1mStep[0m  [8/21], [94mLoss[0m : 2.31644
[1mStep[0m  [10/21], [94mLoss[0m : 2.39284
[1mStep[0m  [12/21], [94mLoss[0m : 2.53617
[1mStep[0m  [14/21], [94mLoss[0m : 2.46052
[1mStep[0m  [16/21], [94mLoss[0m : 2.64799
[1mStep[0m  [18/21], [94mLoss[0m : 2.49132
[1mStep[0m  [20/21], [94mLoss[0m : 2.36458

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.314, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59584
[1mStep[0m  [2/21], [94mLoss[0m : 2.51402
[1mStep[0m  [4/21], [94mLoss[0m : 2.55147
[1mStep[0m  [6/21], [94mLoss[0m : 2.51074
[1mStep[0m  [8/21], [94mLoss[0m : 2.51543
[1mStep[0m  [10/21], [94mLoss[0m : 2.31844
[1mStep[0m  [12/21], [94mLoss[0m : 2.30616
[1mStep[0m  [14/21], [94mLoss[0m : 2.51915
[1mStep[0m  [16/21], [94mLoss[0m : 2.42728
[1mStep[0m  [18/21], [94mLoss[0m : 2.50058
[1mStep[0m  [20/21], [94mLoss[0m : 2.41589

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.316, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.56872
[1mStep[0m  [2/21], [94mLoss[0m : 2.48556
[1mStep[0m  [4/21], [94mLoss[0m : 2.47545
[1mStep[0m  [6/21], [94mLoss[0m : 2.48605
[1mStep[0m  [8/21], [94mLoss[0m : 2.33522
[1mStep[0m  [10/21], [94mLoss[0m : 2.52547
[1mStep[0m  [12/21], [94mLoss[0m : 2.35396
[1mStep[0m  [14/21], [94mLoss[0m : 2.38758
[1mStep[0m  [16/21], [94mLoss[0m : 2.31659
[1mStep[0m  [18/21], [94mLoss[0m : 2.35889
[1mStep[0m  [20/21], [94mLoss[0m : 2.45143

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59114
[1mStep[0m  [2/21], [94mLoss[0m : 2.31719
[1mStep[0m  [4/21], [94mLoss[0m : 2.39324
[1mStep[0m  [6/21], [94mLoss[0m : 2.49805
[1mStep[0m  [8/21], [94mLoss[0m : 2.63275
[1mStep[0m  [10/21], [94mLoss[0m : 2.57782
[1mStep[0m  [12/21], [94mLoss[0m : 2.27326
[1mStep[0m  [14/21], [94mLoss[0m : 2.43211
[1mStep[0m  [16/21], [94mLoss[0m : 2.43096
[1mStep[0m  [18/21], [94mLoss[0m : 2.39022
[1mStep[0m  [20/21], [94mLoss[0m : 2.58330

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39176
[1mStep[0m  [2/21], [94mLoss[0m : 2.41368
[1mStep[0m  [4/21], [94mLoss[0m : 2.39096
[1mStep[0m  [6/21], [94mLoss[0m : 2.32624
[1mStep[0m  [8/21], [94mLoss[0m : 2.35523
[1mStep[0m  [10/21], [94mLoss[0m : 2.46782
[1mStep[0m  [12/21], [94mLoss[0m : 2.31899
[1mStep[0m  [14/21], [94mLoss[0m : 2.51573
[1mStep[0m  [16/21], [94mLoss[0m : 2.44499
[1mStep[0m  [18/21], [94mLoss[0m : 2.44270
[1mStep[0m  [20/21], [94mLoss[0m : 2.36372

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58599
[1mStep[0m  [2/21], [94mLoss[0m : 2.55631
[1mStep[0m  [4/21], [94mLoss[0m : 2.36275
[1mStep[0m  [6/21], [94mLoss[0m : 2.29894
[1mStep[0m  [8/21], [94mLoss[0m : 2.53114
[1mStep[0m  [10/21], [94mLoss[0m : 2.44812
[1mStep[0m  [12/21], [94mLoss[0m : 2.54319
[1mStep[0m  [14/21], [94mLoss[0m : 2.40628
[1mStep[0m  [16/21], [94mLoss[0m : 2.46898
[1mStep[0m  [18/21], [94mLoss[0m : 2.48913
[1mStep[0m  [20/21], [94mLoss[0m : 2.43000

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46757
[1mStep[0m  [2/21], [94mLoss[0m : 2.42980
[1mStep[0m  [4/21], [94mLoss[0m : 2.37140
[1mStep[0m  [6/21], [94mLoss[0m : 2.47119
[1mStep[0m  [8/21], [94mLoss[0m : 2.47334
[1mStep[0m  [10/21], [94mLoss[0m : 2.34162
[1mStep[0m  [12/21], [94mLoss[0m : 2.42575
[1mStep[0m  [14/21], [94mLoss[0m : 2.31339
[1mStep[0m  [16/21], [94mLoss[0m : 2.57691
[1mStep[0m  [18/21], [94mLoss[0m : 2.29752
[1mStep[0m  [20/21], [94mLoss[0m : 2.38074

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44831
[1mStep[0m  [2/21], [94mLoss[0m : 2.38029
[1mStep[0m  [4/21], [94mLoss[0m : 2.34930
[1mStep[0m  [6/21], [94mLoss[0m : 2.47496
[1mStep[0m  [8/21], [94mLoss[0m : 2.38223
[1mStep[0m  [10/21], [94mLoss[0m : 2.46594
[1mStep[0m  [12/21], [94mLoss[0m : 2.53620
[1mStep[0m  [14/21], [94mLoss[0m : 2.35296
[1mStep[0m  [16/21], [94mLoss[0m : 2.29879
[1mStep[0m  [18/21], [94mLoss[0m : 2.44058
[1mStep[0m  [20/21], [94mLoss[0m : 2.29241

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.318, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.49640
[1mStep[0m  [2/21], [94mLoss[0m : 2.44082
[1mStep[0m  [4/21], [94mLoss[0m : 2.49104
[1mStep[0m  [6/21], [94mLoss[0m : 2.35723
[1mStep[0m  [8/21], [94mLoss[0m : 2.41550
[1mStep[0m  [10/21], [94mLoss[0m : 2.44327
[1mStep[0m  [12/21], [94mLoss[0m : 2.39266
[1mStep[0m  [14/21], [94mLoss[0m : 2.30614
[1mStep[0m  [16/21], [94mLoss[0m : 2.46836
[1mStep[0m  [18/21], [94mLoss[0m : 2.38302
[1mStep[0m  [20/21], [94mLoss[0m : 2.50710

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.321, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.335
====================================

Phase 2 - Evaluation MAE:  2.3348781040736606
MAE score P1       2.367146
MAE score P2       2.334878
loss               2.414561
learning_rate      0.002575
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.72948
[1mStep[0m  [33/339], [94mLoss[0m : 2.92788
[1mStep[0m  [66/339], [94mLoss[0m : 2.67133
[1mStep[0m  [99/339], [94mLoss[0m : 2.88770
[1mStep[0m  [132/339], [94mLoss[0m : 3.03194
[1mStep[0m  [165/339], [94mLoss[0m : 2.76097
[1mStep[0m  [198/339], [94mLoss[0m : 3.08761
[1mStep[0m  [231/339], [94mLoss[0m : 3.61341
[1mStep[0m  [264/339], [94mLoss[0m : 2.81080
[1mStep[0m  [297/339], [94mLoss[0m : 3.46374
[1mStep[0m  [330/339], [94mLoss[0m : 2.75843

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.174, [92mTest[0m: 10.827, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14339
[1mStep[0m  [33/339], [94mLoss[0m : 2.55543
[1mStep[0m  [66/339], [94mLoss[0m : 2.11077
[1mStep[0m  [99/339], [94mLoss[0m : 2.89069
[1mStep[0m  [132/339], [94mLoss[0m : 2.38791
[1mStep[0m  [165/339], [94mLoss[0m : 2.24764
[1mStep[0m  [198/339], [94mLoss[0m : 2.03219
[1mStep[0m  [231/339], [94mLoss[0m : 2.80013
[1mStep[0m  [264/339], [94mLoss[0m : 1.98774
[1mStep[0m  [297/339], [94mLoss[0m : 2.39411
[1mStep[0m  [330/339], [94mLoss[0m : 1.77121

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.426, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71029
[1mStep[0m  [33/339], [94mLoss[0m : 2.61848
[1mStep[0m  [66/339], [94mLoss[0m : 2.02098
[1mStep[0m  [99/339], [94mLoss[0m : 1.78536
[1mStep[0m  [132/339], [94mLoss[0m : 1.86138
[1mStep[0m  [165/339], [94mLoss[0m : 2.57572
[1mStep[0m  [198/339], [94mLoss[0m : 2.61835
[1mStep[0m  [231/339], [94mLoss[0m : 2.05247
[1mStep[0m  [264/339], [94mLoss[0m : 2.75428
[1mStep[0m  [297/339], [94mLoss[0m : 2.79469
[1mStep[0m  [330/339], [94mLoss[0m : 2.68908

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.416, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60864
[1mStep[0m  [33/339], [94mLoss[0m : 2.68176
[1mStep[0m  [66/339], [94mLoss[0m : 2.42134
[1mStep[0m  [99/339], [94mLoss[0m : 2.36829
[1mStep[0m  [132/339], [94mLoss[0m : 2.67993
[1mStep[0m  [165/339], [94mLoss[0m : 2.65534
[1mStep[0m  [198/339], [94mLoss[0m : 2.44135
[1mStep[0m  [231/339], [94mLoss[0m : 2.52390
[1mStep[0m  [264/339], [94mLoss[0m : 1.69067
[1mStep[0m  [297/339], [94mLoss[0m : 2.22963
[1mStep[0m  [330/339], [94mLoss[0m : 2.36282

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.365, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56625
[1mStep[0m  [33/339], [94mLoss[0m : 2.48899
[1mStep[0m  [66/339], [94mLoss[0m : 2.08814
[1mStep[0m  [99/339], [94mLoss[0m : 2.24064
[1mStep[0m  [132/339], [94mLoss[0m : 2.34662
[1mStep[0m  [165/339], [94mLoss[0m : 2.05985
[1mStep[0m  [198/339], [94mLoss[0m : 3.23615
[1mStep[0m  [231/339], [94mLoss[0m : 2.87529
[1mStep[0m  [264/339], [94mLoss[0m : 1.96070
[1mStep[0m  [297/339], [94mLoss[0m : 2.55998
[1mStep[0m  [330/339], [94mLoss[0m : 1.92711

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.351, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70498
[1mStep[0m  [33/339], [94mLoss[0m : 2.05107
[1mStep[0m  [66/339], [94mLoss[0m : 2.36790
[1mStep[0m  [99/339], [94mLoss[0m : 2.24281
[1mStep[0m  [132/339], [94mLoss[0m : 2.55715
[1mStep[0m  [165/339], [94mLoss[0m : 2.87208
[1mStep[0m  [198/339], [94mLoss[0m : 2.05438
[1mStep[0m  [231/339], [94mLoss[0m : 2.61884
[1mStep[0m  [264/339], [94mLoss[0m : 1.83073
[1mStep[0m  [297/339], [94mLoss[0m : 2.37078
[1mStep[0m  [330/339], [94mLoss[0m : 2.80117

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.356, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97001
[1mStep[0m  [33/339], [94mLoss[0m : 2.69664
[1mStep[0m  [66/339], [94mLoss[0m : 2.40796
[1mStep[0m  [99/339], [94mLoss[0m : 2.13055
[1mStep[0m  [132/339], [94mLoss[0m : 2.20439
[1mStep[0m  [165/339], [94mLoss[0m : 3.04478
[1mStep[0m  [198/339], [94mLoss[0m : 2.37687
[1mStep[0m  [231/339], [94mLoss[0m : 3.14570
[1mStep[0m  [264/339], [94mLoss[0m : 2.51523
[1mStep[0m  [297/339], [94mLoss[0m : 2.35484
[1mStep[0m  [330/339], [94mLoss[0m : 2.96668

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.382, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78028
[1mStep[0m  [33/339], [94mLoss[0m : 2.78516
[1mStep[0m  [66/339], [94mLoss[0m : 2.58442
[1mStep[0m  [99/339], [94mLoss[0m : 2.29674
[1mStep[0m  [132/339], [94mLoss[0m : 2.42392
[1mStep[0m  [165/339], [94mLoss[0m : 2.75768
[1mStep[0m  [198/339], [94mLoss[0m : 2.32367
[1mStep[0m  [231/339], [94mLoss[0m : 2.59468
[1mStep[0m  [264/339], [94mLoss[0m : 3.04936
[1mStep[0m  [297/339], [94mLoss[0m : 2.06272
[1mStep[0m  [330/339], [94mLoss[0m : 2.48084

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72271
[1mStep[0m  [33/339], [94mLoss[0m : 3.00140
[1mStep[0m  [66/339], [94mLoss[0m : 2.05084
[1mStep[0m  [99/339], [94mLoss[0m : 3.05291
[1mStep[0m  [132/339], [94mLoss[0m : 2.75978
[1mStep[0m  [165/339], [94mLoss[0m : 2.91079
[1mStep[0m  [198/339], [94mLoss[0m : 2.76656
[1mStep[0m  [231/339], [94mLoss[0m : 2.01403
[1mStep[0m  [264/339], [94mLoss[0m : 2.04662
[1mStep[0m  [297/339], [94mLoss[0m : 2.49792
[1mStep[0m  [330/339], [94mLoss[0m : 3.10051

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20201
[1mStep[0m  [33/339], [94mLoss[0m : 2.59363
[1mStep[0m  [66/339], [94mLoss[0m : 2.76410
[1mStep[0m  [99/339], [94mLoss[0m : 1.75343
[1mStep[0m  [132/339], [94mLoss[0m : 2.65084
[1mStep[0m  [165/339], [94mLoss[0m : 2.92207
[1mStep[0m  [198/339], [94mLoss[0m : 2.52487
[1mStep[0m  [231/339], [94mLoss[0m : 1.93532
[1mStep[0m  [264/339], [94mLoss[0m : 2.51835
[1mStep[0m  [297/339], [94mLoss[0m : 2.44218
[1mStep[0m  [330/339], [94mLoss[0m : 2.12981

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.380, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80125
[1mStep[0m  [33/339], [94mLoss[0m : 1.67268
[1mStep[0m  [66/339], [94mLoss[0m : 2.28428
[1mStep[0m  [99/339], [94mLoss[0m : 1.93690
[1mStep[0m  [132/339], [94mLoss[0m : 2.36044
[1mStep[0m  [165/339], [94mLoss[0m : 2.12891
[1mStep[0m  [198/339], [94mLoss[0m : 2.92940
[1mStep[0m  [231/339], [94mLoss[0m : 2.04034
[1mStep[0m  [264/339], [94mLoss[0m : 2.49542
[1mStep[0m  [297/339], [94mLoss[0m : 2.33592
[1mStep[0m  [330/339], [94mLoss[0m : 2.55248

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.413, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64171
[1mStep[0m  [33/339], [94mLoss[0m : 2.60568
[1mStep[0m  [66/339], [94mLoss[0m : 2.36383
[1mStep[0m  [99/339], [94mLoss[0m : 2.09746
[1mStep[0m  [132/339], [94mLoss[0m : 2.55216
[1mStep[0m  [165/339], [94mLoss[0m : 2.19537
[1mStep[0m  [198/339], [94mLoss[0m : 3.24677
[1mStep[0m  [231/339], [94mLoss[0m : 2.31582
[1mStep[0m  [264/339], [94mLoss[0m : 2.54908
[1mStep[0m  [297/339], [94mLoss[0m : 2.20317
[1mStep[0m  [330/339], [94mLoss[0m : 1.97105

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31619
[1mStep[0m  [33/339], [94mLoss[0m : 2.24194
[1mStep[0m  [66/339], [94mLoss[0m : 2.84751
[1mStep[0m  [99/339], [94mLoss[0m : 2.44037
[1mStep[0m  [132/339], [94mLoss[0m : 1.97920
[1mStep[0m  [165/339], [94mLoss[0m : 2.59959
[1mStep[0m  [198/339], [94mLoss[0m : 2.47799
[1mStep[0m  [231/339], [94mLoss[0m : 2.25983
[1mStep[0m  [264/339], [94mLoss[0m : 2.46775
[1mStep[0m  [297/339], [94mLoss[0m : 2.57646
[1mStep[0m  [330/339], [94mLoss[0m : 2.43019

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.341, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86634
[1mStep[0m  [33/339], [94mLoss[0m : 1.92627
[1mStep[0m  [66/339], [94mLoss[0m : 2.60353
[1mStep[0m  [99/339], [94mLoss[0m : 2.12573
[1mStep[0m  [132/339], [94mLoss[0m : 1.90797
[1mStep[0m  [165/339], [94mLoss[0m : 2.08559
[1mStep[0m  [198/339], [94mLoss[0m : 3.12260
[1mStep[0m  [231/339], [94mLoss[0m : 2.25908
[1mStep[0m  [264/339], [94mLoss[0m : 1.92755
[1mStep[0m  [297/339], [94mLoss[0m : 2.01271
[1mStep[0m  [330/339], [94mLoss[0m : 2.78942

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09658
[1mStep[0m  [33/339], [94mLoss[0m : 3.06075
[1mStep[0m  [66/339], [94mLoss[0m : 2.58395
[1mStep[0m  [99/339], [94mLoss[0m : 2.45866
[1mStep[0m  [132/339], [94mLoss[0m : 1.59304
[1mStep[0m  [165/339], [94mLoss[0m : 2.22007
[1mStep[0m  [198/339], [94mLoss[0m : 2.92974
[1mStep[0m  [231/339], [94mLoss[0m : 2.10348
[1mStep[0m  [264/339], [94mLoss[0m : 2.27357
[1mStep[0m  [297/339], [94mLoss[0m : 2.54527
[1mStep[0m  [330/339], [94mLoss[0m : 2.16823

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25984
[1mStep[0m  [33/339], [94mLoss[0m : 2.64365
[1mStep[0m  [66/339], [94mLoss[0m : 2.42492
[1mStep[0m  [99/339], [94mLoss[0m : 1.95829
[1mStep[0m  [132/339], [94mLoss[0m : 2.86671
[1mStep[0m  [165/339], [94mLoss[0m : 2.76148
[1mStep[0m  [198/339], [94mLoss[0m : 1.66671
[1mStep[0m  [231/339], [94mLoss[0m : 1.47720
[1mStep[0m  [264/339], [94mLoss[0m : 2.26552
[1mStep[0m  [297/339], [94mLoss[0m : 2.16894
[1mStep[0m  [330/339], [94mLoss[0m : 2.15029

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.316, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30813
[1mStep[0m  [33/339], [94mLoss[0m : 2.35200
[1mStep[0m  [66/339], [94mLoss[0m : 2.25246
[1mStep[0m  [99/339], [94mLoss[0m : 2.39850
[1mStep[0m  [132/339], [94mLoss[0m : 2.76904
[1mStep[0m  [165/339], [94mLoss[0m : 2.19287
[1mStep[0m  [198/339], [94mLoss[0m : 2.80301
[1mStep[0m  [231/339], [94mLoss[0m : 2.82155
[1mStep[0m  [264/339], [94mLoss[0m : 2.19897
[1mStep[0m  [297/339], [94mLoss[0m : 1.83796
[1mStep[0m  [330/339], [94mLoss[0m : 3.03015

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.79802
[1mStep[0m  [33/339], [94mLoss[0m : 2.92018
[1mStep[0m  [66/339], [94mLoss[0m : 1.69099
[1mStep[0m  [99/339], [94mLoss[0m : 2.12394
[1mStep[0m  [132/339], [94mLoss[0m : 2.05593
[1mStep[0m  [165/339], [94mLoss[0m : 1.72107
[1mStep[0m  [198/339], [94mLoss[0m : 2.38938
[1mStep[0m  [231/339], [94mLoss[0m : 2.58863
[1mStep[0m  [264/339], [94mLoss[0m : 2.59696
[1mStep[0m  [297/339], [94mLoss[0m : 2.33408
[1mStep[0m  [330/339], [94mLoss[0m : 2.23558

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.332, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32180
[1mStep[0m  [33/339], [94mLoss[0m : 2.55524
[1mStep[0m  [66/339], [94mLoss[0m : 2.03053
[1mStep[0m  [99/339], [94mLoss[0m : 2.38936
[1mStep[0m  [132/339], [94mLoss[0m : 1.90738
[1mStep[0m  [165/339], [94mLoss[0m : 2.39966
[1mStep[0m  [198/339], [94mLoss[0m : 2.19279
[1mStep[0m  [231/339], [94mLoss[0m : 2.35871
[1mStep[0m  [264/339], [94mLoss[0m : 2.28511
[1mStep[0m  [297/339], [94mLoss[0m : 2.10743
[1mStep[0m  [330/339], [94mLoss[0m : 2.34412

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.395, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44631
[1mStep[0m  [33/339], [94mLoss[0m : 2.41306
[1mStep[0m  [66/339], [94mLoss[0m : 1.93046
[1mStep[0m  [99/339], [94mLoss[0m : 2.15523
[1mStep[0m  [132/339], [94mLoss[0m : 2.95044
[1mStep[0m  [165/339], [94mLoss[0m : 1.73116
[1mStep[0m  [198/339], [94mLoss[0m : 2.65810
[1mStep[0m  [231/339], [94mLoss[0m : 2.52729
[1mStep[0m  [264/339], [94mLoss[0m : 2.14361
[1mStep[0m  [297/339], [94mLoss[0m : 1.93186
[1mStep[0m  [330/339], [94mLoss[0m : 2.23324

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.333, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45590
[1mStep[0m  [33/339], [94mLoss[0m : 2.45879
[1mStep[0m  [66/339], [94mLoss[0m : 1.98549
[1mStep[0m  [99/339], [94mLoss[0m : 2.51343
[1mStep[0m  [132/339], [94mLoss[0m : 2.22747
[1mStep[0m  [165/339], [94mLoss[0m : 2.03677
[1mStep[0m  [198/339], [94mLoss[0m : 2.69624
[1mStep[0m  [231/339], [94mLoss[0m : 2.79467
[1mStep[0m  [264/339], [94mLoss[0m : 2.48033
[1mStep[0m  [297/339], [94mLoss[0m : 1.97090
[1mStep[0m  [330/339], [94mLoss[0m : 2.09807

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.354, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18549
[1mStep[0m  [33/339], [94mLoss[0m : 2.37888
[1mStep[0m  [66/339], [94mLoss[0m : 2.74642
[1mStep[0m  [99/339], [94mLoss[0m : 2.50737
[1mStep[0m  [132/339], [94mLoss[0m : 2.37615
[1mStep[0m  [165/339], [94mLoss[0m : 2.47671
[1mStep[0m  [198/339], [94mLoss[0m : 2.31116
[1mStep[0m  [231/339], [94mLoss[0m : 2.48939
[1mStep[0m  [264/339], [94mLoss[0m : 2.82912
[1mStep[0m  [297/339], [94mLoss[0m : 2.20614
[1mStep[0m  [330/339], [94mLoss[0m : 2.53591

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32024
[1mStep[0m  [33/339], [94mLoss[0m : 2.82536
[1mStep[0m  [66/339], [94mLoss[0m : 2.15026
[1mStep[0m  [99/339], [94mLoss[0m : 2.17810
[1mStep[0m  [132/339], [94mLoss[0m : 1.83207
[1mStep[0m  [165/339], [94mLoss[0m : 1.73212
[1mStep[0m  [198/339], [94mLoss[0m : 2.82036
[1mStep[0m  [231/339], [94mLoss[0m : 2.14510
[1mStep[0m  [264/339], [94mLoss[0m : 1.76428
[1mStep[0m  [297/339], [94mLoss[0m : 2.97094
[1mStep[0m  [330/339], [94mLoss[0m : 2.17759

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50750
[1mStep[0m  [33/339], [94mLoss[0m : 2.09671
[1mStep[0m  [66/339], [94mLoss[0m : 2.21843
[1mStep[0m  [99/339], [94mLoss[0m : 2.92408
[1mStep[0m  [132/339], [94mLoss[0m : 2.11178
[1mStep[0m  [165/339], [94mLoss[0m : 3.16737
[1mStep[0m  [198/339], [94mLoss[0m : 1.93702
[1mStep[0m  [231/339], [94mLoss[0m : 2.47720
[1mStep[0m  [264/339], [94mLoss[0m : 2.24881
[1mStep[0m  [297/339], [94mLoss[0m : 2.46601
[1mStep[0m  [330/339], [94mLoss[0m : 1.83160

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.316, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35608
[1mStep[0m  [33/339], [94mLoss[0m : 3.13903
[1mStep[0m  [66/339], [94mLoss[0m : 2.29428
[1mStep[0m  [99/339], [94mLoss[0m : 2.12840
[1mStep[0m  [132/339], [94mLoss[0m : 2.10111
[1mStep[0m  [165/339], [94mLoss[0m : 2.88893
[1mStep[0m  [198/339], [94mLoss[0m : 1.92376
[1mStep[0m  [231/339], [94mLoss[0m : 2.04677
[1mStep[0m  [264/339], [94mLoss[0m : 2.54855
[1mStep[0m  [297/339], [94mLoss[0m : 2.28987
[1mStep[0m  [330/339], [94mLoss[0m : 2.48706

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.346, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70705
[1mStep[0m  [33/339], [94mLoss[0m : 2.39957
[1mStep[0m  [66/339], [94mLoss[0m : 2.74413
[1mStep[0m  [99/339], [94mLoss[0m : 2.45660
[1mStep[0m  [132/339], [94mLoss[0m : 2.62653
[1mStep[0m  [165/339], [94mLoss[0m : 2.74550
[1mStep[0m  [198/339], [94mLoss[0m : 2.19633
[1mStep[0m  [231/339], [94mLoss[0m : 2.43492
[1mStep[0m  [264/339], [94mLoss[0m : 2.24608
[1mStep[0m  [297/339], [94mLoss[0m : 2.15432
[1mStep[0m  [330/339], [94mLoss[0m : 2.62669

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12888
[1mStep[0m  [33/339], [94mLoss[0m : 2.20838
[1mStep[0m  [66/339], [94mLoss[0m : 2.35940
[1mStep[0m  [99/339], [94mLoss[0m : 2.28631
[1mStep[0m  [132/339], [94mLoss[0m : 2.58505
[1mStep[0m  [165/339], [94mLoss[0m : 2.56884
[1mStep[0m  [198/339], [94mLoss[0m : 1.76351
[1mStep[0m  [231/339], [94mLoss[0m : 1.70416
[1mStep[0m  [264/339], [94mLoss[0m : 2.42633
[1mStep[0m  [297/339], [94mLoss[0m : 2.63788
[1mStep[0m  [330/339], [94mLoss[0m : 2.76898

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.314, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28442
[1mStep[0m  [33/339], [94mLoss[0m : 1.89453
[1mStep[0m  [66/339], [94mLoss[0m : 2.30976
[1mStep[0m  [99/339], [94mLoss[0m : 3.09625
[1mStep[0m  [132/339], [94mLoss[0m : 2.64459
[1mStep[0m  [165/339], [94mLoss[0m : 2.18816
[1mStep[0m  [198/339], [94mLoss[0m : 2.22525
[1mStep[0m  [231/339], [94mLoss[0m : 1.78816
[1mStep[0m  [264/339], [94mLoss[0m : 3.35202
[1mStep[0m  [297/339], [94mLoss[0m : 2.54415
[1mStep[0m  [330/339], [94mLoss[0m : 2.44134

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.330, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13490
[1mStep[0m  [33/339], [94mLoss[0m : 2.04796
[1mStep[0m  [66/339], [94mLoss[0m : 2.22376
[1mStep[0m  [99/339], [94mLoss[0m : 2.02965
[1mStep[0m  [132/339], [94mLoss[0m : 2.14001
[1mStep[0m  [165/339], [94mLoss[0m : 2.93965
[1mStep[0m  [198/339], [94mLoss[0m : 2.92568
[1mStep[0m  [231/339], [94mLoss[0m : 2.47312
[1mStep[0m  [264/339], [94mLoss[0m : 2.98816
[1mStep[0m  [297/339], [94mLoss[0m : 2.82193
[1mStep[0m  [330/339], [94mLoss[0m : 3.38277

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.307, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22382
[1mStep[0m  [33/339], [94mLoss[0m : 2.28501
[1mStep[0m  [66/339], [94mLoss[0m : 1.93658
[1mStep[0m  [99/339], [94mLoss[0m : 1.92997
[1mStep[0m  [132/339], [94mLoss[0m : 2.50391
[1mStep[0m  [165/339], [94mLoss[0m : 2.02982
[1mStep[0m  [198/339], [94mLoss[0m : 2.36062
[1mStep[0m  [231/339], [94mLoss[0m : 2.32639
[1mStep[0m  [264/339], [94mLoss[0m : 2.35225
[1mStep[0m  [297/339], [94mLoss[0m : 2.08052
[1mStep[0m  [330/339], [94mLoss[0m : 1.72014

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.341, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.315
====================================

Phase 1 - Evaluation MAE:  2.3153350416537934
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.74504
[1mStep[0m  [33/339], [94mLoss[0m : 2.33042
[1mStep[0m  [66/339], [94mLoss[0m : 2.64637
[1mStep[0m  [99/339], [94mLoss[0m : 2.27203
[1mStep[0m  [132/339], [94mLoss[0m : 2.52184
[1mStep[0m  [165/339], [94mLoss[0m : 2.31578
[1mStep[0m  [198/339], [94mLoss[0m : 2.93726
[1mStep[0m  [231/339], [94mLoss[0m : 2.44487
[1mStep[0m  [264/339], [94mLoss[0m : 2.69240
[1mStep[0m  [297/339], [94mLoss[0m : 2.05840
[1mStep[0m  [330/339], [94mLoss[0m : 2.41776

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.314, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23898
[1mStep[0m  [33/339], [94mLoss[0m : 2.26493
[1mStep[0m  [66/339], [94mLoss[0m : 3.03947
[1mStep[0m  [99/339], [94mLoss[0m : 2.58601
[1mStep[0m  [132/339], [94mLoss[0m : 2.01982
[1mStep[0m  [165/339], [94mLoss[0m : 3.62911
[1mStep[0m  [198/339], [94mLoss[0m : 2.87076
[1mStep[0m  [231/339], [94mLoss[0m : 1.77255
[1mStep[0m  [264/339], [94mLoss[0m : 1.91454
[1mStep[0m  [297/339], [94mLoss[0m : 2.14118
[1mStep[0m  [330/339], [94mLoss[0m : 2.01586

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.384, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07355
[1mStep[0m  [33/339], [94mLoss[0m : 2.27981
[1mStep[0m  [66/339], [94mLoss[0m : 2.05151
[1mStep[0m  [99/339], [94mLoss[0m : 2.38808
[1mStep[0m  [132/339], [94mLoss[0m : 2.06996
[1mStep[0m  [165/339], [94mLoss[0m : 2.18411
[1mStep[0m  [198/339], [94mLoss[0m : 2.78664
[1mStep[0m  [231/339], [94mLoss[0m : 1.64849
[1mStep[0m  [264/339], [94mLoss[0m : 2.62213
[1mStep[0m  [297/339], [94mLoss[0m : 2.17131
[1mStep[0m  [330/339], [94mLoss[0m : 2.58713

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.363, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31559
[1mStep[0m  [33/339], [94mLoss[0m : 2.50351
[1mStep[0m  [66/339], [94mLoss[0m : 2.11206
[1mStep[0m  [99/339], [94mLoss[0m : 2.92009
[1mStep[0m  [132/339], [94mLoss[0m : 2.36019
[1mStep[0m  [165/339], [94mLoss[0m : 1.56598
[1mStep[0m  [198/339], [94mLoss[0m : 2.26103
[1mStep[0m  [231/339], [94mLoss[0m : 2.42193
[1mStep[0m  [264/339], [94mLoss[0m : 2.25330
[1mStep[0m  [297/339], [94mLoss[0m : 2.09429
[1mStep[0m  [330/339], [94mLoss[0m : 2.07946

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.361, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47733
[1mStep[0m  [33/339], [94mLoss[0m : 1.92382
[1mStep[0m  [66/339], [94mLoss[0m : 1.65702
[1mStep[0m  [99/339], [94mLoss[0m : 2.40572
[1mStep[0m  [132/339], [94mLoss[0m : 1.62979
[1mStep[0m  [165/339], [94mLoss[0m : 2.13711
[1mStep[0m  [198/339], [94mLoss[0m : 1.84772
[1mStep[0m  [231/339], [94mLoss[0m : 2.15326
[1mStep[0m  [264/339], [94mLoss[0m : 1.94626
[1mStep[0m  [297/339], [94mLoss[0m : 2.85818
[1mStep[0m  [330/339], [94mLoss[0m : 3.38147

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.210, [92mTest[0m: 2.357, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42178
[1mStep[0m  [33/339], [94mLoss[0m : 2.58178
[1mStep[0m  [66/339], [94mLoss[0m : 2.39697
[1mStep[0m  [99/339], [94mLoss[0m : 2.27626
[1mStep[0m  [132/339], [94mLoss[0m : 2.18209
[1mStep[0m  [165/339], [94mLoss[0m : 2.15522
[1mStep[0m  [198/339], [94mLoss[0m : 2.61720
[1mStep[0m  [231/339], [94mLoss[0m : 1.86730
[1mStep[0m  [264/339], [94mLoss[0m : 2.80323
[1mStep[0m  [297/339], [94mLoss[0m : 2.43342
[1mStep[0m  [330/339], [94mLoss[0m : 2.56890

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.375, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27066
[1mStep[0m  [33/339], [94mLoss[0m : 1.99403
[1mStep[0m  [66/339], [94mLoss[0m : 2.13259
[1mStep[0m  [99/339], [94mLoss[0m : 2.68113
[1mStep[0m  [132/339], [94mLoss[0m : 2.35733
[1mStep[0m  [165/339], [94mLoss[0m : 1.92244
[1mStep[0m  [198/339], [94mLoss[0m : 1.98361
[1mStep[0m  [231/339], [94mLoss[0m : 1.49056
[1mStep[0m  [264/339], [94mLoss[0m : 2.13021
[1mStep[0m  [297/339], [94mLoss[0m : 1.69451
[1mStep[0m  [330/339], [94mLoss[0m : 2.27366

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.397, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70257
[1mStep[0m  [33/339], [94mLoss[0m : 1.95282
[1mStep[0m  [66/339], [94mLoss[0m : 2.07301
[1mStep[0m  [99/339], [94mLoss[0m : 2.24825
[1mStep[0m  [132/339], [94mLoss[0m : 2.57759
[1mStep[0m  [165/339], [94mLoss[0m : 2.61332
[1mStep[0m  [198/339], [94mLoss[0m : 2.79678
[1mStep[0m  [231/339], [94mLoss[0m : 1.70234
[1mStep[0m  [264/339], [94mLoss[0m : 1.80505
[1mStep[0m  [297/339], [94mLoss[0m : 1.90082
[1mStep[0m  [330/339], [94mLoss[0m : 2.11411

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.402, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94843
[1mStep[0m  [33/339], [94mLoss[0m : 1.52161
[1mStep[0m  [66/339], [94mLoss[0m : 1.84567
[1mStep[0m  [99/339], [94mLoss[0m : 1.91105
[1mStep[0m  [132/339], [94mLoss[0m : 2.44773
[1mStep[0m  [165/339], [94mLoss[0m : 1.86768
[1mStep[0m  [198/339], [94mLoss[0m : 2.55705
[1mStep[0m  [231/339], [94mLoss[0m : 1.35765
[1mStep[0m  [264/339], [94mLoss[0m : 2.22782
[1mStep[0m  [297/339], [94mLoss[0m : 2.13478
[1mStep[0m  [330/339], [94mLoss[0m : 2.43070

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.399, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60518
[1mStep[0m  [33/339], [94mLoss[0m : 2.08204
[1mStep[0m  [66/339], [94mLoss[0m : 1.72516
[1mStep[0m  [99/339], [94mLoss[0m : 2.14133
[1mStep[0m  [132/339], [94mLoss[0m : 1.33120
[1mStep[0m  [165/339], [94mLoss[0m : 1.83312
[1mStep[0m  [198/339], [94mLoss[0m : 2.03379
[1mStep[0m  [231/339], [94mLoss[0m : 2.16549
[1mStep[0m  [264/339], [94mLoss[0m : 1.78927
[1mStep[0m  [297/339], [94mLoss[0m : 2.51751
[1mStep[0m  [330/339], [94mLoss[0m : 2.47352

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.447, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16394
[1mStep[0m  [33/339], [94mLoss[0m : 1.66998
[1mStep[0m  [66/339], [94mLoss[0m : 2.07143
[1mStep[0m  [99/339], [94mLoss[0m : 1.64603
[1mStep[0m  [132/339], [94mLoss[0m : 2.39782
[1mStep[0m  [165/339], [94mLoss[0m : 1.74961
[1mStep[0m  [198/339], [94mLoss[0m : 1.93644
[1mStep[0m  [231/339], [94mLoss[0m : 1.73705
[1mStep[0m  [264/339], [94mLoss[0m : 2.10418
[1mStep[0m  [297/339], [94mLoss[0m : 1.91848
[1mStep[0m  [330/339], [94mLoss[0m : 2.39950

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.449, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16735
[1mStep[0m  [33/339], [94mLoss[0m : 1.65670
[1mStep[0m  [66/339], [94mLoss[0m : 2.29450
[1mStep[0m  [99/339], [94mLoss[0m : 2.17502
[1mStep[0m  [132/339], [94mLoss[0m : 1.90914
[1mStep[0m  [165/339], [94mLoss[0m : 1.67999
[1mStep[0m  [198/339], [94mLoss[0m : 2.04191
[1mStep[0m  [231/339], [94mLoss[0m : 2.32166
[1mStep[0m  [264/339], [94mLoss[0m : 1.86385
[1mStep[0m  [297/339], [94mLoss[0m : 2.25966
[1mStep[0m  [330/339], [94mLoss[0m : 1.86120

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.446, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78689
[1mStep[0m  [33/339], [94mLoss[0m : 1.95142
[1mStep[0m  [66/339], [94mLoss[0m : 2.05945
[1mStep[0m  [99/339], [94mLoss[0m : 1.20727
[1mStep[0m  [132/339], [94mLoss[0m : 2.27847
[1mStep[0m  [165/339], [94mLoss[0m : 1.58173
[1mStep[0m  [198/339], [94mLoss[0m : 1.83127
[1mStep[0m  [231/339], [94mLoss[0m : 2.00777
[1mStep[0m  [264/339], [94mLoss[0m : 1.95615
[1mStep[0m  [297/339], [94mLoss[0m : 2.05386
[1mStep[0m  [330/339], [94mLoss[0m : 2.30121

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.484, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82619
[1mStep[0m  [33/339], [94mLoss[0m : 2.41036
[1mStep[0m  [66/339], [94mLoss[0m : 2.76442
[1mStep[0m  [99/339], [94mLoss[0m : 1.99403
[1mStep[0m  [132/339], [94mLoss[0m : 1.81730
[1mStep[0m  [165/339], [94mLoss[0m : 1.09982
[1mStep[0m  [198/339], [94mLoss[0m : 1.98562
[1mStep[0m  [231/339], [94mLoss[0m : 1.92412
[1mStep[0m  [264/339], [94mLoss[0m : 1.77114
[1mStep[0m  [297/339], [94mLoss[0m : 1.99385
[1mStep[0m  [330/339], [94mLoss[0m : 2.34889

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.458, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70037
[1mStep[0m  [33/339], [94mLoss[0m : 1.75302
[1mStep[0m  [66/339], [94mLoss[0m : 2.16968
[1mStep[0m  [99/339], [94mLoss[0m : 1.49609
[1mStep[0m  [132/339], [94mLoss[0m : 1.55066
[1mStep[0m  [165/339], [94mLoss[0m : 2.26256
[1mStep[0m  [198/339], [94mLoss[0m : 2.01612
[1mStep[0m  [231/339], [94mLoss[0m : 1.83951
[1mStep[0m  [264/339], [94mLoss[0m : 1.35688
[1mStep[0m  [297/339], [94mLoss[0m : 1.94303
[1mStep[0m  [330/339], [94mLoss[0m : 1.88779

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.516, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87787
[1mStep[0m  [33/339], [94mLoss[0m : 1.81656
[1mStep[0m  [66/339], [94mLoss[0m : 1.72753
[1mStep[0m  [99/339], [94mLoss[0m : 2.01671
[1mStep[0m  [132/339], [94mLoss[0m : 1.84722
[1mStep[0m  [165/339], [94mLoss[0m : 2.07774
[1mStep[0m  [198/339], [94mLoss[0m : 1.66219
[1mStep[0m  [231/339], [94mLoss[0m : 1.65516
[1mStep[0m  [264/339], [94mLoss[0m : 2.46907
[1mStep[0m  [297/339], [94mLoss[0m : 2.70469
[1mStep[0m  [330/339], [94mLoss[0m : 1.95810

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.505, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48202
[1mStep[0m  [33/339], [94mLoss[0m : 1.61907
[1mStep[0m  [66/339], [94mLoss[0m : 1.71636
[1mStep[0m  [99/339], [94mLoss[0m : 1.34965
[1mStep[0m  [132/339], [94mLoss[0m : 1.62044
[1mStep[0m  [165/339], [94mLoss[0m : 1.31826
[1mStep[0m  [198/339], [94mLoss[0m : 1.55543
[1mStep[0m  [231/339], [94mLoss[0m : 1.77195
[1mStep[0m  [264/339], [94mLoss[0m : 1.35156
[1mStep[0m  [297/339], [94mLoss[0m : 1.46911
[1mStep[0m  [330/339], [94mLoss[0m : 1.86221

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.486, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35559
[1mStep[0m  [33/339], [94mLoss[0m : 1.71244
[1mStep[0m  [66/339], [94mLoss[0m : 1.58681
[1mStep[0m  [99/339], [94mLoss[0m : 1.60072
[1mStep[0m  [132/339], [94mLoss[0m : 1.86086
[1mStep[0m  [165/339], [94mLoss[0m : 2.66937
[1mStep[0m  [198/339], [94mLoss[0m : 1.29987
[1mStep[0m  [231/339], [94mLoss[0m : 1.44500
[1mStep[0m  [264/339], [94mLoss[0m : 1.82482
[1mStep[0m  [297/339], [94mLoss[0m : 1.74905
[1mStep[0m  [330/339], [94mLoss[0m : 1.85330

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.500, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.25571
[1mStep[0m  [33/339], [94mLoss[0m : 1.93567
[1mStep[0m  [66/339], [94mLoss[0m : 1.98061
[1mStep[0m  [99/339], [94mLoss[0m : 1.50252
[1mStep[0m  [132/339], [94mLoss[0m : 1.66539
[1mStep[0m  [165/339], [94mLoss[0m : 2.24389
[1mStep[0m  [198/339], [94mLoss[0m : 2.31559
[1mStep[0m  [231/339], [94mLoss[0m : 1.46735
[1mStep[0m  [264/339], [94mLoss[0m : 1.79169
[1mStep[0m  [297/339], [94mLoss[0m : 1.57946
[1mStep[0m  [330/339], [94mLoss[0m : 1.91016

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.477, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46719
[1mStep[0m  [33/339], [94mLoss[0m : 1.44059
[1mStep[0m  [66/339], [94mLoss[0m : 1.59780
[1mStep[0m  [99/339], [94mLoss[0m : 1.56532
[1mStep[0m  [132/339], [94mLoss[0m : 1.60223
[1mStep[0m  [165/339], [94mLoss[0m : 1.51242
[1mStep[0m  [198/339], [94mLoss[0m : 1.51801
[1mStep[0m  [231/339], [94mLoss[0m : 1.56616
[1mStep[0m  [264/339], [94mLoss[0m : 2.28224
[1mStep[0m  [297/339], [94mLoss[0m : 1.60444
[1mStep[0m  [330/339], [94mLoss[0m : 1.54669

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.530, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93973
[1mStep[0m  [33/339], [94mLoss[0m : 1.37764
[1mStep[0m  [66/339], [94mLoss[0m : 1.48116
[1mStep[0m  [99/339], [94mLoss[0m : 1.68173
[1mStep[0m  [132/339], [94mLoss[0m : 1.63821
[1mStep[0m  [165/339], [94mLoss[0m : 1.93991
[1mStep[0m  [198/339], [94mLoss[0m : 2.01220
[1mStep[0m  [231/339], [94mLoss[0m : 1.61186
[1mStep[0m  [264/339], [94mLoss[0m : 1.82129
[1mStep[0m  [297/339], [94mLoss[0m : 1.27590
[1mStep[0m  [330/339], [94mLoss[0m : 1.67426

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.492, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40483
[1mStep[0m  [33/339], [94mLoss[0m : 1.41544
[1mStep[0m  [66/339], [94mLoss[0m : 1.67588
[1mStep[0m  [99/339], [94mLoss[0m : 1.74018
[1mStep[0m  [132/339], [94mLoss[0m : 1.67455
[1mStep[0m  [165/339], [94mLoss[0m : 1.59573
[1mStep[0m  [198/339], [94mLoss[0m : 1.90249
[1mStep[0m  [231/339], [94mLoss[0m : 1.14747
[1mStep[0m  [264/339], [94mLoss[0m : 1.57866
[1mStep[0m  [297/339], [94mLoss[0m : 1.70641
[1mStep[0m  [330/339], [94mLoss[0m : 1.93563

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.562, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.29601
[1mStep[0m  [33/339], [94mLoss[0m : 1.28781
[1mStep[0m  [66/339], [94mLoss[0m : 1.90247
[1mStep[0m  [99/339], [94mLoss[0m : 1.73488
[1mStep[0m  [132/339], [94mLoss[0m : 1.93654
[1mStep[0m  [165/339], [94mLoss[0m : 1.34542
[1mStep[0m  [198/339], [94mLoss[0m : 1.27164
[1mStep[0m  [231/339], [94mLoss[0m : 1.49252
[1mStep[0m  [264/339], [94mLoss[0m : 1.21460
[1mStep[0m  [297/339], [94mLoss[0m : 2.08316
[1mStep[0m  [330/339], [94mLoss[0m : 1.55047

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.694, [92mTest[0m: 2.563, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.22090
[1mStep[0m  [33/339], [94mLoss[0m : 1.83082
[1mStep[0m  [66/339], [94mLoss[0m : 1.76959
[1mStep[0m  [99/339], [94mLoss[0m : 1.83688
[1mStep[0m  [132/339], [94mLoss[0m : 1.64385
[1mStep[0m  [165/339], [94mLoss[0m : 1.38164
[1mStep[0m  [198/339], [94mLoss[0m : 1.61092
[1mStep[0m  [231/339], [94mLoss[0m : 1.60787
[1mStep[0m  [264/339], [94mLoss[0m : 1.86593
[1mStep[0m  [297/339], [94mLoss[0m : 1.56105
[1mStep[0m  [330/339], [94mLoss[0m : 1.40862

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.501, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52161
[1mStep[0m  [33/339], [94mLoss[0m : 1.74093
[1mStep[0m  [66/339], [94mLoss[0m : 1.35547
[1mStep[0m  [99/339], [94mLoss[0m : 1.74572
[1mStep[0m  [132/339], [94mLoss[0m : 2.03755
[1mStep[0m  [165/339], [94mLoss[0m : 1.45393
[1mStep[0m  [198/339], [94mLoss[0m : 1.83629
[1mStep[0m  [231/339], [94mLoss[0m : 1.77540
[1mStep[0m  [264/339], [94mLoss[0m : 1.66051
[1mStep[0m  [297/339], [94mLoss[0m : 2.29828
[1mStep[0m  [330/339], [94mLoss[0m : 1.76598

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.501, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72276
[1mStep[0m  [33/339], [94mLoss[0m : 1.49928
[1mStep[0m  [66/339], [94mLoss[0m : 1.51487
[1mStep[0m  [99/339], [94mLoss[0m : 1.58319
[1mStep[0m  [132/339], [94mLoss[0m : 1.09208
[1mStep[0m  [165/339], [94mLoss[0m : 1.81832
[1mStep[0m  [198/339], [94mLoss[0m : 1.26367
[1mStep[0m  [231/339], [94mLoss[0m : 1.43768
[1mStep[0m  [264/339], [94mLoss[0m : 1.08192
[1mStep[0m  [297/339], [94mLoss[0m : 2.08760
[1mStep[0m  [330/339], [94mLoss[0m : 1.49304

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.542, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68655
[1mStep[0m  [33/339], [94mLoss[0m : 1.89159
[1mStep[0m  [66/339], [94mLoss[0m : 1.67186
[1mStep[0m  [99/339], [94mLoss[0m : 1.69356
[1mStep[0m  [132/339], [94mLoss[0m : 1.47003
[1mStep[0m  [165/339], [94mLoss[0m : 1.84025
[1mStep[0m  [198/339], [94mLoss[0m : 1.79857
[1mStep[0m  [231/339], [94mLoss[0m : 1.54353
[1mStep[0m  [264/339], [94mLoss[0m : 1.97192
[1mStep[0m  [297/339], [94mLoss[0m : 1.93936
[1mStep[0m  [330/339], [94mLoss[0m : 1.70889

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.634, [92mTest[0m: 2.563, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66297
[1mStep[0m  [33/339], [94mLoss[0m : 1.38135
[1mStep[0m  [66/339], [94mLoss[0m : 1.48248
[1mStep[0m  [99/339], [94mLoss[0m : 1.32450
[1mStep[0m  [132/339], [94mLoss[0m : 1.62896
[1mStep[0m  [165/339], [94mLoss[0m : 1.35781
[1mStep[0m  [198/339], [94mLoss[0m : 1.85788
[1mStep[0m  [231/339], [94mLoss[0m : 1.86879
[1mStep[0m  [264/339], [94mLoss[0m : 1.33085
[1mStep[0m  [297/339], [94mLoss[0m : 2.17696
[1mStep[0m  [330/339], [94mLoss[0m : 1.51543

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.506, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41844
[1mStep[0m  [33/339], [94mLoss[0m : 1.24964
[1mStep[0m  [66/339], [94mLoss[0m : 1.41629
[1mStep[0m  [99/339], [94mLoss[0m : 1.56596
[1mStep[0m  [132/339], [94mLoss[0m : 1.81056
[1mStep[0m  [165/339], [94mLoss[0m : 1.33276
[1mStep[0m  [198/339], [94mLoss[0m : 1.77455
[1mStep[0m  [231/339], [94mLoss[0m : 1.48035
[1mStep[0m  [264/339], [94mLoss[0m : 1.73802
[1mStep[0m  [297/339], [94mLoss[0m : 1.51975
[1mStep[0m  [330/339], [94mLoss[0m : 1.45944

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.540, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.08123
[1mStep[0m  [33/339], [94mLoss[0m : 1.60918
[1mStep[0m  [66/339], [94mLoss[0m : 1.54623
[1mStep[0m  [99/339], [94mLoss[0m : 1.28438
[1mStep[0m  [132/339], [94mLoss[0m : 1.91465
[1mStep[0m  [165/339], [94mLoss[0m : 1.66012
[1mStep[0m  [198/339], [94mLoss[0m : 1.38846
[1mStep[0m  [231/339], [94mLoss[0m : 1.77178
[1mStep[0m  [264/339], [94mLoss[0m : 1.64089
[1mStep[0m  [297/339], [94mLoss[0m : 1.39506
[1mStep[0m  [330/339], [94mLoss[0m : 1.71149

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.548, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.586
====================================

Phase 2 - Evaluation MAE:  2.586207421480027
MAE score P1       2.315335
MAE score P2       2.586207
loss               1.577691
learning_rate       0.00505
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay          0.001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.91623
[1mStep[0m  [2/21], [94mLoss[0m : 10.83376
[1mStep[0m  [4/21], [94mLoss[0m : 10.64488
[1mStep[0m  [6/21], [94mLoss[0m : 10.69648
[1mStep[0m  [8/21], [94mLoss[0m : 10.58074
[1mStep[0m  [10/21], [94mLoss[0m : 10.82653
[1mStep[0m  [12/21], [94mLoss[0m : 10.92412
[1mStep[0m  [14/21], [94mLoss[0m : 10.20352
[1mStep[0m  [16/21], [94mLoss[0m : 10.85186
[1mStep[0m  [18/21], [94mLoss[0m : 10.68217
[1mStep[0m  [20/21], [94mLoss[0m : 10.74010

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.804, [92mTest[0m: 10.949, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77310
[1mStep[0m  [2/21], [94mLoss[0m : 11.01986
[1mStep[0m  [4/21], [94mLoss[0m : 10.62788
[1mStep[0m  [6/21], [94mLoss[0m : 10.79338
[1mStep[0m  [8/21], [94mLoss[0m : 10.86547
[1mStep[0m  [10/21], [94mLoss[0m : 11.01416
[1mStep[0m  [12/21], [94mLoss[0m : 10.42846
[1mStep[0m  [14/21], [94mLoss[0m : 10.72752
[1mStep[0m  [16/21], [94mLoss[0m : 10.80869
[1mStep[0m  [18/21], [94mLoss[0m : 10.57848
[1mStep[0m  [20/21], [94mLoss[0m : 10.67666

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.724, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90781
[1mStep[0m  [2/21], [94mLoss[0m : 10.76199
[1mStep[0m  [4/21], [94mLoss[0m : 10.54692
[1mStep[0m  [6/21], [94mLoss[0m : 10.75194
[1mStep[0m  [8/21], [94mLoss[0m : 10.49283
[1mStep[0m  [10/21], [94mLoss[0m : 10.63415
[1mStep[0m  [12/21], [94mLoss[0m : 10.83586
[1mStep[0m  [14/21], [94mLoss[0m : 10.40557
[1mStep[0m  [16/21], [94mLoss[0m : 10.53404
[1mStep[0m  [18/21], [94mLoss[0m : 10.76582
[1mStep[0m  [20/21], [94mLoss[0m : 10.58242

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.611, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74843
[1mStep[0m  [2/21], [94mLoss[0m : 10.51873
[1mStep[0m  [4/21], [94mLoss[0m : 10.81643
[1mStep[0m  [6/21], [94mLoss[0m : 10.29011
[1mStep[0m  [8/21], [94mLoss[0m : 10.58054
[1mStep[0m  [10/21], [94mLoss[0m : 10.58774
[1mStep[0m  [12/21], [94mLoss[0m : 10.74367
[1mStep[0m  [14/21], [94mLoss[0m : 10.75615
[1mStep[0m  [16/21], [94mLoss[0m : 10.49531
[1mStep[0m  [18/21], [94mLoss[0m : 10.63434
[1mStep[0m  [20/21], [94mLoss[0m : 10.37130

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.590, [92mTest[0m: 10.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68515
[1mStep[0m  [2/21], [94mLoss[0m : 10.55984
[1mStep[0m  [4/21], [94mLoss[0m : 10.47478
[1mStep[0m  [6/21], [94mLoss[0m : 10.63110
[1mStep[0m  [8/21], [94mLoss[0m : 10.10659
[1mStep[0m  [10/21], [94mLoss[0m : 10.38551
[1mStep[0m  [12/21], [94mLoss[0m : 10.44044
[1mStep[0m  [14/21], [94mLoss[0m : 10.37412
[1mStep[0m  [16/21], [94mLoss[0m : 10.23776
[1mStep[0m  [18/21], [94mLoss[0m : 10.73163
[1mStep[0m  [20/21], [94mLoss[0m : 10.43536

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.508, [92mTest[0m: 10.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33150
[1mStep[0m  [2/21], [94mLoss[0m : 10.47016
[1mStep[0m  [4/21], [94mLoss[0m : 10.61119
[1mStep[0m  [6/21], [94mLoss[0m : 10.41660
[1mStep[0m  [8/21], [94mLoss[0m : 10.24977
[1mStep[0m  [10/21], [94mLoss[0m : 10.42803
[1mStep[0m  [12/21], [94mLoss[0m : 10.20632
[1mStep[0m  [14/21], [94mLoss[0m : 10.15411
[1mStep[0m  [16/21], [94mLoss[0m : 10.39346
[1mStep[0m  [18/21], [94mLoss[0m : 10.61449
[1mStep[0m  [20/21], [94mLoss[0m : 10.19741

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.431, [92mTest[0m: 10.317, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.24577
[1mStep[0m  [2/21], [94mLoss[0m : 10.37798
[1mStep[0m  [4/21], [94mLoss[0m : 10.71266
[1mStep[0m  [6/21], [94mLoss[0m : 10.54931
[1mStep[0m  [8/21], [94mLoss[0m : 10.37967
[1mStep[0m  [10/21], [94mLoss[0m : 10.51336
[1mStep[0m  [12/21], [94mLoss[0m : 10.13254
[1mStep[0m  [14/21], [94mLoss[0m : 10.16538
[1mStep[0m  [16/21], [94mLoss[0m : 10.10536
[1mStep[0m  [18/21], [94mLoss[0m : 10.05898
[1mStep[0m  [20/21], [94mLoss[0m : 10.48991

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.350, [92mTest[0m: 10.182, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12489
[1mStep[0m  [2/21], [94mLoss[0m : 10.02582
[1mStep[0m  [4/21], [94mLoss[0m : 10.44317
[1mStep[0m  [6/21], [94mLoss[0m : 10.51889
[1mStep[0m  [8/21], [94mLoss[0m : 10.39061
[1mStep[0m  [10/21], [94mLoss[0m : 10.26131
[1mStep[0m  [12/21], [94mLoss[0m : 10.26915
[1mStep[0m  [14/21], [94mLoss[0m : 10.08761
[1mStep[0m  [16/21], [94mLoss[0m : 10.09787
[1mStep[0m  [18/21], [94mLoss[0m : 10.27200
[1mStep[0m  [20/21], [94mLoss[0m : 10.44609

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.277, [92mTest[0m: 10.087, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.14578
[1mStep[0m  [2/21], [94mLoss[0m : 10.39200
[1mStep[0m  [4/21], [94mLoss[0m : 10.29638
[1mStep[0m  [6/21], [94mLoss[0m : 10.35319
[1mStep[0m  [8/21], [94mLoss[0m : 10.39310
[1mStep[0m  [10/21], [94mLoss[0m : 10.14307
[1mStep[0m  [12/21], [94mLoss[0m : 9.92218
[1mStep[0m  [14/21], [94mLoss[0m : 10.06714
[1mStep[0m  [16/21], [94mLoss[0m : 9.99805
[1mStep[0m  [18/21], [94mLoss[0m : 10.29281
[1mStep[0m  [20/21], [94mLoss[0m : 10.15957

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.192, [92mTest[0m: 9.978, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39828
[1mStep[0m  [2/21], [94mLoss[0m : 9.97050
[1mStep[0m  [4/21], [94mLoss[0m : 10.06084
[1mStep[0m  [6/21], [94mLoss[0m : 10.30632
[1mStep[0m  [8/21], [94mLoss[0m : 10.21003
[1mStep[0m  [10/21], [94mLoss[0m : 9.73813
[1mStep[0m  [12/21], [94mLoss[0m : 10.09315
[1mStep[0m  [14/21], [94mLoss[0m : 10.08799
[1mStep[0m  [16/21], [94mLoss[0m : 10.28518
[1mStep[0m  [18/21], [94mLoss[0m : 9.97334
[1mStep[0m  [20/21], [94mLoss[0m : 9.88765

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.107, [92mTest[0m: 9.872, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.71685
[1mStep[0m  [2/21], [94mLoss[0m : 10.03766
[1mStep[0m  [4/21], [94mLoss[0m : 10.12350
[1mStep[0m  [6/21], [94mLoss[0m : 10.11854
[1mStep[0m  [8/21], [94mLoss[0m : 9.95200
[1mStep[0m  [10/21], [94mLoss[0m : 10.17996
[1mStep[0m  [12/21], [94mLoss[0m : 10.08286
[1mStep[0m  [14/21], [94mLoss[0m : 9.92817
[1mStep[0m  [16/21], [94mLoss[0m : 9.92965
[1mStep[0m  [18/21], [94mLoss[0m : 10.01946
[1mStep[0m  [20/21], [94mLoss[0m : 10.02277

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.013, [92mTest[0m: 9.756, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.85006
[1mStep[0m  [2/21], [94mLoss[0m : 9.88092
[1mStep[0m  [4/21], [94mLoss[0m : 9.97478
[1mStep[0m  [6/21], [94mLoss[0m : 9.97095
[1mStep[0m  [8/21], [94mLoss[0m : 9.95150
[1mStep[0m  [10/21], [94mLoss[0m : 10.10553
[1mStep[0m  [12/21], [94mLoss[0m : 9.89232
[1mStep[0m  [14/21], [94mLoss[0m : 10.07965
[1mStep[0m  [16/21], [94mLoss[0m : 9.81121
[1mStep[0m  [18/21], [94mLoss[0m : 9.74539
[1mStep[0m  [20/21], [94mLoss[0m : 9.96943

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.931, [92mTest[0m: 9.649, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.99277
[1mStep[0m  [2/21], [94mLoss[0m : 10.01083
[1mStep[0m  [4/21], [94mLoss[0m : 9.92102
[1mStep[0m  [6/21], [94mLoss[0m : 9.77122
[1mStep[0m  [8/21], [94mLoss[0m : 9.85336
[1mStep[0m  [10/21], [94mLoss[0m : 9.61891
[1mStep[0m  [12/21], [94mLoss[0m : 9.78008
[1mStep[0m  [14/21], [94mLoss[0m : 9.81175
[1mStep[0m  [16/21], [94mLoss[0m : 9.82850
[1mStep[0m  [18/21], [94mLoss[0m : 9.80598
[1mStep[0m  [20/21], [94mLoss[0m : 9.83111

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.842, [92mTest[0m: 9.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.96423
[1mStep[0m  [2/21], [94mLoss[0m : 9.73034
[1mStep[0m  [4/21], [94mLoss[0m : 9.79058
[1mStep[0m  [6/21], [94mLoss[0m : 9.63127
[1mStep[0m  [8/21], [94mLoss[0m : 9.68234
[1mStep[0m  [10/21], [94mLoss[0m : 9.79778
[1mStep[0m  [12/21], [94mLoss[0m : 9.70586
[1mStep[0m  [14/21], [94mLoss[0m : 9.72726
[1mStep[0m  [16/21], [94mLoss[0m : 9.57121
[1mStep[0m  [18/21], [94mLoss[0m : 9.73577
[1mStep[0m  [20/21], [94mLoss[0m : 9.60344

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.748, [92mTest[0m: 9.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.74770
[1mStep[0m  [2/21], [94mLoss[0m : 9.72610
[1mStep[0m  [4/21], [94mLoss[0m : 9.58646
[1mStep[0m  [6/21], [94mLoss[0m : 9.86314
[1mStep[0m  [8/21], [94mLoss[0m : 9.49707
[1mStep[0m  [10/21], [94mLoss[0m : 9.47270
[1mStep[0m  [12/21], [94mLoss[0m : 9.80671
[1mStep[0m  [14/21], [94mLoss[0m : 9.65248
[1mStep[0m  [16/21], [94mLoss[0m : 9.86796
[1mStep[0m  [18/21], [94mLoss[0m : 9.61999
[1mStep[0m  [20/21], [94mLoss[0m : 9.66229

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.656, [92mTest[0m: 9.313, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.74117
[1mStep[0m  [2/21], [94mLoss[0m : 9.37917
[1mStep[0m  [4/21], [94mLoss[0m : 9.65617
[1mStep[0m  [6/21], [94mLoss[0m : 10.08103
[1mStep[0m  [8/21], [94mLoss[0m : 9.46484
[1mStep[0m  [10/21], [94mLoss[0m : 9.47218
[1mStep[0m  [12/21], [94mLoss[0m : 9.63375
[1mStep[0m  [14/21], [94mLoss[0m : 9.35105
[1mStep[0m  [16/21], [94mLoss[0m : 9.24223
[1mStep[0m  [18/21], [94mLoss[0m : 9.55809
[1mStep[0m  [20/21], [94mLoss[0m : 9.61176

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.561, [92mTest[0m: 9.180, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.47964
[1mStep[0m  [2/21], [94mLoss[0m : 9.73802
[1mStep[0m  [4/21], [94mLoss[0m : 9.76651
[1mStep[0m  [6/21], [94mLoss[0m : 9.43153
[1mStep[0m  [8/21], [94mLoss[0m : 9.43932
[1mStep[0m  [10/21], [94mLoss[0m : 9.60155
[1mStep[0m  [12/21], [94mLoss[0m : 9.39501
[1mStep[0m  [14/21], [94mLoss[0m : 9.39926
[1mStep[0m  [16/21], [94mLoss[0m : 9.40728
[1mStep[0m  [18/21], [94mLoss[0m : 9.38457
[1mStep[0m  [20/21], [94mLoss[0m : 9.22865

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.464, [92mTest[0m: 9.065, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.48359
[1mStep[0m  [2/21], [94mLoss[0m : 9.29415
[1mStep[0m  [4/21], [94mLoss[0m : 9.76114
[1mStep[0m  [6/21], [94mLoss[0m : 9.33875
[1mStep[0m  [8/21], [94mLoss[0m : 9.21631
[1mStep[0m  [10/21], [94mLoss[0m : 9.40659
[1mStep[0m  [12/21], [94mLoss[0m : 9.65508
[1mStep[0m  [14/21], [94mLoss[0m : 8.96024
[1mStep[0m  [16/21], [94mLoss[0m : 9.34175
[1mStep[0m  [18/21], [94mLoss[0m : 9.18692
[1mStep[0m  [20/21], [94mLoss[0m : 9.50360

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.355, [92mTest[0m: 8.896, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.26224
[1mStep[0m  [2/21], [94mLoss[0m : 9.19397
[1mStep[0m  [4/21], [94mLoss[0m : 9.35073
[1mStep[0m  [6/21], [94mLoss[0m : 9.31770
[1mStep[0m  [8/21], [94mLoss[0m : 9.42727
[1mStep[0m  [10/21], [94mLoss[0m : 9.36308
[1mStep[0m  [12/21], [94mLoss[0m : 9.23412
[1mStep[0m  [14/21], [94mLoss[0m : 9.27987
[1mStep[0m  [16/21], [94mLoss[0m : 9.22061
[1mStep[0m  [18/21], [94mLoss[0m : 9.08303
[1mStep[0m  [20/21], [94mLoss[0m : 9.26259

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.243, [92mTest[0m: 8.784, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.13715
[1mStep[0m  [2/21], [94mLoss[0m : 9.05319
[1mStep[0m  [4/21], [94mLoss[0m : 8.95147
[1mStep[0m  [6/21], [94mLoss[0m : 9.08523
[1mStep[0m  [8/21], [94mLoss[0m : 9.36194
[1mStep[0m  [10/21], [94mLoss[0m : 9.10393
[1mStep[0m  [12/21], [94mLoss[0m : 9.14638
[1mStep[0m  [14/21], [94mLoss[0m : 9.11599
[1mStep[0m  [16/21], [94mLoss[0m : 9.09423
[1mStep[0m  [18/21], [94mLoss[0m : 9.02071
[1mStep[0m  [20/21], [94mLoss[0m : 9.04081

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.130, [92mTest[0m: 8.648, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.99010
[1mStep[0m  [2/21], [94mLoss[0m : 9.14827
[1mStep[0m  [4/21], [94mLoss[0m : 9.01778
[1mStep[0m  [6/21], [94mLoss[0m : 9.15478
[1mStep[0m  [8/21], [94mLoss[0m : 8.98416
[1mStep[0m  [10/21], [94mLoss[0m : 9.12350
[1mStep[0m  [12/21], [94mLoss[0m : 8.98569
[1mStep[0m  [14/21], [94mLoss[0m : 9.07419
[1mStep[0m  [16/21], [94mLoss[0m : 9.04820
[1mStep[0m  [18/21], [94mLoss[0m : 8.76647
[1mStep[0m  [20/21], [94mLoss[0m : 8.95796

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.021, [92mTest[0m: 8.486, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.06688
[1mStep[0m  [2/21], [94mLoss[0m : 8.88815
[1mStep[0m  [4/21], [94mLoss[0m : 8.92800
[1mStep[0m  [6/21], [94mLoss[0m : 9.08667
[1mStep[0m  [8/21], [94mLoss[0m : 8.85562
[1mStep[0m  [10/21], [94mLoss[0m : 9.12453
[1mStep[0m  [12/21], [94mLoss[0m : 8.80082
[1mStep[0m  [14/21], [94mLoss[0m : 8.80706
[1mStep[0m  [16/21], [94mLoss[0m : 8.85341
[1mStep[0m  [18/21], [94mLoss[0m : 8.65632
[1mStep[0m  [20/21], [94mLoss[0m : 9.07741

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.904, [92mTest[0m: 8.375, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.05749
[1mStep[0m  [2/21], [94mLoss[0m : 8.80790
[1mStep[0m  [4/21], [94mLoss[0m : 9.02088
[1mStep[0m  [6/21], [94mLoss[0m : 8.90029
[1mStep[0m  [8/21], [94mLoss[0m : 8.79477
[1mStep[0m  [10/21], [94mLoss[0m : 8.58947
[1mStep[0m  [12/21], [94mLoss[0m : 8.78993
[1mStep[0m  [14/21], [94mLoss[0m : 8.72848
[1mStep[0m  [16/21], [94mLoss[0m : 8.67558
[1mStep[0m  [18/21], [94mLoss[0m : 8.54405
[1mStep[0m  [20/21], [94mLoss[0m : 8.61741

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.786, [92mTest[0m: 8.238, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.01531
[1mStep[0m  [2/21], [94mLoss[0m : 8.54311
[1mStep[0m  [4/21], [94mLoss[0m : 8.82071
[1mStep[0m  [6/21], [94mLoss[0m : 8.88595
[1mStep[0m  [8/21], [94mLoss[0m : 8.75689
[1mStep[0m  [10/21], [94mLoss[0m : 8.84063
[1mStep[0m  [12/21], [94mLoss[0m : 8.81537
[1mStep[0m  [14/21], [94mLoss[0m : 8.78414
[1mStep[0m  [16/21], [94mLoss[0m : 8.45232
[1mStep[0m  [18/21], [94mLoss[0m : 8.39679
[1mStep[0m  [20/21], [94mLoss[0m : 8.70169

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.673, [92mTest[0m: 8.081, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.61453
[1mStep[0m  [2/21], [94mLoss[0m : 8.65635
[1mStep[0m  [4/21], [94mLoss[0m : 8.52548
[1mStep[0m  [6/21], [94mLoss[0m : 8.42504
[1mStep[0m  [8/21], [94mLoss[0m : 8.45660
[1mStep[0m  [10/21], [94mLoss[0m : 8.41236
[1mStep[0m  [12/21], [94mLoss[0m : 8.44733
[1mStep[0m  [14/21], [94mLoss[0m : 8.30970
[1mStep[0m  [16/21], [94mLoss[0m : 8.74123
[1mStep[0m  [18/21], [94mLoss[0m : 8.81808
[1mStep[0m  [20/21], [94mLoss[0m : 8.48083

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.544, [92mTest[0m: 7.931, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.41966
[1mStep[0m  [2/21], [94mLoss[0m : 8.67348
[1mStep[0m  [4/21], [94mLoss[0m : 8.37258
[1mStep[0m  [6/21], [94mLoss[0m : 8.44269
[1mStep[0m  [8/21], [94mLoss[0m : 8.48582
[1mStep[0m  [10/21], [94mLoss[0m : 8.31569
[1mStep[0m  [12/21], [94mLoss[0m : 8.54690
[1mStep[0m  [14/21], [94mLoss[0m : 8.31162
[1mStep[0m  [16/21], [94mLoss[0m : 8.39700
[1mStep[0m  [18/21], [94mLoss[0m : 8.45752
[1mStep[0m  [20/21], [94mLoss[0m : 8.33391

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.430, [92mTest[0m: 7.777, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.13575
[1mStep[0m  [2/21], [94mLoss[0m : 8.17743
[1mStep[0m  [4/21], [94mLoss[0m : 8.12894
[1mStep[0m  [6/21], [94mLoss[0m : 8.19179
[1mStep[0m  [8/21], [94mLoss[0m : 8.45542
[1mStep[0m  [10/21], [94mLoss[0m : 8.02983
[1mStep[0m  [12/21], [94mLoss[0m : 8.21996
[1mStep[0m  [14/21], [94mLoss[0m : 8.36508
[1mStep[0m  [16/21], [94mLoss[0m : 8.08357
[1mStep[0m  [18/21], [94mLoss[0m : 8.23446
[1mStep[0m  [20/21], [94mLoss[0m : 8.27872

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.294, [92mTest[0m: 7.656, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.94180
[1mStep[0m  [2/21], [94mLoss[0m : 8.31025
[1mStep[0m  [4/21], [94mLoss[0m : 8.15687
[1mStep[0m  [6/21], [94mLoss[0m : 8.15624
[1mStep[0m  [8/21], [94mLoss[0m : 8.17772
[1mStep[0m  [10/21], [94mLoss[0m : 8.08490
[1mStep[0m  [12/21], [94mLoss[0m : 8.15026
[1mStep[0m  [14/21], [94mLoss[0m : 8.45003
[1mStep[0m  [16/21], [94mLoss[0m : 8.13019
[1mStep[0m  [18/21], [94mLoss[0m : 7.85915
[1mStep[0m  [20/21], [94mLoss[0m : 7.95800

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.151, [92mTest[0m: 7.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.98803
[1mStep[0m  [2/21], [94mLoss[0m : 8.00805
[1mStep[0m  [4/21], [94mLoss[0m : 8.27147
[1mStep[0m  [6/21], [94mLoss[0m : 8.05407
[1mStep[0m  [8/21], [94mLoss[0m : 7.92293
[1mStep[0m  [10/21], [94mLoss[0m : 8.03142
[1mStep[0m  [12/21], [94mLoss[0m : 7.93688
[1mStep[0m  [14/21], [94mLoss[0m : 8.07412
[1mStep[0m  [16/21], [94mLoss[0m : 7.61857
[1mStep[0m  [18/21], [94mLoss[0m : 7.83657
[1mStep[0m  [20/21], [94mLoss[0m : 7.90093

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.022, [92mTest[0m: 7.278, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.82613
[1mStep[0m  [2/21], [94mLoss[0m : 7.93449
[1mStep[0m  [4/21], [94mLoss[0m : 7.99621
[1mStep[0m  [6/21], [94mLoss[0m : 7.99187
[1mStep[0m  [8/21], [94mLoss[0m : 7.93362
[1mStep[0m  [10/21], [94mLoss[0m : 7.81198
[1mStep[0m  [12/21], [94mLoss[0m : 7.93674
[1mStep[0m  [14/21], [94mLoss[0m : 7.68138
[1mStep[0m  [16/21], [94mLoss[0m : 7.96299
[1mStep[0m  [18/21], [94mLoss[0m : 7.93018
[1mStep[0m  [20/21], [94mLoss[0m : 7.86432

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.887, [92mTest[0m: 7.169, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.021
====================================

Phase 1 - Evaluation MAE:  7.020676953451974
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 8.05963
[1mStep[0m  [2/21], [94mLoss[0m : 7.82861
[1mStep[0m  [4/21], [94mLoss[0m : 7.58437
[1mStep[0m  [6/21], [94mLoss[0m : 7.94283
[1mStep[0m  [8/21], [94mLoss[0m : 7.77720
[1mStep[0m  [10/21], [94mLoss[0m : 7.74600
[1mStep[0m  [12/21], [94mLoss[0m : 7.76072
[1mStep[0m  [14/21], [94mLoss[0m : 7.89752
[1mStep[0m  [16/21], [94mLoss[0m : 7.60203
[1mStep[0m  [18/21], [94mLoss[0m : 7.49711
[1mStep[0m  [20/21], [94mLoss[0m : 7.59184

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.716, [92mTest[0m: 7.013, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.86481
[1mStep[0m  [2/21], [94mLoss[0m : 7.63489
[1mStep[0m  [4/21], [94mLoss[0m : 7.47634
[1mStep[0m  [6/21], [94mLoss[0m : 7.40921
[1mStep[0m  [8/21], [94mLoss[0m : 7.34359
[1mStep[0m  [10/21], [94mLoss[0m : 7.56797
[1mStep[0m  [12/21], [94mLoss[0m : 7.48728
[1mStep[0m  [14/21], [94mLoss[0m : 7.56379
[1mStep[0m  [16/21], [94mLoss[0m : 7.50768
[1mStep[0m  [18/21], [94mLoss[0m : 7.60317
[1mStep[0m  [20/21], [94mLoss[0m : 7.39335

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.548, [92mTest[0m: 6.759, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.24466
[1mStep[0m  [2/21], [94mLoss[0m : 7.30388
[1mStep[0m  [4/21], [94mLoss[0m : 7.32508
[1mStep[0m  [6/21], [94mLoss[0m : 7.61753
[1mStep[0m  [8/21], [94mLoss[0m : 7.54588
[1mStep[0m  [10/21], [94mLoss[0m : 7.66787
[1mStep[0m  [12/21], [94mLoss[0m : 7.34352
[1mStep[0m  [14/21], [94mLoss[0m : 7.21514
[1mStep[0m  [16/21], [94mLoss[0m : 7.20945
[1mStep[0m  [18/21], [94mLoss[0m : 7.25827
[1mStep[0m  [20/21], [94mLoss[0m : 7.31304

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.361, [92mTest[0m: 6.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.47542
[1mStep[0m  [2/21], [94mLoss[0m : 7.17460
[1mStep[0m  [4/21], [94mLoss[0m : 6.96676
[1mStep[0m  [6/21], [94mLoss[0m : 7.43769
[1mStep[0m  [8/21], [94mLoss[0m : 7.31089
[1mStep[0m  [10/21], [94mLoss[0m : 7.40503
[1mStep[0m  [12/21], [94mLoss[0m : 7.12610
[1mStep[0m  [14/21], [94mLoss[0m : 7.29455
[1mStep[0m  [16/21], [94mLoss[0m : 6.95472
[1mStep[0m  [18/21], [94mLoss[0m : 7.10433
[1mStep[0m  [20/21], [94mLoss[0m : 7.22978

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.209, [92mTest[0m: 6.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.86622
[1mStep[0m  [2/21], [94mLoss[0m : 6.92598
[1mStep[0m  [4/21], [94mLoss[0m : 6.93829
[1mStep[0m  [6/21], [94mLoss[0m : 7.16835
[1mStep[0m  [8/21], [94mLoss[0m : 6.98860
[1mStep[0m  [10/21], [94mLoss[0m : 7.37598
[1mStep[0m  [12/21], [94mLoss[0m : 6.93722
[1mStep[0m  [14/21], [94mLoss[0m : 7.09681
[1mStep[0m  [16/21], [94mLoss[0m : 6.81015
[1mStep[0m  [18/21], [94mLoss[0m : 7.08337
[1mStep[0m  [20/21], [94mLoss[0m : 6.89903

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.055, [92mTest[0m: 7.057, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.14484
[1mStep[0m  [2/21], [94mLoss[0m : 6.85288
[1mStep[0m  [4/21], [94mLoss[0m : 6.99836
[1mStep[0m  [6/21], [94mLoss[0m : 6.98218
[1mStep[0m  [8/21], [94mLoss[0m : 7.05519
[1mStep[0m  [10/21], [94mLoss[0m : 6.89824
[1mStep[0m  [12/21], [94mLoss[0m : 6.86305
[1mStep[0m  [14/21], [94mLoss[0m : 7.05232
[1mStep[0m  [16/21], [94mLoss[0m : 6.73027
[1mStep[0m  [18/21], [94mLoss[0m : 6.90924
[1mStep[0m  [20/21], [94mLoss[0m : 6.80833

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.919, [92mTest[0m: 6.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.72430
[1mStep[0m  [2/21], [94mLoss[0m : 6.83082
[1mStep[0m  [4/21], [94mLoss[0m : 6.78370
[1mStep[0m  [6/21], [94mLoss[0m : 6.85364
[1mStep[0m  [8/21], [94mLoss[0m : 6.92453
[1mStep[0m  [10/21], [94mLoss[0m : 6.61618
[1mStep[0m  [12/21], [94mLoss[0m : 6.42083
[1mStep[0m  [14/21], [94mLoss[0m : 6.68982
[1mStep[0m  [16/21], [94mLoss[0m : 7.11380
[1mStep[0m  [18/21], [94mLoss[0m : 6.48852
[1mStep[0m  [20/21], [94mLoss[0m : 6.99576

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.756, [92mTest[0m: 6.243, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.71506
[1mStep[0m  [2/21], [94mLoss[0m : 6.59365
[1mStep[0m  [4/21], [94mLoss[0m : 6.81320
[1mStep[0m  [6/21], [94mLoss[0m : 6.62054
[1mStep[0m  [8/21], [94mLoss[0m : 6.23282
[1mStep[0m  [10/21], [94mLoss[0m : 6.67484
[1mStep[0m  [12/21], [94mLoss[0m : 6.66862
[1mStep[0m  [14/21], [94mLoss[0m : 6.49352
[1mStep[0m  [16/21], [94mLoss[0m : 6.58483
[1mStep[0m  [18/21], [94mLoss[0m : 6.43247
[1mStep[0m  [20/21], [94mLoss[0m : 6.41379

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.584, [92mTest[0m: 6.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.55024
[1mStep[0m  [2/21], [94mLoss[0m : 6.11591
[1mStep[0m  [4/21], [94mLoss[0m : 6.21728
[1mStep[0m  [6/21], [94mLoss[0m : 6.46122
[1mStep[0m  [8/21], [94mLoss[0m : 6.47893
[1mStep[0m  [10/21], [94mLoss[0m : 6.38956
[1mStep[0m  [12/21], [94mLoss[0m : 6.23827
[1mStep[0m  [14/21], [94mLoss[0m : 6.54376
[1mStep[0m  [16/21], [94mLoss[0m : 6.54965
[1mStep[0m  [18/21], [94mLoss[0m : 6.50483
[1mStep[0m  [20/21], [94mLoss[0m : 6.38941

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.425, [92mTest[0m: 5.998, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.33025
[1mStep[0m  [2/21], [94mLoss[0m : 6.29277
[1mStep[0m  [4/21], [94mLoss[0m : 6.23332
[1mStep[0m  [6/21], [94mLoss[0m : 6.24626
[1mStep[0m  [8/21], [94mLoss[0m : 6.07376
[1mStep[0m  [10/21], [94mLoss[0m : 6.27380
[1mStep[0m  [12/21], [94mLoss[0m : 6.20040
[1mStep[0m  [14/21], [94mLoss[0m : 6.24833
[1mStep[0m  [16/21], [94mLoss[0m : 6.18198
[1mStep[0m  [18/21], [94mLoss[0m : 6.09644
[1mStep[0m  [20/21], [94mLoss[0m : 6.40384

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.256, [92mTest[0m: 5.570, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.03299
[1mStep[0m  [2/21], [94mLoss[0m : 6.17669
[1mStep[0m  [4/21], [94mLoss[0m : 6.40966
[1mStep[0m  [6/21], [94mLoss[0m : 6.10779
[1mStep[0m  [8/21], [94mLoss[0m : 6.14915
[1mStep[0m  [10/21], [94mLoss[0m : 6.12343
[1mStep[0m  [12/21], [94mLoss[0m : 5.93728
[1mStep[0m  [14/21], [94mLoss[0m : 6.02087
[1mStep[0m  [16/21], [94mLoss[0m : 6.07487
[1mStep[0m  [18/21], [94mLoss[0m : 5.90247
[1mStep[0m  [20/21], [94mLoss[0m : 5.81813

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.084, [92mTest[0m: 5.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.87695
[1mStep[0m  [2/21], [94mLoss[0m : 5.80714
[1mStep[0m  [4/21], [94mLoss[0m : 5.97910
[1mStep[0m  [6/21], [94mLoss[0m : 6.05140
[1mStep[0m  [8/21], [94mLoss[0m : 5.97908
[1mStep[0m  [10/21], [94mLoss[0m : 5.74045
[1mStep[0m  [12/21], [94mLoss[0m : 5.97102
[1mStep[0m  [14/21], [94mLoss[0m : 5.52130
[1mStep[0m  [16/21], [94mLoss[0m : 6.07648
[1mStep[0m  [18/21], [94mLoss[0m : 5.74875
[1mStep[0m  [20/21], [94mLoss[0m : 5.98234

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.896, [92mTest[0m: 5.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.05593
[1mStep[0m  [2/21], [94mLoss[0m : 5.75640
[1mStep[0m  [4/21], [94mLoss[0m : 5.73463
[1mStep[0m  [6/21], [94mLoss[0m : 5.59615
[1mStep[0m  [8/21], [94mLoss[0m : 5.67567
[1mStep[0m  [10/21], [94mLoss[0m : 5.59380
[1mStep[0m  [12/21], [94mLoss[0m : 5.70185
[1mStep[0m  [14/21], [94mLoss[0m : 5.65722
[1mStep[0m  [16/21], [94mLoss[0m : 5.64337
[1mStep[0m  [18/21], [94mLoss[0m : 5.59832
[1mStep[0m  [20/21], [94mLoss[0m : 5.58225

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.709, [92mTest[0m: 5.081, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.55705
[1mStep[0m  [2/21], [94mLoss[0m : 5.73324
[1mStep[0m  [4/21], [94mLoss[0m : 5.43051
[1mStep[0m  [6/21], [94mLoss[0m : 5.27890
[1mStep[0m  [8/21], [94mLoss[0m : 5.42904
[1mStep[0m  [10/21], [94mLoss[0m : 5.53208
[1mStep[0m  [12/21], [94mLoss[0m : 5.49501
[1mStep[0m  [14/21], [94mLoss[0m : 5.45307
[1mStep[0m  [16/21], [94mLoss[0m : 5.61531
[1mStep[0m  [18/21], [94mLoss[0m : 5.43870
[1mStep[0m  [20/21], [94mLoss[0m : 5.45473

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.511, [92mTest[0m: 4.981, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.41149
[1mStep[0m  [2/21], [94mLoss[0m : 5.21904
[1mStep[0m  [4/21], [94mLoss[0m : 5.21918
[1mStep[0m  [6/21], [94mLoss[0m : 5.50692
[1mStep[0m  [8/21], [94mLoss[0m : 5.33657
[1mStep[0m  [10/21], [94mLoss[0m : 5.37745
[1mStep[0m  [12/21], [94mLoss[0m : 5.40802
[1mStep[0m  [14/21], [94mLoss[0m : 5.13445
[1mStep[0m  [16/21], [94mLoss[0m : 5.42229
[1mStep[0m  [18/21], [94mLoss[0m : 5.36865
[1mStep[0m  [20/21], [94mLoss[0m : 5.30957

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.333, [92mTest[0m: 4.696, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.21942
[1mStep[0m  [2/21], [94mLoss[0m : 5.15781
[1mStep[0m  [4/21], [94mLoss[0m : 5.24064
[1mStep[0m  [6/21], [94mLoss[0m : 5.19512
[1mStep[0m  [8/21], [94mLoss[0m : 5.32059
[1mStep[0m  [10/21], [94mLoss[0m : 5.11898
[1mStep[0m  [12/21], [94mLoss[0m : 5.05464
[1mStep[0m  [14/21], [94mLoss[0m : 4.91038
[1mStep[0m  [16/21], [94mLoss[0m : 5.20756
[1mStep[0m  [18/21], [94mLoss[0m : 5.10890
[1mStep[0m  [20/21], [94mLoss[0m : 4.79913

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.130, [92mTest[0m: 4.600, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.98144
[1mStep[0m  [2/21], [94mLoss[0m : 5.03331
[1mStep[0m  [4/21], [94mLoss[0m : 5.10495
[1mStep[0m  [6/21], [94mLoss[0m : 4.92573
[1mStep[0m  [8/21], [94mLoss[0m : 5.02270
[1mStep[0m  [10/21], [94mLoss[0m : 5.23092
[1mStep[0m  [12/21], [94mLoss[0m : 4.87708
[1mStep[0m  [14/21], [94mLoss[0m : 4.85241
[1mStep[0m  [16/21], [94mLoss[0m : 4.86379
[1mStep[0m  [18/21], [94mLoss[0m : 4.88185
[1mStep[0m  [20/21], [94mLoss[0m : 4.94988

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.943, [92mTest[0m: 4.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.65187
[1mStep[0m  [2/21], [94mLoss[0m : 5.15666
[1mStep[0m  [4/21], [94mLoss[0m : 4.48676
[1mStep[0m  [6/21], [94mLoss[0m : 4.77294
[1mStep[0m  [8/21], [94mLoss[0m : 4.64730
[1mStep[0m  [10/21], [94mLoss[0m : 5.06835
[1mStep[0m  [12/21], [94mLoss[0m : 4.66508
[1mStep[0m  [14/21], [94mLoss[0m : 4.52558
[1mStep[0m  [16/21], [94mLoss[0m : 4.93743
[1mStep[0m  [18/21], [94mLoss[0m : 4.36478
[1mStep[0m  [20/21], [94mLoss[0m : 4.67951

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.756, [92mTest[0m: 4.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.64968
[1mStep[0m  [2/21], [94mLoss[0m : 4.67815
[1mStep[0m  [4/21], [94mLoss[0m : 4.43388
[1mStep[0m  [6/21], [94mLoss[0m : 4.77984
[1mStep[0m  [8/21], [94mLoss[0m : 4.65405
[1mStep[0m  [10/21], [94mLoss[0m : 4.73221
[1mStep[0m  [12/21], [94mLoss[0m : 4.56176
[1mStep[0m  [14/21], [94mLoss[0m : 4.58935
[1mStep[0m  [16/21], [94mLoss[0m : 4.45216
[1mStep[0m  [18/21], [94mLoss[0m : 4.45282
[1mStep[0m  [20/21], [94mLoss[0m : 4.72716

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.617, [92mTest[0m: 4.314, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.50887
[1mStep[0m  [2/21], [94mLoss[0m : 4.68709
[1mStep[0m  [4/21], [94mLoss[0m : 4.61850
[1mStep[0m  [6/21], [94mLoss[0m : 4.35943
[1mStep[0m  [8/21], [94mLoss[0m : 4.37625
[1mStep[0m  [10/21], [94mLoss[0m : 4.34922
[1mStep[0m  [12/21], [94mLoss[0m : 4.47455
[1mStep[0m  [14/21], [94mLoss[0m : 4.48945
[1mStep[0m  [16/21], [94mLoss[0m : 4.38128
[1mStep[0m  [18/21], [94mLoss[0m : 4.39093
[1mStep[0m  [20/21], [94mLoss[0m : 4.28344

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.447, [92mTest[0m: 4.165, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.44927
[1mStep[0m  [2/21], [94mLoss[0m : 4.06006
[1mStep[0m  [4/21], [94mLoss[0m : 4.14861
[1mStep[0m  [6/21], [94mLoss[0m : 4.21233
[1mStep[0m  [8/21], [94mLoss[0m : 4.39941
[1mStep[0m  [10/21], [94mLoss[0m : 4.29210
[1mStep[0m  [12/21], [94mLoss[0m : 4.30046
[1mStep[0m  [14/21], [94mLoss[0m : 4.46493
[1mStep[0m  [16/21], [94mLoss[0m : 4.32754
[1mStep[0m  [18/21], [94mLoss[0m : 4.34479
[1mStep[0m  [20/21], [94mLoss[0m : 4.44236

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.271, [92mTest[0m: 4.040, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.03284
[1mStep[0m  [2/21], [94mLoss[0m : 4.05835
[1mStep[0m  [4/21], [94mLoss[0m : 4.08758
[1mStep[0m  [6/21], [94mLoss[0m : 4.14621
[1mStep[0m  [8/21], [94mLoss[0m : 3.99863
[1mStep[0m  [10/21], [94mLoss[0m : 4.14519
[1mStep[0m  [12/21], [94mLoss[0m : 4.12479
[1mStep[0m  [14/21], [94mLoss[0m : 4.24980
[1mStep[0m  [16/21], [94mLoss[0m : 4.24909
[1mStep[0m  [18/21], [94mLoss[0m : 4.15865
[1mStep[0m  [20/21], [94mLoss[0m : 4.05402

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.175, [92mTest[0m: 3.945, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.34936
[1mStep[0m  [2/21], [94mLoss[0m : 4.34023
[1mStep[0m  [4/21], [94mLoss[0m : 4.14726
[1mStep[0m  [6/21], [94mLoss[0m : 4.20727
[1mStep[0m  [8/21], [94mLoss[0m : 3.95256
[1mStep[0m  [10/21], [94mLoss[0m : 4.42439
[1mStep[0m  [12/21], [94mLoss[0m : 3.89526
[1mStep[0m  [14/21], [94mLoss[0m : 3.87598
[1mStep[0m  [16/21], [94mLoss[0m : 4.05911
[1mStep[0m  [18/21], [94mLoss[0m : 4.02089
[1mStep[0m  [20/21], [94mLoss[0m : 4.12897

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.050, [92mTest[0m: 3.711, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.96758
[1mStep[0m  [2/21], [94mLoss[0m : 4.05967
[1mStep[0m  [4/21], [94mLoss[0m : 3.78916
[1mStep[0m  [6/21], [94mLoss[0m : 3.83055
[1mStep[0m  [8/21], [94mLoss[0m : 3.95532
[1mStep[0m  [10/21], [94mLoss[0m : 3.72424
[1mStep[0m  [12/21], [94mLoss[0m : 3.86065
[1mStep[0m  [14/21], [94mLoss[0m : 3.83148
[1mStep[0m  [16/21], [94mLoss[0m : 4.05249
[1mStep[0m  [18/21], [94mLoss[0m : 4.01203
[1mStep[0m  [20/21], [94mLoss[0m : 3.87203

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.937, [92mTest[0m: 3.682, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.88220
[1mStep[0m  [2/21], [94mLoss[0m : 3.85705
[1mStep[0m  [4/21], [94mLoss[0m : 3.93144
[1mStep[0m  [6/21], [94mLoss[0m : 3.96024
[1mStep[0m  [8/21], [94mLoss[0m : 3.74767
[1mStep[0m  [10/21], [94mLoss[0m : 3.80929
[1mStep[0m  [12/21], [94mLoss[0m : 3.82737
[1mStep[0m  [14/21], [94mLoss[0m : 3.91173
[1mStep[0m  [16/21], [94mLoss[0m : 3.95407
[1mStep[0m  [18/21], [94mLoss[0m : 3.88743
[1mStep[0m  [20/21], [94mLoss[0m : 3.80033

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.844, [92mTest[0m: 3.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.66587
[1mStep[0m  [2/21], [94mLoss[0m : 3.86436
[1mStep[0m  [4/21], [94mLoss[0m : 3.81980
[1mStep[0m  [6/21], [94mLoss[0m : 3.76332
[1mStep[0m  [8/21], [94mLoss[0m : 3.49168
[1mStep[0m  [10/21], [94mLoss[0m : 3.54225
[1mStep[0m  [12/21], [94mLoss[0m : 3.60506
[1mStep[0m  [14/21], [94mLoss[0m : 3.55502
[1mStep[0m  [16/21], [94mLoss[0m : 3.71617
[1mStep[0m  [18/21], [94mLoss[0m : 3.76287
[1mStep[0m  [20/21], [94mLoss[0m : 3.61908

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.688, [92mTest[0m: 3.485, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.85188
[1mStep[0m  [2/21], [94mLoss[0m : 3.59930
[1mStep[0m  [4/21], [94mLoss[0m : 3.74173
[1mStep[0m  [6/21], [94mLoss[0m : 3.65052
[1mStep[0m  [8/21], [94mLoss[0m : 3.65966
[1mStep[0m  [10/21], [94mLoss[0m : 3.67968
[1mStep[0m  [12/21], [94mLoss[0m : 3.63511
[1mStep[0m  [14/21], [94mLoss[0m : 3.64888
[1mStep[0m  [16/21], [94mLoss[0m : 3.65591
[1mStep[0m  [18/21], [94mLoss[0m : 3.60632
[1mStep[0m  [20/21], [94mLoss[0m : 3.39538

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.615, [92mTest[0m: 3.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.58113
[1mStep[0m  [2/21], [94mLoss[0m : 3.50462
[1mStep[0m  [4/21], [94mLoss[0m : 3.47241
[1mStep[0m  [6/21], [94mLoss[0m : 3.49451
[1mStep[0m  [8/21], [94mLoss[0m : 3.63718
[1mStep[0m  [10/21], [94mLoss[0m : 3.77184
[1mStep[0m  [12/21], [94mLoss[0m : 3.47041
[1mStep[0m  [14/21], [94mLoss[0m : 3.44437
[1mStep[0m  [16/21], [94mLoss[0m : 3.53534
[1mStep[0m  [18/21], [94mLoss[0m : 3.58014
[1mStep[0m  [20/21], [94mLoss[0m : 3.30448

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.538, [92mTest[0m: 3.292, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.47388
[1mStep[0m  [2/21], [94mLoss[0m : 3.48604
[1mStep[0m  [4/21], [94mLoss[0m : 3.39262
[1mStep[0m  [6/21], [94mLoss[0m : 3.54194
[1mStep[0m  [8/21], [94mLoss[0m : 3.43074
[1mStep[0m  [10/21], [94mLoss[0m : 3.47233
[1mStep[0m  [12/21], [94mLoss[0m : 3.36179
[1mStep[0m  [14/21], [94mLoss[0m : 3.88668
[1mStep[0m  [16/21], [94mLoss[0m : 3.29616
[1mStep[0m  [18/21], [94mLoss[0m : 3.17062
[1mStep[0m  [20/21], [94mLoss[0m : 3.41565

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.437, [92mTest[0m: 3.222, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.45458
[1mStep[0m  [2/21], [94mLoss[0m : 3.31424
[1mStep[0m  [4/21], [94mLoss[0m : 3.42953
[1mStep[0m  [6/21], [94mLoss[0m : 3.20934
[1mStep[0m  [8/21], [94mLoss[0m : 3.26853
[1mStep[0m  [10/21], [94mLoss[0m : 3.47382
[1mStep[0m  [12/21], [94mLoss[0m : 3.42038
[1mStep[0m  [14/21], [94mLoss[0m : 3.35910
[1mStep[0m  [16/21], [94mLoss[0m : 3.53516
[1mStep[0m  [18/21], [94mLoss[0m : 3.38516
[1mStep[0m  [20/21], [94mLoss[0m : 3.38718

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.370, [92mTest[0m: 3.282, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.141
====================================

Phase 2 - Evaluation MAE:  3.1412405967712402
MAE score P1        7.020677
MAE score P2        3.141241
loss                3.369763
learning_rate       0.002575
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay          0.0001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.09242
[1mStep[0m  [2/21], [94mLoss[0m : 10.93549
[1mStep[0m  [4/21], [94mLoss[0m : 10.92094
[1mStep[0m  [6/21], [94mLoss[0m : 10.53216
[1mStep[0m  [8/21], [94mLoss[0m : 10.82427
[1mStep[0m  [10/21], [94mLoss[0m : 10.91958
[1mStep[0m  [12/21], [94mLoss[0m : 10.49160
[1mStep[0m  [14/21], [94mLoss[0m : 10.79530
[1mStep[0m  [16/21], [94mLoss[0m : 10.73977
[1mStep[0m  [18/21], [94mLoss[0m : 10.63853
[1mStep[0m  [20/21], [94mLoss[0m : 10.39162

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.734, [92mTest[0m: 10.970, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40944
[1mStep[0m  [2/21], [94mLoss[0m : 10.51041
[1mStep[0m  [4/21], [94mLoss[0m : 10.29013
[1mStep[0m  [6/21], [94mLoss[0m : 10.10151
[1mStep[0m  [8/21], [94mLoss[0m : 10.42726
[1mStep[0m  [10/21], [94mLoss[0m : 10.16569
[1mStep[0m  [12/21], [94mLoss[0m : 10.37265
[1mStep[0m  [14/21], [94mLoss[0m : 10.15140
[1mStep[0m  [16/21], [94mLoss[0m : 10.04634
[1mStep[0m  [18/21], [94mLoss[0m : 9.88784
[1mStep[0m  [20/21], [94mLoss[0m : 9.88223

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.213, [92mTest[0m: 10.765, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.01505
[1mStep[0m  [2/21], [94mLoss[0m : 9.65568
[1mStep[0m  [4/21], [94mLoss[0m : 10.02564
[1mStep[0m  [6/21], [94mLoss[0m : 9.89994
[1mStep[0m  [8/21], [94mLoss[0m : 9.71788
[1mStep[0m  [10/21], [94mLoss[0m : 9.70186
[1mStep[0m  [12/21], [94mLoss[0m : 9.63266
[1mStep[0m  [14/21], [94mLoss[0m : 9.29858
[1mStep[0m  [16/21], [94mLoss[0m : 9.82589
[1mStep[0m  [18/21], [94mLoss[0m : 9.30384
[1mStep[0m  [20/21], [94mLoss[0m : 9.55054

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.704, [92mTest[0m: 10.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.56433
[1mStep[0m  [2/21], [94mLoss[0m : 9.20659
[1mStep[0m  [4/21], [94mLoss[0m : 9.38091
[1mStep[0m  [6/21], [94mLoss[0m : 9.05456
[1mStep[0m  [8/21], [94mLoss[0m : 9.22430
[1mStep[0m  [10/21], [94mLoss[0m : 9.13326
[1mStep[0m  [12/21], [94mLoss[0m : 9.23555
[1mStep[0m  [14/21], [94mLoss[0m : 9.00109
[1mStep[0m  [16/21], [94mLoss[0m : 8.95391
[1mStep[0m  [18/21], [94mLoss[0m : 9.13763
[1mStep[0m  [20/21], [94mLoss[0m : 9.14583

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.195, [92mTest[0m: 9.995, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.91422
[1mStep[0m  [2/21], [94mLoss[0m : 8.79410
[1mStep[0m  [4/21], [94mLoss[0m : 8.95810
[1mStep[0m  [6/21], [94mLoss[0m : 8.83242
[1mStep[0m  [8/21], [94mLoss[0m : 8.72610
[1mStep[0m  [10/21], [94mLoss[0m : 8.91399
[1mStep[0m  [12/21], [94mLoss[0m : 8.66268
[1mStep[0m  [14/21], [94mLoss[0m : 8.36083
[1mStep[0m  [16/21], [94mLoss[0m : 8.39678
[1mStep[0m  [18/21], [94mLoss[0m : 8.40931
[1mStep[0m  [20/21], [94mLoss[0m : 8.13923

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.659, [92mTest[0m: 9.668, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.34799
[1mStep[0m  [2/21], [94mLoss[0m : 8.43024
[1mStep[0m  [4/21], [94mLoss[0m : 8.44592
[1mStep[0m  [6/21], [94mLoss[0m : 8.27527
[1mStep[0m  [8/21], [94mLoss[0m : 8.26654
[1mStep[0m  [10/21], [94mLoss[0m : 8.44022
[1mStep[0m  [12/21], [94mLoss[0m : 8.32161
[1mStep[0m  [14/21], [94mLoss[0m : 7.79329
[1mStep[0m  [16/21], [94mLoss[0m : 7.62419
[1mStep[0m  [18/21], [94mLoss[0m : 7.86369
[1mStep[0m  [20/21], [94mLoss[0m : 8.00063

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.122, [92mTest[0m: 9.313, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.91521
[1mStep[0m  [2/21], [94mLoss[0m : 7.93208
[1mStep[0m  [4/21], [94mLoss[0m : 7.72720
[1mStep[0m  [6/21], [94mLoss[0m : 7.65832
[1mStep[0m  [8/21], [94mLoss[0m : 7.64429
[1mStep[0m  [10/21], [94mLoss[0m : 7.83017
[1mStep[0m  [12/21], [94mLoss[0m : 7.21786
[1mStep[0m  [14/21], [94mLoss[0m : 7.39542
[1mStep[0m  [16/21], [94mLoss[0m : 7.81263
[1mStep[0m  [18/21], [94mLoss[0m : 7.23697
[1mStep[0m  [20/21], [94mLoss[0m : 7.63505

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.613, [92mTest[0m: 8.962, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.74685
[1mStep[0m  [2/21], [94mLoss[0m : 7.20773
[1mStep[0m  [4/21], [94mLoss[0m : 7.42754
[1mStep[0m  [6/21], [94mLoss[0m : 7.08924
[1mStep[0m  [8/21], [94mLoss[0m : 7.24960
[1mStep[0m  [10/21], [94mLoss[0m : 7.04692
[1mStep[0m  [12/21], [94mLoss[0m : 6.83591
[1mStep[0m  [14/21], [94mLoss[0m : 7.24168
[1mStep[0m  [16/21], [94mLoss[0m : 7.14625
[1mStep[0m  [18/21], [94mLoss[0m : 6.87370
[1mStep[0m  [20/21], [94mLoss[0m : 6.35682

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.054, [92mTest[0m: 8.584, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.72489
[1mStep[0m  [2/21], [94mLoss[0m : 6.83082
[1mStep[0m  [4/21], [94mLoss[0m : 6.76272
[1mStep[0m  [6/21], [94mLoss[0m : 6.56783
[1mStep[0m  [8/21], [94mLoss[0m : 6.51646
[1mStep[0m  [10/21], [94mLoss[0m : 6.41169
[1mStep[0m  [12/21], [94mLoss[0m : 6.23274
[1mStep[0m  [14/21], [94mLoss[0m : 6.28477
[1mStep[0m  [16/21], [94mLoss[0m : 6.38780
[1mStep[0m  [18/21], [94mLoss[0m : 6.31234
[1mStep[0m  [20/21], [94mLoss[0m : 6.70854

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.517, [92mTest[0m: 8.175, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.32308
[1mStep[0m  [2/21], [94mLoss[0m : 6.04571
[1mStep[0m  [4/21], [94mLoss[0m : 6.08094
[1mStep[0m  [6/21], [94mLoss[0m : 6.07163
[1mStep[0m  [8/21], [94mLoss[0m : 5.89995
[1mStep[0m  [10/21], [94mLoss[0m : 5.81817
[1mStep[0m  [12/21], [94mLoss[0m : 6.06884
[1mStep[0m  [14/21], [94mLoss[0m : 6.14052
[1mStep[0m  [16/21], [94mLoss[0m : 5.64006
[1mStep[0m  [18/21], [94mLoss[0m : 5.69511
[1mStep[0m  [20/21], [94mLoss[0m : 5.83915

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.967, [92mTest[0m: 7.728, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 9 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.208
====================================

Phase 1 - Evaluation MAE:  7.207697187151227
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 5.55803
[1mStep[0m  [2/21], [94mLoss[0m : 5.56655
[1mStep[0m  [4/21], [94mLoss[0m : 5.70690
[1mStep[0m  [6/21], [94mLoss[0m : 5.61151
[1mStep[0m  [8/21], [94mLoss[0m : 5.55707
[1mStep[0m  [10/21], [94mLoss[0m : 5.36180
[1mStep[0m  [12/21], [94mLoss[0m : 5.14399
[1mStep[0m  [14/21], [94mLoss[0m : 5.29621
[1mStep[0m  [16/21], [94mLoss[0m : 5.29194
[1mStep[0m  [18/21], [94mLoss[0m : 5.17619
[1mStep[0m  [20/21], [94mLoss[0m : 5.11211

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.437, [92mTest[0m: 7.232, [96mlr[0m: 0.002575
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.626
====================================

Phase 2 - Evaluation MAE:  7.62594359261649
MAE score P1       7.207697
MAE score P2       7.625944
loss               5.437328
learning_rate      0.002575
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.63663
[1mStep[0m  [33/339], [94mLoss[0m : 6.25723
[1mStep[0m  [66/339], [94mLoss[0m : 3.43735
[1mStep[0m  [99/339], [94mLoss[0m : 2.60220
[1mStep[0m  [132/339], [94mLoss[0m : 3.13445
[1mStep[0m  [165/339], [94mLoss[0m : 2.51732
[1mStep[0m  [198/339], [94mLoss[0m : 2.87770
[1mStep[0m  [231/339], [94mLoss[0m : 2.71421
[1mStep[0m  [264/339], [94mLoss[0m : 2.61644
[1mStep[0m  [297/339], [94mLoss[0m : 2.51084
[1mStep[0m  [330/339], [94mLoss[0m : 2.71569

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.303, [92mTest[0m: 10.662, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71040
[1mStep[0m  [33/339], [94mLoss[0m : 2.10227
[1mStep[0m  [66/339], [94mLoss[0m : 2.47373
[1mStep[0m  [99/339], [94mLoss[0m : 2.91775
[1mStep[0m  [132/339], [94mLoss[0m : 1.74937
[1mStep[0m  [165/339], [94mLoss[0m : 2.22598
[1mStep[0m  [198/339], [94mLoss[0m : 2.82182
[1mStep[0m  [231/339], [94mLoss[0m : 2.26998
[1mStep[0m  [264/339], [94mLoss[0m : 2.50765
[1mStep[0m  [297/339], [94mLoss[0m : 3.08437
[1mStep[0m  [330/339], [94mLoss[0m : 2.37440

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.393, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78379
[1mStep[0m  [33/339], [94mLoss[0m : 2.81057
[1mStep[0m  [66/339], [94mLoss[0m : 2.96972
[1mStep[0m  [99/339], [94mLoss[0m : 2.13930
[1mStep[0m  [132/339], [94mLoss[0m : 2.56548
[1mStep[0m  [165/339], [94mLoss[0m : 2.39722
[1mStep[0m  [198/339], [94mLoss[0m : 2.52794
[1mStep[0m  [231/339], [94mLoss[0m : 2.43331
[1mStep[0m  [264/339], [94mLoss[0m : 3.14295
[1mStep[0m  [297/339], [94mLoss[0m : 2.30389
[1mStep[0m  [330/339], [94mLoss[0m : 2.21144

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.344, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07051
[1mStep[0m  [33/339], [94mLoss[0m : 2.08652
[1mStep[0m  [66/339], [94mLoss[0m : 3.02084
[1mStep[0m  [99/339], [94mLoss[0m : 3.17516
[1mStep[0m  [132/339], [94mLoss[0m : 2.20374
[1mStep[0m  [165/339], [94mLoss[0m : 2.22971
[1mStep[0m  [198/339], [94mLoss[0m : 2.72935
[1mStep[0m  [231/339], [94mLoss[0m : 2.19413
[1mStep[0m  [264/339], [94mLoss[0m : 2.85119
[1mStep[0m  [297/339], [94mLoss[0m : 2.26469
[1mStep[0m  [330/339], [94mLoss[0m : 2.97555

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.328, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56581
[1mStep[0m  [33/339], [94mLoss[0m : 2.17602
[1mStep[0m  [66/339], [94mLoss[0m : 2.45012
[1mStep[0m  [99/339], [94mLoss[0m : 2.55237
[1mStep[0m  [132/339], [94mLoss[0m : 2.22381
[1mStep[0m  [165/339], [94mLoss[0m : 2.40530
[1mStep[0m  [198/339], [94mLoss[0m : 2.12148
[1mStep[0m  [231/339], [94mLoss[0m : 2.53804
[1mStep[0m  [264/339], [94mLoss[0m : 2.50208
[1mStep[0m  [297/339], [94mLoss[0m : 2.32316
[1mStep[0m  [330/339], [94mLoss[0m : 3.28166

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.325, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64633
[1mStep[0m  [33/339], [94mLoss[0m : 1.90967
[1mStep[0m  [66/339], [94mLoss[0m : 2.96158
[1mStep[0m  [99/339], [94mLoss[0m : 2.81368
[1mStep[0m  [132/339], [94mLoss[0m : 2.91719
[1mStep[0m  [165/339], [94mLoss[0m : 2.21321
[1mStep[0m  [198/339], [94mLoss[0m : 2.32482
[1mStep[0m  [231/339], [94mLoss[0m : 3.29351
[1mStep[0m  [264/339], [94mLoss[0m : 1.95170
[1mStep[0m  [297/339], [94mLoss[0m : 2.26359
[1mStep[0m  [330/339], [94mLoss[0m : 2.09164

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.05863
[1mStep[0m  [33/339], [94mLoss[0m : 2.73569
[1mStep[0m  [66/339], [94mLoss[0m : 2.84257
[1mStep[0m  [99/339], [94mLoss[0m : 2.79082
[1mStep[0m  [132/339], [94mLoss[0m : 2.27315
[1mStep[0m  [165/339], [94mLoss[0m : 3.17381
[1mStep[0m  [198/339], [94mLoss[0m : 2.52543
[1mStep[0m  [231/339], [94mLoss[0m : 2.52861
[1mStep[0m  [264/339], [94mLoss[0m : 1.56106
[1mStep[0m  [297/339], [94mLoss[0m : 2.66820
[1mStep[0m  [330/339], [94mLoss[0m : 2.80956

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.335, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97279
[1mStep[0m  [33/339], [94mLoss[0m : 2.50535
[1mStep[0m  [66/339], [94mLoss[0m : 3.06984
[1mStep[0m  [99/339], [94mLoss[0m : 2.72372
[1mStep[0m  [132/339], [94mLoss[0m : 1.88262
[1mStep[0m  [165/339], [94mLoss[0m : 2.42317
[1mStep[0m  [198/339], [94mLoss[0m : 2.37692
[1mStep[0m  [231/339], [94mLoss[0m : 1.79473
[1mStep[0m  [264/339], [94mLoss[0m : 2.91194
[1mStep[0m  [297/339], [94mLoss[0m : 2.55043
[1mStep[0m  [330/339], [94mLoss[0m : 2.15820

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.345, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.03280
[1mStep[0m  [33/339], [94mLoss[0m : 2.42084
[1mStep[0m  [66/339], [94mLoss[0m : 2.42514
[1mStep[0m  [99/339], [94mLoss[0m : 2.62252
[1mStep[0m  [132/339], [94mLoss[0m : 2.56978
[1mStep[0m  [165/339], [94mLoss[0m : 2.24391
[1mStep[0m  [198/339], [94mLoss[0m : 2.17549
[1mStep[0m  [231/339], [94mLoss[0m : 3.03363
[1mStep[0m  [264/339], [94mLoss[0m : 2.38600
[1mStep[0m  [297/339], [94mLoss[0m : 3.01332
[1mStep[0m  [330/339], [94mLoss[0m : 2.66296

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.320, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.15381
[1mStep[0m  [33/339], [94mLoss[0m : 2.03257
[1mStep[0m  [66/339], [94mLoss[0m : 2.04574
[1mStep[0m  [99/339], [94mLoss[0m : 2.79873
[1mStep[0m  [132/339], [94mLoss[0m : 2.18485
[1mStep[0m  [165/339], [94mLoss[0m : 2.74091
[1mStep[0m  [198/339], [94mLoss[0m : 2.50844
[1mStep[0m  [231/339], [94mLoss[0m : 2.37054
[1mStep[0m  [264/339], [94mLoss[0m : 2.43739
[1mStep[0m  [297/339], [94mLoss[0m : 2.37598
[1mStep[0m  [330/339], [94mLoss[0m : 2.74573

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.322, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45868
[1mStep[0m  [33/339], [94mLoss[0m : 2.76990
[1mStep[0m  [66/339], [94mLoss[0m : 2.94922
[1mStep[0m  [99/339], [94mLoss[0m : 2.03710
[1mStep[0m  [132/339], [94mLoss[0m : 2.77508
[1mStep[0m  [165/339], [94mLoss[0m : 2.38253
[1mStep[0m  [198/339], [94mLoss[0m : 3.10776
[1mStep[0m  [231/339], [94mLoss[0m : 2.80432
[1mStep[0m  [264/339], [94mLoss[0m : 2.11295
[1mStep[0m  [297/339], [94mLoss[0m : 2.48510
[1mStep[0m  [330/339], [94mLoss[0m : 2.07513

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.330, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.90650
[1mStep[0m  [33/339], [94mLoss[0m : 2.71245
[1mStep[0m  [66/339], [94mLoss[0m : 2.58363
[1mStep[0m  [99/339], [94mLoss[0m : 2.77183
[1mStep[0m  [132/339], [94mLoss[0m : 2.80912
[1mStep[0m  [165/339], [94mLoss[0m : 2.08817
[1mStep[0m  [198/339], [94mLoss[0m : 2.86747
[1mStep[0m  [231/339], [94mLoss[0m : 2.80395
[1mStep[0m  [264/339], [94mLoss[0m : 1.79768
[1mStep[0m  [297/339], [94mLoss[0m : 2.06099
[1mStep[0m  [330/339], [94mLoss[0m : 2.31770

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.316, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08595
[1mStep[0m  [33/339], [94mLoss[0m : 2.41420
[1mStep[0m  [66/339], [94mLoss[0m : 2.86221
[1mStep[0m  [99/339], [94mLoss[0m : 2.65336
[1mStep[0m  [132/339], [94mLoss[0m : 2.67387
[1mStep[0m  [165/339], [94mLoss[0m : 2.90885
[1mStep[0m  [198/339], [94mLoss[0m : 3.18213
[1mStep[0m  [231/339], [94mLoss[0m : 3.02241
[1mStep[0m  [264/339], [94mLoss[0m : 2.33182
[1mStep[0m  [297/339], [94mLoss[0m : 2.64161
[1mStep[0m  [330/339], [94mLoss[0m : 2.26698

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72094
[1mStep[0m  [33/339], [94mLoss[0m : 2.66709
[1mStep[0m  [66/339], [94mLoss[0m : 2.30207
[1mStep[0m  [99/339], [94mLoss[0m : 2.57809
[1mStep[0m  [132/339], [94mLoss[0m : 1.87506
[1mStep[0m  [165/339], [94mLoss[0m : 1.90268
[1mStep[0m  [198/339], [94mLoss[0m : 2.40245
[1mStep[0m  [231/339], [94mLoss[0m : 2.70039
[1mStep[0m  [264/339], [94mLoss[0m : 2.52629
[1mStep[0m  [297/339], [94mLoss[0m : 2.44453
[1mStep[0m  [330/339], [94mLoss[0m : 2.62553

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.334, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76504
[1mStep[0m  [33/339], [94mLoss[0m : 2.53939
[1mStep[0m  [66/339], [94mLoss[0m : 2.21225
[1mStep[0m  [99/339], [94mLoss[0m : 2.46514
[1mStep[0m  [132/339], [94mLoss[0m : 2.11828
[1mStep[0m  [165/339], [94mLoss[0m : 2.54578
[1mStep[0m  [198/339], [94mLoss[0m : 2.25252
[1mStep[0m  [231/339], [94mLoss[0m : 2.62592
[1mStep[0m  [264/339], [94mLoss[0m : 2.25418
[1mStep[0m  [297/339], [94mLoss[0m : 2.20552
[1mStep[0m  [330/339], [94mLoss[0m : 2.51898

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.324, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50400
[1mStep[0m  [33/339], [94mLoss[0m : 3.24298
[1mStep[0m  [66/339], [94mLoss[0m : 1.88244
[1mStep[0m  [99/339], [94mLoss[0m : 1.75196
[1mStep[0m  [132/339], [94mLoss[0m : 2.42462
[1mStep[0m  [165/339], [94mLoss[0m : 2.38137
[1mStep[0m  [198/339], [94mLoss[0m : 3.14377
[1mStep[0m  [231/339], [94mLoss[0m : 2.20964
[1mStep[0m  [264/339], [94mLoss[0m : 2.43283
[1mStep[0m  [297/339], [94mLoss[0m : 2.05688
[1mStep[0m  [330/339], [94mLoss[0m : 2.50417

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.327, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.15120
[1mStep[0m  [33/339], [94mLoss[0m : 2.20706
[1mStep[0m  [66/339], [94mLoss[0m : 2.44919
[1mStep[0m  [99/339], [94mLoss[0m : 2.68398
[1mStep[0m  [132/339], [94mLoss[0m : 2.84328
[1mStep[0m  [165/339], [94mLoss[0m : 2.59352
[1mStep[0m  [198/339], [94mLoss[0m : 2.56833
[1mStep[0m  [231/339], [94mLoss[0m : 2.66877
[1mStep[0m  [264/339], [94mLoss[0m : 2.05819
[1mStep[0m  [297/339], [94mLoss[0m : 2.21660
[1mStep[0m  [330/339], [94mLoss[0m : 2.67656

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.336, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86885
[1mStep[0m  [33/339], [94mLoss[0m : 2.30386
[1mStep[0m  [66/339], [94mLoss[0m : 2.46527
[1mStep[0m  [99/339], [94mLoss[0m : 2.17768
[1mStep[0m  [132/339], [94mLoss[0m : 2.51253
[1mStep[0m  [165/339], [94mLoss[0m : 2.44759
[1mStep[0m  [198/339], [94mLoss[0m : 2.16189
[1mStep[0m  [231/339], [94mLoss[0m : 2.50891
[1mStep[0m  [264/339], [94mLoss[0m : 2.16698
[1mStep[0m  [297/339], [94mLoss[0m : 1.95354
[1mStep[0m  [330/339], [94mLoss[0m : 2.07201

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.329, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34048
[1mStep[0m  [33/339], [94mLoss[0m : 2.26536
[1mStep[0m  [66/339], [94mLoss[0m : 2.20008
[1mStep[0m  [99/339], [94mLoss[0m : 2.46570
[1mStep[0m  [132/339], [94mLoss[0m : 2.59558
[1mStep[0m  [165/339], [94mLoss[0m : 1.78771
[1mStep[0m  [198/339], [94mLoss[0m : 2.41119
[1mStep[0m  [231/339], [94mLoss[0m : 2.19576
[1mStep[0m  [264/339], [94mLoss[0m : 2.65006
[1mStep[0m  [297/339], [94mLoss[0m : 2.98087
[1mStep[0m  [330/339], [94mLoss[0m : 2.49161

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.326, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70064
[1mStep[0m  [33/339], [94mLoss[0m : 2.03064
[1mStep[0m  [66/339], [94mLoss[0m : 2.23769
[1mStep[0m  [99/339], [94mLoss[0m : 2.40114
[1mStep[0m  [132/339], [94mLoss[0m : 2.20609
[1mStep[0m  [165/339], [94mLoss[0m : 1.95947
[1mStep[0m  [198/339], [94mLoss[0m : 2.58320
[1mStep[0m  [231/339], [94mLoss[0m : 2.79169
[1mStep[0m  [264/339], [94mLoss[0m : 2.33839
[1mStep[0m  [297/339], [94mLoss[0m : 2.61642
[1mStep[0m  [330/339], [94mLoss[0m : 2.38197

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.327, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51803
[1mStep[0m  [33/339], [94mLoss[0m : 2.41316
[1mStep[0m  [66/339], [94mLoss[0m : 2.25673
[1mStep[0m  [99/339], [94mLoss[0m : 2.32686
[1mStep[0m  [132/339], [94mLoss[0m : 2.33593
[1mStep[0m  [165/339], [94mLoss[0m : 2.06232
[1mStep[0m  [198/339], [94mLoss[0m : 2.92424
[1mStep[0m  [231/339], [94mLoss[0m : 3.21477
[1mStep[0m  [264/339], [94mLoss[0m : 2.89892
[1mStep[0m  [297/339], [94mLoss[0m : 2.56282
[1mStep[0m  [330/339], [94mLoss[0m : 2.68707

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.320, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00081
[1mStep[0m  [33/339], [94mLoss[0m : 2.32297
[1mStep[0m  [66/339], [94mLoss[0m : 2.59792
[1mStep[0m  [99/339], [94mLoss[0m : 3.04367
[1mStep[0m  [132/339], [94mLoss[0m : 2.10937
[1mStep[0m  [165/339], [94mLoss[0m : 2.49894
[1mStep[0m  [198/339], [94mLoss[0m : 3.11510
[1mStep[0m  [231/339], [94mLoss[0m : 2.85354
[1mStep[0m  [264/339], [94mLoss[0m : 2.54223
[1mStep[0m  [297/339], [94mLoss[0m : 2.84842
[1mStep[0m  [330/339], [94mLoss[0m : 2.22859

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.343, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72552
[1mStep[0m  [33/339], [94mLoss[0m : 2.67108
[1mStep[0m  [66/339], [94mLoss[0m : 2.11925
[1mStep[0m  [99/339], [94mLoss[0m : 2.64812
[1mStep[0m  [132/339], [94mLoss[0m : 3.10673
[1mStep[0m  [165/339], [94mLoss[0m : 2.29662
[1mStep[0m  [198/339], [94mLoss[0m : 2.45864
[1mStep[0m  [231/339], [94mLoss[0m : 2.15401
[1mStep[0m  [264/339], [94mLoss[0m : 3.06695
[1mStep[0m  [297/339], [94mLoss[0m : 2.92638
[1mStep[0m  [330/339], [94mLoss[0m : 2.40688

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.318, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18816
[1mStep[0m  [33/339], [94mLoss[0m : 2.34320
[1mStep[0m  [66/339], [94mLoss[0m : 2.54914
[1mStep[0m  [99/339], [94mLoss[0m : 2.15775
[1mStep[0m  [132/339], [94mLoss[0m : 2.34369
[1mStep[0m  [165/339], [94mLoss[0m : 3.17561
[1mStep[0m  [198/339], [94mLoss[0m : 2.32007
[1mStep[0m  [231/339], [94mLoss[0m : 2.33182
[1mStep[0m  [264/339], [94mLoss[0m : 2.79500
[1mStep[0m  [297/339], [94mLoss[0m : 2.19803
[1mStep[0m  [330/339], [94mLoss[0m : 3.04280

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.326, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14074
[1mStep[0m  [33/339], [94mLoss[0m : 2.80231
[1mStep[0m  [66/339], [94mLoss[0m : 3.10935
[1mStep[0m  [99/339], [94mLoss[0m : 1.94502
[1mStep[0m  [132/339], [94mLoss[0m : 2.32575
[1mStep[0m  [165/339], [94mLoss[0m : 2.75169
[1mStep[0m  [198/339], [94mLoss[0m : 2.45362
[1mStep[0m  [231/339], [94mLoss[0m : 2.80913
[1mStep[0m  [264/339], [94mLoss[0m : 3.22653
[1mStep[0m  [297/339], [94mLoss[0m : 2.04494
[1mStep[0m  [330/339], [94mLoss[0m : 2.55878

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.331, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60995
[1mStep[0m  [33/339], [94mLoss[0m : 2.96787
[1mStep[0m  [66/339], [94mLoss[0m : 2.28117
[1mStep[0m  [99/339], [94mLoss[0m : 2.77462
[1mStep[0m  [132/339], [94mLoss[0m : 2.13851
[1mStep[0m  [165/339], [94mLoss[0m : 2.34253
[1mStep[0m  [198/339], [94mLoss[0m : 2.76134
[1mStep[0m  [231/339], [94mLoss[0m : 2.10220
[1mStep[0m  [264/339], [94mLoss[0m : 2.77435
[1mStep[0m  [297/339], [94mLoss[0m : 2.83966
[1mStep[0m  [330/339], [94mLoss[0m : 2.43285

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.324, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47197
[1mStep[0m  [33/339], [94mLoss[0m : 2.10927
[1mStep[0m  [66/339], [94mLoss[0m : 2.30332
[1mStep[0m  [99/339], [94mLoss[0m : 2.63465
[1mStep[0m  [132/339], [94mLoss[0m : 2.59347
[1mStep[0m  [165/339], [94mLoss[0m : 2.31872
[1mStep[0m  [198/339], [94mLoss[0m : 2.85573
[1mStep[0m  [231/339], [94mLoss[0m : 2.30345
[1mStep[0m  [264/339], [94mLoss[0m : 2.32966
[1mStep[0m  [297/339], [94mLoss[0m : 2.71386
[1mStep[0m  [330/339], [94mLoss[0m : 2.12738

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.319, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29020
[1mStep[0m  [33/339], [94mLoss[0m : 3.16833
[1mStep[0m  [66/339], [94mLoss[0m : 2.77839
[1mStep[0m  [99/339], [94mLoss[0m : 2.62848
[1mStep[0m  [132/339], [94mLoss[0m : 2.33099
[1mStep[0m  [165/339], [94mLoss[0m : 2.47740
[1mStep[0m  [198/339], [94mLoss[0m : 2.54499
[1mStep[0m  [231/339], [94mLoss[0m : 2.78123
[1mStep[0m  [264/339], [94mLoss[0m : 2.45996
[1mStep[0m  [297/339], [94mLoss[0m : 2.01124
[1mStep[0m  [330/339], [94mLoss[0m : 3.38681

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.335, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27667
[1mStep[0m  [33/339], [94mLoss[0m : 3.24624
[1mStep[0m  [66/339], [94mLoss[0m : 2.71207
[1mStep[0m  [99/339], [94mLoss[0m : 2.33315
[1mStep[0m  [132/339], [94mLoss[0m : 2.70397
[1mStep[0m  [165/339], [94mLoss[0m : 2.42911
[1mStep[0m  [198/339], [94mLoss[0m : 2.73640
[1mStep[0m  [231/339], [94mLoss[0m : 2.33112
[1mStep[0m  [264/339], [94mLoss[0m : 2.66166
[1mStep[0m  [297/339], [94mLoss[0m : 2.77122
[1mStep[0m  [330/339], [94mLoss[0m : 2.30531

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.340, [96mlr[0m: 0.004545
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.26975
[1mStep[0m  [33/339], [94mLoss[0m : 2.07054
[1mStep[0m  [66/339], [94mLoss[0m : 2.42116
[1mStep[0m  [99/339], [94mLoss[0m : 2.24919
[1mStep[0m  [132/339], [94mLoss[0m : 2.01445
[1mStep[0m  [165/339], [94mLoss[0m : 2.11003
[1mStep[0m  [198/339], [94mLoss[0m : 2.71250
[1mStep[0m  [231/339], [94mLoss[0m : 3.26330
[1mStep[0m  [264/339], [94mLoss[0m : 2.94617
[1mStep[0m  [297/339], [94mLoss[0m : 3.04076
[1mStep[0m  [330/339], [94mLoss[0m : 2.07817

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.332, [96mlr[0m: 0.004545
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.343
====================================

Phase 1 - Evaluation MAE:  2.3433471686017198
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.005050000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 1.63708
[1mStep[0m  [33/339], [94mLoss[0m : 3.21122
[1mStep[0m  [66/339], [94mLoss[0m : 2.37185
[1mStep[0m  [99/339], [94mLoss[0m : 2.75540
[1mStep[0m  [132/339], [94mLoss[0m : 2.60923
[1mStep[0m  [165/339], [94mLoss[0m : 2.57869
[1mStep[0m  [198/339], [94mLoss[0m : 2.93094
[1mStep[0m  [231/339], [94mLoss[0m : 2.18179
[1mStep[0m  [264/339], [94mLoss[0m : 2.53043
[1mStep[0m  [297/339], [94mLoss[0m : 2.61902
[1mStep[0m  [330/339], [94mLoss[0m : 2.59119

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.343, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54160
[1mStep[0m  [33/339], [94mLoss[0m : 2.16813
[1mStep[0m  [66/339], [94mLoss[0m : 2.00435
[1mStep[0m  [99/339], [94mLoss[0m : 1.72968
[1mStep[0m  [132/339], [94mLoss[0m : 2.74669
[1mStep[0m  [165/339], [94mLoss[0m : 2.43881
[1mStep[0m  [198/339], [94mLoss[0m : 1.69215
[1mStep[0m  [231/339], [94mLoss[0m : 2.74154
[1mStep[0m  [264/339], [94mLoss[0m : 2.84271
[1mStep[0m  [297/339], [94mLoss[0m : 2.72357
[1mStep[0m  [330/339], [94mLoss[0m : 2.41575

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.536, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28636
[1mStep[0m  [33/339], [94mLoss[0m : 2.23064
[1mStep[0m  [66/339], [94mLoss[0m : 2.74861
[1mStep[0m  [99/339], [94mLoss[0m : 2.28487
[1mStep[0m  [132/339], [94mLoss[0m : 2.79161
[1mStep[0m  [165/339], [94mLoss[0m : 2.61650
[1mStep[0m  [198/339], [94mLoss[0m : 2.48489
[1mStep[0m  [231/339], [94mLoss[0m : 2.36575
[1mStep[0m  [264/339], [94mLoss[0m : 2.01113
[1mStep[0m  [297/339], [94mLoss[0m : 2.91901
[1mStep[0m  [330/339], [94mLoss[0m : 1.57424

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.443, [96mlr[0m: 0.005050000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01254
[1mStep[0m  [33/339], [94mLoss[0m : 2.64616
[1mStep[0m  [66/339], [94mLoss[0m : 2.70967
[1mStep[0m  [99/339], [94mLoss[0m : 2.23679
[1mStep[0m  [132/339], [94mLoss[0m : 2.10989
