no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  1
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.70602
[1mStep[0m  [8/84], [94mLoss[0m : 11.59098
[1mStep[0m  [16/84], [94mLoss[0m : 11.10376
[1mStep[0m  [24/84], [94mLoss[0m : 10.89997
[1mStep[0m  [32/84], [94mLoss[0m : 11.25719
[1mStep[0m  [40/84], [94mLoss[0m : 11.07630
[1mStep[0m  [48/84], [94mLoss[0m : 11.11390
[1mStep[0m  [56/84], [94mLoss[0m : 10.81308
[1mStep[0m  [64/84], [94mLoss[0m : 11.12304
[1mStep[0m  [72/84], [94mLoss[0m : 10.60371
[1mStep[0m  [80/84], [94mLoss[0m : 11.39860

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.111, [92mTest[0m: 11.121, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.11590
[1mStep[0m  [8/84], [94mLoss[0m : 11.25567
[1mStep[0m  [16/84], [94mLoss[0m : 10.91155
[1mStep[0m  [24/84], [94mLoss[0m : 10.39247
[1mStep[0m  [32/84], [94mLoss[0m : 11.24264
[1mStep[0m  [40/84], [94mLoss[0m : 10.75345
[1mStep[0m  [48/84], [94mLoss[0m : 11.77003
[1mStep[0m  [56/84], [94mLoss[0m : 10.49913
[1mStep[0m  [64/84], [94mLoss[0m : 10.90473
[1mStep[0m  [72/84], [94mLoss[0m : 10.75196
[1mStep[0m  [80/84], [94mLoss[0m : 11.06208

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.989, [92mTest[0m: 11.053, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.98773
[1mStep[0m  [8/84], [94mLoss[0m : 10.90683
[1mStep[0m  [16/84], [94mLoss[0m : 10.67590
[1mStep[0m  [24/84], [94mLoss[0m : 10.80548
[1mStep[0m  [32/84], [94mLoss[0m : 11.37605
[1mStep[0m  [40/84], [94mLoss[0m : 10.64361
[1mStep[0m  [48/84], [94mLoss[0m : 11.40795
[1mStep[0m  [56/84], [94mLoss[0m : 11.19899
[1mStep[0m  [64/84], [94mLoss[0m : 10.41978
[1mStep[0m  [72/84], [94mLoss[0m : 10.38100
[1mStep[0m  [80/84], [94mLoss[0m : 10.17934

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.862, [92mTest[0m: 10.903, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.67379
[1mStep[0m  [8/84], [94mLoss[0m : 10.54677
[1mStep[0m  [16/84], [94mLoss[0m : 10.73301
[1mStep[0m  [24/84], [94mLoss[0m : 10.61288
[1mStep[0m  [32/84], [94mLoss[0m : 10.56463
[1mStep[0m  [40/84], [94mLoss[0m : 10.95158
[1mStep[0m  [48/84], [94mLoss[0m : 11.09502
[1mStep[0m  [56/84], [94mLoss[0m : 9.88487
[1mStep[0m  [64/84], [94mLoss[0m : 10.68088
[1mStep[0m  [72/84], [94mLoss[0m : 10.57491
[1mStep[0m  [80/84], [94mLoss[0m : 10.41653

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.739, [92mTest[0m: 10.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.41395
[1mStep[0m  [8/84], [94mLoss[0m : 11.05937
[1mStep[0m  [16/84], [94mLoss[0m : 10.54758
[1mStep[0m  [24/84], [94mLoss[0m : 10.43699
[1mStep[0m  [32/84], [94mLoss[0m : 10.79095
[1mStep[0m  [40/84], [94mLoss[0m : 10.90654
[1mStep[0m  [48/84], [94mLoss[0m : 10.35473
[1mStep[0m  [56/84], [94mLoss[0m : 10.69500
[1mStep[0m  [64/84], [94mLoss[0m : 10.98328
[1mStep[0m  [72/84], [94mLoss[0m : 10.54719
[1mStep[0m  [80/84], [94mLoss[0m : 10.44663

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.608, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.17146
[1mStep[0m  [8/84], [94mLoss[0m : 10.29507
[1mStep[0m  [16/84], [94mLoss[0m : 10.04336
[1mStep[0m  [24/84], [94mLoss[0m : 10.65124
[1mStep[0m  [32/84], [94mLoss[0m : 10.79164
[1mStep[0m  [40/84], [94mLoss[0m : 9.81751
[1mStep[0m  [48/84], [94mLoss[0m : 11.00571
[1mStep[0m  [56/84], [94mLoss[0m : 11.20716
[1mStep[0m  [64/84], [94mLoss[0m : 9.91047
[1mStep[0m  [72/84], [94mLoss[0m : 10.58974
[1mStep[0m  [80/84], [94mLoss[0m : 10.45451

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.488, [92mTest[0m: 10.521, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.43001
[1mStep[0m  [8/84], [94mLoss[0m : 10.32433
[1mStep[0m  [16/84], [94mLoss[0m : 10.23361
[1mStep[0m  [24/84], [94mLoss[0m : 10.43268
[1mStep[0m  [32/84], [94mLoss[0m : 10.13550
[1mStep[0m  [40/84], [94mLoss[0m : 10.43857
[1mStep[0m  [48/84], [94mLoss[0m : 11.05782
[1mStep[0m  [56/84], [94mLoss[0m : 10.13042
[1mStep[0m  [64/84], [94mLoss[0m : 9.95537
[1mStep[0m  [72/84], [94mLoss[0m : 10.18560
[1mStep[0m  [80/84], [94mLoss[0m : 10.22235

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.358, [92mTest[0m: 10.419, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.39338
[1mStep[0m  [8/84], [94mLoss[0m : 10.40504
[1mStep[0m  [16/84], [94mLoss[0m : 10.72470
[1mStep[0m  [24/84], [94mLoss[0m : 10.45897
[1mStep[0m  [32/84], [94mLoss[0m : 10.48571
[1mStep[0m  [40/84], [94mLoss[0m : 10.11122
[1mStep[0m  [48/84], [94mLoss[0m : 9.96025
[1mStep[0m  [56/84], [94mLoss[0m : 9.74630
[1mStep[0m  [64/84], [94mLoss[0m : 10.09262
[1mStep[0m  [72/84], [94mLoss[0m : 10.52518
[1mStep[0m  [80/84], [94mLoss[0m : 9.97463

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.245, [92mTest[0m: 10.281, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.31350
[1mStep[0m  [8/84], [94mLoss[0m : 9.92402
[1mStep[0m  [16/84], [94mLoss[0m : 10.02195
[1mStep[0m  [24/84], [94mLoss[0m : 10.09527
[1mStep[0m  [32/84], [94mLoss[0m : 10.08375
[1mStep[0m  [40/84], [94mLoss[0m : 10.17440
[1mStep[0m  [48/84], [94mLoss[0m : 9.96172
[1mStep[0m  [56/84], [94mLoss[0m : 10.07191
[1mStep[0m  [64/84], [94mLoss[0m : 9.62465
[1mStep[0m  [72/84], [94mLoss[0m : 10.19448
[1mStep[0m  [80/84], [94mLoss[0m : 9.62144

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.106, [92mTest[0m: 10.147, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.75806
[1mStep[0m  [8/84], [94mLoss[0m : 9.77259
[1mStep[0m  [16/84], [94mLoss[0m : 10.19545
[1mStep[0m  [24/84], [94mLoss[0m : 10.08897
[1mStep[0m  [32/84], [94mLoss[0m : 10.15907
[1mStep[0m  [40/84], [94mLoss[0m : 10.17476
[1mStep[0m  [48/84], [94mLoss[0m : 10.15881
[1mStep[0m  [56/84], [94mLoss[0m : 9.67843
[1mStep[0m  [64/84], [94mLoss[0m : 10.01555
[1mStep[0m  [72/84], [94mLoss[0m : 9.64238
[1mStep[0m  [80/84], [94mLoss[0m : 10.08782

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.975, [92mTest[0m: 10.035, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.84495
[1mStep[0m  [8/84], [94mLoss[0m : 9.54393
[1mStep[0m  [16/84], [94mLoss[0m : 9.34664
[1mStep[0m  [24/84], [94mLoss[0m : 10.58306
[1mStep[0m  [32/84], [94mLoss[0m : 10.13273
[1mStep[0m  [40/84], [94mLoss[0m : 9.46300
[1mStep[0m  [48/84], [94mLoss[0m : 9.92990
[1mStep[0m  [56/84], [94mLoss[0m : 9.85230
[1mStep[0m  [64/84], [94mLoss[0m : 9.67038
[1mStep[0m  [72/84], [94mLoss[0m : 9.58537
[1mStep[0m  [80/84], [94mLoss[0m : 9.87374

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.851, [92mTest[0m: 9.907, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.80679
[1mStep[0m  [8/84], [94mLoss[0m : 10.21945
[1mStep[0m  [16/84], [94mLoss[0m : 10.01929
[1mStep[0m  [24/84], [94mLoss[0m : 9.30552
[1mStep[0m  [32/84], [94mLoss[0m : 9.55058
[1mStep[0m  [40/84], [94mLoss[0m : 9.00558
[1mStep[0m  [48/84], [94mLoss[0m : 9.81940
[1mStep[0m  [56/84], [94mLoss[0m : 9.86609
[1mStep[0m  [64/84], [94mLoss[0m : 9.80976
[1mStep[0m  [72/84], [94mLoss[0m : 9.09955
[1mStep[0m  [80/84], [94mLoss[0m : 9.86124

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.731, [92mTest[0m: 9.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.36827
[1mStep[0m  [8/84], [94mLoss[0m : 9.76595
[1mStep[0m  [16/84], [94mLoss[0m : 9.21615
[1mStep[0m  [24/84], [94mLoss[0m : 9.43506
[1mStep[0m  [32/84], [94mLoss[0m : 9.40558
[1mStep[0m  [40/84], [94mLoss[0m : 9.67676
[1mStep[0m  [48/84], [94mLoss[0m : 9.98988
[1mStep[0m  [56/84], [94mLoss[0m : 9.60492
[1mStep[0m  [64/84], [94mLoss[0m : 9.76111
[1mStep[0m  [72/84], [94mLoss[0m : 9.73681
[1mStep[0m  [80/84], [94mLoss[0m : 9.43724

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.600, [92mTest[0m: 9.654, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.53633
[1mStep[0m  [8/84], [94mLoss[0m : 10.12250
[1mStep[0m  [16/84], [94mLoss[0m : 9.60338
[1mStep[0m  [24/84], [94mLoss[0m : 10.08708
[1mStep[0m  [32/84], [94mLoss[0m : 9.41417
[1mStep[0m  [40/84], [94mLoss[0m : 9.39632
[1mStep[0m  [48/84], [94mLoss[0m : 8.47108
[1mStep[0m  [56/84], [94mLoss[0m : 9.91516
[1mStep[0m  [64/84], [94mLoss[0m : 9.29922
[1mStep[0m  [72/84], [94mLoss[0m : 9.21073
[1mStep[0m  [80/84], [94mLoss[0m : 9.25083

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.477, [92mTest[0m: 9.520, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.59178
[1mStep[0m  [8/84], [94mLoss[0m : 9.81157
[1mStep[0m  [16/84], [94mLoss[0m : 9.27522
[1mStep[0m  [24/84], [94mLoss[0m : 10.55497
[1mStep[0m  [32/84], [94mLoss[0m : 9.48035
[1mStep[0m  [40/84], [94mLoss[0m : 9.34704
[1mStep[0m  [48/84], [94mLoss[0m : 9.08117
[1mStep[0m  [56/84], [94mLoss[0m : 9.23914
[1mStep[0m  [64/84], [94mLoss[0m : 8.95628
[1mStep[0m  [72/84], [94mLoss[0m : 9.19180
[1mStep[0m  [80/84], [94mLoss[0m : 9.14050

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.345, [92mTest[0m: 9.409, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.60659
[1mStep[0m  [8/84], [94mLoss[0m : 9.55596
[1mStep[0m  [16/84], [94mLoss[0m : 9.52140
[1mStep[0m  [24/84], [94mLoss[0m : 9.34636
[1mStep[0m  [32/84], [94mLoss[0m : 9.36434
[1mStep[0m  [40/84], [94mLoss[0m : 9.14065
[1mStep[0m  [48/84], [94mLoss[0m : 9.40856
[1mStep[0m  [56/84], [94mLoss[0m : 9.11966
[1mStep[0m  [64/84], [94mLoss[0m : 8.76025
[1mStep[0m  [72/84], [94mLoss[0m : 8.87903
[1mStep[0m  [80/84], [94mLoss[0m : 9.18527

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.212, [92mTest[0m: 9.261, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.85354
[1mStep[0m  [8/84], [94mLoss[0m : 9.08328
[1mStep[0m  [16/84], [94mLoss[0m : 9.14670
[1mStep[0m  [24/84], [94mLoss[0m : 9.03976
[1mStep[0m  [32/84], [94mLoss[0m : 9.65125
[1mStep[0m  [40/84], [94mLoss[0m : 8.82172
[1mStep[0m  [48/84], [94mLoss[0m : 8.89744
[1mStep[0m  [56/84], [94mLoss[0m : 9.07841
[1mStep[0m  [64/84], [94mLoss[0m : 8.74933
[1mStep[0m  [72/84], [94mLoss[0m : 8.89050
[1mStep[0m  [80/84], [94mLoss[0m : 8.49587

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.095, [92mTest[0m: 9.151, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.77371
[1mStep[0m  [8/84], [94mLoss[0m : 8.88658
[1mStep[0m  [16/84], [94mLoss[0m : 8.60852
[1mStep[0m  [24/84], [94mLoss[0m : 8.63138
[1mStep[0m  [32/84], [94mLoss[0m : 8.57879
[1mStep[0m  [40/84], [94mLoss[0m : 8.88934
[1mStep[0m  [48/84], [94mLoss[0m : 8.89477
[1mStep[0m  [56/84], [94mLoss[0m : 8.98711
[1mStep[0m  [64/84], [94mLoss[0m : 9.77740
[1mStep[0m  [72/84], [94mLoss[0m : 8.78969
[1mStep[0m  [80/84], [94mLoss[0m : 8.62825

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.967, [92mTest[0m: 9.022, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.97054
[1mStep[0m  [8/84], [94mLoss[0m : 8.82503
[1mStep[0m  [16/84], [94mLoss[0m : 8.45322
[1mStep[0m  [24/84], [94mLoss[0m : 8.69144
[1mStep[0m  [32/84], [94mLoss[0m : 9.00107
[1mStep[0m  [40/84], [94mLoss[0m : 8.10938
[1mStep[0m  [48/84], [94mLoss[0m : 8.43654
[1mStep[0m  [56/84], [94mLoss[0m : 9.05316
[1mStep[0m  [64/84], [94mLoss[0m : 8.67828
[1mStep[0m  [72/84], [94mLoss[0m : 9.10641
[1mStep[0m  [80/84], [94mLoss[0m : 9.13208

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.845, [92mTest[0m: 8.897, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.97758
[1mStep[0m  [8/84], [94mLoss[0m : 9.16280
[1mStep[0m  [16/84], [94mLoss[0m : 8.65184
[1mStep[0m  [24/84], [94mLoss[0m : 8.44069
[1mStep[0m  [32/84], [94mLoss[0m : 8.84234
[1mStep[0m  [40/84], [94mLoss[0m : 9.11056
[1mStep[0m  [48/84], [94mLoss[0m : 9.02443
[1mStep[0m  [56/84], [94mLoss[0m : 8.39006
[1mStep[0m  [64/84], [94mLoss[0m : 8.51417
[1mStep[0m  [72/84], [94mLoss[0m : 8.74376
[1mStep[0m  [80/84], [94mLoss[0m : 8.83475

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.709, [92mTest[0m: 8.766, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.61118
[1mStep[0m  [8/84], [94mLoss[0m : 8.66746
[1mStep[0m  [16/84], [94mLoss[0m : 8.11565
[1mStep[0m  [24/84], [94mLoss[0m : 8.52218
[1mStep[0m  [32/84], [94mLoss[0m : 8.05009
[1mStep[0m  [40/84], [94mLoss[0m : 8.77488
[1mStep[0m  [48/84], [94mLoss[0m : 8.42452
[1mStep[0m  [56/84], [94mLoss[0m : 8.69171
[1mStep[0m  [64/84], [94mLoss[0m : 7.97095
[1mStep[0m  [72/84], [94mLoss[0m : 8.61294
[1mStep[0m  [80/84], [94mLoss[0m : 8.77324

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.599, [92mTest[0m: 8.628, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.62068
[1mStep[0m  [8/84], [94mLoss[0m : 8.30705
[1mStep[0m  [16/84], [94mLoss[0m : 8.09808
[1mStep[0m  [24/84], [94mLoss[0m : 8.30067
[1mStep[0m  [32/84], [94mLoss[0m : 8.45250
[1mStep[0m  [40/84], [94mLoss[0m : 8.90137
[1mStep[0m  [48/84], [94mLoss[0m : 9.08385
[1mStep[0m  [56/84], [94mLoss[0m : 8.24658
[1mStep[0m  [64/84], [94mLoss[0m : 8.13555
[1mStep[0m  [72/84], [94mLoss[0m : 8.21969
[1mStep[0m  [80/84], [94mLoss[0m : 8.66264

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.488, [92mTest[0m: 8.532, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.52286
[1mStep[0m  [8/84], [94mLoss[0m : 8.39153
[1mStep[0m  [16/84], [94mLoss[0m : 8.47315
[1mStep[0m  [24/84], [94mLoss[0m : 8.04576
[1mStep[0m  [32/84], [94mLoss[0m : 8.70099
[1mStep[0m  [40/84], [94mLoss[0m : 8.20187
[1mStep[0m  [48/84], [94mLoss[0m : 8.37022
[1mStep[0m  [56/84], [94mLoss[0m : 8.09861
[1mStep[0m  [64/84], [94mLoss[0m : 8.43491
[1mStep[0m  [72/84], [94mLoss[0m : 8.00947
[1mStep[0m  [80/84], [94mLoss[0m : 8.80925

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.367, [92mTest[0m: 8.406, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.77125
[1mStep[0m  [8/84], [94mLoss[0m : 8.30466
[1mStep[0m  [16/84], [94mLoss[0m : 8.03214
[1mStep[0m  [24/84], [94mLoss[0m : 8.32613
[1mStep[0m  [32/84], [94mLoss[0m : 8.16192
[1mStep[0m  [40/84], [94mLoss[0m : 7.81971
[1mStep[0m  [48/84], [94mLoss[0m : 8.13574
[1mStep[0m  [56/84], [94mLoss[0m : 8.00987
[1mStep[0m  [64/84], [94mLoss[0m : 7.93609
[1mStep[0m  [72/84], [94mLoss[0m : 8.68573
[1mStep[0m  [80/84], [94mLoss[0m : 8.12325

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.257, [92mTest[0m: 8.290, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.86454
[1mStep[0m  [8/84], [94mLoss[0m : 8.03478
[1mStep[0m  [16/84], [94mLoss[0m : 7.86259
[1mStep[0m  [24/84], [94mLoss[0m : 8.20279
[1mStep[0m  [32/84], [94mLoss[0m : 7.67646
[1mStep[0m  [40/84], [94mLoss[0m : 8.26501
[1mStep[0m  [48/84], [94mLoss[0m : 8.69053
[1mStep[0m  [56/84], [94mLoss[0m : 8.07506
[1mStep[0m  [64/84], [94mLoss[0m : 7.78971
[1mStep[0m  [72/84], [94mLoss[0m : 8.20111
[1mStep[0m  [80/84], [94mLoss[0m : 8.32143

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.138, [92mTest[0m: 8.185, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.22427
[1mStep[0m  [8/84], [94mLoss[0m : 8.51855
[1mStep[0m  [16/84], [94mLoss[0m : 8.07795
[1mStep[0m  [24/84], [94mLoss[0m : 7.71763
[1mStep[0m  [32/84], [94mLoss[0m : 7.96318
[1mStep[0m  [40/84], [94mLoss[0m : 8.16291
[1mStep[0m  [48/84], [94mLoss[0m : 8.81496
[1mStep[0m  [56/84], [94mLoss[0m : 8.09501
[1mStep[0m  [64/84], [94mLoss[0m : 7.96716
[1mStep[0m  [72/84], [94mLoss[0m : 7.97496
[1mStep[0m  [80/84], [94mLoss[0m : 7.96466

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.030, [92mTest[0m: 8.077, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.70958
[1mStep[0m  [8/84], [94mLoss[0m : 7.51788
[1mStep[0m  [16/84], [94mLoss[0m : 7.99472
[1mStep[0m  [24/84], [94mLoss[0m : 8.19994
[1mStep[0m  [32/84], [94mLoss[0m : 7.91644
[1mStep[0m  [40/84], [94mLoss[0m : 7.64816
[1mStep[0m  [48/84], [94mLoss[0m : 7.35199
[1mStep[0m  [56/84], [94mLoss[0m : 7.78331
[1mStep[0m  [64/84], [94mLoss[0m : 7.94960
[1mStep[0m  [72/84], [94mLoss[0m : 8.33231
[1mStep[0m  [80/84], [94mLoss[0m : 8.26765

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.908, [92mTest[0m: 7.951, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.11866
[1mStep[0m  [8/84], [94mLoss[0m : 7.43014
[1mStep[0m  [16/84], [94mLoss[0m : 8.52610
[1mStep[0m  [24/84], [94mLoss[0m : 7.10257
[1mStep[0m  [32/84], [94mLoss[0m : 7.55743
[1mStep[0m  [40/84], [94mLoss[0m : 7.75677
[1mStep[0m  [48/84], [94mLoss[0m : 7.49631
[1mStep[0m  [56/84], [94mLoss[0m : 7.30352
[1mStep[0m  [64/84], [94mLoss[0m : 7.44835
[1mStep[0m  [72/84], [94mLoss[0m : 7.40410
[1mStep[0m  [80/84], [94mLoss[0m : 8.27290

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.792, [92mTest[0m: 7.843, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.54986
[1mStep[0m  [8/84], [94mLoss[0m : 7.67537
[1mStep[0m  [16/84], [94mLoss[0m : 8.31018
[1mStep[0m  [24/84], [94mLoss[0m : 7.73410
[1mStep[0m  [32/84], [94mLoss[0m : 7.57770
[1mStep[0m  [40/84], [94mLoss[0m : 7.27362
[1mStep[0m  [48/84], [94mLoss[0m : 7.40024
[1mStep[0m  [56/84], [94mLoss[0m : 7.77579
[1mStep[0m  [64/84], [94mLoss[0m : 7.62092
[1mStep[0m  [72/84], [94mLoss[0m : 7.66542
[1mStep[0m  [80/84], [94mLoss[0m : 7.53835

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.678, [92mTest[0m: 7.732, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.54042
[1mStep[0m  [8/84], [94mLoss[0m : 7.36010
[1mStep[0m  [16/84], [94mLoss[0m : 7.41797
[1mStep[0m  [24/84], [94mLoss[0m : 8.18479
[1mStep[0m  [32/84], [94mLoss[0m : 7.04581
[1mStep[0m  [40/84], [94mLoss[0m : 7.66695
[1mStep[0m  [48/84], [94mLoss[0m : 7.79029
[1mStep[0m  [56/84], [94mLoss[0m : 7.57473
[1mStep[0m  [64/84], [94mLoss[0m : 7.93279
[1mStep[0m  [72/84], [94mLoss[0m : 8.11494
[1mStep[0m  [80/84], [94mLoss[0m : 7.87875

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.568, [92mTest[0m: 7.603, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.487
====================================

Phase 1 - Evaluation MAE:  7.4873441968645364
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 7.26821
[1mStep[0m  [8/84], [94mLoss[0m : 7.33063
[1mStep[0m  [16/84], [94mLoss[0m : 7.08506
[1mStep[0m  [24/84], [94mLoss[0m : 7.33589
[1mStep[0m  [32/84], [94mLoss[0m : 7.40132
[1mStep[0m  [40/84], [94mLoss[0m : 7.70808
[1mStep[0m  [48/84], [94mLoss[0m : 6.90558
[1mStep[0m  [56/84], [94mLoss[0m : 7.92161
[1mStep[0m  [64/84], [94mLoss[0m : 7.73245
[1mStep[0m  [72/84], [94mLoss[0m : 7.38717
[1mStep[0m  [80/84], [94mLoss[0m : 7.34377

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.457, [92mTest[0m: 7.488, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.64783
[1mStep[0m  [8/84], [94mLoss[0m : 7.50394
[1mStep[0m  [16/84], [94mLoss[0m : 7.24505
[1mStep[0m  [24/84], [94mLoss[0m : 7.31653
[1mStep[0m  [32/84], [94mLoss[0m : 7.70335
[1mStep[0m  [40/84], [94mLoss[0m : 7.68630
[1mStep[0m  [48/84], [94mLoss[0m : 6.90756
[1mStep[0m  [56/84], [94mLoss[0m : 7.46003
[1mStep[0m  [64/84], [94mLoss[0m : 7.01172
[1mStep[0m  [72/84], [94mLoss[0m : 6.97226
[1mStep[0m  [80/84], [94mLoss[0m : 6.72315

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.328, [92mTest[0m: 7.362, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.06479
[1mStep[0m  [8/84], [94mLoss[0m : 6.44663
[1mStep[0m  [16/84], [94mLoss[0m : 7.70875
[1mStep[0m  [24/84], [94mLoss[0m : 7.32889
[1mStep[0m  [32/84], [94mLoss[0m : 7.95996
[1mStep[0m  [40/84], [94mLoss[0m : 7.27188
[1mStep[0m  [48/84], [94mLoss[0m : 7.82811
[1mStep[0m  [56/84], [94mLoss[0m : 7.09465
[1mStep[0m  [64/84], [94mLoss[0m : 6.54681
[1mStep[0m  [72/84], [94mLoss[0m : 7.12897
[1mStep[0m  [80/84], [94mLoss[0m : 7.45364

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.203, [92mTest[0m: 7.240, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.00178
[1mStep[0m  [8/84], [94mLoss[0m : 6.88013
[1mStep[0m  [16/84], [94mLoss[0m : 7.06927
[1mStep[0m  [24/84], [94mLoss[0m : 6.91092
[1mStep[0m  [32/84], [94mLoss[0m : 6.94394
[1mStep[0m  [40/84], [94mLoss[0m : 6.40898
[1mStep[0m  [48/84], [94mLoss[0m : 7.07431
[1mStep[0m  [56/84], [94mLoss[0m : 6.55808
[1mStep[0m  [64/84], [94mLoss[0m : 6.80368
[1mStep[0m  [72/84], [94mLoss[0m : 6.93066
[1mStep[0m  [80/84], [94mLoss[0m : 6.98041

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.073, [92mTest[0m: 7.106, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.30015
[1mStep[0m  [8/84], [94mLoss[0m : 6.85981
[1mStep[0m  [16/84], [94mLoss[0m : 6.25238
[1mStep[0m  [24/84], [94mLoss[0m : 6.27102
[1mStep[0m  [32/84], [94mLoss[0m : 6.98180
[1mStep[0m  [40/84], [94mLoss[0m : 6.87439
[1mStep[0m  [48/84], [94mLoss[0m : 7.33306
[1mStep[0m  [56/84], [94mLoss[0m : 7.25451
[1mStep[0m  [64/84], [94mLoss[0m : 6.70691
[1mStep[0m  [72/84], [94mLoss[0m : 7.08313
[1mStep[0m  [80/84], [94mLoss[0m : 6.59841

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.951, [92mTest[0m: 6.983, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.73538
[1mStep[0m  [8/84], [94mLoss[0m : 6.23466
[1mStep[0m  [16/84], [94mLoss[0m : 7.16023
[1mStep[0m  [24/84], [94mLoss[0m : 6.82084
[1mStep[0m  [32/84], [94mLoss[0m : 6.30124
[1mStep[0m  [40/84], [94mLoss[0m : 7.10664
[1mStep[0m  [48/84], [94mLoss[0m : 6.58922
[1mStep[0m  [56/84], [94mLoss[0m : 7.60320
[1mStep[0m  [64/84], [94mLoss[0m : 7.02854
[1mStep[0m  [72/84], [94mLoss[0m : 6.70634
[1mStep[0m  [80/84], [94mLoss[0m : 6.60526

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.816, [92mTest[0m: 6.868, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.41345
[1mStep[0m  [8/84], [94mLoss[0m : 6.57475
[1mStep[0m  [16/84], [94mLoss[0m : 6.29262
[1mStep[0m  [24/84], [94mLoss[0m : 6.79777
[1mStep[0m  [32/84], [94mLoss[0m : 6.40701
[1mStep[0m  [40/84], [94mLoss[0m : 6.41858
[1mStep[0m  [48/84], [94mLoss[0m : 6.63588
[1mStep[0m  [56/84], [94mLoss[0m : 6.61997
[1mStep[0m  [64/84], [94mLoss[0m : 6.96475
[1mStep[0m  [72/84], [94mLoss[0m : 6.51917
[1mStep[0m  [80/84], [94mLoss[0m : 6.44173

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.693, [92mTest[0m: 6.729, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.27069
[1mStep[0m  [8/84], [94mLoss[0m : 6.70271
[1mStep[0m  [16/84], [94mLoss[0m : 6.46344
[1mStep[0m  [24/84], [94mLoss[0m : 6.25662
[1mStep[0m  [32/84], [94mLoss[0m : 6.06920
[1mStep[0m  [40/84], [94mLoss[0m : 6.71747
[1mStep[0m  [48/84], [94mLoss[0m : 7.31643
[1mStep[0m  [56/84], [94mLoss[0m : 5.94077
[1mStep[0m  [64/84], [94mLoss[0m : 6.77580
[1mStep[0m  [72/84], [94mLoss[0m : 6.46798
[1mStep[0m  [80/84], [94mLoss[0m : 6.02541

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.565, [92mTest[0m: 6.605, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.51172
[1mStep[0m  [8/84], [94mLoss[0m : 6.92657
[1mStep[0m  [16/84], [94mLoss[0m : 6.14615
[1mStep[0m  [24/84], [94mLoss[0m : 6.49084
[1mStep[0m  [32/84], [94mLoss[0m : 6.73350
[1mStep[0m  [40/84], [94mLoss[0m : 6.44991
[1mStep[0m  [48/84], [94mLoss[0m : 6.68748
[1mStep[0m  [56/84], [94mLoss[0m : 6.23922
[1mStep[0m  [64/84], [94mLoss[0m : 5.92821
[1mStep[0m  [72/84], [94mLoss[0m : 5.80939
[1mStep[0m  [80/84], [94mLoss[0m : 6.07961

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.445, [92mTest[0m: 6.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.01487
[1mStep[0m  [8/84], [94mLoss[0m : 6.10895
[1mStep[0m  [16/84], [94mLoss[0m : 6.16612
[1mStep[0m  [24/84], [94mLoss[0m : 6.76527
[1mStep[0m  [32/84], [94mLoss[0m : 6.76575
[1mStep[0m  [40/84], [94mLoss[0m : 5.96975
[1mStep[0m  [48/84], [94mLoss[0m : 6.41838
[1mStep[0m  [56/84], [94mLoss[0m : 6.53037
[1mStep[0m  [64/84], [94mLoss[0m : 5.88579
[1mStep[0m  [72/84], [94mLoss[0m : 6.50024
[1mStep[0m  [80/84], [94mLoss[0m : 6.40780

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.312, [92mTest[0m: 6.347, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.55645
[1mStep[0m  [8/84], [94mLoss[0m : 6.40683
[1mStep[0m  [16/84], [94mLoss[0m : 6.35455
[1mStep[0m  [24/84], [94mLoss[0m : 6.51840
[1mStep[0m  [32/84], [94mLoss[0m : 6.00234
[1mStep[0m  [40/84], [94mLoss[0m : 6.00530
[1mStep[0m  [48/84], [94mLoss[0m : 6.45455
[1mStep[0m  [56/84], [94mLoss[0m : 5.93606
[1mStep[0m  [64/84], [94mLoss[0m : 6.00245
[1mStep[0m  [72/84], [94mLoss[0m : 6.30042
[1mStep[0m  [80/84], [94mLoss[0m : 5.71070

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.191, [92mTest[0m: 6.231, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.05067
[1mStep[0m  [8/84], [94mLoss[0m : 5.98912
[1mStep[0m  [16/84], [94mLoss[0m : 5.94161
[1mStep[0m  [24/84], [94mLoss[0m : 5.87145
[1mStep[0m  [32/84], [94mLoss[0m : 5.59473
[1mStep[0m  [40/84], [94mLoss[0m : 5.59697
[1mStep[0m  [48/84], [94mLoss[0m : 6.01838
[1mStep[0m  [56/84], [94mLoss[0m : 6.19155
[1mStep[0m  [64/84], [94mLoss[0m : 5.99586
[1mStep[0m  [72/84], [94mLoss[0m : 6.37358
[1mStep[0m  [80/84], [94mLoss[0m : 6.06395

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.058, [92mTest[0m: 6.109, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.94486
[1mStep[0m  [8/84], [94mLoss[0m : 6.28742
[1mStep[0m  [16/84], [94mLoss[0m : 6.26244
[1mStep[0m  [24/84], [94mLoss[0m : 6.22601
[1mStep[0m  [32/84], [94mLoss[0m : 5.91639
[1mStep[0m  [40/84], [94mLoss[0m : 5.85086
[1mStep[0m  [48/84], [94mLoss[0m : 5.90733
[1mStep[0m  [56/84], [94mLoss[0m : 6.28186
[1mStep[0m  [64/84], [94mLoss[0m : 5.91006
[1mStep[0m  [72/84], [94mLoss[0m : 6.58812
[1mStep[0m  [80/84], [94mLoss[0m : 6.56621

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.945, [92mTest[0m: 5.971, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.93365
[1mStep[0m  [8/84], [94mLoss[0m : 5.87592
[1mStep[0m  [16/84], [94mLoss[0m : 6.25142
[1mStep[0m  [24/84], [94mLoss[0m : 5.88025
[1mStep[0m  [32/84], [94mLoss[0m : 5.78384
[1mStep[0m  [40/84], [94mLoss[0m : 6.15178
[1mStep[0m  [48/84], [94mLoss[0m : 6.30848
[1mStep[0m  [56/84], [94mLoss[0m : 6.07944
[1mStep[0m  [64/84], [94mLoss[0m : 5.90053
[1mStep[0m  [72/84], [94mLoss[0m : 5.96298
[1mStep[0m  [80/84], [94mLoss[0m : 5.01179

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.817, [92mTest[0m: 5.844, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.80466
[1mStep[0m  [8/84], [94mLoss[0m : 6.14131
[1mStep[0m  [16/84], [94mLoss[0m : 5.64442
[1mStep[0m  [24/84], [94mLoss[0m : 6.24713
[1mStep[0m  [32/84], [94mLoss[0m : 5.82108
[1mStep[0m  [40/84], [94mLoss[0m : 5.46563
[1mStep[0m  [48/84], [94mLoss[0m : 6.34072
[1mStep[0m  [56/84], [94mLoss[0m : 5.41049
[1mStep[0m  [64/84], [94mLoss[0m : 6.12074
[1mStep[0m  [72/84], [94mLoss[0m : 5.81687
[1mStep[0m  [80/84], [94mLoss[0m : 5.61649

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.697, [92mTest[0m: 5.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.20901
[1mStep[0m  [8/84], [94mLoss[0m : 5.65120
[1mStep[0m  [16/84], [94mLoss[0m : 5.97649
[1mStep[0m  [24/84], [94mLoss[0m : 5.42924
[1mStep[0m  [32/84], [94mLoss[0m : 5.57962
[1mStep[0m  [40/84], [94mLoss[0m : 5.77354
[1mStep[0m  [48/84], [94mLoss[0m : 5.49109
[1mStep[0m  [56/84], [94mLoss[0m : 5.44365
[1mStep[0m  [64/84], [94mLoss[0m : 5.26521
[1mStep[0m  [72/84], [94mLoss[0m : 5.34474
[1mStep[0m  [80/84], [94mLoss[0m : 5.43687

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.578, [92mTest[0m: 5.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.54233
[1mStep[0m  [8/84], [94mLoss[0m : 5.32546
[1mStep[0m  [16/84], [94mLoss[0m : 5.67248
[1mStep[0m  [24/84], [94mLoss[0m : 5.09497
[1mStep[0m  [32/84], [94mLoss[0m : 5.56412
[1mStep[0m  [40/84], [94mLoss[0m : 5.14743
[1mStep[0m  [48/84], [94mLoss[0m : 5.39859
[1mStep[0m  [56/84], [94mLoss[0m : 5.42639
[1mStep[0m  [64/84], [94mLoss[0m : 5.37157
[1mStep[0m  [72/84], [94mLoss[0m : 5.15485
[1mStep[0m  [80/84], [94mLoss[0m : 4.85305

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.472, [92mTest[0m: 5.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.30810
[1mStep[0m  [8/84], [94mLoss[0m : 5.22139
[1mStep[0m  [16/84], [94mLoss[0m : 5.46112
[1mStep[0m  [24/84], [94mLoss[0m : 4.90806
[1mStep[0m  [32/84], [94mLoss[0m : 5.53810
[1mStep[0m  [40/84], [94mLoss[0m : 5.09137
[1mStep[0m  [48/84], [94mLoss[0m : 5.21163
[1mStep[0m  [56/84], [94mLoss[0m : 5.48262
[1mStep[0m  [64/84], [94mLoss[0m : 5.11628
[1mStep[0m  [72/84], [94mLoss[0m : 5.44805
[1mStep[0m  [80/84], [94mLoss[0m : 5.65638

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.359, [92mTest[0m: 5.373, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.87942
[1mStep[0m  [8/84], [94mLoss[0m : 5.55221
[1mStep[0m  [16/84], [94mLoss[0m : 5.15821
[1mStep[0m  [24/84], [94mLoss[0m : 5.26108
[1mStep[0m  [32/84], [94mLoss[0m : 5.14565
[1mStep[0m  [40/84], [94mLoss[0m : 5.16528
[1mStep[0m  [48/84], [94mLoss[0m : 5.24932
[1mStep[0m  [56/84], [94mLoss[0m : 5.37284
[1mStep[0m  [64/84], [94mLoss[0m : 5.51578
[1mStep[0m  [72/84], [94mLoss[0m : 4.78741
[1mStep[0m  [80/84], [94mLoss[0m : 4.82813

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.237, [92mTest[0m: 5.241, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.24962
[1mStep[0m  [8/84], [94mLoss[0m : 5.38689
[1mStep[0m  [16/84], [94mLoss[0m : 5.31825
[1mStep[0m  [24/84], [94mLoss[0m : 5.12413
[1mStep[0m  [32/84], [94mLoss[0m : 5.07550
[1mStep[0m  [40/84], [94mLoss[0m : 5.66085
[1mStep[0m  [48/84], [94mLoss[0m : 4.72927
[1mStep[0m  [56/84], [94mLoss[0m : 5.22022
[1mStep[0m  [64/84], [94mLoss[0m : 4.95315
[1mStep[0m  [72/84], [94mLoss[0m : 4.60553
[1mStep[0m  [80/84], [94mLoss[0m : 4.88092

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.131, [92mTest[0m: 5.142, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.05071
[1mStep[0m  [8/84], [94mLoss[0m : 5.13570
[1mStep[0m  [16/84], [94mLoss[0m : 5.04855
[1mStep[0m  [24/84], [94mLoss[0m : 5.46764
[1mStep[0m  [32/84], [94mLoss[0m : 5.05713
[1mStep[0m  [40/84], [94mLoss[0m : 5.57521
[1mStep[0m  [48/84], [94mLoss[0m : 4.63444
[1mStep[0m  [56/84], [94mLoss[0m : 4.98001
[1mStep[0m  [64/84], [94mLoss[0m : 5.21727
[1mStep[0m  [72/84], [94mLoss[0m : 5.02681
[1mStep[0m  [80/84], [94mLoss[0m : 5.22980

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.032, [92mTest[0m: 5.034, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.18475
[1mStep[0m  [8/84], [94mLoss[0m : 5.00772
[1mStep[0m  [16/84], [94mLoss[0m : 4.13989
[1mStep[0m  [24/84], [94mLoss[0m : 5.35524
[1mStep[0m  [32/84], [94mLoss[0m : 4.95401
[1mStep[0m  [40/84], [94mLoss[0m : 4.93610
[1mStep[0m  [48/84], [94mLoss[0m : 5.24173
[1mStep[0m  [56/84], [94mLoss[0m : 5.07194
[1mStep[0m  [64/84], [94mLoss[0m : 5.10061
[1mStep[0m  [72/84], [94mLoss[0m : 4.90806
[1mStep[0m  [80/84], [94mLoss[0m : 4.96648

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.942, [92mTest[0m: 4.945, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.63599
[1mStep[0m  [8/84], [94mLoss[0m : 4.67447
[1mStep[0m  [16/84], [94mLoss[0m : 4.89377
[1mStep[0m  [24/84], [94mLoss[0m : 5.04267
[1mStep[0m  [32/84], [94mLoss[0m : 4.75270
[1mStep[0m  [40/84], [94mLoss[0m : 4.36937
[1mStep[0m  [48/84], [94mLoss[0m : 5.04607
[1mStep[0m  [56/84], [94mLoss[0m : 4.39473
[1mStep[0m  [64/84], [94mLoss[0m : 5.04576
[1mStep[0m  [72/84], [94mLoss[0m : 4.81623
[1mStep[0m  [80/84], [94mLoss[0m : 5.07083

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.861, [92mTest[0m: 4.855, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.72882
[1mStep[0m  [8/84], [94mLoss[0m : 4.45441
[1mStep[0m  [16/84], [94mLoss[0m : 4.69165
[1mStep[0m  [24/84], [94mLoss[0m : 5.55673
[1mStep[0m  [32/84], [94mLoss[0m : 5.04567
[1mStep[0m  [40/84], [94mLoss[0m : 4.91512
[1mStep[0m  [48/84], [94mLoss[0m : 5.32799
[1mStep[0m  [56/84], [94mLoss[0m : 4.91124
[1mStep[0m  [64/84], [94mLoss[0m : 4.99392
[1mStep[0m  [72/84], [94mLoss[0m : 4.24701
[1mStep[0m  [80/84], [94mLoss[0m : 4.48283

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.771, [92mTest[0m: 4.748, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.54545
[1mStep[0m  [8/84], [94mLoss[0m : 4.90702
[1mStep[0m  [16/84], [94mLoss[0m : 4.68493
[1mStep[0m  [24/84], [94mLoss[0m : 4.36454
[1mStep[0m  [32/84], [94mLoss[0m : 4.14908
[1mStep[0m  [40/84], [94mLoss[0m : 4.84430
[1mStep[0m  [48/84], [94mLoss[0m : 4.32663
[1mStep[0m  [56/84], [94mLoss[0m : 4.70012
[1mStep[0m  [64/84], [94mLoss[0m : 4.76628
[1mStep[0m  [72/84], [94mLoss[0m : 4.35459
[1mStep[0m  [80/84], [94mLoss[0m : 4.89431

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.686, [92mTest[0m: 4.680, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.20798
[1mStep[0m  [8/84], [94mLoss[0m : 4.98708
[1mStep[0m  [16/84], [94mLoss[0m : 4.26524
[1mStep[0m  [24/84], [94mLoss[0m : 4.85737
[1mStep[0m  [32/84], [94mLoss[0m : 4.44780
[1mStep[0m  [40/84], [94mLoss[0m : 4.48010
[1mStep[0m  [48/84], [94mLoss[0m : 5.04406
[1mStep[0m  [56/84], [94mLoss[0m : 4.52032
[1mStep[0m  [64/84], [94mLoss[0m : 4.35542
[1mStep[0m  [72/84], [94mLoss[0m : 4.84341
[1mStep[0m  [80/84], [94mLoss[0m : 4.62782

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.600, [92mTest[0m: 4.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.48420
[1mStep[0m  [8/84], [94mLoss[0m : 4.36579
[1mStep[0m  [16/84], [94mLoss[0m : 4.17316
[1mStep[0m  [24/84], [94mLoss[0m : 4.18806
[1mStep[0m  [32/84], [94mLoss[0m : 4.38753
[1mStep[0m  [40/84], [94mLoss[0m : 4.09932
[1mStep[0m  [48/84], [94mLoss[0m : 4.18017
[1mStep[0m  [56/84], [94mLoss[0m : 4.32204
[1mStep[0m  [64/84], [94mLoss[0m : 4.48837
[1mStep[0m  [72/84], [94mLoss[0m : 4.63577
[1mStep[0m  [80/84], [94mLoss[0m : 4.73323

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.529, [92mTest[0m: 4.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.02537
[1mStep[0m  [8/84], [94mLoss[0m : 4.30960
[1mStep[0m  [16/84], [94mLoss[0m : 4.63583
[1mStep[0m  [24/84], [94mLoss[0m : 4.70841
[1mStep[0m  [32/84], [94mLoss[0m : 4.32751
[1mStep[0m  [40/84], [94mLoss[0m : 4.70655
[1mStep[0m  [48/84], [94mLoss[0m : 4.31134
[1mStep[0m  [56/84], [94mLoss[0m : 5.04340
[1mStep[0m  [64/84], [94mLoss[0m : 4.58177
[1mStep[0m  [72/84], [94mLoss[0m : 4.15372
[1mStep[0m  [80/84], [94mLoss[0m : 4.04513

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.456, [92mTest[0m: 4.430, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.67531
[1mStep[0m  [8/84], [94mLoss[0m : 4.22785
[1mStep[0m  [16/84], [94mLoss[0m : 4.45384
[1mStep[0m  [24/84], [94mLoss[0m : 4.31862
[1mStep[0m  [32/84], [94mLoss[0m : 4.04786
[1mStep[0m  [40/84], [94mLoss[0m : 4.31742
[1mStep[0m  [48/84], [94mLoss[0m : 4.19529
[1mStep[0m  [56/84], [94mLoss[0m : 4.29550
[1mStep[0m  [64/84], [94mLoss[0m : 4.37740
[1mStep[0m  [72/84], [94mLoss[0m : 4.38199
[1mStep[0m  [80/84], [94mLoss[0m : 4.24543

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.369, [92mTest[0m: 4.334, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.53718
[1mStep[0m  [8/84], [94mLoss[0m : 4.18381
[1mStep[0m  [16/84], [94mLoss[0m : 4.19182
[1mStep[0m  [24/84], [94mLoss[0m : 4.38889
[1mStep[0m  [32/84], [94mLoss[0m : 4.03547
[1mStep[0m  [40/84], [94mLoss[0m : 4.57230
[1mStep[0m  [48/84], [94mLoss[0m : 4.13014
[1mStep[0m  [56/84], [94mLoss[0m : 4.26132
[1mStep[0m  [64/84], [94mLoss[0m : 4.69480
[1mStep[0m  [72/84], [94mLoss[0m : 3.98623
[1mStep[0m  [80/84], [94mLoss[0m : 4.23662

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.299, [92mTest[0m: 4.267, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.199
====================================

Phase 2 - Evaluation MAE:  4.1986633368900845
MAE score P1       7.487344
MAE score P2       4.198663
loss               4.298546
learning_rate        0.0001
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay           0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.03972
[1mStep[0m  [16/169], [94mLoss[0m : 11.58534
[1mStep[0m  [32/169], [94mLoss[0m : 11.55842
[1mStep[0m  [48/169], [94mLoss[0m : 11.45009
[1mStep[0m  [64/169], [94mLoss[0m : 10.51381
[1mStep[0m  [80/169], [94mLoss[0m : 11.26388
[1mStep[0m  [96/169], [94mLoss[0m : 10.85406
[1mStep[0m  [112/169], [94mLoss[0m : 10.81696
[1mStep[0m  [128/169], [94mLoss[0m : 10.52192
[1mStep[0m  [144/169], [94mLoss[0m : 10.72618
[1mStep[0m  [160/169], [94mLoss[0m : 10.11287

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.954, [92mTest[0m: 11.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.79554
[1mStep[0m  [16/169], [94mLoss[0m : 10.55221
[1mStep[0m  [32/169], [94mLoss[0m : 10.99023
[1mStep[0m  [48/169], [94mLoss[0m : 11.49094
[1mStep[0m  [64/169], [94mLoss[0m : 10.29394
[1mStep[0m  [80/169], [94mLoss[0m : 10.61461
[1mStep[0m  [96/169], [94mLoss[0m : 10.49261
[1mStep[0m  [112/169], [94mLoss[0m : 10.54399
[1mStep[0m  [128/169], [94mLoss[0m : 10.40290
[1mStep[0m  [144/169], [94mLoss[0m : 10.38012
[1mStep[0m  [160/169], [94mLoss[0m : 10.26453

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.459, [92mTest[0m: 10.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.68930
[1mStep[0m  [16/169], [94mLoss[0m : 10.65565
[1mStep[0m  [32/169], [94mLoss[0m : 10.41010
[1mStep[0m  [48/169], [94mLoss[0m : 10.49720
[1mStep[0m  [64/169], [94mLoss[0m : 10.27531
[1mStep[0m  [80/169], [94mLoss[0m : 10.21028
[1mStep[0m  [96/169], [94mLoss[0m : 10.33371
[1mStep[0m  [112/169], [94mLoss[0m : 9.52050
[1mStep[0m  [128/169], [94mLoss[0m : 10.14210
[1mStep[0m  [144/169], [94mLoss[0m : 10.13771
[1mStep[0m  [160/169], [94mLoss[0m : 9.84833

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.961, [92mTest[0m: 10.220, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.61214
[1mStep[0m  [16/169], [94mLoss[0m : 10.29361
[1mStep[0m  [32/169], [94mLoss[0m : 9.83979
[1mStep[0m  [48/169], [94mLoss[0m : 9.65876
[1mStep[0m  [64/169], [94mLoss[0m : 9.17784
[1mStep[0m  [80/169], [94mLoss[0m : 9.14397
[1mStep[0m  [96/169], [94mLoss[0m : 10.32209
[1mStep[0m  [112/169], [94mLoss[0m : 9.83883
[1mStep[0m  [128/169], [94mLoss[0m : 9.02380
[1mStep[0m  [144/169], [94mLoss[0m : 9.32286
[1mStep[0m  [160/169], [94mLoss[0m : 9.13624

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.486, [92mTest[0m: 9.721, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.39126
[1mStep[0m  [16/169], [94mLoss[0m : 8.83307
[1mStep[0m  [32/169], [94mLoss[0m : 8.56601
[1mStep[0m  [48/169], [94mLoss[0m : 9.60353
[1mStep[0m  [64/169], [94mLoss[0m : 9.08216
[1mStep[0m  [80/169], [94mLoss[0m : 9.43680
[1mStep[0m  [96/169], [94mLoss[0m : 9.17723
[1mStep[0m  [112/169], [94mLoss[0m : 9.14735
[1mStep[0m  [128/169], [94mLoss[0m : 9.07473
[1mStep[0m  [144/169], [94mLoss[0m : 9.38444
[1mStep[0m  [160/169], [94mLoss[0m : 9.24406

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.995, [92mTest[0m: 9.225, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.78895
[1mStep[0m  [16/169], [94mLoss[0m : 9.02017
[1mStep[0m  [32/169], [94mLoss[0m : 8.75145
[1mStep[0m  [48/169], [94mLoss[0m : 8.61127
[1mStep[0m  [64/169], [94mLoss[0m : 8.83104
[1mStep[0m  [80/169], [94mLoss[0m : 8.27167
[1mStep[0m  [96/169], [94mLoss[0m : 8.69886
[1mStep[0m  [112/169], [94mLoss[0m : 8.13797
[1mStep[0m  [128/169], [94mLoss[0m : 8.10840
[1mStep[0m  [144/169], [94mLoss[0m : 8.72555
[1mStep[0m  [160/169], [94mLoss[0m : 8.51448

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.508, [92mTest[0m: 8.750, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.13281
[1mStep[0m  [16/169], [94mLoss[0m : 8.26257
[1mStep[0m  [32/169], [94mLoss[0m : 8.88498
[1mStep[0m  [48/169], [94mLoss[0m : 8.58402
[1mStep[0m  [64/169], [94mLoss[0m : 7.77669
[1mStep[0m  [80/169], [94mLoss[0m : 7.40030
[1mStep[0m  [96/169], [94mLoss[0m : 8.39813
[1mStep[0m  [112/169], [94mLoss[0m : 7.76929
[1mStep[0m  [128/169], [94mLoss[0m : 8.44200
[1mStep[0m  [144/169], [94mLoss[0m : 8.14505
[1mStep[0m  [160/169], [94mLoss[0m : 7.36847

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.006, [92mTest[0m: 8.267, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.73027
[1mStep[0m  [16/169], [94mLoss[0m : 6.57512
[1mStep[0m  [32/169], [94mLoss[0m : 7.19517
[1mStep[0m  [48/169], [94mLoss[0m : 7.39802
[1mStep[0m  [64/169], [94mLoss[0m : 7.26774
[1mStep[0m  [80/169], [94mLoss[0m : 7.64736
[1mStep[0m  [96/169], [94mLoss[0m : 7.01884
[1mStep[0m  [112/169], [94mLoss[0m : 7.69030
[1mStep[0m  [128/169], [94mLoss[0m : 7.37463
[1mStep[0m  [144/169], [94mLoss[0m : 7.84315
[1mStep[0m  [160/169], [94mLoss[0m : 6.97991

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.522, [92mTest[0m: 7.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.37421
[1mStep[0m  [16/169], [94mLoss[0m : 6.73558
[1mStep[0m  [32/169], [94mLoss[0m : 7.89869
[1mStep[0m  [48/169], [94mLoss[0m : 7.34390
[1mStep[0m  [64/169], [94mLoss[0m : 6.87885
[1mStep[0m  [80/169], [94mLoss[0m : 7.25912
[1mStep[0m  [96/169], [94mLoss[0m : 6.04107
[1mStep[0m  [112/169], [94mLoss[0m : 6.63086
[1mStep[0m  [128/169], [94mLoss[0m : 7.16401
[1mStep[0m  [144/169], [94mLoss[0m : 6.64023
[1mStep[0m  [160/169], [94mLoss[0m : 7.47621

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.043, [92mTest[0m: 7.288, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.95852
[1mStep[0m  [16/169], [94mLoss[0m : 6.60759
[1mStep[0m  [32/169], [94mLoss[0m : 6.76434
[1mStep[0m  [48/169], [94mLoss[0m : 7.03967
[1mStep[0m  [64/169], [94mLoss[0m : 6.94647
[1mStep[0m  [80/169], [94mLoss[0m : 6.90403
[1mStep[0m  [96/169], [94mLoss[0m : 7.03381
[1mStep[0m  [112/169], [94mLoss[0m : 6.22388
[1mStep[0m  [128/169], [94mLoss[0m : 6.19209
[1mStep[0m  [144/169], [94mLoss[0m : 6.02348
[1mStep[0m  [160/169], [94mLoss[0m : 6.39456

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.556, [92mTest[0m: 6.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.11565
[1mStep[0m  [16/169], [94mLoss[0m : 6.79828
[1mStep[0m  [32/169], [94mLoss[0m : 6.73987
[1mStep[0m  [48/169], [94mLoss[0m : 5.57098
[1mStep[0m  [64/169], [94mLoss[0m : 6.05387
[1mStep[0m  [80/169], [94mLoss[0m : 6.67185
[1mStep[0m  [96/169], [94mLoss[0m : 5.79186
[1mStep[0m  [112/169], [94mLoss[0m : 6.33507
[1mStep[0m  [128/169], [94mLoss[0m : 5.99192
[1mStep[0m  [144/169], [94mLoss[0m : 5.62595
[1mStep[0m  [160/169], [94mLoss[0m : 6.59049

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.065, [92mTest[0m: 6.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.61296
[1mStep[0m  [16/169], [94mLoss[0m : 6.39080
[1mStep[0m  [32/169], [94mLoss[0m : 5.72640
[1mStep[0m  [48/169], [94mLoss[0m : 5.59831
[1mStep[0m  [64/169], [94mLoss[0m : 6.05124
[1mStep[0m  [80/169], [94mLoss[0m : 4.93309
[1mStep[0m  [96/169], [94mLoss[0m : 5.48340
[1mStep[0m  [112/169], [94mLoss[0m : 5.18643
[1mStep[0m  [128/169], [94mLoss[0m : 5.59723
[1mStep[0m  [144/169], [94mLoss[0m : 6.01327
[1mStep[0m  [160/169], [94mLoss[0m : 4.88996

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.599, [92mTest[0m: 5.822, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.43418
[1mStep[0m  [16/169], [94mLoss[0m : 5.37585
[1mStep[0m  [32/169], [94mLoss[0m : 5.30791
[1mStep[0m  [48/169], [94mLoss[0m : 5.05060
[1mStep[0m  [64/169], [94mLoss[0m : 4.75073
[1mStep[0m  [80/169], [94mLoss[0m : 5.20866
[1mStep[0m  [96/169], [94mLoss[0m : 5.15872
[1mStep[0m  [112/169], [94mLoss[0m : 5.22520
[1mStep[0m  [128/169], [94mLoss[0m : 5.03504
[1mStep[0m  [144/169], [94mLoss[0m : 4.87204
[1mStep[0m  [160/169], [94mLoss[0m : 5.11413

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.155, [92mTest[0m: 5.372, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.21621
[1mStep[0m  [16/169], [94mLoss[0m : 4.64847
[1mStep[0m  [32/169], [94mLoss[0m : 3.70862
[1mStep[0m  [48/169], [94mLoss[0m : 4.74719
[1mStep[0m  [64/169], [94mLoss[0m : 5.22475
[1mStep[0m  [80/169], [94mLoss[0m : 4.62854
[1mStep[0m  [96/169], [94mLoss[0m : 4.91717
[1mStep[0m  [112/169], [94mLoss[0m : 4.81317
[1mStep[0m  [128/169], [94mLoss[0m : 4.13000
[1mStep[0m  [144/169], [94mLoss[0m : 4.83022
[1mStep[0m  [160/169], [94mLoss[0m : 4.90941

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.777, [92mTest[0m: 4.938, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.69934
[1mStep[0m  [16/169], [94mLoss[0m : 5.08461
[1mStep[0m  [32/169], [94mLoss[0m : 4.52722
[1mStep[0m  [48/169], [94mLoss[0m : 4.46715
[1mStep[0m  [64/169], [94mLoss[0m : 4.48271
[1mStep[0m  [80/169], [94mLoss[0m : 4.57293
[1mStep[0m  [96/169], [94mLoss[0m : 4.78991
[1mStep[0m  [112/169], [94mLoss[0m : 4.77923
[1mStep[0m  [128/169], [94mLoss[0m : 3.88935
[1mStep[0m  [144/169], [94mLoss[0m : 3.95411
[1mStep[0m  [160/169], [94mLoss[0m : 4.22223

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.452, [92mTest[0m: 4.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.59786
[1mStep[0m  [16/169], [94mLoss[0m : 3.94820
[1mStep[0m  [32/169], [94mLoss[0m : 4.41172
[1mStep[0m  [48/169], [94mLoss[0m : 4.33335
[1mStep[0m  [64/169], [94mLoss[0m : 5.13033
[1mStep[0m  [80/169], [94mLoss[0m : 4.62350
[1mStep[0m  [96/169], [94mLoss[0m : 4.17911
[1mStep[0m  [112/169], [94mLoss[0m : 3.98579
[1mStep[0m  [128/169], [94mLoss[0m : 3.74026
[1mStep[0m  [144/169], [94mLoss[0m : 3.95159
[1mStep[0m  [160/169], [94mLoss[0m : 3.59792

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.155, [92mTest[0m: 4.284, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.99795
[1mStep[0m  [16/169], [94mLoss[0m : 4.27188
[1mStep[0m  [32/169], [94mLoss[0m : 3.82655
[1mStep[0m  [48/169], [94mLoss[0m : 3.62929
[1mStep[0m  [64/169], [94mLoss[0m : 4.13633
[1mStep[0m  [80/169], [94mLoss[0m : 3.58792
[1mStep[0m  [96/169], [94mLoss[0m : 3.97345
[1mStep[0m  [112/169], [94mLoss[0m : 4.31517
[1mStep[0m  [128/169], [94mLoss[0m : 4.23338
[1mStep[0m  [144/169], [94mLoss[0m : 4.06051
[1mStep[0m  [160/169], [94mLoss[0m : 4.42262

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.929, [92mTest[0m: 4.017, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.56971
[1mStep[0m  [16/169], [94mLoss[0m : 4.20157
[1mStep[0m  [32/169], [94mLoss[0m : 3.48390
[1mStep[0m  [48/169], [94mLoss[0m : 3.40587
[1mStep[0m  [64/169], [94mLoss[0m : 3.83861
[1mStep[0m  [80/169], [94mLoss[0m : 3.46073
[1mStep[0m  [96/169], [94mLoss[0m : 3.73103
[1mStep[0m  [112/169], [94mLoss[0m : 3.90086
[1mStep[0m  [128/169], [94mLoss[0m : 3.84807
[1mStep[0m  [144/169], [94mLoss[0m : 3.71612
[1mStep[0m  [160/169], [94mLoss[0m : 3.75496

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.739, [92mTest[0m: 3.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.74359
[1mStep[0m  [16/169], [94mLoss[0m : 4.04276
[1mStep[0m  [32/169], [94mLoss[0m : 4.10263
[1mStep[0m  [48/169], [94mLoss[0m : 4.31290
[1mStep[0m  [64/169], [94mLoss[0m : 3.62025
[1mStep[0m  [80/169], [94mLoss[0m : 3.53874
[1mStep[0m  [96/169], [94mLoss[0m : 3.20035
[1mStep[0m  [112/169], [94mLoss[0m : 3.85374
[1mStep[0m  [128/169], [94mLoss[0m : 3.81492
[1mStep[0m  [144/169], [94mLoss[0m : 3.13139
[1mStep[0m  [160/169], [94mLoss[0m : 3.64480

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.583, [92mTest[0m: 3.613, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.95201
[1mStep[0m  [16/169], [94mLoss[0m : 3.66197
[1mStep[0m  [32/169], [94mLoss[0m : 3.60233
[1mStep[0m  [48/169], [94mLoss[0m : 3.41332
[1mStep[0m  [64/169], [94mLoss[0m : 3.21068
[1mStep[0m  [80/169], [94mLoss[0m : 4.07146
[1mStep[0m  [96/169], [94mLoss[0m : 3.38609
[1mStep[0m  [112/169], [94mLoss[0m : 3.09556
[1mStep[0m  [128/169], [94mLoss[0m : 3.45391
[1mStep[0m  [144/169], [94mLoss[0m : 3.49506
[1mStep[0m  [160/169], [94mLoss[0m : 3.96548

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.446, [92mTest[0m: 3.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.41895
[1mStep[0m  [16/169], [94mLoss[0m : 3.62237
[1mStep[0m  [32/169], [94mLoss[0m : 3.22988
[1mStep[0m  [48/169], [94mLoss[0m : 3.68668
[1mStep[0m  [64/169], [94mLoss[0m : 3.01607
[1mStep[0m  [80/169], [94mLoss[0m : 3.69234
[1mStep[0m  [96/169], [94mLoss[0m : 4.27541
[1mStep[0m  [112/169], [94mLoss[0m : 3.88062
[1mStep[0m  [128/169], [94mLoss[0m : 2.59635
[1mStep[0m  [144/169], [94mLoss[0m : 3.63639
[1mStep[0m  [160/169], [94mLoss[0m : 2.98284

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.338, [92mTest[0m: 3.323, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.23907
[1mStep[0m  [16/169], [94mLoss[0m : 2.64527
[1mStep[0m  [32/169], [94mLoss[0m : 2.95051
[1mStep[0m  [48/169], [94mLoss[0m : 2.87033
[1mStep[0m  [64/169], [94mLoss[0m : 2.97354
[1mStep[0m  [80/169], [94mLoss[0m : 3.36804
[1mStep[0m  [96/169], [94mLoss[0m : 3.09063
[1mStep[0m  [112/169], [94mLoss[0m : 3.22341
[1mStep[0m  [128/169], [94mLoss[0m : 2.84813
[1mStep[0m  [144/169], [94mLoss[0m : 3.91929
[1mStep[0m  [160/169], [94mLoss[0m : 3.22554

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.267, [92mTest[0m: 3.226, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.21091
[1mStep[0m  [16/169], [94mLoss[0m : 3.27997
[1mStep[0m  [32/169], [94mLoss[0m : 2.84826
[1mStep[0m  [48/169], [94mLoss[0m : 3.28435
[1mStep[0m  [64/169], [94mLoss[0m : 2.66186
[1mStep[0m  [80/169], [94mLoss[0m : 3.58965
[1mStep[0m  [96/169], [94mLoss[0m : 3.03830
[1mStep[0m  [112/169], [94mLoss[0m : 2.82450
[1mStep[0m  [128/169], [94mLoss[0m : 3.24133
[1mStep[0m  [144/169], [94mLoss[0m : 2.68299
[1mStep[0m  [160/169], [94mLoss[0m : 3.27701

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.202, [92mTest[0m: 3.139, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.59877
[1mStep[0m  [16/169], [94mLoss[0m : 3.01946
[1mStep[0m  [32/169], [94mLoss[0m : 3.35146
[1mStep[0m  [48/169], [94mLoss[0m : 3.82058
[1mStep[0m  [64/169], [94mLoss[0m : 3.06759
[1mStep[0m  [80/169], [94mLoss[0m : 2.89878
[1mStep[0m  [96/169], [94mLoss[0m : 3.21690
[1mStep[0m  [112/169], [94mLoss[0m : 3.25588
[1mStep[0m  [128/169], [94mLoss[0m : 3.17136
[1mStep[0m  [144/169], [94mLoss[0m : 3.41572
[1mStep[0m  [160/169], [94mLoss[0m : 2.95428

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.130, [92mTest[0m: 3.063, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.47895
[1mStep[0m  [16/169], [94mLoss[0m : 2.88461
[1mStep[0m  [32/169], [94mLoss[0m : 3.62667
[1mStep[0m  [48/169], [94mLoss[0m : 3.40942
[1mStep[0m  [64/169], [94mLoss[0m : 3.02876
[1mStep[0m  [80/169], [94mLoss[0m : 3.16683
[1mStep[0m  [96/169], [94mLoss[0m : 3.46518
[1mStep[0m  [112/169], [94mLoss[0m : 3.12275
[1mStep[0m  [128/169], [94mLoss[0m : 3.77480
[1mStep[0m  [144/169], [94mLoss[0m : 3.45848
[1mStep[0m  [160/169], [94mLoss[0m : 2.91345

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.107, [92mTest[0m: 3.021, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73366
[1mStep[0m  [16/169], [94mLoss[0m : 3.26099
[1mStep[0m  [32/169], [94mLoss[0m : 3.18952
[1mStep[0m  [48/169], [94mLoss[0m : 3.02321
[1mStep[0m  [64/169], [94mLoss[0m : 2.67489
[1mStep[0m  [80/169], [94mLoss[0m : 2.93873
[1mStep[0m  [96/169], [94mLoss[0m : 2.75663
[1mStep[0m  [112/169], [94mLoss[0m : 2.92657
[1mStep[0m  [128/169], [94mLoss[0m : 3.39400
[1mStep[0m  [144/169], [94mLoss[0m : 2.90554
[1mStep[0m  [160/169], [94mLoss[0m : 3.10458

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.055, [92mTest[0m: 2.959, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.19095
[1mStep[0m  [16/169], [94mLoss[0m : 3.12599
[1mStep[0m  [32/169], [94mLoss[0m : 2.86170
[1mStep[0m  [48/169], [94mLoss[0m : 3.07631
[1mStep[0m  [64/169], [94mLoss[0m : 2.65734
[1mStep[0m  [80/169], [94mLoss[0m : 3.25292
[1mStep[0m  [96/169], [94mLoss[0m : 2.96281
[1mStep[0m  [112/169], [94mLoss[0m : 3.06578
[1mStep[0m  [128/169], [94mLoss[0m : 3.14065
[1mStep[0m  [144/169], [94mLoss[0m : 2.78072
[1mStep[0m  [160/169], [94mLoss[0m : 2.60506

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.011, [92mTest[0m: 2.930, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32474
[1mStep[0m  [16/169], [94mLoss[0m : 2.34788
[1mStep[0m  [32/169], [94mLoss[0m : 3.67484
[1mStep[0m  [48/169], [94mLoss[0m : 3.14305
[1mStep[0m  [64/169], [94mLoss[0m : 3.30471
[1mStep[0m  [80/169], [94mLoss[0m : 3.04761
[1mStep[0m  [96/169], [94mLoss[0m : 2.84468
[1mStep[0m  [112/169], [94mLoss[0m : 2.88913
[1mStep[0m  [128/169], [94mLoss[0m : 3.55044
[1mStep[0m  [144/169], [94mLoss[0m : 2.82251
[1mStep[0m  [160/169], [94mLoss[0m : 2.43089

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.981, [92mTest[0m: 2.890, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44169
[1mStep[0m  [16/169], [94mLoss[0m : 3.62834
[1mStep[0m  [32/169], [94mLoss[0m : 2.94897
[1mStep[0m  [48/169], [94mLoss[0m : 2.94154
[1mStep[0m  [64/169], [94mLoss[0m : 3.23603
[1mStep[0m  [80/169], [94mLoss[0m : 3.16518
[1mStep[0m  [96/169], [94mLoss[0m : 2.60551
[1mStep[0m  [112/169], [94mLoss[0m : 3.50565
[1mStep[0m  [128/169], [94mLoss[0m : 2.58350
[1mStep[0m  [144/169], [94mLoss[0m : 2.97849
[1mStep[0m  [160/169], [94mLoss[0m : 3.06423

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.955, [92mTest[0m: 2.858, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90108
[1mStep[0m  [16/169], [94mLoss[0m : 2.93439
[1mStep[0m  [32/169], [94mLoss[0m : 2.96812
[1mStep[0m  [48/169], [94mLoss[0m : 3.23564
[1mStep[0m  [64/169], [94mLoss[0m : 2.39404
[1mStep[0m  [80/169], [94mLoss[0m : 3.12141
[1mStep[0m  [96/169], [94mLoss[0m : 2.83398
[1mStep[0m  [112/169], [94mLoss[0m : 3.57330
[1mStep[0m  [128/169], [94mLoss[0m : 2.57353
[1mStep[0m  [144/169], [94mLoss[0m : 2.83872
[1mStep[0m  [160/169], [94mLoss[0m : 2.53507

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.936, [92mTest[0m: 2.816, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.780
====================================

Phase 1 - Evaluation MAE:  2.7801590902464732
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.80632
[1mStep[0m  [16/169], [94mLoss[0m : 3.21517
[1mStep[0m  [32/169], [94mLoss[0m : 3.30167
[1mStep[0m  [48/169], [94mLoss[0m : 3.23737
[1mStep[0m  [64/169], [94mLoss[0m : 2.74322
[1mStep[0m  [80/169], [94mLoss[0m : 3.44527
[1mStep[0m  [96/169], [94mLoss[0m : 2.39945
[1mStep[0m  [112/169], [94mLoss[0m : 3.15540
[1mStep[0m  [128/169], [94mLoss[0m : 3.28863
[1mStep[0m  [144/169], [94mLoss[0m : 3.01216
[1mStep[0m  [160/169], [94mLoss[0m : 2.54572

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.904, [92mTest[0m: 2.786, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87194
[1mStep[0m  [16/169], [94mLoss[0m : 2.47519
[1mStep[0m  [32/169], [94mLoss[0m : 3.12924
[1mStep[0m  [48/169], [94mLoss[0m : 2.93583
[1mStep[0m  [64/169], [94mLoss[0m : 2.63443
[1mStep[0m  [80/169], [94mLoss[0m : 3.21719
[1mStep[0m  [96/169], [94mLoss[0m : 2.39521
[1mStep[0m  [112/169], [94mLoss[0m : 2.61711
[1mStep[0m  [128/169], [94mLoss[0m : 2.87016
[1mStep[0m  [144/169], [94mLoss[0m : 2.73737
[1mStep[0m  [160/169], [94mLoss[0m : 3.27049

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.870, [92mTest[0m: 2.739, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83305
[1mStep[0m  [16/169], [94mLoss[0m : 2.98567
[1mStep[0m  [32/169], [94mLoss[0m : 3.31250
[1mStep[0m  [48/169], [94mLoss[0m : 2.82829
[1mStep[0m  [64/169], [94mLoss[0m : 3.08566
[1mStep[0m  [80/169], [94mLoss[0m : 2.67226
[1mStep[0m  [96/169], [94mLoss[0m : 2.54430
[1mStep[0m  [112/169], [94mLoss[0m : 2.70254
[1mStep[0m  [128/169], [94mLoss[0m : 3.24504
[1mStep[0m  [144/169], [94mLoss[0m : 2.92122
[1mStep[0m  [160/169], [94mLoss[0m : 3.30150

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.845, [92mTest[0m: 2.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62700
[1mStep[0m  [16/169], [94mLoss[0m : 3.04996
[1mStep[0m  [32/169], [94mLoss[0m : 2.92816
[1mStep[0m  [48/169], [94mLoss[0m : 2.70072
[1mStep[0m  [64/169], [94mLoss[0m : 2.69800
[1mStep[0m  [80/169], [94mLoss[0m : 2.49617
[1mStep[0m  [96/169], [94mLoss[0m : 2.56147
[1mStep[0m  [112/169], [94mLoss[0m : 3.11922
[1mStep[0m  [128/169], [94mLoss[0m : 2.61566
[1mStep[0m  [144/169], [94mLoss[0m : 2.70001
[1mStep[0m  [160/169], [94mLoss[0m : 3.41420

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.821, [92mTest[0m: 2.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.13556
[1mStep[0m  [16/169], [94mLoss[0m : 3.35351
[1mStep[0m  [32/169], [94mLoss[0m : 3.43817
[1mStep[0m  [48/169], [94mLoss[0m : 2.84696
[1mStep[0m  [64/169], [94mLoss[0m : 3.14226
[1mStep[0m  [80/169], [94mLoss[0m : 2.52287
[1mStep[0m  [96/169], [94mLoss[0m : 2.63189
[1mStep[0m  [112/169], [94mLoss[0m : 2.56640
[1mStep[0m  [128/169], [94mLoss[0m : 2.64889
[1mStep[0m  [144/169], [94mLoss[0m : 2.99432
[1mStep[0m  [160/169], [94mLoss[0m : 2.85890

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.801, [92mTest[0m: 2.643, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78668
[1mStep[0m  [16/169], [94mLoss[0m : 3.12447
[1mStep[0m  [32/169], [94mLoss[0m : 3.10364
[1mStep[0m  [48/169], [94mLoss[0m : 2.40496
[1mStep[0m  [64/169], [94mLoss[0m : 3.12445
[1mStep[0m  [80/169], [94mLoss[0m : 2.83329
[1mStep[0m  [96/169], [94mLoss[0m : 2.96938
[1mStep[0m  [112/169], [94mLoss[0m : 3.01144
[1mStep[0m  [128/169], [94mLoss[0m : 2.67576
[1mStep[0m  [144/169], [94mLoss[0m : 2.53645
[1mStep[0m  [160/169], [94mLoss[0m : 2.33163

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.787, [92mTest[0m: 2.609, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81553
[1mStep[0m  [16/169], [94mLoss[0m : 2.80022
[1mStep[0m  [32/169], [94mLoss[0m : 2.77351
[1mStep[0m  [48/169], [94mLoss[0m : 2.79073
[1mStep[0m  [64/169], [94mLoss[0m : 2.83511
[1mStep[0m  [80/169], [94mLoss[0m : 2.76638
[1mStep[0m  [96/169], [94mLoss[0m : 2.52968
[1mStep[0m  [112/169], [94mLoss[0m : 2.46761
[1mStep[0m  [128/169], [94mLoss[0m : 2.94982
[1mStep[0m  [144/169], [94mLoss[0m : 3.35160
[1mStep[0m  [160/169], [94mLoss[0m : 2.87192

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.589, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71867
[1mStep[0m  [16/169], [94mLoss[0m : 2.26124
[1mStep[0m  [32/169], [94mLoss[0m : 2.52268
[1mStep[0m  [48/169], [94mLoss[0m : 3.48540
[1mStep[0m  [64/169], [94mLoss[0m : 2.82392
[1mStep[0m  [80/169], [94mLoss[0m : 3.36733
[1mStep[0m  [96/169], [94mLoss[0m : 3.07697
[1mStep[0m  [112/169], [94mLoss[0m : 2.96332
[1mStep[0m  [128/169], [94mLoss[0m : 2.75650
[1mStep[0m  [144/169], [94mLoss[0m : 2.40151
[1mStep[0m  [160/169], [94mLoss[0m : 2.08210

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34116
[1mStep[0m  [16/169], [94mLoss[0m : 3.27414
[1mStep[0m  [32/169], [94mLoss[0m : 2.61342
[1mStep[0m  [48/169], [94mLoss[0m : 2.62879
[1mStep[0m  [64/169], [94mLoss[0m : 2.88877
[1mStep[0m  [80/169], [94mLoss[0m : 2.58669
[1mStep[0m  [96/169], [94mLoss[0m : 2.37188
[1mStep[0m  [112/169], [94mLoss[0m : 3.08158
[1mStep[0m  [128/169], [94mLoss[0m : 2.36900
[1mStep[0m  [144/169], [94mLoss[0m : 2.66568
[1mStep[0m  [160/169], [94mLoss[0m : 2.77025

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96952
[1mStep[0m  [16/169], [94mLoss[0m : 3.33912
[1mStep[0m  [32/169], [94mLoss[0m : 2.72487
[1mStep[0m  [48/169], [94mLoss[0m : 2.65733
[1mStep[0m  [64/169], [94mLoss[0m : 3.00433
[1mStep[0m  [80/169], [94mLoss[0m : 2.85593
[1mStep[0m  [96/169], [94mLoss[0m : 2.88313
[1mStep[0m  [112/169], [94mLoss[0m : 2.95572
[1mStep[0m  [128/169], [94mLoss[0m : 2.59013
[1mStep[0m  [144/169], [94mLoss[0m : 2.34938
[1mStep[0m  [160/169], [94mLoss[0m : 2.88596

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.744, [92mTest[0m: 2.553, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59354
[1mStep[0m  [16/169], [94mLoss[0m : 2.74004
[1mStep[0m  [32/169], [94mLoss[0m : 2.81299
[1mStep[0m  [48/169], [94mLoss[0m : 2.27340
[1mStep[0m  [64/169], [94mLoss[0m : 3.01701
[1mStep[0m  [80/169], [94mLoss[0m : 2.41244
[1mStep[0m  [96/169], [94mLoss[0m : 2.21156
[1mStep[0m  [112/169], [94mLoss[0m : 2.75220
[1mStep[0m  [128/169], [94mLoss[0m : 2.44550
[1mStep[0m  [144/169], [94mLoss[0m : 2.53293
[1mStep[0m  [160/169], [94mLoss[0m : 2.77582

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27392
[1mStep[0m  [16/169], [94mLoss[0m : 2.41849
[1mStep[0m  [32/169], [94mLoss[0m : 2.95475
[1mStep[0m  [48/169], [94mLoss[0m : 2.66915
[1mStep[0m  [64/169], [94mLoss[0m : 2.36441
[1mStep[0m  [80/169], [94mLoss[0m : 2.62953
[1mStep[0m  [96/169], [94mLoss[0m : 2.46491
[1mStep[0m  [112/169], [94mLoss[0m : 3.52349
[1mStep[0m  [128/169], [94mLoss[0m : 3.06728
[1mStep[0m  [144/169], [94mLoss[0m : 3.16233
[1mStep[0m  [160/169], [94mLoss[0m : 2.50242

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.523, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.99667
[1mStep[0m  [16/169], [94mLoss[0m : 2.52899
[1mStep[0m  [32/169], [94mLoss[0m : 2.76572
[1mStep[0m  [48/169], [94mLoss[0m : 2.72536
[1mStep[0m  [64/169], [94mLoss[0m : 2.93759
[1mStep[0m  [80/169], [94mLoss[0m : 2.31891
[1mStep[0m  [96/169], [94mLoss[0m : 2.76688
[1mStep[0m  [112/169], [94mLoss[0m : 2.47561
[1mStep[0m  [128/169], [94mLoss[0m : 2.96929
[1mStep[0m  [144/169], [94mLoss[0m : 3.40032
[1mStep[0m  [160/169], [94mLoss[0m : 2.76716

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.696, [92mTest[0m: 2.506, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25053
[1mStep[0m  [16/169], [94mLoss[0m : 2.70977
[1mStep[0m  [32/169], [94mLoss[0m : 2.91233
[1mStep[0m  [48/169], [94mLoss[0m : 2.90437
[1mStep[0m  [64/169], [94mLoss[0m : 2.70963
[1mStep[0m  [80/169], [94mLoss[0m : 2.41394
[1mStep[0m  [96/169], [94mLoss[0m : 2.67293
[1mStep[0m  [112/169], [94mLoss[0m : 2.36357
[1mStep[0m  [128/169], [94mLoss[0m : 2.69026
[1mStep[0m  [144/169], [94mLoss[0m : 2.67771
[1mStep[0m  [160/169], [94mLoss[0m : 2.29314

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.493, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68667
[1mStep[0m  [16/169], [94mLoss[0m : 2.53769
[1mStep[0m  [32/169], [94mLoss[0m : 2.81942
[1mStep[0m  [48/169], [94mLoss[0m : 2.84257
[1mStep[0m  [64/169], [94mLoss[0m : 2.98044
[1mStep[0m  [80/169], [94mLoss[0m : 2.71445
[1mStep[0m  [96/169], [94mLoss[0m : 2.74749
[1mStep[0m  [112/169], [94mLoss[0m : 2.66241
[1mStep[0m  [128/169], [94mLoss[0m : 2.34672
[1mStep[0m  [144/169], [94mLoss[0m : 2.75936
[1mStep[0m  [160/169], [94mLoss[0m : 2.70023

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53506
[1mStep[0m  [16/169], [94mLoss[0m : 2.45366
[1mStep[0m  [32/169], [94mLoss[0m : 2.73782
[1mStep[0m  [48/169], [94mLoss[0m : 3.12810
[1mStep[0m  [64/169], [94mLoss[0m : 3.11192
[1mStep[0m  [80/169], [94mLoss[0m : 2.71941
[1mStep[0m  [96/169], [94mLoss[0m : 2.94629
[1mStep[0m  [112/169], [94mLoss[0m : 2.77483
[1mStep[0m  [128/169], [94mLoss[0m : 2.71373
[1mStep[0m  [144/169], [94mLoss[0m : 2.22834
[1mStep[0m  [160/169], [94mLoss[0m : 2.60483

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41274
[1mStep[0m  [16/169], [94mLoss[0m : 2.73290
[1mStep[0m  [32/169], [94mLoss[0m : 2.64858
[1mStep[0m  [48/169], [94mLoss[0m : 2.96905
[1mStep[0m  [64/169], [94mLoss[0m : 2.89525
[1mStep[0m  [80/169], [94mLoss[0m : 2.98257
[1mStep[0m  [96/169], [94mLoss[0m : 2.64605
[1mStep[0m  [112/169], [94mLoss[0m : 2.52076
[1mStep[0m  [128/169], [94mLoss[0m : 2.60457
[1mStep[0m  [144/169], [94mLoss[0m : 3.00569
[1mStep[0m  [160/169], [94mLoss[0m : 2.67886

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90779
[1mStep[0m  [16/169], [94mLoss[0m : 2.61893
[1mStep[0m  [32/169], [94mLoss[0m : 2.37368
[1mStep[0m  [48/169], [94mLoss[0m : 2.53775
[1mStep[0m  [64/169], [94mLoss[0m : 2.91186
[1mStep[0m  [80/169], [94mLoss[0m : 2.78571
[1mStep[0m  [96/169], [94mLoss[0m : 2.75394
[1mStep[0m  [112/169], [94mLoss[0m : 2.55781
[1mStep[0m  [128/169], [94mLoss[0m : 2.79860
[1mStep[0m  [144/169], [94mLoss[0m : 2.90017
[1mStep[0m  [160/169], [94mLoss[0m : 2.89916

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.477, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50499
[1mStep[0m  [16/169], [94mLoss[0m : 2.64798
[1mStep[0m  [32/169], [94mLoss[0m : 2.62810
[1mStep[0m  [48/169], [94mLoss[0m : 2.67062
[1mStep[0m  [64/169], [94mLoss[0m : 2.58716
[1mStep[0m  [80/169], [94mLoss[0m : 2.63563
[1mStep[0m  [96/169], [94mLoss[0m : 2.55470
[1mStep[0m  [112/169], [94mLoss[0m : 2.26226
[1mStep[0m  [128/169], [94mLoss[0m : 2.54345
[1mStep[0m  [144/169], [94mLoss[0m : 2.73891
[1mStep[0m  [160/169], [94mLoss[0m : 2.60429

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50482
[1mStep[0m  [16/169], [94mLoss[0m : 2.63722
[1mStep[0m  [32/169], [94mLoss[0m : 2.82176
[1mStep[0m  [48/169], [94mLoss[0m : 2.81600
[1mStep[0m  [64/169], [94mLoss[0m : 2.40395
[1mStep[0m  [80/169], [94mLoss[0m : 2.73736
[1mStep[0m  [96/169], [94mLoss[0m : 2.89309
[1mStep[0m  [112/169], [94mLoss[0m : 3.15703
[1mStep[0m  [128/169], [94mLoss[0m : 2.78913
[1mStep[0m  [144/169], [94mLoss[0m : 3.10646
[1mStep[0m  [160/169], [94mLoss[0m : 2.52808

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56179
[1mStep[0m  [16/169], [94mLoss[0m : 2.30713
[1mStep[0m  [32/169], [94mLoss[0m : 2.65068
[1mStep[0m  [48/169], [94mLoss[0m : 2.77994
[1mStep[0m  [64/169], [94mLoss[0m : 3.18484
[1mStep[0m  [80/169], [94mLoss[0m : 2.24111
[1mStep[0m  [96/169], [94mLoss[0m : 2.34522
[1mStep[0m  [112/169], [94mLoss[0m : 2.78493
[1mStep[0m  [128/169], [94mLoss[0m : 2.72397
[1mStep[0m  [144/169], [94mLoss[0m : 2.47836
[1mStep[0m  [160/169], [94mLoss[0m : 2.24171

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19560
[1mStep[0m  [16/169], [94mLoss[0m : 2.79358
[1mStep[0m  [32/169], [94mLoss[0m : 2.68875
[1mStep[0m  [48/169], [94mLoss[0m : 2.64432
[1mStep[0m  [64/169], [94mLoss[0m : 2.59617
[1mStep[0m  [80/169], [94mLoss[0m : 2.88991
[1mStep[0m  [96/169], [94mLoss[0m : 2.53969
[1mStep[0m  [112/169], [94mLoss[0m : 3.04480
[1mStep[0m  [128/169], [94mLoss[0m : 2.75463
[1mStep[0m  [144/169], [94mLoss[0m : 2.78760
[1mStep[0m  [160/169], [94mLoss[0m : 2.71654

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.461, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.11472
[1mStep[0m  [16/169], [94mLoss[0m : 2.48121
[1mStep[0m  [32/169], [94mLoss[0m : 2.80264
[1mStep[0m  [48/169], [94mLoss[0m : 2.48547
[1mStep[0m  [64/169], [94mLoss[0m : 2.52341
[1mStep[0m  [80/169], [94mLoss[0m : 2.69478
[1mStep[0m  [96/169], [94mLoss[0m : 2.46191
[1mStep[0m  [112/169], [94mLoss[0m : 2.69829
[1mStep[0m  [128/169], [94mLoss[0m : 2.06802
[1mStep[0m  [144/169], [94mLoss[0m : 2.59391
[1mStep[0m  [160/169], [94mLoss[0m : 2.25685

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.443, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90753
[1mStep[0m  [16/169], [94mLoss[0m : 2.64735
[1mStep[0m  [32/169], [94mLoss[0m : 2.74454
[1mStep[0m  [48/169], [94mLoss[0m : 2.95746
[1mStep[0m  [64/169], [94mLoss[0m : 2.64304
[1mStep[0m  [80/169], [94mLoss[0m : 2.50245
[1mStep[0m  [96/169], [94mLoss[0m : 3.14129
[1mStep[0m  [112/169], [94mLoss[0m : 2.46913
[1mStep[0m  [128/169], [94mLoss[0m : 3.00920
[1mStep[0m  [144/169], [94mLoss[0m : 2.71151
[1mStep[0m  [160/169], [94mLoss[0m : 2.55587

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58613
[1mStep[0m  [16/169], [94mLoss[0m : 3.20152
[1mStep[0m  [32/169], [94mLoss[0m : 3.28264
[1mStep[0m  [48/169], [94mLoss[0m : 2.99218
[1mStep[0m  [64/169], [94mLoss[0m : 2.40810
[1mStep[0m  [80/169], [94mLoss[0m : 2.77444
[1mStep[0m  [96/169], [94mLoss[0m : 2.39638
[1mStep[0m  [112/169], [94mLoss[0m : 3.08353
[1mStep[0m  [128/169], [94mLoss[0m : 2.68540
[1mStep[0m  [144/169], [94mLoss[0m : 2.51131
[1mStep[0m  [160/169], [94mLoss[0m : 2.79727

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71273
[1mStep[0m  [16/169], [94mLoss[0m : 2.78796
[1mStep[0m  [32/169], [94mLoss[0m : 2.50804
[1mStep[0m  [48/169], [94mLoss[0m : 2.58115
[1mStep[0m  [64/169], [94mLoss[0m : 2.72801
[1mStep[0m  [80/169], [94mLoss[0m : 3.12456
[1mStep[0m  [96/169], [94mLoss[0m : 2.94480
[1mStep[0m  [112/169], [94mLoss[0m : 2.54019
[1mStep[0m  [128/169], [94mLoss[0m : 2.56741
[1mStep[0m  [144/169], [94mLoss[0m : 2.54204
[1mStep[0m  [160/169], [94mLoss[0m : 2.43913

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.437, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.01666
[1mStep[0m  [16/169], [94mLoss[0m : 2.99519
[1mStep[0m  [32/169], [94mLoss[0m : 2.38351
[1mStep[0m  [48/169], [94mLoss[0m : 2.52919
[1mStep[0m  [64/169], [94mLoss[0m : 2.42920
[1mStep[0m  [80/169], [94mLoss[0m : 2.88162
[1mStep[0m  [96/169], [94mLoss[0m : 2.37364
[1mStep[0m  [112/169], [94mLoss[0m : 2.69366
[1mStep[0m  [128/169], [94mLoss[0m : 2.49336
[1mStep[0m  [144/169], [94mLoss[0m : 2.58161
[1mStep[0m  [160/169], [94mLoss[0m : 3.09389

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.437, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76771
[1mStep[0m  [16/169], [94mLoss[0m : 2.60931
[1mStep[0m  [32/169], [94mLoss[0m : 2.68975
[1mStep[0m  [48/169], [94mLoss[0m : 2.51364
[1mStep[0m  [64/169], [94mLoss[0m : 2.40672
[1mStep[0m  [80/169], [94mLoss[0m : 2.55807
[1mStep[0m  [96/169], [94mLoss[0m : 2.85893
[1mStep[0m  [112/169], [94mLoss[0m : 2.85674
[1mStep[0m  [128/169], [94mLoss[0m : 2.43945
[1mStep[0m  [144/169], [94mLoss[0m : 2.57127
[1mStep[0m  [160/169], [94mLoss[0m : 2.77767

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65025
[1mStep[0m  [16/169], [94mLoss[0m : 2.56555
[1mStep[0m  [32/169], [94mLoss[0m : 2.57362
[1mStep[0m  [48/169], [94mLoss[0m : 2.76304
[1mStep[0m  [64/169], [94mLoss[0m : 2.57219
[1mStep[0m  [80/169], [94mLoss[0m : 2.79939
[1mStep[0m  [96/169], [94mLoss[0m : 3.03819
[1mStep[0m  [112/169], [94mLoss[0m : 2.55137
[1mStep[0m  [128/169], [94mLoss[0m : 2.31899
[1mStep[0m  [144/169], [94mLoss[0m : 2.99088
[1mStep[0m  [160/169], [94mLoss[0m : 2.41732

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.438, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76657
[1mStep[0m  [16/169], [94mLoss[0m : 2.63152
[1mStep[0m  [32/169], [94mLoss[0m : 2.79599
[1mStep[0m  [48/169], [94mLoss[0m : 2.59826
[1mStep[0m  [64/169], [94mLoss[0m : 2.34422
[1mStep[0m  [80/169], [94mLoss[0m : 2.62679
[1mStep[0m  [96/169], [94mLoss[0m : 2.83299
[1mStep[0m  [112/169], [94mLoss[0m : 2.58353
[1mStep[0m  [128/169], [94mLoss[0m : 2.55693
[1mStep[0m  [144/169], [94mLoss[0m : 2.64097
[1mStep[0m  [160/169], [94mLoss[0m : 2.61850

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.421, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.418
====================================

Phase 2 - Evaluation MAE:  2.417843922972679
MAE score P1        2.780159
MAE score P2        2.417844
loss                2.604224
learning_rate         0.0001
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.1
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.54106
[1mStep[0m  [8/84], [94mLoss[0m : 10.28493
[1mStep[0m  [16/84], [94mLoss[0m : 10.42163
[1mStep[0m  [24/84], [94mLoss[0m : 10.98317
[1mStep[0m  [32/84], [94mLoss[0m : 10.45120
[1mStep[0m  [40/84], [94mLoss[0m : 10.55471
[1mStep[0m  [48/84], [94mLoss[0m : 9.97644
[1mStep[0m  [56/84], [94mLoss[0m : 10.02615
[1mStep[0m  [64/84], [94mLoss[0m : 10.32448
[1mStep[0m  [72/84], [94mLoss[0m : 10.11932
[1mStep[0m  [80/84], [94mLoss[0m : 10.38909

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.558, [92mTest[0m: 10.871, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.04428
[1mStep[0m  [8/84], [94mLoss[0m : 9.90568
[1mStep[0m  [16/84], [94mLoss[0m : 10.06810
[1mStep[0m  [24/84], [94mLoss[0m : 10.00142
[1mStep[0m  [32/84], [94mLoss[0m : 10.01273
[1mStep[0m  [40/84], [94mLoss[0m : 9.74162
[1mStep[0m  [48/84], [94mLoss[0m : 9.90507
[1mStep[0m  [56/84], [94mLoss[0m : 9.99676
[1mStep[0m  [64/84], [94mLoss[0m : 10.03966
[1mStep[0m  [72/84], [94mLoss[0m : 9.37595
[1mStep[0m  [80/84], [94mLoss[0m : 9.68423

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.777, [92mTest[0m: 10.281, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.27644
[1mStep[0m  [8/84], [94mLoss[0m : 9.55243
[1mStep[0m  [16/84], [94mLoss[0m : 8.86827
[1mStep[0m  [24/84], [94mLoss[0m : 9.27216
[1mStep[0m  [32/84], [94mLoss[0m : 9.22826
[1mStep[0m  [40/84], [94mLoss[0m : 8.83820
[1mStep[0m  [48/84], [94mLoss[0m : 9.35759
[1mStep[0m  [56/84], [94mLoss[0m : 8.98563
[1mStep[0m  [64/84], [94mLoss[0m : 8.75001
[1mStep[0m  [72/84], [94mLoss[0m : 9.09701
[1mStep[0m  [80/84], [94mLoss[0m : 8.82616

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.998, [92mTest[0m: 9.633, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.23473
[1mStep[0m  [8/84], [94mLoss[0m : 9.37800
[1mStep[0m  [16/84], [94mLoss[0m : 8.33653
[1mStep[0m  [24/84], [94mLoss[0m : 8.62524
[1mStep[0m  [32/84], [94mLoss[0m : 8.09030
[1mStep[0m  [40/84], [94mLoss[0m : 7.95475
[1mStep[0m  [48/84], [94mLoss[0m : 7.39057
[1mStep[0m  [56/84], [94mLoss[0m : 7.85844
[1mStep[0m  [64/84], [94mLoss[0m : 8.38816
[1mStep[0m  [72/84], [94mLoss[0m : 7.90410
[1mStep[0m  [80/84], [94mLoss[0m : 8.23815

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.230, [92mTest[0m: 8.976, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.12273
[1mStep[0m  [8/84], [94mLoss[0m : 7.65363
[1mStep[0m  [16/84], [94mLoss[0m : 7.26424
[1mStep[0m  [24/84], [94mLoss[0m : 8.27836
[1mStep[0m  [32/84], [94mLoss[0m : 7.49474
[1mStep[0m  [40/84], [94mLoss[0m : 7.32709
[1mStep[0m  [48/84], [94mLoss[0m : 7.34105
[1mStep[0m  [56/84], [94mLoss[0m : 7.62242
[1mStep[0m  [64/84], [94mLoss[0m : 7.07880
[1mStep[0m  [72/84], [94mLoss[0m : 7.66719
[1mStep[0m  [80/84], [94mLoss[0m : 7.54631

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.484, [92mTest[0m: 8.416, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.48268
[1mStep[0m  [8/84], [94mLoss[0m : 7.25275
[1mStep[0m  [16/84], [94mLoss[0m : 7.27742
[1mStep[0m  [24/84], [94mLoss[0m : 7.46034
[1mStep[0m  [32/84], [94mLoss[0m : 6.24171
[1mStep[0m  [40/84], [94mLoss[0m : 6.47706
[1mStep[0m  [48/84], [94mLoss[0m : 6.53454
[1mStep[0m  [56/84], [94mLoss[0m : 7.02521
[1mStep[0m  [64/84], [94mLoss[0m : 6.67462
[1mStep[0m  [72/84], [94mLoss[0m : 7.43255
[1mStep[0m  [80/84], [94mLoss[0m : 6.07274

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.757, [92mTest[0m: 7.736, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.13471
[1mStep[0m  [8/84], [94mLoss[0m : 6.42279
[1mStep[0m  [16/84], [94mLoss[0m : 5.90450
[1mStep[0m  [24/84], [94mLoss[0m : 6.44746
[1mStep[0m  [32/84], [94mLoss[0m : 5.62623
[1mStep[0m  [40/84], [94mLoss[0m : 6.02032
[1mStep[0m  [48/84], [94mLoss[0m : 5.70804
[1mStep[0m  [56/84], [94mLoss[0m : 6.79763
[1mStep[0m  [64/84], [94mLoss[0m : 5.99582
[1mStep[0m  [72/84], [94mLoss[0m : 6.14102
[1mStep[0m  [80/84], [94mLoss[0m : 5.75246

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.108, [92mTest[0m: 7.181, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.18029
[1mStep[0m  [8/84], [94mLoss[0m : 5.82057
[1mStep[0m  [16/84], [94mLoss[0m : 5.88774
[1mStep[0m  [24/84], [94mLoss[0m : 5.16567
[1mStep[0m  [32/84], [94mLoss[0m : 4.86355
[1mStep[0m  [40/84], [94mLoss[0m : 5.28156
[1mStep[0m  [48/84], [94mLoss[0m : 5.40182
[1mStep[0m  [56/84], [94mLoss[0m : 5.17412
[1mStep[0m  [64/84], [94mLoss[0m : 6.25876
[1mStep[0m  [72/84], [94mLoss[0m : 5.52883
[1mStep[0m  [80/84], [94mLoss[0m : 4.97099

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.565, [92mTest[0m: 6.644, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.93491
[1mStep[0m  [8/84], [94mLoss[0m : 5.41597
[1mStep[0m  [16/84], [94mLoss[0m : 4.68036
[1mStep[0m  [24/84], [94mLoss[0m : 5.34567
[1mStep[0m  [32/84], [94mLoss[0m : 4.92284
[1mStep[0m  [40/84], [94mLoss[0m : 5.52262
[1mStep[0m  [48/84], [94mLoss[0m : 4.83252
[1mStep[0m  [56/84], [94mLoss[0m : 4.78946
[1mStep[0m  [64/84], [94mLoss[0m : 4.95453
[1mStep[0m  [72/84], [94mLoss[0m : 4.32186
[1mStep[0m  [80/84], [94mLoss[0m : 5.04216

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.071, [92mTest[0m: 6.131, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.89335
[1mStep[0m  [8/84], [94mLoss[0m : 5.16274
[1mStep[0m  [16/84], [94mLoss[0m : 4.50853
[1mStep[0m  [24/84], [94mLoss[0m : 5.00896
[1mStep[0m  [32/84], [94mLoss[0m : 5.14662
[1mStep[0m  [40/84], [94mLoss[0m : 4.87937
[1mStep[0m  [48/84], [94mLoss[0m : 4.85546
[1mStep[0m  [56/84], [94mLoss[0m : 4.04036
[1mStep[0m  [64/84], [94mLoss[0m : 4.52792
[1mStep[0m  [72/84], [94mLoss[0m : 3.91692
[1mStep[0m  [80/84], [94mLoss[0m : 4.43771

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.642, [92mTest[0m: 5.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.80901
[1mStep[0m  [8/84], [94mLoss[0m : 4.96236
[1mStep[0m  [16/84], [94mLoss[0m : 4.54866
[1mStep[0m  [24/84], [94mLoss[0m : 3.97053
[1mStep[0m  [32/84], [94mLoss[0m : 4.71901
[1mStep[0m  [40/84], [94mLoss[0m : 4.79247
[1mStep[0m  [48/84], [94mLoss[0m : 4.10544
[1mStep[0m  [56/84], [94mLoss[0m : 4.63258
[1mStep[0m  [64/84], [94mLoss[0m : 4.49355
[1mStep[0m  [72/84], [94mLoss[0m : 3.70639
[1mStep[0m  [80/84], [94mLoss[0m : 3.67336

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.276, [92mTest[0m: 5.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.96229
[1mStep[0m  [8/84], [94mLoss[0m : 3.91023
[1mStep[0m  [16/84], [94mLoss[0m : 3.82028
[1mStep[0m  [24/84], [94mLoss[0m : 3.83174
[1mStep[0m  [32/84], [94mLoss[0m : 3.91621
[1mStep[0m  [40/84], [94mLoss[0m : 3.93211
[1mStep[0m  [48/84], [94mLoss[0m : 3.89892
[1mStep[0m  [56/84], [94mLoss[0m : 3.84753
[1mStep[0m  [64/84], [94mLoss[0m : 4.12679
[1mStep[0m  [72/84], [94mLoss[0m : 3.70376
[1mStep[0m  [80/84], [94mLoss[0m : 4.19440

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.944, [92mTest[0m: 4.838, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.79962
[1mStep[0m  [8/84], [94mLoss[0m : 3.65979
[1mStep[0m  [16/84], [94mLoss[0m : 3.85929
[1mStep[0m  [24/84], [94mLoss[0m : 4.22350
[1mStep[0m  [32/84], [94mLoss[0m : 3.52857
[1mStep[0m  [40/84], [94mLoss[0m : 3.46407
[1mStep[0m  [48/84], [94mLoss[0m : 3.56229
[1mStep[0m  [56/84], [94mLoss[0m : 3.71681
[1mStep[0m  [64/84], [94mLoss[0m : 3.87084
[1mStep[0m  [72/84], [94mLoss[0m : 4.15242
[1mStep[0m  [80/84], [94mLoss[0m : 3.60713

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.657, [92mTest[0m: 4.448, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.65449
[1mStep[0m  [8/84], [94mLoss[0m : 3.69753
[1mStep[0m  [16/84], [94mLoss[0m : 3.36059
[1mStep[0m  [24/84], [94mLoss[0m : 3.12752
[1mStep[0m  [32/84], [94mLoss[0m : 3.50909
[1mStep[0m  [40/84], [94mLoss[0m : 3.54049
[1mStep[0m  [48/84], [94mLoss[0m : 3.50844
[1mStep[0m  [56/84], [94mLoss[0m : 3.01017
[1mStep[0m  [64/84], [94mLoss[0m : 3.34145
[1mStep[0m  [72/84], [94mLoss[0m : 3.48267
[1mStep[0m  [80/84], [94mLoss[0m : 3.26826

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.425, [92mTest[0m: 4.082, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.24632
[1mStep[0m  [8/84], [94mLoss[0m : 3.45718
[1mStep[0m  [16/84], [94mLoss[0m : 3.32951
[1mStep[0m  [24/84], [94mLoss[0m : 3.55065
[1mStep[0m  [32/84], [94mLoss[0m : 3.18385
[1mStep[0m  [40/84], [94mLoss[0m : 3.00806
[1mStep[0m  [48/84], [94mLoss[0m : 2.85101
[1mStep[0m  [56/84], [94mLoss[0m : 3.54613
[1mStep[0m  [64/84], [94mLoss[0m : 3.40029
[1mStep[0m  [72/84], [94mLoss[0m : 3.10343
[1mStep[0m  [80/84], [94mLoss[0m : 3.07346

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.228, [92mTest[0m: 3.817, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.22767
[1mStep[0m  [8/84], [94mLoss[0m : 3.32480
[1mStep[0m  [16/84], [94mLoss[0m : 3.10649
[1mStep[0m  [24/84], [94mLoss[0m : 2.94864
[1mStep[0m  [32/84], [94mLoss[0m : 3.11370
[1mStep[0m  [40/84], [94mLoss[0m : 3.01593
[1mStep[0m  [48/84], [94mLoss[0m : 2.95506
[1mStep[0m  [56/84], [94mLoss[0m : 2.96388
[1mStep[0m  [64/84], [94mLoss[0m : 3.11414
[1mStep[0m  [72/84], [94mLoss[0m : 3.16911
[1mStep[0m  [80/84], [94mLoss[0m : 2.81618

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.088, [92mTest[0m: 3.596, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.21810
[1mStep[0m  [8/84], [94mLoss[0m : 2.90813
[1mStep[0m  [16/84], [94mLoss[0m : 2.95181
[1mStep[0m  [24/84], [94mLoss[0m : 2.89611
[1mStep[0m  [32/84], [94mLoss[0m : 2.97616
[1mStep[0m  [40/84], [94mLoss[0m : 2.95254
[1mStep[0m  [48/84], [94mLoss[0m : 2.94100
[1mStep[0m  [56/84], [94mLoss[0m : 3.02504
[1mStep[0m  [64/84], [94mLoss[0m : 2.92825
[1mStep[0m  [72/84], [94mLoss[0m : 3.00104
[1mStep[0m  [80/84], [94mLoss[0m : 2.94379

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.990, [92mTest[0m: 3.364, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96826
[1mStep[0m  [8/84], [94mLoss[0m : 3.06070
[1mStep[0m  [16/84], [94mLoss[0m : 2.51285
[1mStep[0m  [24/84], [94mLoss[0m : 2.82872
[1mStep[0m  [32/84], [94mLoss[0m : 3.06407
[1mStep[0m  [40/84], [94mLoss[0m : 3.18489
[1mStep[0m  [48/84], [94mLoss[0m : 3.32980
[1mStep[0m  [56/84], [94mLoss[0m : 2.74397
[1mStep[0m  [64/84], [94mLoss[0m : 3.14016
[1mStep[0m  [72/84], [94mLoss[0m : 2.73664
[1mStep[0m  [80/84], [94mLoss[0m : 2.97943

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.925, [92mTest[0m: 3.204, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.94800
[1mStep[0m  [8/84], [94mLoss[0m : 3.28432
[1mStep[0m  [16/84], [94mLoss[0m : 3.16405
[1mStep[0m  [24/84], [94mLoss[0m : 2.65847
[1mStep[0m  [32/84], [94mLoss[0m : 2.65412
[1mStep[0m  [40/84], [94mLoss[0m : 3.42161
[1mStep[0m  [48/84], [94mLoss[0m : 2.60243
[1mStep[0m  [56/84], [94mLoss[0m : 2.40817
[1mStep[0m  [64/84], [94mLoss[0m : 2.66987
[1mStep[0m  [72/84], [94mLoss[0m : 3.02074
[1mStep[0m  [80/84], [94mLoss[0m : 2.72665

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.858, [92mTest[0m: 3.107, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63275
[1mStep[0m  [8/84], [94mLoss[0m : 2.94659
[1mStep[0m  [16/84], [94mLoss[0m : 2.75166
[1mStep[0m  [24/84], [94mLoss[0m : 2.83832
[1mStep[0m  [32/84], [94mLoss[0m : 2.53914
[1mStep[0m  [40/84], [94mLoss[0m : 2.96385
[1mStep[0m  [48/84], [94mLoss[0m : 2.97599
[1mStep[0m  [56/84], [94mLoss[0m : 3.00080
[1mStep[0m  [64/84], [94mLoss[0m : 2.45981
[1mStep[0m  [72/84], [94mLoss[0m : 2.92978
[1mStep[0m  [80/84], [94mLoss[0m : 2.66041

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.810, [92mTest[0m: 2.973, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77602
[1mStep[0m  [8/84], [94mLoss[0m : 2.73957
[1mStep[0m  [16/84], [94mLoss[0m : 2.91066
[1mStep[0m  [24/84], [94mLoss[0m : 3.02060
[1mStep[0m  [32/84], [94mLoss[0m : 2.57812
[1mStep[0m  [40/84], [94mLoss[0m : 2.96183
[1mStep[0m  [48/84], [94mLoss[0m : 2.89092
[1mStep[0m  [56/84], [94mLoss[0m : 2.58141
[1mStep[0m  [64/84], [94mLoss[0m : 2.68564
[1mStep[0m  [72/84], [94mLoss[0m : 2.76776
[1mStep[0m  [80/84], [94mLoss[0m : 3.02905

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.775, [92mTest[0m: 2.904, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.15996
[1mStep[0m  [8/84], [94mLoss[0m : 3.19898
[1mStep[0m  [16/84], [94mLoss[0m : 2.68068
[1mStep[0m  [24/84], [94mLoss[0m : 2.64918
[1mStep[0m  [32/84], [94mLoss[0m : 2.72883
[1mStep[0m  [40/84], [94mLoss[0m : 2.62015
[1mStep[0m  [48/84], [94mLoss[0m : 2.61206
[1mStep[0m  [56/84], [94mLoss[0m : 2.90001
[1mStep[0m  [64/84], [94mLoss[0m : 2.88751
[1mStep[0m  [72/84], [94mLoss[0m : 2.76429
[1mStep[0m  [80/84], [94mLoss[0m : 2.86043

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.920, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43636
[1mStep[0m  [8/84], [94mLoss[0m : 3.09223
[1mStep[0m  [16/84], [94mLoss[0m : 2.80058
[1mStep[0m  [24/84], [94mLoss[0m : 2.65542
[1mStep[0m  [32/84], [94mLoss[0m : 2.78981
[1mStep[0m  [40/84], [94mLoss[0m : 2.72088
[1mStep[0m  [48/84], [94mLoss[0m : 2.88624
[1mStep[0m  [56/84], [94mLoss[0m : 2.53663
[1mStep[0m  [64/84], [94mLoss[0m : 2.80887
[1mStep[0m  [72/84], [94mLoss[0m : 2.26319
[1mStep[0m  [80/84], [94mLoss[0m : 2.87332

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.744, [92mTest[0m: 2.867, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59383
[1mStep[0m  [8/84], [94mLoss[0m : 2.89832
[1mStep[0m  [16/84], [94mLoss[0m : 2.66844
[1mStep[0m  [24/84], [94mLoss[0m : 2.85071
[1mStep[0m  [32/84], [94mLoss[0m : 2.80607
[1mStep[0m  [40/84], [94mLoss[0m : 2.49115
[1mStep[0m  [48/84], [94mLoss[0m : 2.36274
[1mStep[0m  [56/84], [94mLoss[0m : 2.87996
[1mStep[0m  [64/84], [94mLoss[0m : 3.00451
[1mStep[0m  [72/84], [94mLoss[0m : 2.64834
[1mStep[0m  [80/84], [94mLoss[0m : 2.65343

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.804, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.04968
[1mStep[0m  [8/84], [94mLoss[0m : 2.79649
[1mStep[0m  [16/84], [94mLoss[0m : 2.59884
[1mStep[0m  [24/84], [94mLoss[0m : 2.70598
[1mStep[0m  [32/84], [94mLoss[0m : 3.12543
[1mStep[0m  [40/84], [94mLoss[0m : 3.11419
[1mStep[0m  [48/84], [94mLoss[0m : 2.73201
[1mStep[0m  [56/84], [94mLoss[0m : 3.15170
[1mStep[0m  [64/84], [94mLoss[0m : 2.68219
[1mStep[0m  [72/84], [94mLoss[0m : 2.64569
[1mStep[0m  [80/84], [94mLoss[0m : 2.55393

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.760, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23460
[1mStep[0m  [8/84], [94mLoss[0m : 2.80309
[1mStep[0m  [16/84], [94mLoss[0m : 2.71244
[1mStep[0m  [24/84], [94mLoss[0m : 2.94239
[1mStep[0m  [32/84], [94mLoss[0m : 2.92609
[1mStep[0m  [40/84], [94mLoss[0m : 2.40661
[1mStep[0m  [48/84], [94mLoss[0m : 2.45777
[1mStep[0m  [56/84], [94mLoss[0m : 2.98074
[1mStep[0m  [64/84], [94mLoss[0m : 2.60824
[1mStep[0m  [72/84], [94mLoss[0m : 2.66959
[1mStep[0m  [80/84], [94mLoss[0m : 3.00820

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.772, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81686
[1mStep[0m  [8/84], [94mLoss[0m : 2.55936
[1mStep[0m  [16/84], [94mLoss[0m : 2.75334
[1mStep[0m  [24/84], [94mLoss[0m : 2.61134
[1mStep[0m  [32/84], [94mLoss[0m : 2.65250
[1mStep[0m  [40/84], [94mLoss[0m : 2.99727
[1mStep[0m  [48/84], [94mLoss[0m : 3.09196
[1mStep[0m  [56/84], [94mLoss[0m : 3.03181
[1mStep[0m  [64/84], [94mLoss[0m : 2.81971
[1mStep[0m  [72/84], [94mLoss[0m : 2.81354
[1mStep[0m  [80/84], [94mLoss[0m : 2.58293

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.750, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67576
[1mStep[0m  [8/84], [94mLoss[0m : 2.84377
[1mStep[0m  [16/84], [94mLoss[0m : 2.62685
[1mStep[0m  [24/84], [94mLoss[0m : 3.05848
[1mStep[0m  [32/84], [94mLoss[0m : 2.46860
[1mStep[0m  [40/84], [94mLoss[0m : 3.12323
[1mStep[0m  [48/84], [94mLoss[0m : 2.35855
[1mStep[0m  [56/84], [94mLoss[0m : 2.97929
[1mStep[0m  [64/84], [94mLoss[0m : 2.77171
[1mStep[0m  [72/84], [94mLoss[0m : 2.72094
[1mStep[0m  [80/84], [94mLoss[0m : 2.82886

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.766, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93814
[1mStep[0m  [8/84], [94mLoss[0m : 2.31674
[1mStep[0m  [16/84], [94mLoss[0m : 2.61068
[1mStep[0m  [24/84], [94mLoss[0m : 2.81366
[1mStep[0m  [32/84], [94mLoss[0m : 2.80641
[1mStep[0m  [40/84], [94mLoss[0m : 2.64007
[1mStep[0m  [48/84], [94mLoss[0m : 2.74287
[1mStep[0m  [56/84], [94mLoss[0m : 2.88683
[1mStep[0m  [64/84], [94mLoss[0m : 2.42783
[1mStep[0m  [72/84], [94mLoss[0m : 2.82329
[1mStep[0m  [80/84], [94mLoss[0m : 2.51351

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.719, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79230
[1mStep[0m  [8/84], [94mLoss[0m : 2.85468
[1mStep[0m  [16/84], [94mLoss[0m : 2.50407
[1mStep[0m  [24/84], [94mLoss[0m : 2.98071
[1mStep[0m  [32/84], [94mLoss[0m : 2.75249
[1mStep[0m  [40/84], [94mLoss[0m : 2.69266
[1mStep[0m  [48/84], [94mLoss[0m : 2.76421
[1mStep[0m  [56/84], [94mLoss[0m : 2.55273
[1mStep[0m  [64/84], [94mLoss[0m : 2.55504
[1mStep[0m  [72/84], [94mLoss[0m : 2.55171
[1mStep[0m  [80/84], [94mLoss[0m : 2.24021

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.680, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.723
====================================

Phase 1 - Evaluation MAE:  2.722933386053358
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.91672
[1mStep[0m  [8/84], [94mLoss[0m : 2.99145
[1mStep[0m  [16/84], [94mLoss[0m : 2.90124
[1mStep[0m  [24/84], [94mLoss[0m : 2.42361
[1mStep[0m  [32/84], [94mLoss[0m : 2.82701
[1mStep[0m  [40/84], [94mLoss[0m : 2.48209
[1mStep[0m  [48/84], [94mLoss[0m : 2.77809
[1mStep[0m  [56/84], [94mLoss[0m : 2.60902
[1mStep[0m  [64/84], [94mLoss[0m : 2.33916
[1mStep[0m  [72/84], [94mLoss[0m : 2.61140
[1mStep[0m  [80/84], [94mLoss[0m : 2.66699

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50579
[1mStep[0m  [8/84], [94mLoss[0m : 2.93818
[1mStep[0m  [16/84], [94mLoss[0m : 2.65970
[1mStep[0m  [24/84], [94mLoss[0m : 2.68376
[1mStep[0m  [32/84], [94mLoss[0m : 2.32062
[1mStep[0m  [40/84], [94mLoss[0m : 2.89565
[1mStep[0m  [48/84], [94mLoss[0m : 2.75611
[1mStep[0m  [56/84], [94mLoss[0m : 2.67223
[1mStep[0m  [64/84], [94mLoss[0m : 2.74379
[1mStep[0m  [72/84], [94mLoss[0m : 2.47818
[1mStep[0m  [80/84], [94mLoss[0m : 3.00878

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.606, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77222
[1mStep[0m  [8/84], [94mLoss[0m : 2.93630
[1mStep[0m  [16/84], [94mLoss[0m : 2.49804
[1mStep[0m  [24/84], [94mLoss[0m : 2.43869
[1mStep[0m  [32/84], [94mLoss[0m : 2.59026
[1mStep[0m  [40/84], [94mLoss[0m : 2.34220
[1mStep[0m  [48/84], [94mLoss[0m : 2.67815
[1mStep[0m  [56/84], [94mLoss[0m : 2.68937
[1mStep[0m  [64/84], [94mLoss[0m : 2.65408
[1mStep[0m  [72/84], [94mLoss[0m : 2.55305
[1mStep[0m  [80/84], [94mLoss[0m : 2.58016

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49675
[1mStep[0m  [8/84], [94mLoss[0m : 2.82667
[1mStep[0m  [16/84], [94mLoss[0m : 2.54901
[1mStep[0m  [24/84], [94mLoss[0m : 2.72771
[1mStep[0m  [32/84], [94mLoss[0m : 2.40168
[1mStep[0m  [40/84], [94mLoss[0m : 2.50882
[1mStep[0m  [48/84], [94mLoss[0m : 2.87553
[1mStep[0m  [56/84], [94mLoss[0m : 2.64135
[1mStep[0m  [64/84], [94mLoss[0m : 2.42412
[1mStep[0m  [72/84], [94mLoss[0m : 2.70712
[1mStep[0m  [80/84], [94mLoss[0m : 2.63629

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.553, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56398
[1mStep[0m  [8/84], [94mLoss[0m : 2.52083
[1mStep[0m  [16/84], [94mLoss[0m : 2.51832
[1mStep[0m  [24/84], [94mLoss[0m : 2.61347
[1mStep[0m  [32/84], [94mLoss[0m : 2.23671
[1mStep[0m  [40/84], [94mLoss[0m : 2.27024
[1mStep[0m  [48/84], [94mLoss[0m : 2.60334
[1mStep[0m  [56/84], [94mLoss[0m : 2.70467
[1mStep[0m  [64/84], [94mLoss[0m : 2.37473
[1mStep[0m  [72/84], [94mLoss[0m : 2.75076
[1mStep[0m  [80/84], [94mLoss[0m : 2.61850

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.547, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66965
[1mStep[0m  [8/84], [94mLoss[0m : 2.75708
[1mStep[0m  [16/84], [94mLoss[0m : 2.76721
[1mStep[0m  [24/84], [94mLoss[0m : 2.71769
[1mStep[0m  [32/84], [94mLoss[0m : 2.76755
[1mStep[0m  [40/84], [94mLoss[0m : 2.78766
[1mStep[0m  [48/84], [94mLoss[0m : 2.87714
[1mStep[0m  [56/84], [94mLoss[0m : 2.71134
[1mStep[0m  [64/84], [94mLoss[0m : 2.74815
[1mStep[0m  [72/84], [94mLoss[0m : 2.73306
[1mStep[0m  [80/84], [94mLoss[0m : 2.78325

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.586, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47132
[1mStep[0m  [8/84], [94mLoss[0m : 2.58280
[1mStep[0m  [16/84], [94mLoss[0m : 2.58298
[1mStep[0m  [24/84], [94mLoss[0m : 2.44060
[1mStep[0m  [32/84], [94mLoss[0m : 2.46313
[1mStep[0m  [40/84], [94mLoss[0m : 2.53122
[1mStep[0m  [48/84], [94mLoss[0m : 2.52190
[1mStep[0m  [56/84], [94mLoss[0m : 2.43267
[1mStep[0m  [64/84], [94mLoss[0m : 2.59734
[1mStep[0m  [72/84], [94mLoss[0m : 2.72885
[1mStep[0m  [80/84], [94mLoss[0m : 2.65792

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69542
[1mStep[0m  [8/84], [94mLoss[0m : 2.59470
[1mStep[0m  [16/84], [94mLoss[0m : 2.57577
[1mStep[0m  [24/84], [94mLoss[0m : 2.59264
[1mStep[0m  [32/84], [94mLoss[0m : 2.38881
[1mStep[0m  [40/84], [94mLoss[0m : 2.46226
[1mStep[0m  [48/84], [94mLoss[0m : 2.61096
[1mStep[0m  [56/84], [94mLoss[0m : 2.51529
[1mStep[0m  [64/84], [94mLoss[0m : 2.87467
[1mStep[0m  [72/84], [94mLoss[0m : 2.61106
[1mStep[0m  [80/84], [94mLoss[0m : 2.65330

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56829
[1mStep[0m  [8/84], [94mLoss[0m : 2.24621
[1mStep[0m  [16/84], [94mLoss[0m : 2.46373
[1mStep[0m  [24/84], [94mLoss[0m : 2.42112
[1mStep[0m  [32/84], [94mLoss[0m : 2.42053
[1mStep[0m  [40/84], [94mLoss[0m : 2.78945
[1mStep[0m  [48/84], [94mLoss[0m : 2.79026
[1mStep[0m  [56/84], [94mLoss[0m : 2.57748
[1mStep[0m  [64/84], [94mLoss[0m : 2.47379
[1mStep[0m  [72/84], [94mLoss[0m : 2.51871
[1mStep[0m  [80/84], [94mLoss[0m : 2.53342

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.582, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74074
[1mStep[0m  [8/84], [94mLoss[0m : 2.62805
[1mStep[0m  [16/84], [94mLoss[0m : 2.60311
[1mStep[0m  [24/84], [94mLoss[0m : 2.30430
[1mStep[0m  [32/84], [94mLoss[0m : 2.72925
[1mStep[0m  [40/84], [94mLoss[0m : 2.49407
[1mStep[0m  [48/84], [94mLoss[0m : 2.63064
[1mStep[0m  [56/84], [94mLoss[0m : 2.64041
[1mStep[0m  [64/84], [94mLoss[0m : 2.75621
[1mStep[0m  [72/84], [94mLoss[0m : 2.57321
[1mStep[0m  [80/84], [94mLoss[0m : 2.48988

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68450
[1mStep[0m  [8/84], [94mLoss[0m : 2.57328
[1mStep[0m  [16/84], [94mLoss[0m : 2.34656
[1mStep[0m  [24/84], [94mLoss[0m : 2.68329
[1mStep[0m  [32/84], [94mLoss[0m : 2.84675
[1mStep[0m  [40/84], [94mLoss[0m : 2.98473
[1mStep[0m  [48/84], [94mLoss[0m : 2.86531
[1mStep[0m  [56/84], [94mLoss[0m : 2.53223
[1mStep[0m  [64/84], [94mLoss[0m : 2.35581
[1mStep[0m  [72/84], [94mLoss[0m : 2.69091
[1mStep[0m  [80/84], [94mLoss[0m : 2.48009

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74660
[1mStep[0m  [8/84], [94mLoss[0m : 2.63493
[1mStep[0m  [16/84], [94mLoss[0m : 2.85103
[1mStep[0m  [24/84], [94mLoss[0m : 2.83011
[1mStep[0m  [32/84], [94mLoss[0m : 2.60476
[1mStep[0m  [40/84], [94mLoss[0m : 2.46468
[1mStep[0m  [48/84], [94mLoss[0m : 2.66672
[1mStep[0m  [56/84], [94mLoss[0m : 2.55360
[1mStep[0m  [64/84], [94mLoss[0m : 2.57803
[1mStep[0m  [72/84], [94mLoss[0m : 2.47357
[1mStep[0m  [80/84], [94mLoss[0m : 2.37305

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.622, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44559
[1mStep[0m  [8/84], [94mLoss[0m : 2.68587
[1mStep[0m  [16/84], [94mLoss[0m : 2.76919
[1mStep[0m  [24/84], [94mLoss[0m : 2.77033
[1mStep[0m  [32/84], [94mLoss[0m : 2.77136
[1mStep[0m  [40/84], [94mLoss[0m : 2.40570
[1mStep[0m  [48/84], [94mLoss[0m : 2.31687
[1mStep[0m  [56/84], [94mLoss[0m : 2.48002
[1mStep[0m  [64/84], [94mLoss[0m : 2.59143
[1mStep[0m  [72/84], [94mLoss[0m : 2.32960
[1mStep[0m  [80/84], [94mLoss[0m : 2.52653

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58544
[1mStep[0m  [8/84], [94mLoss[0m : 2.98278
[1mStep[0m  [16/84], [94mLoss[0m : 2.49070
[1mStep[0m  [24/84], [94mLoss[0m : 2.47409
[1mStep[0m  [32/84], [94mLoss[0m : 2.32671
[1mStep[0m  [40/84], [94mLoss[0m : 2.67480
[1mStep[0m  [48/84], [94mLoss[0m : 2.77969
[1mStep[0m  [56/84], [94mLoss[0m : 2.59074
[1mStep[0m  [64/84], [94mLoss[0m : 2.59873
[1mStep[0m  [72/84], [94mLoss[0m : 2.15119
[1mStep[0m  [80/84], [94mLoss[0m : 2.47314

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27068
[1mStep[0m  [8/84], [94mLoss[0m : 2.26092
[1mStep[0m  [16/84], [94mLoss[0m : 2.65573
[1mStep[0m  [24/84], [94mLoss[0m : 2.60913
[1mStep[0m  [32/84], [94mLoss[0m : 2.46586
[1mStep[0m  [40/84], [94mLoss[0m : 2.47382
[1mStep[0m  [48/84], [94mLoss[0m : 2.82417
[1mStep[0m  [56/84], [94mLoss[0m : 3.03394
[1mStep[0m  [64/84], [94mLoss[0m : 2.31304
[1mStep[0m  [72/84], [94mLoss[0m : 2.37839
[1mStep[0m  [80/84], [94mLoss[0m : 2.49705

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90003
[1mStep[0m  [8/84], [94mLoss[0m : 2.67497
[1mStep[0m  [16/84], [94mLoss[0m : 2.56740
[1mStep[0m  [24/84], [94mLoss[0m : 2.60850
[1mStep[0m  [32/84], [94mLoss[0m : 2.72526
[1mStep[0m  [40/84], [94mLoss[0m : 2.32647
[1mStep[0m  [48/84], [94mLoss[0m : 2.68081
[1mStep[0m  [56/84], [94mLoss[0m : 2.47923
[1mStep[0m  [64/84], [94mLoss[0m : 2.60742
[1mStep[0m  [72/84], [94mLoss[0m : 2.41986
[1mStep[0m  [80/84], [94mLoss[0m : 2.56349

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.679, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59517
[1mStep[0m  [8/84], [94mLoss[0m : 2.95327
[1mStep[0m  [16/84], [94mLoss[0m : 2.80282
[1mStep[0m  [24/84], [94mLoss[0m : 3.02749
[1mStep[0m  [32/84], [94mLoss[0m : 2.15256
[1mStep[0m  [40/84], [94mLoss[0m : 2.54464
[1mStep[0m  [48/84], [94mLoss[0m : 2.71105
[1mStep[0m  [56/84], [94mLoss[0m : 2.56320
[1mStep[0m  [64/84], [94mLoss[0m : 2.55528
[1mStep[0m  [72/84], [94mLoss[0m : 2.52944
[1mStep[0m  [80/84], [94mLoss[0m : 2.42336

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54518
[1mStep[0m  [8/84], [94mLoss[0m : 2.68299
[1mStep[0m  [16/84], [94mLoss[0m : 2.78036
[1mStep[0m  [24/84], [94mLoss[0m : 2.52895
[1mStep[0m  [32/84], [94mLoss[0m : 2.73539
[1mStep[0m  [40/84], [94mLoss[0m : 2.22528
[1mStep[0m  [48/84], [94mLoss[0m : 2.53987
[1mStep[0m  [56/84], [94mLoss[0m : 2.90494
[1mStep[0m  [64/84], [94mLoss[0m : 2.56152
[1mStep[0m  [72/84], [94mLoss[0m : 2.91906
[1mStep[0m  [80/84], [94mLoss[0m : 2.52235

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.572, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34056
[1mStep[0m  [8/84], [94mLoss[0m : 2.71391
[1mStep[0m  [16/84], [94mLoss[0m : 2.51484
[1mStep[0m  [24/84], [94mLoss[0m : 2.36914
[1mStep[0m  [32/84], [94mLoss[0m : 2.31647
[1mStep[0m  [40/84], [94mLoss[0m : 2.50209
[1mStep[0m  [48/84], [94mLoss[0m : 2.50091
[1mStep[0m  [56/84], [94mLoss[0m : 2.34438
[1mStep[0m  [64/84], [94mLoss[0m : 2.31473
[1mStep[0m  [72/84], [94mLoss[0m : 2.62520
[1mStep[0m  [80/84], [94mLoss[0m : 2.39882

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.571, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43323
[1mStep[0m  [8/84], [94mLoss[0m : 2.37363
[1mStep[0m  [16/84], [94mLoss[0m : 2.63596
[1mStep[0m  [24/84], [94mLoss[0m : 2.50289
[1mStep[0m  [32/84], [94mLoss[0m : 2.34662
[1mStep[0m  [40/84], [94mLoss[0m : 2.79316
[1mStep[0m  [48/84], [94mLoss[0m : 2.71855
[1mStep[0m  [56/84], [94mLoss[0m : 2.65024
[1mStep[0m  [64/84], [94mLoss[0m : 2.35578
[1mStep[0m  [72/84], [94mLoss[0m : 2.57507
[1mStep[0m  [80/84], [94mLoss[0m : 2.13673

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.618, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51318
[1mStep[0m  [8/84], [94mLoss[0m : 2.50814
[1mStep[0m  [16/84], [94mLoss[0m : 2.41905
[1mStep[0m  [24/84], [94mLoss[0m : 2.51822
[1mStep[0m  [32/84], [94mLoss[0m : 2.26899
[1mStep[0m  [40/84], [94mLoss[0m : 2.21811
[1mStep[0m  [48/84], [94mLoss[0m : 2.62850
[1mStep[0m  [56/84], [94mLoss[0m : 2.87555
[1mStep[0m  [64/84], [94mLoss[0m : 2.47182
[1mStep[0m  [72/84], [94mLoss[0m : 2.53082
[1mStep[0m  [80/84], [94mLoss[0m : 2.45243

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.631, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61792
[1mStep[0m  [8/84], [94mLoss[0m : 2.42889
[1mStep[0m  [16/84], [94mLoss[0m : 2.58845
[1mStep[0m  [24/84], [94mLoss[0m : 2.49020
[1mStep[0m  [32/84], [94mLoss[0m : 2.50332
[1mStep[0m  [40/84], [94mLoss[0m : 2.27703
[1mStep[0m  [48/84], [94mLoss[0m : 2.61455
[1mStep[0m  [56/84], [94mLoss[0m : 2.84453
[1mStep[0m  [64/84], [94mLoss[0m : 2.48990
[1mStep[0m  [72/84], [94mLoss[0m : 2.20005
[1mStep[0m  [80/84], [94mLoss[0m : 2.49215

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.557, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09437
[1mStep[0m  [8/84], [94mLoss[0m : 2.45058
[1mStep[0m  [16/84], [94mLoss[0m : 2.53960
[1mStep[0m  [24/84], [94mLoss[0m : 2.30170
[1mStep[0m  [32/84], [94mLoss[0m : 2.38208
[1mStep[0m  [40/84], [94mLoss[0m : 2.35953
[1mStep[0m  [48/84], [94mLoss[0m : 2.54859
[1mStep[0m  [56/84], [94mLoss[0m : 2.53604
[1mStep[0m  [64/84], [94mLoss[0m : 2.71028
[1mStep[0m  [72/84], [94mLoss[0m : 2.48290
[1mStep[0m  [80/84], [94mLoss[0m : 2.40263

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.596, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47574
[1mStep[0m  [8/84], [94mLoss[0m : 2.60826
[1mStep[0m  [16/84], [94mLoss[0m : 2.63406
[1mStep[0m  [24/84], [94mLoss[0m : 2.59651
[1mStep[0m  [32/84], [94mLoss[0m : 2.24680
[1mStep[0m  [40/84], [94mLoss[0m : 2.47951
[1mStep[0m  [48/84], [94mLoss[0m : 2.49688
[1mStep[0m  [56/84], [94mLoss[0m : 2.34161
[1mStep[0m  [64/84], [94mLoss[0m : 2.71901
[1mStep[0m  [72/84], [94mLoss[0m : 2.42129
[1mStep[0m  [80/84], [94mLoss[0m : 2.70008

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.533, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35697
[1mStep[0m  [8/84], [94mLoss[0m : 2.90964
[1mStep[0m  [16/84], [94mLoss[0m : 2.59351
[1mStep[0m  [24/84], [94mLoss[0m : 2.28569
[1mStep[0m  [32/84], [94mLoss[0m : 2.66976
[1mStep[0m  [40/84], [94mLoss[0m : 2.65857
[1mStep[0m  [48/84], [94mLoss[0m : 2.52340
[1mStep[0m  [56/84], [94mLoss[0m : 2.50092
[1mStep[0m  [64/84], [94mLoss[0m : 2.48423
[1mStep[0m  [72/84], [94mLoss[0m : 2.82599
[1mStep[0m  [80/84], [94mLoss[0m : 2.35358

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53280
[1mStep[0m  [8/84], [94mLoss[0m : 2.38982
[1mStep[0m  [16/84], [94mLoss[0m : 2.86629
[1mStep[0m  [24/84], [94mLoss[0m : 2.42765
[1mStep[0m  [32/84], [94mLoss[0m : 2.82617
[1mStep[0m  [40/84], [94mLoss[0m : 2.61303
[1mStep[0m  [48/84], [94mLoss[0m : 2.65386
[1mStep[0m  [56/84], [94mLoss[0m : 2.31731
[1mStep[0m  [64/84], [94mLoss[0m : 2.29400
[1mStep[0m  [72/84], [94mLoss[0m : 2.50523
[1mStep[0m  [80/84], [94mLoss[0m : 2.49949

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.493, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.16150
[1mStep[0m  [8/84], [94mLoss[0m : 2.49733
[1mStep[0m  [16/84], [94mLoss[0m : 2.53896
[1mStep[0m  [24/84], [94mLoss[0m : 2.76367
[1mStep[0m  [32/84], [94mLoss[0m : 2.26284
[1mStep[0m  [40/84], [94mLoss[0m : 2.25826
[1mStep[0m  [48/84], [94mLoss[0m : 2.52079
[1mStep[0m  [56/84], [94mLoss[0m : 2.30206
[1mStep[0m  [64/84], [94mLoss[0m : 2.51282
[1mStep[0m  [72/84], [94mLoss[0m : 2.33392
[1mStep[0m  [80/84], [94mLoss[0m : 2.81270

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.511, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40141
[1mStep[0m  [8/84], [94mLoss[0m : 2.47343
[1mStep[0m  [16/84], [94mLoss[0m : 2.37795
[1mStep[0m  [24/84], [94mLoss[0m : 2.50151
[1mStep[0m  [32/84], [94mLoss[0m : 2.66095
[1mStep[0m  [40/84], [94mLoss[0m : 2.55085
[1mStep[0m  [48/84], [94mLoss[0m : 2.50357
[1mStep[0m  [56/84], [94mLoss[0m : 2.64749
[1mStep[0m  [64/84], [94mLoss[0m : 2.51518
[1mStep[0m  [72/84], [94mLoss[0m : 2.50731
[1mStep[0m  [80/84], [94mLoss[0m : 2.72601

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.543, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46281
[1mStep[0m  [8/84], [94mLoss[0m : 2.54860
[1mStep[0m  [16/84], [94mLoss[0m : 2.54363
[1mStep[0m  [24/84], [94mLoss[0m : 2.42235
[1mStep[0m  [32/84], [94mLoss[0m : 2.43751
[1mStep[0m  [40/84], [94mLoss[0m : 2.38210
[1mStep[0m  [48/84], [94mLoss[0m : 2.23379
[1mStep[0m  [56/84], [94mLoss[0m : 2.39807
[1mStep[0m  [64/84], [94mLoss[0m : 2.68365
[1mStep[0m  [72/84], [94mLoss[0m : 2.21014
[1mStep[0m  [80/84], [94mLoss[0m : 2.48289

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53099
[1mStep[0m  [8/84], [94mLoss[0m : 2.67002
[1mStep[0m  [16/84], [94mLoss[0m : 2.13212
[1mStep[0m  [24/84], [94mLoss[0m : 2.29603
[1mStep[0m  [32/84], [94mLoss[0m : 2.54818
[1mStep[0m  [40/84], [94mLoss[0m : 2.46300
[1mStep[0m  [48/84], [94mLoss[0m : 2.44663
[1mStep[0m  [56/84], [94mLoss[0m : 2.28426
[1mStep[0m  [64/84], [94mLoss[0m : 2.35514
[1mStep[0m  [72/84], [94mLoss[0m : 2.74113
[1mStep[0m  [80/84], [94mLoss[0m : 2.63190

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.537, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.509
====================================

Phase 2 - Evaluation MAE:  2.5094917672021047
MAE score P1      2.722933
MAE score P2      2.509492
loss              2.474535
learning_rate       0.0001
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay         0.001
Name: 2, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.02364
[1mStep[0m  [16/169], [94mLoss[0m : 11.17253
[1mStep[0m  [32/169], [94mLoss[0m : 11.30387
[1mStep[0m  [48/169], [94mLoss[0m : 12.41341
[1mStep[0m  [64/169], [94mLoss[0m : 11.36841
[1mStep[0m  [80/169], [94mLoss[0m : 11.66448
[1mStep[0m  [96/169], [94mLoss[0m : 10.67833
[1mStep[0m  [112/169], [94mLoss[0m : 11.12635
[1mStep[0m  [128/169], [94mLoss[0m : 10.33670
[1mStep[0m  [144/169], [94mLoss[0m : 10.95073
[1mStep[0m  [160/169], [94mLoss[0m : 10.26605

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.873, [92mTest[0m: 10.930, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.22238
[1mStep[0m  [16/169], [94mLoss[0m : 11.16498
[1mStep[0m  [32/169], [94mLoss[0m : 10.64011
[1mStep[0m  [48/169], [94mLoss[0m : 10.80665
[1mStep[0m  [64/169], [94mLoss[0m : 11.02582
[1mStep[0m  [80/169], [94mLoss[0m : 10.49225
[1mStep[0m  [96/169], [94mLoss[0m : 10.70750
[1mStep[0m  [112/169], [94mLoss[0m : 10.69725
[1mStep[0m  [128/169], [94mLoss[0m : 10.55580
[1mStep[0m  [144/169], [94mLoss[0m : 10.91846
[1mStep[0m  [160/169], [94mLoss[0m : 10.41490

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.44650
[1mStep[0m  [16/169], [94mLoss[0m : 10.87132
[1mStep[0m  [32/169], [94mLoss[0m : 10.80540
[1mStep[0m  [48/169], [94mLoss[0m : 10.90548
[1mStep[0m  [64/169], [94mLoss[0m : 11.70935
[1mStep[0m  [80/169], [94mLoss[0m : 10.72974
[1mStep[0m  [96/169], [94mLoss[0m : 9.83973
[1mStep[0m  [112/169], [94mLoss[0m : 10.49037
[1mStep[0m  [128/169], [94mLoss[0m : 11.82918
[1mStep[0m  [144/169], [94mLoss[0m : 10.17976
[1mStep[0m  [160/169], [94mLoss[0m : 10.12245

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.456, [92mTest[0m: 10.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.40701
[1mStep[0m  [16/169], [94mLoss[0m : 10.18980
[1mStep[0m  [32/169], [94mLoss[0m : 9.93703
[1mStep[0m  [48/169], [94mLoss[0m : 10.36342
[1mStep[0m  [64/169], [94mLoss[0m : 10.25452
[1mStep[0m  [80/169], [94mLoss[0m : 10.02115
[1mStep[0m  [96/169], [94mLoss[0m : 10.14147
[1mStep[0m  [112/169], [94mLoss[0m : 9.82619
[1mStep[0m  [128/169], [94mLoss[0m : 10.46467
[1mStep[0m  [144/169], [94mLoss[0m : 10.81972
[1mStep[0m  [160/169], [94mLoss[0m : 10.38262

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.238, [92mTest[0m: 10.145, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.28747
[1mStep[0m  [16/169], [94mLoss[0m : 9.60030
[1mStep[0m  [32/169], [94mLoss[0m : 10.00650
[1mStep[0m  [48/169], [94mLoss[0m : 10.78378
[1mStep[0m  [64/169], [94mLoss[0m : 10.76960
[1mStep[0m  [80/169], [94mLoss[0m : 9.31211
[1mStep[0m  [96/169], [94mLoss[0m : 9.41037
[1mStep[0m  [112/169], [94mLoss[0m : 9.52177
[1mStep[0m  [128/169], [94mLoss[0m : 10.21744
[1mStep[0m  [144/169], [94mLoss[0m : 10.13767
[1mStep[0m  [160/169], [94mLoss[0m : 9.78661

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.010, [92mTest[0m: 9.915, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.10686
[1mStep[0m  [16/169], [94mLoss[0m : 10.12433
[1mStep[0m  [32/169], [94mLoss[0m : 8.96820
[1mStep[0m  [48/169], [94mLoss[0m : 9.99420
[1mStep[0m  [64/169], [94mLoss[0m : 9.46538
[1mStep[0m  [80/169], [94mLoss[0m : 9.72688
[1mStep[0m  [96/169], [94mLoss[0m : 9.54044
[1mStep[0m  [112/169], [94mLoss[0m : 10.18374
[1mStep[0m  [128/169], [94mLoss[0m : 9.06466
[1mStep[0m  [144/169], [94mLoss[0m : 10.36466
[1mStep[0m  [160/169], [94mLoss[0m : 9.73826

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.780, [92mTest[0m: 9.611, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.01647
[1mStep[0m  [16/169], [94mLoss[0m : 9.57130
[1mStep[0m  [32/169], [94mLoss[0m : 10.25026
[1mStep[0m  [48/169], [94mLoss[0m : 8.68356
[1mStep[0m  [64/169], [94mLoss[0m : 9.79794
[1mStep[0m  [80/169], [94mLoss[0m : 9.88419
[1mStep[0m  [96/169], [94mLoss[0m : 9.61142
[1mStep[0m  [112/169], [94mLoss[0m : 9.53644
[1mStep[0m  [128/169], [94mLoss[0m : 9.39777
[1mStep[0m  [144/169], [94mLoss[0m : 9.67841
[1mStep[0m  [160/169], [94mLoss[0m : 9.68207

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.517, [92mTest[0m: 9.270, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.62710
[1mStep[0m  [16/169], [94mLoss[0m : 9.52156
[1mStep[0m  [32/169], [94mLoss[0m : 9.60915
[1mStep[0m  [48/169], [94mLoss[0m : 9.73256
[1mStep[0m  [64/169], [94mLoss[0m : 9.24664
[1mStep[0m  [80/169], [94mLoss[0m : 9.16259
[1mStep[0m  [96/169], [94mLoss[0m : 9.50895
[1mStep[0m  [112/169], [94mLoss[0m : 8.96979
[1mStep[0m  [128/169], [94mLoss[0m : 8.54401
[1mStep[0m  [144/169], [94mLoss[0m : 9.06974
[1mStep[0m  [160/169], [94mLoss[0m : 8.98028

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.255, [92mTest[0m: 9.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.90809
[1mStep[0m  [16/169], [94mLoss[0m : 8.26826
[1mStep[0m  [32/169], [94mLoss[0m : 9.36741
[1mStep[0m  [48/169], [94mLoss[0m : 8.87890
[1mStep[0m  [64/169], [94mLoss[0m : 9.12049
[1mStep[0m  [80/169], [94mLoss[0m : 9.10165
[1mStep[0m  [96/169], [94mLoss[0m : 9.50578
[1mStep[0m  [112/169], [94mLoss[0m : 9.07898
[1mStep[0m  [128/169], [94mLoss[0m : 8.73658
[1mStep[0m  [144/169], [94mLoss[0m : 9.04804
[1mStep[0m  [160/169], [94mLoss[0m : 8.63871

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.963, [92mTest[0m: 8.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.52999
[1mStep[0m  [16/169], [94mLoss[0m : 8.81230
[1mStep[0m  [32/169], [94mLoss[0m : 9.14033
[1mStep[0m  [48/169], [94mLoss[0m : 9.28237
[1mStep[0m  [64/169], [94mLoss[0m : 8.56581
[1mStep[0m  [80/169], [94mLoss[0m : 9.06222
[1mStep[0m  [96/169], [94mLoss[0m : 8.85079
[1mStep[0m  [112/169], [94mLoss[0m : 7.98725
[1mStep[0m  [128/169], [94mLoss[0m : 8.30078
[1mStep[0m  [144/169], [94mLoss[0m : 8.19517
[1mStep[0m  [160/169], [94mLoss[0m : 8.19215

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.627, [92mTest[0m: 8.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.35546
[1mStep[0m  [16/169], [94mLoss[0m : 8.44577
[1mStep[0m  [32/169], [94mLoss[0m : 7.59589
[1mStep[0m  [48/169], [94mLoss[0m : 7.85323
[1mStep[0m  [64/169], [94mLoss[0m : 8.25721
[1mStep[0m  [80/169], [94mLoss[0m : 8.31136
[1mStep[0m  [96/169], [94mLoss[0m : 8.76580
[1mStep[0m  [112/169], [94mLoss[0m : 8.08835
[1mStep[0m  [128/169], [94mLoss[0m : 8.39941
[1mStep[0m  [144/169], [94mLoss[0m : 9.11244
[1mStep[0m  [160/169], [94mLoss[0m : 7.80978

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.241, [92mTest[0m: 7.691, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.12741
[1mStep[0m  [16/169], [94mLoss[0m : 8.10111
[1mStep[0m  [32/169], [94mLoss[0m : 7.70181
[1mStep[0m  [48/169], [94mLoss[0m : 7.83354
[1mStep[0m  [64/169], [94mLoss[0m : 8.09087
[1mStep[0m  [80/169], [94mLoss[0m : 8.17670
[1mStep[0m  [96/169], [94mLoss[0m : 8.17253
[1mStep[0m  [112/169], [94mLoss[0m : 8.22904
[1mStep[0m  [128/169], [94mLoss[0m : 7.64856
[1mStep[0m  [144/169], [94mLoss[0m : 7.72243
[1mStep[0m  [160/169], [94mLoss[0m : 7.74886

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.815, [92mTest[0m: 7.459, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.02004
[1mStep[0m  [16/169], [94mLoss[0m : 7.28491
[1mStep[0m  [32/169], [94mLoss[0m : 7.66952
[1mStep[0m  [48/169], [94mLoss[0m : 7.13821
[1mStep[0m  [64/169], [94mLoss[0m : 6.51894
[1mStep[0m  [80/169], [94mLoss[0m : 7.86398
[1mStep[0m  [96/169], [94mLoss[0m : 6.54518
[1mStep[0m  [112/169], [94mLoss[0m : 7.31695
[1mStep[0m  [128/169], [94mLoss[0m : 7.46874
[1mStep[0m  [144/169], [94mLoss[0m : 6.86905
[1mStep[0m  [160/169], [94mLoss[0m : 7.10357

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.396, [92mTest[0m: 6.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.66194
[1mStep[0m  [16/169], [94mLoss[0m : 7.07770
[1mStep[0m  [32/169], [94mLoss[0m : 7.15773
[1mStep[0m  [48/169], [94mLoss[0m : 6.88630
[1mStep[0m  [64/169], [94mLoss[0m : 7.21688
[1mStep[0m  [80/169], [94mLoss[0m : 7.64880
[1mStep[0m  [96/169], [94mLoss[0m : 7.23226
[1mStep[0m  [112/169], [94mLoss[0m : 7.03634
[1mStep[0m  [128/169], [94mLoss[0m : 7.28809
[1mStep[0m  [144/169], [94mLoss[0m : 6.30766
[1mStep[0m  [160/169], [94mLoss[0m : 6.60554

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.967, [92mTest[0m: 6.226, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.61875
[1mStep[0m  [16/169], [94mLoss[0m : 6.46156
[1mStep[0m  [32/169], [94mLoss[0m : 7.26016
[1mStep[0m  [48/169], [94mLoss[0m : 7.16885
[1mStep[0m  [64/169], [94mLoss[0m : 6.66660
[1mStep[0m  [80/169], [94mLoss[0m : 6.59817
[1mStep[0m  [96/169], [94mLoss[0m : 6.00433
[1mStep[0m  [112/169], [94mLoss[0m : 7.00341
[1mStep[0m  [128/169], [94mLoss[0m : 7.52556
[1mStep[0m  [144/169], [94mLoss[0m : 6.17874
[1mStep[0m  [160/169], [94mLoss[0m : 6.94384

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.559, [92mTest[0m: 5.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.26943
[1mStep[0m  [16/169], [94mLoss[0m : 5.98259
[1mStep[0m  [32/169], [94mLoss[0m : 6.29934
[1mStep[0m  [48/169], [94mLoss[0m : 6.65046
[1mStep[0m  [64/169], [94mLoss[0m : 5.90135
[1mStep[0m  [80/169], [94mLoss[0m : 6.12460
[1mStep[0m  [96/169], [94mLoss[0m : 6.20555
[1mStep[0m  [112/169], [94mLoss[0m : 6.75895
[1mStep[0m  [128/169], [94mLoss[0m : 5.92741
[1mStep[0m  [144/169], [94mLoss[0m : 6.56916
[1mStep[0m  [160/169], [94mLoss[0m : 6.01980

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.148, [92mTest[0m: 5.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.90118
[1mStep[0m  [16/169], [94mLoss[0m : 5.44701
[1mStep[0m  [32/169], [94mLoss[0m : 6.47817
[1mStep[0m  [48/169], [94mLoss[0m : 6.57391
[1mStep[0m  [64/169], [94mLoss[0m : 5.52390
[1mStep[0m  [80/169], [94mLoss[0m : 6.69776
[1mStep[0m  [96/169], [94mLoss[0m : 5.32274
[1mStep[0m  [112/169], [94mLoss[0m : 5.28373
[1mStep[0m  [128/169], [94mLoss[0m : 5.51400
[1mStep[0m  [144/169], [94mLoss[0m : 5.49817
[1mStep[0m  [160/169], [94mLoss[0m : 5.93024

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.776, [92mTest[0m: 4.993, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.38282
[1mStep[0m  [16/169], [94mLoss[0m : 5.92431
[1mStep[0m  [32/169], [94mLoss[0m : 5.55155
[1mStep[0m  [48/169], [94mLoss[0m : 6.01451
[1mStep[0m  [64/169], [94mLoss[0m : 5.81694
[1mStep[0m  [80/169], [94mLoss[0m : 5.44046
[1mStep[0m  [96/169], [94mLoss[0m : 5.46257
[1mStep[0m  [112/169], [94mLoss[0m : 5.29968
[1mStep[0m  [128/169], [94mLoss[0m : 4.91808
[1mStep[0m  [144/169], [94mLoss[0m : 4.91516
[1mStep[0m  [160/169], [94mLoss[0m : 5.03392

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.402, [92mTest[0m: 4.725, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.60837
[1mStep[0m  [16/169], [94mLoss[0m : 5.09036
[1mStep[0m  [32/169], [94mLoss[0m : 5.33174
[1mStep[0m  [48/169], [94mLoss[0m : 5.80900
[1mStep[0m  [64/169], [94mLoss[0m : 4.83734
[1mStep[0m  [80/169], [94mLoss[0m : 4.62589
[1mStep[0m  [96/169], [94mLoss[0m : 4.91204
[1mStep[0m  [112/169], [94mLoss[0m : 4.72468
[1mStep[0m  [128/169], [94mLoss[0m : 5.34132
[1mStep[0m  [144/169], [94mLoss[0m : 4.82901
[1mStep[0m  [160/169], [94mLoss[0m : 4.40541

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.969, [92mTest[0m: 4.291, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.49402
[1mStep[0m  [16/169], [94mLoss[0m : 4.32141
[1mStep[0m  [32/169], [94mLoss[0m : 4.72364
[1mStep[0m  [48/169], [94mLoss[0m : 5.21422
[1mStep[0m  [64/169], [94mLoss[0m : 4.85705
[1mStep[0m  [80/169], [94mLoss[0m : 4.82862
[1mStep[0m  [96/169], [94mLoss[0m : 4.43328
[1mStep[0m  [112/169], [94mLoss[0m : 4.30852
[1mStep[0m  [128/169], [94mLoss[0m : 4.35682
[1mStep[0m  [144/169], [94mLoss[0m : 4.67233
[1mStep[0m  [160/169], [94mLoss[0m : 3.51053

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.581, [92mTest[0m: 3.910, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.30479
[1mStep[0m  [16/169], [94mLoss[0m : 4.62367
[1mStep[0m  [32/169], [94mLoss[0m : 4.17373
[1mStep[0m  [48/169], [94mLoss[0m : 4.25532
[1mStep[0m  [64/169], [94mLoss[0m : 4.35248
[1mStep[0m  [80/169], [94mLoss[0m : 4.15786
[1mStep[0m  [96/169], [94mLoss[0m : 4.01159
[1mStep[0m  [112/169], [94mLoss[0m : 3.58996
[1mStep[0m  [128/169], [94mLoss[0m : 3.84071
[1mStep[0m  [144/169], [94mLoss[0m : 3.66570
[1mStep[0m  [160/169], [94mLoss[0m : 3.56695

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.201, [92mTest[0m: 3.727, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.12363
[1mStep[0m  [16/169], [94mLoss[0m : 3.20583
[1mStep[0m  [32/169], [94mLoss[0m : 3.63249
[1mStep[0m  [48/169], [94mLoss[0m : 4.24259
[1mStep[0m  [64/169], [94mLoss[0m : 3.77388
[1mStep[0m  [80/169], [94mLoss[0m : 4.13357
[1mStep[0m  [96/169], [94mLoss[0m : 3.35891
[1mStep[0m  [112/169], [94mLoss[0m : 3.72696
[1mStep[0m  [128/169], [94mLoss[0m : 4.14395
[1mStep[0m  [144/169], [94mLoss[0m : 3.66947
[1mStep[0m  [160/169], [94mLoss[0m : 4.07360

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.853, [92mTest[0m: 3.214, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.32410
[1mStep[0m  [16/169], [94mLoss[0m : 3.60600
[1mStep[0m  [32/169], [94mLoss[0m : 3.80750
[1mStep[0m  [48/169], [94mLoss[0m : 2.87905
[1mStep[0m  [64/169], [94mLoss[0m : 4.06567
[1mStep[0m  [80/169], [94mLoss[0m : 4.06520
[1mStep[0m  [96/169], [94mLoss[0m : 3.96320
[1mStep[0m  [112/169], [94mLoss[0m : 3.36664
[1mStep[0m  [128/169], [94mLoss[0m : 3.87862
[1mStep[0m  [144/169], [94mLoss[0m : 2.78792
[1mStep[0m  [160/169], [94mLoss[0m : 3.64433

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.546, [92mTest[0m: 3.067, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.63844
[1mStep[0m  [16/169], [94mLoss[0m : 3.16324
[1mStep[0m  [32/169], [94mLoss[0m : 3.31625
[1mStep[0m  [48/169], [94mLoss[0m : 2.92019
[1mStep[0m  [64/169], [94mLoss[0m : 3.17690
[1mStep[0m  [80/169], [94mLoss[0m : 3.42457
[1mStep[0m  [96/169], [94mLoss[0m : 2.96521
[1mStep[0m  [112/169], [94mLoss[0m : 3.11533
[1mStep[0m  [128/169], [94mLoss[0m : 3.43582
[1mStep[0m  [144/169], [94mLoss[0m : 3.50769
[1mStep[0m  [160/169], [94mLoss[0m : 2.73724

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.305, [92mTest[0m: 2.760, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.25850
[1mStep[0m  [16/169], [94mLoss[0m : 3.19352
[1mStep[0m  [32/169], [94mLoss[0m : 3.01939
[1mStep[0m  [48/169], [94mLoss[0m : 3.46262
[1mStep[0m  [64/169], [94mLoss[0m : 3.31089
[1mStep[0m  [80/169], [94mLoss[0m : 3.26074
[1mStep[0m  [96/169], [94mLoss[0m : 2.93286
[1mStep[0m  [112/169], [94mLoss[0m : 2.66133
[1mStep[0m  [128/169], [94mLoss[0m : 2.51450
[1mStep[0m  [144/169], [94mLoss[0m : 2.98142
[1mStep[0m  [160/169], [94mLoss[0m : 2.82506

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.078, [92mTest[0m: 2.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53750
[1mStep[0m  [16/169], [94mLoss[0m : 2.59601
[1mStep[0m  [32/169], [94mLoss[0m : 2.95602
[1mStep[0m  [48/169], [94mLoss[0m : 3.21230
[1mStep[0m  [64/169], [94mLoss[0m : 3.06663
[1mStep[0m  [80/169], [94mLoss[0m : 3.27097
[1mStep[0m  [96/169], [94mLoss[0m : 2.94676
[1mStep[0m  [112/169], [94mLoss[0m : 2.58506
[1mStep[0m  [128/169], [94mLoss[0m : 2.85588
[1mStep[0m  [144/169], [94mLoss[0m : 3.08278
[1mStep[0m  [160/169], [94mLoss[0m : 2.55364

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.916, [92mTest[0m: 2.505, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75664
[1mStep[0m  [16/169], [94mLoss[0m : 2.33804
[1mStep[0m  [32/169], [94mLoss[0m : 3.09901
[1mStep[0m  [48/169], [94mLoss[0m : 2.60568
[1mStep[0m  [64/169], [94mLoss[0m : 2.82172
[1mStep[0m  [80/169], [94mLoss[0m : 2.77099
[1mStep[0m  [96/169], [94mLoss[0m : 2.70177
[1mStep[0m  [112/169], [94mLoss[0m : 3.27943
[1mStep[0m  [128/169], [94mLoss[0m : 2.63199
[1mStep[0m  [144/169], [94mLoss[0m : 2.65422
[1mStep[0m  [160/169], [94mLoss[0m : 2.86574

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.829, [92mTest[0m: 2.408, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.07132
[1mStep[0m  [16/169], [94mLoss[0m : 2.61845
[1mStep[0m  [32/169], [94mLoss[0m : 2.98471
[1mStep[0m  [48/169], [94mLoss[0m : 2.75407
[1mStep[0m  [64/169], [94mLoss[0m : 2.51334
[1mStep[0m  [80/169], [94mLoss[0m : 2.86231
[1mStep[0m  [96/169], [94mLoss[0m : 2.83571
[1mStep[0m  [112/169], [94mLoss[0m : 2.66434
[1mStep[0m  [128/169], [94mLoss[0m : 2.48201
[1mStep[0m  [144/169], [94mLoss[0m : 2.47126
[1mStep[0m  [160/169], [94mLoss[0m : 2.74057

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.793, [92mTest[0m: 2.381, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.02170
[1mStep[0m  [16/169], [94mLoss[0m : 3.02986
[1mStep[0m  [32/169], [94mLoss[0m : 2.94643
[1mStep[0m  [48/169], [94mLoss[0m : 2.59978
[1mStep[0m  [64/169], [94mLoss[0m : 2.81051
[1mStep[0m  [80/169], [94mLoss[0m : 2.79997
[1mStep[0m  [96/169], [94mLoss[0m : 2.44553
[1mStep[0m  [112/169], [94mLoss[0m : 2.68686
[1mStep[0m  [128/169], [94mLoss[0m : 2.52015
[1mStep[0m  [144/169], [94mLoss[0m : 2.85223
[1mStep[0m  [160/169], [94mLoss[0m : 2.70109

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.736, [92mTest[0m: 2.378, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73399
[1mStep[0m  [16/169], [94mLoss[0m : 2.63389
[1mStep[0m  [32/169], [94mLoss[0m : 2.98262
[1mStep[0m  [48/169], [94mLoss[0m : 3.11085
[1mStep[0m  [64/169], [94mLoss[0m : 2.97277
[1mStep[0m  [80/169], [94mLoss[0m : 2.55295
[1mStep[0m  [96/169], [94mLoss[0m : 2.46441
[1mStep[0m  [112/169], [94mLoss[0m : 2.32700
[1mStep[0m  [128/169], [94mLoss[0m : 2.75056
[1mStep[0m  [144/169], [94mLoss[0m : 2.91902
[1mStep[0m  [160/169], [94mLoss[0m : 2.79473

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.371, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.373
====================================

Phase 1 - Evaluation MAE:  2.3732270087514604
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.84869
[1mStep[0m  [16/169], [94mLoss[0m : 3.11103
[1mStep[0m  [32/169], [94mLoss[0m : 2.40426
[1mStep[0m  [48/169], [94mLoss[0m : 2.56417
[1mStep[0m  [64/169], [94mLoss[0m : 2.97775
[1mStep[0m  [80/169], [94mLoss[0m : 2.67948
[1mStep[0m  [96/169], [94mLoss[0m : 2.62340
[1mStep[0m  [112/169], [94mLoss[0m : 2.66048
[1mStep[0m  [128/169], [94mLoss[0m : 2.70910
[1mStep[0m  [144/169], [94mLoss[0m : 2.98075
[1mStep[0m  [160/169], [94mLoss[0m : 2.91791

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.774, [92mTest[0m: 2.367, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65754
[1mStep[0m  [16/169], [94mLoss[0m : 2.56415
[1mStep[0m  [32/169], [94mLoss[0m : 2.58076
[1mStep[0m  [48/169], [94mLoss[0m : 2.88050
[1mStep[0m  [64/169], [94mLoss[0m : 3.01380
[1mStep[0m  [80/169], [94mLoss[0m : 2.65580
[1mStep[0m  [96/169], [94mLoss[0m : 2.85247
[1mStep[0m  [112/169], [94mLoss[0m : 2.34885
[1mStep[0m  [128/169], [94mLoss[0m : 2.58496
[1mStep[0m  [144/169], [94mLoss[0m : 2.66894
[1mStep[0m  [160/169], [94mLoss[0m : 2.69997

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.27811
[1mStep[0m  [16/169], [94mLoss[0m : 2.66036
[1mStep[0m  [32/169], [94mLoss[0m : 2.62637
[1mStep[0m  [48/169], [94mLoss[0m : 2.98662
[1mStep[0m  [64/169], [94mLoss[0m : 2.94370
[1mStep[0m  [80/169], [94mLoss[0m : 2.43780
[1mStep[0m  [96/169], [94mLoss[0m : 2.38983
[1mStep[0m  [112/169], [94mLoss[0m : 2.84401
[1mStep[0m  [128/169], [94mLoss[0m : 2.58132
[1mStep[0m  [144/169], [94mLoss[0m : 2.83826
[1mStep[0m  [160/169], [94mLoss[0m : 2.60806

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67619
[1mStep[0m  [16/169], [94mLoss[0m : 2.48989
[1mStep[0m  [32/169], [94mLoss[0m : 2.70712
[1mStep[0m  [48/169], [94mLoss[0m : 2.78978
[1mStep[0m  [64/169], [94mLoss[0m : 2.99634
[1mStep[0m  [80/169], [94mLoss[0m : 2.24467
[1mStep[0m  [96/169], [94mLoss[0m : 2.84527
[1mStep[0m  [112/169], [94mLoss[0m : 2.59597
[1mStep[0m  [128/169], [94mLoss[0m : 2.52534
[1mStep[0m  [144/169], [94mLoss[0m : 2.91957
[1mStep[0m  [160/169], [94mLoss[0m : 2.23595

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.92184
[1mStep[0m  [16/169], [94mLoss[0m : 2.18392
[1mStep[0m  [32/169], [94mLoss[0m : 3.10214
[1mStep[0m  [48/169], [94mLoss[0m : 2.03566
[1mStep[0m  [64/169], [94mLoss[0m : 2.20710
[1mStep[0m  [80/169], [94mLoss[0m : 2.41726
[1mStep[0m  [96/169], [94mLoss[0m : 3.02432
[1mStep[0m  [112/169], [94mLoss[0m : 3.05134
[1mStep[0m  [128/169], [94mLoss[0m : 3.10847
[1mStep[0m  [144/169], [94mLoss[0m : 2.88453
[1mStep[0m  [160/169], [94mLoss[0m : 2.16156

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63771
[1mStep[0m  [16/169], [94mLoss[0m : 2.68433
[1mStep[0m  [32/169], [94mLoss[0m : 2.72844
[1mStep[0m  [48/169], [94mLoss[0m : 2.45495
[1mStep[0m  [64/169], [94mLoss[0m : 2.65412
[1mStep[0m  [80/169], [94mLoss[0m : 3.02614
[1mStep[0m  [96/169], [94mLoss[0m : 2.70128
[1mStep[0m  [112/169], [94mLoss[0m : 2.79674
[1mStep[0m  [128/169], [94mLoss[0m : 2.07380
[1mStep[0m  [144/169], [94mLoss[0m : 2.40001
[1mStep[0m  [160/169], [94mLoss[0m : 2.02231

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42530
[1mStep[0m  [16/169], [94mLoss[0m : 2.79025
[1mStep[0m  [32/169], [94mLoss[0m : 2.36200
[1mStep[0m  [48/169], [94mLoss[0m : 2.42445
[1mStep[0m  [64/169], [94mLoss[0m : 2.84084
[1mStep[0m  [80/169], [94mLoss[0m : 2.56073
[1mStep[0m  [96/169], [94mLoss[0m : 2.49296
[1mStep[0m  [112/169], [94mLoss[0m : 2.18702
[1mStep[0m  [128/169], [94mLoss[0m : 2.54986
[1mStep[0m  [144/169], [94mLoss[0m : 3.18766
[1mStep[0m  [160/169], [94mLoss[0m : 3.00733

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57481
[1mStep[0m  [16/169], [94mLoss[0m : 2.45815
[1mStep[0m  [32/169], [94mLoss[0m : 2.87735
[1mStep[0m  [48/169], [94mLoss[0m : 2.42093
[1mStep[0m  [64/169], [94mLoss[0m : 2.88186
[1mStep[0m  [80/169], [94mLoss[0m : 2.74227
[1mStep[0m  [96/169], [94mLoss[0m : 2.49233
[1mStep[0m  [112/169], [94mLoss[0m : 2.43580
[1mStep[0m  [128/169], [94mLoss[0m : 3.02472
[1mStep[0m  [144/169], [94mLoss[0m : 2.92062
[1mStep[0m  [160/169], [94mLoss[0m : 2.63592

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.573, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26352
[1mStep[0m  [16/169], [94mLoss[0m : 2.87740
[1mStep[0m  [32/169], [94mLoss[0m : 2.55582
[1mStep[0m  [48/169], [94mLoss[0m : 2.11941
[1mStep[0m  [64/169], [94mLoss[0m : 2.71823
[1mStep[0m  [80/169], [94mLoss[0m : 2.42109
[1mStep[0m  [96/169], [94mLoss[0m : 3.12324
[1mStep[0m  [112/169], [94mLoss[0m : 2.36613
[1mStep[0m  [128/169], [94mLoss[0m : 2.72262
[1mStep[0m  [144/169], [94mLoss[0m : 2.87494
[1mStep[0m  [160/169], [94mLoss[0m : 2.68259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.545, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67142
[1mStep[0m  [16/169], [94mLoss[0m : 2.61903
[1mStep[0m  [32/169], [94mLoss[0m : 2.69577
[1mStep[0m  [48/169], [94mLoss[0m : 2.94644
[1mStep[0m  [64/169], [94mLoss[0m : 2.65163
[1mStep[0m  [80/169], [94mLoss[0m : 2.37806
[1mStep[0m  [96/169], [94mLoss[0m : 2.62739
[1mStep[0m  [112/169], [94mLoss[0m : 2.32011
[1mStep[0m  [128/169], [94mLoss[0m : 2.70172
[1mStep[0m  [144/169], [94mLoss[0m : 2.47590
[1mStep[0m  [160/169], [94mLoss[0m : 2.38385

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.553, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78593
[1mStep[0m  [16/169], [94mLoss[0m : 2.24760
[1mStep[0m  [32/169], [94mLoss[0m : 2.40297
[1mStep[0m  [48/169], [94mLoss[0m : 2.51274
[1mStep[0m  [64/169], [94mLoss[0m : 2.86936
[1mStep[0m  [80/169], [94mLoss[0m : 2.18751
[1mStep[0m  [96/169], [94mLoss[0m : 2.51514
[1mStep[0m  [112/169], [94mLoss[0m : 2.60760
[1mStep[0m  [128/169], [94mLoss[0m : 2.67296
[1mStep[0m  [144/169], [94mLoss[0m : 2.48522
[1mStep[0m  [160/169], [94mLoss[0m : 2.58503

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.531, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02811
[1mStep[0m  [16/169], [94mLoss[0m : 3.01789
[1mStep[0m  [32/169], [94mLoss[0m : 2.32201
[1mStep[0m  [48/169], [94mLoss[0m : 2.39078
[1mStep[0m  [64/169], [94mLoss[0m : 2.68037
[1mStep[0m  [80/169], [94mLoss[0m : 2.56592
[1mStep[0m  [96/169], [94mLoss[0m : 2.83279
[1mStep[0m  [112/169], [94mLoss[0m : 2.61698
[1mStep[0m  [128/169], [94mLoss[0m : 2.69372
[1mStep[0m  [144/169], [94mLoss[0m : 2.35258
[1mStep[0m  [160/169], [94mLoss[0m : 2.34713

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.25611
[1mStep[0m  [16/169], [94mLoss[0m : 2.20163
[1mStep[0m  [32/169], [94mLoss[0m : 2.37496
[1mStep[0m  [48/169], [94mLoss[0m : 2.69535
[1mStep[0m  [64/169], [94mLoss[0m : 2.90216
[1mStep[0m  [80/169], [94mLoss[0m : 2.34664
[1mStep[0m  [96/169], [94mLoss[0m : 2.92992
[1mStep[0m  [112/169], [94mLoss[0m : 2.65169
[1mStep[0m  [128/169], [94mLoss[0m : 2.64459
[1mStep[0m  [144/169], [94mLoss[0m : 2.49339
[1mStep[0m  [160/169], [94mLoss[0m : 2.78459

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.587, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37741
[1mStep[0m  [16/169], [94mLoss[0m : 2.52490
[1mStep[0m  [32/169], [94mLoss[0m : 2.47289
[1mStep[0m  [48/169], [94mLoss[0m : 2.88989
[1mStep[0m  [64/169], [94mLoss[0m : 2.31472
[1mStep[0m  [80/169], [94mLoss[0m : 2.51614
[1mStep[0m  [96/169], [94mLoss[0m : 2.35417
[1mStep[0m  [112/169], [94mLoss[0m : 2.78118
[1mStep[0m  [128/169], [94mLoss[0m : 2.46120
[1mStep[0m  [144/169], [94mLoss[0m : 3.05856
[1mStep[0m  [160/169], [94mLoss[0m : 2.47472

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.533, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63205
[1mStep[0m  [16/169], [94mLoss[0m : 2.30590
[1mStep[0m  [32/169], [94mLoss[0m : 2.16858
[1mStep[0m  [48/169], [94mLoss[0m : 2.40658
[1mStep[0m  [64/169], [94mLoss[0m : 2.43748
[1mStep[0m  [80/169], [94mLoss[0m : 3.15398
[1mStep[0m  [96/169], [94mLoss[0m : 1.92012
[1mStep[0m  [112/169], [94mLoss[0m : 2.30670
[1mStep[0m  [128/169], [94mLoss[0m : 2.41081
[1mStep[0m  [144/169], [94mLoss[0m : 2.19124
[1mStep[0m  [160/169], [94mLoss[0m : 2.38551

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54697
[1mStep[0m  [16/169], [94mLoss[0m : 2.67897
[1mStep[0m  [32/169], [94mLoss[0m : 2.65730
[1mStep[0m  [48/169], [94mLoss[0m : 2.86682
[1mStep[0m  [64/169], [94mLoss[0m : 2.22416
[1mStep[0m  [80/169], [94mLoss[0m : 2.59134
[1mStep[0m  [96/169], [94mLoss[0m : 2.32544
[1mStep[0m  [112/169], [94mLoss[0m : 2.45010
[1mStep[0m  [128/169], [94mLoss[0m : 2.36077
[1mStep[0m  [144/169], [94mLoss[0m : 2.33635
[1mStep[0m  [160/169], [94mLoss[0m : 2.33845

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.552, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10028
[1mStep[0m  [16/169], [94mLoss[0m : 2.24492
[1mStep[0m  [32/169], [94mLoss[0m : 2.55760
[1mStep[0m  [48/169], [94mLoss[0m : 2.15505
[1mStep[0m  [64/169], [94mLoss[0m : 2.72739
[1mStep[0m  [80/169], [94mLoss[0m : 2.55806
[1mStep[0m  [96/169], [94mLoss[0m : 2.75945
[1mStep[0m  [112/169], [94mLoss[0m : 2.58369
[1mStep[0m  [128/169], [94mLoss[0m : 2.32733
[1mStep[0m  [144/169], [94mLoss[0m : 2.74803
[1mStep[0m  [160/169], [94mLoss[0m : 2.55842

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20644
[1mStep[0m  [16/169], [94mLoss[0m : 2.81913
[1mStep[0m  [32/169], [94mLoss[0m : 2.15988
[1mStep[0m  [48/169], [94mLoss[0m : 2.53649
[1mStep[0m  [64/169], [94mLoss[0m : 2.81937
[1mStep[0m  [80/169], [94mLoss[0m : 2.50623
[1mStep[0m  [96/169], [94mLoss[0m : 2.36137
[1mStep[0m  [112/169], [94mLoss[0m : 2.00115
[1mStep[0m  [128/169], [94mLoss[0m : 2.70607
[1mStep[0m  [144/169], [94mLoss[0m : 2.41998
[1mStep[0m  [160/169], [94mLoss[0m : 2.74144

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.510, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61821
[1mStep[0m  [16/169], [94mLoss[0m : 2.65086
[1mStep[0m  [32/169], [94mLoss[0m : 2.44089
[1mStep[0m  [48/169], [94mLoss[0m : 2.36732
[1mStep[0m  [64/169], [94mLoss[0m : 2.52327
[1mStep[0m  [80/169], [94mLoss[0m : 2.71327
[1mStep[0m  [96/169], [94mLoss[0m : 2.73182
[1mStep[0m  [112/169], [94mLoss[0m : 2.50604
[1mStep[0m  [128/169], [94mLoss[0m : 2.31134
[1mStep[0m  [144/169], [94mLoss[0m : 2.64447
[1mStep[0m  [160/169], [94mLoss[0m : 2.54134

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.490, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26212
[1mStep[0m  [16/169], [94mLoss[0m : 2.63529
[1mStep[0m  [32/169], [94mLoss[0m : 2.80681
[1mStep[0m  [48/169], [94mLoss[0m : 2.38340
[1mStep[0m  [64/169], [94mLoss[0m : 2.25504
[1mStep[0m  [80/169], [94mLoss[0m : 2.33827
[1mStep[0m  [96/169], [94mLoss[0m : 2.64391
[1mStep[0m  [112/169], [94mLoss[0m : 2.31017
[1mStep[0m  [128/169], [94mLoss[0m : 2.45832
[1mStep[0m  [144/169], [94mLoss[0m : 2.35705
[1mStep[0m  [160/169], [94mLoss[0m : 2.32904

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.524, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02881
[1mStep[0m  [16/169], [94mLoss[0m : 2.67245
[1mStep[0m  [32/169], [94mLoss[0m : 2.14639
[1mStep[0m  [48/169], [94mLoss[0m : 2.36594
[1mStep[0m  [64/169], [94mLoss[0m : 2.34283
[1mStep[0m  [80/169], [94mLoss[0m : 2.06564
[1mStep[0m  [96/169], [94mLoss[0m : 2.13579
[1mStep[0m  [112/169], [94mLoss[0m : 2.72270
[1mStep[0m  [128/169], [94mLoss[0m : 2.72325
[1mStep[0m  [144/169], [94mLoss[0m : 2.34420
[1mStep[0m  [160/169], [94mLoss[0m : 2.77248

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.509, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67350
[1mStep[0m  [16/169], [94mLoss[0m : 2.36142
[1mStep[0m  [32/169], [94mLoss[0m : 2.60186
[1mStep[0m  [48/169], [94mLoss[0m : 2.61839
[1mStep[0m  [64/169], [94mLoss[0m : 2.57254
[1mStep[0m  [80/169], [94mLoss[0m : 2.43805
[1mStep[0m  [96/169], [94mLoss[0m : 2.42068
[1mStep[0m  [112/169], [94mLoss[0m : 2.42842
[1mStep[0m  [128/169], [94mLoss[0m : 2.74512
[1mStep[0m  [144/169], [94mLoss[0m : 2.77236
[1mStep[0m  [160/169], [94mLoss[0m : 2.75849

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.489, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74387
[1mStep[0m  [16/169], [94mLoss[0m : 2.57560
[1mStep[0m  [32/169], [94mLoss[0m : 1.91869
[1mStep[0m  [48/169], [94mLoss[0m : 2.41582
[1mStep[0m  [64/169], [94mLoss[0m : 2.72115
[1mStep[0m  [80/169], [94mLoss[0m : 2.35306
[1mStep[0m  [96/169], [94mLoss[0m : 2.27855
[1mStep[0m  [112/169], [94mLoss[0m : 2.19309
[1mStep[0m  [128/169], [94mLoss[0m : 2.50103
[1mStep[0m  [144/169], [94mLoss[0m : 2.16972
[1mStep[0m  [160/169], [94mLoss[0m : 2.12240

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.490, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63428
[1mStep[0m  [16/169], [94mLoss[0m : 2.20556
[1mStep[0m  [32/169], [94mLoss[0m : 2.25278
[1mStep[0m  [48/169], [94mLoss[0m : 2.42408
[1mStep[0m  [64/169], [94mLoss[0m : 2.56371
[1mStep[0m  [80/169], [94mLoss[0m : 2.24929
[1mStep[0m  [96/169], [94mLoss[0m : 2.47558
[1mStep[0m  [112/169], [94mLoss[0m : 2.46200
[1mStep[0m  [128/169], [94mLoss[0m : 3.03777
[1mStep[0m  [144/169], [94mLoss[0m : 2.63709
[1mStep[0m  [160/169], [94mLoss[0m : 2.54540

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.535, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65066
[1mStep[0m  [16/169], [94mLoss[0m : 2.21504
[1mStep[0m  [32/169], [94mLoss[0m : 2.54950
[1mStep[0m  [48/169], [94mLoss[0m : 2.25547
[1mStep[0m  [64/169], [94mLoss[0m : 2.34710
[1mStep[0m  [80/169], [94mLoss[0m : 2.29420
[1mStep[0m  [96/169], [94mLoss[0m : 2.74084
[1mStep[0m  [112/169], [94mLoss[0m : 2.54343
[1mStep[0m  [128/169], [94mLoss[0m : 2.26651
[1mStep[0m  [144/169], [94mLoss[0m : 2.22024
[1mStep[0m  [160/169], [94mLoss[0m : 2.65149

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.475, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51138
[1mStep[0m  [16/169], [94mLoss[0m : 2.58849
[1mStep[0m  [32/169], [94mLoss[0m : 2.36371
[1mStep[0m  [48/169], [94mLoss[0m : 2.09305
[1mStep[0m  [64/169], [94mLoss[0m : 2.35958
[1mStep[0m  [80/169], [94mLoss[0m : 2.40467
[1mStep[0m  [96/169], [94mLoss[0m : 2.48830
[1mStep[0m  [112/169], [94mLoss[0m : 2.40488
[1mStep[0m  [128/169], [94mLoss[0m : 2.29881
[1mStep[0m  [144/169], [94mLoss[0m : 2.59196
[1mStep[0m  [160/169], [94mLoss[0m : 2.70029

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.452, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42504
[1mStep[0m  [16/169], [94mLoss[0m : 2.44960
[1mStep[0m  [32/169], [94mLoss[0m : 2.13350
[1mStep[0m  [48/169], [94mLoss[0m : 2.31044
[1mStep[0m  [64/169], [94mLoss[0m : 2.37815
[1mStep[0m  [80/169], [94mLoss[0m : 2.41185
[1mStep[0m  [96/169], [94mLoss[0m : 2.24362
[1mStep[0m  [112/169], [94mLoss[0m : 2.32524
[1mStep[0m  [128/169], [94mLoss[0m : 2.93259
[1mStep[0m  [144/169], [94mLoss[0m : 2.09039
[1mStep[0m  [160/169], [94mLoss[0m : 1.75955

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58591
[1mStep[0m  [16/169], [94mLoss[0m : 2.78310
[1mStep[0m  [32/169], [94mLoss[0m : 2.08692
[1mStep[0m  [48/169], [94mLoss[0m : 2.38036
[1mStep[0m  [64/169], [94mLoss[0m : 2.51160
[1mStep[0m  [80/169], [94mLoss[0m : 2.27055
[1mStep[0m  [96/169], [94mLoss[0m : 1.92633
[1mStep[0m  [112/169], [94mLoss[0m : 2.66498
[1mStep[0m  [128/169], [94mLoss[0m : 2.50409
[1mStep[0m  [144/169], [94mLoss[0m : 2.13018
[1mStep[0m  [160/169], [94mLoss[0m : 2.34310

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.497, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35624
[1mStep[0m  [16/169], [94mLoss[0m : 2.55019
[1mStep[0m  [32/169], [94mLoss[0m : 2.53131
[1mStep[0m  [48/169], [94mLoss[0m : 2.42060
[1mStep[0m  [64/169], [94mLoss[0m : 2.31648
[1mStep[0m  [80/169], [94mLoss[0m : 2.22050
[1mStep[0m  [96/169], [94mLoss[0m : 2.14365
[1mStep[0m  [112/169], [94mLoss[0m : 2.30126
[1mStep[0m  [128/169], [94mLoss[0m : 2.16645
[1mStep[0m  [144/169], [94mLoss[0m : 2.74193
[1mStep[0m  [160/169], [94mLoss[0m : 2.30841

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.431, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39059
[1mStep[0m  [16/169], [94mLoss[0m : 2.14191
[1mStep[0m  [32/169], [94mLoss[0m : 2.78573
[1mStep[0m  [48/169], [94mLoss[0m : 2.32731
[1mStep[0m  [64/169], [94mLoss[0m : 1.93474
[1mStep[0m  [80/169], [94mLoss[0m : 2.34025
[1mStep[0m  [96/169], [94mLoss[0m : 2.59662
[1mStep[0m  [112/169], [94mLoss[0m : 2.13491
[1mStep[0m  [128/169], [94mLoss[0m : 2.75994
[1mStep[0m  [144/169], [94mLoss[0m : 2.28893
[1mStep[0m  [160/169], [94mLoss[0m : 2.31654

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.523, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.497
====================================

Phase 2 - Evaluation MAE:  2.497035047837666
MAE score P1        2.373227
MAE score P2        2.497035
loss                2.324094
learning_rate         0.0001
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.04556
[1mStep[0m  [8/84], [94mLoss[0m : 10.80128
[1mStep[0m  [16/84], [94mLoss[0m : 11.61461
[1mStep[0m  [24/84], [94mLoss[0m : 10.80149
[1mStep[0m  [32/84], [94mLoss[0m : 11.33937
[1mStep[0m  [40/84], [94mLoss[0m : 11.22096
[1mStep[0m  [48/84], [94mLoss[0m : 11.02658
[1mStep[0m  [56/84], [94mLoss[0m : 10.90239
[1mStep[0m  [64/84], [94mLoss[0m : 10.74223
[1mStep[0m  [72/84], [94mLoss[0m : 10.58731
[1mStep[0m  [80/84], [94mLoss[0m : 10.37606

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.996, [92mTest[0m: 11.034, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.07716
[1mStep[0m  [8/84], [94mLoss[0m : 11.23395
[1mStep[0m  [16/84], [94mLoss[0m : 10.19753
[1mStep[0m  [24/84], [94mLoss[0m : 10.67676
[1mStep[0m  [32/84], [94mLoss[0m : 11.25422
[1mStep[0m  [40/84], [94mLoss[0m : 11.19894
[1mStep[0m  [48/84], [94mLoss[0m : 10.74656
[1mStep[0m  [56/84], [94mLoss[0m : 10.89286
[1mStep[0m  [64/84], [94mLoss[0m : 11.66294
[1mStep[0m  [72/84], [94mLoss[0m : 10.55337
[1mStep[0m  [80/84], [94mLoss[0m : 10.23563

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.98416
[1mStep[0m  [8/84], [94mLoss[0m : 10.41428
[1mStep[0m  [16/84], [94mLoss[0m : 10.48650
[1mStep[0m  [24/84], [94mLoss[0m : 10.58075
[1mStep[0m  [32/84], [94mLoss[0m : 11.26984
[1mStep[0m  [40/84], [94mLoss[0m : 10.52117
[1mStep[0m  [48/84], [94mLoss[0m : 10.79816
[1mStep[0m  [56/84], [94mLoss[0m : 10.78173
[1mStep[0m  [64/84], [94mLoss[0m : 10.64194
[1mStep[0m  [72/84], [94mLoss[0m : 10.48727
[1mStep[0m  [80/84], [94mLoss[0m : 10.78762

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.783, [92mTest[0m: 10.761, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.81550
[1mStep[0m  [8/84], [94mLoss[0m : 10.70709
[1mStep[0m  [16/84], [94mLoss[0m : 10.62064
[1mStep[0m  [24/84], [94mLoss[0m : 10.39579
[1mStep[0m  [32/84], [94mLoss[0m : 10.88925
[1mStep[0m  [40/84], [94mLoss[0m : 10.25261
[1mStep[0m  [48/84], [94mLoss[0m : 10.70707
[1mStep[0m  [56/84], [94mLoss[0m : 10.79239
[1mStep[0m  [64/84], [94mLoss[0m : 10.30965
[1mStep[0m  [72/84], [94mLoss[0m : 10.64965
[1mStep[0m  [80/84], [94mLoss[0m : 10.71379

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.680, [92mTest[0m: 10.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.09215
[1mStep[0m  [8/84], [94mLoss[0m : 10.84835
[1mStep[0m  [16/84], [94mLoss[0m : 10.12462
[1mStep[0m  [24/84], [94mLoss[0m : 10.68932
[1mStep[0m  [32/84], [94mLoss[0m : 10.92995
[1mStep[0m  [40/84], [94mLoss[0m : 10.41848
[1mStep[0m  [48/84], [94mLoss[0m : 10.04764
[1mStep[0m  [56/84], [94mLoss[0m : 10.35590
[1mStep[0m  [64/84], [94mLoss[0m : 10.98950
[1mStep[0m  [72/84], [94mLoss[0m : 10.28632
[1mStep[0m  [80/84], [94mLoss[0m : 10.81052

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.591, [92mTest[0m: 10.517, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.37827
[1mStep[0m  [8/84], [94mLoss[0m : 10.26170
[1mStep[0m  [16/84], [94mLoss[0m : 10.45938
[1mStep[0m  [24/84], [94mLoss[0m : 10.82736
[1mStep[0m  [32/84], [94mLoss[0m : 10.15013
[1mStep[0m  [40/84], [94mLoss[0m : 10.43382
[1mStep[0m  [48/84], [94mLoss[0m : 10.58974
[1mStep[0m  [56/84], [94mLoss[0m : 10.23662
[1mStep[0m  [64/84], [94mLoss[0m : 10.61377
[1mStep[0m  [72/84], [94mLoss[0m : 10.92302
[1mStep[0m  [80/84], [94mLoss[0m : 10.35392

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.480, [92mTest[0m: 10.379, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.28644
[1mStep[0m  [8/84], [94mLoss[0m : 10.41753
[1mStep[0m  [16/84], [94mLoss[0m : 10.29470
[1mStep[0m  [24/84], [94mLoss[0m : 10.05978
[1mStep[0m  [32/84], [94mLoss[0m : 10.53116
[1mStep[0m  [40/84], [94mLoss[0m : 10.11368
[1mStep[0m  [48/84], [94mLoss[0m : 10.35839
[1mStep[0m  [56/84], [94mLoss[0m : 10.57164
[1mStep[0m  [64/84], [94mLoss[0m : 10.01141
[1mStep[0m  [72/84], [94mLoss[0m : 10.47898
[1mStep[0m  [80/84], [94mLoss[0m : 10.19501

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.374, [92mTest[0m: 10.227, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.20811
[1mStep[0m  [8/84], [94mLoss[0m : 10.09823
[1mStep[0m  [16/84], [94mLoss[0m : 10.24722
[1mStep[0m  [24/84], [94mLoss[0m : 9.88477
[1mStep[0m  [32/84], [94mLoss[0m : 10.35873
[1mStep[0m  [40/84], [94mLoss[0m : 10.05938
[1mStep[0m  [48/84], [94mLoss[0m : 10.42125
[1mStep[0m  [56/84], [94mLoss[0m : 9.98324
[1mStep[0m  [64/84], [94mLoss[0m : 10.77693
[1mStep[0m  [72/84], [94mLoss[0m : 10.58115
[1mStep[0m  [80/84], [94mLoss[0m : 10.13405

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.262, [92mTest[0m: 10.135, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.40983
[1mStep[0m  [8/84], [94mLoss[0m : 9.87625
[1mStep[0m  [16/84], [94mLoss[0m : 10.04518
[1mStep[0m  [24/84], [94mLoss[0m : 10.11828
[1mStep[0m  [32/84], [94mLoss[0m : 10.66941
[1mStep[0m  [40/84], [94mLoss[0m : 9.73538
[1mStep[0m  [48/84], [94mLoss[0m : 10.40752
[1mStep[0m  [56/84], [94mLoss[0m : 10.00574
[1mStep[0m  [64/84], [94mLoss[0m : 10.24161
[1mStep[0m  [72/84], [94mLoss[0m : 9.57629
[1mStep[0m  [80/84], [94mLoss[0m : 9.86727

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.156, [92mTest[0m: 9.981, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.77138
[1mStep[0m  [8/84], [94mLoss[0m : 10.42222
[1mStep[0m  [16/84], [94mLoss[0m : 9.46051
[1mStep[0m  [24/84], [94mLoss[0m : 9.63985
[1mStep[0m  [32/84], [94mLoss[0m : 10.03690
[1mStep[0m  [40/84], [94mLoss[0m : 9.70542
[1mStep[0m  [48/84], [94mLoss[0m : 9.99709
[1mStep[0m  [56/84], [94mLoss[0m : 9.75396
[1mStep[0m  [64/84], [94mLoss[0m : 10.22741
[1mStep[0m  [72/84], [94mLoss[0m : 9.98088
[1mStep[0m  [80/84], [94mLoss[0m : 10.20545

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.043, [92mTest[0m: 9.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.24126
[1mStep[0m  [8/84], [94mLoss[0m : 10.13192
[1mStep[0m  [16/84], [94mLoss[0m : 9.63794
[1mStep[0m  [24/84], [94mLoss[0m : 10.16315
[1mStep[0m  [32/84], [94mLoss[0m : 10.48422
[1mStep[0m  [40/84], [94mLoss[0m : 10.24697
[1mStep[0m  [48/84], [94mLoss[0m : 9.78365
[1mStep[0m  [56/84], [94mLoss[0m : 9.81544
[1mStep[0m  [64/84], [94mLoss[0m : 9.80967
[1mStep[0m  [72/84], [94mLoss[0m : 10.04306
[1mStep[0m  [80/84], [94mLoss[0m : 9.40806

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.926, [92mTest[0m: 9.689, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.76707
[1mStep[0m  [8/84], [94mLoss[0m : 9.78271
[1mStep[0m  [16/84], [94mLoss[0m : 9.63215
[1mStep[0m  [24/84], [94mLoss[0m : 9.84737
[1mStep[0m  [32/84], [94mLoss[0m : 10.09242
[1mStep[0m  [40/84], [94mLoss[0m : 9.87177
[1mStep[0m  [48/84], [94mLoss[0m : 10.71628
[1mStep[0m  [56/84], [94mLoss[0m : 9.85905
[1mStep[0m  [64/84], [94mLoss[0m : 10.22510
[1mStep[0m  [72/84], [94mLoss[0m : 9.72791
[1mStep[0m  [80/84], [94mLoss[0m : 9.62612

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.813, [92mTest[0m: 9.537, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.91719
[1mStep[0m  [8/84], [94mLoss[0m : 9.78881
[1mStep[0m  [16/84], [94mLoss[0m : 9.66138
[1mStep[0m  [24/84], [94mLoss[0m : 9.37837
[1mStep[0m  [32/84], [94mLoss[0m : 9.59395
[1mStep[0m  [40/84], [94mLoss[0m : 9.29996
[1mStep[0m  [48/84], [94mLoss[0m : 9.78894
[1mStep[0m  [56/84], [94mLoss[0m : 9.32808
[1mStep[0m  [64/84], [94mLoss[0m : 9.82492
[1mStep[0m  [72/84], [94mLoss[0m : 10.09139
[1mStep[0m  [80/84], [94mLoss[0m : 9.56372

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.678, [92mTest[0m: 9.404, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.68607
[1mStep[0m  [8/84], [94mLoss[0m : 10.24201
[1mStep[0m  [16/84], [94mLoss[0m : 9.24711
[1mStep[0m  [24/84], [94mLoss[0m : 9.63888
[1mStep[0m  [32/84], [94mLoss[0m : 10.14826
[1mStep[0m  [40/84], [94mLoss[0m : 9.88067
[1mStep[0m  [48/84], [94mLoss[0m : 9.56148
[1mStep[0m  [56/84], [94mLoss[0m : 9.78210
[1mStep[0m  [64/84], [94mLoss[0m : 9.77710
[1mStep[0m  [72/84], [94mLoss[0m : 9.96507
[1mStep[0m  [80/84], [94mLoss[0m : 9.00642

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.551, [92mTest[0m: 9.284, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.23760
[1mStep[0m  [8/84], [94mLoss[0m : 9.27226
[1mStep[0m  [16/84], [94mLoss[0m : 9.35616
[1mStep[0m  [24/84], [94mLoss[0m : 9.11317
[1mStep[0m  [32/84], [94mLoss[0m : 9.26753
[1mStep[0m  [40/84], [94mLoss[0m : 9.06110
[1mStep[0m  [48/84], [94mLoss[0m : 9.22876
[1mStep[0m  [56/84], [94mLoss[0m : 9.57536
[1mStep[0m  [64/84], [94mLoss[0m : 8.56074
[1mStep[0m  [72/84], [94mLoss[0m : 9.35712
[1mStep[0m  [80/84], [94mLoss[0m : 9.40939

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.424, [92mTest[0m: 9.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.59332
[1mStep[0m  [8/84], [94mLoss[0m : 9.48776
[1mStep[0m  [16/84], [94mLoss[0m : 9.55916
[1mStep[0m  [24/84], [94mLoss[0m : 9.39659
[1mStep[0m  [32/84], [94mLoss[0m : 8.92372
[1mStep[0m  [40/84], [94mLoss[0m : 9.30251
[1mStep[0m  [48/84], [94mLoss[0m : 9.24866
[1mStep[0m  [56/84], [94mLoss[0m : 9.17221
[1mStep[0m  [64/84], [94mLoss[0m : 9.09752
[1mStep[0m  [72/84], [94mLoss[0m : 9.30895
[1mStep[0m  [80/84], [94mLoss[0m : 9.22121

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.291, [92mTest[0m: 8.933, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.15082
[1mStep[0m  [8/84], [94mLoss[0m : 8.62921
[1mStep[0m  [16/84], [94mLoss[0m : 9.09770
[1mStep[0m  [24/84], [94mLoss[0m : 8.84949
[1mStep[0m  [32/84], [94mLoss[0m : 8.96892
[1mStep[0m  [40/84], [94mLoss[0m : 9.16212
[1mStep[0m  [48/84], [94mLoss[0m : 9.08490
[1mStep[0m  [56/84], [94mLoss[0m : 8.82890
[1mStep[0m  [64/84], [94mLoss[0m : 8.83601
[1mStep[0m  [72/84], [94mLoss[0m : 8.85078
[1mStep[0m  [80/84], [94mLoss[0m : 9.21881

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.141, [92mTest[0m: 8.840, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.87022
[1mStep[0m  [8/84], [94mLoss[0m : 9.82049
[1mStep[0m  [16/84], [94mLoss[0m : 9.44422
[1mStep[0m  [24/84], [94mLoss[0m : 9.03404
[1mStep[0m  [32/84], [94mLoss[0m : 9.22507
[1mStep[0m  [40/84], [94mLoss[0m : 8.74505
[1mStep[0m  [48/84], [94mLoss[0m : 9.73648
[1mStep[0m  [56/84], [94mLoss[0m : 9.11189
[1mStep[0m  [64/84], [94mLoss[0m : 8.47231
[1mStep[0m  [72/84], [94mLoss[0m : 9.08176
[1mStep[0m  [80/84], [94mLoss[0m : 8.62320

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.985, [92mTest[0m: 8.611, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.88654
[1mStep[0m  [8/84], [94mLoss[0m : 8.99742
[1mStep[0m  [16/84], [94mLoss[0m : 9.12821
[1mStep[0m  [24/84], [94mLoss[0m : 8.68279
[1mStep[0m  [32/84], [94mLoss[0m : 9.06130
[1mStep[0m  [40/84], [94mLoss[0m : 9.12982
[1mStep[0m  [48/84], [94mLoss[0m : 8.33228
[1mStep[0m  [56/84], [94mLoss[0m : 8.97754
[1mStep[0m  [64/84], [94mLoss[0m : 8.68502
[1mStep[0m  [72/84], [94mLoss[0m : 8.62729
[1mStep[0m  [80/84], [94mLoss[0m : 9.00350

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.817, [92mTest[0m: 8.350, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.74941
[1mStep[0m  [8/84], [94mLoss[0m : 8.92966
[1mStep[0m  [16/84], [94mLoss[0m : 8.76145
[1mStep[0m  [24/84], [94mLoss[0m : 8.59764
[1mStep[0m  [32/84], [94mLoss[0m : 8.68480
[1mStep[0m  [40/84], [94mLoss[0m : 8.26353
[1mStep[0m  [48/84], [94mLoss[0m : 8.13579
[1mStep[0m  [56/84], [94mLoss[0m : 8.78865
[1mStep[0m  [64/84], [94mLoss[0m : 8.77983
[1mStep[0m  [72/84], [94mLoss[0m : 9.22828
[1mStep[0m  [80/84], [94mLoss[0m : 8.24706

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.644, [92mTest[0m: 8.158, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.38861
[1mStep[0m  [8/84], [94mLoss[0m : 8.49081
[1mStep[0m  [16/84], [94mLoss[0m : 8.33039
[1mStep[0m  [24/84], [94mLoss[0m : 8.75098
[1mStep[0m  [32/84], [94mLoss[0m : 8.23644
[1mStep[0m  [40/84], [94mLoss[0m : 9.01110
[1mStep[0m  [48/84], [94mLoss[0m : 8.24196
[1mStep[0m  [56/84], [94mLoss[0m : 8.88181
[1mStep[0m  [64/84], [94mLoss[0m : 8.24651
[1mStep[0m  [72/84], [94mLoss[0m : 9.13466
[1mStep[0m  [80/84], [94mLoss[0m : 8.61446

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.480, [92mTest[0m: 8.053, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.31769
[1mStep[0m  [8/84], [94mLoss[0m : 8.97183
[1mStep[0m  [16/84], [94mLoss[0m : 8.34205
[1mStep[0m  [24/84], [94mLoss[0m : 8.33396
[1mStep[0m  [32/84], [94mLoss[0m : 8.40945
[1mStep[0m  [40/84], [94mLoss[0m : 7.79218
[1mStep[0m  [48/84], [94mLoss[0m : 8.93180
[1mStep[0m  [56/84], [94mLoss[0m : 7.92522
[1mStep[0m  [64/84], [94mLoss[0m : 8.63427
[1mStep[0m  [72/84], [94mLoss[0m : 8.02959
[1mStep[0m  [80/84], [94mLoss[0m : 8.30558

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.287, [92mTest[0m: 7.666, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.26015
[1mStep[0m  [8/84], [94mLoss[0m : 8.46583
[1mStep[0m  [16/84], [94mLoss[0m : 8.71387
[1mStep[0m  [24/84], [94mLoss[0m : 8.64897
[1mStep[0m  [32/84], [94mLoss[0m : 8.06357
[1mStep[0m  [40/84], [94mLoss[0m : 8.54374
[1mStep[0m  [48/84], [94mLoss[0m : 8.09748
[1mStep[0m  [56/84], [94mLoss[0m : 8.18741
[1mStep[0m  [64/84], [94mLoss[0m : 8.24074
[1mStep[0m  [72/84], [94mLoss[0m : 8.14513
[1mStep[0m  [80/84], [94mLoss[0m : 7.22670

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.119, [92mTest[0m: 7.477, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.93905
[1mStep[0m  [8/84], [94mLoss[0m : 7.70036
[1mStep[0m  [16/84], [94mLoss[0m : 7.90904
[1mStep[0m  [24/84], [94mLoss[0m : 7.81898
[1mStep[0m  [32/84], [94mLoss[0m : 7.77269
[1mStep[0m  [40/84], [94mLoss[0m : 7.89311
[1mStep[0m  [48/84], [94mLoss[0m : 8.01168
[1mStep[0m  [56/84], [94mLoss[0m : 8.26948
[1mStep[0m  [64/84], [94mLoss[0m : 7.91221
[1mStep[0m  [72/84], [94mLoss[0m : 7.63804
[1mStep[0m  [80/84], [94mLoss[0m : 7.91574

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.935, [92mTest[0m: 7.256, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.82975
[1mStep[0m  [8/84], [94mLoss[0m : 7.79566
[1mStep[0m  [16/84], [94mLoss[0m : 7.84866
[1mStep[0m  [24/84], [94mLoss[0m : 7.58127
[1mStep[0m  [32/84], [94mLoss[0m : 7.40755
[1mStep[0m  [40/84], [94mLoss[0m : 8.62691
[1mStep[0m  [48/84], [94mLoss[0m : 7.16292
[1mStep[0m  [56/84], [94mLoss[0m : 7.79340
[1mStep[0m  [64/84], [94mLoss[0m : 7.64880
[1mStep[0m  [72/84], [94mLoss[0m : 7.18857
[1mStep[0m  [80/84], [94mLoss[0m : 7.88688

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.750, [92mTest[0m: 7.142, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.79627
[1mStep[0m  [8/84], [94mLoss[0m : 7.55029
[1mStep[0m  [16/84], [94mLoss[0m : 7.24315
[1mStep[0m  [24/84], [94mLoss[0m : 7.77382
[1mStep[0m  [32/84], [94mLoss[0m : 8.03662
[1mStep[0m  [40/84], [94mLoss[0m : 7.39575
[1mStep[0m  [48/84], [94mLoss[0m : 8.09628
[1mStep[0m  [56/84], [94mLoss[0m : 7.60216
[1mStep[0m  [64/84], [94mLoss[0m : 7.29270
[1mStep[0m  [72/84], [94mLoss[0m : 7.40508
[1mStep[0m  [80/84], [94mLoss[0m : 7.38548

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.546, [92mTest[0m: 6.917, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.28011
[1mStep[0m  [8/84], [94mLoss[0m : 7.72197
[1mStep[0m  [16/84], [94mLoss[0m : 6.58226
[1mStep[0m  [24/84], [94mLoss[0m : 7.12735
[1mStep[0m  [32/84], [94mLoss[0m : 7.51409
[1mStep[0m  [40/84], [94mLoss[0m : 7.46789
[1mStep[0m  [48/84], [94mLoss[0m : 7.52309
[1mStep[0m  [56/84], [94mLoss[0m : 7.09280
[1mStep[0m  [64/84], [94mLoss[0m : 6.78925
[1mStep[0m  [72/84], [94mLoss[0m : 6.65197
[1mStep[0m  [80/84], [94mLoss[0m : 7.17197

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.367, [92mTest[0m: 6.639, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.50897
[1mStep[0m  [8/84], [94mLoss[0m : 7.06487
[1mStep[0m  [16/84], [94mLoss[0m : 7.28247
[1mStep[0m  [24/84], [94mLoss[0m : 7.22538
[1mStep[0m  [32/84], [94mLoss[0m : 7.48049
[1mStep[0m  [40/84], [94mLoss[0m : 6.94488
[1mStep[0m  [48/84], [94mLoss[0m : 6.73883
[1mStep[0m  [56/84], [94mLoss[0m : 7.66253
[1mStep[0m  [64/84], [94mLoss[0m : 7.26719
[1mStep[0m  [72/84], [94mLoss[0m : 6.89886
[1mStep[0m  [80/84], [94mLoss[0m : 6.34335

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.171, [92mTest[0m: 6.554, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.64950
[1mStep[0m  [8/84], [94mLoss[0m : 7.37583
[1mStep[0m  [16/84], [94mLoss[0m : 7.04621
[1mStep[0m  [24/84], [94mLoss[0m : 7.02829
[1mStep[0m  [32/84], [94mLoss[0m : 7.45211
[1mStep[0m  [40/84], [94mLoss[0m : 6.98388
[1mStep[0m  [48/84], [94mLoss[0m : 6.75721
[1mStep[0m  [56/84], [94mLoss[0m : 6.81436
[1mStep[0m  [64/84], [94mLoss[0m : 7.56455
[1mStep[0m  [72/84], [94mLoss[0m : 7.14800
[1mStep[0m  [80/84], [94mLoss[0m : 7.22044

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.981, [92mTest[0m: 6.249, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.12384
[1mStep[0m  [8/84], [94mLoss[0m : 7.54545
[1mStep[0m  [16/84], [94mLoss[0m : 7.06482
[1mStep[0m  [24/84], [94mLoss[0m : 7.06967
[1mStep[0m  [32/84], [94mLoss[0m : 6.94209
[1mStep[0m  [40/84], [94mLoss[0m : 6.48381
[1mStep[0m  [48/84], [94mLoss[0m : 6.24879
[1mStep[0m  [56/84], [94mLoss[0m : 6.89300
[1mStep[0m  [64/84], [94mLoss[0m : 6.57505
[1mStep[0m  [72/84], [94mLoss[0m : 5.60368
[1mStep[0m  [80/84], [94mLoss[0m : 6.78444

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.807, [92mTest[0m: 6.107, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.800
====================================

Phase 1 - Evaluation MAE:  5.799782378332956
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 6.58687
[1mStep[0m  [8/84], [94mLoss[0m : 6.19728
[1mStep[0m  [16/84], [94mLoss[0m : 6.69409
[1mStep[0m  [24/84], [94mLoss[0m : 6.77910
[1mStep[0m  [32/84], [94mLoss[0m : 6.75087
[1mStep[0m  [40/84], [94mLoss[0m : 7.19384
[1mStep[0m  [48/84], [94mLoss[0m : 6.88421
[1mStep[0m  [56/84], [94mLoss[0m : 6.51963
[1mStep[0m  [64/84], [94mLoss[0m : 6.36972
[1mStep[0m  [72/84], [94mLoss[0m : 6.93543
[1mStep[0m  [80/84], [94mLoss[0m : 7.05423

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.661, [92mTest[0m: 5.801, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.08407
[1mStep[0m  [8/84], [94mLoss[0m : 6.70772
[1mStep[0m  [16/84], [94mLoss[0m : 6.14232
[1mStep[0m  [24/84], [94mLoss[0m : 6.10322
[1mStep[0m  [32/84], [94mLoss[0m : 6.34095
[1mStep[0m  [40/84], [94mLoss[0m : 7.01958
[1mStep[0m  [48/84], [94mLoss[0m : 6.76172
[1mStep[0m  [56/84], [94mLoss[0m : 6.60535
[1mStep[0m  [64/84], [94mLoss[0m : 6.38743
[1mStep[0m  [72/84], [94mLoss[0m : 6.09954
[1mStep[0m  [80/84], [94mLoss[0m : 6.63848

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.456, [92mTest[0m: 5.769, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.55170
[1mStep[0m  [8/84], [94mLoss[0m : 6.39424
[1mStep[0m  [16/84], [94mLoss[0m : 6.26025
[1mStep[0m  [24/84], [94mLoss[0m : 6.41818
[1mStep[0m  [32/84], [94mLoss[0m : 6.96129
[1mStep[0m  [40/84], [94mLoss[0m : 6.13735
[1mStep[0m  [48/84], [94mLoss[0m : 6.10720
[1mStep[0m  [56/84], [94mLoss[0m : 6.29889
[1mStep[0m  [64/84], [94mLoss[0m : 6.11053
[1mStep[0m  [72/84], [94mLoss[0m : 5.95017
[1mStep[0m  [80/84], [94mLoss[0m : 6.35378

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.242, [92mTest[0m: 5.888, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.26686
[1mStep[0m  [8/84], [94mLoss[0m : 6.25668
[1mStep[0m  [16/84], [94mLoss[0m : 6.34789
[1mStep[0m  [24/84], [94mLoss[0m : 5.60414
[1mStep[0m  [32/84], [94mLoss[0m : 6.21041
[1mStep[0m  [40/84], [94mLoss[0m : 6.30877
[1mStep[0m  [48/84], [94mLoss[0m : 5.83927
[1mStep[0m  [56/84], [94mLoss[0m : 6.23341
[1mStep[0m  [64/84], [94mLoss[0m : 6.09823
[1mStep[0m  [72/84], [94mLoss[0m : 5.80499
[1mStep[0m  [80/84], [94mLoss[0m : 6.02287

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.026, [92mTest[0m: 5.721, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.71117
[1mStep[0m  [8/84], [94mLoss[0m : 5.51819
[1mStep[0m  [16/84], [94mLoss[0m : 6.24802
[1mStep[0m  [24/84], [94mLoss[0m : 6.03861
[1mStep[0m  [32/84], [94mLoss[0m : 6.10410
[1mStep[0m  [40/84], [94mLoss[0m : 6.28744
[1mStep[0m  [48/84], [94mLoss[0m : 5.70779
[1mStep[0m  [56/84], [94mLoss[0m : 5.73505
[1mStep[0m  [64/84], [94mLoss[0m : 6.07887
[1mStep[0m  [72/84], [94mLoss[0m : 5.95533
[1mStep[0m  [80/84], [94mLoss[0m : 5.58613

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.822, [92mTest[0m: 5.934, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.78363
[1mStep[0m  [8/84], [94mLoss[0m : 5.44069
[1mStep[0m  [16/84], [94mLoss[0m : 5.56966
[1mStep[0m  [24/84], [94mLoss[0m : 6.10660
[1mStep[0m  [32/84], [94mLoss[0m : 5.83116
[1mStep[0m  [40/84], [94mLoss[0m : 5.40221
[1mStep[0m  [48/84], [94mLoss[0m : 5.69816
[1mStep[0m  [56/84], [94mLoss[0m : 5.64337
[1mStep[0m  [64/84], [94mLoss[0m : 5.12357
[1mStep[0m  [72/84], [94mLoss[0m : 5.82633
[1mStep[0m  [80/84], [94mLoss[0m : 5.24299

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.625, [92mTest[0m: 5.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.30938
[1mStep[0m  [8/84], [94mLoss[0m : 5.12254
[1mStep[0m  [16/84], [94mLoss[0m : 5.23149
[1mStep[0m  [24/84], [94mLoss[0m : 5.97793
[1mStep[0m  [32/84], [94mLoss[0m : 5.50158
[1mStep[0m  [40/84], [94mLoss[0m : 5.70154
[1mStep[0m  [48/84], [94mLoss[0m : 5.84923
[1mStep[0m  [56/84], [94mLoss[0m : 5.72073
[1mStep[0m  [64/84], [94mLoss[0m : 5.43329
[1mStep[0m  [72/84], [94mLoss[0m : 4.76207
[1mStep[0m  [80/84], [94mLoss[0m : 5.75345

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.422, [92mTest[0m: 5.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.54826
[1mStep[0m  [8/84], [94mLoss[0m : 5.38293
[1mStep[0m  [16/84], [94mLoss[0m : 5.32563
[1mStep[0m  [24/84], [94mLoss[0m : 5.26249
[1mStep[0m  [32/84], [94mLoss[0m : 5.28819
[1mStep[0m  [40/84], [94mLoss[0m : 5.24379
[1mStep[0m  [48/84], [94mLoss[0m : 5.20399
[1mStep[0m  [56/84], [94mLoss[0m : 5.19590
[1mStep[0m  [64/84], [94mLoss[0m : 5.10679
[1mStep[0m  [72/84], [94mLoss[0m : 5.05598
[1mStep[0m  [80/84], [94mLoss[0m : 4.89657

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.207, [92mTest[0m: 5.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.89695
[1mStep[0m  [8/84], [94mLoss[0m : 5.25254
[1mStep[0m  [16/84], [94mLoss[0m : 4.85763
[1mStep[0m  [24/84], [94mLoss[0m : 4.53103
[1mStep[0m  [32/84], [94mLoss[0m : 4.63750
[1mStep[0m  [40/84], [94mLoss[0m : 5.00094
[1mStep[0m  [48/84], [94mLoss[0m : 4.95166
[1mStep[0m  [56/84], [94mLoss[0m : 4.79248
[1mStep[0m  [64/84], [94mLoss[0m : 4.69777
[1mStep[0m  [72/84], [94mLoss[0m : 5.08670
[1mStep[0m  [80/84], [94mLoss[0m : 4.83365

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.016, [92mTest[0m: 5.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.92271
[1mStep[0m  [8/84], [94mLoss[0m : 4.76754
[1mStep[0m  [16/84], [94mLoss[0m : 5.18621
[1mStep[0m  [24/84], [94mLoss[0m : 4.69971
[1mStep[0m  [32/84], [94mLoss[0m : 5.48750
[1mStep[0m  [40/84], [94mLoss[0m : 4.98307
[1mStep[0m  [48/84], [94mLoss[0m : 4.89322
[1mStep[0m  [56/84], [94mLoss[0m : 4.54944
[1mStep[0m  [64/84], [94mLoss[0m : 5.33394
[1mStep[0m  [72/84], [94mLoss[0m : 4.96147
[1mStep[0m  [80/84], [94mLoss[0m : 4.54374

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.874, [92mTest[0m: 4.813, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.38712
[1mStep[0m  [8/84], [94mLoss[0m : 4.68285
[1mStep[0m  [16/84], [94mLoss[0m : 4.38037
[1mStep[0m  [24/84], [94mLoss[0m : 4.78367
[1mStep[0m  [32/84], [94mLoss[0m : 4.58274
[1mStep[0m  [40/84], [94mLoss[0m : 4.70601
[1mStep[0m  [48/84], [94mLoss[0m : 4.66245
[1mStep[0m  [56/84], [94mLoss[0m : 5.13836
[1mStep[0m  [64/84], [94mLoss[0m : 4.54882
[1mStep[0m  [72/84], [94mLoss[0m : 4.38266
[1mStep[0m  [80/84], [94mLoss[0m : 5.02774

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.658, [92mTest[0m: 4.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.93465
[1mStep[0m  [8/84], [94mLoss[0m : 4.71440
[1mStep[0m  [16/84], [94mLoss[0m : 4.57274
[1mStep[0m  [24/84], [94mLoss[0m : 4.67701
[1mStep[0m  [32/84], [94mLoss[0m : 4.34738
[1mStep[0m  [40/84], [94mLoss[0m : 4.64962
[1mStep[0m  [48/84], [94mLoss[0m : 4.39348
[1mStep[0m  [56/84], [94mLoss[0m : 4.10414
[1mStep[0m  [64/84], [94mLoss[0m : 4.50050
[1mStep[0m  [72/84], [94mLoss[0m : 4.36458
[1mStep[0m  [80/84], [94mLoss[0m : 4.74751

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.533, [92mTest[0m: 4.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.23247
[1mStep[0m  [8/84], [94mLoss[0m : 4.22290
[1mStep[0m  [16/84], [94mLoss[0m : 4.85258
[1mStep[0m  [24/84], [94mLoss[0m : 4.36675
[1mStep[0m  [32/84], [94mLoss[0m : 4.36891
[1mStep[0m  [40/84], [94mLoss[0m : 4.46430
[1mStep[0m  [48/84], [94mLoss[0m : 4.45611
[1mStep[0m  [56/84], [94mLoss[0m : 4.65611
[1mStep[0m  [64/84], [94mLoss[0m : 4.84246
[1mStep[0m  [72/84], [94mLoss[0m : 4.34111
[1mStep[0m  [80/84], [94mLoss[0m : 4.14670

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.360, [92mTest[0m: 4.382, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.27872
[1mStep[0m  [8/84], [94mLoss[0m : 4.60519
[1mStep[0m  [16/84], [94mLoss[0m : 4.48114
[1mStep[0m  [24/84], [94mLoss[0m : 4.17249
[1mStep[0m  [32/84], [94mLoss[0m : 4.64330
[1mStep[0m  [40/84], [94mLoss[0m : 4.13094
[1mStep[0m  [48/84], [94mLoss[0m : 3.85906
[1mStep[0m  [56/84], [94mLoss[0m : 4.03237
[1mStep[0m  [64/84], [94mLoss[0m : 4.14471
[1mStep[0m  [72/84], [94mLoss[0m : 4.06512
[1mStep[0m  [80/84], [94mLoss[0m : 3.91985

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.222, [92mTest[0m: 4.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.36524
[1mStep[0m  [8/84], [94mLoss[0m : 3.46838
[1mStep[0m  [16/84], [94mLoss[0m : 4.19929
[1mStep[0m  [24/84], [94mLoss[0m : 4.41105
[1mStep[0m  [32/84], [94mLoss[0m : 4.19856
[1mStep[0m  [40/84], [94mLoss[0m : 3.96408
[1mStep[0m  [48/84], [94mLoss[0m : 3.75096
[1mStep[0m  [56/84], [94mLoss[0m : 4.54746
[1mStep[0m  [64/84], [94mLoss[0m : 4.57695
[1mStep[0m  [72/84], [94mLoss[0m : 4.16796
[1mStep[0m  [80/84], [94mLoss[0m : 3.72730

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.076, [92mTest[0m: 4.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.12668
[1mStep[0m  [8/84], [94mLoss[0m : 4.02725
[1mStep[0m  [16/84], [94mLoss[0m : 4.31035
[1mStep[0m  [24/84], [94mLoss[0m : 3.83262
[1mStep[0m  [32/84], [94mLoss[0m : 3.61133
[1mStep[0m  [40/84], [94mLoss[0m : 3.88530
[1mStep[0m  [48/84], [94mLoss[0m : 3.92453
[1mStep[0m  [56/84], [94mLoss[0m : 4.02355
[1mStep[0m  [64/84], [94mLoss[0m : 3.68672
[1mStep[0m  [72/84], [94mLoss[0m : 4.28674
[1mStep[0m  [80/84], [94mLoss[0m : 3.83030

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.961, [92mTest[0m: 3.915, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.85948
[1mStep[0m  [8/84], [94mLoss[0m : 4.05276
[1mStep[0m  [16/84], [94mLoss[0m : 4.06830
[1mStep[0m  [24/84], [94mLoss[0m : 4.00109
[1mStep[0m  [32/84], [94mLoss[0m : 3.86712
[1mStep[0m  [40/84], [94mLoss[0m : 3.88807
[1mStep[0m  [48/84], [94mLoss[0m : 3.83579
[1mStep[0m  [56/84], [94mLoss[0m : 3.96444
[1mStep[0m  [64/84], [94mLoss[0m : 3.64810
[1mStep[0m  [72/84], [94mLoss[0m : 3.63807
[1mStep[0m  [80/84], [94mLoss[0m : 3.85152

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.894, [92mTest[0m: 3.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.61528
[1mStep[0m  [8/84], [94mLoss[0m : 3.93670
[1mStep[0m  [16/84], [94mLoss[0m : 3.79035
[1mStep[0m  [24/84], [94mLoss[0m : 3.37106
[1mStep[0m  [32/84], [94mLoss[0m : 3.53598
[1mStep[0m  [40/84], [94mLoss[0m : 3.71273
[1mStep[0m  [48/84], [94mLoss[0m : 4.15736
[1mStep[0m  [56/84], [94mLoss[0m : 3.86858
[1mStep[0m  [64/84], [94mLoss[0m : 3.81154
[1mStep[0m  [72/84], [94mLoss[0m : 3.69134
[1mStep[0m  [80/84], [94mLoss[0m : 3.83098

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.740, [92mTest[0m: 3.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.27588
[1mStep[0m  [8/84], [94mLoss[0m : 3.77751
[1mStep[0m  [16/84], [94mLoss[0m : 3.81234
[1mStep[0m  [24/84], [94mLoss[0m : 3.40552
[1mStep[0m  [32/84], [94mLoss[0m : 3.55305
[1mStep[0m  [40/84], [94mLoss[0m : 3.97336
[1mStep[0m  [48/84], [94mLoss[0m : 3.55713
[1mStep[0m  [56/84], [94mLoss[0m : 3.88367
[1mStep[0m  [64/84], [94mLoss[0m : 3.34106
[1mStep[0m  [72/84], [94mLoss[0m : 3.78300
[1mStep[0m  [80/84], [94mLoss[0m : 4.12219

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.673, [92mTest[0m: 3.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.73960
[1mStep[0m  [8/84], [94mLoss[0m : 3.48081
[1mStep[0m  [16/84], [94mLoss[0m : 3.98466
[1mStep[0m  [24/84], [94mLoss[0m : 3.25221
[1mStep[0m  [32/84], [94mLoss[0m : 3.57941
[1mStep[0m  [40/84], [94mLoss[0m : 3.32933
[1mStep[0m  [48/84], [94mLoss[0m : 3.84437
[1mStep[0m  [56/84], [94mLoss[0m : 3.28651
[1mStep[0m  [64/84], [94mLoss[0m : 3.46423
[1mStep[0m  [72/84], [94mLoss[0m : 3.81421
[1mStep[0m  [80/84], [94mLoss[0m : 3.69204

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.561, [92mTest[0m: 3.274, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.43576
[1mStep[0m  [8/84], [94mLoss[0m : 3.32534
[1mStep[0m  [16/84], [94mLoss[0m : 3.29065
[1mStep[0m  [24/84], [94mLoss[0m : 3.55305
[1mStep[0m  [32/84], [94mLoss[0m : 3.57061
[1mStep[0m  [40/84], [94mLoss[0m : 3.51669
[1mStep[0m  [48/84], [94mLoss[0m : 3.53095
[1mStep[0m  [56/84], [94mLoss[0m : 3.27676
[1mStep[0m  [64/84], [94mLoss[0m : 3.00277
[1mStep[0m  [72/84], [94mLoss[0m : 3.51122
[1mStep[0m  [80/84], [94mLoss[0m : 3.46742

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.501, [92mTest[0m: 3.242, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.35608
[1mStep[0m  [8/84], [94mLoss[0m : 3.19321
[1mStep[0m  [16/84], [94mLoss[0m : 3.62194
[1mStep[0m  [24/84], [94mLoss[0m : 3.77977
[1mStep[0m  [32/84], [94mLoss[0m : 3.47377
[1mStep[0m  [40/84], [94mLoss[0m : 2.98553
[1mStep[0m  [48/84], [94mLoss[0m : 3.17355
[1mStep[0m  [56/84], [94mLoss[0m : 3.18211
[1mStep[0m  [64/84], [94mLoss[0m : 3.47207
[1mStep[0m  [72/84], [94mLoss[0m : 3.32417
[1mStep[0m  [80/84], [94mLoss[0m : 3.14124

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.428, [92mTest[0m: 3.165, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.06827
[1mStep[0m  [8/84], [94mLoss[0m : 3.26290
[1mStep[0m  [16/84], [94mLoss[0m : 3.64866
[1mStep[0m  [24/84], [94mLoss[0m : 3.20632
[1mStep[0m  [32/84], [94mLoss[0m : 3.61122
[1mStep[0m  [40/84], [94mLoss[0m : 3.77283
[1mStep[0m  [48/84], [94mLoss[0m : 3.57774
[1mStep[0m  [56/84], [94mLoss[0m : 3.50503
[1mStep[0m  [64/84], [94mLoss[0m : 3.62391
[1mStep[0m  [72/84], [94mLoss[0m : 3.51758
[1mStep[0m  [80/84], [94mLoss[0m : 3.46424

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.379, [92mTest[0m: 3.158, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.15848
[1mStep[0m  [8/84], [94mLoss[0m : 3.58714
[1mStep[0m  [16/84], [94mLoss[0m : 3.56772
[1mStep[0m  [24/84], [94mLoss[0m : 3.19639
[1mStep[0m  [32/84], [94mLoss[0m : 3.11761
[1mStep[0m  [40/84], [94mLoss[0m : 3.25130
[1mStep[0m  [48/84], [94mLoss[0m : 3.10025
[1mStep[0m  [56/84], [94mLoss[0m : 2.70723
[1mStep[0m  [64/84], [94mLoss[0m : 3.74811
[1mStep[0m  [72/84], [94mLoss[0m : 2.78327
[1mStep[0m  [80/84], [94mLoss[0m : 2.95892

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.307, [92mTest[0m: 3.043, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.38049
[1mStep[0m  [8/84], [94mLoss[0m : 3.28294
[1mStep[0m  [16/84], [94mLoss[0m : 2.92434
[1mStep[0m  [24/84], [94mLoss[0m : 3.45192
[1mStep[0m  [32/84], [94mLoss[0m : 3.29786
[1mStep[0m  [40/84], [94mLoss[0m : 3.21073
[1mStep[0m  [48/84], [94mLoss[0m : 3.06258
[1mStep[0m  [56/84], [94mLoss[0m : 3.36958
[1mStep[0m  [64/84], [94mLoss[0m : 3.21758
[1mStep[0m  [72/84], [94mLoss[0m : 3.04938
[1mStep[0m  [80/84], [94mLoss[0m : 3.15894

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.275, [92mTest[0m: 2.947, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.91732
[1mStep[0m  [8/84], [94mLoss[0m : 3.56224
[1mStep[0m  [16/84], [94mLoss[0m : 3.22728
[1mStep[0m  [24/84], [94mLoss[0m : 3.08021
[1mStep[0m  [32/84], [94mLoss[0m : 3.45477
[1mStep[0m  [40/84], [94mLoss[0m : 3.57961
[1mStep[0m  [48/84], [94mLoss[0m : 3.31463
[1mStep[0m  [56/84], [94mLoss[0m : 3.34119
[1mStep[0m  [64/84], [94mLoss[0m : 3.48376
[1mStep[0m  [72/84], [94mLoss[0m : 3.19851
[1mStep[0m  [80/84], [94mLoss[0m : 3.10212

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.214, [92mTest[0m: 3.003, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.41557
[1mStep[0m  [8/84], [94mLoss[0m : 3.37069
[1mStep[0m  [16/84], [94mLoss[0m : 2.76900
[1mStep[0m  [24/84], [94mLoss[0m : 3.41038
[1mStep[0m  [32/84], [94mLoss[0m : 3.40449
[1mStep[0m  [40/84], [94mLoss[0m : 2.88053
[1mStep[0m  [48/84], [94mLoss[0m : 3.31328
[1mStep[0m  [56/84], [94mLoss[0m : 3.12421
[1mStep[0m  [64/84], [94mLoss[0m : 3.04559
[1mStep[0m  [72/84], [94mLoss[0m : 2.88422
[1mStep[0m  [80/84], [94mLoss[0m : 3.05876

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.169, [92mTest[0m: 3.008, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.43635
[1mStep[0m  [8/84], [94mLoss[0m : 3.05365
[1mStep[0m  [16/84], [94mLoss[0m : 3.04463
[1mStep[0m  [24/84], [94mLoss[0m : 3.30664
[1mStep[0m  [32/84], [94mLoss[0m : 2.90782
[1mStep[0m  [40/84], [94mLoss[0m : 3.17303
[1mStep[0m  [48/84], [94mLoss[0m : 3.01553
[1mStep[0m  [56/84], [94mLoss[0m : 3.72102
[1mStep[0m  [64/84], [94mLoss[0m : 3.35571
[1mStep[0m  [72/84], [94mLoss[0m : 3.39525
[1mStep[0m  [80/84], [94mLoss[0m : 3.18095

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.161, [92mTest[0m: 2.880, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.10616
[1mStep[0m  [8/84], [94mLoss[0m : 2.88025
[1mStep[0m  [16/84], [94mLoss[0m : 2.74108
[1mStep[0m  [24/84], [94mLoss[0m : 2.78417
[1mStep[0m  [32/84], [94mLoss[0m : 2.91925
[1mStep[0m  [40/84], [94mLoss[0m : 3.05666
[1mStep[0m  [48/84], [94mLoss[0m : 3.30637
[1mStep[0m  [56/84], [94mLoss[0m : 3.27995
[1mStep[0m  [64/84], [94mLoss[0m : 2.96380
[1mStep[0m  [72/84], [94mLoss[0m : 2.94733
[1mStep[0m  [80/84], [94mLoss[0m : 3.42877

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.097, [92mTest[0m: 2.892, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.19646
[1mStep[0m  [8/84], [94mLoss[0m : 3.45456
[1mStep[0m  [16/84], [94mLoss[0m : 3.13939
[1mStep[0m  [24/84], [94mLoss[0m : 2.89851
[1mStep[0m  [32/84], [94mLoss[0m : 3.28633
[1mStep[0m  [40/84], [94mLoss[0m : 2.87883
[1mStep[0m  [48/84], [94mLoss[0m : 3.27771
[1mStep[0m  [56/84], [94mLoss[0m : 2.76673
[1mStep[0m  [64/84], [94mLoss[0m : 2.94479
[1mStep[0m  [72/84], [94mLoss[0m : 2.88241
[1mStep[0m  [80/84], [94mLoss[0m : 3.20472

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.095, [92mTest[0m: 2.832, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.737
====================================

Phase 2 - Evaluation MAE:  2.7369053789547513
MAE score P1       5.799782
MAE score P2       2.736905
loss               3.094649
learning_rate        0.0001
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.06357
[1mStep[0m  [8/84], [94mLoss[0m : 11.13148
[1mStep[0m  [16/84], [94mLoss[0m : 11.35227
[1mStep[0m  [24/84], [94mLoss[0m : 10.98086
[1mStep[0m  [32/84], [94mLoss[0m : 10.95310
[1mStep[0m  [40/84], [94mLoss[0m : 10.66480
[1mStep[0m  [48/84], [94mLoss[0m : 10.82100
[1mStep[0m  [56/84], [94mLoss[0m : 10.79339
[1mStep[0m  [64/84], [94mLoss[0m : 10.97640
[1mStep[0m  [72/84], [94mLoss[0m : 11.00817
[1mStep[0m  [80/84], [94mLoss[0m : 11.17084

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.925, [92mTest[0m: 10.932, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.66741
[1mStep[0m  [8/84], [94mLoss[0m : 10.78362
[1mStep[0m  [16/84], [94mLoss[0m : 10.68186
[1mStep[0m  [24/84], [94mLoss[0m : 10.68680
[1mStep[0m  [32/84], [94mLoss[0m : 11.00209
[1mStep[0m  [40/84], [94mLoss[0m : 11.04595
[1mStep[0m  [48/84], [94mLoss[0m : 11.49439
[1mStep[0m  [56/84], [94mLoss[0m : 11.00250
[1mStep[0m  [64/84], [94mLoss[0m : 10.76486
[1mStep[0m  [72/84], [94mLoss[0m : 11.40621
[1mStep[0m  [80/84], [94mLoss[0m : 10.71526

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.923, [92mTest[0m: 10.929, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.10950
[1mStep[0m  [8/84], [94mLoss[0m : 10.53372
[1mStep[0m  [16/84], [94mLoss[0m : 11.47179
[1mStep[0m  [24/84], [94mLoss[0m : 10.90051
[1mStep[0m  [32/84], [94mLoss[0m : 10.66479
[1mStep[0m  [40/84], [94mLoss[0m : 10.94294
[1mStep[0m  [48/84], [94mLoss[0m : 10.53426
[1mStep[0m  [56/84], [94mLoss[0m : 10.98700
[1mStep[0m  [64/84], [94mLoss[0m : 10.95760
[1mStep[0m  [72/84], [94mLoss[0m : 10.91599
[1mStep[0m  [80/84], [94mLoss[0m : 11.12462

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.898, [92mTest[0m: 10.914, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.90157
[1mStep[0m  [8/84], [94mLoss[0m : 10.98578
[1mStep[0m  [16/84], [94mLoss[0m : 10.53299
[1mStep[0m  [24/84], [94mLoss[0m : 10.48298
[1mStep[0m  [32/84], [94mLoss[0m : 10.87040
[1mStep[0m  [40/84], [94mLoss[0m : 11.22526
[1mStep[0m  [48/84], [94mLoss[0m : 10.75164
[1mStep[0m  [56/84], [94mLoss[0m : 10.39329
[1mStep[0m  [64/84], [94mLoss[0m : 11.14376
[1mStep[0m  [72/84], [94mLoss[0m : 11.04213
[1mStep[0m  [80/84], [94mLoss[0m : 10.42696

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.895, [92mTest[0m: 10.910, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.64832
[1mStep[0m  [8/84], [94mLoss[0m : 11.33966
[1mStep[0m  [16/84], [94mLoss[0m : 10.18807
[1mStep[0m  [24/84], [94mLoss[0m : 10.77336
[1mStep[0m  [32/84], [94mLoss[0m : 11.17525
[1mStep[0m  [40/84], [94mLoss[0m : 11.13083
[1mStep[0m  [48/84], [94mLoss[0m : 10.59555
[1mStep[0m  [56/84], [94mLoss[0m : 11.09813
[1mStep[0m  [64/84], [94mLoss[0m : 10.73308
[1mStep[0m  [72/84], [94mLoss[0m : 10.96657
[1mStep[0m  [80/84], [94mLoss[0m : 11.00508

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.870, [92mTest[0m: 10.875, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.13420
[1mStep[0m  [8/84], [94mLoss[0m : 11.23216
[1mStep[0m  [16/84], [94mLoss[0m : 10.20739
[1mStep[0m  [24/84], [94mLoss[0m : 11.12191
[1mStep[0m  [32/84], [94mLoss[0m : 10.62341
[1mStep[0m  [40/84], [94mLoss[0m : 10.75669
[1mStep[0m  [48/84], [94mLoss[0m : 10.49272
[1mStep[0m  [56/84], [94mLoss[0m : 10.35273
[1mStep[0m  [64/84], [94mLoss[0m : 10.93078
[1mStep[0m  [72/84], [94mLoss[0m : 10.95936
[1mStep[0m  [80/84], [94mLoss[0m : 11.05593

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.854, [92mTest[0m: 10.854, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.03803
[1mStep[0m  [8/84], [94mLoss[0m : 11.34807
[1mStep[0m  [16/84], [94mLoss[0m : 10.49315
[1mStep[0m  [24/84], [94mLoss[0m : 11.12210
[1mStep[0m  [32/84], [94mLoss[0m : 10.44984
[1mStep[0m  [40/84], [94mLoss[0m : 10.50295
[1mStep[0m  [48/84], [94mLoss[0m : 11.19804
[1mStep[0m  [56/84], [94mLoss[0m : 11.18197
[1mStep[0m  [64/84], [94mLoss[0m : 10.47350
[1mStep[0m  [72/84], [94mLoss[0m : 10.86031
[1mStep[0m  [80/84], [94mLoss[0m : 11.29936

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.840, [92mTest[0m: 10.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.21389
[1mStep[0m  [8/84], [94mLoss[0m : 10.92468
[1mStep[0m  [16/84], [94mLoss[0m : 10.50036
[1mStep[0m  [24/84], [94mLoss[0m : 10.96323
[1mStep[0m  [32/84], [94mLoss[0m : 10.64156
[1mStep[0m  [40/84], [94mLoss[0m : 10.33478
[1mStep[0m  [48/84], [94mLoss[0m : 10.52992
[1mStep[0m  [56/84], [94mLoss[0m : 10.68013
[1mStep[0m  [64/84], [94mLoss[0m : 10.57247
[1mStep[0m  [72/84], [94mLoss[0m : 10.67734
[1mStep[0m  [80/84], [94mLoss[0m : 10.64629

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.44732
[1mStep[0m  [8/84], [94mLoss[0m : 10.65309
[1mStep[0m  [16/84], [94mLoss[0m : 11.22198
[1mStep[0m  [24/84], [94mLoss[0m : 10.90117
[1mStep[0m  [32/84], [94mLoss[0m : 10.55993
[1mStep[0m  [40/84], [94mLoss[0m : 10.52979
[1mStep[0m  [48/84], [94mLoss[0m : 10.43416
[1mStep[0m  [56/84], [94mLoss[0m : 11.18901
[1mStep[0m  [64/84], [94mLoss[0m : 10.89982
[1mStep[0m  [72/84], [94mLoss[0m : 10.40298
[1mStep[0m  [80/84], [94mLoss[0m : 10.75618

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.818, [92mTest[0m: 10.781, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.00219
[1mStep[0m  [8/84], [94mLoss[0m : 10.86751
[1mStep[0m  [16/84], [94mLoss[0m : 10.04352
[1mStep[0m  [24/84], [94mLoss[0m : 10.68919
[1mStep[0m  [32/84], [94mLoss[0m : 10.31577
[1mStep[0m  [40/84], [94mLoss[0m : 10.65163
[1mStep[0m  [48/84], [94mLoss[0m : 10.73968
[1mStep[0m  [56/84], [94mLoss[0m : 10.96237
[1mStep[0m  [64/84], [94mLoss[0m : 11.63028
[1mStep[0m  [72/84], [94mLoss[0m : 10.57355
[1mStep[0m  [80/84], [94mLoss[0m : 11.47324

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.794, [92mTest[0m: 10.769, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.16655
[1mStep[0m  [8/84], [94mLoss[0m : 10.85246
[1mStep[0m  [16/84], [94mLoss[0m : 10.82447
[1mStep[0m  [24/84], [94mLoss[0m : 10.77007
[1mStep[0m  [32/84], [94mLoss[0m : 10.98096
[1mStep[0m  [40/84], [94mLoss[0m : 11.21898
[1mStep[0m  [48/84], [94mLoss[0m : 10.83923
[1mStep[0m  [56/84], [94mLoss[0m : 10.59872
[1mStep[0m  [64/84], [94mLoss[0m : 11.03060
[1mStep[0m  [72/84], [94mLoss[0m : 10.52221
[1mStep[0m  [80/84], [94mLoss[0m : 10.35636

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.780, [92mTest[0m: 10.732, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.51003
[1mStep[0m  [8/84], [94mLoss[0m : 10.82743
[1mStep[0m  [16/84], [94mLoss[0m : 10.16280
[1mStep[0m  [24/84], [94mLoss[0m : 10.66222
[1mStep[0m  [32/84], [94mLoss[0m : 10.35751
[1mStep[0m  [40/84], [94mLoss[0m : 10.97722
[1mStep[0m  [48/84], [94mLoss[0m : 10.68647
[1mStep[0m  [56/84], [94mLoss[0m : 10.96952
[1mStep[0m  [64/84], [94mLoss[0m : 10.35190
[1mStep[0m  [72/84], [94mLoss[0m : 10.64090
[1mStep[0m  [80/84], [94mLoss[0m : 11.31385

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.766, [92mTest[0m: 10.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.97589
[1mStep[0m  [8/84], [94mLoss[0m : 10.21139
[1mStep[0m  [16/84], [94mLoss[0m : 11.20100
[1mStep[0m  [24/84], [94mLoss[0m : 10.26886
[1mStep[0m  [32/84], [94mLoss[0m : 10.28531
[1mStep[0m  [40/84], [94mLoss[0m : 11.04382
[1mStep[0m  [48/84], [94mLoss[0m : 10.66089
[1mStep[0m  [56/84], [94mLoss[0m : 11.05087
[1mStep[0m  [64/84], [94mLoss[0m : 10.94158
[1mStep[0m  [72/84], [94mLoss[0m : 10.53192
[1mStep[0m  [80/84], [94mLoss[0m : 10.92599

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.759, [92mTest[0m: 10.721, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.07709
[1mStep[0m  [8/84], [94mLoss[0m : 10.30220
[1mStep[0m  [16/84], [94mLoss[0m : 10.12300
[1mStep[0m  [24/84], [94mLoss[0m : 10.93993
[1mStep[0m  [32/84], [94mLoss[0m : 10.94865
[1mStep[0m  [40/84], [94mLoss[0m : 10.72611
[1mStep[0m  [48/84], [94mLoss[0m : 10.73319
[1mStep[0m  [56/84], [94mLoss[0m : 10.27547
[1mStep[0m  [64/84], [94mLoss[0m : 10.34365
[1mStep[0m  [72/84], [94mLoss[0m : 11.01272
[1mStep[0m  [80/84], [94mLoss[0m : 10.98263

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.735, [92mTest[0m: 10.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.98375
[1mStep[0m  [8/84], [94mLoss[0m : 11.08441
[1mStep[0m  [16/84], [94mLoss[0m : 10.92941
[1mStep[0m  [24/84], [94mLoss[0m : 10.52663
[1mStep[0m  [32/84], [94mLoss[0m : 10.54434
[1mStep[0m  [40/84], [94mLoss[0m : 11.20951
[1mStep[0m  [48/84], [94mLoss[0m : 10.83469
[1mStep[0m  [56/84], [94mLoss[0m : 10.25571
[1mStep[0m  [64/84], [94mLoss[0m : 10.44405
[1mStep[0m  [72/84], [94mLoss[0m : 11.16370
[1mStep[0m  [80/84], [94mLoss[0m : 10.67984

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.720, [92mTest[0m: 10.640, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.11016
[1mStep[0m  [8/84], [94mLoss[0m : 11.16361
[1mStep[0m  [16/84], [94mLoss[0m : 10.82661
[1mStep[0m  [24/84], [94mLoss[0m : 10.68705
[1mStep[0m  [32/84], [94mLoss[0m : 10.93555
[1mStep[0m  [40/84], [94mLoss[0m : 10.51786
[1mStep[0m  [48/84], [94mLoss[0m : 10.91747
[1mStep[0m  [56/84], [94mLoss[0m : 10.55195
[1mStep[0m  [64/84], [94mLoss[0m : 10.67704
[1mStep[0m  [72/84], [94mLoss[0m : 11.12647
[1mStep[0m  [80/84], [94mLoss[0m : 10.61355

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.703, [92mTest[0m: 10.650, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.32260
[1mStep[0m  [8/84], [94mLoss[0m : 10.63469
[1mStep[0m  [16/84], [94mLoss[0m : 11.25720
[1mStep[0m  [24/84], [94mLoss[0m : 10.41900
[1mStep[0m  [32/84], [94mLoss[0m : 10.70614
[1mStep[0m  [40/84], [94mLoss[0m : 10.38234
[1mStep[0m  [48/84], [94mLoss[0m : 11.13762
[1mStep[0m  [56/84], [94mLoss[0m : 11.06193
[1mStep[0m  [64/84], [94mLoss[0m : 10.67165
[1mStep[0m  [72/84], [94mLoss[0m : 10.46173
[1mStep[0m  [80/84], [94mLoss[0m : 10.15586

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.685, [92mTest[0m: 10.608, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.46308
[1mStep[0m  [8/84], [94mLoss[0m : 10.75561
[1mStep[0m  [16/84], [94mLoss[0m : 10.54810
[1mStep[0m  [24/84], [94mLoss[0m : 10.78634
[1mStep[0m  [32/84], [94mLoss[0m : 10.43822
[1mStep[0m  [40/84], [94mLoss[0m : 11.02600
[1mStep[0m  [48/84], [94mLoss[0m : 10.58221
[1mStep[0m  [56/84], [94mLoss[0m : 10.22673
[1mStep[0m  [64/84], [94mLoss[0m : 10.86096
[1mStep[0m  [72/84], [94mLoss[0m : 11.36428
[1mStep[0m  [80/84], [94mLoss[0m : 10.74475

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.679, [92mTest[0m: 10.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.57883
[1mStep[0m  [8/84], [94mLoss[0m : 10.63862
[1mStep[0m  [16/84], [94mLoss[0m : 10.48844
[1mStep[0m  [24/84], [94mLoss[0m : 10.08138
[1mStep[0m  [32/84], [94mLoss[0m : 10.44916
[1mStep[0m  [40/84], [94mLoss[0m : 10.58395
[1mStep[0m  [48/84], [94mLoss[0m : 10.40607
[1mStep[0m  [56/84], [94mLoss[0m : 10.11102
[1mStep[0m  [64/84], [94mLoss[0m : 10.41409
[1mStep[0m  [72/84], [94mLoss[0m : 11.26866
[1mStep[0m  [80/84], [94mLoss[0m : 10.28973

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.02252
[1mStep[0m  [8/84], [94mLoss[0m : 10.65989
[1mStep[0m  [16/84], [94mLoss[0m : 10.89789
[1mStep[0m  [24/84], [94mLoss[0m : 10.77651
[1mStep[0m  [32/84], [94mLoss[0m : 11.01403
[1mStep[0m  [40/84], [94mLoss[0m : 10.62411
[1mStep[0m  [48/84], [94mLoss[0m : 10.69370
[1mStep[0m  [56/84], [94mLoss[0m : 11.03365
[1mStep[0m  [64/84], [94mLoss[0m : 10.45360
[1mStep[0m  [72/84], [94mLoss[0m : 10.81289
[1mStep[0m  [80/84], [94mLoss[0m : 10.68657

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.648, [92mTest[0m: 10.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.06981
[1mStep[0m  [8/84], [94mLoss[0m : 11.07880
[1mStep[0m  [16/84], [94mLoss[0m : 11.10330
[1mStep[0m  [24/84], [94mLoss[0m : 10.62600
[1mStep[0m  [32/84], [94mLoss[0m : 11.18390
[1mStep[0m  [40/84], [94mLoss[0m : 10.81970
[1mStep[0m  [48/84], [94mLoss[0m : 10.62909
[1mStep[0m  [56/84], [94mLoss[0m : 10.54838
[1mStep[0m  [64/84], [94mLoss[0m : 10.94494
[1mStep[0m  [72/84], [94mLoss[0m : 10.88512
[1mStep[0m  [80/84], [94mLoss[0m : 10.99032

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.629, [92mTest[0m: 10.546, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.77170
[1mStep[0m  [8/84], [94mLoss[0m : 10.64708
[1mStep[0m  [16/84], [94mLoss[0m : 10.44100
[1mStep[0m  [24/84], [94mLoss[0m : 11.17431
[1mStep[0m  [32/84], [94mLoss[0m : 10.41830
[1mStep[0m  [40/84], [94mLoss[0m : 10.30364
[1mStep[0m  [48/84], [94mLoss[0m : 10.73353
[1mStep[0m  [56/84], [94mLoss[0m : 10.28166
[1mStep[0m  [64/84], [94mLoss[0m : 10.70639
[1mStep[0m  [72/84], [94mLoss[0m : 11.22246
[1mStep[0m  [80/84], [94mLoss[0m : 10.84607

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.622, [92mTest[0m: 10.526, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.57080
[1mStep[0m  [8/84], [94mLoss[0m : 10.72367
[1mStep[0m  [16/84], [94mLoss[0m : 10.51113
[1mStep[0m  [24/84], [94mLoss[0m : 10.26358
[1mStep[0m  [32/84], [94mLoss[0m : 10.71546
[1mStep[0m  [40/84], [94mLoss[0m : 10.74471
[1mStep[0m  [48/84], [94mLoss[0m : 10.95123
[1mStep[0m  [56/84], [94mLoss[0m : 10.71932
[1mStep[0m  [64/84], [94mLoss[0m : 10.20349
[1mStep[0m  [72/84], [94mLoss[0m : 10.01054
[1mStep[0m  [80/84], [94mLoss[0m : 10.27894

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.614, [92mTest[0m: 10.485, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.81711
[1mStep[0m  [8/84], [94mLoss[0m : 10.54918
[1mStep[0m  [16/84], [94mLoss[0m : 10.49510
[1mStep[0m  [24/84], [94mLoss[0m : 10.30389
[1mStep[0m  [32/84], [94mLoss[0m : 10.77694
[1mStep[0m  [40/84], [94mLoss[0m : 10.60882
[1mStep[0m  [48/84], [94mLoss[0m : 10.57685
[1mStep[0m  [56/84], [94mLoss[0m : 10.69643
[1mStep[0m  [64/84], [94mLoss[0m : 10.43243
[1mStep[0m  [72/84], [94mLoss[0m : 11.18988
[1mStep[0m  [80/84], [94mLoss[0m : 10.47515

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.593, [92mTest[0m: 10.461, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.10433
[1mStep[0m  [8/84], [94mLoss[0m : 11.04184
[1mStep[0m  [16/84], [94mLoss[0m : 10.19352
[1mStep[0m  [24/84], [94mLoss[0m : 10.17844
[1mStep[0m  [32/84], [94mLoss[0m : 10.48800
[1mStep[0m  [40/84], [94mLoss[0m : 10.46271
[1mStep[0m  [48/84], [94mLoss[0m : 11.37063
[1mStep[0m  [56/84], [94mLoss[0m : 10.63557
[1mStep[0m  [64/84], [94mLoss[0m : 10.56843
[1mStep[0m  [72/84], [94mLoss[0m : 10.97183
[1mStep[0m  [80/84], [94mLoss[0m : 10.80714

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.573, [92mTest[0m: 10.434, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.66777
[1mStep[0m  [8/84], [94mLoss[0m : 10.50610
[1mStep[0m  [16/84], [94mLoss[0m : 11.07516
[1mStep[0m  [24/84], [94mLoss[0m : 10.61156
[1mStep[0m  [32/84], [94mLoss[0m : 10.41409
[1mStep[0m  [40/84], [94mLoss[0m : 11.28502
[1mStep[0m  [48/84], [94mLoss[0m : 10.52378
[1mStep[0m  [56/84], [94mLoss[0m : 10.25325
[1mStep[0m  [64/84], [94mLoss[0m : 10.99323
[1mStep[0m  [72/84], [94mLoss[0m : 11.00079
[1mStep[0m  [80/84], [94mLoss[0m : 10.54920

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.568, [92mTest[0m: 10.451, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.39469
[1mStep[0m  [8/84], [94mLoss[0m : 10.34148
[1mStep[0m  [16/84], [94mLoss[0m : 10.32046
[1mStep[0m  [24/84], [94mLoss[0m : 10.86245
[1mStep[0m  [32/84], [94mLoss[0m : 10.52764
[1mStep[0m  [40/84], [94mLoss[0m : 10.82612
[1mStep[0m  [48/84], [94mLoss[0m : 11.07752
[1mStep[0m  [56/84], [94mLoss[0m : 10.65269
[1mStep[0m  [64/84], [94mLoss[0m : 10.84792
[1mStep[0m  [72/84], [94mLoss[0m : 11.25771
[1mStep[0m  [80/84], [94mLoss[0m : 10.69527

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.555, [92mTest[0m: 10.427, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.28298
[1mStep[0m  [8/84], [94mLoss[0m : 10.35950
[1mStep[0m  [16/84], [94mLoss[0m : 10.69695
[1mStep[0m  [24/84], [94mLoss[0m : 10.68625
[1mStep[0m  [32/84], [94mLoss[0m : 10.52259
[1mStep[0m  [40/84], [94mLoss[0m : 10.86330
[1mStep[0m  [48/84], [94mLoss[0m : 10.91278
[1mStep[0m  [56/84], [94mLoss[0m : 10.12889
[1mStep[0m  [64/84], [94mLoss[0m : 10.18126
[1mStep[0m  [72/84], [94mLoss[0m : 10.45746
[1mStep[0m  [80/84], [94mLoss[0m : 11.01806

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.536, [92mTest[0m: 10.409, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.07105
[1mStep[0m  [8/84], [94mLoss[0m : 10.30375
[1mStep[0m  [16/84], [94mLoss[0m : 10.61008
[1mStep[0m  [24/84], [94mLoss[0m : 10.16853
[1mStep[0m  [32/84], [94mLoss[0m : 10.83262
[1mStep[0m  [40/84], [94mLoss[0m : 10.59219
[1mStep[0m  [48/84], [94mLoss[0m : 10.75265
[1mStep[0m  [56/84], [94mLoss[0m : 11.10524
[1mStep[0m  [64/84], [94mLoss[0m : 10.34062
[1mStep[0m  [72/84], [94mLoss[0m : 10.96665
[1mStep[0m  [80/84], [94mLoss[0m : 10.46846

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.521, [92mTest[0m: 10.381, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.36974
[1mStep[0m  [8/84], [94mLoss[0m : 10.75755
[1mStep[0m  [16/84], [94mLoss[0m : 10.84871
[1mStep[0m  [24/84], [94mLoss[0m : 10.00316
[1mStep[0m  [32/84], [94mLoss[0m : 10.25305
[1mStep[0m  [40/84], [94mLoss[0m : 9.88959
[1mStep[0m  [48/84], [94mLoss[0m : 10.73221
[1mStep[0m  [56/84], [94mLoss[0m : 10.29590
[1mStep[0m  [64/84], [94mLoss[0m : 10.65784
[1mStep[0m  [72/84], [94mLoss[0m : 10.56828
[1mStep[0m  [80/84], [94mLoss[0m : 10.42384

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.511, [92mTest[0m: 10.360, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.314
====================================

Phase 1 - Evaluation MAE:  10.313859019960676
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.60624
[1mStep[0m  [8/84], [94mLoss[0m : 10.24845
[1mStep[0m  [16/84], [94mLoss[0m : 10.22269
[1mStep[0m  [24/84], [94mLoss[0m : 10.18619
[1mStep[0m  [32/84], [94mLoss[0m : 10.17951
[1mStep[0m  [40/84], [94mLoss[0m : 10.29381
[1mStep[0m  [48/84], [94mLoss[0m : 10.74792
[1mStep[0m  [56/84], [94mLoss[0m : 10.51095
[1mStep[0m  [64/84], [94mLoss[0m : 10.06103
[1mStep[0m  [72/84], [94mLoss[0m : 10.83781
[1mStep[0m  [80/84], [94mLoss[0m : 10.16372

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.491, [92mTest[0m: 10.310, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.80879
[1mStep[0m  [8/84], [94mLoss[0m : 10.43378
[1mStep[0m  [16/84], [94mLoss[0m : 10.73857
[1mStep[0m  [24/84], [94mLoss[0m : 10.58096
[1mStep[0m  [32/84], [94mLoss[0m : 10.31284
[1mStep[0m  [40/84], [94mLoss[0m : 9.67047
[1mStep[0m  [48/84], [94mLoss[0m : 10.99089
[1mStep[0m  [56/84], [94mLoss[0m : 10.61308
[1mStep[0m  [64/84], [94mLoss[0m : 11.09927
[1mStep[0m  [72/84], [94mLoss[0m : 10.20019
[1mStep[0m  [80/84], [94mLoss[0m : 10.55057

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.469, [92mTest[0m: 10.318, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.53266
[1mStep[0m  [8/84], [94mLoss[0m : 11.02120
[1mStep[0m  [16/84], [94mLoss[0m : 11.20265
[1mStep[0m  [24/84], [94mLoss[0m : 10.44957
[1mStep[0m  [32/84], [94mLoss[0m : 10.08214
[1mStep[0m  [40/84], [94mLoss[0m : 10.27724
[1mStep[0m  [48/84], [94mLoss[0m : 9.98662
[1mStep[0m  [56/84], [94mLoss[0m : 10.30446
[1mStep[0m  [64/84], [94mLoss[0m : 10.88220
[1mStep[0m  [72/84], [94mLoss[0m : 10.41916
[1mStep[0m  [80/84], [94mLoss[0m : 10.23123

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.438, [92mTest[0m: 10.306, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.50614
[1mStep[0m  [8/84], [94mLoss[0m : 10.51592
[1mStep[0m  [16/84], [94mLoss[0m : 10.11435
[1mStep[0m  [24/84], [94mLoss[0m : 10.36042
[1mStep[0m  [32/84], [94mLoss[0m : 10.77783
[1mStep[0m  [40/84], [94mLoss[0m : 10.43353
[1mStep[0m  [48/84], [94mLoss[0m : 11.00063
[1mStep[0m  [56/84], [94mLoss[0m : 10.70881
[1mStep[0m  [64/84], [94mLoss[0m : 10.30809
[1mStep[0m  [72/84], [94mLoss[0m : 10.58931
[1mStep[0m  [80/84], [94mLoss[0m : 10.42544

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.406, [92mTest[0m: 10.273, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.53414
[1mStep[0m  [8/84], [94mLoss[0m : 10.98528
[1mStep[0m  [16/84], [94mLoss[0m : 10.74074
[1mStep[0m  [24/84], [94mLoss[0m : 10.56122
[1mStep[0m  [32/84], [94mLoss[0m : 10.08476
[1mStep[0m  [40/84], [94mLoss[0m : 10.43921
[1mStep[0m  [48/84], [94mLoss[0m : 11.10290
[1mStep[0m  [56/84], [94mLoss[0m : 10.16014
[1mStep[0m  [64/84], [94mLoss[0m : 10.43936
[1mStep[0m  [72/84], [94mLoss[0m : 10.40189
[1mStep[0m  [80/84], [94mLoss[0m : 10.52813

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.376, [92mTest[0m: 10.213, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.46525
[1mStep[0m  [8/84], [94mLoss[0m : 10.56678
[1mStep[0m  [16/84], [94mLoss[0m : 10.11802
[1mStep[0m  [24/84], [94mLoss[0m : 10.39990
[1mStep[0m  [32/84], [94mLoss[0m : 10.73994
[1mStep[0m  [40/84], [94mLoss[0m : 10.11390
[1mStep[0m  [48/84], [94mLoss[0m : 10.52133
[1mStep[0m  [56/84], [94mLoss[0m : 10.65457
[1mStep[0m  [64/84], [94mLoss[0m : 10.32771
[1mStep[0m  [72/84], [94mLoss[0m : 10.10654
[1mStep[0m  [80/84], [94mLoss[0m : 10.37918

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.345, [92mTest[0m: 10.175, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.30530
[1mStep[0m  [8/84], [94mLoss[0m : 10.15293
[1mStep[0m  [16/84], [94mLoss[0m : 10.57908
[1mStep[0m  [24/84], [94mLoss[0m : 10.18464
[1mStep[0m  [32/84], [94mLoss[0m : 10.66863
[1mStep[0m  [40/84], [94mLoss[0m : 10.50568
[1mStep[0m  [48/84], [94mLoss[0m : 10.07168
[1mStep[0m  [56/84], [94mLoss[0m : 10.11531
[1mStep[0m  [64/84], [94mLoss[0m : 10.55290
[1mStep[0m  [72/84], [94mLoss[0m : 10.04210
[1mStep[0m  [80/84], [94mLoss[0m : 10.33033

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.318, [92mTest[0m: 10.211, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.99627
[1mStep[0m  [8/84], [94mLoss[0m : 10.10298
[1mStep[0m  [16/84], [94mLoss[0m : 9.73154
[1mStep[0m  [24/84], [94mLoss[0m : 10.34663
[1mStep[0m  [32/84], [94mLoss[0m : 10.85502
[1mStep[0m  [40/84], [94mLoss[0m : 9.34747
[1mStep[0m  [48/84], [94mLoss[0m : 10.61229
[1mStep[0m  [56/84], [94mLoss[0m : 10.27681
[1mStep[0m  [64/84], [94mLoss[0m : 10.33883
[1mStep[0m  [72/84], [94mLoss[0m : 10.39163
[1mStep[0m  [80/84], [94mLoss[0m : 10.63918

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.283, [92mTest[0m: 10.169, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.52697
[1mStep[0m  [8/84], [94mLoss[0m : 9.85914
[1mStep[0m  [16/84], [94mLoss[0m : 10.21075
[1mStep[0m  [24/84], [94mLoss[0m : 10.85101
[1mStep[0m  [32/84], [94mLoss[0m : 10.39585
[1mStep[0m  [40/84], [94mLoss[0m : 10.09329
[1mStep[0m  [48/84], [94mLoss[0m : 10.22166
[1mStep[0m  [56/84], [94mLoss[0m : 10.10532
[1mStep[0m  [64/84], [94mLoss[0m : 9.95535
[1mStep[0m  [72/84], [94mLoss[0m : 9.81535
[1mStep[0m  [80/84], [94mLoss[0m : 10.22653

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.248, [92mTest[0m: 10.138, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.21973
[1mStep[0m  [8/84], [94mLoss[0m : 10.16838
[1mStep[0m  [16/84], [94mLoss[0m : 10.03980
[1mStep[0m  [24/84], [94mLoss[0m : 9.92504
[1mStep[0m  [32/84], [94mLoss[0m : 10.42884
[1mStep[0m  [40/84], [94mLoss[0m : 10.75483
[1mStep[0m  [48/84], [94mLoss[0m : 9.99769
[1mStep[0m  [56/84], [94mLoss[0m : 10.52344
[1mStep[0m  [64/84], [94mLoss[0m : 10.46831
[1mStep[0m  [72/84], [94mLoss[0m : 10.29787
[1mStep[0m  [80/84], [94mLoss[0m : 11.01898

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.220, [92mTest[0m: 10.143, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.85544
[1mStep[0m  [8/84], [94mLoss[0m : 9.97613
[1mStep[0m  [16/84], [94mLoss[0m : 10.59074
[1mStep[0m  [24/84], [94mLoss[0m : 10.08115
[1mStep[0m  [32/84], [94mLoss[0m : 10.57365
[1mStep[0m  [40/84], [94mLoss[0m : 10.29778
[1mStep[0m  [48/84], [94mLoss[0m : 10.25069
[1mStep[0m  [56/84], [94mLoss[0m : 10.20014
[1mStep[0m  [64/84], [94mLoss[0m : 10.80762
[1mStep[0m  [72/84], [94mLoss[0m : 10.24729
[1mStep[0m  [80/84], [94mLoss[0m : 10.42346

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.198, [92mTest[0m: 10.070, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.15474
[1mStep[0m  [8/84], [94mLoss[0m : 9.58914
[1mStep[0m  [16/84], [94mLoss[0m : 10.44336
[1mStep[0m  [24/84], [94mLoss[0m : 9.85206
[1mStep[0m  [32/84], [94mLoss[0m : 9.84321
[1mStep[0m  [40/84], [94mLoss[0m : 10.28036
[1mStep[0m  [48/84], [94mLoss[0m : 9.74613
[1mStep[0m  [56/84], [94mLoss[0m : 10.04520
[1mStep[0m  [64/84], [94mLoss[0m : 10.40273
[1mStep[0m  [72/84], [94mLoss[0m : 10.72256
[1mStep[0m  [80/84], [94mLoss[0m : 10.70893

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.161, [92mTest[0m: 10.050, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.59977
[1mStep[0m  [8/84], [94mLoss[0m : 10.06587
[1mStep[0m  [16/84], [94mLoss[0m : 10.21611
[1mStep[0m  [24/84], [94mLoss[0m : 10.69677
[1mStep[0m  [32/84], [94mLoss[0m : 10.20954
[1mStep[0m  [40/84], [94mLoss[0m : 10.29310
[1mStep[0m  [48/84], [94mLoss[0m : 10.42372
[1mStep[0m  [56/84], [94mLoss[0m : 9.27594
[1mStep[0m  [64/84], [94mLoss[0m : 9.93262
[1mStep[0m  [72/84], [94mLoss[0m : 9.86277
[1mStep[0m  [80/84], [94mLoss[0m : 10.20873

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.127, [92mTest[0m: 10.058, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.32631
[1mStep[0m  [8/84], [94mLoss[0m : 9.81318
[1mStep[0m  [16/84], [94mLoss[0m : 10.40767
[1mStep[0m  [24/84], [94mLoss[0m : 10.00646
[1mStep[0m  [32/84], [94mLoss[0m : 10.05035
[1mStep[0m  [40/84], [94mLoss[0m : 9.93558
[1mStep[0m  [48/84], [94mLoss[0m : 10.92145
[1mStep[0m  [56/84], [94mLoss[0m : 10.02577
[1mStep[0m  [64/84], [94mLoss[0m : 10.08840
[1mStep[0m  [72/84], [94mLoss[0m : 10.19975
[1mStep[0m  [80/84], [94mLoss[0m : 10.00606

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.086, [92mTest[0m: 9.965, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.04461
[1mStep[0m  [8/84], [94mLoss[0m : 9.71880
[1mStep[0m  [16/84], [94mLoss[0m : 11.08083
[1mStep[0m  [24/84], [94mLoss[0m : 9.77758
[1mStep[0m  [32/84], [94mLoss[0m : 9.70920
[1mStep[0m  [40/84], [94mLoss[0m : 9.81027
[1mStep[0m  [48/84], [94mLoss[0m : 10.15382
[1mStep[0m  [56/84], [94mLoss[0m : 9.92330
[1mStep[0m  [64/84], [94mLoss[0m : 10.05417
[1mStep[0m  [72/84], [94mLoss[0m : 10.16879
[1mStep[0m  [80/84], [94mLoss[0m : 10.22352

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.069, [92mTest[0m: 9.927, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.00201
[1mStep[0m  [8/84], [94mLoss[0m : 10.15724
[1mStep[0m  [16/84], [94mLoss[0m : 10.14514
[1mStep[0m  [24/84], [94mLoss[0m : 10.06833
[1mStep[0m  [32/84], [94mLoss[0m : 9.72768
[1mStep[0m  [40/84], [94mLoss[0m : 9.94498
[1mStep[0m  [48/84], [94mLoss[0m : 9.42715
[1mStep[0m  [56/84], [94mLoss[0m : 10.42593
[1mStep[0m  [64/84], [94mLoss[0m : 10.31067
[1mStep[0m  [72/84], [94mLoss[0m : 10.06839
[1mStep[0m  [80/84], [94mLoss[0m : 9.78830

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.030, [92mTest[0m: 9.872, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.80994
[1mStep[0m  [8/84], [94mLoss[0m : 9.74799
[1mStep[0m  [16/84], [94mLoss[0m : 10.17883
[1mStep[0m  [24/84], [94mLoss[0m : 10.45003
[1mStep[0m  [32/84], [94mLoss[0m : 10.51628
[1mStep[0m  [40/84], [94mLoss[0m : 10.37055
[1mStep[0m  [48/84], [94mLoss[0m : 10.08376
[1mStep[0m  [56/84], [94mLoss[0m : 10.35767
[1mStep[0m  [64/84], [94mLoss[0m : 9.96576
[1mStep[0m  [72/84], [94mLoss[0m : 10.31903
[1mStep[0m  [80/84], [94mLoss[0m : 10.55326

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.004, [92mTest[0m: 9.884, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.01415
[1mStep[0m  [8/84], [94mLoss[0m : 9.57915
[1mStep[0m  [16/84], [94mLoss[0m : 9.76309
[1mStep[0m  [24/84], [94mLoss[0m : 9.89825
[1mStep[0m  [32/84], [94mLoss[0m : 10.09653
[1mStep[0m  [40/84], [94mLoss[0m : 9.88365
[1mStep[0m  [48/84], [94mLoss[0m : 10.30471
[1mStep[0m  [56/84], [94mLoss[0m : 9.71200
[1mStep[0m  [64/84], [94mLoss[0m : 10.29554
[1mStep[0m  [72/84], [94mLoss[0m : 10.28784
[1mStep[0m  [80/84], [94mLoss[0m : 9.56474

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.966, [92mTest[0m: 9.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.82622
[1mStep[0m  [8/84], [94mLoss[0m : 10.06971
[1mStep[0m  [16/84], [94mLoss[0m : 9.67914
[1mStep[0m  [24/84], [94mLoss[0m : 9.75070
[1mStep[0m  [32/84], [94mLoss[0m : 9.53251
[1mStep[0m  [40/84], [94mLoss[0m : 9.42279
[1mStep[0m  [48/84], [94mLoss[0m : 9.41879
[1mStep[0m  [56/84], [94mLoss[0m : 9.67291
[1mStep[0m  [64/84], [94mLoss[0m : 10.11367
[1mStep[0m  [72/84], [94mLoss[0m : 10.12659
[1mStep[0m  [80/84], [94mLoss[0m : 9.80882

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.940, [92mTest[0m: 9.765, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.02495
[1mStep[0m  [8/84], [94mLoss[0m : 10.13816
[1mStep[0m  [16/84], [94mLoss[0m : 9.60182
[1mStep[0m  [24/84], [94mLoss[0m : 9.61594
[1mStep[0m  [32/84], [94mLoss[0m : 10.01915
[1mStep[0m  [40/84], [94mLoss[0m : 9.85205
[1mStep[0m  [48/84], [94mLoss[0m : 9.94220
[1mStep[0m  [56/84], [94mLoss[0m : 9.41239
[1mStep[0m  [64/84], [94mLoss[0m : 9.82895
[1mStep[0m  [72/84], [94mLoss[0m : 9.59997
[1mStep[0m  [80/84], [94mLoss[0m : 9.41261

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.897, [92mTest[0m: 9.693, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.72533
[1mStep[0m  [8/84], [94mLoss[0m : 9.79795
[1mStep[0m  [16/84], [94mLoss[0m : 10.22443
[1mStep[0m  [24/84], [94mLoss[0m : 9.97810
[1mStep[0m  [32/84], [94mLoss[0m : 9.35859
[1mStep[0m  [40/84], [94mLoss[0m : 9.89706
[1mStep[0m  [48/84], [94mLoss[0m : 9.65377
[1mStep[0m  [56/84], [94mLoss[0m : 10.12373
[1mStep[0m  [64/84], [94mLoss[0m : 10.15316
[1mStep[0m  [72/84], [94mLoss[0m : 10.05679
[1mStep[0m  [80/84], [94mLoss[0m : 9.52933

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.868, [92mTest[0m: 9.723, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.29394
[1mStep[0m  [8/84], [94mLoss[0m : 9.64678
[1mStep[0m  [16/84], [94mLoss[0m : 9.67209
[1mStep[0m  [24/84], [94mLoss[0m : 9.30106
[1mStep[0m  [32/84], [94mLoss[0m : 9.96321
[1mStep[0m  [40/84], [94mLoss[0m : 10.08345
[1mStep[0m  [48/84], [94mLoss[0m : 10.07832
[1mStep[0m  [56/84], [94mLoss[0m : 9.98946
[1mStep[0m  [64/84], [94mLoss[0m : 10.19180
[1mStep[0m  [72/84], [94mLoss[0m : 9.82761
[1mStep[0m  [80/84], [94mLoss[0m : 9.45110

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.837, [92mTest[0m: 9.762, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.09295
[1mStep[0m  [8/84], [94mLoss[0m : 9.85242
[1mStep[0m  [16/84], [94mLoss[0m : 9.56574
[1mStep[0m  [24/84], [94mLoss[0m : 9.74344
[1mStep[0m  [32/84], [94mLoss[0m : 10.22653
[1mStep[0m  [40/84], [94mLoss[0m : 9.75823
[1mStep[0m  [48/84], [94mLoss[0m : 9.67385
[1mStep[0m  [56/84], [94mLoss[0m : 9.45878
[1mStep[0m  [64/84], [94mLoss[0m : 9.99840
[1mStep[0m  [72/84], [94mLoss[0m : 10.03066
[1mStep[0m  [80/84], [94mLoss[0m : 9.99007

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.798, [92mTest[0m: 9.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.70318
[1mStep[0m  [8/84], [94mLoss[0m : 10.04118
[1mStep[0m  [16/84], [94mLoss[0m : 9.65040
[1mStep[0m  [24/84], [94mLoss[0m : 8.98704
[1mStep[0m  [32/84], [94mLoss[0m : 9.70288
[1mStep[0m  [40/84], [94mLoss[0m : 9.38647
[1mStep[0m  [48/84], [94mLoss[0m : 9.89052
[1mStep[0m  [56/84], [94mLoss[0m : 9.39449
[1mStep[0m  [64/84], [94mLoss[0m : 10.03523
[1mStep[0m  [72/84], [94mLoss[0m : 9.54833
[1mStep[0m  [80/84], [94mLoss[0m : 9.81272

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.771, [92mTest[0m: 9.622, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.65014
[1mStep[0m  [8/84], [94mLoss[0m : 9.79020
[1mStep[0m  [16/84], [94mLoss[0m : 9.68850
[1mStep[0m  [24/84], [94mLoss[0m : 9.22449
[1mStep[0m  [32/84], [94mLoss[0m : 9.69972
[1mStep[0m  [40/84], [94mLoss[0m : 9.96042
[1mStep[0m  [48/84], [94mLoss[0m : 9.57991
[1mStep[0m  [56/84], [94mLoss[0m : 9.79221
[1mStep[0m  [64/84], [94mLoss[0m : 10.05168
[1mStep[0m  [72/84], [94mLoss[0m : 9.67133
[1mStep[0m  [80/84], [94mLoss[0m : 9.31722

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.744, [92mTest[0m: 9.612, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.54750
[1mStep[0m  [8/84], [94mLoss[0m : 9.44509
[1mStep[0m  [16/84], [94mLoss[0m : 9.85810
[1mStep[0m  [24/84], [94mLoss[0m : 9.24943
[1mStep[0m  [32/84], [94mLoss[0m : 9.67111
[1mStep[0m  [40/84], [94mLoss[0m : 9.45037
[1mStep[0m  [48/84], [94mLoss[0m : 9.67326
[1mStep[0m  [56/84], [94mLoss[0m : 9.59198
[1mStep[0m  [64/84], [94mLoss[0m : 9.97665
[1mStep[0m  [72/84], [94mLoss[0m : 9.93082
[1mStep[0m  [80/84], [94mLoss[0m : 9.16111

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.715, [92mTest[0m: 9.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.58129
[1mStep[0m  [8/84], [94mLoss[0m : 9.66870
[1mStep[0m  [16/84], [94mLoss[0m : 10.01540
[1mStep[0m  [24/84], [94mLoss[0m : 9.76120
[1mStep[0m  [32/84], [94mLoss[0m : 9.48894
[1mStep[0m  [40/84], [94mLoss[0m : 9.60557
[1mStep[0m  [48/84], [94mLoss[0m : 9.62426
[1mStep[0m  [56/84], [94mLoss[0m : 9.66396
[1mStep[0m  [64/84], [94mLoss[0m : 9.58800
[1mStep[0m  [72/84], [94mLoss[0m : 9.42624
[1mStep[0m  [80/84], [94mLoss[0m : 9.62636

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.680, [92mTest[0m: 9.342, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.32166
[1mStep[0m  [8/84], [94mLoss[0m : 10.17503
[1mStep[0m  [16/84], [94mLoss[0m : 8.96765
[1mStep[0m  [24/84], [94mLoss[0m : 9.56867
[1mStep[0m  [32/84], [94mLoss[0m : 9.69667
[1mStep[0m  [40/84], [94mLoss[0m : 9.62761
[1mStep[0m  [48/84], [94mLoss[0m : 9.22373
[1mStep[0m  [56/84], [94mLoss[0m : 9.90835
[1mStep[0m  [64/84], [94mLoss[0m : 10.21252
[1mStep[0m  [72/84], [94mLoss[0m : 9.02311
[1mStep[0m  [80/84], [94mLoss[0m : 9.45273

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.638, [92mTest[0m: 9.540, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.74190
[1mStep[0m  [8/84], [94mLoss[0m : 9.81799
[1mStep[0m  [16/84], [94mLoss[0m : 9.60570
[1mStep[0m  [24/84], [94mLoss[0m : 9.54532
[1mStep[0m  [32/84], [94mLoss[0m : 10.04454
[1mStep[0m  [40/84], [94mLoss[0m : 9.54448
[1mStep[0m  [48/84], [94mLoss[0m : 10.10918
[1mStep[0m  [56/84], [94mLoss[0m : 10.17594
[1mStep[0m  [64/84], [94mLoss[0m : 8.96180
[1mStep[0m  [72/84], [94mLoss[0m : 9.51103
[1mStep[0m  [80/84], [94mLoss[0m : 9.69276

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.619, [92mTest[0m: 9.397, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.89966
[1mStep[0m  [8/84], [94mLoss[0m : 9.64281
[1mStep[0m  [16/84], [94mLoss[0m : 9.41877
[1mStep[0m  [24/84], [94mLoss[0m : 9.20487
[1mStep[0m  [32/84], [94mLoss[0m : 9.61606
[1mStep[0m  [40/84], [94mLoss[0m : 9.45616
[1mStep[0m  [48/84], [94mLoss[0m : 9.63634
[1mStep[0m  [56/84], [94mLoss[0m : 9.53663
[1mStep[0m  [64/84], [94mLoss[0m : 9.48858
[1mStep[0m  [72/84], [94mLoss[0m : 9.33335
[1mStep[0m  [80/84], [94mLoss[0m : 9.30924

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.577, [92mTest[0m: 9.333, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.260
====================================

Phase 2 - Evaluation MAE:  9.259798969541277
MAE score P1      10.313859
MAE score P2       9.259799
loss               9.577276
learning_rate        0.0001
batch_size              128
hidden_sizes          [300]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.97521
[1mStep[0m  [4/42], [94mLoss[0m : 11.22995
[1mStep[0m  [8/42], [94mLoss[0m : 11.18857
[1mStep[0m  [12/42], [94mLoss[0m : 10.79137
[1mStep[0m  [16/42], [94mLoss[0m : 10.58197
[1mStep[0m  [20/42], [94mLoss[0m : 10.55826
[1mStep[0m  [24/42], [94mLoss[0m : 10.92876
[1mStep[0m  [28/42], [94mLoss[0m : 10.86236
[1mStep[0m  [32/42], [94mLoss[0m : 10.86001
[1mStep[0m  [36/42], [94mLoss[0m : 10.75997
[1mStep[0m  [40/42], [94mLoss[0m : 10.98096

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.888, [92mTest[0m: 10.964, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63768
[1mStep[0m  [4/42], [94mLoss[0m : 10.71382
[1mStep[0m  [8/42], [94mLoss[0m : 10.95368
[1mStep[0m  [12/42], [94mLoss[0m : 10.83170
[1mStep[0m  [16/42], [94mLoss[0m : 10.40751
[1mStep[0m  [20/42], [94mLoss[0m : 10.68247
[1mStep[0m  [24/42], [94mLoss[0m : 11.03817
[1mStep[0m  [28/42], [94mLoss[0m : 10.83226
[1mStep[0m  [32/42], [94mLoss[0m : 11.02031
[1mStep[0m  [36/42], [94mLoss[0m : 10.94114
[1mStep[0m  [40/42], [94mLoss[0m : 11.05529

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.880, [92mTest[0m: 10.920, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54703
[1mStep[0m  [4/42], [94mLoss[0m : 10.94073
[1mStep[0m  [8/42], [94mLoss[0m : 10.88718
[1mStep[0m  [12/42], [94mLoss[0m : 10.67603
[1mStep[0m  [16/42], [94mLoss[0m : 11.19513
[1mStep[0m  [20/42], [94mLoss[0m : 10.47771
[1mStep[0m  [24/42], [94mLoss[0m : 11.22834
[1mStep[0m  [28/42], [94mLoss[0m : 10.94952
[1mStep[0m  [32/42], [94mLoss[0m : 10.88468
[1mStep[0m  [36/42], [94mLoss[0m : 10.79387
[1mStep[0m  [40/42], [94mLoss[0m : 11.08552

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.865, [92mTest[0m: 10.899, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87970
[1mStep[0m  [4/42], [94mLoss[0m : 10.99047
[1mStep[0m  [8/42], [94mLoss[0m : 11.20272
[1mStep[0m  [12/42], [94mLoss[0m : 11.12642
[1mStep[0m  [16/42], [94mLoss[0m : 10.74192
[1mStep[0m  [20/42], [94mLoss[0m : 10.62430
[1mStep[0m  [24/42], [94mLoss[0m : 11.09815
[1mStep[0m  [28/42], [94mLoss[0m : 10.84398
[1mStep[0m  [32/42], [94mLoss[0m : 11.15874
[1mStep[0m  [36/42], [94mLoss[0m : 10.60343
[1mStep[0m  [40/42], [94mLoss[0m : 10.87786

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.849, [92mTest[0m: 10.884, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95456
[1mStep[0m  [4/42], [94mLoss[0m : 11.11232
[1mStep[0m  [8/42], [94mLoss[0m : 10.64143
[1mStep[0m  [12/42], [94mLoss[0m : 11.29460
[1mStep[0m  [16/42], [94mLoss[0m : 10.73662
[1mStep[0m  [20/42], [94mLoss[0m : 10.92178
[1mStep[0m  [24/42], [94mLoss[0m : 10.54670
[1mStep[0m  [28/42], [94mLoss[0m : 10.86736
[1mStep[0m  [32/42], [94mLoss[0m : 10.71625
[1mStep[0m  [36/42], [94mLoss[0m : 10.58179
[1mStep[0m  [40/42], [94mLoss[0m : 10.92026

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.839, [92mTest[0m: 10.849, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.67157
[1mStep[0m  [4/42], [94mLoss[0m : 10.75463
[1mStep[0m  [8/42], [94mLoss[0m : 10.71007
[1mStep[0m  [12/42], [94mLoss[0m : 10.91809
[1mStep[0m  [16/42], [94mLoss[0m : 11.07801
[1mStep[0m  [20/42], [94mLoss[0m : 10.99628
[1mStep[0m  [24/42], [94mLoss[0m : 11.08663
[1mStep[0m  [28/42], [94mLoss[0m : 11.23014
[1mStep[0m  [32/42], [94mLoss[0m : 10.80296
[1mStep[0m  [36/42], [94mLoss[0m : 11.15325
[1mStep[0m  [40/42], [94mLoss[0m : 10.90234

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.819, [92mTest[0m: 10.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81769
[1mStep[0m  [4/42], [94mLoss[0m : 10.70473
[1mStep[0m  [8/42], [94mLoss[0m : 10.72509
[1mStep[0m  [12/42], [94mLoss[0m : 10.69483
[1mStep[0m  [16/42], [94mLoss[0m : 10.64773
[1mStep[0m  [20/42], [94mLoss[0m : 10.74612
[1mStep[0m  [24/42], [94mLoss[0m : 10.60472
[1mStep[0m  [28/42], [94mLoss[0m : 10.71655
[1mStep[0m  [32/42], [94mLoss[0m : 10.74096
[1mStep[0m  [36/42], [94mLoss[0m : 11.07211
[1mStep[0m  [40/42], [94mLoss[0m : 10.64078

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.810, [92mTest[0m: 10.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.17111
[1mStep[0m  [4/42], [94mLoss[0m : 10.38756
[1mStep[0m  [8/42], [94mLoss[0m : 10.65443
[1mStep[0m  [12/42], [94mLoss[0m : 11.22377
[1mStep[0m  [16/42], [94mLoss[0m : 10.89895
[1mStep[0m  [20/42], [94mLoss[0m : 10.42834
[1mStep[0m  [24/42], [94mLoss[0m : 10.85139
[1mStep[0m  [28/42], [94mLoss[0m : 10.92845
[1mStep[0m  [32/42], [94mLoss[0m : 10.58726
[1mStep[0m  [36/42], [94mLoss[0m : 10.68473
[1mStep[0m  [40/42], [94mLoss[0m : 11.10044

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.797, [92mTest[0m: 10.800, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94905
[1mStep[0m  [4/42], [94mLoss[0m : 10.82462
[1mStep[0m  [8/42], [94mLoss[0m : 11.20009
[1mStep[0m  [12/42], [94mLoss[0m : 10.66306
[1mStep[0m  [16/42], [94mLoss[0m : 10.87273
[1mStep[0m  [20/42], [94mLoss[0m : 10.79954
[1mStep[0m  [24/42], [94mLoss[0m : 10.95346
[1mStep[0m  [28/42], [94mLoss[0m : 10.74469
[1mStep[0m  [32/42], [94mLoss[0m : 10.84928
[1mStep[0m  [36/42], [94mLoss[0m : 10.73812
[1mStep[0m  [40/42], [94mLoss[0m : 10.72177

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.781, [92mTest[0m: 10.784, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.67131
[1mStep[0m  [4/42], [94mLoss[0m : 10.76753
[1mStep[0m  [8/42], [94mLoss[0m : 10.85896
[1mStep[0m  [12/42], [94mLoss[0m : 10.89022
[1mStep[0m  [16/42], [94mLoss[0m : 10.87974
[1mStep[0m  [20/42], [94mLoss[0m : 10.91469
[1mStep[0m  [24/42], [94mLoss[0m : 10.62844
[1mStep[0m  [28/42], [94mLoss[0m : 10.42134
[1mStep[0m  [32/42], [94mLoss[0m : 10.93708
[1mStep[0m  [36/42], [94mLoss[0m : 11.17274
[1mStep[0m  [40/42], [94mLoss[0m : 10.67514

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.767, [92mTest[0m: 10.759, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93485
[1mStep[0m  [4/42], [94mLoss[0m : 10.84541
[1mStep[0m  [8/42], [94mLoss[0m : 11.14615
[1mStep[0m  [12/42], [94mLoss[0m : 10.78510
[1mStep[0m  [16/42], [94mLoss[0m : 10.69870
[1mStep[0m  [20/42], [94mLoss[0m : 10.46941
[1mStep[0m  [24/42], [94mLoss[0m : 10.91016
[1mStep[0m  [28/42], [94mLoss[0m : 10.85503
[1mStep[0m  [32/42], [94mLoss[0m : 10.95299
[1mStep[0m  [36/42], [94mLoss[0m : 10.32168
[1mStep[0m  [40/42], [94mLoss[0m : 10.66202

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.753, [92mTest[0m: 10.745, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42152
[1mStep[0m  [4/42], [94mLoss[0m : 10.78156
[1mStep[0m  [8/42], [94mLoss[0m : 10.47261
[1mStep[0m  [12/42], [94mLoss[0m : 10.60731
[1mStep[0m  [16/42], [94mLoss[0m : 10.76910
[1mStep[0m  [20/42], [94mLoss[0m : 10.71719
[1mStep[0m  [24/42], [94mLoss[0m : 10.75012
[1mStep[0m  [28/42], [94mLoss[0m : 11.07571
[1mStep[0m  [32/42], [94mLoss[0m : 10.84405
[1mStep[0m  [36/42], [94mLoss[0m : 11.03903
[1mStep[0m  [40/42], [94mLoss[0m : 10.60140

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.733, [92mTest[0m: 10.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.07017
[1mStep[0m  [4/42], [94mLoss[0m : 10.61241
[1mStep[0m  [8/42], [94mLoss[0m : 10.68882
[1mStep[0m  [12/42], [94mLoss[0m : 10.56993
[1mStep[0m  [16/42], [94mLoss[0m : 10.39042
[1mStep[0m  [20/42], [94mLoss[0m : 10.65843
[1mStep[0m  [24/42], [94mLoss[0m : 10.53667
[1mStep[0m  [28/42], [94mLoss[0m : 10.63405
[1mStep[0m  [32/42], [94mLoss[0m : 10.53856
[1mStep[0m  [36/42], [94mLoss[0m : 11.02072
[1mStep[0m  [40/42], [94mLoss[0m : 10.65361

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.726, [92mTest[0m: 10.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74360
[1mStep[0m  [4/42], [94mLoss[0m : 10.64547
[1mStep[0m  [8/42], [94mLoss[0m : 11.04337
[1mStep[0m  [12/42], [94mLoss[0m : 11.01348
[1mStep[0m  [16/42], [94mLoss[0m : 10.76739
[1mStep[0m  [20/42], [94mLoss[0m : 10.82368
[1mStep[0m  [24/42], [94mLoss[0m : 10.50909
[1mStep[0m  [28/42], [94mLoss[0m : 10.76520
[1mStep[0m  [32/42], [94mLoss[0m : 10.62980
[1mStep[0m  [36/42], [94mLoss[0m : 10.91813
[1mStep[0m  [40/42], [94mLoss[0m : 10.72500

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.720, [92mTest[0m: 10.669, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.75928
[1mStep[0m  [4/42], [94mLoss[0m : 10.99023
[1mStep[0m  [8/42], [94mLoss[0m : 10.72452
[1mStep[0m  [12/42], [94mLoss[0m : 10.94088
[1mStep[0m  [16/42], [94mLoss[0m : 10.47312
[1mStep[0m  [20/42], [94mLoss[0m : 10.95881
[1mStep[0m  [24/42], [94mLoss[0m : 10.43884
[1mStep[0m  [28/42], [94mLoss[0m : 10.38844
[1mStep[0m  [32/42], [94mLoss[0m : 10.69816
[1mStep[0m  [36/42], [94mLoss[0m : 10.51337
[1mStep[0m  [40/42], [94mLoss[0m : 10.59819

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.689, [92mTest[0m: 10.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87102
[1mStep[0m  [4/42], [94mLoss[0m : 10.27547
[1mStep[0m  [8/42], [94mLoss[0m : 10.60218
[1mStep[0m  [12/42], [94mLoss[0m : 10.65139
[1mStep[0m  [16/42], [94mLoss[0m : 10.55391
[1mStep[0m  [20/42], [94mLoss[0m : 10.30212
[1mStep[0m  [24/42], [94mLoss[0m : 10.80871
[1mStep[0m  [28/42], [94mLoss[0m : 10.63696
[1mStep[0m  [32/42], [94mLoss[0m : 10.57913
[1mStep[0m  [36/42], [94mLoss[0m : 10.44402
[1mStep[0m  [40/42], [94mLoss[0m : 10.76150

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.681, [92mTest[0m: 10.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.43221
[1mStep[0m  [4/42], [94mLoss[0m : 10.76253
[1mStep[0m  [8/42], [94mLoss[0m : 10.74988
[1mStep[0m  [12/42], [94mLoss[0m : 10.37547
[1mStep[0m  [16/42], [94mLoss[0m : 10.50256
[1mStep[0m  [20/42], [94mLoss[0m : 10.86771
[1mStep[0m  [24/42], [94mLoss[0m : 10.81253
[1mStep[0m  [28/42], [94mLoss[0m : 10.45151
[1mStep[0m  [32/42], [94mLoss[0m : 10.27173
[1mStep[0m  [36/42], [94mLoss[0m : 10.74699
[1mStep[0m  [40/42], [94mLoss[0m : 11.07170

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.664, [92mTest[0m: 10.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81281
[1mStep[0m  [4/42], [94mLoss[0m : 10.09321
[1mStep[0m  [8/42], [94mLoss[0m : 10.59169
[1mStep[0m  [12/42], [94mLoss[0m : 10.62596
[1mStep[0m  [16/42], [94mLoss[0m : 10.57011
[1mStep[0m  [20/42], [94mLoss[0m : 10.66236
[1mStep[0m  [24/42], [94mLoss[0m : 10.74651
[1mStep[0m  [28/42], [94mLoss[0m : 10.71869
[1mStep[0m  [32/42], [94mLoss[0m : 10.31736
[1mStep[0m  [36/42], [94mLoss[0m : 11.00226
[1mStep[0m  [40/42], [94mLoss[0m : 10.57566

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.647, [92mTest[0m: 10.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.70390
[1mStep[0m  [4/42], [94mLoss[0m : 10.50247
[1mStep[0m  [8/42], [94mLoss[0m : 10.85530
[1mStep[0m  [12/42], [94mLoss[0m : 10.89832
[1mStep[0m  [16/42], [94mLoss[0m : 10.77238
[1mStep[0m  [20/42], [94mLoss[0m : 10.76101
[1mStep[0m  [24/42], [94mLoss[0m : 10.35712
[1mStep[0m  [28/42], [94mLoss[0m : 10.45720
[1mStep[0m  [32/42], [94mLoss[0m : 10.68739
[1mStep[0m  [36/42], [94mLoss[0m : 10.51776
[1mStep[0m  [40/42], [94mLoss[0m : 10.78921

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.638, [92mTest[0m: 10.584, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.73130
[1mStep[0m  [4/42], [94mLoss[0m : 10.65113
[1mStep[0m  [8/42], [94mLoss[0m : 10.84193
[1mStep[0m  [12/42], [94mLoss[0m : 9.99865
[1mStep[0m  [16/42], [94mLoss[0m : 10.58527
[1mStep[0m  [20/42], [94mLoss[0m : 11.26864
[1mStep[0m  [24/42], [94mLoss[0m : 10.76779
[1mStep[0m  [28/42], [94mLoss[0m : 10.50462
[1mStep[0m  [32/42], [94mLoss[0m : 10.91302
[1mStep[0m  [36/42], [94mLoss[0m : 10.26371
[1mStep[0m  [40/42], [94mLoss[0m : 10.84317

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.623, [92mTest[0m: 10.554, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53905
[1mStep[0m  [4/42], [94mLoss[0m : 10.53783
[1mStep[0m  [8/42], [94mLoss[0m : 11.16756
[1mStep[0m  [12/42], [94mLoss[0m : 10.39728
[1mStep[0m  [16/42], [94mLoss[0m : 10.65837
[1mStep[0m  [20/42], [94mLoss[0m : 10.50701
[1mStep[0m  [24/42], [94mLoss[0m : 10.49455
[1mStep[0m  [28/42], [94mLoss[0m : 10.59369
[1mStep[0m  [32/42], [94mLoss[0m : 10.49775
[1mStep[0m  [36/42], [94mLoss[0m : 10.85259
[1mStep[0m  [40/42], [94mLoss[0m : 10.29712

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.612, [92mTest[0m: 10.526, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54458
[1mStep[0m  [4/42], [94mLoss[0m : 10.75272
[1mStep[0m  [8/42], [94mLoss[0m : 10.76581
[1mStep[0m  [12/42], [94mLoss[0m : 10.34664
[1mStep[0m  [16/42], [94mLoss[0m : 10.43455
[1mStep[0m  [20/42], [94mLoss[0m : 10.39348
[1mStep[0m  [24/42], [94mLoss[0m : 10.65564
[1mStep[0m  [28/42], [94mLoss[0m : 10.53554
[1mStep[0m  [32/42], [94mLoss[0m : 10.84520
[1mStep[0m  [36/42], [94mLoss[0m : 10.92039
[1mStep[0m  [40/42], [94mLoss[0m : 10.72298

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.594, [92mTest[0m: 10.514, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65001
[1mStep[0m  [4/42], [94mLoss[0m : 10.78816
[1mStep[0m  [8/42], [94mLoss[0m : 10.59313
[1mStep[0m  [12/42], [94mLoss[0m : 10.40950
[1mStep[0m  [16/42], [94mLoss[0m : 10.52649
[1mStep[0m  [20/42], [94mLoss[0m : 10.79055
[1mStep[0m  [24/42], [94mLoss[0m : 10.35381
[1mStep[0m  [28/42], [94mLoss[0m : 10.57602
[1mStep[0m  [32/42], [94mLoss[0m : 10.43655
[1mStep[0m  [36/42], [94mLoss[0m : 10.24002
[1mStep[0m  [40/42], [94mLoss[0m : 10.46572

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.580, [92mTest[0m: 10.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81383
[1mStep[0m  [4/42], [94mLoss[0m : 10.24051
[1mStep[0m  [8/42], [94mLoss[0m : 10.25031
[1mStep[0m  [12/42], [94mLoss[0m : 10.64816
[1mStep[0m  [16/42], [94mLoss[0m : 10.39715
[1mStep[0m  [20/42], [94mLoss[0m : 10.59200
[1mStep[0m  [24/42], [94mLoss[0m : 10.43159
[1mStep[0m  [28/42], [94mLoss[0m : 10.73107
[1mStep[0m  [32/42], [94mLoss[0m : 10.75021
[1mStep[0m  [36/42], [94mLoss[0m : 10.82677
[1mStep[0m  [40/42], [94mLoss[0m : 11.23569

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.575, [92mTest[0m: 10.483, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.50063
[1mStep[0m  [4/42], [94mLoss[0m : 10.56250
[1mStep[0m  [8/42], [94mLoss[0m : 10.58020
[1mStep[0m  [12/42], [94mLoss[0m : 10.26269
[1mStep[0m  [16/42], [94mLoss[0m : 10.12916
[1mStep[0m  [20/42], [94mLoss[0m : 10.67688
[1mStep[0m  [24/42], [94mLoss[0m : 10.95748
[1mStep[0m  [28/42], [94mLoss[0m : 10.64928
[1mStep[0m  [32/42], [94mLoss[0m : 10.71072
[1mStep[0m  [36/42], [94mLoss[0m : 10.31836
[1mStep[0m  [40/42], [94mLoss[0m : 10.44362

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.555, [92mTest[0m: 10.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65332
[1mStep[0m  [4/42], [94mLoss[0m : 10.51857
[1mStep[0m  [8/42], [94mLoss[0m : 10.69749
[1mStep[0m  [12/42], [94mLoss[0m : 10.87910
[1mStep[0m  [16/42], [94mLoss[0m : 10.54023
[1mStep[0m  [20/42], [94mLoss[0m : 10.54060
[1mStep[0m  [24/42], [94mLoss[0m : 10.35984
[1mStep[0m  [28/42], [94mLoss[0m : 10.28928
[1mStep[0m  [32/42], [94mLoss[0m : 10.23827
[1mStep[0m  [36/42], [94mLoss[0m : 10.69230
[1mStep[0m  [40/42], [94mLoss[0m : 10.09564

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.543, [92mTest[0m: 10.406, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.56429
[1mStep[0m  [4/42], [94mLoss[0m : 10.68779
[1mStep[0m  [8/42], [94mLoss[0m : 10.20770
[1mStep[0m  [12/42], [94mLoss[0m : 10.77772
[1mStep[0m  [16/42], [94mLoss[0m : 10.36484
[1mStep[0m  [20/42], [94mLoss[0m : 10.72144
[1mStep[0m  [24/42], [94mLoss[0m : 10.70957
[1mStep[0m  [28/42], [94mLoss[0m : 10.23459
[1mStep[0m  [32/42], [94mLoss[0m : 10.58812
[1mStep[0m  [36/42], [94mLoss[0m : 10.65621
[1mStep[0m  [40/42], [94mLoss[0m : 10.45204

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.536, [92mTest[0m: 10.433, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.34858
[1mStep[0m  [4/42], [94mLoss[0m : 10.48425
[1mStep[0m  [8/42], [94mLoss[0m : 10.67471
[1mStep[0m  [12/42], [94mLoss[0m : 10.36613
[1mStep[0m  [16/42], [94mLoss[0m : 10.43587
[1mStep[0m  [20/42], [94mLoss[0m : 10.54342
[1mStep[0m  [24/42], [94mLoss[0m : 9.98088
[1mStep[0m  [28/42], [94mLoss[0m : 10.34514
[1mStep[0m  [32/42], [94mLoss[0m : 10.75827
[1mStep[0m  [36/42], [94mLoss[0m : 10.60392
[1mStep[0m  [40/42], [94mLoss[0m : 10.32477

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.522, [92mTest[0m: 10.422, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36450
[1mStep[0m  [4/42], [94mLoss[0m : 10.58598
[1mStep[0m  [8/42], [94mLoss[0m : 10.52463
[1mStep[0m  [12/42], [94mLoss[0m : 10.21278
[1mStep[0m  [16/42], [94mLoss[0m : 10.61569
[1mStep[0m  [20/42], [94mLoss[0m : 10.38330
[1mStep[0m  [24/42], [94mLoss[0m : 10.59848
[1mStep[0m  [28/42], [94mLoss[0m : 10.68769
[1mStep[0m  [32/42], [94mLoss[0m : 10.24433
[1mStep[0m  [36/42], [94mLoss[0m : 10.11107
[1mStep[0m  [40/42], [94mLoss[0m : 10.38373

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.515, [92mTest[0m: 10.381, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51947
[1mStep[0m  [4/42], [94mLoss[0m : 10.26797
[1mStep[0m  [8/42], [94mLoss[0m : 10.40785
[1mStep[0m  [12/42], [94mLoss[0m : 10.15003
[1mStep[0m  [16/42], [94mLoss[0m : 10.08835
[1mStep[0m  [20/42], [94mLoss[0m : 10.56733
[1mStep[0m  [24/42], [94mLoss[0m : 10.60687
[1mStep[0m  [28/42], [94mLoss[0m : 10.70709
[1mStep[0m  [32/42], [94mLoss[0m : 10.43312
[1mStep[0m  [36/42], [94mLoss[0m : 10.82478
[1mStep[0m  [40/42], [94mLoss[0m : 10.28469

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.495, [92mTest[0m: 10.372, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.345
====================================

Phase 1 - Evaluation MAE:  10.344989163534981
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.54216
[1mStep[0m  [4/42], [94mLoss[0m : 10.82416
[1mStep[0m  [8/42], [94mLoss[0m : 10.17144
[1mStep[0m  [12/42], [94mLoss[0m : 10.31839
[1mStep[0m  [16/42], [94mLoss[0m : 10.69125
[1mStep[0m  [20/42], [94mLoss[0m : 10.55999
[1mStep[0m  [24/42], [94mLoss[0m : 10.28127
[1mStep[0m  [28/42], [94mLoss[0m : 10.40088
[1mStep[0m  [32/42], [94mLoss[0m : 10.80079
[1mStep[0m  [36/42], [94mLoss[0m : 11.01535
[1mStep[0m  [40/42], [94mLoss[0m : 10.12815

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.480, [92mTest[0m: 10.346, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25071
[1mStep[0m  [4/42], [94mLoss[0m : 10.33051
[1mStep[0m  [8/42], [94mLoss[0m : 10.56088
[1mStep[0m  [12/42], [94mLoss[0m : 10.50201
[1mStep[0m  [16/42], [94mLoss[0m : 10.16203
[1mStep[0m  [20/42], [94mLoss[0m : 10.00787
[1mStep[0m  [24/42], [94mLoss[0m : 10.74751
[1mStep[0m  [28/42], [94mLoss[0m : 10.26933
[1mStep[0m  [32/42], [94mLoss[0m : 10.34122
[1mStep[0m  [36/42], [94mLoss[0m : 10.47438
[1mStep[0m  [40/42], [94mLoss[0m : 10.52762

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.446, [92mTest[0m: 10.364, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42367
[1mStep[0m  [4/42], [94mLoss[0m : 10.33217
[1mStep[0m  [8/42], [94mLoss[0m : 10.54850
[1mStep[0m  [12/42], [94mLoss[0m : 10.44673
[1mStep[0m  [16/42], [94mLoss[0m : 10.46928
[1mStep[0m  [20/42], [94mLoss[0m : 10.07651
[1mStep[0m  [24/42], [94mLoss[0m : 10.72244
[1mStep[0m  [28/42], [94mLoss[0m : 10.21490
[1mStep[0m  [32/42], [94mLoss[0m : 10.50276
[1mStep[0m  [36/42], [94mLoss[0m : 10.28475
[1mStep[0m  [40/42], [94mLoss[0m : 10.58211

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.433, [92mTest[0m: 10.310, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.31415
[1mStep[0m  [4/42], [94mLoss[0m : 10.79813
[1mStep[0m  [8/42], [94mLoss[0m : 10.25505
[1mStep[0m  [12/42], [94mLoss[0m : 10.02662
[1mStep[0m  [16/42], [94mLoss[0m : 10.74618
[1mStep[0m  [20/42], [94mLoss[0m : 10.35290
[1mStep[0m  [24/42], [94mLoss[0m : 10.24692
[1mStep[0m  [28/42], [94mLoss[0m : 10.13749
[1mStep[0m  [32/42], [94mLoss[0m : 10.15521
[1mStep[0m  [36/42], [94mLoss[0m : 10.49442
[1mStep[0m  [40/42], [94mLoss[0m : 10.64038

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.398, [92mTest[0m: 10.282, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54211
[1mStep[0m  [4/42], [94mLoss[0m : 10.38371
[1mStep[0m  [8/42], [94mLoss[0m : 10.24261
[1mStep[0m  [12/42], [94mLoss[0m : 10.33727
[1mStep[0m  [16/42], [94mLoss[0m : 10.13459
[1mStep[0m  [20/42], [94mLoss[0m : 10.41652
[1mStep[0m  [24/42], [94mLoss[0m : 9.87272
[1mStep[0m  [28/42], [94mLoss[0m : 10.31507
[1mStep[0m  [32/42], [94mLoss[0m : 10.00596
[1mStep[0m  [36/42], [94mLoss[0m : 10.17920
[1mStep[0m  [40/42], [94mLoss[0m : 10.66811

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.361, [92mTest[0m: 10.254, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25808
[1mStep[0m  [4/42], [94mLoss[0m : 10.41930
[1mStep[0m  [8/42], [94mLoss[0m : 11.00087
[1mStep[0m  [12/42], [94mLoss[0m : 10.35718
[1mStep[0m  [16/42], [94mLoss[0m : 10.48096
[1mStep[0m  [20/42], [94mLoss[0m : 9.83176
[1mStep[0m  [24/42], [94mLoss[0m : 10.16377
[1mStep[0m  [28/42], [94mLoss[0m : 10.49374
[1mStep[0m  [32/42], [94mLoss[0m : 10.21687
[1mStep[0m  [36/42], [94mLoss[0m : 10.26121
[1mStep[0m  [40/42], [94mLoss[0m : 9.85535

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.324, [92mTest[0m: 10.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.41452
[1mStep[0m  [4/42], [94mLoss[0m : 10.03536
[1mStep[0m  [8/42], [94mLoss[0m : 10.38411
[1mStep[0m  [12/42], [94mLoss[0m : 10.10666
[1mStep[0m  [16/42], [94mLoss[0m : 10.03899
[1mStep[0m  [20/42], [94mLoss[0m : 10.25624
[1mStep[0m  [24/42], [94mLoss[0m : 10.15919
[1mStep[0m  [28/42], [94mLoss[0m : 9.99468
[1mStep[0m  [32/42], [94mLoss[0m : 10.05197
[1mStep[0m  [36/42], [94mLoss[0m : 10.39242
[1mStep[0m  [40/42], [94mLoss[0m : 10.29794

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.297, [92mTest[0m: 10.151, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.35558
[1mStep[0m  [4/42], [94mLoss[0m : 10.14162
[1mStep[0m  [8/42], [94mLoss[0m : 10.01843
[1mStep[0m  [12/42], [94mLoss[0m : 10.47320
[1mStep[0m  [16/42], [94mLoss[0m : 10.33310
[1mStep[0m  [20/42], [94mLoss[0m : 10.37414
[1mStep[0m  [24/42], [94mLoss[0m : 9.79222
[1mStep[0m  [28/42], [94mLoss[0m : 10.26935
[1mStep[0m  [32/42], [94mLoss[0m : 10.36699
[1mStep[0m  [36/42], [94mLoss[0m : 10.30101
[1mStep[0m  [40/42], [94mLoss[0m : 10.34677

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.261, [92mTest[0m: 10.147, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.01928
[1mStep[0m  [4/42], [94mLoss[0m : 10.47020
[1mStep[0m  [8/42], [94mLoss[0m : 10.26776
[1mStep[0m  [12/42], [94mLoss[0m : 10.16999
[1mStep[0m  [16/42], [94mLoss[0m : 10.28851
[1mStep[0m  [20/42], [94mLoss[0m : 10.17858
[1mStep[0m  [24/42], [94mLoss[0m : 10.08108
[1mStep[0m  [28/42], [94mLoss[0m : 10.63098
[1mStep[0m  [32/42], [94mLoss[0m : 9.96058
[1mStep[0m  [36/42], [94mLoss[0m : 9.94329
[1mStep[0m  [40/42], [94mLoss[0m : 10.12557

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.228, [92mTest[0m: 10.094, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42132
[1mStep[0m  [4/42], [94mLoss[0m : 10.50587
[1mStep[0m  [8/42], [94mLoss[0m : 9.59970
[1mStep[0m  [12/42], [94mLoss[0m : 10.24427
[1mStep[0m  [16/42], [94mLoss[0m : 10.65699
[1mStep[0m  [20/42], [94mLoss[0m : 10.46397
[1mStep[0m  [24/42], [94mLoss[0m : 10.30200
[1mStep[0m  [28/42], [94mLoss[0m : 10.02697
[1mStep[0m  [32/42], [94mLoss[0m : 10.14597
[1mStep[0m  [36/42], [94mLoss[0m : 10.18762
[1mStep[0m  [40/42], [94mLoss[0m : 10.33740

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.194, [92mTest[0m: 10.027, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.02620
[1mStep[0m  [4/42], [94mLoss[0m : 10.11650
[1mStep[0m  [8/42], [94mLoss[0m : 9.99926
[1mStep[0m  [12/42], [94mLoss[0m : 10.40891
[1mStep[0m  [16/42], [94mLoss[0m : 10.36553
[1mStep[0m  [20/42], [94mLoss[0m : 10.45188
[1mStep[0m  [24/42], [94mLoss[0m : 10.12736
[1mStep[0m  [28/42], [94mLoss[0m : 10.22477
[1mStep[0m  [32/42], [94mLoss[0m : 10.08932
[1mStep[0m  [36/42], [94mLoss[0m : 10.12160
[1mStep[0m  [40/42], [94mLoss[0m : 10.08048

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.164, [92mTest[0m: 10.019, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.33318
[1mStep[0m  [4/42], [94mLoss[0m : 9.74636
[1mStep[0m  [8/42], [94mLoss[0m : 10.20306
[1mStep[0m  [12/42], [94mLoss[0m : 9.98240
[1mStep[0m  [16/42], [94mLoss[0m : 10.24636
[1mStep[0m  [20/42], [94mLoss[0m : 10.17363
[1mStep[0m  [24/42], [94mLoss[0m : 9.99240
[1mStep[0m  [28/42], [94mLoss[0m : 10.46314
[1mStep[0m  [32/42], [94mLoss[0m : 10.01186
[1mStep[0m  [36/42], [94mLoss[0m : 9.63910
[1mStep[0m  [40/42], [94mLoss[0m : 10.14689

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.133, [92mTest[0m: 10.004, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.06577
[1mStep[0m  [4/42], [94mLoss[0m : 9.89504
[1mStep[0m  [8/42], [94mLoss[0m : 9.99047
[1mStep[0m  [12/42], [94mLoss[0m : 10.16322
[1mStep[0m  [16/42], [94mLoss[0m : 9.69457
[1mStep[0m  [20/42], [94mLoss[0m : 9.88651
[1mStep[0m  [24/42], [94mLoss[0m : 10.20600
[1mStep[0m  [28/42], [94mLoss[0m : 10.13527
[1mStep[0m  [32/42], [94mLoss[0m : 10.05822
[1mStep[0m  [36/42], [94mLoss[0m : 10.21920
[1mStep[0m  [40/42], [94mLoss[0m : 9.98665

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.104, [92mTest[0m: 9.948, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37615
[1mStep[0m  [4/42], [94mLoss[0m : 10.36898
[1mStep[0m  [8/42], [94mLoss[0m : 9.81239
[1mStep[0m  [12/42], [94mLoss[0m : 10.09911
[1mStep[0m  [16/42], [94mLoss[0m : 10.08103
[1mStep[0m  [20/42], [94mLoss[0m : 10.06355
[1mStep[0m  [24/42], [94mLoss[0m : 10.09080
[1mStep[0m  [28/42], [94mLoss[0m : 10.11336
[1mStep[0m  [32/42], [94mLoss[0m : 10.09220
[1mStep[0m  [36/42], [94mLoss[0m : 10.21501
[1mStep[0m  [40/42], [94mLoss[0m : 9.74766

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.069, [92mTest[0m: 9.936, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.76056
[1mStep[0m  [4/42], [94mLoss[0m : 10.12828
[1mStep[0m  [8/42], [94mLoss[0m : 10.02721
[1mStep[0m  [12/42], [94mLoss[0m : 10.13393
[1mStep[0m  [16/42], [94mLoss[0m : 9.95342
[1mStep[0m  [20/42], [94mLoss[0m : 10.33683
[1mStep[0m  [24/42], [94mLoss[0m : 9.89794
[1mStep[0m  [28/42], [94mLoss[0m : 9.99508
[1mStep[0m  [32/42], [94mLoss[0m : 9.98478
[1mStep[0m  [36/42], [94mLoss[0m : 10.18822
[1mStep[0m  [40/42], [94mLoss[0m : 10.12602

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.040, [92mTest[0m: 9.928, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.77215
[1mStep[0m  [4/42], [94mLoss[0m : 10.12438
[1mStep[0m  [8/42], [94mLoss[0m : 9.79490
[1mStep[0m  [12/42], [94mLoss[0m : 9.88439
[1mStep[0m  [16/42], [94mLoss[0m : 9.69160
[1mStep[0m  [20/42], [94mLoss[0m : 9.88607
[1mStep[0m  [24/42], [94mLoss[0m : 9.77091
[1mStep[0m  [28/42], [94mLoss[0m : 9.82354
[1mStep[0m  [32/42], [94mLoss[0m : 9.83571
[1mStep[0m  [36/42], [94mLoss[0m : 10.17512
[1mStep[0m  [40/42], [94mLoss[0m : 9.98850

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.012, [92mTest[0m: 9.840, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.78575
[1mStep[0m  [4/42], [94mLoss[0m : 10.32338
[1mStep[0m  [8/42], [94mLoss[0m : 9.86505
[1mStep[0m  [12/42], [94mLoss[0m : 9.98655
[1mStep[0m  [16/42], [94mLoss[0m : 10.13123
[1mStep[0m  [20/42], [94mLoss[0m : 9.82919
[1mStep[0m  [24/42], [94mLoss[0m : 9.99907
[1mStep[0m  [28/42], [94mLoss[0m : 9.87357
[1mStep[0m  [32/42], [94mLoss[0m : 9.77193
[1mStep[0m  [36/42], [94mLoss[0m : 9.97385
[1mStep[0m  [40/42], [94mLoss[0m : 9.74355

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.982, [92mTest[0m: 9.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.79210
[1mStep[0m  [4/42], [94mLoss[0m : 9.87660
[1mStep[0m  [8/42], [94mLoss[0m : 9.86250
[1mStep[0m  [12/42], [94mLoss[0m : 10.22387
[1mStep[0m  [16/42], [94mLoss[0m : 9.82648
[1mStep[0m  [20/42], [94mLoss[0m : 10.20234
[1mStep[0m  [24/42], [94mLoss[0m : 9.82993
[1mStep[0m  [28/42], [94mLoss[0m : 9.84086
[1mStep[0m  [32/42], [94mLoss[0m : 9.84649
[1mStep[0m  [36/42], [94mLoss[0m : 9.74369
[1mStep[0m  [40/42], [94mLoss[0m : 10.12479

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.957, [92mTest[0m: 9.847, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.95466
[1mStep[0m  [4/42], [94mLoss[0m : 9.75499
[1mStep[0m  [8/42], [94mLoss[0m : 9.83435
[1mStep[0m  [12/42], [94mLoss[0m : 9.82939
[1mStep[0m  [16/42], [94mLoss[0m : 10.03948
[1mStep[0m  [20/42], [94mLoss[0m : 9.84542
[1mStep[0m  [24/42], [94mLoss[0m : 9.77946
[1mStep[0m  [28/42], [94mLoss[0m : 10.26192
[1mStep[0m  [32/42], [94mLoss[0m : 9.88279
[1mStep[0m  [36/42], [94mLoss[0m : 10.03013
[1mStep[0m  [40/42], [94mLoss[0m : 9.65208

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.922, [92mTest[0m: 9.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.55707
[1mStep[0m  [4/42], [94mLoss[0m : 10.22993
[1mStep[0m  [8/42], [94mLoss[0m : 9.80326
[1mStep[0m  [12/42], [94mLoss[0m : 9.78816
[1mStep[0m  [16/42], [94mLoss[0m : 9.94385
[1mStep[0m  [20/42], [94mLoss[0m : 9.24671
[1mStep[0m  [24/42], [94mLoss[0m : 9.97113
[1mStep[0m  [28/42], [94mLoss[0m : 9.81346
[1mStep[0m  [32/42], [94mLoss[0m : 9.59548
[1mStep[0m  [36/42], [94mLoss[0m : 10.07751
[1mStep[0m  [40/42], [94mLoss[0m : 9.96664

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.890, [92mTest[0m: 9.686, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.54063
[1mStep[0m  [4/42], [94mLoss[0m : 9.87814
[1mStep[0m  [8/42], [94mLoss[0m : 9.96408
[1mStep[0m  [12/42], [94mLoss[0m : 9.97488
[1mStep[0m  [16/42], [94mLoss[0m : 9.88479
[1mStep[0m  [20/42], [94mLoss[0m : 9.44852
[1mStep[0m  [24/42], [94mLoss[0m : 9.78574
[1mStep[0m  [28/42], [94mLoss[0m : 9.81143
[1mStep[0m  [32/42], [94mLoss[0m : 9.37946
[1mStep[0m  [36/42], [94mLoss[0m : 9.92018
[1mStep[0m  [40/42], [94mLoss[0m : 10.37316

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.861, [92mTest[0m: 9.698, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.05561
[1mStep[0m  [4/42], [94mLoss[0m : 9.71627
[1mStep[0m  [8/42], [94mLoss[0m : 9.80459
[1mStep[0m  [12/42], [94mLoss[0m : 9.68478
[1mStep[0m  [16/42], [94mLoss[0m : 9.60760
[1mStep[0m  [20/42], [94mLoss[0m : 9.90682
[1mStep[0m  [24/42], [94mLoss[0m : 9.44241
[1mStep[0m  [28/42], [94mLoss[0m : 10.59055
[1mStep[0m  [32/42], [94mLoss[0m : 9.80252
[1mStep[0m  [36/42], [94mLoss[0m : 9.77902
[1mStep[0m  [40/42], [94mLoss[0m : 9.78890

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.826, [92mTest[0m: 9.716, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.84869
[1mStep[0m  [4/42], [94mLoss[0m : 9.95158
[1mStep[0m  [8/42], [94mLoss[0m : 9.91390
[1mStep[0m  [12/42], [94mLoss[0m : 9.91961
[1mStep[0m  [16/42], [94mLoss[0m : 10.00844
[1mStep[0m  [20/42], [94mLoss[0m : 9.48992
[1mStep[0m  [24/42], [94mLoss[0m : 9.42568
[1mStep[0m  [28/42], [94mLoss[0m : 9.73549
[1mStep[0m  [32/42], [94mLoss[0m : 9.47664
[1mStep[0m  [36/42], [94mLoss[0m : 9.62732
[1mStep[0m  [40/42], [94mLoss[0m : 9.69211

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.800, [92mTest[0m: 9.632, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.49318
[1mStep[0m  [4/42], [94mLoss[0m : 10.20624
[1mStep[0m  [8/42], [94mLoss[0m : 9.53392
[1mStep[0m  [12/42], [94mLoss[0m : 9.62140
[1mStep[0m  [16/42], [94mLoss[0m : 9.81477
[1mStep[0m  [20/42], [94mLoss[0m : 9.88346
[1mStep[0m  [24/42], [94mLoss[0m : 9.92253
[1mStep[0m  [28/42], [94mLoss[0m : 9.86978
[1mStep[0m  [32/42], [94mLoss[0m : 9.63802
[1mStep[0m  [36/42], [94mLoss[0m : 9.97483
[1mStep[0m  [40/42], [94mLoss[0m : 10.00708

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.766, [92mTest[0m: 9.634, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.74907
[1mStep[0m  [4/42], [94mLoss[0m : 9.69968
[1mStep[0m  [8/42], [94mLoss[0m : 9.88448
[1mStep[0m  [12/42], [94mLoss[0m : 9.60184
[1mStep[0m  [16/42], [94mLoss[0m : 9.95926
[1mStep[0m  [20/42], [94mLoss[0m : 9.86040
[1mStep[0m  [24/42], [94mLoss[0m : 9.79298
[1mStep[0m  [28/42], [94mLoss[0m : 9.46590
[1mStep[0m  [32/42], [94mLoss[0m : 9.52168
[1mStep[0m  [36/42], [94mLoss[0m : 10.29331
[1mStep[0m  [40/42], [94mLoss[0m : 9.46069

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.740, [92mTest[0m: 9.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.96833
[1mStep[0m  [4/42], [94mLoss[0m : 9.80904
[1mStep[0m  [8/42], [94mLoss[0m : 9.50227
[1mStep[0m  [12/42], [94mLoss[0m : 9.74066
[1mStep[0m  [16/42], [94mLoss[0m : 9.62474
[1mStep[0m  [20/42], [94mLoss[0m : 9.72834
[1mStep[0m  [24/42], [94mLoss[0m : 9.98664
[1mStep[0m  [28/42], [94mLoss[0m : 9.57935
[1mStep[0m  [32/42], [94mLoss[0m : 10.15044
[1mStep[0m  [36/42], [94mLoss[0m : 9.31859
[1mStep[0m  [40/42], [94mLoss[0m : 9.56332

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.704, [92mTest[0m: 9.533, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.84365
[1mStep[0m  [4/42], [94mLoss[0m : 9.49506
[1mStep[0m  [8/42], [94mLoss[0m : 9.63022
[1mStep[0m  [12/42], [94mLoss[0m : 9.84832
[1mStep[0m  [16/42], [94mLoss[0m : 9.90120
[1mStep[0m  [20/42], [94mLoss[0m : 9.81277
[1mStep[0m  [24/42], [94mLoss[0m : 9.64174
[1mStep[0m  [28/42], [94mLoss[0m : 9.59412
[1mStep[0m  [32/42], [94mLoss[0m : 9.70452
[1mStep[0m  [36/42], [94mLoss[0m : 9.64397
[1mStep[0m  [40/42], [94mLoss[0m : 10.04070

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.677, [92mTest[0m: 9.452, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.25575
[1mStep[0m  [4/42], [94mLoss[0m : 10.00312
[1mStep[0m  [8/42], [94mLoss[0m : 9.58029
[1mStep[0m  [12/42], [94mLoss[0m : 9.87298
[1mStep[0m  [16/42], [94mLoss[0m : 9.55903
[1mStep[0m  [20/42], [94mLoss[0m : 9.55357
[1mStep[0m  [24/42], [94mLoss[0m : 9.79941
[1mStep[0m  [28/42], [94mLoss[0m : 9.49050
[1mStep[0m  [32/42], [94mLoss[0m : 9.68536
[1mStep[0m  [36/42], [94mLoss[0m : 9.90476
[1mStep[0m  [40/42], [94mLoss[0m : 9.77127

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.641, [92mTest[0m: 9.483, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.34526
[1mStep[0m  [4/42], [94mLoss[0m : 9.79372
[1mStep[0m  [8/42], [94mLoss[0m : 9.43820
[1mStep[0m  [12/42], [94mLoss[0m : 9.54418
[1mStep[0m  [16/42], [94mLoss[0m : 9.85971
[1mStep[0m  [20/42], [94mLoss[0m : 9.35251
[1mStep[0m  [24/42], [94mLoss[0m : 9.66500
[1mStep[0m  [28/42], [94mLoss[0m : 9.70133
[1mStep[0m  [32/42], [94mLoss[0m : 9.27410
[1mStep[0m  [36/42], [94mLoss[0m : 9.80387
[1mStep[0m  [40/42], [94mLoss[0m : 9.46915

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.620, [92mTest[0m: 9.381, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.61220
[1mStep[0m  [4/42], [94mLoss[0m : 9.50169
[1mStep[0m  [8/42], [94mLoss[0m : 9.62997
[1mStep[0m  [12/42], [94mLoss[0m : 9.45817
[1mStep[0m  [16/42], [94mLoss[0m : 9.68365
[1mStep[0m  [20/42], [94mLoss[0m : 9.62380
[1mStep[0m  [24/42], [94mLoss[0m : 9.79691
[1mStep[0m  [28/42], [94mLoss[0m : 9.52942
[1mStep[0m  [32/42], [94mLoss[0m : 9.47921
[1mStep[0m  [36/42], [94mLoss[0m : 9.22534
[1mStep[0m  [40/42], [94mLoss[0m : 9.25068

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.595, [92mTest[0m: 9.426, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.274
====================================

Phase 2 - Evaluation MAE:  9.27405309677124
MAE score P1      10.344989
MAE score P2       9.274053
loss               9.595103
learning_rate        0.0001
batch_size              256
hidden_sizes          [300]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.47543
[1mStep[0m  [16/169], [94mLoss[0m : 10.97510
[1mStep[0m  [32/169], [94mLoss[0m : 10.68583
[1mStep[0m  [48/169], [94mLoss[0m : 11.28409
[1mStep[0m  [64/169], [94mLoss[0m : 11.01625
[1mStep[0m  [80/169], [94mLoss[0m : 10.59649
[1mStep[0m  [96/169], [94mLoss[0m : 10.72539
[1mStep[0m  [112/169], [94mLoss[0m : 11.25616
[1mStep[0m  [128/169], [94mLoss[0m : 11.00415
[1mStep[0m  [144/169], [94mLoss[0m : 10.96441
[1mStep[0m  [160/169], [94mLoss[0m : 10.77953

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.806, [92mTest[0m: 10.882, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.28691
[1mStep[0m  [16/169], [94mLoss[0m : 10.66811
[1mStep[0m  [32/169], [94mLoss[0m : 10.69589
[1mStep[0m  [48/169], [94mLoss[0m : 11.00561
[1mStep[0m  [64/169], [94mLoss[0m : 10.59077
[1mStep[0m  [80/169], [94mLoss[0m : 10.35538
[1mStep[0m  [96/169], [94mLoss[0m : 10.43118
[1mStep[0m  [112/169], [94mLoss[0m : 10.21963
[1mStep[0m  [128/169], [94mLoss[0m : 10.64775
[1mStep[0m  [144/169], [94mLoss[0m : 10.76548
[1mStep[0m  [160/169], [94mLoss[0m : 11.70333

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.777, [92mTest[0m: 10.742, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.68061
[1mStep[0m  [16/169], [94mLoss[0m : 11.05649
[1mStep[0m  [32/169], [94mLoss[0m : 11.23787
[1mStep[0m  [48/169], [94mLoss[0m : 10.53716
[1mStep[0m  [64/169], [94mLoss[0m : 11.12373
[1mStep[0m  [80/169], [94mLoss[0m : 10.50537
[1mStep[0m  [96/169], [94mLoss[0m : 10.71163
[1mStep[0m  [112/169], [94mLoss[0m : 11.41071
[1mStep[0m  [128/169], [94mLoss[0m : 10.68729
[1mStep[0m  [144/169], [94mLoss[0m : 11.17985
[1mStep[0m  [160/169], [94mLoss[0m : 9.98710

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.762, [92mTest[0m: 10.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.75415
[1mStep[0m  [16/169], [94mLoss[0m : 10.08303
[1mStep[0m  [32/169], [94mLoss[0m : 10.67834
[1mStep[0m  [48/169], [94mLoss[0m : 10.02487
[1mStep[0m  [64/169], [94mLoss[0m : 10.99773
[1mStep[0m  [80/169], [94mLoss[0m : 10.87521
[1mStep[0m  [96/169], [94mLoss[0m : 11.19180
[1mStep[0m  [112/169], [94mLoss[0m : 10.77838
[1mStep[0m  [128/169], [94mLoss[0m : 11.52186
[1mStep[0m  [144/169], [94mLoss[0m : 10.93562
[1mStep[0m  [160/169], [94mLoss[0m : 11.04703

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.736, [92mTest[0m: 10.702, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.21141
[1mStep[0m  [16/169], [94mLoss[0m : 10.32963
[1mStep[0m  [32/169], [94mLoss[0m : 11.29674
[1mStep[0m  [48/169], [94mLoss[0m : 10.96946
[1mStep[0m  [64/169], [94mLoss[0m : 10.56467
[1mStep[0m  [80/169], [94mLoss[0m : 10.57313
[1mStep[0m  [96/169], [94mLoss[0m : 10.33932
[1mStep[0m  [112/169], [94mLoss[0m : 10.95352
[1mStep[0m  [128/169], [94mLoss[0m : 9.99885
[1mStep[0m  [144/169], [94mLoss[0m : 11.49665
[1mStep[0m  [160/169], [94mLoss[0m : 11.08977

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.45647
[1mStep[0m  [16/169], [94mLoss[0m : 11.03591
[1mStep[0m  [32/169], [94mLoss[0m : 9.56150
[1mStep[0m  [48/169], [94mLoss[0m : 10.87185
[1mStep[0m  [64/169], [94mLoss[0m : 10.78422
[1mStep[0m  [80/169], [94mLoss[0m : 10.14888
[1mStep[0m  [96/169], [94mLoss[0m : 11.48769
[1mStep[0m  [112/169], [94mLoss[0m : 10.93231
[1mStep[0m  [128/169], [94mLoss[0m : 10.12017
[1mStep[0m  [144/169], [94mLoss[0m : 10.15171
[1mStep[0m  [160/169], [94mLoss[0m : 10.76005

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.689, [92mTest[0m: 10.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.02865
[1mStep[0m  [16/169], [94mLoss[0m : 10.86813
[1mStep[0m  [32/169], [94mLoss[0m : 9.61358
[1mStep[0m  [48/169], [94mLoss[0m : 10.84320
[1mStep[0m  [64/169], [94mLoss[0m : 9.99068
[1mStep[0m  [80/169], [94mLoss[0m : 10.63116
[1mStep[0m  [96/169], [94mLoss[0m : 10.54872
[1mStep[0m  [112/169], [94mLoss[0m : 10.95661
[1mStep[0m  [128/169], [94mLoss[0m : 10.30550
[1mStep[0m  [144/169], [94mLoss[0m : 10.67030
[1mStep[0m  [160/169], [94mLoss[0m : 10.86520

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.667, [92mTest[0m: 10.598, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.30420
[1mStep[0m  [16/169], [94mLoss[0m : 10.81927
[1mStep[0m  [32/169], [94mLoss[0m : 10.05697
[1mStep[0m  [48/169], [94mLoss[0m : 10.13229
[1mStep[0m  [64/169], [94mLoss[0m : 10.19665
[1mStep[0m  [80/169], [94mLoss[0m : 10.69647
[1mStep[0m  [96/169], [94mLoss[0m : 10.26042
[1mStep[0m  [112/169], [94mLoss[0m : 10.62831
[1mStep[0m  [128/169], [94mLoss[0m : 10.10062
[1mStep[0m  [144/169], [94mLoss[0m : 10.35198
[1mStep[0m  [160/169], [94mLoss[0m : 10.60311

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.579, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.83127
[1mStep[0m  [16/169], [94mLoss[0m : 10.97790
[1mStep[0m  [32/169], [94mLoss[0m : 10.72160
[1mStep[0m  [48/169], [94mLoss[0m : 10.53117
[1mStep[0m  [64/169], [94mLoss[0m : 11.36598
[1mStep[0m  [80/169], [94mLoss[0m : 11.22335
[1mStep[0m  [96/169], [94mLoss[0m : 10.81620
[1mStep[0m  [112/169], [94mLoss[0m : 10.49042
[1mStep[0m  [128/169], [94mLoss[0m : 10.67143
[1mStep[0m  [144/169], [94mLoss[0m : 10.94597
[1mStep[0m  [160/169], [94mLoss[0m : 10.36411

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.628, [92mTest[0m: 10.535, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.31714
[1mStep[0m  [16/169], [94mLoss[0m : 10.23324
[1mStep[0m  [32/169], [94mLoss[0m : 10.44261
[1mStep[0m  [48/169], [94mLoss[0m : 10.48222
[1mStep[0m  [64/169], [94mLoss[0m : 9.98491
[1mStep[0m  [80/169], [94mLoss[0m : 10.04543
[1mStep[0m  [96/169], [94mLoss[0m : 10.08683
[1mStep[0m  [112/169], [94mLoss[0m : 10.71208
[1mStep[0m  [128/169], [94mLoss[0m : 11.10168
[1mStep[0m  [144/169], [94mLoss[0m : 10.69600
[1mStep[0m  [160/169], [94mLoss[0m : 10.53394

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.531, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.17116
[1mStep[0m  [16/169], [94mLoss[0m : 11.00680
[1mStep[0m  [32/169], [94mLoss[0m : 11.51161
[1mStep[0m  [48/169], [94mLoss[0m : 10.42668
[1mStep[0m  [64/169], [94mLoss[0m : 11.28007
[1mStep[0m  [80/169], [94mLoss[0m : 10.64605
[1mStep[0m  [96/169], [94mLoss[0m : 10.56199
[1mStep[0m  [112/169], [94mLoss[0m : 11.00718
[1mStep[0m  [128/169], [94mLoss[0m : 10.07899
[1mStep[0m  [144/169], [94mLoss[0m : 10.55811
[1mStep[0m  [160/169], [94mLoss[0m : 10.04837

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.577, [92mTest[0m: 10.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.72355
[1mStep[0m  [16/169], [94mLoss[0m : 10.35318
[1mStep[0m  [32/169], [94mLoss[0m : 10.96602
[1mStep[0m  [48/169], [94mLoss[0m : 11.03072
[1mStep[0m  [64/169], [94mLoss[0m : 10.34558
[1mStep[0m  [80/169], [94mLoss[0m : 10.65751
[1mStep[0m  [96/169], [94mLoss[0m : 10.44167
[1mStep[0m  [112/169], [94mLoss[0m : 10.37438
[1mStep[0m  [128/169], [94mLoss[0m : 10.65646
[1mStep[0m  [144/169], [94mLoss[0m : 10.81317
[1mStep[0m  [160/169], [94mLoss[0m : 10.36378

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.565, [92mTest[0m: 10.473, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.75961
[1mStep[0m  [16/169], [94mLoss[0m : 10.83197
[1mStep[0m  [32/169], [94mLoss[0m : 10.54577
[1mStep[0m  [48/169], [94mLoss[0m : 11.03454
[1mStep[0m  [64/169], [94mLoss[0m : 10.91503
[1mStep[0m  [80/169], [94mLoss[0m : 9.99412
[1mStep[0m  [96/169], [94mLoss[0m : 10.19835
[1mStep[0m  [112/169], [94mLoss[0m : 10.21541
[1mStep[0m  [128/169], [94mLoss[0m : 11.34335
[1mStep[0m  [144/169], [94mLoss[0m : 11.65144
[1mStep[0m  [160/169], [94mLoss[0m : 10.00462

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.533, [92mTest[0m: 10.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.74093
[1mStep[0m  [16/169], [94mLoss[0m : 10.32214
[1mStep[0m  [32/169], [94mLoss[0m : 11.01361
[1mStep[0m  [48/169], [94mLoss[0m : 10.50500
[1mStep[0m  [64/169], [94mLoss[0m : 10.63432
[1mStep[0m  [80/169], [94mLoss[0m : 10.90369
[1mStep[0m  [96/169], [94mLoss[0m : 10.57148
[1mStep[0m  [112/169], [94mLoss[0m : 10.44389
[1mStep[0m  [128/169], [94mLoss[0m : 10.51531
[1mStep[0m  [144/169], [94mLoss[0m : 10.48994
[1mStep[0m  [160/169], [94mLoss[0m : 10.56322

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.514, [92mTest[0m: 10.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.74014
[1mStep[0m  [16/169], [94mLoss[0m : 10.49699
[1mStep[0m  [32/169], [94mLoss[0m : 11.33305
[1mStep[0m  [48/169], [94mLoss[0m : 11.01426
[1mStep[0m  [64/169], [94mLoss[0m : 11.23074
[1mStep[0m  [80/169], [94mLoss[0m : 10.60296
[1mStep[0m  [96/169], [94mLoss[0m : 9.90163
[1mStep[0m  [112/169], [94mLoss[0m : 10.26122
[1mStep[0m  [128/169], [94mLoss[0m : 10.38131
[1mStep[0m  [144/169], [94mLoss[0m : 10.84918
[1mStep[0m  [160/169], [94mLoss[0m : 9.90858

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.494, [92mTest[0m: 10.377, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.20500
[1mStep[0m  [16/169], [94mLoss[0m : 11.25881
[1mStep[0m  [32/169], [94mLoss[0m : 10.84201
[1mStep[0m  [48/169], [94mLoss[0m : 11.16639
[1mStep[0m  [64/169], [94mLoss[0m : 10.08328
[1mStep[0m  [80/169], [94mLoss[0m : 10.32246
[1mStep[0m  [96/169], [94mLoss[0m : 9.99586
[1mStep[0m  [112/169], [94mLoss[0m : 10.59993
[1mStep[0m  [128/169], [94mLoss[0m : 10.49684
[1mStep[0m  [144/169], [94mLoss[0m : 11.47790
[1mStep[0m  [160/169], [94mLoss[0m : 10.56069

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.463, [92mTest[0m: 10.359, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.38432
[1mStep[0m  [16/169], [94mLoss[0m : 10.43983
[1mStep[0m  [32/169], [94mLoss[0m : 10.14007
[1mStep[0m  [48/169], [94mLoss[0m : 10.56727
[1mStep[0m  [64/169], [94mLoss[0m : 10.21269
[1mStep[0m  [80/169], [94mLoss[0m : 10.86683
[1mStep[0m  [96/169], [94mLoss[0m : 11.01959
[1mStep[0m  [112/169], [94mLoss[0m : 10.83648
[1mStep[0m  [128/169], [94mLoss[0m : 10.93532
[1mStep[0m  [144/169], [94mLoss[0m : 10.29866
[1mStep[0m  [160/169], [94mLoss[0m : 10.52396

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.443, [92mTest[0m: 10.346, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.02123
[1mStep[0m  [16/169], [94mLoss[0m : 10.50209
[1mStep[0m  [32/169], [94mLoss[0m : 10.17759
[1mStep[0m  [48/169], [94mLoss[0m : 11.36480
[1mStep[0m  [64/169], [94mLoss[0m : 10.16931
[1mStep[0m  [80/169], [94mLoss[0m : 10.74776
[1mStep[0m  [96/169], [94mLoss[0m : 9.94472
[1mStep[0m  [112/169], [94mLoss[0m : 10.62483
[1mStep[0m  [128/169], [94mLoss[0m : 9.74955
[1mStep[0m  [144/169], [94mLoss[0m : 10.55842
[1mStep[0m  [160/169], [94mLoss[0m : 10.36329

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.415, [92mTest[0m: 10.293, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.78534
[1mStep[0m  [16/169], [94mLoss[0m : 10.47077
[1mStep[0m  [32/169], [94mLoss[0m : 9.09746
[1mStep[0m  [48/169], [94mLoss[0m : 10.65237
[1mStep[0m  [64/169], [94mLoss[0m : 11.10425
[1mStep[0m  [80/169], [94mLoss[0m : 10.17930
[1mStep[0m  [96/169], [94mLoss[0m : 10.33601
[1mStep[0m  [112/169], [94mLoss[0m : 10.05069
[1mStep[0m  [128/169], [94mLoss[0m : 10.42224
[1mStep[0m  [144/169], [94mLoss[0m : 9.98110
[1mStep[0m  [160/169], [94mLoss[0m : 10.06245

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.398, [92mTest[0m: 10.291, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.47910
[1mStep[0m  [16/169], [94mLoss[0m : 11.33300
[1mStep[0m  [32/169], [94mLoss[0m : 10.18609
[1mStep[0m  [48/169], [94mLoss[0m : 10.12397
[1mStep[0m  [64/169], [94mLoss[0m : 10.19473
[1mStep[0m  [80/169], [94mLoss[0m : 10.09502
[1mStep[0m  [96/169], [94mLoss[0m : 10.38608
[1mStep[0m  [112/169], [94mLoss[0m : 10.18286
[1mStep[0m  [128/169], [94mLoss[0m : 11.25871
[1mStep[0m  [144/169], [94mLoss[0m : 10.78146
[1mStep[0m  [160/169], [94mLoss[0m : 10.50638

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.372, [92mTest[0m: 10.255, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.52650
[1mStep[0m  [16/169], [94mLoss[0m : 10.50738
[1mStep[0m  [32/169], [94mLoss[0m : 10.04579
[1mStep[0m  [48/169], [94mLoss[0m : 10.50367
[1mStep[0m  [64/169], [94mLoss[0m : 10.67177
[1mStep[0m  [80/169], [94mLoss[0m : 9.53981
[1mStep[0m  [96/169], [94mLoss[0m : 9.77359
[1mStep[0m  [112/169], [94mLoss[0m : 10.23226
[1mStep[0m  [128/169], [94mLoss[0m : 10.07340
[1mStep[0m  [144/169], [94mLoss[0m : 9.69661
[1mStep[0m  [160/169], [94mLoss[0m : 10.39811

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.352, [92mTest[0m: 10.198, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.25364
[1mStep[0m  [16/169], [94mLoss[0m : 10.07908
[1mStep[0m  [32/169], [94mLoss[0m : 10.54501
[1mStep[0m  [48/169], [94mLoss[0m : 9.66780
[1mStep[0m  [64/169], [94mLoss[0m : 10.00585
[1mStep[0m  [80/169], [94mLoss[0m : 10.50264
[1mStep[0m  [96/169], [94mLoss[0m : 9.65755
[1mStep[0m  [112/169], [94mLoss[0m : 9.97539
[1mStep[0m  [128/169], [94mLoss[0m : 10.51840
[1mStep[0m  [144/169], [94mLoss[0m : 9.78129
[1mStep[0m  [160/169], [94mLoss[0m : 10.51769

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.328, [92mTest[0m: 10.232, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.96526
[1mStep[0m  [16/169], [94mLoss[0m : 10.29997
[1mStep[0m  [32/169], [94mLoss[0m : 10.38456
[1mStep[0m  [48/169], [94mLoss[0m : 10.14041
[1mStep[0m  [64/169], [94mLoss[0m : 10.69894
[1mStep[0m  [80/169], [94mLoss[0m : 11.13201
[1mStep[0m  [96/169], [94mLoss[0m : 10.35070
[1mStep[0m  [112/169], [94mLoss[0m : 10.11332
[1mStep[0m  [128/169], [94mLoss[0m : 10.24788
[1mStep[0m  [144/169], [94mLoss[0m : 10.51775
[1mStep[0m  [160/169], [94mLoss[0m : 10.22296

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.311, [92mTest[0m: 10.131, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.11866
[1mStep[0m  [16/169], [94mLoss[0m : 11.18806
[1mStep[0m  [32/169], [94mLoss[0m : 10.49967
[1mStep[0m  [48/169], [94mLoss[0m : 10.50172
[1mStep[0m  [64/169], [94mLoss[0m : 9.98903
[1mStep[0m  [80/169], [94mLoss[0m : 10.41950
[1mStep[0m  [96/169], [94mLoss[0m : 10.06474
[1mStep[0m  [112/169], [94mLoss[0m : 9.44867
[1mStep[0m  [128/169], [94mLoss[0m : 10.68891
[1mStep[0m  [144/169], [94mLoss[0m : 10.89360
[1mStep[0m  [160/169], [94mLoss[0m : 10.88964

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.281, [92mTest[0m: 10.138, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.17606
[1mStep[0m  [16/169], [94mLoss[0m : 10.47526
[1mStep[0m  [32/169], [94mLoss[0m : 9.93403
[1mStep[0m  [48/169], [94mLoss[0m : 11.50492
[1mStep[0m  [64/169], [94mLoss[0m : 10.93789
[1mStep[0m  [80/169], [94mLoss[0m : 9.94373
[1mStep[0m  [96/169], [94mLoss[0m : 10.19278
[1mStep[0m  [112/169], [94mLoss[0m : 9.44671
[1mStep[0m  [128/169], [94mLoss[0m : 10.60905
[1mStep[0m  [144/169], [94mLoss[0m : 10.08390
[1mStep[0m  [160/169], [94mLoss[0m : 10.11622

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.260, [92mTest[0m: 10.049, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.81405
[1mStep[0m  [16/169], [94mLoss[0m : 9.99785
[1mStep[0m  [32/169], [94mLoss[0m : 9.31075
[1mStep[0m  [48/169], [94mLoss[0m : 9.72361
[1mStep[0m  [64/169], [94mLoss[0m : 9.59329
[1mStep[0m  [80/169], [94mLoss[0m : 10.31133
[1mStep[0m  [96/169], [94mLoss[0m : 9.69174
[1mStep[0m  [112/169], [94mLoss[0m : 10.07834
[1mStep[0m  [128/169], [94mLoss[0m : 10.02189
[1mStep[0m  [144/169], [94mLoss[0m : 10.54698
[1mStep[0m  [160/169], [94mLoss[0m : 10.04238

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.252, [92mTest[0m: 10.160, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.39175
[1mStep[0m  [16/169], [94mLoss[0m : 10.07511
[1mStep[0m  [32/169], [94mLoss[0m : 10.42936
[1mStep[0m  [48/169], [94mLoss[0m : 10.66311
[1mStep[0m  [64/169], [94mLoss[0m : 9.73761
[1mStep[0m  [80/169], [94mLoss[0m : 9.99433
[1mStep[0m  [96/169], [94mLoss[0m : 9.61255
[1mStep[0m  [112/169], [94mLoss[0m : 10.52826
[1mStep[0m  [128/169], [94mLoss[0m : 9.95578
[1mStep[0m  [144/169], [94mLoss[0m : 9.84804
[1mStep[0m  [160/169], [94mLoss[0m : 9.82720

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.223, [92mTest[0m: 10.063, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.17299
[1mStep[0m  [16/169], [94mLoss[0m : 9.41493
[1mStep[0m  [32/169], [94mLoss[0m : 10.09376
[1mStep[0m  [48/169], [94mLoss[0m : 10.35651
[1mStep[0m  [64/169], [94mLoss[0m : 10.40802
[1mStep[0m  [80/169], [94mLoss[0m : 9.82167
[1mStep[0m  [96/169], [94mLoss[0m : 11.16416
[1mStep[0m  [112/169], [94mLoss[0m : 10.34006
[1mStep[0m  [128/169], [94mLoss[0m : 10.26233
[1mStep[0m  [144/169], [94mLoss[0m : 9.65324
[1mStep[0m  [160/169], [94mLoss[0m : 10.32097

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.203, [92mTest[0m: 10.062, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.91375
[1mStep[0m  [16/169], [94mLoss[0m : 9.91668
[1mStep[0m  [32/169], [94mLoss[0m : 9.83763
[1mStep[0m  [48/169], [94mLoss[0m : 10.27489
[1mStep[0m  [64/169], [94mLoss[0m : 10.32831
[1mStep[0m  [80/169], [94mLoss[0m : 10.13883
[1mStep[0m  [96/169], [94mLoss[0m : 10.18463
[1mStep[0m  [112/169], [94mLoss[0m : 10.21291
[1mStep[0m  [128/169], [94mLoss[0m : 10.42394
[1mStep[0m  [144/169], [94mLoss[0m : 10.27282
[1mStep[0m  [160/169], [94mLoss[0m : 9.96046

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.188, [92mTest[0m: 10.029, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.88319
[1mStep[0m  [16/169], [94mLoss[0m : 9.58209
[1mStep[0m  [32/169], [94mLoss[0m : 10.74861
[1mStep[0m  [48/169], [94mLoss[0m : 9.04505
[1mStep[0m  [64/169], [94mLoss[0m : 10.31764
[1mStep[0m  [80/169], [94mLoss[0m : 8.88099
[1mStep[0m  [96/169], [94mLoss[0m : 10.31585
[1mStep[0m  [112/169], [94mLoss[0m : 9.57362
[1mStep[0m  [128/169], [94mLoss[0m : 10.22862
[1mStep[0m  [144/169], [94mLoss[0m : 9.84332
[1mStep[0m  [160/169], [94mLoss[0m : 10.23424

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.161, [92mTest[0m: 10.007, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.927
====================================

Phase 1 - Evaluation MAE:  9.926653623580933
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.07776
[1mStep[0m  [16/169], [94mLoss[0m : 9.82904
[1mStep[0m  [32/169], [94mLoss[0m : 10.49283
[1mStep[0m  [48/169], [94mLoss[0m : 9.66722
[1mStep[0m  [64/169], [94mLoss[0m : 10.26720
[1mStep[0m  [80/169], [94mLoss[0m : 10.33185
[1mStep[0m  [96/169], [94mLoss[0m : 10.28981
[1mStep[0m  [112/169], [94mLoss[0m : 10.47886
[1mStep[0m  [128/169], [94mLoss[0m : 10.54738
[1mStep[0m  [144/169], [94mLoss[0m : 9.03354
[1mStep[0m  [160/169], [94mLoss[0m : 9.98675

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.136, [92mTest[0m: 9.941, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.29131
[1mStep[0m  [16/169], [94mLoss[0m : 9.53019
[1mStep[0m  [32/169], [94mLoss[0m : 10.50738
[1mStep[0m  [48/169], [94mLoss[0m : 9.00874
[1mStep[0m  [64/169], [94mLoss[0m : 10.48607
[1mStep[0m  [80/169], [94mLoss[0m : 10.15285
[1mStep[0m  [96/169], [94mLoss[0m : 9.46410
[1mStep[0m  [112/169], [94mLoss[0m : 9.79411
[1mStep[0m  [128/169], [94mLoss[0m : 10.17793
[1mStep[0m  [144/169], [94mLoss[0m : 10.21779
[1mStep[0m  [160/169], [94mLoss[0m : 8.98199

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.122, [92mTest[0m: 9.959, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.31645
[1mStep[0m  [16/169], [94mLoss[0m : 10.79150
[1mStep[0m  [32/169], [94mLoss[0m : 10.60283
[1mStep[0m  [48/169], [94mLoss[0m : 9.73569
[1mStep[0m  [64/169], [94mLoss[0m : 10.12151
[1mStep[0m  [80/169], [94mLoss[0m : 9.36954
[1mStep[0m  [96/169], [94mLoss[0m : 10.31199
[1mStep[0m  [112/169], [94mLoss[0m : 9.80389
[1mStep[0m  [128/169], [94mLoss[0m : 10.02950
[1mStep[0m  [144/169], [94mLoss[0m : 10.67950
[1mStep[0m  [160/169], [94mLoss[0m : 9.97101

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.084, [92mTest[0m: 9.929, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.75927
[1mStep[0m  [16/169], [94mLoss[0m : 9.41021
[1mStep[0m  [32/169], [94mLoss[0m : 9.81402
[1mStep[0m  [48/169], [94mLoss[0m : 9.92765
[1mStep[0m  [64/169], [94mLoss[0m : 9.59766
[1mStep[0m  [80/169], [94mLoss[0m : 9.60398
[1mStep[0m  [96/169], [94mLoss[0m : 9.89692
[1mStep[0m  [112/169], [94mLoss[0m : 9.84547
[1mStep[0m  [128/169], [94mLoss[0m : 9.76684
[1mStep[0m  [144/169], [94mLoss[0m : 9.62745
[1mStep[0m  [160/169], [94mLoss[0m : 9.90870

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.051, [92mTest[0m: 9.894, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.20755
[1mStep[0m  [16/169], [94mLoss[0m : 9.32037
[1mStep[0m  [32/169], [94mLoss[0m : 9.50260
[1mStep[0m  [48/169], [94mLoss[0m : 10.32329
[1mStep[0m  [64/169], [94mLoss[0m : 9.94118
[1mStep[0m  [80/169], [94mLoss[0m : 10.17278
[1mStep[0m  [96/169], [94mLoss[0m : 9.95173
[1mStep[0m  [112/169], [94mLoss[0m : 9.58387
[1mStep[0m  [128/169], [94mLoss[0m : 9.86017
[1mStep[0m  [144/169], [94mLoss[0m : 10.14865
[1mStep[0m  [160/169], [94mLoss[0m : 10.04287

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.028, [92mTest[0m: 9.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.37403
[1mStep[0m  [16/169], [94mLoss[0m : 10.92161
[1mStep[0m  [32/169], [94mLoss[0m : 10.21092
[1mStep[0m  [48/169], [94mLoss[0m : 9.82767
[1mStep[0m  [64/169], [94mLoss[0m : 10.34409
[1mStep[0m  [80/169], [94mLoss[0m : 9.88421
[1mStep[0m  [96/169], [94mLoss[0m : 10.62164
[1mStep[0m  [112/169], [94mLoss[0m : 10.79376
[1mStep[0m  [128/169], [94mLoss[0m : 10.40284
[1mStep[0m  [144/169], [94mLoss[0m : 9.35525
[1mStep[0m  [160/169], [94mLoss[0m : 10.33580

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.001, [92mTest[0m: 9.781, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.52534
[1mStep[0m  [16/169], [94mLoss[0m : 9.98147
[1mStep[0m  [32/169], [94mLoss[0m : 9.59367
[1mStep[0m  [48/169], [94mLoss[0m : 9.70400
[1mStep[0m  [64/169], [94mLoss[0m : 10.42873
[1mStep[0m  [80/169], [94mLoss[0m : 10.18913
[1mStep[0m  [96/169], [94mLoss[0m : 9.16665
[1mStep[0m  [112/169], [94mLoss[0m : 9.87499
[1mStep[0m  [128/169], [94mLoss[0m : 9.70836
[1mStep[0m  [144/169], [94mLoss[0m : 9.12102
[1mStep[0m  [160/169], [94mLoss[0m : 10.19248

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.978, [92mTest[0m: 9.720, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.78673
[1mStep[0m  [16/169], [94mLoss[0m : 10.49798
[1mStep[0m  [32/169], [94mLoss[0m : 10.48684
[1mStep[0m  [48/169], [94mLoss[0m : 10.24592
[1mStep[0m  [64/169], [94mLoss[0m : 9.47585
[1mStep[0m  [80/169], [94mLoss[0m : 10.09644
[1mStep[0m  [96/169], [94mLoss[0m : 9.73715
[1mStep[0m  [112/169], [94mLoss[0m : 10.00483
[1mStep[0m  [128/169], [94mLoss[0m : 9.43001
[1mStep[0m  [144/169], [94mLoss[0m : 9.45210
[1mStep[0m  [160/169], [94mLoss[0m : 9.97743

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.940, [92mTest[0m: 9.720, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.13127
[1mStep[0m  [16/169], [94mLoss[0m : 10.30611
[1mStep[0m  [32/169], [94mLoss[0m : 10.29227
[1mStep[0m  [48/169], [94mLoss[0m : 9.22357
[1mStep[0m  [64/169], [94mLoss[0m : 9.60334
[1mStep[0m  [80/169], [94mLoss[0m : 9.98687
[1mStep[0m  [96/169], [94mLoss[0m : 9.37949
[1mStep[0m  [112/169], [94mLoss[0m : 9.94442
[1mStep[0m  [128/169], [94mLoss[0m : 8.91521
[1mStep[0m  [144/169], [94mLoss[0m : 10.09101
[1mStep[0m  [160/169], [94mLoss[0m : 9.05297

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.911, [92mTest[0m: 9.725, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.85187
[1mStep[0m  [16/169], [94mLoss[0m : 9.44873
[1mStep[0m  [32/169], [94mLoss[0m : 9.32513
[1mStep[0m  [48/169], [94mLoss[0m : 9.59106
[1mStep[0m  [64/169], [94mLoss[0m : 10.03023
[1mStep[0m  [80/169], [94mLoss[0m : 9.64381
[1mStep[0m  [96/169], [94mLoss[0m : 10.13383
[1mStep[0m  [112/169], [94mLoss[0m : 9.66713
[1mStep[0m  [128/169], [94mLoss[0m : 9.93574
[1mStep[0m  [144/169], [94mLoss[0m : 9.96318
[1mStep[0m  [160/169], [94mLoss[0m : 9.79196

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.880, [92mTest[0m: 9.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.65617
[1mStep[0m  [16/169], [94mLoss[0m : 10.00664
[1mStep[0m  [32/169], [94mLoss[0m : 10.11851
[1mStep[0m  [48/169], [94mLoss[0m : 9.93002
[1mStep[0m  [64/169], [94mLoss[0m : 10.16099
[1mStep[0m  [80/169], [94mLoss[0m : 9.76010
[1mStep[0m  [96/169], [94mLoss[0m : 9.43146
[1mStep[0m  [112/169], [94mLoss[0m : 9.36579
[1mStep[0m  [128/169], [94mLoss[0m : 9.86258
[1mStep[0m  [144/169], [94mLoss[0m : 9.81764
[1mStep[0m  [160/169], [94mLoss[0m : 9.18688

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.849, [92mTest[0m: 9.609, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.88736
[1mStep[0m  [16/169], [94mLoss[0m : 10.22144
[1mStep[0m  [32/169], [94mLoss[0m : 10.28316
[1mStep[0m  [48/169], [94mLoss[0m : 9.95514
[1mStep[0m  [64/169], [94mLoss[0m : 8.93225
[1mStep[0m  [80/169], [94mLoss[0m : 10.01062
[1mStep[0m  [96/169], [94mLoss[0m : 10.26725
[1mStep[0m  [112/169], [94mLoss[0m : 10.49338
[1mStep[0m  [128/169], [94mLoss[0m : 10.23181
[1mStep[0m  [144/169], [94mLoss[0m : 10.14752
[1mStep[0m  [160/169], [94mLoss[0m : 10.03226

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.818, [92mTest[0m: 9.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.00231
[1mStep[0m  [16/169], [94mLoss[0m : 9.91099
[1mStep[0m  [32/169], [94mLoss[0m : 9.59882
[1mStep[0m  [48/169], [94mLoss[0m : 9.53573
[1mStep[0m  [64/169], [94mLoss[0m : 10.68017
[1mStep[0m  [80/169], [94mLoss[0m : 10.34517
[1mStep[0m  [96/169], [94mLoss[0m : 9.82773
[1mStep[0m  [112/169], [94mLoss[0m : 9.80902
[1mStep[0m  [128/169], [94mLoss[0m : 10.42684
[1mStep[0m  [144/169], [94mLoss[0m : 10.19468
[1mStep[0m  [160/169], [94mLoss[0m : 9.76787

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.788, [92mTest[0m: 9.588, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.65708
[1mStep[0m  [16/169], [94mLoss[0m : 10.03150
[1mStep[0m  [32/169], [94mLoss[0m : 9.78346
[1mStep[0m  [48/169], [94mLoss[0m : 9.13185
[1mStep[0m  [64/169], [94mLoss[0m : 10.67514
[1mStep[0m  [80/169], [94mLoss[0m : 9.52886
[1mStep[0m  [96/169], [94mLoss[0m : 9.12074
[1mStep[0m  [112/169], [94mLoss[0m : 10.29977
[1mStep[0m  [128/169], [94mLoss[0m : 10.22195
[1mStep[0m  [144/169], [94mLoss[0m : 9.93762
[1mStep[0m  [160/169], [94mLoss[0m : 9.71833

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.751, [92mTest[0m: 9.513, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.64399
[1mStep[0m  [16/169], [94mLoss[0m : 10.33461
[1mStep[0m  [32/169], [94mLoss[0m : 9.26303
[1mStep[0m  [48/169], [94mLoss[0m : 9.85975
[1mStep[0m  [64/169], [94mLoss[0m : 9.72255
[1mStep[0m  [80/169], [94mLoss[0m : 9.80638
[1mStep[0m  [96/169], [94mLoss[0m : 8.83972
[1mStep[0m  [112/169], [94mLoss[0m : 10.27962
[1mStep[0m  [128/169], [94mLoss[0m : 9.18150
[1mStep[0m  [144/169], [94mLoss[0m : 8.97599
[1mStep[0m  [160/169], [94mLoss[0m : 9.88159

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.718, [92mTest[0m: 9.483, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.08673
[1mStep[0m  [16/169], [94mLoss[0m : 9.90083
[1mStep[0m  [32/169], [94mLoss[0m : 8.87456
[1mStep[0m  [48/169], [94mLoss[0m : 8.89450
[1mStep[0m  [64/169], [94mLoss[0m : 9.44433
[1mStep[0m  [80/169], [94mLoss[0m : 8.91575
[1mStep[0m  [96/169], [94mLoss[0m : 9.05931
[1mStep[0m  [112/169], [94mLoss[0m : 9.90620
[1mStep[0m  [128/169], [94mLoss[0m : 9.30139
[1mStep[0m  [144/169], [94mLoss[0m : 8.64256
[1mStep[0m  [160/169], [94mLoss[0m : 9.33428

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.693, [92mTest[0m: 9.504, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.95309
[1mStep[0m  [16/169], [94mLoss[0m : 9.82372
[1mStep[0m  [32/169], [94mLoss[0m : 9.64965
[1mStep[0m  [48/169], [94mLoss[0m : 9.96461
[1mStep[0m  [64/169], [94mLoss[0m : 9.75125
[1mStep[0m  [80/169], [94mLoss[0m : 9.76396
[1mStep[0m  [96/169], [94mLoss[0m : 9.38039
[1mStep[0m  [112/169], [94mLoss[0m : 9.23721
[1mStep[0m  [128/169], [94mLoss[0m : 10.25963
[1mStep[0m  [144/169], [94mLoss[0m : 9.56574
[1mStep[0m  [160/169], [94mLoss[0m : 10.49032

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.656, [92mTest[0m: 9.434, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.74273
[1mStep[0m  [16/169], [94mLoss[0m : 9.59995
[1mStep[0m  [32/169], [94mLoss[0m : 9.31655
[1mStep[0m  [48/169], [94mLoss[0m : 9.75576
[1mStep[0m  [64/169], [94mLoss[0m : 9.31857
[1mStep[0m  [80/169], [94mLoss[0m : 10.24654
[1mStep[0m  [96/169], [94mLoss[0m : 9.03810
[1mStep[0m  [112/169], [94mLoss[0m : 9.84304
[1mStep[0m  [128/169], [94mLoss[0m : 9.55050
[1mStep[0m  [144/169], [94mLoss[0m : 9.80784
[1mStep[0m  [160/169], [94mLoss[0m : 9.56962

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.629, [92mTest[0m: 9.427, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.87571
[1mStep[0m  [16/169], [94mLoss[0m : 9.11582
[1mStep[0m  [32/169], [94mLoss[0m : 9.41945
[1mStep[0m  [48/169], [94mLoss[0m : 10.35666
[1mStep[0m  [64/169], [94mLoss[0m : 9.80623
[1mStep[0m  [80/169], [94mLoss[0m : 9.94216
[1mStep[0m  [96/169], [94mLoss[0m : 9.56711
[1mStep[0m  [112/169], [94mLoss[0m : 9.47577
[1mStep[0m  [128/169], [94mLoss[0m : 9.78537
[1mStep[0m  [144/169], [94mLoss[0m : 10.83107
[1mStep[0m  [160/169], [94mLoss[0m : 9.31581

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.597, [92mTest[0m: 9.290, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.31660
[1mStep[0m  [16/169], [94mLoss[0m : 9.11778
[1mStep[0m  [32/169], [94mLoss[0m : 9.96134
[1mStep[0m  [48/169], [94mLoss[0m : 9.51988
[1mStep[0m  [64/169], [94mLoss[0m : 9.79455
[1mStep[0m  [80/169], [94mLoss[0m : 8.82133
[1mStep[0m  [96/169], [94mLoss[0m : 9.52498
[1mStep[0m  [112/169], [94mLoss[0m : 9.68763
[1mStep[0m  [128/169], [94mLoss[0m : 9.08401
[1mStep[0m  [144/169], [94mLoss[0m : 9.55586
[1mStep[0m  [160/169], [94mLoss[0m : 9.99532

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.565, [92mTest[0m: 9.252, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.85539
[1mStep[0m  [16/169], [94mLoss[0m : 8.91415
[1mStep[0m  [32/169], [94mLoss[0m : 9.55920
[1mStep[0m  [48/169], [94mLoss[0m : 9.73479
[1mStep[0m  [64/169], [94mLoss[0m : 9.71222
[1mStep[0m  [80/169], [94mLoss[0m : 8.91402
[1mStep[0m  [96/169], [94mLoss[0m : 9.27999
[1mStep[0m  [112/169], [94mLoss[0m : 9.66870
[1mStep[0m  [128/169], [94mLoss[0m : 9.58305
[1mStep[0m  [144/169], [94mLoss[0m : 9.69430
[1mStep[0m  [160/169], [94mLoss[0m : 10.03364

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.530, [92mTest[0m: 9.307, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.45909
[1mStep[0m  [16/169], [94mLoss[0m : 9.83523
[1mStep[0m  [32/169], [94mLoss[0m : 8.99472
[1mStep[0m  [48/169], [94mLoss[0m : 9.89537
[1mStep[0m  [64/169], [94mLoss[0m : 9.71629
[1mStep[0m  [80/169], [94mLoss[0m : 9.07706
[1mStep[0m  [96/169], [94mLoss[0m : 9.91953
[1mStep[0m  [112/169], [94mLoss[0m : 8.98781
[1mStep[0m  [128/169], [94mLoss[0m : 8.98273
[1mStep[0m  [144/169], [94mLoss[0m : 9.95841
[1mStep[0m  [160/169], [94mLoss[0m : 9.32388

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.498, [92mTest[0m: 9.272, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.39706
[1mStep[0m  [16/169], [94mLoss[0m : 9.69134
[1mStep[0m  [32/169], [94mLoss[0m : 8.98908
[1mStep[0m  [48/169], [94mLoss[0m : 9.25549
[1mStep[0m  [64/169], [94mLoss[0m : 9.02811
[1mStep[0m  [80/169], [94mLoss[0m : 9.34424
[1mStep[0m  [96/169], [94mLoss[0m : 9.16559
[1mStep[0m  [112/169], [94mLoss[0m : 8.79831
[1mStep[0m  [128/169], [94mLoss[0m : 9.38784
[1mStep[0m  [144/169], [94mLoss[0m : 9.74208
[1mStep[0m  [160/169], [94mLoss[0m : 9.50760

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.472, [92mTest[0m: 9.232, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.67596
[1mStep[0m  [16/169], [94mLoss[0m : 9.61087
[1mStep[0m  [32/169], [94mLoss[0m : 9.56197
[1mStep[0m  [48/169], [94mLoss[0m : 9.28092
[1mStep[0m  [64/169], [94mLoss[0m : 9.59165
[1mStep[0m  [80/169], [94mLoss[0m : 9.13948
[1mStep[0m  [96/169], [94mLoss[0m : 9.58289
[1mStep[0m  [112/169], [94mLoss[0m : 9.44533
[1mStep[0m  [128/169], [94mLoss[0m : 10.00943
[1mStep[0m  [144/169], [94mLoss[0m : 9.19913
[1mStep[0m  [160/169], [94mLoss[0m : 9.83247

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.437, [92mTest[0m: 9.201, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.57391
[1mStep[0m  [16/169], [94mLoss[0m : 9.51348
[1mStep[0m  [32/169], [94mLoss[0m : 8.63003
[1mStep[0m  [48/169], [94mLoss[0m : 10.16324
[1mStep[0m  [64/169], [94mLoss[0m : 9.25920
[1mStep[0m  [80/169], [94mLoss[0m : 9.74157
[1mStep[0m  [96/169], [94mLoss[0m : 9.69970
[1mStep[0m  [112/169], [94mLoss[0m : 9.51110
[1mStep[0m  [128/169], [94mLoss[0m : 8.52459
[1mStep[0m  [144/169], [94mLoss[0m : 9.78845
[1mStep[0m  [160/169], [94mLoss[0m : 9.24178

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.405, [92mTest[0m: 9.120, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.67736
[1mStep[0m  [16/169], [94mLoss[0m : 9.48206
[1mStep[0m  [32/169], [94mLoss[0m : 9.54005
[1mStep[0m  [48/169], [94mLoss[0m : 9.68127
[1mStep[0m  [64/169], [94mLoss[0m : 8.75937
[1mStep[0m  [80/169], [94mLoss[0m : 9.64857
[1mStep[0m  [96/169], [94mLoss[0m : 9.30646
[1mStep[0m  [112/169], [94mLoss[0m : 9.27259
[1mStep[0m  [128/169], [94mLoss[0m : 9.46923
[1mStep[0m  [144/169], [94mLoss[0m : 9.65719
[1mStep[0m  [160/169], [94mLoss[0m : 9.20752

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.382, [92mTest[0m: 9.123, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.98516
[1mStep[0m  [16/169], [94mLoss[0m : 8.92244
[1mStep[0m  [32/169], [94mLoss[0m : 9.02859
[1mStep[0m  [48/169], [94mLoss[0m : 9.28910
[1mStep[0m  [64/169], [94mLoss[0m : 8.71437
[1mStep[0m  [80/169], [94mLoss[0m : 9.64875
[1mStep[0m  [96/169], [94mLoss[0m : 9.40796
[1mStep[0m  [112/169], [94mLoss[0m : 9.30626
[1mStep[0m  [128/169], [94mLoss[0m : 8.95265
[1mStep[0m  [144/169], [94mLoss[0m : 9.94830
[1mStep[0m  [160/169], [94mLoss[0m : 10.03142

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.343, [92mTest[0m: 9.072, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.96644
[1mStep[0m  [16/169], [94mLoss[0m : 9.48918
[1mStep[0m  [32/169], [94mLoss[0m : 9.65792
[1mStep[0m  [48/169], [94mLoss[0m : 8.92930
[1mStep[0m  [64/169], [94mLoss[0m : 9.29803
[1mStep[0m  [80/169], [94mLoss[0m : 9.40523
[1mStep[0m  [96/169], [94mLoss[0m : 9.87690
[1mStep[0m  [112/169], [94mLoss[0m : 9.79177
[1mStep[0m  [128/169], [94mLoss[0m : 9.10152
[1mStep[0m  [144/169], [94mLoss[0m : 8.51125
[1mStep[0m  [160/169], [94mLoss[0m : 8.76092

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.305, [92mTest[0m: 9.027, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.50764
[1mStep[0m  [16/169], [94mLoss[0m : 9.06844
[1mStep[0m  [32/169], [94mLoss[0m : 9.43358
[1mStep[0m  [48/169], [94mLoss[0m : 9.46641
[1mStep[0m  [64/169], [94mLoss[0m : 9.62916
[1mStep[0m  [80/169], [94mLoss[0m : 8.83591
[1mStep[0m  [96/169], [94mLoss[0m : 9.16840
[1mStep[0m  [112/169], [94mLoss[0m : 9.11672
[1mStep[0m  [128/169], [94mLoss[0m : 9.18010
[1mStep[0m  [144/169], [94mLoss[0m : 9.19529
[1mStep[0m  [160/169], [94mLoss[0m : 8.78446

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.272, [92mTest[0m: 8.960, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.96793
[1mStep[0m  [16/169], [94mLoss[0m : 9.14585
[1mStep[0m  [32/169], [94mLoss[0m : 9.32444
[1mStep[0m  [48/169], [94mLoss[0m : 9.30205
[1mStep[0m  [64/169], [94mLoss[0m : 9.53119
[1mStep[0m  [80/169], [94mLoss[0m : 9.97032
[1mStep[0m  [96/169], [94mLoss[0m : 9.34059
[1mStep[0m  [112/169], [94mLoss[0m : 9.30457
[1mStep[0m  [128/169], [94mLoss[0m : 9.22196
[1mStep[0m  [144/169], [94mLoss[0m : 8.86901
[1mStep[0m  [160/169], [94mLoss[0m : 9.11685

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.253, [92mTest[0m: 9.032, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.873
====================================

Phase 2 - Evaluation MAE:  8.873422818524498
MAE score P1       9.926654
MAE score P2       8.873423
loss               9.252915
learning_rate        0.0001
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.02519
[1mStep[0m  [16/169], [94mLoss[0m : 10.51058
[1mStep[0m  [32/169], [94mLoss[0m : 11.25274
[1mStep[0m  [48/169], [94mLoss[0m : 10.83542
[1mStep[0m  [64/169], [94mLoss[0m : 10.56540
[1mStep[0m  [80/169], [94mLoss[0m : 10.22545
[1mStep[0m  [96/169], [94mLoss[0m : 10.67342
[1mStep[0m  [112/169], [94mLoss[0m : 11.02587
[1mStep[0m  [128/169], [94mLoss[0m : 10.49722
[1mStep[0m  [144/169], [94mLoss[0m : 10.26290
[1mStep[0m  [160/169], [94mLoss[0m : 11.59906

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.855, [92mTest[0m: 10.847, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.07296
[1mStep[0m  [16/169], [94mLoss[0m : 11.02195
[1mStep[0m  [32/169], [94mLoss[0m : 11.08045
[1mStep[0m  [48/169], [94mLoss[0m : 10.82339
[1mStep[0m  [64/169], [94mLoss[0m : 10.73063
[1mStep[0m  [80/169], [94mLoss[0m : 10.11111
[1mStep[0m  [96/169], [94mLoss[0m : 10.26117
[1mStep[0m  [112/169], [94mLoss[0m : 10.70828
[1mStep[0m  [128/169], [94mLoss[0m : 10.74380
[1mStep[0m  [144/169], [94mLoss[0m : 11.59269
[1mStep[0m  [160/169], [94mLoss[0m : 10.13014

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.830, [92mTest[0m: 10.855, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.96107
[1mStep[0m  [16/169], [94mLoss[0m : 10.23047
[1mStep[0m  [32/169], [94mLoss[0m : 11.03342
[1mStep[0m  [48/169], [94mLoss[0m : 10.67725
[1mStep[0m  [64/169], [94mLoss[0m : 11.13964
[1mStep[0m  [80/169], [94mLoss[0m : 11.35482
[1mStep[0m  [96/169], [94mLoss[0m : 11.33102
[1mStep[0m  [112/169], [94mLoss[0m : 11.10353
[1mStep[0m  [128/169], [94mLoss[0m : 10.31712
[1mStep[0m  [144/169], [94mLoss[0m : 10.52114
[1mStep[0m  [160/169], [94mLoss[0m : 10.03507

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.810, [92mTest[0m: 10.834, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.25734
[1mStep[0m  [16/169], [94mLoss[0m : 10.64531
[1mStep[0m  [32/169], [94mLoss[0m : 10.43191
[1mStep[0m  [48/169], [94mLoss[0m : 11.26635
[1mStep[0m  [64/169], [94mLoss[0m : 10.66496
[1mStep[0m  [80/169], [94mLoss[0m : 11.76709
[1mStep[0m  [96/169], [94mLoss[0m : 10.79806
[1mStep[0m  [112/169], [94mLoss[0m : 11.10192
[1mStep[0m  [128/169], [94mLoss[0m : 10.32212
[1mStep[0m  [144/169], [94mLoss[0m : 11.19669
[1mStep[0m  [160/169], [94mLoss[0m : 11.25113

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.87622
[1mStep[0m  [16/169], [94mLoss[0m : 11.55463
[1mStep[0m  [32/169], [94mLoss[0m : 11.18919
[1mStep[0m  [48/169], [94mLoss[0m : 9.90547
[1mStep[0m  [64/169], [94mLoss[0m : 11.42119
[1mStep[0m  [80/169], [94mLoss[0m : 9.97008
[1mStep[0m  [96/169], [94mLoss[0m : 10.50096
[1mStep[0m  [112/169], [94mLoss[0m : 11.41493
[1mStep[0m  [128/169], [94mLoss[0m : 10.70556
[1mStep[0m  [144/169], [94mLoss[0m : 10.92700
[1mStep[0m  [160/169], [94mLoss[0m : 10.12329

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.768, [92mTest[0m: 10.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.59154
[1mStep[0m  [16/169], [94mLoss[0m : 10.42034
[1mStep[0m  [32/169], [94mLoss[0m : 10.30232
[1mStep[0m  [48/169], [94mLoss[0m : 11.39932
[1mStep[0m  [64/169], [94mLoss[0m : 11.40671
[1mStep[0m  [80/169], [94mLoss[0m : 10.68806
[1mStep[0m  [96/169], [94mLoss[0m : 11.17615
[1mStep[0m  [112/169], [94mLoss[0m : 11.03101
[1mStep[0m  [128/169], [94mLoss[0m : 10.96418
[1mStep[0m  [144/169], [94mLoss[0m : 10.53534
[1mStep[0m  [160/169], [94mLoss[0m : 10.43204

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.21912
[1mStep[0m  [16/169], [94mLoss[0m : 10.59990
[1mStep[0m  [32/169], [94mLoss[0m : 10.97408
[1mStep[0m  [48/169], [94mLoss[0m : 11.18230
[1mStep[0m  [64/169], [94mLoss[0m : 11.00639
[1mStep[0m  [80/169], [94mLoss[0m : 10.52199
[1mStep[0m  [96/169], [94mLoss[0m : 10.45057
[1mStep[0m  [112/169], [94mLoss[0m : 10.60232
[1mStep[0m  [128/169], [94mLoss[0m : 11.43013
[1mStep[0m  [144/169], [94mLoss[0m : 11.47377
[1mStep[0m  [160/169], [94mLoss[0m : 10.50293

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.725, [92mTest[0m: 10.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.39543
[1mStep[0m  [16/169], [94mLoss[0m : 10.54108
[1mStep[0m  [32/169], [94mLoss[0m : 10.68965
[1mStep[0m  [48/169], [94mLoss[0m : 10.40246
[1mStep[0m  [64/169], [94mLoss[0m : 11.00952
[1mStep[0m  [80/169], [94mLoss[0m : 11.00834
[1mStep[0m  [96/169], [94mLoss[0m : 10.25137
[1mStep[0m  [112/169], [94mLoss[0m : 10.49980
[1mStep[0m  [128/169], [94mLoss[0m : 10.01102
[1mStep[0m  [144/169], [94mLoss[0m : 11.16323
[1mStep[0m  [160/169], [94mLoss[0m : 10.69966

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.702, [92mTest[0m: 10.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.74858
[1mStep[0m  [16/169], [94mLoss[0m : 10.88269
[1mStep[0m  [32/169], [94mLoss[0m : 10.32138
[1mStep[0m  [48/169], [94mLoss[0m : 11.22329
[1mStep[0m  [64/169], [94mLoss[0m : 10.73699
[1mStep[0m  [80/169], [94mLoss[0m : 10.03259
[1mStep[0m  [96/169], [94mLoss[0m : 10.67838
[1mStep[0m  [112/169], [94mLoss[0m : 10.85027
[1mStep[0m  [128/169], [94mLoss[0m : 10.61663
[1mStep[0m  [144/169], [94mLoss[0m : 10.60933
[1mStep[0m  [160/169], [94mLoss[0m : 11.03053

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.22201
[1mStep[0m  [16/169], [94mLoss[0m : 10.31414
[1mStep[0m  [32/169], [94mLoss[0m : 10.10482
[1mStep[0m  [48/169], [94mLoss[0m : 10.62553
[1mStep[0m  [64/169], [94mLoss[0m : 10.20848
[1mStep[0m  [80/169], [94mLoss[0m : 10.74903
[1mStep[0m  [96/169], [94mLoss[0m : 11.41090
[1mStep[0m  [112/169], [94mLoss[0m : 10.69255
[1mStep[0m  [128/169], [94mLoss[0m : 10.06949
[1mStep[0m  [144/169], [94mLoss[0m : 10.31875
[1mStep[0m  [160/169], [94mLoss[0m : 10.87637

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.663, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.48172
[1mStep[0m  [16/169], [94mLoss[0m : 10.27965
[1mStep[0m  [32/169], [94mLoss[0m : 10.50705
[1mStep[0m  [48/169], [94mLoss[0m : 10.65553
[1mStep[0m  [64/169], [94mLoss[0m : 10.76680
[1mStep[0m  [80/169], [94mLoss[0m : 10.26086
[1mStep[0m  [96/169], [94mLoss[0m : 10.32955
[1mStep[0m  [112/169], [94mLoss[0m : 9.87484
[1mStep[0m  [128/169], [94mLoss[0m : 11.72515
[1mStep[0m  [144/169], [94mLoss[0m : 10.71991
[1mStep[0m  [160/169], [94mLoss[0m : 9.76069

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.639, [92mTest[0m: 10.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.55122
[1mStep[0m  [16/169], [94mLoss[0m : 9.93656
[1mStep[0m  [32/169], [94mLoss[0m : 10.98927
[1mStep[0m  [48/169], [94mLoss[0m : 10.87826
[1mStep[0m  [64/169], [94mLoss[0m : 10.38585
[1mStep[0m  [80/169], [94mLoss[0m : 9.51859
[1mStep[0m  [96/169], [94mLoss[0m : 11.23923
[1mStep[0m  [112/169], [94mLoss[0m : 11.28587
[1mStep[0m  [128/169], [94mLoss[0m : 11.15155
[1mStep[0m  [144/169], [94mLoss[0m : 11.04220
[1mStep[0m  [160/169], [94mLoss[0m : 10.95832

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.614, [92mTest[0m: 10.624, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.52388
[1mStep[0m  [16/169], [94mLoss[0m : 10.38557
[1mStep[0m  [32/169], [94mLoss[0m : 10.56358
[1mStep[0m  [48/169], [94mLoss[0m : 11.45005
[1mStep[0m  [64/169], [94mLoss[0m : 11.43266
[1mStep[0m  [80/169], [94mLoss[0m : 10.86423
[1mStep[0m  [96/169], [94mLoss[0m : 10.73454
[1mStep[0m  [112/169], [94mLoss[0m : 10.68754
[1mStep[0m  [128/169], [94mLoss[0m : 9.48706
[1mStep[0m  [144/169], [94mLoss[0m : 10.88416
[1mStep[0m  [160/169], [94mLoss[0m : 10.46997

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.594, [92mTest[0m: 10.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.69251
[1mStep[0m  [16/169], [94mLoss[0m : 10.61758
[1mStep[0m  [32/169], [94mLoss[0m : 10.87431
[1mStep[0m  [48/169], [94mLoss[0m : 11.23829
[1mStep[0m  [64/169], [94mLoss[0m : 10.45338
[1mStep[0m  [80/169], [94mLoss[0m : 10.41782
[1mStep[0m  [96/169], [94mLoss[0m : 10.24900
[1mStep[0m  [112/169], [94mLoss[0m : 9.78638
[1mStep[0m  [128/169], [94mLoss[0m : 10.70668
[1mStep[0m  [144/169], [94mLoss[0m : 9.59381
[1mStep[0m  [160/169], [94mLoss[0m : 10.70457

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.575, [92mTest[0m: 10.539, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.82066
[1mStep[0m  [16/169], [94mLoss[0m : 11.05626
[1mStep[0m  [32/169], [94mLoss[0m : 11.28621
[1mStep[0m  [48/169], [94mLoss[0m : 11.24571
[1mStep[0m  [64/169], [94mLoss[0m : 10.40449
[1mStep[0m  [80/169], [94mLoss[0m : 10.70943
[1mStep[0m  [96/169], [94mLoss[0m : 10.89541
[1mStep[0m  [112/169], [94mLoss[0m : 9.64699
[1mStep[0m  [128/169], [94mLoss[0m : 10.57492
[1mStep[0m  [144/169], [94mLoss[0m : 10.38655
[1mStep[0m  [160/169], [94mLoss[0m : 10.79003

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.550, [92mTest[0m: 10.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.89768
[1mStep[0m  [16/169], [94mLoss[0m : 11.91563
[1mStep[0m  [32/169], [94mLoss[0m : 10.55082
[1mStep[0m  [48/169], [94mLoss[0m : 10.13234
[1mStep[0m  [64/169], [94mLoss[0m : 10.53954
[1mStep[0m  [80/169], [94mLoss[0m : 11.04128
[1mStep[0m  [96/169], [94mLoss[0m : 11.33681
[1mStep[0m  [112/169], [94mLoss[0m : 9.73948
[1mStep[0m  [128/169], [94mLoss[0m : 10.47915
[1mStep[0m  [144/169], [94mLoss[0m : 10.55219
[1mStep[0m  [160/169], [94mLoss[0m : 10.68165

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.527, [92mTest[0m: 10.513, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.65995
[1mStep[0m  [16/169], [94mLoss[0m : 10.36805
[1mStep[0m  [32/169], [94mLoss[0m : 10.46602
[1mStep[0m  [48/169], [94mLoss[0m : 10.44058
[1mStep[0m  [64/169], [94mLoss[0m : 10.49204
[1mStep[0m  [80/169], [94mLoss[0m : 9.72890
[1mStep[0m  [96/169], [94mLoss[0m : 10.42216
[1mStep[0m  [112/169], [94mLoss[0m : 10.92328
[1mStep[0m  [128/169], [94mLoss[0m : 10.30878
[1mStep[0m  [144/169], [94mLoss[0m : 10.49515
[1mStep[0m  [160/169], [94mLoss[0m : 9.98888

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.510, [92mTest[0m: 10.484, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.69125
[1mStep[0m  [16/169], [94mLoss[0m : 9.87856
[1mStep[0m  [32/169], [94mLoss[0m : 10.76650
[1mStep[0m  [48/169], [94mLoss[0m : 10.46405
[1mStep[0m  [64/169], [94mLoss[0m : 9.02672
[1mStep[0m  [80/169], [94mLoss[0m : 9.98168
[1mStep[0m  [96/169], [94mLoss[0m : 10.31831
[1mStep[0m  [112/169], [94mLoss[0m : 10.02085
[1mStep[0m  [128/169], [94mLoss[0m : 10.72958
[1mStep[0m  [144/169], [94mLoss[0m : 10.17219
[1mStep[0m  [160/169], [94mLoss[0m : 11.95269

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.490, [92mTest[0m: 10.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.97263
[1mStep[0m  [16/169], [94mLoss[0m : 9.93447
[1mStep[0m  [32/169], [94mLoss[0m : 10.18788
[1mStep[0m  [48/169], [94mLoss[0m : 9.77532
[1mStep[0m  [64/169], [94mLoss[0m : 10.73536
[1mStep[0m  [80/169], [94mLoss[0m : 10.31598
[1mStep[0m  [96/169], [94mLoss[0m : 10.49569
[1mStep[0m  [112/169], [94mLoss[0m : 10.05097
[1mStep[0m  [128/169], [94mLoss[0m : 10.19383
[1mStep[0m  [144/169], [94mLoss[0m : 11.29705
[1mStep[0m  [160/169], [94mLoss[0m : 11.03169

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.464, [92mTest[0m: 10.399, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.22773
[1mStep[0m  [16/169], [94mLoss[0m : 10.41322
[1mStep[0m  [32/169], [94mLoss[0m : 10.81455
[1mStep[0m  [48/169], [94mLoss[0m : 10.33033
[1mStep[0m  [64/169], [94mLoss[0m : 10.59756
[1mStep[0m  [80/169], [94mLoss[0m : 10.41876
[1mStep[0m  [96/169], [94mLoss[0m : 10.70264
[1mStep[0m  [112/169], [94mLoss[0m : 9.68100
[1mStep[0m  [128/169], [94mLoss[0m : 10.23897
[1mStep[0m  [144/169], [94mLoss[0m : 9.93291
[1mStep[0m  [160/169], [94mLoss[0m : 10.22882

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.436, [92mTest[0m: 10.386, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.53854
[1mStep[0m  [16/169], [94mLoss[0m : 10.13088
[1mStep[0m  [32/169], [94mLoss[0m : 11.24180
[1mStep[0m  [48/169], [94mLoss[0m : 10.44537
[1mStep[0m  [64/169], [94mLoss[0m : 9.95867
[1mStep[0m  [80/169], [94mLoss[0m : 11.07665
[1mStep[0m  [96/169], [94mLoss[0m : 10.83759
[1mStep[0m  [112/169], [94mLoss[0m : 10.96411
[1mStep[0m  [128/169], [94mLoss[0m : 10.26468
[1mStep[0m  [144/169], [94mLoss[0m : 10.26102
[1mStep[0m  [160/169], [94mLoss[0m : 10.62959

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.420, [92mTest[0m: 10.345, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.82969
[1mStep[0m  [16/169], [94mLoss[0m : 10.71059
[1mStep[0m  [32/169], [94mLoss[0m : 10.89387
[1mStep[0m  [48/169], [94mLoss[0m : 10.44425
[1mStep[0m  [64/169], [94mLoss[0m : 9.86769
[1mStep[0m  [80/169], [94mLoss[0m : 9.71562
[1mStep[0m  [96/169], [94mLoss[0m : 9.81927
[1mStep[0m  [112/169], [94mLoss[0m : 10.04616
[1mStep[0m  [128/169], [94mLoss[0m : 10.81143
[1mStep[0m  [144/169], [94mLoss[0m : 9.42431
[1mStep[0m  [160/169], [94mLoss[0m : 9.82287

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.348, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.54772
[1mStep[0m  [16/169], [94mLoss[0m : 9.64929
[1mStep[0m  [32/169], [94mLoss[0m : 10.07293
[1mStep[0m  [48/169], [94mLoss[0m : 10.37843
[1mStep[0m  [64/169], [94mLoss[0m : 10.41412
[1mStep[0m  [80/169], [94mLoss[0m : 10.76852
[1mStep[0m  [96/169], [94mLoss[0m : 10.63358
[1mStep[0m  [112/169], [94mLoss[0m : 10.25809
[1mStep[0m  [128/169], [94mLoss[0m : 10.57780
[1mStep[0m  [144/169], [94mLoss[0m : 9.64995
[1mStep[0m  [160/169], [94mLoss[0m : 11.44446

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.376, [92mTest[0m: 10.296, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.37461
[1mStep[0m  [16/169], [94mLoss[0m : 10.69884
[1mStep[0m  [32/169], [94mLoss[0m : 9.85728
[1mStep[0m  [48/169], [94mLoss[0m : 10.52030
[1mStep[0m  [64/169], [94mLoss[0m : 10.26461
[1mStep[0m  [80/169], [94mLoss[0m : 10.43796
[1mStep[0m  [96/169], [94mLoss[0m : 11.11944
[1mStep[0m  [112/169], [94mLoss[0m : 10.97445
[1mStep[0m  [128/169], [94mLoss[0m : 10.57730
[1mStep[0m  [144/169], [94mLoss[0m : 11.22910
[1mStep[0m  [160/169], [94mLoss[0m : 11.54725

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.358, [92mTest[0m: 10.288, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.59789
[1mStep[0m  [16/169], [94mLoss[0m : 10.46760
[1mStep[0m  [32/169], [94mLoss[0m : 10.65392
[1mStep[0m  [48/169], [94mLoss[0m : 10.76122
[1mStep[0m  [64/169], [94mLoss[0m : 10.20875
[1mStep[0m  [80/169], [94mLoss[0m : 10.14297
[1mStep[0m  [96/169], [94mLoss[0m : 9.71227
[1mStep[0m  [112/169], [94mLoss[0m : 11.04757
[1mStep[0m  [128/169], [94mLoss[0m : 10.39330
[1mStep[0m  [144/169], [94mLoss[0m : 9.84202
[1mStep[0m  [160/169], [94mLoss[0m : 10.39068

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.340, [92mTest[0m: 10.265, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.97276
[1mStep[0m  [16/169], [94mLoss[0m : 10.50056
[1mStep[0m  [32/169], [94mLoss[0m : 10.54517
[1mStep[0m  [48/169], [94mLoss[0m : 10.56898
[1mStep[0m  [64/169], [94mLoss[0m : 10.34664
[1mStep[0m  [80/169], [94mLoss[0m : 9.73066
[1mStep[0m  [96/169], [94mLoss[0m : 10.53009
[1mStep[0m  [112/169], [94mLoss[0m : 11.29362
[1mStep[0m  [128/169], [94mLoss[0m : 11.09653
[1mStep[0m  [144/169], [94mLoss[0m : 10.12479
[1mStep[0m  [160/169], [94mLoss[0m : 10.63181

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.322, [92mTest[0m: 10.248, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.42414
[1mStep[0m  [16/169], [94mLoss[0m : 9.61871
[1mStep[0m  [32/169], [94mLoss[0m : 10.15008
[1mStep[0m  [48/169], [94mLoss[0m : 11.01732
[1mStep[0m  [64/169], [94mLoss[0m : 10.36858
[1mStep[0m  [80/169], [94mLoss[0m : 9.90109
[1mStep[0m  [96/169], [94mLoss[0m : 10.18170
[1mStep[0m  [112/169], [94mLoss[0m : 9.99187
[1mStep[0m  [128/169], [94mLoss[0m : 10.86456
[1mStep[0m  [144/169], [94mLoss[0m : 10.50716
[1mStep[0m  [160/169], [94mLoss[0m : 10.43268

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.296, [92mTest[0m: 10.231, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.84296
[1mStep[0m  [16/169], [94mLoss[0m : 9.95817
[1mStep[0m  [32/169], [94mLoss[0m : 10.51714
[1mStep[0m  [48/169], [94mLoss[0m : 10.08234
[1mStep[0m  [64/169], [94mLoss[0m : 11.37184
[1mStep[0m  [80/169], [94mLoss[0m : 9.54490
[1mStep[0m  [96/169], [94mLoss[0m : 10.82593
[1mStep[0m  [112/169], [94mLoss[0m : 10.04266
[1mStep[0m  [128/169], [94mLoss[0m : 10.75279
[1mStep[0m  [144/169], [94mLoss[0m : 10.04248
[1mStep[0m  [160/169], [94mLoss[0m : 10.46235

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.273, [92mTest[0m: 10.188, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.14901
[1mStep[0m  [16/169], [94mLoss[0m : 10.43147
[1mStep[0m  [32/169], [94mLoss[0m : 10.20600
[1mStep[0m  [48/169], [94mLoss[0m : 10.47435
[1mStep[0m  [64/169], [94mLoss[0m : 10.33429
[1mStep[0m  [80/169], [94mLoss[0m : 10.26031
[1mStep[0m  [96/169], [94mLoss[0m : 10.75196
[1mStep[0m  [112/169], [94mLoss[0m : 9.31001
[1mStep[0m  [128/169], [94mLoss[0m : 11.06771
[1mStep[0m  [144/169], [94mLoss[0m : 10.05710
[1mStep[0m  [160/169], [94mLoss[0m : 9.70095

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.257, [92mTest[0m: 10.194, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.41035
[1mStep[0m  [16/169], [94mLoss[0m : 10.29502
[1mStep[0m  [32/169], [94mLoss[0m : 9.88363
[1mStep[0m  [48/169], [94mLoss[0m : 9.62870
[1mStep[0m  [64/169], [94mLoss[0m : 10.23277
[1mStep[0m  [80/169], [94mLoss[0m : 9.08892
[1mStep[0m  [96/169], [94mLoss[0m : 10.03281
[1mStep[0m  [112/169], [94mLoss[0m : 10.15542
[1mStep[0m  [128/169], [94mLoss[0m : 10.29107
[1mStep[0m  [144/169], [94mLoss[0m : 10.23001
[1mStep[0m  [160/169], [94mLoss[0m : 10.02962

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.232, [92mTest[0m: 10.105, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.128
====================================

Phase 1 - Evaluation MAE:  10.127862402370997
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.31314
[1mStep[0m  [16/169], [94mLoss[0m : 9.22160
[1mStep[0m  [32/169], [94mLoss[0m : 10.04078
[1mStep[0m  [48/169], [94mLoss[0m : 10.31333
[1mStep[0m  [64/169], [94mLoss[0m : 10.33149
[1mStep[0m  [80/169], [94mLoss[0m : 10.38790
[1mStep[0m  [96/169], [94mLoss[0m : 10.36314
[1mStep[0m  [112/169], [94mLoss[0m : 10.50006
[1mStep[0m  [128/169], [94mLoss[0m : 9.71806
[1mStep[0m  [144/169], [94mLoss[0m : 10.02089
[1mStep[0m  [160/169], [94mLoss[0m : 10.20262

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.212, [92mTest[0m: 10.133, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.58657
[1mStep[0m  [16/169], [94mLoss[0m : 10.71889
[1mStep[0m  [32/169], [94mLoss[0m : 9.90780
[1mStep[0m  [48/169], [94mLoss[0m : 10.34713
[1mStep[0m  [64/169], [94mLoss[0m : 9.94081
[1mStep[0m  [80/169], [94mLoss[0m : 10.04415
[1mStep[0m  [96/169], [94mLoss[0m : 10.65891
[1mStep[0m  [112/169], [94mLoss[0m : 10.05290
[1mStep[0m  [128/169], [94mLoss[0m : 10.07571
[1mStep[0m  [144/169], [94mLoss[0m : 10.24824
[1mStep[0m  [160/169], [94mLoss[0m : 9.68536

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.187, [92mTest[0m: 10.126, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.61966
[1mStep[0m  [16/169], [94mLoss[0m : 10.32062
[1mStep[0m  [32/169], [94mLoss[0m : 10.36055
[1mStep[0m  [48/169], [94mLoss[0m : 9.57693
[1mStep[0m  [64/169], [94mLoss[0m : 10.87006
[1mStep[0m  [80/169], [94mLoss[0m : 9.41587
[1mStep[0m  [96/169], [94mLoss[0m : 11.03203
[1mStep[0m  [112/169], [94mLoss[0m : 10.62738
[1mStep[0m  [128/169], [94mLoss[0m : 9.56398
[1mStep[0m  [144/169], [94mLoss[0m : 9.21795
[1mStep[0m  [160/169], [94mLoss[0m : 9.99650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.162, [92mTest[0m: 10.088, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.83988
[1mStep[0m  [16/169], [94mLoss[0m : 9.61732
[1mStep[0m  [32/169], [94mLoss[0m : 9.59937
[1mStep[0m  [48/169], [94mLoss[0m : 9.65997
[1mStep[0m  [64/169], [94mLoss[0m : 10.70082
[1mStep[0m  [80/169], [94mLoss[0m : 10.37864
[1mStep[0m  [96/169], [94mLoss[0m : 9.42734
[1mStep[0m  [112/169], [94mLoss[0m : 10.12817
[1mStep[0m  [128/169], [94mLoss[0m : 9.75357
[1mStep[0m  [144/169], [94mLoss[0m : 9.45676
[1mStep[0m  [160/169], [94mLoss[0m : 9.90155

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.137, [92mTest[0m: 10.050, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.73482
[1mStep[0m  [16/169], [94mLoss[0m : 10.50184
[1mStep[0m  [32/169], [94mLoss[0m : 10.07511
[1mStep[0m  [48/169], [94mLoss[0m : 10.07862
[1mStep[0m  [64/169], [94mLoss[0m : 10.27619
[1mStep[0m  [80/169], [94mLoss[0m : 10.21142
[1mStep[0m  [96/169], [94mLoss[0m : 10.27596
[1mStep[0m  [112/169], [94mLoss[0m : 10.07684
[1mStep[0m  [128/169], [94mLoss[0m : 10.29343
[1mStep[0m  [144/169], [94mLoss[0m : 9.12752
[1mStep[0m  [160/169], [94mLoss[0m : 10.26813

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.110, [92mTest[0m: 9.995, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.30204
[1mStep[0m  [16/169], [94mLoss[0m : 9.96586
[1mStep[0m  [32/169], [94mLoss[0m : 9.94384
[1mStep[0m  [48/169], [94mLoss[0m : 9.50667
[1mStep[0m  [64/169], [94mLoss[0m : 10.04926
[1mStep[0m  [80/169], [94mLoss[0m : 10.16008
[1mStep[0m  [96/169], [94mLoss[0m : 9.48287
[1mStep[0m  [112/169], [94mLoss[0m : 9.52526
[1mStep[0m  [128/169], [94mLoss[0m : 11.29652
[1mStep[0m  [144/169], [94mLoss[0m : 10.51748
[1mStep[0m  [160/169], [94mLoss[0m : 9.87511

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.088, [92mTest[0m: 10.033, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.55303
[1mStep[0m  [16/169], [94mLoss[0m : 10.42044
[1mStep[0m  [32/169], [94mLoss[0m : 10.33849
[1mStep[0m  [48/169], [94mLoss[0m : 9.63025
[1mStep[0m  [64/169], [94mLoss[0m : 9.91468
[1mStep[0m  [80/169], [94mLoss[0m : 9.32381
[1mStep[0m  [96/169], [94mLoss[0m : 10.68916
[1mStep[0m  [112/169], [94mLoss[0m : 9.82032
[1mStep[0m  [128/169], [94mLoss[0m : 10.02592
[1mStep[0m  [144/169], [94mLoss[0m : 9.69929
[1mStep[0m  [160/169], [94mLoss[0m : 10.65024

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.059, [92mTest[0m: 9.923, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.26935
[1mStep[0m  [16/169], [94mLoss[0m : 10.58582
[1mStep[0m  [32/169], [94mLoss[0m : 9.63271
[1mStep[0m  [48/169], [94mLoss[0m : 9.74848
[1mStep[0m  [64/169], [94mLoss[0m : 10.14291
[1mStep[0m  [80/169], [94mLoss[0m : 9.53349
[1mStep[0m  [96/169], [94mLoss[0m : 9.71050
[1mStep[0m  [112/169], [94mLoss[0m : 10.20496
[1mStep[0m  [128/169], [94mLoss[0m : 9.64911
[1mStep[0m  [144/169], [94mLoss[0m : 10.24496
[1mStep[0m  [160/169], [94mLoss[0m : 9.51834

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.035, [92mTest[0m: 9.967, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.64703
[1mStep[0m  [16/169], [94mLoss[0m : 9.32866
[1mStep[0m  [32/169], [94mLoss[0m : 9.13570
[1mStep[0m  [48/169], [94mLoss[0m : 10.18450
[1mStep[0m  [64/169], [94mLoss[0m : 9.80214
[1mStep[0m  [80/169], [94mLoss[0m : 9.57445
[1mStep[0m  [96/169], [94mLoss[0m : 9.43024
[1mStep[0m  [112/169], [94mLoss[0m : 10.46928
[1mStep[0m  [128/169], [94mLoss[0m : 10.27833
[1mStep[0m  [144/169], [94mLoss[0m : 9.57348
[1mStep[0m  [160/169], [94mLoss[0m : 10.05021

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.009, [92mTest[0m: 9.919, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.00416
[1mStep[0m  [16/169], [94mLoss[0m : 9.49110
[1mStep[0m  [32/169], [94mLoss[0m : 9.73391
[1mStep[0m  [48/169], [94mLoss[0m : 10.37504
[1mStep[0m  [64/169], [94mLoss[0m : 9.76696
[1mStep[0m  [80/169], [94mLoss[0m : 10.19751
[1mStep[0m  [96/169], [94mLoss[0m : 9.46248
[1mStep[0m  [112/169], [94mLoss[0m : 9.49577
[1mStep[0m  [128/169], [94mLoss[0m : 10.01722
[1mStep[0m  [144/169], [94mLoss[0m : 10.36904
[1mStep[0m  [160/169], [94mLoss[0m : 11.05574

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.987, [92mTest[0m: 9.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.15435
[1mStep[0m  [16/169], [94mLoss[0m : 9.44884
[1mStep[0m  [32/169], [94mLoss[0m : 9.99431
[1mStep[0m  [48/169], [94mLoss[0m : 11.03548
[1mStep[0m  [64/169], [94mLoss[0m : 9.66657
[1mStep[0m  [80/169], [94mLoss[0m : 9.82878
[1mStep[0m  [96/169], [94mLoss[0m : 9.64815
[1mStep[0m  [112/169], [94mLoss[0m : 10.11674
[1mStep[0m  [128/169], [94mLoss[0m : 10.55763
[1mStep[0m  [144/169], [94mLoss[0m : 9.47614
[1mStep[0m  [160/169], [94mLoss[0m : 9.78537

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.956, [92mTest[0m: 9.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.81541
[1mStep[0m  [16/169], [94mLoss[0m : 10.20959
[1mStep[0m  [32/169], [94mLoss[0m : 9.81673
[1mStep[0m  [48/169], [94mLoss[0m : 10.07227
[1mStep[0m  [64/169], [94mLoss[0m : 10.52692
[1mStep[0m  [80/169], [94mLoss[0m : 9.44222
[1mStep[0m  [96/169], [94mLoss[0m : 9.53980
[1mStep[0m  [112/169], [94mLoss[0m : 9.92579
[1mStep[0m  [128/169], [94mLoss[0m : 9.81245
[1mStep[0m  [144/169], [94mLoss[0m : 9.58252
[1mStep[0m  [160/169], [94mLoss[0m : 10.62436

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.934, [92mTest[0m: 9.818, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.22903
[1mStep[0m  [16/169], [94mLoss[0m : 10.17167
[1mStep[0m  [32/169], [94mLoss[0m : 9.88964
[1mStep[0m  [48/169], [94mLoss[0m : 9.08843
[1mStep[0m  [64/169], [94mLoss[0m : 10.16753
[1mStep[0m  [80/169], [94mLoss[0m : 9.41860
[1mStep[0m  [96/169], [94mLoss[0m : 9.80183
[1mStep[0m  [112/169], [94mLoss[0m : 9.32732
[1mStep[0m  [128/169], [94mLoss[0m : 9.64943
[1mStep[0m  [144/169], [94mLoss[0m : 10.00243
[1mStep[0m  [160/169], [94mLoss[0m : 9.91561

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.909, [92mTest[0m: 9.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.20650
[1mStep[0m  [16/169], [94mLoss[0m : 9.34555
[1mStep[0m  [32/169], [94mLoss[0m : 9.86443
[1mStep[0m  [48/169], [94mLoss[0m : 9.64373
[1mStep[0m  [64/169], [94mLoss[0m : 10.01022
[1mStep[0m  [80/169], [94mLoss[0m : 9.71620
[1mStep[0m  [96/169], [94mLoss[0m : 9.93314
[1mStep[0m  [112/169], [94mLoss[0m : 9.83794
[1mStep[0m  [128/169], [94mLoss[0m : 9.89708
[1mStep[0m  [144/169], [94mLoss[0m : 9.68526
[1mStep[0m  [160/169], [94mLoss[0m : 9.99791

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.881, [92mTest[0m: 9.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.88751
[1mStep[0m  [16/169], [94mLoss[0m : 10.14213
[1mStep[0m  [32/169], [94mLoss[0m : 10.17348
[1mStep[0m  [48/169], [94mLoss[0m : 9.51642
[1mStep[0m  [64/169], [94mLoss[0m : 10.04859
[1mStep[0m  [80/169], [94mLoss[0m : 10.31264
[1mStep[0m  [96/169], [94mLoss[0m : 10.66900
[1mStep[0m  [112/169], [94mLoss[0m : 10.33937
[1mStep[0m  [128/169], [94mLoss[0m : 9.77567
[1mStep[0m  [144/169], [94mLoss[0m : 9.55910
[1mStep[0m  [160/169], [94mLoss[0m : 9.69651

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.851, [92mTest[0m: 9.763, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.23173
[1mStep[0m  [16/169], [94mLoss[0m : 9.65880
[1mStep[0m  [32/169], [94mLoss[0m : 10.36241
[1mStep[0m  [48/169], [94mLoss[0m : 9.43142
[1mStep[0m  [64/169], [94mLoss[0m : 9.63894
[1mStep[0m  [80/169], [94mLoss[0m : 10.52958
[1mStep[0m  [96/169], [94mLoss[0m : 10.27925
[1mStep[0m  [112/169], [94mLoss[0m : 10.13026
[1mStep[0m  [128/169], [94mLoss[0m : 9.63766
[1mStep[0m  [144/169], [94mLoss[0m : 9.67106
[1mStep[0m  [160/169], [94mLoss[0m : 9.54636

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.827, [92mTest[0m: 9.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.83436
[1mStep[0m  [16/169], [94mLoss[0m : 9.93046
[1mStep[0m  [32/169], [94mLoss[0m : 9.79600
[1mStep[0m  [48/169], [94mLoss[0m : 9.78308
[1mStep[0m  [64/169], [94mLoss[0m : 9.87306
[1mStep[0m  [80/169], [94mLoss[0m : 9.71767
[1mStep[0m  [96/169], [94mLoss[0m : 10.41263
[1mStep[0m  [112/169], [94mLoss[0m : 9.67672
[1mStep[0m  [128/169], [94mLoss[0m : 10.22662
[1mStep[0m  [144/169], [94mLoss[0m : 9.69680
[1mStep[0m  [160/169], [94mLoss[0m : 10.20914

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.800, [92mTest[0m: 9.636, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.21253
[1mStep[0m  [16/169], [94mLoss[0m : 9.74577
[1mStep[0m  [32/169], [94mLoss[0m : 9.45940
[1mStep[0m  [48/169], [94mLoss[0m : 9.32921
[1mStep[0m  [64/169], [94mLoss[0m : 8.82823
[1mStep[0m  [80/169], [94mLoss[0m : 9.60793
[1mStep[0m  [96/169], [94mLoss[0m : 10.18140
[1mStep[0m  [112/169], [94mLoss[0m : 10.15518
[1mStep[0m  [128/169], [94mLoss[0m : 9.50669
[1mStep[0m  [144/169], [94mLoss[0m : 9.88379
[1mStep[0m  [160/169], [94mLoss[0m : 9.70374

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.768, [92mTest[0m: 9.629, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.86734
[1mStep[0m  [16/169], [94mLoss[0m : 9.90405
[1mStep[0m  [32/169], [94mLoss[0m : 9.78195
[1mStep[0m  [48/169], [94mLoss[0m : 10.02466
[1mStep[0m  [64/169], [94mLoss[0m : 9.10888
[1mStep[0m  [80/169], [94mLoss[0m : 9.97206
[1mStep[0m  [96/169], [94mLoss[0m : 9.26273
[1mStep[0m  [112/169], [94mLoss[0m : 10.29969
[1mStep[0m  [128/169], [94mLoss[0m : 10.17581
[1mStep[0m  [144/169], [94mLoss[0m : 9.58618
[1mStep[0m  [160/169], [94mLoss[0m : 9.70675

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.743, [92mTest[0m: 9.587, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.36912
[1mStep[0m  [16/169], [94mLoss[0m : 9.45389
[1mStep[0m  [32/169], [94mLoss[0m : 9.32974
[1mStep[0m  [48/169], [94mLoss[0m : 9.51773
[1mStep[0m  [64/169], [94mLoss[0m : 9.70301
[1mStep[0m  [80/169], [94mLoss[0m : 9.55090
[1mStep[0m  [96/169], [94mLoss[0m : 9.24012
[1mStep[0m  [112/169], [94mLoss[0m : 9.88195
[1mStep[0m  [128/169], [94mLoss[0m : 9.76402
[1mStep[0m  [144/169], [94mLoss[0m : 9.43162
[1mStep[0m  [160/169], [94mLoss[0m : 8.75850

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.714, [92mTest[0m: 9.528, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.45095
[1mStep[0m  [16/169], [94mLoss[0m : 9.46051
[1mStep[0m  [32/169], [94mLoss[0m : 9.63576
[1mStep[0m  [48/169], [94mLoss[0m : 10.97076
[1mStep[0m  [64/169], [94mLoss[0m : 9.97612
[1mStep[0m  [80/169], [94mLoss[0m : 9.96245
[1mStep[0m  [96/169], [94mLoss[0m : 9.72556
[1mStep[0m  [112/169], [94mLoss[0m : 9.01442
[1mStep[0m  [128/169], [94mLoss[0m : 8.92559
[1mStep[0m  [144/169], [94mLoss[0m : 9.32783
[1mStep[0m  [160/169], [94mLoss[0m : 9.33880

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.689, [92mTest[0m: 9.510, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.15547
[1mStep[0m  [16/169], [94mLoss[0m : 10.11346
[1mStep[0m  [32/169], [94mLoss[0m : 9.65490
[1mStep[0m  [48/169], [94mLoss[0m : 9.18642
[1mStep[0m  [64/169], [94mLoss[0m : 9.49010
[1mStep[0m  [80/169], [94mLoss[0m : 9.51588
[1mStep[0m  [96/169], [94mLoss[0m : 9.99664
[1mStep[0m  [112/169], [94mLoss[0m : 9.56956
[1mStep[0m  [128/169], [94mLoss[0m : 9.27587
[1mStep[0m  [144/169], [94mLoss[0m : 9.17626
[1mStep[0m  [160/169], [94mLoss[0m : 9.01613

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.657, [92mTest[0m: 9.450, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.11576
[1mStep[0m  [16/169], [94mLoss[0m : 10.02860
[1mStep[0m  [32/169], [94mLoss[0m : 10.22681
[1mStep[0m  [48/169], [94mLoss[0m : 9.89597
[1mStep[0m  [64/169], [94mLoss[0m : 9.99539
[1mStep[0m  [80/169], [94mLoss[0m : 9.47039
[1mStep[0m  [96/169], [94mLoss[0m : 8.80692
[1mStep[0m  [112/169], [94mLoss[0m : 10.22967
[1mStep[0m  [128/169], [94mLoss[0m : 9.48846
[1mStep[0m  [144/169], [94mLoss[0m : 9.76681
[1mStep[0m  [160/169], [94mLoss[0m : 9.24541

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.635, [92mTest[0m: 9.536, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.37335
[1mStep[0m  [16/169], [94mLoss[0m : 9.78827
[1mStep[0m  [32/169], [94mLoss[0m : 9.65893
[1mStep[0m  [48/169], [94mLoss[0m : 10.27269
[1mStep[0m  [64/169], [94mLoss[0m : 9.50745
[1mStep[0m  [80/169], [94mLoss[0m : 9.68875
[1mStep[0m  [96/169], [94mLoss[0m : 9.63702
[1mStep[0m  [112/169], [94mLoss[0m : 9.69462
[1mStep[0m  [128/169], [94mLoss[0m : 10.12629
[1mStep[0m  [144/169], [94mLoss[0m : 9.39790
[1mStep[0m  [160/169], [94mLoss[0m : 9.06831

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.601, [92mTest[0m: 9.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.76032
[1mStep[0m  [16/169], [94mLoss[0m : 9.30351
[1mStep[0m  [32/169], [94mLoss[0m : 9.08853
[1mStep[0m  [48/169], [94mLoss[0m : 9.98669
[1mStep[0m  [64/169], [94mLoss[0m : 9.62951
[1mStep[0m  [80/169], [94mLoss[0m : 9.65888
[1mStep[0m  [96/169], [94mLoss[0m : 9.96872
[1mStep[0m  [112/169], [94mLoss[0m : 8.43358
[1mStep[0m  [128/169], [94mLoss[0m : 10.35016
[1mStep[0m  [144/169], [94mLoss[0m : 10.10270
[1mStep[0m  [160/169], [94mLoss[0m : 9.41095

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.582, [92mTest[0m: 9.393, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.19445
[1mStep[0m  [16/169], [94mLoss[0m : 9.43323
[1mStep[0m  [32/169], [94mLoss[0m : 9.27673
[1mStep[0m  [48/169], [94mLoss[0m : 9.60599
[1mStep[0m  [64/169], [94mLoss[0m : 9.46799
[1mStep[0m  [80/169], [94mLoss[0m : 9.77183
[1mStep[0m  [96/169], [94mLoss[0m : 8.69792
[1mStep[0m  [112/169], [94mLoss[0m : 9.57965
[1mStep[0m  [128/169], [94mLoss[0m : 9.34120
[1mStep[0m  [144/169], [94mLoss[0m : 9.76880
[1mStep[0m  [160/169], [94mLoss[0m : 9.47288

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.548, [92mTest[0m: 9.365, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.90495
[1mStep[0m  [16/169], [94mLoss[0m : 10.17827
[1mStep[0m  [32/169], [94mLoss[0m : 9.39202
[1mStep[0m  [48/169], [94mLoss[0m : 9.60689
[1mStep[0m  [64/169], [94mLoss[0m : 9.85508
[1mStep[0m  [80/169], [94mLoss[0m : 9.84098
[1mStep[0m  [96/169], [94mLoss[0m : 9.62000
[1mStep[0m  [112/169], [94mLoss[0m : 9.18339
[1mStep[0m  [128/169], [94mLoss[0m : 9.86072
[1mStep[0m  [144/169], [94mLoss[0m : 9.51046
[1mStep[0m  [160/169], [94mLoss[0m : 8.64437

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.531, [92mTest[0m: 9.314, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.60102
[1mStep[0m  [16/169], [94mLoss[0m : 10.13338
[1mStep[0m  [32/169], [94mLoss[0m : 10.06414
[1mStep[0m  [48/169], [94mLoss[0m : 9.79088
[1mStep[0m  [64/169], [94mLoss[0m : 10.75235
[1mStep[0m  [80/169], [94mLoss[0m : 9.23810
[1mStep[0m  [96/169], [94mLoss[0m : 9.61115
[1mStep[0m  [112/169], [94mLoss[0m : 9.10976
[1mStep[0m  [128/169], [94mLoss[0m : 9.69474
[1mStep[0m  [144/169], [94mLoss[0m : 8.80688
[1mStep[0m  [160/169], [94mLoss[0m : 9.90892

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.498, [92mTest[0m: 9.225, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.17913
[1mStep[0m  [16/169], [94mLoss[0m : 9.53681
[1mStep[0m  [32/169], [94mLoss[0m : 9.10094
[1mStep[0m  [48/169], [94mLoss[0m : 9.52317
[1mStep[0m  [64/169], [94mLoss[0m : 8.83573
[1mStep[0m  [80/169], [94mLoss[0m : 10.08346
[1mStep[0m  [96/169], [94mLoss[0m : 9.35702
[1mStep[0m  [112/169], [94mLoss[0m : 9.62709
[1mStep[0m  [128/169], [94mLoss[0m : 8.75887
[1mStep[0m  [144/169], [94mLoss[0m : 10.00154
[1mStep[0m  [160/169], [94mLoss[0m : 10.01708

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.476, [92mTest[0m: 9.321, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.82705
[1mStep[0m  [16/169], [94mLoss[0m : 8.84440
[1mStep[0m  [32/169], [94mLoss[0m : 9.36889
[1mStep[0m  [48/169], [94mLoss[0m : 9.13466
[1mStep[0m  [64/169], [94mLoss[0m : 8.93243
[1mStep[0m  [80/169], [94mLoss[0m : 9.34330
[1mStep[0m  [96/169], [94mLoss[0m : 8.86366
[1mStep[0m  [112/169], [94mLoss[0m : 9.33054
[1mStep[0m  [128/169], [94mLoss[0m : 9.95108
[1mStep[0m  [144/169], [94mLoss[0m : 9.50616
[1mStep[0m  [160/169], [94mLoss[0m : 9.88062

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.440, [92mTest[0m: 9.252, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.182
====================================

Phase 2 - Evaluation MAE:  9.182383452142988
MAE score P1      10.127862
MAE score P2       9.182383
loss               9.440259
learning_rate        0.0001
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.26618
[1mStep[0m  [16/169], [94mLoss[0m : 11.26144
[1mStep[0m  [32/169], [94mLoss[0m : 10.46260
[1mStep[0m  [48/169], [94mLoss[0m : 10.70638
[1mStep[0m  [64/169], [94mLoss[0m : 10.21961
[1mStep[0m  [80/169], [94mLoss[0m : 10.52559
[1mStep[0m  [96/169], [94mLoss[0m : 10.82787
[1mStep[0m  [112/169], [94mLoss[0m : 10.92156
[1mStep[0m  [128/169], [94mLoss[0m : 11.34597
[1mStep[0m  [144/169], [94mLoss[0m : 10.89859
[1mStep[0m  [160/169], [94mLoss[0m : 11.16361

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.07659
[1mStep[0m  [16/169], [94mLoss[0m : 10.49569
[1mStep[0m  [32/169], [94mLoss[0m : 10.26051
[1mStep[0m  [48/169], [94mLoss[0m : 10.39525
[1mStep[0m  [64/169], [94mLoss[0m : 10.63231
[1mStep[0m  [80/169], [94mLoss[0m : 11.23968
[1mStep[0m  [96/169], [94mLoss[0m : 10.39175
[1mStep[0m  [112/169], [94mLoss[0m : 10.49030
[1mStep[0m  [128/169], [94mLoss[0m : 10.81998
[1mStep[0m  [144/169], [94mLoss[0m : 10.57043
[1mStep[0m  [160/169], [94mLoss[0m : 9.82503

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.469, [92mTest[0m: 10.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.63218
[1mStep[0m  [16/169], [94mLoss[0m : 10.50043
[1mStep[0m  [32/169], [94mLoss[0m : 10.03541
[1mStep[0m  [48/169], [94mLoss[0m : 9.38746
[1mStep[0m  [64/169], [94mLoss[0m : 9.66717
[1mStep[0m  [80/169], [94mLoss[0m : 9.93865
[1mStep[0m  [96/169], [94mLoss[0m : 10.04678
[1mStep[0m  [112/169], [94mLoss[0m : 10.60151
[1mStep[0m  [128/169], [94mLoss[0m : 10.13170
[1mStep[0m  [144/169], [94mLoss[0m : 9.11941
[1mStep[0m  [160/169], [94mLoss[0m : 10.40992

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.211, [92mTest[0m: 10.329, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.40478
[1mStep[0m  [16/169], [94mLoss[0m : 9.14767
[1mStep[0m  [32/169], [94mLoss[0m : 10.32859
[1mStep[0m  [48/169], [94mLoss[0m : 10.17227
[1mStep[0m  [64/169], [94mLoss[0m : 10.38473
[1mStep[0m  [80/169], [94mLoss[0m : 9.85894
[1mStep[0m  [96/169], [94mLoss[0m : 10.32827
[1mStep[0m  [112/169], [94mLoss[0m : 9.16265
[1mStep[0m  [128/169], [94mLoss[0m : 9.69456
[1mStep[0m  [144/169], [94mLoss[0m : 10.22343
[1mStep[0m  [160/169], [94mLoss[0m : 9.56066

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.958, [92mTest[0m: 10.084, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.88342
[1mStep[0m  [16/169], [94mLoss[0m : 9.42626
[1mStep[0m  [32/169], [94mLoss[0m : 10.27599
[1mStep[0m  [48/169], [94mLoss[0m : 10.30539
[1mStep[0m  [64/169], [94mLoss[0m : 9.70351
[1mStep[0m  [80/169], [94mLoss[0m : 9.45017
[1mStep[0m  [96/169], [94mLoss[0m : 10.20994
[1mStep[0m  [112/169], [94mLoss[0m : 10.22927
[1mStep[0m  [128/169], [94mLoss[0m : 9.20359
[1mStep[0m  [144/169], [94mLoss[0m : 9.45403
[1mStep[0m  [160/169], [94mLoss[0m : 9.64552

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.704, [92mTest[0m: 9.809, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.73722
[1mStep[0m  [16/169], [94mLoss[0m : 9.06254
[1mStep[0m  [32/169], [94mLoss[0m : 9.40439
[1mStep[0m  [48/169], [94mLoss[0m : 9.45854
[1mStep[0m  [64/169], [94mLoss[0m : 8.92827
[1mStep[0m  [80/169], [94mLoss[0m : 8.75615
[1mStep[0m  [96/169], [94mLoss[0m : 9.63135
[1mStep[0m  [112/169], [94mLoss[0m : 10.12870
[1mStep[0m  [128/169], [94mLoss[0m : 10.15494
[1mStep[0m  [144/169], [94mLoss[0m : 9.15720
[1mStep[0m  [160/169], [94mLoss[0m : 8.29921

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.446, [92mTest[0m: 9.570, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.50035
[1mStep[0m  [16/169], [94mLoss[0m : 9.01935
[1mStep[0m  [32/169], [94mLoss[0m : 9.45357
[1mStep[0m  [48/169], [94mLoss[0m : 9.11403
[1mStep[0m  [64/169], [94mLoss[0m : 9.03523
[1mStep[0m  [80/169], [94mLoss[0m : 9.41762
[1mStep[0m  [96/169], [94mLoss[0m : 9.37106
[1mStep[0m  [112/169], [94mLoss[0m : 9.86042
[1mStep[0m  [128/169], [94mLoss[0m : 9.60542
[1mStep[0m  [144/169], [94mLoss[0m : 8.29589
[1mStep[0m  [160/169], [94mLoss[0m : 10.13426

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.197, [92mTest[0m: 9.313, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.17266
[1mStep[0m  [16/169], [94mLoss[0m : 8.85401
[1mStep[0m  [32/169], [94mLoss[0m : 7.92358
[1mStep[0m  [48/169], [94mLoss[0m : 9.66469
[1mStep[0m  [64/169], [94mLoss[0m : 9.31539
[1mStep[0m  [80/169], [94mLoss[0m : 8.76517
[1mStep[0m  [96/169], [94mLoss[0m : 9.64951
[1mStep[0m  [112/169], [94mLoss[0m : 9.97142
[1mStep[0m  [128/169], [94mLoss[0m : 8.22295
[1mStep[0m  [144/169], [94mLoss[0m : 8.60377
[1mStep[0m  [160/169], [94mLoss[0m : 9.03647

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.944, [92mTest[0m: 9.071, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.72387
[1mStep[0m  [16/169], [94mLoss[0m : 8.55715
[1mStep[0m  [32/169], [94mLoss[0m : 8.13605
[1mStep[0m  [48/169], [94mLoss[0m : 9.07427
[1mStep[0m  [64/169], [94mLoss[0m : 8.19984
[1mStep[0m  [80/169], [94mLoss[0m : 8.75919
[1mStep[0m  [96/169], [94mLoss[0m : 9.02740
[1mStep[0m  [112/169], [94mLoss[0m : 8.98624
[1mStep[0m  [128/169], [94mLoss[0m : 8.05413
[1mStep[0m  [144/169], [94mLoss[0m : 8.47398
[1mStep[0m  [160/169], [94mLoss[0m : 8.92160

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.694, [92mTest[0m: 8.793, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.35732
[1mStep[0m  [16/169], [94mLoss[0m : 8.87780
[1mStep[0m  [32/169], [94mLoss[0m : 8.37670
[1mStep[0m  [48/169], [94mLoss[0m : 8.75587
[1mStep[0m  [64/169], [94mLoss[0m : 8.46708
[1mStep[0m  [80/169], [94mLoss[0m : 8.05800
[1mStep[0m  [96/169], [94mLoss[0m : 7.83668
[1mStep[0m  [112/169], [94mLoss[0m : 8.55688
[1mStep[0m  [128/169], [94mLoss[0m : 8.13075
[1mStep[0m  [144/169], [94mLoss[0m : 8.39636
[1mStep[0m  [160/169], [94mLoss[0m : 8.52618

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.432, [92mTest[0m: 8.540, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.96873
[1mStep[0m  [16/169], [94mLoss[0m : 8.25193
[1mStep[0m  [32/169], [94mLoss[0m : 8.13312
[1mStep[0m  [48/169], [94mLoss[0m : 8.11023
[1mStep[0m  [64/169], [94mLoss[0m : 7.47014
[1mStep[0m  [80/169], [94mLoss[0m : 8.46686
[1mStep[0m  [96/169], [94mLoss[0m : 7.97076
[1mStep[0m  [112/169], [94mLoss[0m : 9.03783
[1mStep[0m  [128/169], [94mLoss[0m : 8.43081
[1mStep[0m  [144/169], [94mLoss[0m : 7.70443
[1mStep[0m  [160/169], [94mLoss[0m : 8.00918

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.179, [92mTest[0m: 8.278, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.57754
[1mStep[0m  [16/169], [94mLoss[0m : 8.23182
[1mStep[0m  [32/169], [94mLoss[0m : 8.29333
[1mStep[0m  [48/169], [94mLoss[0m : 7.13332
[1mStep[0m  [64/169], [94mLoss[0m : 8.00507
[1mStep[0m  [80/169], [94mLoss[0m : 7.58592
[1mStep[0m  [96/169], [94mLoss[0m : 7.93207
[1mStep[0m  [112/169], [94mLoss[0m : 7.74745
[1mStep[0m  [128/169], [94mLoss[0m : 7.78242
[1mStep[0m  [144/169], [94mLoss[0m : 7.66440
[1mStep[0m  [160/169], [94mLoss[0m : 7.80633

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.918, [92mTest[0m: 8.049, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.88470
[1mStep[0m  [16/169], [94mLoss[0m : 7.69065
[1mStep[0m  [32/169], [94mLoss[0m : 7.67120
[1mStep[0m  [48/169], [94mLoss[0m : 7.45407
[1mStep[0m  [64/169], [94mLoss[0m : 7.58054
[1mStep[0m  [80/169], [94mLoss[0m : 7.33343
[1mStep[0m  [96/169], [94mLoss[0m : 7.52409
[1mStep[0m  [112/169], [94mLoss[0m : 7.95777
[1mStep[0m  [128/169], [94mLoss[0m : 7.41450
[1mStep[0m  [144/169], [94mLoss[0m : 7.79208
[1mStep[0m  [160/169], [94mLoss[0m : 7.70160

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.672, [92mTest[0m: 7.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.27343
[1mStep[0m  [16/169], [94mLoss[0m : 8.13545
[1mStep[0m  [32/169], [94mLoss[0m : 7.71822
[1mStep[0m  [48/169], [94mLoss[0m : 7.81898
[1mStep[0m  [64/169], [94mLoss[0m : 6.89298
[1mStep[0m  [80/169], [94mLoss[0m : 6.77664
[1mStep[0m  [96/169], [94mLoss[0m : 7.15655
[1mStep[0m  [112/169], [94mLoss[0m : 7.87700
[1mStep[0m  [128/169], [94mLoss[0m : 7.31283
[1mStep[0m  [144/169], [94mLoss[0m : 7.64334
[1mStep[0m  [160/169], [94mLoss[0m : 6.75269

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.413, [92mTest[0m: 7.535, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.40205
[1mStep[0m  [16/169], [94mLoss[0m : 7.01725
[1mStep[0m  [32/169], [94mLoss[0m : 7.66890
[1mStep[0m  [48/169], [94mLoss[0m : 7.37490
[1mStep[0m  [64/169], [94mLoss[0m : 7.43743
[1mStep[0m  [80/169], [94mLoss[0m : 6.89878
[1mStep[0m  [96/169], [94mLoss[0m : 6.11153
[1mStep[0m  [112/169], [94mLoss[0m : 7.57341
[1mStep[0m  [128/169], [94mLoss[0m : 6.97608
[1mStep[0m  [144/169], [94mLoss[0m : 7.31332
[1mStep[0m  [160/169], [94mLoss[0m : 7.40626

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.161, [92mTest[0m: 7.276, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.29942
[1mStep[0m  [16/169], [94mLoss[0m : 7.21844
[1mStep[0m  [32/169], [94mLoss[0m : 7.08828
[1mStep[0m  [48/169], [94mLoss[0m : 7.60475
[1mStep[0m  [64/169], [94mLoss[0m : 6.79579
[1mStep[0m  [80/169], [94mLoss[0m : 6.46335
[1mStep[0m  [96/169], [94mLoss[0m : 7.38793
[1mStep[0m  [112/169], [94mLoss[0m : 6.23913
[1mStep[0m  [128/169], [94mLoss[0m : 5.86587
[1mStep[0m  [144/169], [94mLoss[0m : 7.32766
[1mStep[0m  [160/169], [94mLoss[0m : 5.69817

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.913, [92mTest[0m: 7.014, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.95733
[1mStep[0m  [16/169], [94mLoss[0m : 5.85108
[1mStep[0m  [32/169], [94mLoss[0m : 7.05117
[1mStep[0m  [48/169], [94mLoss[0m : 6.86741
[1mStep[0m  [64/169], [94mLoss[0m : 6.22278
[1mStep[0m  [80/169], [94mLoss[0m : 6.27209
[1mStep[0m  [96/169], [94mLoss[0m : 6.31188
[1mStep[0m  [112/169], [94mLoss[0m : 6.90095
[1mStep[0m  [128/169], [94mLoss[0m : 6.06599
[1mStep[0m  [144/169], [94mLoss[0m : 6.86980
[1mStep[0m  [160/169], [94mLoss[0m : 7.22338

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.639, [92mTest[0m: 6.744, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.89025
[1mStep[0m  [16/169], [94mLoss[0m : 6.88113
[1mStep[0m  [32/169], [94mLoss[0m : 6.63351
[1mStep[0m  [48/169], [94mLoss[0m : 6.24824
[1mStep[0m  [64/169], [94mLoss[0m : 6.70653
[1mStep[0m  [80/169], [94mLoss[0m : 6.45948
[1mStep[0m  [96/169], [94mLoss[0m : 5.24499
[1mStep[0m  [112/169], [94mLoss[0m : 6.88409
[1mStep[0m  [128/169], [94mLoss[0m : 5.98452
[1mStep[0m  [144/169], [94mLoss[0m : 6.14404
[1mStep[0m  [160/169], [94mLoss[0m : 6.35755

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.393, [92mTest[0m: 6.512, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.42399
[1mStep[0m  [16/169], [94mLoss[0m : 6.50395
[1mStep[0m  [32/169], [94mLoss[0m : 6.21737
[1mStep[0m  [48/169], [94mLoss[0m : 6.45651
[1mStep[0m  [64/169], [94mLoss[0m : 6.82717
[1mStep[0m  [80/169], [94mLoss[0m : 6.35271
[1mStep[0m  [96/169], [94mLoss[0m : 6.59869
[1mStep[0m  [112/169], [94mLoss[0m : 5.62756
[1mStep[0m  [128/169], [94mLoss[0m : 5.56702
[1mStep[0m  [144/169], [94mLoss[0m : 6.33196
[1mStep[0m  [160/169], [94mLoss[0m : 5.47655

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.148, [92mTest[0m: 6.261, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.61013
[1mStep[0m  [16/169], [94mLoss[0m : 5.50433
[1mStep[0m  [32/169], [94mLoss[0m : 5.71577
[1mStep[0m  [48/169], [94mLoss[0m : 5.27555
[1mStep[0m  [64/169], [94mLoss[0m : 6.08289
[1mStep[0m  [80/169], [94mLoss[0m : 5.83011
[1mStep[0m  [96/169], [94mLoss[0m : 6.93032
[1mStep[0m  [112/169], [94mLoss[0m : 5.73315
[1mStep[0m  [128/169], [94mLoss[0m : 6.36563
[1mStep[0m  [144/169], [94mLoss[0m : 6.24427
[1mStep[0m  [160/169], [94mLoss[0m : 5.66705

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.913, [92mTest[0m: 6.005, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.31643
[1mStep[0m  [16/169], [94mLoss[0m : 6.31261
[1mStep[0m  [32/169], [94mLoss[0m : 5.26162
[1mStep[0m  [48/169], [94mLoss[0m : 6.01191
[1mStep[0m  [64/169], [94mLoss[0m : 5.71482
[1mStep[0m  [80/169], [94mLoss[0m : 5.63934
[1mStep[0m  [96/169], [94mLoss[0m : 5.70890
[1mStep[0m  [112/169], [94mLoss[0m : 5.72579
[1mStep[0m  [128/169], [94mLoss[0m : 6.01725
[1mStep[0m  [144/169], [94mLoss[0m : 5.42113
[1mStep[0m  [160/169], [94mLoss[0m : 5.72667

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.669, [92mTest[0m: 5.725, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.46688
[1mStep[0m  [16/169], [94mLoss[0m : 4.91753
[1mStep[0m  [32/169], [94mLoss[0m : 5.44959
[1mStep[0m  [48/169], [94mLoss[0m : 5.41105
[1mStep[0m  [64/169], [94mLoss[0m : 5.35058
[1mStep[0m  [80/169], [94mLoss[0m : 5.53380
[1mStep[0m  [96/169], [94mLoss[0m : 5.54765
[1mStep[0m  [112/169], [94mLoss[0m : 5.36593
[1mStep[0m  [128/169], [94mLoss[0m : 6.24810
[1mStep[0m  [144/169], [94mLoss[0m : 6.00226
[1mStep[0m  [160/169], [94mLoss[0m : 5.22091

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.465, [92mTest[0m: 5.532, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.93749
[1mStep[0m  [16/169], [94mLoss[0m : 5.41352
[1mStep[0m  [32/169], [94mLoss[0m : 4.89278
[1mStep[0m  [48/169], [94mLoss[0m : 5.58297
[1mStep[0m  [64/169], [94mLoss[0m : 5.51862
[1mStep[0m  [80/169], [94mLoss[0m : 4.85789
[1mStep[0m  [96/169], [94mLoss[0m : 5.81512
[1mStep[0m  [112/169], [94mLoss[0m : 4.72213
[1mStep[0m  [128/169], [94mLoss[0m : 6.37952
[1mStep[0m  [144/169], [94mLoss[0m : 5.28683
[1mStep[0m  [160/169], [94mLoss[0m : 4.74839

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.255, [92mTest[0m: 5.305, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.24499
[1mStep[0m  [16/169], [94mLoss[0m : 5.69483
[1mStep[0m  [32/169], [94mLoss[0m : 5.26959
[1mStep[0m  [48/169], [94mLoss[0m : 5.80619
[1mStep[0m  [64/169], [94mLoss[0m : 4.84056
[1mStep[0m  [80/169], [94mLoss[0m : 4.27852
[1mStep[0m  [96/169], [94mLoss[0m : 4.65439
[1mStep[0m  [112/169], [94mLoss[0m : 5.57567
[1mStep[0m  [128/169], [94mLoss[0m : 5.40293
[1mStep[0m  [144/169], [94mLoss[0m : 5.32959
[1mStep[0m  [160/169], [94mLoss[0m : 5.51575

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.071, [92mTest[0m: 5.105, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.60139
[1mStep[0m  [16/169], [94mLoss[0m : 4.87784
[1mStep[0m  [32/169], [94mLoss[0m : 5.31276
[1mStep[0m  [48/169], [94mLoss[0m : 4.77519
[1mStep[0m  [64/169], [94mLoss[0m : 4.19082
[1mStep[0m  [80/169], [94mLoss[0m : 4.78149
[1mStep[0m  [96/169], [94mLoss[0m : 4.69830
[1mStep[0m  [112/169], [94mLoss[0m : 4.72194
[1mStep[0m  [128/169], [94mLoss[0m : 4.53784
[1mStep[0m  [144/169], [94mLoss[0m : 4.43069
[1mStep[0m  [160/169], [94mLoss[0m : 5.15327

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.894, [92mTest[0m: 4.932, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.14311
[1mStep[0m  [16/169], [94mLoss[0m : 5.60682
[1mStep[0m  [32/169], [94mLoss[0m : 5.11456
[1mStep[0m  [48/169], [94mLoss[0m : 5.30702
[1mStep[0m  [64/169], [94mLoss[0m : 5.08900
[1mStep[0m  [80/169], [94mLoss[0m : 4.93472
[1mStep[0m  [96/169], [94mLoss[0m : 4.78200
[1mStep[0m  [112/169], [94mLoss[0m : 4.81678
[1mStep[0m  [128/169], [94mLoss[0m : 5.14581
[1mStep[0m  [144/169], [94mLoss[0m : 4.62857
[1mStep[0m  [160/169], [94mLoss[0m : 4.60752

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.735, [92mTest[0m: 4.712, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.37375
[1mStep[0m  [16/169], [94mLoss[0m : 4.46420
[1mStep[0m  [32/169], [94mLoss[0m : 4.27372
[1mStep[0m  [48/169], [94mLoss[0m : 4.53704
[1mStep[0m  [64/169], [94mLoss[0m : 4.87038
[1mStep[0m  [80/169], [94mLoss[0m : 4.08644
[1mStep[0m  [96/169], [94mLoss[0m : 4.48009
[1mStep[0m  [112/169], [94mLoss[0m : 4.82709
[1mStep[0m  [128/169], [94mLoss[0m : 4.88318
[1mStep[0m  [144/169], [94mLoss[0m : 4.71100
[1mStep[0m  [160/169], [94mLoss[0m : 4.24075

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.565, [92mTest[0m: 4.571, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.19641
[1mStep[0m  [16/169], [94mLoss[0m : 4.63944
[1mStep[0m  [32/169], [94mLoss[0m : 4.89386
[1mStep[0m  [48/169], [94mLoss[0m : 4.24371
[1mStep[0m  [64/169], [94mLoss[0m : 4.98474
[1mStep[0m  [80/169], [94mLoss[0m : 4.34511
[1mStep[0m  [96/169], [94mLoss[0m : 3.95173
[1mStep[0m  [112/169], [94mLoss[0m : 4.02599
[1mStep[0m  [128/169], [94mLoss[0m : 5.00607
[1mStep[0m  [144/169], [94mLoss[0m : 3.92384
[1mStep[0m  [160/169], [94mLoss[0m : 4.37677

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.428, [92mTest[0m: 4.430, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.86445
[1mStep[0m  [16/169], [94mLoss[0m : 4.95305
[1mStep[0m  [32/169], [94mLoss[0m : 3.91547
[1mStep[0m  [48/169], [94mLoss[0m : 3.66757
[1mStep[0m  [64/169], [94mLoss[0m : 4.34182
[1mStep[0m  [80/169], [94mLoss[0m : 4.17725
[1mStep[0m  [96/169], [94mLoss[0m : 3.99739
[1mStep[0m  [112/169], [94mLoss[0m : 4.26254
[1mStep[0m  [128/169], [94mLoss[0m : 3.91260
[1mStep[0m  [144/169], [94mLoss[0m : 4.69499
[1mStep[0m  [160/169], [94mLoss[0m : 4.63059

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.296, [92mTest[0m: 4.259, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.91229
[1mStep[0m  [16/169], [94mLoss[0m : 3.76997
[1mStep[0m  [32/169], [94mLoss[0m : 4.53147
[1mStep[0m  [48/169], [94mLoss[0m : 3.66653
[1mStep[0m  [64/169], [94mLoss[0m : 4.47515
[1mStep[0m  [80/169], [94mLoss[0m : 3.84044
[1mStep[0m  [96/169], [94mLoss[0m : 4.31604
[1mStep[0m  [112/169], [94mLoss[0m : 4.13085
[1mStep[0m  [128/169], [94mLoss[0m : 5.03205
[1mStep[0m  [144/169], [94mLoss[0m : 4.13815
[1mStep[0m  [160/169], [94mLoss[0m : 4.40759

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.170, [92mTest[0m: 4.126, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.000
====================================

Phase 1 - Evaluation MAE:  4.000293484755924
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 4.57804
[1mStep[0m  [16/169], [94mLoss[0m : 4.58772
[1mStep[0m  [32/169], [94mLoss[0m : 4.33355
[1mStep[0m  [48/169], [94mLoss[0m : 4.00991
[1mStep[0m  [64/169], [94mLoss[0m : 4.76230
[1mStep[0m  [80/169], [94mLoss[0m : 3.40987
[1mStep[0m  [96/169], [94mLoss[0m : 3.42207
[1mStep[0m  [112/169], [94mLoss[0m : 5.03702
[1mStep[0m  [128/169], [94mLoss[0m : 3.88796
[1mStep[0m  [144/169], [94mLoss[0m : 4.91857
[1mStep[0m  [160/169], [94mLoss[0m : 3.98768

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.054, [92mTest[0m: 3.998, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.42230
[1mStep[0m  [16/169], [94mLoss[0m : 3.24995
[1mStep[0m  [32/169], [94mLoss[0m : 3.77586
[1mStep[0m  [48/169], [94mLoss[0m : 4.01533
[1mStep[0m  [64/169], [94mLoss[0m : 3.93964
[1mStep[0m  [80/169], [94mLoss[0m : 4.48916
[1mStep[0m  [96/169], [94mLoss[0m : 3.73057
[1mStep[0m  [112/169], [94mLoss[0m : 4.56513
[1mStep[0m  [128/169], [94mLoss[0m : 3.67610
[1mStep[0m  [144/169], [94mLoss[0m : 4.17767
[1mStep[0m  [160/169], [94mLoss[0m : 3.81943

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.943, [92mTest[0m: 3.900, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.11565
[1mStep[0m  [16/169], [94mLoss[0m : 4.03088
[1mStep[0m  [32/169], [94mLoss[0m : 4.34932
[1mStep[0m  [48/169], [94mLoss[0m : 3.68382
[1mStep[0m  [64/169], [94mLoss[0m : 3.71688
[1mStep[0m  [80/169], [94mLoss[0m : 4.22498
[1mStep[0m  [96/169], [94mLoss[0m : 4.11199
[1mStep[0m  [112/169], [94mLoss[0m : 3.50435
[1mStep[0m  [128/169], [94mLoss[0m : 3.52524
[1mStep[0m  [144/169], [94mLoss[0m : 4.10536
[1mStep[0m  [160/169], [94mLoss[0m : 4.21120

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.829, [92mTest[0m: 3.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.09618
[1mStep[0m  [16/169], [94mLoss[0m : 3.53465
[1mStep[0m  [32/169], [94mLoss[0m : 3.86581
[1mStep[0m  [48/169], [94mLoss[0m : 3.32999
[1mStep[0m  [64/169], [94mLoss[0m : 3.74290
[1mStep[0m  [80/169], [94mLoss[0m : 3.27124
[1mStep[0m  [96/169], [94mLoss[0m : 4.03272
[1mStep[0m  [112/169], [94mLoss[0m : 3.49928
[1mStep[0m  [128/169], [94mLoss[0m : 3.14417
[1mStep[0m  [144/169], [94mLoss[0m : 3.50934
[1mStep[0m  [160/169], [94mLoss[0m : 3.79980

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.713, [92mTest[0m: 3.641, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.89355
[1mStep[0m  [16/169], [94mLoss[0m : 3.92184
[1mStep[0m  [32/169], [94mLoss[0m : 3.59125
[1mStep[0m  [48/169], [94mLoss[0m : 3.93700
[1mStep[0m  [64/169], [94mLoss[0m : 3.60167
[1mStep[0m  [80/169], [94mLoss[0m : 3.50188
[1mStep[0m  [96/169], [94mLoss[0m : 3.98432
[1mStep[0m  [112/169], [94mLoss[0m : 3.18260
[1mStep[0m  [128/169], [94mLoss[0m : 3.44662
[1mStep[0m  [144/169], [94mLoss[0m : 3.41908
[1mStep[0m  [160/169], [94mLoss[0m : 3.81118

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.640, [92mTest[0m: 3.514, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.77038
[1mStep[0m  [16/169], [94mLoss[0m : 3.11526
[1mStep[0m  [32/169], [94mLoss[0m : 3.45487
[1mStep[0m  [48/169], [94mLoss[0m : 3.72138
[1mStep[0m  [64/169], [94mLoss[0m : 3.60339
[1mStep[0m  [80/169], [94mLoss[0m : 3.28310
[1mStep[0m  [96/169], [94mLoss[0m : 2.77138
[1mStep[0m  [112/169], [94mLoss[0m : 3.43115
[1mStep[0m  [128/169], [94mLoss[0m : 3.42302
[1mStep[0m  [144/169], [94mLoss[0m : 3.75139
[1mStep[0m  [160/169], [94mLoss[0m : 2.94995

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.538, [92mTest[0m: 3.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.20192
[1mStep[0m  [16/169], [94mLoss[0m : 3.55633
[1mStep[0m  [32/169], [94mLoss[0m : 3.22123
[1mStep[0m  [48/169], [94mLoss[0m : 4.15690
[1mStep[0m  [64/169], [94mLoss[0m : 3.88314
[1mStep[0m  [80/169], [94mLoss[0m : 3.55447
[1mStep[0m  [96/169], [94mLoss[0m : 3.32366
[1mStep[0m  [112/169], [94mLoss[0m : 3.90098
[1mStep[0m  [128/169], [94mLoss[0m : 3.89117
[1mStep[0m  [144/169], [94mLoss[0m : 3.54644
[1mStep[0m  [160/169], [94mLoss[0m : 3.18683

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.462, [92mTest[0m: 3.335, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.54142
[1mStep[0m  [16/169], [94mLoss[0m : 3.54825
[1mStep[0m  [32/169], [94mLoss[0m : 3.66861
[1mStep[0m  [48/169], [94mLoss[0m : 3.50397
[1mStep[0m  [64/169], [94mLoss[0m : 3.25082
[1mStep[0m  [80/169], [94mLoss[0m : 2.92746
[1mStep[0m  [96/169], [94mLoss[0m : 3.65597
[1mStep[0m  [112/169], [94mLoss[0m : 3.82714
[1mStep[0m  [128/169], [94mLoss[0m : 4.20299
[1mStep[0m  [144/169], [94mLoss[0m : 3.50849
[1mStep[0m  [160/169], [94mLoss[0m : 2.81728

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.392, [92mTest[0m: 3.256, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.46175
[1mStep[0m  [16/169], [94mLoss[0m : 2.88063
[1mStep[0m  [32/169], [94mLoss[0m : 3.15991
[1mStep[0m  [48/169], [94mLoss[0m : 3.06609
[1mStep[0m  [64/169], [94mLoss[0m : 3.06178
[1mStep[0m  [80/169], [94mLoss[0m : 3.46699
[1mStep[0m  [96/169], [94mLoss[0m : 3.44646
[1mStep[0m  [112/169], [94mLoss[0m : 3.50023
[1mStep[0m  [128/169], [94mLoss[0m : 3.35545
[1mStep[0m  [144/169], [94mLoss[0m : 3.14579
[1mStep[0m  [160/169], [94mLoss[0m : 3.52815

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.340, [92mTest[0m: 3.173, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.10859
[1mStep[0m  [16/169], [94mLoss[0m : 3.31598
[1mStep[0m  [32/169], [94mLoss[0m : 2.83015
[1mStep[0m  [48/169], [94mLoss[0m : 3.60211
[1mStep[0m  [64/169], [94mLoss[0m : 2.51269
[1mStep[0m  [80/169], [94mLoss[0m : 3.66003
[1mStep[0m  [96/169], [94mLoss[0m : 3.69101
[1mStep[0m  [112/169], [94mLoss[0m : 3.25820
[1mStep[0m  [128/169], [94mLoss[0m : 3.26431
[1mStep[0m  [144/169], [94mLoss[0m : 3.58240
[1mStep[0m  [160/169], [94mLoss[0m : 3.01654

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.275, [92mTest[0m: 3.116, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75543
[1mStep[0m  [16/169], [94mLoss[0m : 2.98536
[1mStep[0m  [32/169], [94mLoss[0m : 2.52303
[1mStep[0m  [48/169], [94mLoss[0m : 3.37069
[1mStep[0m  [64/169], [94mLoss[0m : 3.10980
[1mStep[0m  [80/169], [94mLoss[0m : 3.18918
[1mStep[0m  [96/169], [94mLoss[0m : 2.85838
[1mStep[0m  [112/169], [94mLoss[0m : 3.41584
[1mStep[0m  [128/169], [94mLoss[0m : 3.56292
[1mStep[0m  [144/169], [94mLoss[0m : 2.90001
[1mStep[0m  [160/169], [94mLoss[0m : 3.04214

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.226, [92mTest[0m: 3.039, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.53978
[1mStep[0m  [16/169], [94mLoss[0m : 2.93737
[1mStep[0m  [32/169], [94mLoss[0m : 3.58200
[1mStep[0m  [48/169], [94mLoss[0m : 2.95024
[1mStep[0m  [64/169], [94mLoss[0m : 3.23841
[1mStep[0m  [80/169], [94mLoss[0m : 3.34691
[1mStep[0m  [96/169], [94mLoss[0m : 3.21576
[1mStep[0m  [112/169], [94mLoss[0m : 3.24561
[1mStep[0m  [128/169], [94mLoss[0m : 2.79885
[1mStep[0m  [144/169], [94mLoss[0m : 3.25097
[1mStep[0m  [160/169], [94mLoss[0m : 3.12018

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.168, [92mTest[0m: 2.981, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.09171
[1mStep[0m  [16/169], [94mLoss[0m : 3.03174
[1mStep[0m  [32/169], [94mLoss[0m : 2.76843
[1mStep[0m  [48/169], [94mLoss[0m : 3.13219
[1mStep[0m  [64/169], [94mLoss[0m : 3.41799
[1mStep[0m  [80/169], [94mLoss[0m : 2.79769
[1mStep[0m  [96/169], [94mLoss[0m : 3.16585
[1mStep[0m  [112/169], [94mLoss[0m : 3.18196
[1mStep[0m  [128/169], [94mLoss[0m : 3.31396
[1mStep[0m  [144/169], [94mLoss[0m : 2.47294
[1mStep[0m  [160/169], [94mLoss[0m : 3.76355

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.134, [92mTest[0m: 2.923, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68964
[1mStep[0m  [16/169], [94mLoss[0m : 3.23697
[1mStep[0m  [32/169], [94mLoss[0m : 3.51586
[1mStep[0m  [48/169], [94mLoss[0m : 2.82748
[1mStep[0m  [64/169], [94mLoss[0m : 3.45711
[1mStep[0m  [80/169], [94mLoss[0m : 2.87818
[1mStep[0m  [96/169], [94mLoss[0m : 3.12244
[1mStep[0m  [112/169], [94mLoss[0m : 2.99666
[1mStep[0m  [128/169], [94mLoss[0m : 3.02914
[1mStep[0m  [144/169], [94mLoss[0m : 2.79080
[1mStep[0m  [160/169], [94mLoss[0m : 4.14109

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.067, [92mTest[0m: 2.894, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.16513
[1mStep[0m  [16/169], [94mLoss[0m : 2.65460
[1mStep[0m  [32/169], [94mLoss[0m : 3.27320
[1mStep[0m  [48/169], [94mLoss[0m : 3.03398
[1mStep[0m  [64/169], [94mLoss[0m : 2.79575
[1mStep[0m  [80/169], [94mLoss[0m : 3.24443
[1mStep[0m  [96/169], [94mLoss[0m : 2.89089
[1mStep[0m  [112/169], [94mLoss[0m : 3.46789
[1mStep[0m  [128/169], [94mLoss[0m : 3.07374
[1mStep[0m  [144/169], [94mLoss[0m : 2.49068
[1mStep[0m  [160/169], [94mLoss[0m : 2.78738

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.052, [92mTest[0m: 2.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.41228
[1mStep[0m  [16/169], [94mLoss[0m : 2.88556
[1mStep[0m  [32/169], [94mLoss[0m : 3.35985
[1mStep[0m  [48/169], [94mLoss[0m : 3.19803
[1mStep[0m  [64/169], [94mLoss[0m : 3.03929
[1mStep[0m  [80/169], [94mLoss[0m : 2.69837
[1mStep[0m  [96/169], [94mLoss[0m : 3.03679
[1mStep[0m  [112/169], [94mLoss[0m : 3.21005
[1mStep[0m  [128/169], [94mLoss[0m : 3.41561
[1mStep[0m  [144/169], [94mLoss[0m : 2.65009
[1mStep[0m  [160/169], [94mLoss[0m : 3.40588

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.015, [92mTest[0m: 2.804, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72439
[1mStep[0m  [16/169], [94mLoss[0m : 2.50119
[1mStep[0m  [32/169], [94mLoss[0m : 3.47634
[1mStep[0m  [48/169], [94mLoss[0m : 2.68403
[1mStep[0m  [64/169], [94mLoss[0m : 2.67578
[1mStep[0m  [80/169], [94mLoss[0m : 3.01524
[1mStep[0m  [96/169], [94mLoss[0m : 3.07581
[1mStep[0m  [112/169], [94mLoss[0m : 3.00344
[1mStep[0m  [128/169], [94mLoss[0m : 3.10389
[1mStep[0m  [144/169], [94mLoss[0m : 2.81453
[1mStep[0m  [160/169], [94mLoss[0m : 3.31460

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.995, [92mTest[0m: 2.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57665
[1mStep[0m  [16/169], [94mLoss[0m : 4.33801
[1mStep[0m  [32/169], [94mLoss[0m : 2.74231
[1mStep[0m  [48/169], [94mLoss[0m : 3.01627
[1mStep[0m  [64/169], [94mLoss[0m : 3.63915
[1mStep[0m  [80/169], [94mLoss[0m : 2.79758
[1mStep[0m  [96/169], [94mLoss[0m : 3.38474
[1mStep[0m  [112/169], [94mLoss[0m : 3.75146
[1mStep[0m  [128/169], [94mLoss[0m : 2.89550
[1mStep[0m  [144/169], [94mLoss[0m : 2.54590
[1mStep[0m  [160/169], [94mLoss[0m : 3.31687

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.952, [92mTest[0m: 2.733, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.95469
[1mStep[0m  [16/169], [94mLoss[0m : 2.82098
[1mStep[0m  [32/169], [94mLoss[0m : 3.16635
[1mStep[0m  [48/169], [94mLoss[0m : 2.70752
[1mStep[0m  [64/169], [94mLoss[0m : 2.53789
[1mStep[0m  [80/169], [94mLoss[0m : 2.87795
[1mStep[0m  [96/169], [94mLoss[0m : 3.02692
[1mStep[0m  [112/169], [94mLoss[0m : 2.50814
[1mStep[0m  [128/169], [94mLoss[0m : 2.84872
[1mStep[0m  [144/169], [94mLoss[0m : 2.74834
[1mStep[0m  [160/169], [94mLoss[0m : 3.21483

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.909, [92mTest[0m: 2.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.40723
[1mStep[0m  [16/169], [94mLoss[0m : 3.08452
[1mStep[0m  [32/169], [94mLoss[0m : 2.67614
[1mStep[0m  [48/169], [94mLoss[0m : 3.32433
[1mStep[0m  [64/169], [94mLoss[0m : 2.95423
[1mStep[0m  [80/169], [94mLoss[0m : 3.01711
[1mStep[0m  [96/169], [94mLoss[0m : 3.28004
[1mStep[0m  [112/169], [94mLoss[0m : 2.89954
[1mStep[0m  [128/169], [94mLoss[0m : 3.31115
[1mStep[0m  [144/169], [94mLoss[0m : 2.49148
[1mStep[0m  [160/169], [94mLoss[0m : 3.00927

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.892, [92mTest[0m: 2.669, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85730
[1mStep[0m  [16/169], [94mLoss[0m : 3.34777
[1mStep[0m  [32/169], [94mLoss[0m : 2.69545
[1mStep[0m  [48/169], [94mLoss[0m : 2.96923
[1mStep[0m  [64/169], [94mLoss[0m : 2.62965
[1mStep[0m  [80/169], [94mLoss[0m : 2.44909
[1mStep[0m  [96/169], [94mLoss[0m : 2.42769
[1mStep[0m  [112/169], [94mLoss[0m : 2.77349
[1mStep[0m  [128/169], [94mLoss[0m : 2.92018
[1mStep[0m  [144/169], [94mLoss[0m : 2.24890
[1mStep[0m  [160/169], [94mLoss[0m : 2.33174

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.899, [92mTest[0m: 2.628, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.05661
[1mStep[0m  [16/169], [94mLoss[0m : 2.79170
[1mStep[0m  [32/169], [94mLoss[0m : 2.79088
[1mStep[0m  [48/169], [94mLoss[0m : 3.31835
[1mStep[0m  [64/169], [94mLoss[0m : 2.96888
[1mStep[0m  [80/169], [94mLoss[0m : 2.80678
[1mStep[0m  [96/169], [94mLoss[0m : 2.71635
[1mStep[0m  [112/169], [94mLoss[0m : 2.85361
[1mStep[0m  [128/169], [94mLoss[0m : 3.04823
[1mStep[0m  [144/169], [94mLoss[0m : 2.52293
[1mStep[0m  [160/169], [94mLoss[0m : 3.11330

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.863, [92mTest[0m: 2.620, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70944
[1mStep[0m  [16/169], [94mLoss[0m : 3.06327
[1mStep[0m  [32/169], [94mLoss[0m : 2.48632
[1mStep[0m  [48/169], [94mLoss[0m : 2.25735
[1mStep[0m  [64/169], [94mLoss[0m : 3.68127
[1mStep[0m  [80/169], [94mLoss[0m : 2.54586
[1mStep[0m  [96/169], [94mLoss[0m : 3.07267
[1mStep[0m  [112/169], [94mLoss[0m : 3.23655
[1mStep[0m  [128/169], [94mLoss[0m : 3.13583
[1mStep[0m  [144/169], [94mLoss[0m : 3.62722
[1mStep[0m  [160/169], [94mLoss[0m : 2.95825

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.852, [92mTest[0m: 2.585, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87474
[1mStep[0m  [16/169], [94mLoss[0m : 3.01657
[1mStep[0m  [32/169], [94mLoss[0m : 3.03784
[1mStep[0m  [48/169], [94mLoss[0m : 3.15463
[1mStep[0m  [64/169], [94mLoss[0m : 2.74196
[1mStep[0m  [80/169], [94mLoss[0m : 2.75466
[1mStep[0m  [96/169], [94mLoss[0m : 2.78083
[1mStep[0m  [112/169], [94mLoss[0m : 2.73943
[1mStep[0m  [128/169], [94mLoss[0m : 2.39121
[1mStep[0m  [144/169], [94mLoss[0m : 3.11242
[1mStep[0m  [160/169], [94mLoss[0m : 3.07108

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.849, [92mTest[0m: 2.577, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.07677
[1mStep[0m  [16/169], [94mLoss[0m : 2.66255
[1mStep[0m  [32/169], [94mLoss[0m : 2.68620
[1mStep[0m  [48/169], [94mLoss[0m : 2.45268
[1mStep[0m  [64/169], [94mLoss[0m : 2.63341
[1mStep[0m  [80/169], [94mLoss[0m : 3.30998
[1mStep[0m  [96/169], [94mLoss[0m : 2.99481
[1mStep[0m  [112/169], [94mLoss[0m : 3.16029
[1mStep[0m  [128/169], [94mLoss[0m : 2.57035
[1mStep[0m  [144/169], [94mLoss[0m : 2.77346
[1mStep[0m  [160/169], [94mLoss[0m : 2.83509

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.832, [92mTest[0m: 2.555, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26987
[1mStep[0m  [16/169], [94mLoss[0m : 2.67479
[1mStep[0m  [32/169], [94mLoss[0m : 2.79835
[1mStep[0m  [48/169], [94mLoss[0m : 3.36571
[1mStep[0m  [64/169], [94mLoss[0m : 2.30472
[1mStep[0m  [80/169], [94mLoss[0m : 2.93397
[1mStep[0m  [96/169], [94mLoss[0m : 2.79107
[1mStep[0m  [112/169], [94mLoss[0m : 2.76112
[1mStep[0m  [128/169], [94mLoss[0m : 3.05572
[1mStep[0m  [144/169], [94mLoss[0m : 2.52003
[1mStep[0m  [160/169], [94mLoss[0m : 2.59400

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.801, [92mTest[0m: 2.551, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85683
[1mStep[0m  [16/169], [94mLoss[0m : 3.13033
[1mStep[0m  [32/169], [94mLoss[0m : 3.14088
[1mStep[0m  [48/169], [94mLoss[0m : 3.10987
[1mStep[0m  [64/169], [94mLoss[0m : 2.98405
[1mStep[0m  [80/169], [94mLoss[0m : 2.83406
[1mStep[0m  [96/169], [94mLoss[0m : 2.42661
[1mStep[0m  [112/169], [94mLoss[0m : 2.86918
[1mStep[0m  [128/169], [94mLoss[0m : 2.27736
[1mStep[0m  [144/169], [94mLoss[0m : 2.54935
[1mStep[0m  [160/169], [94mLoss[0m : 2.69262

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.533, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70674
[1mStep[0m  [16/169], [94mLoss[0m : 2.99048
[1mStep[0m  [32/169], [94mLoss[0m : 2.70842
[1mStep[0m  [48/169], [94mLoss[0m : 2.64340
[1mStep[0m  [64/169], [94mLoss[0m : 2.77603
[1mStep[0m  [80/169], [94mLoss[0m : 2.46910
[1mStep[0m  [96/169], [94mLoss[0m : 2.80760
[1mStep[0m  [112/169], [94mLoss[0m : 2.77669
[1mStep[0m  [128/169], [94mLoss[0m : 3.08001
[1mStep[0m  [144/169], [94mLoss[0m : 3.57656
[1mStep[0m  [160/169], [94mLoss[0m : 2.74535

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.792, [92mTest[0m: 2.523, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77340
[1mStep[0m  [16/169], [94mLoss[0m : 3.04679
[1mStep[0m  [32/169], [94mLoss[0m : 2.80295
[1mStep[0m  [48/169], [94mLoss[0m : 2.90692
[1mStep[0m  [64/169], [94mLoss[0m : 2.67084
[1mStep[0m  [80/169], [94mLoss[0m : 2.80371
[1mStep[0m  [96/169], [94mLoss[0m : 2.80872
[1mStep[0m  [112/169], [94mLoss[0m : 2.93012
[1mStep[0m  [128/169], [94mLoss[0m : 2.93821
[1mStep[0m  [144/169], [94mLoss[0m : 2.71265
[1mStep[0m  [160/169], [94mLoss[0m : 3.05732

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.769, [92mTest[0m: 2.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79140
[1mStep[0m  [16/169], [94mLoss[0m : 2.91515
[1mStep[0m  [32/169], [94mLoss[0m : 2.93681
[1mStep[0m  [48/169], [94mLoss[0m : 2.76537
[1mStep[0m  [64/169], [94mLoss[0m : 2.95060
[1mStep[0m  [80/169], [94mLoss[0m : 3.25481
[1mStep[0m  [96/169], [94mLoss[0m : 2.65277
[1mStep[0m  [112/169], [94mLoss[0m : 2.87756
[1mStep[0m  [128/169], [94mLoss[0m : 2.90257
[1mStep[0m  [144/169], [94mLoss[0m : 3.09996
[1mStep[0m  [160/169], [94mLoss[0m : 2.46078

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.762, [92mTest[0m: 2.516, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.482
====================================

Phase 2 - Evaluation MAE:  2.482127519590514
MAE score P1       4.000293
MAE score P2       2.482128
loss               2.761627
learning_rate        0.0001
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay         0.0001
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.30521
[1mStep[0m  [8/84], [94mLoss[0m : 10.82638
[1mStep[0m  [16/84], [94mLoss[0m : 10.67081
[1mStep[0m  [24/84], [94mLoss[0m : 10.77781
[1mStep[0m  [32/84], [94mLoss[0m : 9.43460
[1mStep[0m  [40/84], [94mLoss[0m : 9.70607
[1mStep[0m  [48/84], [94mLoss[0m : 9.40760
[1mStep[0m  [56/84], [94mLoss[0m : 10.07813
[1mStep[0m  [64/84], [94mLoss[0m : 9.54127
[1mStep[0m  [72/84], [94mLoss[0m : 9.29892
[1mStep[0m  [80/84], [94mLoss[0m : 8.09381

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.823, [92mTest[0m: 10.691, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.96005
[1mStep[0m  [8/84], [94mLoss[0m : 8.45531
[1mStep[0m  [16/84], [94mLoss[0m : 8.63773
[1mStep[0m  [24/84], [94mLoss[0m : 8.06844
[1mStep[0m  [32/84], [94mLoss[0m : 8.12836
[1mStep[0m  [40/84], [94mLoss[0m : 7.75892
[1mStep[0m  [48/84], [94mLoss[0m : 7.22119
[1mStep[0m  [56/84], [94mLoss[0m : 7.25880
[1mStep[0m  [64/84], [94mLoss[0m : 7.12122
[1mStep[0m  [72/84], [94mLoss[0m : 6.89410
[1mStep[0m  [80/84], [94mLoss[0m : 7.01707

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.669, [92mTest[0m: 8.729, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.00476
[1mStep[0m  [8/84], [94mLoss[0m : 6.43383
[1mStep[0m  [16/84], [94mLoss[0m : 5.97694
[1mStep[0m  [24/84], [94mLoss[0m : 5.72387
[1mStep[0m  [32/84], [94mLoss[0m : 5.92435
[1mStep[0m  [40/84], [94mLoss[0m : 6.12533
[1mStep[0m  [48/84], [94mLoss[0m : 5.74747
[1mStep[0m  [56/84], [94mLoss[0m : 4.99615
[1mStep[0m  [64/84], [94mLoss[0m : 5.08899
[1mStep[0m  [72/84], [94mLoss[0m : 5.13940
[1mStep[0m  [80/84], [94mLoss[0m : 4.64922

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.540, [92mTest[0m: 6.558, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.72190
[1mStep[0m  [8/84], [94mLoss[0m : 4.78349
[1mStep[0m  [16/84], [94mLoss[0m : 4.04477
[1mStep[0m  [24/84], [94mLoss[0m : 4.28964
[1mStep[0m  [32/84], [94mLoss[0m : 4.45445
[1mStep[0m  [40/84], [94mLoss[0m : 3.91383
[1mStep[0m  [48/84], [94mLoss[0m : 3.87697
[1mStep[0m  [56/84], [94mLoss[0m : 4.21370
[1mStep[0m  [64/84], [94mLoss[0m : 3.39120
[1mStep[0m  [72/84], [94mLoss[0m : 3.28099
[1mStep[0m  [80/84], [94mLoss[0m : 2.77770

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.986, [92mTest[0m: 4.595, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.11289
[1mStep[0m  [8/84], [94mLoss[0m : 3.10410
[1mStep[0m  [16/84], [94mLoss[0m : 3.60539
[1mStep[0m  [24/84], [94mLoss[0m : 3.16569
[1mStep[0m  [32/84], [94mLoss[0m : 3.79740
[1mStep[0m  [40/84], [94mLoss[0m : 2.83472
[1mStep[0m  [48/84], [94mLoss[0m : 3.50855
[1mStep[0m  [56/84], [94mLoss[0m : 3.31686
[1mStep[0m  [64/84], [94mLoss[0m : 2.82652
[1mStep[0m  [72/84], [94mLoss[0m : 2.94316
[1mStep[0m  [80/84], [94mLoss[0m : 2.91161

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.246, [92mTest[0m: 3.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.05697
[1mStep[0m  [8/84], [94mLoss[0m : 2.99150
[1mStep[0m  [16/84], [94mLoss[0m : 2.90645
[1mStep[0m  [24/84], [94mLoss[0m : 3.29576
[1mStep[0m  [32/84], [94mLoss[0m : 2.79999
[1mStep[0m  [40/84], [94mLoss[0m : 2.60106
[1mStep[0m  [48/84], [94mLoss[0m : 2.91047
[1mStep[0m  [56/84], [94mLoss[0m : 2.70681
[1mStep[0m  [64/84], [94mLoss[0m : 3.16436
[1mStep[0m  [72/84], [94mLoss[0m : 3.33194
[1mStep[0m  [80/84], [94mLoss[0m : 2.81004

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.901, [92mTest[0m: 2.912, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66976
[1mStep[0m  [8/84], [94mLoss[0m : 2.66444
[1mStep[0m  [16/84], [94mLoss[0m : 2.65723
[1mStep[0m  [24/84], [94mLoss[0m : 2.73282
[1mStep[0m  [32/84], [94mLoss[0m : 2.58449
[1mStep[0m  [40/84], [94mLoss[0m : 2.81122
[1mStep[0m  [48/84], [94mLoss[0m : 2.43892
[1mStep[0m  [56/84], [94mLoss[0m : 2.68146
[1mStep[0m  [64/84], [94mLoss[0m : 2.64171
[1mStep[0m  [72/84], [94mLoss[0m : 2.58400
[1mStep[0m  [80/84], [94mLoss[0m : 2.83298

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.649, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70213
[1mStep[0m  [8/84], [94mLoss[0m : 2.91471
[1mStep[0m  [16/84], [94mLoss[0m : 2.91393
[1mStep[0m  [24/84], [94mLoss[0m : 2.26073
[1mStep[0m  [32/84], [94mLoss[0m : 2.76530
[1mStep[0m  [40/84], [94mLoss[0m : 2.92445
[1mStep[0m  [48/84], [94mLoss[0m : 2.89988
[1mStep[0m  [56/84], [94mLoss[0m : 2.42399
[1mStep[0m  [64/84], [94mLoss[0m : 2.78001
[1mStep[0m  [72/84], [94mLoss[0m : 2.51990
[1mStep[0m  [80/84], [94mLoss[0m : 2.36073

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60050
[1mStep[0m  [8/84], [94mLoss[0m : 2.43176
[1mStep[0m  [16/84], [94mLoss[0m : 2.20920
[1mStep[0m  [24/84], [94mLoss[0m : 2.79584
[1mStep[0m  [32/84], [94mLoss[0m : 2.56053
[1mStep[0m  [40/84], [94mLoss[0m : 2.18596
[1mStep[0m  [48/84], [94mLoss[0m : 2.50307
[1mStep[0m  [56/84], [94mLoss[0m : 2.49897
[1mStep[0m  [64/84], [94mLoss[0m : 2.22668
[1mStep[0m  [72/84], [94mLoss[0m : 2.60843
[1mStep[0m  [80/84], [94mLoss[0m : 2.67714

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41959
[1mStep[0m  [8/84], [94mLoss[0m : 2.53118
[1mStep[0m  [16/84], [94mLoss[0m : 2.33470
[1mStep[0m  [24/84], [94mLoss[0m : 2.52882
[1mStep[0m  [32/84], [94mLoss[0m : 2.40586
[1mStep[0m  [40/84], [94mLoss[0m : 2.35283
[1mStep[0m  [48/84], [94mLoss[0m : 2.46900
[1mStep[0m  [56/84], [94mLoss[0m : 2.75415
[1mStep[0m  [64/84], [94mLoss[0m : 2.59450
[1mStep[0m  [72/84], [94mLoss[0m : 2.60670
[1mStep[0m  [80/84], [94mLoss[0m : 2.33278

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52838
[1mStep[0m  [8/84], [94mLoss[0m : 2.56245
[1mStep[0m  [16/84], [94mLoss[0m : 2.69690
[1mStep[0m  [24/84], [94mLoss[0m : 2.68496
[1mStep[0m  [32/84], [94mLoss[0m : 2.12390
[1mStep[0m  [40/84], [94mLoss[0m : 2.66053
[1mStep[0m  [48/84], [94mLoss[0m : 2.49967
[1mStep[0m  [56/84], [94mLoss[0m : 2.59840
[1mStep[0m  [64/84], [94mLoss[0m : 2.54816
[1mStep[0m  [72/84], [94mLoss[0m : 2.47716
[1mStep[0m  [80/84], [94mLoss[0m : 2.69470

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.451, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38090
[1mStep[0m  [8/84], [94mLoss[0m : 2.57422
[1mStep[0m  [16/84], [94mLoss[0m : 2.30877
[1mStep[0m  [24/84], [94mLoss[0m : 2.65376
[1mStep[0m  [32/84], [94mLoss[0m : 2.70813
[1mStep[0m  [40/84], [94mLoss[0m : 2.76128
[1mStep[0m  [48/84], [94mLoss[0m : 2.34141
[1mStep[0m  [56/84], [94mLoss[0m : 2.52462
[1mStep[0m  [64/84], [94mLoss[0m : 2.39923
[1mStep[0m  [72/84], [94mLoss[0m : 2.55722
[1mStep[0m  [80/84], [94mLoss[0m : 2.51329

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53562
[1mStep[0m  [8/84], [94mLoss[0m : 2.59758
[1mStep[0m  [16/84], [94mLoss[0m : 2.56142
[1mStep[0m  [24/84], [94mLoss[0m : 2.82527
[1mStep[0m  [32/84], [94mLoss[0m : 2.99013
[1mStep[0m  [40/84], [94mLoss[0m : 2.76113
[1mStep[0m  [48/84], [94mLoss[0m : 2.74855
[1mStep[0m  [56/84], [94mLoss[0m : 2.53803
[1mStep[0m  [64/84], [94mLoss[0m : 2.47845
[1mStep[0m  [72/84], [94mLoss[0m : 2.93792
[1mStep[0m  [80/84], [94mLoss[0m : 2.46151

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57747
[1mStep[0m  [8/84], [94mLoss[0m : 2.56091
[1mStep[0m  [16/84], [94mLoss[0m : 2.76628
[1mStep[0m  [24/84], [94mLoss[0m : 2.51216
[1mStep[0m  [32/84], [94mLoss[0m : 2.53848
[1mStep[0m  [40/84], [94mLoss[0m : 2.60405
[1mStep[0m  [48/84], [94mLoss[0m : 2.47276
[1mStep[0m  [56/84], [94mLoss[0m : 2.38341
[1mStep[0m  [64/84], [94mLoss[0m : 2.53982
[1mStep[0m  [72/84], [94mLoss[0m : 2.69430
[1mStep[0m  [80/84], [94mLoss[0m : 2.48797

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70867
[1mStep[0m  [8/84], [94mLoss[0m : 2.66770
[1mStep[0m  [16/84], [94mLoss[0m : 2.50474
[1mStep[0m  [24/84], [94mLoss[0m : 2.65225
[1mStep[0m  [32/84], [94mLoss[0m : 2.59251
[1mStep[0m  [40/84], [94mLoss[0m : 2.41214
[1mStep[0m  [48/84], [94mLoss[0m : 2.74509
[1mStep[0m  [56/84], [94mLoss[0m : 2.60826
[1mStep[0m  [64/84], [94mLoss[0m : 2.44666
[1mStep[0m  [72/84], [94mLoss[0m : 2.54878
[1mStep[0m  [80/84], [94mLoss[0m : 2.38176

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56807
[1mStep[0m  [8/84], [94mLoss[0m : 2.49398
[1mStep[0m  [16/84], [94mLoss[0m : 2.50728
[1mStep[0m  [24/84], [94mLoss[0m : 2.45356
[1mStep[0m  [32/84], [94mLoss[0m : 2.68358
[1mStep[0m  [40/84], [94mLoss[0m : 2.64492
[1mStep[0m  [48/84], [94mLoss[0m : 2.74269
[1mStep[0m  [56/84], [94mLoss[0m : 2.65994
[1mStep[0m  [64/84], [94mLoss[0m : 2.54002
[1mStep[0m  [72/84], [94mLoss[0m : 2.45866
[1mStep[0m  [80/84], [94mLoss[0m : 2.43481

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.407, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48569
[1mStep[0m  [8/84], [94mLoss[0m : 2.63085
[1mStep[0m  [16/84], [94mLoss[0m : 2.87282
[1mStep[0m  [24/84], [94mLoss[0m : 2.50292
[1mStep[0m  [32/84], [94mLoss[0m : 2.34406
[1mStep[0m  [40/84], [94mLoss[0m : 2.59593
[1mStep[0m  [48/84], [94mLoss[0m : 2.44170
[1mStep[0m  [56/84], [94mLoss[0m : 2.49662
[1mStep[0m  [64/84], [94mLoss[0m : 2.44479
[1mStep[0m  [72/84], [94mLoss[0m : 2.45812
[1mStep[0m  [80/84], [94mLoss[0m : 2.42589

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70348
[1mStep[0m  [8/84], [94mLoss[0m : 2.35921
[1mStep[0m  [16/84], [94mLoss[0m : 2.40161
[1mStep[0m  [24/84], [94mLoss[0m : 2.57651
[1mStep[0m  [32/84], [94mLoss[0m : 2.40577
[1mStep[0m  [40/84], [94mLoss[0m : 2.49783
[1mStep[0m  [48/84], [94mLoss[0m : 2.73941
[1mStep[0m  [56/84], [94mLoss[0m : 2.66547
[1mStep[0m  [64/84], [94mLoss[0m : 2.38603
[1mStep[0m  [72/84], [94mLoss[0m : 2.36790
[1mStep[0m  [80/84], [94mLoss[0m : 2.68617

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.401, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53252
[1mStep[0m  [8/84], [94mLoss[0m : 2.35639
[1mStep[0m  [16/84], [94mLoss[0m : 2.44982
[1mStep[0m  [24/84], [94mLoss[0m : 2.83641
[1mStep[0m  [32/84], [94mLoss[0m : 2.62573
[1mStep[0m  [40/84], [94mLoss[0m : 2.54468
[1mStep[0m  [48/84], [94mLoss[0m : 2.55315
[1mStep[0m  [56/84], [94mLoss[0m : 2.57298
[1mStep[0m  [64/84], [94mLoss[0m : 2.51628
[1mStep[0m  [72/84], [94mLoss[0m : 2.57178
[1mStep[0m  [80/84], [94mLoss[0m : 2.43118

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.390, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55498
[1mStep[0m  [8/84], [94mLoss[0m : 2.47195
[1mStep[0m  [16/84], [94mLoss[0m : 2.77679
[1mStep[0m  [24/84], [94mLoss[0m : 2.89658
[1mStep[0m  [32/84], [94mLoss[0m : 2.68699
[1mStep[0m  [40/84], [94mLoss[0m : 2.51980
[1mStep[0m  [48/84], [94mLoss[0m : 2.31507
[1mStep[0m  [56/84], [94mLoss[0m : 2.20469
[1mStep[0m  [64/84], [94mLoss[0m : 2.27296
[1mStep[0m  [72/84], [94mLoss[0m : 2.86956
[1mStep[0m  [80/84], [94mLoss[0m : 2.53560

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.386, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55258
[1mStep[0m  [8/84], [94mLoss[0m : 2.70588
[1mStep[0m  [16/84], [94mLoss[0m : 2.66060
[1mStep[0m  [24/84], [94mLoss[0m : 2.62264
[1mStep[0m  [32/84], [94mLoss[0m : 2.94219
[1mStep[0m  [40/84], [94mLoss[0m : 2.39992
[1mStep[0m  [48/84], [94mLoss[0m : 2.67900
[1mStep[0m  [56/84], [94mLoss[0m : 2.44173
[1mStep[0m  [64/84], [94mLoss[0m : 2.49972
[1mStep[0m  [72/84], [94mLoss[0m : 2.43357
[1mStep[0m  [80/84], [94mLoss[0m : 2.79224

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.385, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65522
[1mStep[0m  [8/84], [94mLoss[0m : 2.42463
[1mStep[0m  [16/84], [94mLoss[0m : 2.47363
[1mStep[0m  [24/84], [94mLoss[0m : 2.41603
[1mStep[0m  [32/84], [94mLoss[0m : 2.66602
[1mStep[0m  [40/84], [94mLoss[0m : 2.40063
[1mStep[0m  [48/84], [94mLoss[0m : 2.54308
[1mStep[0m  [56/84], [94mLoss[0m : 2.60533
[1mStep[0m  [64/84], [94mLoss[0m : 2.70895
[1mStep[0m  [72/84], [94mLoss[0m : 2.70198
[1mStep[0m  [80/84], [94mLoss[0m : 2.61100

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.386, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51962
[1mStep[0m  [8/84], [94mLoss[0m : 2.56482
[1mStep[0m  [16/84], [94mLoss[0m : 2.42717
[1mStep[0m  [24/84], [94mLoss[0m : 2.37632
[1mStep[0m  [32/84], [94mLoss[0m : 2.42169
[1mStep[0m  [40/84], [94mLoss[0m : 2.53881
[1mStep[0m  [48/84], [94mLoss[0m : 2.39126
[1mStep[0m  [56/84], [94mLoss[0m : 2.38476
[1mStep[0m  [64/84], [94mLoss[0m : 2.51123
[1mStep[0m  [72/84], [94mLoss[0m : 2.80768
[1mStep[0m  [80/84], [94mLoss[0m : 2.16339

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.378, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54648
[1mStep[0m  [8/84], [94mLoss[0m : 2.63968
[1mStep[0m  [16/84], [94mLoss[0m : 2.46777
[1mStep[0m  [24/84], [94mLoss[0m : 2.69891
[1mStep[0m  [32/84], [94mLoss[0m : 2.62266
[1mStep[0m  [40/84], [94mLoss[0m : 2.47215
[1mStep[0m  [48/84], [94mLoss[0m : 2.47329
[1mStep[0m  [56/84], [94mLoss[0m : 2.32832
[1mStep[0m  [64/84], [94mLoss[0m : 2.48371
[1mStep[0m  [72/84], [94mLoss[0m : 2.43819
[1mStep[0m  [80/84], [94mLoss[0m : 2.41275

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.380, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52354
[1mStep[0m  [8/84], [94mLoss[0m : 2.53058
[1mStep[0m  [16/84], [94mLoss[0m : 2.35690
[1mStep[0m  [24/84], [94mLoss[0m : 2.52336
[1mStep[0m  [32/84], [94mLoss[0m : 2.63195
[1mStep[0m  [40/84], [94mLoss[0m : 2.41148
[1mStep[0m  [48/84], [94mLoss[0m : 2.78729
[1mStep[0m  [56/84], [94mLoss[0m : 2.71399
[1mStep[0m  [64/84], [94mLoss[0m : 2.36009
[1mStep[0m  [72/84], [94mLoss[0m : 2.80351
[1mStep[0m  [80/84], [94mLoss[0m : 2.38500

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.378, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34714
[1mStep[0m  [8/84], [94mLoss[0m : 2.46636
[1mStep[0m  [16/84], [94mLoss[0m : 2.49847
[1mStep[0m  [24/84], [94mLoss[0m : 2.29665
[1mStep[0m  [32/84], [94mLoss[0m : 2.35169
[1mStep[0m  [40/84], [94mLoss[0m : 2.43168
[1mStep[0m  [48/84], [94mLoss[0m : 2.75587
[1mStep[0m  [56/84], [94mLoss[0m : 2.49406
[1mStep[0m  [64/84], [94mLoss[0m : 2.74437
[1mStep[0m  [72/84], [94mLoss[0m : 2.68190
[1mStep[0m  [80/84], [94mLoss[0m : 2.78150

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52677
[1mStep[0m  [8/84], [94mLoss[0m : 2.71128
[1mStep[0m  [16/84], [94mLoss[0m : 2.17744
[1mStep[0m  [24/84], [94mLoss[0m : 2.56284
[1mStep[0m  [32/84], [94mLoss[0m : 2.36627
[1mStep[0m  [40/84], [94mLoss[0m : 2.35907
[1mStep[0m  [48/84], [94mLoss[0m : 2.58108
[1mStep[0m  [56/84], [94mLoss[0m : 2.56250
[1mStep[0m  [64/84], [94mLoss[0m : 2.48545
[1mStep[0m  [72/84], [94mLoss[0m : 2.56903
[1mStep[0m  [80/84], [94mLoss[0m : 2.48772

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44922
[1mStep[0m  [8/84], [94mLoss[0m : 2.57076
[1mStep[0m  [16/84], [94mLoss[0m : 2.23560
[1mStep[0m  [24/84], [94mLoss[0m : 2.54676
[1mStep[0m  [32/84], [94mLoss[0m : 2.51649
[1mStep[0m  [40/84], [94mLoss[0m : 2.62065
[1mStep[0m  [48/84], [94mLoss[0m : 2.60591
[1mStep[0m  [56/84], [94mLoss[0m : 2.56011
[1mStep[0m  [64/84], [94mLoss[0m : 2.34371
[1mStep[0m  [72/84], [94mLoss[0m : 2.59767
[1mStep[0m  [80/84], [94mLoss[0m : 2.82483

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38075
[1mStep[0m  [8/84], [94mLoss[0m : 2.42365
[1mStep[0m  [16/84], [94mLoss[0m : 2.72154
[1mStep[0m  [24/84], [94mLoss[0m : 2.35099
[1mStep[0m  [32/84], [94mLoss[0m : 2.53557
[1mStep[0m  [40/84], [94mLoss[0m : 2.22247
[1mStep[0m  [48/84], [94mLoss[0m : 2.47984
[1mStep[0m  [56/84], [94mLoss[0m : 2.86312
[1mStep[0m  [64/84], [94mLoss[0m : 2.49199
[1mStep[0m  [72/84], [94mLoss[0m : 2.61288
[1mStep[0m  [80/84], [94mLoss[0m : 2.53142

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.359, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69920
[1mStep[0m  [8/84], [94mLoss[0m : 2.70682
[1mStep[0m  [16/84], [94mLoss[0m : 2.59138
[1mStep[0m  [24/84], [94mLoss[0m : 2.28236
[1mStep[0m  [32/84], [94mLoss[0m : 2.28212
[1mStep[0m  [40/84], [94mLoss[0m : 2.47703
[1mStep[0m  [48/84], [94mLoss[0m : 2.61176
[1mStep[0m  [56/84], [94mLoss[0m : 2.53996
[1mStep[0m  [64/84], [94mLoss[0m : 2.57461
[1mStep[0m  [72/84], [94mLoss[0m : 2.69946
[1mStep[0m  [80/84], [94mLoss[0m : 2.67192

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.367, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.3562103595052446
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.39143
[1mStep[0m  [8/84], [94mLoss[0m : 2.62305
[1mStep[0m  [16/84], [94mLoss[0m : 2.58627
[1mStep[0m  [24/84], [94mLoss[0m : 2.74886
[1mStep[0m  [32/84], [94mLoss[0m : 2.55318
[1mStep[0m  [40/84], [94mLoss[0m : 2.69589
[1mStep[0m  [48/84], [94mLoss[0m : 2.80609
[1mStep[0m  [56/84], [94mLoss[0m : 2.73839
[1mStep[0m  [64/84], [94mLoss[0m : 2.66583
[1mStep[0m  [72/84], [94mLoss[0m : 2.51587
[1mStep[0m  [80/84], [94mLoss[0m : 2.52056

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.365, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34840
[1mStep[0m  [8/84], [94mLoss[0m : 2.61717
[1mStep[0m  [16/84], [94mLoss[0m : 2.47307
[1mStep[0m  [24/84], [94mLoss[0m : 2.35412
[1mStep[0m  [32/84], [94mLoss[0m : 2.13541
[1mStep[0m  [40/84], [94mLoss[0m : 2.31998
[1mStep[0m  [48/84], [94mLoss[0m : 2.68937
[1mStep[0m  [56/84], [94mLoss[0m : 2.31983
[1mStep[0m  [64/84], [94mLoss[0m : 2.22855
[1mStep[0m  [72/84], [94mLoss[0m : 2.85183
[1mStep[0m  [80/84], [94mLoss[0m : 2.30516

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.376, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51912
[1mStep[0m  [8/84], [94mLoss[0m : 2.38442
[1mStep[0m  [16/84], [94mLoss[0m : 2.65102
[1mStep[0m  [24/84], [94mLoss[0m : 2.42542
[1mStep[0m  [32/84], [94mLoss[0m : 2.43371
[1mStep[0m  [40/84], [94mLoss[0m : 2.33846
[1mStep[0m  [48/84], [94mLoss[0m : 2.51654
[1mStep[0m  [56/84], [94mLoss[0m : 2.48651
[1mStep[0m  [64/84], [94mLoss[0m : 2.38078
[1mStep[0m  [72/84], [94mLoss[0m : 2.50882
[1mStep[0m  [80/84], [94mLoss[0m : 2.73063

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35624
[1mStep[0m  [8/84], [94mLoss[0m : 2.60164
[1mStep[0m  [16/84], [94mLoss[0m : 3.01605
[1mStep[0m  [24/84], [94mLoss[0m : 2.17798
[1mStep[0m  [32/84], [94mLoss[0m : 2.46378
[1mStep[0m  [40/84], [94mLoss[0m : 2.35297
[1mStep[0m  [48/84], [94mLoss[0m : 2.46862
[1mStep[0m  [56/84], [94mLoss[0m : 2.61758
[1mStep[0m  [64/84], [94mLoss[0m : 2.35906
[1mStep[0m  [72/84], [94mLoss[0m : 2.66651
[1mStep[0m  [80/84], [94mLoss[0m : 2.74688

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.412, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55755
[1mStep[0m  [8/84], [94mLoss[0m : 2.04741
[1mStep[0m  [16/84], [94mLoss[0m : 2.16160
[1mStep[0m  [24/84], [94mLoss[0m : 2.90428
[1mStep[0m  [32/84], [94mLoss[0m : 2.17850
[1mStep[0m  [40/84], [94mLoss[0m : 2.58108
[1mStep[0m  [48/84], [94mLoss[0m : 2.49867
[1mStep[0m  [56/84], [94mLoss[0m : 2.37389
[1mStep[0m  [64/84], [94mLoss[0m : 2.61532
[1mStep[0m  [72/84], [94mLoss[0m : 2.60667
[1mStep[0m  [80/84], [94mLoss[0m : 2.61735

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.460, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41794
[1mStep[0m  [8/84], [94mLoss[0m : 2.62545
[1mStep[0m  [16/84], [94mLoss[0m : 2.26322
[1mStep[0m  [24/84], [94mLoss[0m : 2.65204
[1mStep[0m  [32/84], [94mLoss[0m : 2.78077
[1mStep[0m  [40/84], [94mLoss[0m : 2.25213
[1mStep[0m  [48/84], [94mLoss[0m : 2.53295
[1mStep[0m  [56/84], [94mLoss[0m : 2.30482
[1mStep[0m  [64/84], [94mLoss[0m : 2.33716
[1mStep[0m  [72/84], [94mLoss[0m : 2.26811
[1mStep[0m  [80/84], [94mLoss[0m : 2.54345

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.438, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32725
[1mStep[0m  [8/84], [94mLoss[0m : 2.47281
[1mStep[0m  [16/84], [94mLoss[0m : 2.11364
[1mStep[0m  [24/84], [94mLoss[0m : 2.42660
[1mStep[0m  [32/84], [94mLoss[0m : 2.45298
[1mStep[0m  [40/84], [94mLoss[0m : 2.34977
[1mStep[0m  [48/84], [94mLoss[0m : 2.35803
[1mStep[0m  [56/84], [94mLoss[0m : 2.27967
[1mStep[0m  [64/84], [94mLoss[0m : 2.48098
[1mStep[0m  [72/84], [94mLoss[0m : 2.50182
[1mStep[0m  [80/84], [94mLoss[0m : 2.19743

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.471, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19294
[1mStep[0m  [8/84], [94mLoss[0m : 2.62927
[1mStep[0m  [16/84], [94mLoss[0m : 2.62514
[1mStep[0m  [24/84], [94mLoss[0m : 2.16949
[1mStep[0m  [32/84], [94mLoss[0m : 2.70845
[1mStep[0m  [40/84], [94mLoss[0m : 2.32719
[1mStep[0m  [48/84], [94mLoss[0m : 2.45775
[1mStep[0m  [56/84], [94mLoss[0m : 2.40930
[1mStep[0m  [64/84], [94mLoss[0m : 2.76536
[1mStep[0m  [72/84], [94mLoss[0m : 2.47848
[1mStep[0m  [80/84], [94mLoss[0m : 2.15087

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.459, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47407
[1mStep[0m  [8/84], [94mLoss[0m : 2.43628
[1mStep[0m  [16/84], [94mLoss[0m : 2.38931
[1mStep[0m  [24/84], [94mLoss[0m : 2.33585
[1mStep[0m  [32/84], [94mLoss[0m : 2.54579
[1mStep[0m  [40/84], [94mLoss[0m : 3.00155
[1mStep[0m  [48/84], [94mLoss[0m : 2.60673
[1mStep[0m  [56/84], [94mLoss[0m : 2.42642
[1mStep[0m  [64/84], [94mLoss[0m : 2.63038
[1mStep[0m  [72/84], [94mLoss[0m : 2.51356
[1mStep[0m  [80/84], [94mLoss[0m : 2.44215

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.477, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23298
[1mStep[0m  [8/84], [94mLoss[0m : 2.34543
[1mStep[0m  [16/84], [94mLoss[0m : 2.35300
[1mStep[0m  [24/84], [94mLoss[0m : 2.42511
[1mStep[0m  [32/84], [94mLoss[0m : 2.34889
[1mStep[0m  [40/84], [94mLoss[0m : 2.34942
[1mStep[0m  [48/84], [94mLoss[0m : 2.73604
[1mStep[0m  [56/84], [94mLoss[0m : 2.38327
[1mStep[0m  [64/84], [94mLoss[0m : 2.67916
[1mStep[0m  [72/84], [94mLoss[0m : 2.59733
[1mStep[0m  [80/84], [94mLoss[0m : 2.39669

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73139
[1mStep[0m  [8/84], [94mLoss[0m : 2.38136
[1mStep[0m  [16/84], [94mLoss[0m : 2.63279
[1mStep[0m  [24/84], [94mLoss[0m : 2.53602
[1mStep[0m  [32/84], [94mLoss[0m : 2.61350
[1mStep[0m  [40/84], [94mLoss[0m : 2.33999
[1mStep[0m  [48/84], [94mLoss[0m : 2.29074
[1mStep[0m  [56/84], [94mLoss[0m : 2.61152
[1mStep[0m  [64/84], [94mLoss[0m : 2.54028
[1mStep[0m  [72/84], [94mLoss[0m : 2.42877
[1mStep[0m  [80/84], [94mLoss[0m : 2.54827

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38933
[1mStep[0m  [8/84], [94mLoss[0m : 2.96579
[1mStep[0m  [16/84], [94mLoss[0m : 2.60558
[1mStep[0m  [24/84], [94mLoss[0m : 2.40397
[1mStep[0m  [32/84], [94mLoss[0m : 2.55927
[1mStep[0m  [40/84], [94mLoss[0m : 2.47548
[1mStep[0m  [48/84], [94mLoss[0m : 2.62342
[1mStep[0m  [56/84], [94mLoss[0m : 2.51756
[1mStep[0m  [64/84], [94mLoss[0m : 2.51522
[1mStep[0m  [72/84], [94mLoss[0m : 2.56844
[1mStep[0m  [80/84], [94mLoss[0m : 2.96619

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38463
[1mStep[0m  [8/84], [94mLoss[0m : 2.56997
[1mStep[0m  [16/84], [94mLoss[0m : 2.20645
[1mStep[0m  [24/84], [94mLoss[0m : 2.42530
[1mStep[0m  [32/84], [94mLoss[0m : 2.56366
[1mStep[0m  [40/84], [94mLoss[0m : 2.29200
[1mStep[0m  [48/84], [94mLoss[0m : 2.46111
[1mStep[0m  [56/84], [94mLoss[0m : 2.36502
[1mStep[0m  [64/84], [94mLoss[0m : 2.23590
[1mStep[0m  [72/84], [94mLoss[0m : 2.52687
[1mStep[0m  [80/84], [94mLoss[0m : 2.51823

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51785
[1mStep[0m  [8/84], [94mLoss[0m : 2.59207
[1mStep[0m  [16/84], [94mLoss[0m : 2.36355
[1mStep[0m  [24/84], [94mLoss[0m : 2.49356
[1mStep[0m  [32/84], [94mLoss[0m : 2.54020
[1mStep[0m  [40/84], [94mLoss[0m : 2.76498
[1mStep[0m  [48/84], [94mLoss[0m : 2.66714
[1mStep[0m  [56/84], [94mLoss[0m : 2.07670
[1mStep[0m  [64/84], [94mLoss[0m : 2.40735
[1mStep[0m  [72/84], [94mLoss[0m : 2.45387
[1mStep[0m  [80/84], [94mLoss[0m : 2.45009

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.559, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44873
[1mStep[0m  [8/84], [94mLoss[0m : 2.36383
[1mStep[0m  [16/84], [94mLoss[0m : 2.62433
[1mStep[0m  [24/84], [94mLoss[0m : 2.31698
[1mStep[0m  [32/84], [94mLoss[0m : 2.73490
[1mStep[0m  [40/84], [94mLoss[0m : 2.47845
[1mStep[0m  [48/84], [94mLoss[0m : 2.48258
[1mStep[0m  [56/84], [94mLoss[0m : 2.61333
[1mStep[0m  [64/84], [94mLoss[0m : 2.37047
[1mStep[0m  [72/84], [94mLoss[0m : 2.62860
[1mStep[0m  [80/84], [94mLoss[0m : 2.54865

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.541, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55188
[1mStep[0m  [8/84], [94mLoss[0m : 2.54675
[1mStep[0m  [16/84], [94mLoss[0m : 2.44511
[1mStep[0m  [24/84], [94mLoss[0m : 2.28420
[1mStep[0m  [32/84], [94mLoss[0m : 2.47927
[1mStep[0m  [40/84], [94mLoss[0m : 2.68282
[1mStep[0m  [48/84], [94mLoss[0m : 2.51847
[1mStep[0m  [56/84], [94mLoss[0m : 2.16377
[1mStep[0m  [64/84], [94mLoss[0m : 2.65735
[1mStep[0m  [72/84], [94mLoss[0m : 2.47969
[1mStep[0m  [80/84], [94mLoss[0m : 2.70287

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51158
[1mStep[0m  [8/84], [94mLoss[0m : 2.49908
[1mStep[0m  [16/84], [94mLoss[0m : 2.37481
[1mStep[0m  [24/84], [94mLoss[0m : 2.28536
[1mStep[0m  [32/84], [94mLoss[0m : 2.43591
[1mStep[0m  [40/84], [94mLoss[0m : 2.25569
[1mStep[0m  [48/84], [94mLoss[0m : 2.63833
[1mStep[0m  [56/84], [94mLoss[0m : 2.41815
[1mStep[0m  [64/84], [94mLoss[0m : 2.43753
[1mStep[0m  [72/84], [94mLoss[0m : 2.48385
[1mStep[0m  [80/84], [94mLoss[0m : 2.86469

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.504, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59832
[1mStep[0m  [8/84], [94mLoss[0m : 2.39253
[1mStep[0m  [16/84], [94mLoss[0m : 2.50830
[1mStep[0m  [24/84], [94mLoss[0m : 2.52013
[1mStep[0m  [32/84], [94mLoss[0m : 2.19514
[1mStep[0m  [40/84], [94mLoss[0m : 2.51941
[1mStep[0m  [48/84], [94mLoss[0m : 2.49411
[1mStep[0m  [56/84], [94mLoss[0m : 2.74077
[1mStep[0m  [64/84], [94mLoss[0m : 2.20663
[1mStep[0m  [72/84], [94mLoss[0m : 2.54518
[1mStep[0m  [80/84], [94mLoss[0m : 2.19999

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54639
[1mStep[0m  [8/84], [94mLoss[0m : 2.27156
[1mStep[0m  [16/84], [94mLoss[0m : 2.63137
[1mStep[0m  [24/84], [94mLoss[0m : 2.08382
[1mStep[0m  [32/84], [94mLoss[0m : 2.32738
[1mStep[0m  [40/84], [94mLoss[0m : 2.52301
[1mStep[0m  [48/84], [94mLoss[0m : 2.44917
[1mStep[0m  [56/84], [94mLoss[0m : 2.29529
[1mStep[0m  [64/84], [94mLoss[0m : 2.29298
[1mStep[0m  [72/84], [94mLoss[0m : 2.58024
[1mStep[0m  [80/84], [94mLoss[0m : 2.52747

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.561, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53920
[1mStep[0m  [8/84], [94mLoss[0m : 2.41410
[1mStep[0m  [16/84], [94mLoss[0m : 2.61936
[1mStep[0m  [24/84], [94mLoss[0m : 2.13639
[1mStep[0m  [32/84], [94mLoss[0m : 2.75769
[1mStep[0m  [40/84], [94mLoss[0m : 2.30397
[1mStep[0m  [48/84], [94mLoss[0m : 2.24597
[1mStep[0m  [56/84], [94mLoss[0m : 2.49940
[1mStep[0m  [64/84], [94mLoss[0m : 2.29902
[1mStep[0m  [72/84], [94mLoss[0m : 2.42321
[1mStep[0m  [80/84], [94mLoss[0m : 2.44320

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84851
[1mStep[0m  [8/84], [94mLoss[0m : 2.20215
[1mStep[0m  [16/84], [94mLoss[0m : 2.47067
[1mStep[0m  [24/84], [94mLoss[0m : 2.22463
[1mStep[0m  [32/84], [94mLoss[0m : 2.28092
[1mStep[0m  [40/84], [94mLoss[0m : 2.36719
[1mStep[0m  [48/84], [94mLoss[0m : 2.43942
[1mStep[0m  [56/84], [94mLoss[0m : 2.83084
[1mStep[0m  [64/84], [94mLoss[0m : 2.10562
[1mStep[0m  [72/84], [94mLoss[0m : 2.67730
[1mStep[0m  [80/84], [94mLoss[0m : 2.31413

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.495, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36976
[1mStep[0m  [8/84], [94mLoss[0m : 2.19464
[1mStep[0m  [16/84], [94mLoss[0m : 2.47372
[1mStep[0m  [24/84], [94mLoss[0m : 2.20513
[1mStep[0m  [32/84], [94mLoss[0m : 2.49315
[1mStep[0m  [40/84], [94mLoss[0m : 2.39803
[1mStep[0m  [48/84], [94mLoss[0m : 2.37169
[1mStep[0m  [56/84], [94mLoss[0m : 2.55374
[1mStep[0m  [64/84], [94mLoss[0m : 2.28259
[1mStep[0m  [72/84], [94mLoss[0m : 2.17847
[1mStep[0m  [80/84], [94mLoss[0m : 2.39504

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.529, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74604
[1mStep[0m  [8/84], [94mLoss[0m : 2.65325
[1mStep[0m  [16/84], [94mLoss[0m : 2.74722
[1mStep[0m  [24/84], [94mLoss[0m : 2.56652
[1mStep[0m  [32/84], [94mLoss[0m : 2.41145
[1mStep[0m  [40/84], [94mLoss[0m : 2.66133
[1mStep[0m  [48/84], [94mLoss[0m : 2.09331
[1mStep[0m  [56/84], [94mLoss[0m : 2.34305
[1mStep[0m  [64/84], [94mLoss[0m : 2.34838
[1mStep[0m  [72/84], [94mLoss[0m : 2.25607
[1mStep[0m  [80/84], [94mLoss[0m : 2.53039

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.511, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27751
[1mStep[0m  [8/84], [94mLoss[0m : 2.55535
[1mStep[0m  [16/84], [94mLoss[0m : 2.52247
[1mStep[0m  [24/84], [94mLoss[0m : 2.59184
[1mStep[0m  [32/84], [94mLoss[0m : 2.55312
[1mStep[0m  [40/84], [94mLoss[0m : 2.18527
[1mStep[0m  [48/84], [94mLoss[0m : 2.23794
[1mStep[0m  [56/84], [94mLoss[0m : 2.43214
[1mStep[0m  [64/84], [94mLoss[0m : 2.26486
[1mStep[0m  [72/84], [94mLoss[0m : 2.28686
[1mStep[0m  [80/84], [94mLoss[0m : 2.88322

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.540, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00496
[1mStep[0m  [8/84], [94mLoss[0m : 2.67608
[1mStep[0m  [16/84], [94mLoss[0m : 2.46146
[1mStep[0m  [24/84], [94mLoss[0m : 2.36632
[1mStep[0m  [32/84], [94mLoss[0m : 2.37595
[1mStep[0m  [40/84], [94mLoss[0m : 2.52307
[1mStep[0m  [48/84], [94mLoss[0m : 2.20722
[1mStep[0m  [56/84], [94mLoss[0m : 2.34124
[1mStep[0m  [64/84], [94mLoss[0m : 2.42294
[1mStep[0m  [72/84], [94mLoss[0m : 2.30801
[1mStep[0m  [80/84], [94mLoss[0m : 2.37992

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.469, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28782
[1mStep[0m  [8/84], [94mLoss[0m : 2.36389
[1mStep[0m  [16/84], [94mLoss[0m : 2.42253
[1mStep[0m  [24/84], [94mLoss[0m : 2.26621
[1mStep[0m  [32/84], [94mLoss[0m : 2.27943
[1mStep[0m  [40/84], [94mLoss[0m : 2.40898
[1mStep[0m  [48/84], [94mLoss[0m : 2.61470
[1mStep[0m  [56/84], [94mLoss[0m : 2.29998
[1mStep[0m  [64/84], [94mLoss[0m : 2.11140
[1mStep[0m  [72/84], [94mLoss[0m : 2.49090
[1mStep[0m  [80/84], [94mLoss[0m : 2.39273

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.497, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39011
[1mStep[0m  [8/84], [94mLoss[0m : 2.25670
[1mStep[0m  [16/84], [94mLoss[0m : 2.75673
[1mStep[0m  [24/84], [94mLoss[0m : 2.21254
[1mStep[0m  [32/84], [94mLoss[0m : 2.64876
[1mStep[0m  [40/84], [94mLoss[0m : 2.50831
[1mStep[0m  [48/84], [94mLoss[0m : 2.62498
[1mStep[0m  [56/84], [94mLoss[0m : 2.48839
[1mStep[0m  [64/84], [94mLoss[0m : 2.48455
[1mStep[0m  [72/84], [94mLoss[0m : 2.70433
[1mStep[0m  [80/84], [94mLoss[0m : 2.22852

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.514, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41935
[1mStep[0m  [8/84], [94mLoss[0m : 2.38887
[1mStep[0m  [16/84], [94mLoss[0m : 2.36708
[1mStep[0m  [24/84], [94mLoss[0m : 2.24594
[1mStep[0m  [32/84], [94mLoss[0m : 2.32296
[1mStep[0m  [40/84], [94mLoss[0m : 2.16268
[1mStep[0m  [48/84], [94mLoss[0m : 2.40262
[1mStep[0m  [56/84], [94mLoss[0m : 2.35067
[1mStep[0m  [64/84], [94mLoss[0m : 2.10238
[1mStep[0m  [72/84], [94mLoss[0m : 2.42943
[1mStep[0m  [80/84], [94mLoss[0m : 2.50016

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.564, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33086
[1mStep[0m  [8/84], [94mLoss[0m : 2.46140
[1mStep[0m  [16/84], [94mLoss[0m : 2.37198
[1mStep[0m  [24/84], [94mLoss[0m : 2.51539
[1mStep[0m  [32/84], [94mLoss[0m : 2.42061
[1mStep[0m  [40/84], [94mLoss[0m : 2.24111
[1mStep[0m  [48/84], [94mLoss[0m : 2.42913
[1mStep[0m  [56/84], [94mLoss[0m : 2.01409
[1mStep[0m  [64/84], [94mLoss[0m : 2.34163
[1mStep[0m  [72/84], [94mLoss[0m : 2.42062
[1mStep[0m  [80/84], [94mLoss[0m : 2.56274

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.498, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55811
[1mStep[0m  [8/84], [94mLoss[0m : 2.10799
[1mStep[0m  [16/84], [94mLoss[0m : 2.34853
[1mStep[0m  [24/84], [94mLoss[0m : 2.32798
[1mStep[0m  [32/84], [94mLoss[0m : 2.80323
[1mStep[0m  [40/84], [94mLoss[0m : 2.51175
[1mStep[0m  [48/84], [94mLoss[0m : 2.45818
[1mStep[0m  [56/84], [94mLoss[0m : 2.54524
[1mStep[0m  [64/84], [94mLoss[0m : 2.12014
[1mStep[0m  [72/84], [94mLoss[0m : 2.50074
[1mStep[0m  [80/84], [94mLoss[0m : 2.34450

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.523, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.480
====================================

Phase 2 - Evaluation MAE:  2.479962408542633
MAE score P1       2.35621
MAE score P2      2.479962
loss              2.374426
learning_rate       0.0001
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay        0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.44936
[1mStep[0m  [4/42], [94mLoss[0m : 10.98697
[1mStep[0m  [8/42], [94mLoss[0m : 10.53257
[1mStep[0m  [12/42], [94mLoss[0m : 9.84125
[1mStep[0m  [16/42], [94mLoss[0m : 10.42311
[1mStep[0m  [20/42], [94mLoss[0m : 9.73241
[1mStep[0m  [24/42], [94mLoss[0m : 9.70987
[1mStep[0m  [28/42], [94mLoss[0m : 9.51367
[1mStep[0m  [32/42], [94mLoss[0m : 9.12468
[1mStep[0m  [36/42], [94mLoss[0m : 8.64937
[1mStep[0m  [40/42], [94mLoss[0m : 8.38209

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.857, [92mTest[0m: 10.904, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.69039
[1mStep[0m  [4/42], [94mLoss[0m : 8.09844
[1mStep[0m  [8/42], [94mLoss[0m : 8.03434
[1mStep[0m  [12/42], [94mLoss[0m : 7.48702
[1mStep[0m  [16/42], [94mLoss[0m : 6.96157
[1mStep[0m  [20/42], [94mLoss[0m : 6.94777
[1mStep[0m  [24/42], [94mLoss[0m : 6.26301
[1mStep[0m  [28/42], [94mLoss[0m : 5.93035
[1mStep[0m  [32/42], [94mLoss[0m : 6.17952
[1mStep[0m  [36/42], [94mLoss[0m : 5.64499
[1mStep[0m  [40/42], [94mLoss[0m : 5.48660

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.836, [92mTest[0m: 8.374, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.49371
[1mStep[0m  [4/42], [94mLoss[0m : 4.76756
[1mStep[0m  [8/42], [94mLoss[0m : 4.38849
[1mStep[0m  [12/42], [94mLoss[0m : 4.55380
[1mStep[0m  [16/42], [94mLoss[0m : 4.68549
[1mStep[0m  [20/42], [94mLoss[0m : 3.76420
[1mStep[0m  [24/42], [94mLoss[0m : 3.78260
[1mStep[0m  [28/42], [94mLoss[0m : 3.80801
[1mStep[0m  [32/42], [94mLoss[0m : 3.55724
[1mStep[0m  [36/42], [94mLoss[0m : 3.28745
[1mStep[0m  [40/42], [94mLoss[0m : 3.15249

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.132, [92mTest[0m: 5.237, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.76519
[1mStep[0m  [4/42], [94mLoss[0m : 3.35028
[1mStep[0m  [8/42], [94mLoss[0m : 3.11028
[1mStep[0m  [12/42], [94mLoss[0m : 3.27072
[1mStep[0m  [16/42], [94mLoss[0m : 3.26622
[1mStep[0m  [20/42], [94mLoss[0m : 3.39050
[1mStep[0m  [24/42], [94mLoss[0m : 3.16862
[1mStep[0m  [28/42], [94mLoss[0m : 3.10594
[1mStep[0m  [32/42], [94mLoss[0m : 2.98972
[1mStep[0m  [36/42], [94mLoss[0m : 2.97459
[1mStep[0m  [40/42], [94mLoss[0m : 3.00674

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.106, [92mTest[0m: 3.360, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.10439
[1mStep[0m  [4/42], [94mLoss[0m : 2.98323
[1mStep[0m  [8/42], [94mLoss[0m : 2.86371
[1mStep[0m  [12/42], [94mLoss[0m : 3.13292
[1mStep[0m  [16/42], [94mLoss[0m : 2.86908
[1mStep[0m  [20/42], [94mLoss[0m : 2.56304
[1mStep[0m  [24/42], [94mLoss[0m : 2.69191
[1mStep[0m  [28/42], [94mLoss[0m : 2.77224
[1mStep[0m  [32/42], [94mLoss[0m : 3.06223
[1mStep[0m  [36/42], [94mLoss[0m : 2.94876
[1mStep[0m  [40/42], [94mLoss[0m : 2.83763

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.869, [92mTest[0m: 2.920, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04718
[1mStep[0m  [4/42], [94mLoss[0m : 2.89715
[1mStep[0m  [8/42], [94mLoss[0m : 2.75235
[1mStep[0m  [12/42], [94mLoss[0m : 2.55142
[1mStep[0m  [16/42], [94mLoss[0m : 2.81676
[1mStep[0m  [20/42], [94mLoss[0m : 2.68499
[1mStep[0m  [24/42], [94mLoss[0m : 2.77732
[1mStep[0m  [28/42], [94mLoss[0m : 2.74171
[1mStep[0m  [32/42], [94mLoss[0m : 2.82376
[1mStep[0m  [36/42], [94mLoss[0m : 2.65566
[1mStep[0m  [40/42], [94mLoss[0m : 2.66039

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.773, [92mTest[0m: 2.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84071
[1mStep[0m  [4/42], [94mLoss[0m : 2.71455
[1mStep[0m  [8/42], [94mLoss[0m : 2.79926
[1mStep[0m  [12/42], [94mLoss[0m : 2.60347
[1mStep[0m  [16/42], [94mLoss[0m : 2.80750
[1mStep[0m  [20/42], [94mLoss[0m : 2.62105
[1mStep[0m  [24/42], [94mLoss[0m : 2.64563
[1mStep[0m  [28/42], [94mLoss[0m : 2.72478
[1mStep[0m  [32/42], [94mLoss[0m : 2.82949
[1mStep[0m  [36/42], [94mLoss[0m : 2.60284
[1mStep[0m  [40/42], [94mLoss[0m : 2.85561

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.715, [92mTest[0m: 2.701, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76344
[1mStep[0m  [4/42], [94mLoss[0m : 2.78427
[1mStep[0m  [8/42], [94mLoss[0m : 2.82600
[1mStep[0m  [12/42], [94mLoss[0m : 2.78938
[1mStep[0m  [16/42], [94mLoss[0m : 2.57019
[1mStep[0m  [20/42], [94mLoss[0m : 2.55913
[1mStep[0m  [24/42], [94mLoss[0m : 2.66635
[1mStep[0m  [28/42], [94mLoss[0m : 2.77657
[1mStep[0m  [32/42], [94mLoss[0m : 2.77464
[1mStep[0m  [36/42], [94mLoss[0m : 2.76121
[1mStep[0m  [40/42], [94mLoss[0m : 2.82327

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.640, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55136
[1mStep[0m  [4/42], [94mLoss[0m : 2.54249
[1mStep[0m  [8/42], [94mLoss[0m : 2.63922
[1mStep[0m  [12/42], [94mLoss[0m : 2.51421
[1mStep[0m  [16/42], [94mLoss[0m : 2.78196
[1mStep[0m  [20/42], [94mLoss[0m : 2.58358
[1mStep[0m  [24/42], [94mLoss[0m : 2.52100
[1mStep[0m  [28/42], [94mLoss[0m : 2.66496
[1mStep[0m  [32/42], [94mLoss[0m : 2.58666
[1mStep[0m  [36/42], [94mLoss[0m : 2.52599
[1mStep[0m  [40/42], [94mLoss[0m : 2.67418

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.641, [92mTest[0m: 2.606, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65768
[1mStep[0m  [4/42], [94mLoss[0m : 2.55425
[1mStep[0m  [8/42], [94mLoss[0m : 2.81754
[1mStep[0m  [12/42], [94mLoss[0m : 2.54111
[1mStep[0m  [16/42], [94mLoss[0m : 2.60836
[1mStep[0m  [20/42], [94mLoss[0m : 2.80736
[1mStep[0m  [24/42], [94mLoss[0m : 2.47844
[1mStep[0m  [28/42], [94mLoss[0m : 2.79921
[1mStep[0m  [32/42], [94mLoss[0m : 2.69256
[1mStep[0m  [36/42], [94mLoss[0m : 2.48048
[1mStep[0m  [40/42], [94mLoss[0m : 2.67578

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73454
[1mStep[0m  [4/42], [94mLoss[0m : 2.56568
[1mStep[0m  [8/42], [94mLoss[0m : 2.70841
[1mStep[0m  [12/42], [94mLoss[0m : 2.44621
[1mStep[0m  [16/42], [94mLoss[0m : 2.90868
[1mStep[0m  [20/42], [94mLoss[0m : 2.98150
[1mStep[0m  [24/42], [94mLoss[0m : 2.50523
[1mStep[0m  [28/42], [94mLoss[0m : 2.55077
[1mStep[0m  [32/42], [94mLoss[0m : 2.85943
[1mStep[0m  [36/42], [94mLoss[0m : 2.78256
[1mStep[0m  [40/42], [94mLoss[0m : 2.50849

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.548, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65245
[1mStep[0m  [4/42], [94mLoss[0m : 2.68181
[1mStep[0m  [8/42], [94mLoss[0m : 2.71518
[1mStep[0m  [12/42], [94mLoss[0m : 2.62659
[1mStep[0m  [16/42], [94mLoss[0m : 2.44994
[1mStep[0m  [20/42], [94mLoss[0m : 2.66129
[1mStep[0m  [24/42], [94mLoss[0m : 2.65436
[1mStep[0m  [28/42], [94mLoss[0m : 2.53185
[1mStep[0m  [32/42], [94mLoss[0m : 2.63011
[1mStep[0m  [36/42], [94mLoss[0m : 2.49830
[1mStep[0m  [40/42], [94mLoss[0m : 2.46124

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57643
[1mStep[0m  [4/42], [94mLoss[0m : 2.68119
[1mStep[0m  [8/42], [94mLoss[0m : 2.65816
[1mStep[0m  [12/42], [94mLoss[0m : 2.57774
[1mStep[0m  [16/42], [94mLoss[0m : 2.76637
[1mStep[0m  [20/42], [94mLoss[0m : 2.80788
[1mStep[0m  [24/42], [94mLoss[0m : 2.50887
[1mStep[0m  [28/42], [94mLoss[0m : 2.32343
[1mStep[0m  [32/42], [94mLoss[0m : 2.69444
[1mStep[0m  [36/42], [94mLoss[0m : 2.38803
[1mStep[0m  [40/42], [94mLoss[0m : 2.43485

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.519, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40117
[1mStep[0m  [4/42], [94mLoss[0m : 2.73731
[1mStep[0m  [8/42], [94mLoss[0m : 2.39241
[1mStep[0m  [12/42], [94mLoss[0m : 2.48263
[1mStep[0m  [16/42], [94mLoss[0m : 2.62997
[1mStep[0m  [20/42], [94mLoss[0m : 2.51865
[1mStep[0m  [24/42], [94mLoss[0m : 2.61690
[1mStep[0m  [28/42], [94mLoss[0m : 2.36078
[1mStep[0m  [32/42], [94mLoss[0m : 2.55879
[1mStep[0m  [36/42], [94mLoss[0m : 2.82397
[1mStep[0m  [40/42], [94mLoss[0m : 2.44877

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56205
[1mStep[0m  [4/42], [94mLoss[0m : 2.61139
[1mStep[0m  [8/42], [94mLoss[0m : 2.42798
[1mStep[0m  [12/42], [94mLoss[0m : 2.48474
[1mStep[0m  [16/42], [94mLoss[0m : 2.44657
[1mStep[0m  [20/42], [94mLoss[0m : 2.43941
[1mStep[0m  [24/42], [94mLoss[0m : 2.58559
[1mStep[0m  [28/42], [94mLoss[0m : 2.54820
[1mStep[0m  [32/42], [94mLoss[0m : 2.32100
[1mStep[0m  [36/42], [94mLoss[0m : 2.46022
[1mStep[0m  [40/42], [94mLoss[0m : 2.52427

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44741
[1mStep[0m  [4/42], [94mLoss[0m : 2.60749
[1mStep[0m  [8/42], [94mLoss[0m : 2.68571
[1mStep[0m  [12/42], [94mLoss[0m : 2.70178
[1mStep[0m  [16/42], [94mLoss[0m : 2.62128
[1mStep[0m  [20/42], [94mLoss[0m : 2.51000
[1mStep[0m  [24/42], [94mLoss[0m : 2.66383
[1mStep[0m  [28/42], [94mLoss[0m : 2.62910
[1mStep[0m  [32/42], [94mLoss[0m : 2.39098
[1mStep[0m  [36/42], [94mLoss[0m : 2.64066
[1mStep[0m  [40/42], [94mLoss[0m : 2.73721

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51444
[1mStep[0m  [4/42], [94mLoss[0m : 2.59796
[1mStep[0m  [8/42], [94mLoss[0m : 2.53890
[1mStep[0m  [12/42], [94mLoss[0m : 2.52631
[1mStep[0m  [16/42], [94mLoss[0m : 2.46091
[1mStep[0m  [20/42], [94mLoss[0m : 2.47525
[1mStep[0m  [24/42], [94mLoss[0m : 2.51316
[1mStep[0m  [28/42], [94mLoss[0m : 2.34986
[1mStep[0m  [32/42], [94mLoss[0m : 2.56376
[1mStep[0m  [36/42], [94mLoss[0m : 2.45150
[1mStep[0m  [40/42], [94mLoss[0m : 2.48139

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.467, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49477
[1mStep[0m  [4/42], [94mLoss[0m : 2.42608
[1mStep[0m  [8/42], [94mLoss[0m : 2.34717
[1mStep[0m  [12/42], [94mLoss[0m : 2.47394
[1mStep[0m  [16/42], [94mLoss[0m : 2.54600
[1mStep[0m  [20/42], [94mLoss[0m : 2.45833
[1mStep[0m  [24/42], [94mLoss[0m : 2.42414
[1mStep[0m  [28/42], [94mLoss[0m : 2.64561
[1mStep[0m  [32/42], [94mLoss[0m : 2.76737
[1mStep[0m  [36/42], [94mLoss[0m : 2.43834
[1mStep[0m  [40/42], [94mLoss[0m : 2.42242

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47867
[1mStep[0m  [4/42], [94mLoss[0m : 2.40136
[1mStep[0m  [8/42], [94mLoss[0m : 2.59074
[1mStep[0m  [12/42], [94mLoss[0m : 2.59505
[1mStep[0m  [16/42], [94mLoss[0m : 2.38113
[1mStep[0m  [20/42], [94mLoss[0m : 2.43858
[1mStep[0m  [24/42], [94mLoss[0m : 2.39807
[1mStep[0m  [28/42], [94mLoss[0m : 2.55061
[1mStep[0m  [32/42], [94mLoss[0m : 2.55997
[1mStep[0m  [36/42], [94mLoss[0m : 2.40212
[1mStep[0m  [40/42], [94mLoss[0m : 2.63831

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.461, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56769
[1mStep[0m  [4/42], [94mLoss[0m : 2.56453
[1mStep[0m  [8/42], [94mLoss[0m : 2.50988
[1mStep[0m  [12/42], [94mLoss[0m : 2.57101
[1mStep[0m  [16/42], [94mLoss[0m : 2.49890
[1mStep[0m  [20/42], [94mLoss[0m : 2.38202
[1mStep[0m  [24/42], [94mLoss[0m : 2.56961
[1mStep[0m  [28/42], [94mLoss[0m : 2.50693
[1mStep[0m  [32/42], [94mLoss[0m : 2.89908
[1mStep[0m  [36/42], [94mLoss[0m : 2.39710
[1mStep[0m  [40/42], [94mLoss[0m : 2.38573

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.460, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49194
[1mStep[0m  [4/42], [94mLoss[0m : 2.58867
[1mStep[0m  [8/42], [94mLoss[0m : 2.38595
[1mStep[0m  [12/42], [94mLoss[0m : 2.46351
[1mStep[0m  [16/42], [94mLoss[0m : 2.43977
[1mStep[0m  [20/42], [94mLoss[0m : 2.25577
[1mStep[0m  [24/42], [94mLoss[0m : 2.29972
[1mStep[0m  [28/42], [94mLoss[0m : 2.49124
[1mStep[0m  [32/42], [94mLoss[0m : 2.52213
[1mStep[0m  [36/42], [94mLoss[0m : 2.54866
[1mStep[0m  [40/42], [94mLoss[0m : 2.46792

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.451, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48430
[1mStep[0m  [4/42], [94mLoss[0m : 2.44472
[1mStep[0m  [8/42], [94mLoss[0m : 2.65017
[1mStep[0m  [12/42], [94mLoss[0m : 2.42739
[1mStep[0m  [16/42], [94mLoss[0m : 2.57457
[1mStep[0m  [20/42], [94mLoss[0m : 2.52104
[1mStep[0m  [24/42], [94mLoss[0m : 2.53046
[1mStep[0m  [28/42], [94mLoss[0m : 2.53742
[1mStep[0m  [32/42], [94mLoss[0m : 2.35182
[1mStep[0m  [36/42], [94mLoss[0m : 2.53834
[1mStep[0m  [40/42], [94mLoss[0m : 2.47934

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51067
[1mStep[0m  [4/42], [94mLoss[0m : 2.48070
[1mStep[0m  [8/42], [94mLoss[0m : 2.50899
[1mStep[0m  [12/42], [94mLoss[0m : 2.40877
[1mStep[0m  [16/42], [94mLoss[0m : 2.39118
[1mStep[0m  [20/42], [94mLoss[0m : 2.44224
[1mStep[0m  [24/42], [94mLoss[0m : 2.63883
[1mStep[0m  [28/42], [94mLoss[0m : 2.57459
[1mStep[0m  [32/42], [94mLoss[0m : 2.63740
[1mStep[0m  [36/42], [94mLoss[0m : 2.28822
[1mStep[0m  [40/42], [94mLoss[0m : 2.58434

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.448, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40967
[1mStep[0m  [4/42], [94mLoss[0m : 2.41468
[1mStep[0m  [8/42], [94mLoss[0m : 2.37897
[1mStep[0m  [12/42], [94mLoss[0m : 2.73173
[1mStep[0m  [16/42], [94mLoss[0m : 2.55905
[1mStep[0m  [20/42], [94mLoss[0m : 2.49117
[1mStep[0m  [24/42], [94mLoss[0m : 2.44542
[1mStep[0m  [28/42], [94mLoss[0m : 2.55012
[1mStep[0m  [32/42], [94mLoss[0m : 2.38005
[1mStep[0m  [36/42], [94mLoss[0m : 2.49823
[1mStep[0m  [40/42], [94mLoss[0m : 2.48118

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.442, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54323
[1mStep[0m  [4/42], [94mLoss[0m : 2.29950
[1mStep[0m  [8/42], [94mLoss[0m : 2.51510
[1mStep[0m  [12/42], [94mLoss[0m : 2.64141
[1mStep[0m  [16/42], [94mLoss[0m : 2.33043
[1mStep[0m  [20/42], [94mLoss[0m : 2.42375
[1mStep[0m  [24/42], [94mLoss[0m : 2.53548
[1mStep[0m  [28/42], [94mLoss[0m : 2.37895
[1mStep[0m  [32/42], [94mLoss[0m : 2.57733
[1mStep[0m  [36/42], [94mLoss[0m : 2.47414
[1mStep[0m  [40/42], [94mLoss[0m : 2.28796

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.442, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36515
[1mStep[0m  [4/42], [94mLoss[0m : 2.57544
[1mStep[0m  [8/42], [94mLoss[0m : 2.59620
[1mStep[0m  [12/42], [94mLoss[0m : 2.49841
[1mStep[0m  [16/42], [94mLoss[0m : 2.39556
[1mStep[0m  [20/42], [94mLoss[0m : 2.66552
[1mStep[0m  [24/42], [94mLoss[0m : 2.33207
[1mStep[0m  [28/42], [94mLoss[0m : 2.49361
[1mStep[0m  [32/42], [94mLoss[0m : 2.35233
[1mStep[0m  [36/42], [94mLoss[0m : 2.58301
[1mStep[0m  [40/42], [94mLoss[0m : 2.42097

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21035
[1mStep[0m  [4/42], [94mLoss[0m : 2.51870
[1mStep[0m  [8/42], [94mLoss[0m : 2.58831
[1mStep[0m  [12/42], [94mLoss[0m : 2.59536
[1mStep[0m  [16/42], [94mLoss[0m : 2.38318
[1mStep[0m  [20/42], [94mLoss[0m : 2.59551
[1mStep[0m  [24/42], [94mLoss[0m : 2.43851
[1mStep[0m  [28/42], [94mLoss[0m : 2.42191
[1mStep[0m  [32/42], [94mLoss[0m : 2.52053
[1mStep[0m  [36/42], [94mLoss[0m : 2.76090
[1mStep[0m  [40/42], [94mLoss[0m : 2.54312

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.434, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40439
[1mStep[0m  [4/42], [94mLoss[0m : 2.47760
[1mStep[0m  [8/42], [94mLoss[0m : 2.35241
[1mStep[0m  [12/42], [94mLoss[0m : 2.60563
[1mStep[0m  [16/42], [94mLoss[0m : 2.56748
[1mStep[0m  [20/42], [94mLoss[0m : 2.66405
[1mStep[0m  [24/42], [94mLoss[0m : 2.42370
[1mStep[0m  [28/42], [94mLoss[0m : 2.46936
[1mStep[0m  [32/42], [94mLoss[0m : 2.64450
[1mStep[0m  [36/42], [94mLoss[0m : 2.54594
[1mStep[0m  [40/42], [94mLoss[0m : 2.55600

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.443, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54406
[1mStep[0m  [4/42], [94mLoss[0m : 2.50491
[1mStep[0m  [8/42], [94mLoss[0m : 2.39695
[1mStep[0m  [12/42], [94mLoss[0m : 2.39642
[1mStep[0m  [16/42], [94mLoss[0m : 2.62800
[1mStep[0m  [20/42], [94mLoss[0m : 2.50236
[1mStep[0m  [24/42], [94mLoss[0m : 2.59798
[1mStep[0m  [28/42], [94mLoss[0m : 2.46965
[1mStep[0m  [32/42], [94mLoss[0m : 2.52974
[1mStep[0m  [36/42], [94mLoss[0m : 2.48171
[1mStep[0m  [40/42], [94mLoss[0m : 2.50458

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.432, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40902
[1mStep[0m  [4/42], [94mLoss[0m : 2.49772
[1mStep[0m  [8/42], [94mLoss[0m : 2.61529
[1mStep[0m  [12/42], [94mLoss[0m : 2.46827
[1mStep[0m  [16/42], [94mLoss[0m : 2.63283
[1mStep[0m  [20/42], [94mLoss[0m : 2.46910
[1mStep[0m  [24/42], [94mLoss[0m : 2.44651
[1mStep[0m  [28/42], [94mLoss[0m : 2.48737
[1mStep[0m  [32/42], [94mLoss[0m : 2.34623
[1mStep[0m  [36/42], [94mLoss[0m : 2.38978
[1mStep[0m  [40/42], [94mLoss[0m : 2.63959

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.425, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.423
====================================

Phase 1 - Evaluation MAE:  2.4232499258858815
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 2.30955
[1mStep[0m  [4/42], [94mLoss[0m : 2.45749
[1mStep[0m  [8/42], [94mLoss[0m : 2.66603
[1mStep[0m  [12/42], [94mLoss[0m : 2.49391
[1mStep[0m  [16/42], [94mLoss[0m : 2.37630
[1mStep[0m  [20/42], [94mLoss[0m : 2.39700
[1mStep[0m  [24/42], [94mLoss[0m : 2.51189
[1mStep[0m  [28/42], [94mLoss[0m : 2.38797
[1mStep[0m  [32/42], [94mLoss[0m : 2.64356
[1mStep[0m  [36/42], [94mLoss[0m : 2.46293
[1mStep[0m  [40/42], [94mLoss[0m : 2.45178

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43633
[1mStep[0m  [4/42], [94mLoss[0m : 2.57628
[1mStep[0m  [8/42], [94mLoss[0m : 2.29057
[1mStep[0m  [12/42], [94mLoss[0m : 2.57877
[1mStep[0m  [16/42], [94mLoss[0m : 2.50110
[1mStep[0m  [20/42], [94mLoss[0m : 2.59405
[1mStep[0m  [24/42], [94mLoss[0m : 2.57322
[1mStep[0m  [28/42], [94mLoss[0m : 2.51017
[1mStep[0m  [32/42], [94mLoss[0m : 2.50220
[1mStep[0m  [36/42], [94mLoss[0m : 2.35291
[1mStep[0m  [40/42], [94mLoss[0m : 2.43039

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44077
[1mStep[0m  [4/42], [94mLoss[0m : 2.48050
[1mStep[0m  [8/42], [94mLoss[0m : 2.79971
[1mStep[0m  [12/42], [94mLoss[0m : 2.45174
[1mStep[0m  [16/42], [94mLoss[0m : 2.54136
[1mStep[0m  [20/42], [94mLoss[0m : 2.62665
[1mStep[0m  [24/42], [94mLoss[0m : 2.57822
[1mStep[0m  [28/42], [94mLoss[0m : 2.45685
[1mStep[0m  [32/42], [94mLoss[0m : 2.50846
[1mStep[0m  [36/42], [94mLoss[0m : 2.18977
[1mStep[0m  [40/42], [94mLoss[0m : 2.53744

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.419, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42552
[1mStep[0m  [4/42], [94mLoss[0m : 2.59344
[1mStep[0m  [8/42], [94mLoss[0m : 2.62655
[1mStep[0m  [12/42], [94mLoss[0m : 2.35793
[1mStep[0m  [16/42], [94mLoss[0m : 2.45647
[1mStep[0m  [20/42], [94mLoss[0m : 2.36598
[1mStep[0m  [24/42], [94mLoss[0m : 2.54602
[1mStep[0m  [28/42], [94mLoss[0m : 2.39062
[1mStep[0m  [32/42], [94mLoss[0m : 2.50710
[1mStep[0m  [36/42], [94mLoss[0m : 2.48514
[1mStep[0m  [40/42], [94mLoss[0m : 2.41893

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.411, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56547
[1mStep[0m  [4/42], [94mLoss[0m : 2.56810
[1mStep[0m  [8/42], [94mLoss[0m : 2.42234
[1mStep[0m  [12/42], [94mLoss[0m : 2.43902
[1mStep[0m  [16/42], [94mLoss[0m : 2.26313
[1mStep[0m  [20/42], [94mLoss[0m : 2.62779
[1mStep[0m  [24/42], [94mLoss[0m : 2.48522
[1mStep[0m  [28/42], [94mLoss[0m : 2.35076
[1mStep[0m  [32/42], [94mLoss[0m : 2.39451
[1mStep[0m  [36/42], [94mLoss[0m : 2.44874
[1mStep[0m  [40/42], [94mLoss[0m : 2.47706

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.404, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.18258
[1mStep[0m  [4/42], [94mLoss[0m : 2.39817
[1mStep[0m  [8/42], [94mLoss[0m : 2.25241
[1mStep[0m  [12/42], [94mLoss[0m : 2.57458
[1mStep[0m  [16/42], [94mLoss[0m : 2.49816
[1mStep[0m  [20/42], [94mLoss[0m : 2.54039
[1mStep[0m  [24/42], [94mLoss[0m : 2.32709
[1mStep[0m  [28/42], [94mLoss[0m : 2.53727
[1mStep[0m  [32/42], [94mLoss[0m : 2.31170
[1mStep[0m  [36/42], [94mLoss[0m : 2.50159
[1mStep[0m  [40/42], [94mLoss[0m : 2.28484

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.410, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38586
[1mStep[0m  [4/42], [94mLoss[0m : 2.35485
[1mStep[0m  [8/42], [94mLoss[0m : 2.38616
[1mStep[0m  [12/42], [94mLoss[0m : 2.34536
[1mStep[0m  [16/42], [94mLoss[0m : 2.55334
[1mStep[0m  [20/42], [94mLoss[0m : 2.78038
[1mStep[0m  [24/42], [94mLoss[0m : 2.58998
[1mStep[0m  [28/42], [94mLoss[0m : 2.33928
[1mStep[0m  [32/42], [94mLoss[0m : 2.37459
[1mStep[0m  [36/42], [94mLoss[0m : 2.26894
[1mStep[0m  [40/42], [94mLoss[0m : 2.65807

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.405, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28874
[1mStep[0m  [4/42], [94mLoss[0m : 2.37520
[1mStep[0m  [8/42], [94mLoss[0m : 2.47511
[1mStep[0m  [12/42], [94mLoss[0m : 2.40760
[1mStep[0m  [16/42], [94mLoss[0m : 2.35563
[1mStep[0m  [20/42], [94mLoss[0m : 2.37402
[1mStep[0m  [24/42], [94mLoss[0m : 2.49186
[1mStep[0m  [28/42], [94mLoss[0m : 2.51070
[1mStep[0m  [32/42], [94mLoss[0m : 2.42180
[1mStep[0m  [36/42], [94mLoss[0m : 2.43525
[1mStep[0m  [40/42], [94mLoss[0m : 2.52027

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.399, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34419
[1mStep[0m  [4/42], [94mLoss[0m : 2.54995
[1mStep[0m  [8/42], [94mLoss[0m : 2.45801
[1mStep[0m  [12/42], [94mLoss[0m : 2.46569
[1mStep[0m  [16/42], [94mLoss[0m : 2.39779
[1mStep[0m  [20/42], [94mLoss[0m : 2.44203
[1mStep[0m  [24/42], [94mLoss[0m : 2.61332
[1mStep[0m  [28/42], [94mLoss[0m : 2.61275
[1mStep[0m  [32/42], [94mLoss[0m : 2.43120
[1mStep[0m  [36/42], [94mLoss[0m : 2.42779
[1mStep[0m  [40/42], [94mLoss[0m : 2.45014

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.393, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29556
[1mStep[0m  [4/42], [94mLoss[0m : 2.53232
[1mStep[0m  [8/42], [94mLoss[0m : 2.46546
[1mStep[0m  [12/42], [94mLoss[0m : 2.42854
[1mStep[0m  [16/42], [94mLoss[0m : 2.44754
[1mStep[0m  [20/42], [94mLoss[0m : 2.42364
[1mStep[0m  [24/42], [94mLoss[0m : 2.42282
[1mStep[0m  [28/42], [94mLoss[0m : 2.50541
[1mStep[0m  [32/42], [94mLoss[0m : 2.56182
[1mStep[0m  [36/42], [94mLoss[0m : 2.69285
[1mStep[0m  [40/42], [94mLoss[0m : 2.28751

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.392, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46418
[1mStep[0m  [4/42], [94mLoss[0m : 2.46030
[1mStep[0m  [8/42], [94mLoss[0m : 2.56702
[1mStep[0m  [12/42], [94mLoss[0m : 2.60886
[1mStep[0m  [16/42], [94mLoss[0m : 2.41878
[1mStep[0m  [20/42], [94mLoss[0m : 2.25923
[1mStep[0m  [24/42], [94mLoss[0m : 2.39595
[1mStep[0m  [28/42], [94mLoss[0m : 2.40741
[1mStep[0m  [32/42], [94mLoss[0m : 2.42962
[1mStep[0m  [36/42], [94mLoss[0m : 2.38735
[1mStep[0m  [40/42], [94mLoss[0m : 2.30479

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.390, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65284
[1mStep[0m  [4/42], [94mLoss[0m : 2.42126
[1mStep[0m  [8/42], [94mLoss[0m : 2.47327
[1mStep[0m  [12/42], [94mLoss[0m : 2.55732
[1mStep[0m  [16/42], [94mLoss[0m : 2.22865
[1mStep[0m  [20/42], [94mLoss[0m : 2.40197
[1mStep[0m  [24/42], [94mLoss[0m : 2.29545
[1mStep[0m  [28/42], [94mLoss[0m : 2.34535
[1mStep[0m  [32/42], [94mLoss[0m : 2.58848
[1mStep[0m  [36/42], [94mLoss[0m : 2.36470
[1mStep[0m  [40/42], [94mLoss[0m : 2.32480

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.385, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10340
[1mStep[0m  [4/42], [94mLoss[0m : 2.46160
[1mStep[0m  [8/42], [94mLoss[0m : 2.34389
[1mStep[0m  [12/42], [94mLoss[0m : 2.48825
[1mStep[0m  [16/42], [94mLoss[0m : 2.37819
[1mStep[0m  [20/42], [94mLoss[0m : 2.26873
[1mStep[0m  [24/42], [94mLoss[0m : 2.44913
[1mStep[0m  [28/42], [94mLoss[0m : 2.34588
[1mStep[0m  [32/42], [94mLoss[0m : 2.31939
[1mStep[0m  [36/42], [94mLoss[0m : 2.46047
[1mStep[0m  [40/42], [94mLoss[0m : 2.50036

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.383, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39042
[1mStep[0m  [4/42], [94mLoss[0m : 2.31972
[1mStep[0m  [8/42], [94mLoss[0m : 2.37913
[1mStep[0m  [12/42], [94mLoss[0m : 2.48705
[1mStep[0m  [16/42], [94mLoss[0m : 2.30740
[1mStep[0m  [20/42], [94mLoss[0m : 2.48998
[1mStep[0m  [24/42], [94mLoss[0m : 2.70099
[1mStep[0m  [28/42], [94mLoss[0m : 2.58577
[1mStep[0m  [32/42], [94mLoss[0m : 2.53390
[1mStep[0m  [36/42], [94mLoss[0m : 2.26289
[1mStep[0m  [40/42], [94mLoss[0m : 2.45574

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.395, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45055
[1mStep[0m  [4/42], [94mLoss[0m : 2.46497
[1mStep[0m  [8/42], [94mLoss[0m : 2.40568
[1mStep[0m  [12/42], [94mLoss[0m : 2.41763
[1mStep[0m  [16/42], [94mLoss[0m : 2.46294
[1mStep[0m  [20/42], [94mLoss[0m : 2.39120
[1mStep[0m  [24/42], [94mLoss[0m : 2.17452
[1mStep[0m  [28/42], [94mLoss[0m : 2.55897
[1mStep[0m  [32/42], [94mLoss[0m : 2.45528
[1mStep[0m  [36/42], [94mLoss[0m : 2.67651
[1mStep[0m  [40/42], [94mLoss[0m : 2.39844

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36789
[1mStep[0m  [4/42], [94mLoss[0m : 2.41055
[1mStep[0m  [8/42], [94mLoss[0m : 2.48154
[1mStep[0m  [12/42], [94mLoss[0m : 2.36554
[1mStep[0m  [16/42], [94mLoss[0m : 2.34971
[1mStep[0m  [20/42], [94mLoss[0m : 2.43370
[1mStep[0m  [24/42], [94mLoss[0m : 2.38841
[1mStep[0m  [28/42], [94mLoss[0m : 2.38934
[1mStep[0m  [32/42], [94mLoss[0m : 2.46001
[1mStep[0m  [36/42], [94mLoss[0m : 2.25320
[1mStep[0m  [40/42], [94mLoss[0m : 2.35483

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.380, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28514
[1mStep[0m  [4/42], [94mLoss[0m : 2.28876
[1mStep[0m  [8/42], [94mLoss[0m : 2.38923
[1mStep[0m  [12/42], [94mLoss[0m : 2.44387
[1mStep[0m  [16/42], [94mLoss[0m : 2.29239
[1mStep[0m  [20/42], [94mLoss[0m : 2.48774
[1mStep[0m  [24/42], [94mLoss[0m : 2.41738
[1mStep[0m  [28/42], [94mLoss[0m : 2.35340
[1mStep[0m  [32/42], [94mLoss[0m : 2.36993
[1mStep[0m  [36/42], [94mLoss[0m : 2.39652
[1mStep[0m  [40/42], [94mLoss[0m : 2.53788

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.386, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39869
[1mStep[0m  [4/42], [94mLoss[0m : 2.37706
[1mStep[0m  [8/42], [94mLoss[0m : 2.39742
[1mStep[0m  [12/42], [94mLoss[0m : 2.23637
[1mStep[0m  [16/42], [94mLoss[0m : 2.29800
[1mStep[0m  [20/42], [94mLoss[0m : 2.40097
[1mStep[0m  [24/42], [94mLoss[0m : 2.39483
[1mStep[0m  [28/42], [94mLoss[0m : 2.43326
[1mStep[0m  [32/42], [94mLoss[0m : 2.48923
[1mStep[0m  [36/42], [94mLoss[0m : 2.31461
[1mStep[0m  [40/42], [94mLoss[0m : 2.39402

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.375, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27544
[1mStep[0m  [4/42], [94mLoss[0m : 2.35532
[1mStep[0m  [8/42], [94mLoss[0m : 2.28389
[1mStep[0m  [12/42], [94mLoss[0m : 2.40404
[1mStep[0m  [16/42], [94mLoss[0m : 2.39569
[1mStep[0m  [20/42], [94mLoss[0m : 2.47823
[1mStep[0m  [24/42], [94mLoss[0m : 2.31422
[1mStep[0m  [28/42], [94mLoss[0m : 2.42212
[1mStep[0m  [32/42], [94mLoss[0m : 2.41847
[1mStep[0m  [36/42], [94mLoss[0m : 2.39849
[1mStep[0m  [40/42], [94mLoss[0m : 2.56988

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.375, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29416
[1mStep[0m  [4/42], [94mLoss[0m : 2.38391
[1mStep[0m  [8/42], [94mLoss[0m : 2.32416
[1mStep[0m  [12/42], [94mLoss[0m : 2.40627
[1mStep[0m  [16/42], [94mLoss[0m : 2.58395
[1mStep[0m  [20/42], [94mLoss[0m : 2.36135
[1mStep[0m  [24/42], [94mLoss[0m : 2.27611
[1mStep[0m  [28/42], [94mLoss[0m : 2.19167
[1mStep[0m  [32/42], [94mLoss[0m : 2.46222
[1mStep[0m  [36/42], [94mLoss[0m : 2.43574
[1mStep[0m  [40/42], [94mLoss[0m : 2.46220

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.381, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37234
[1mStep[0m  [4/42], [94mLoss[0m : 2.27760
[1mStep[0m  [8/42], [94mLoss[0m : 2.32198
[1mStep[0m  [12/42], [94mLoss[0m : 2.37838
[1mStep[0m  [16/42], [94mLoss[0m : 2.33966
[1mStep[0m  [20/42], [94mLoss[0m : 2.28308
[1mStep[0m  [24/42], [94mLoss[0m : 2.40147
[1mStep[0m  [28/42], [94mLoss[0m : 2.27400
[1mStep[0m  [32/42], [94mLoss[0m : 2.57134
[1mStep[0m  [36/42], [94mLoss[0m : 2.39012
[1mStep[0m  [40/42], [94mLoss[0m : 2.45327

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.378, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37780
[1mStep[0m  [4/42], [94mLoss[0m : 2.43289
[1mStep[0m  [8/42], [94mLoss[0m : 2.40720
[1mStep[0m  [12/42], [94mLoss[0m : 2.46726
[1mStep[0m  [16/42], [94mLoss[0m : 2.46178
[1mStep[0m  [20/42], [94mLoss[0m : 2.34472
[1mStep[0m  [24/42], [94mLoss[0m : 2.38635
[1mStep[0m  [28/42], [94mLoss[0m : 2.24464
[1mStep[0m  [32/42], [94mLoss[0m : 2.32486
[1mStep[0m  [36/42], [94mLoss[0m : 2.38030
[1mStep[0m  [40/42], [94mLoss[0m : 2.34149

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.355, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39289
[1mStep[0m  [4/42], [94mLoss[0m : 2.44138
[1mStep[0m  [8/42], [94mLoss[0m : 2.40137
[1mStep[0m  [12/42], [94mLoss[0m : 2.56668
[1mStep[0m  [16/42], [94mLoss[0m : 2.40336
[1mStep[0m  [20/42], [94mLoss[0m : 2.31302
[1mStep[0m  [24/42], [94mLoss[0m : 2.35810
[1mStep[0m  [28/42], [94mLoss[0m : 2.33815
[1mStep[0m  [32/42], [94mLoss[0m : 2.38442
[1mStep[0m  [36/42], [94mLoss[0m : 2.40155
[1mStep[0m  [40/42], [94mLoss[0m : 2.57005

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36083
[1mStep[0m  [4/42], [94mLoss[0m : 2.49888
[1mStep[0m  [8/42], [94mLoss[0m : 2.34002
[1mStep[0m  [12/42], [94mLoss[0m : 2.38223
[1mStep[0m  [16/42], [94mLoss[0m : 2.33804
[1mStep[0m  [20/42], [94mLoss[0m : 2.33033
[1mStep[0m  [24/42], [94mLoss[0m : 2.39964
[1mStep[0m  [28/42], [94mLoss[0m : 2.47036
[1mStep[0m  [32/42], [94mLoss[0m : 2.41018
[1mStep[0m  [36/42], [94mLoss[0m : 2.22117
[1mStep[0m  [40/42], [94mLoss[0m : 2.41835

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.377, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45009
[1mStep[0m  [4/42], [94mLoss[0m : 2.51071
[1mStep[0m  [8/42], [94mLoss[0m : 2.25467
[1mStep[0m  [12/42], [94mLoss[0m : 2.51942
[1mStep[0m  [16/42], [94mLoss[0m : 2.24517
[1mStep[0m  [20/42], [94mLoss[0m : 2.43202
[1mStep[0m  [24/42], [94mLoss[0m : 2.51346
[1mStep[0m  [28/42], [94mLoss[0m : 2.32406
[1mStep[0m  [32/42], [94mLoss[0m : 2.36498
[1mStep[0m  [36/42], [94mLoss[0m : 2.40068
[1mStep[0m  [40/42], [94mLoss[0m : 2.63240

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56195
[1mStep[0m  [4/42], [94mLoss[0m : 2.42010
[1mStep[0m  [8/42], [94mLoss[0m : 2.32310
[1mStep[0m  [12/42], [94mLoss[0m : 2.30850
[1mStep[0m  [16/42], [94mLoss[0m : 2.30314
[1mStep[0m  [20/42], [94mLoss[0m : 2.29076
[1mStep[0m  [24/42], [94mLoss[0m : 2.27606
[1mStep[0m  [28/42], [94mLoss[0m : 2.45085
[1mStep[0m  [32/42], [94mLoss[0m : 2.45047
[1mStep[0m  [36/42], [94mLoss[0m : 2.48457
[1mStep[0m  [40/42], [94mLoss[0m : 2.23274

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.373, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34188
[1mStep[0m  [4/42], [94mLoss[0m : 2.37006
[1mStep[0m  [8/42], [94mLoss[0m : 2.34215
[1mStep[0m  [12/42], [94mLoss[0m : 2.62216
[1mStep[0m  [16/42], [94mLoss[0m : 2.35863
[1mStep[0m  [20/42], [94mLoss[0m : 2.10567
[1mStep[0m  [24/42], [94mLoss[0m : 2.46257
[1mStep[0m  [28/42], [94mLoss[0m : 2.20525
[1mStep[0m  [32/42], [94mLoss[0m : 2.22821
[1mStep[0m  [36/42], [94mLoss[0m : 2.17318
[1mStep[0m  [40/42], [94mLoss[0m : 2.25639

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.377, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45300
[1mStep[0m  [4/42], [94mLoss[0m : 2.50789
[1mStep[0m  [8/42], [94mLoss[0m : 2.29247
[1mStep[0m  [12/42], [94mLoss[0m : 2.54578
[1mStep[0m  [16/42], [94mLoss[0m : 2.15767
[1mStep[0m  [20/42], [94mLoss[0m : 2.46002
[1mStep[0m  [24/42], [94mLoss[0m : 2.19764
[1mStep[0m  [28/42], [94mLoss[0m : 2.37750
[1mStep[0m  [32/42], [94mLoss[0m : 2.37961
[1mStep[0m  [36/42], [94mLoss[0m : 2.36887
[1mStep[0m  [40/42], [94mLoss[0m : 2.33801

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.380, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33218
[1mStep[0m  [4/42], [94mLoss[0m : 2.16834
[1mStep[0m  [8/42], [94mLoss[0m : 2.37238
[1mStep[0m  [12/42], [94mLoss[0m : 2.40951
[1mStep[0m  [16/42], [94mLoss[0m : 2.22892
[1mStep[0m  [20/42], [94mLoss[0m : 2.39942
[1mStep[0m  [24/42], [94mLoss[0m : 2.19150
[1mStep[0m  [28/42], [94mLoss[0m : 2.36141
[1mStep[0m  [32/42], [94mLoss[0m : 2.44309
[1mStep[0m  [36/42], [94mLoss[0m : 2.24096
[1mStep[0m  [40/42], [94mLoss[0m : 2.26599

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.363, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54983
[1mStep[0m  [4/42], [94mLoss[0m : 2.45246
[1mStep[0m  [8/42], [94mLoss[0m : 2.32268
[1mStep[0m  [12/42], [94mLoss[0m : 2.39443
[1mStep[0m  [16/42], [94mLoss[0m : 2.30387
[1mStep[0m  [20/42], [94mLoss[0m : 2.52827
[1mStep[0m  [24/42], [94mLoss[0m : 2.43625
[1mStep[0m  [28/42], [94mLoss[0m : 2.21792
[1mStep[0m  [32/42], [94mLoss[0m : 2.74946
[1mStep[0m  [36/42], [94mLoss[0m : 2.34379
[1mStep[0m  [40/42], [94mLoss[0m : 2.44963

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.359, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.377
====================================

Phase 2 - Evaluation MAE:  2.3772746665137157
MAE score P1       2.42325
MAE score P2      2.377275
loss              2.358711
learning_rate       0.0001
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.00525
[1mStep[0m  [8/84], [94mLoss[0m : 11.24651
[1mStep[0m  [16/84], [94mLoss[0m : 10.96714
[1mStep[0m  [24/84], [94mLoss[0m : 10.78494
[1mStep[0m  [32/84], [94mLoss[0m : 10.61902
[1mStep[0m  [40/84], [94mLoss[0m : 11.37874
[1mStep[0m  [48/84], [94mLoss[0m : 10.74915
[1mStep[0m  [56/84], [94mLoss[0m : 11.21252
[1mStep[0m  [64/84], [94mLoss[0m : 11.13468
[1mStep[0m  [72/84], [94mLoss[0m : 10.83107
[1mStep[0m  [80/84], [94mLoss[0m : 10.84251

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.942, [92mTest[0m: 11.097, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.76223
[1mStep[0m  [8/84], [94mLoss[0m : 10.82476
[1mStep[0m  [16/84], [94mLoss[0m : 10.95685
[1mStep[0m  [24/84], [94mLoss[0m : 10.99871
[1mStep[0m  [32/84], [94mLoss[0m : 10.87758
[1mStep[0m  [40/84], [94mLoss[0m : 10.73187
[1mStep[0m  [48/84], [94mLoss[0m : 11.61158
[1mStep[0m  [56/84], [94mLoss[0m : 11.22535
[1mStep[0m  [64/84], [94mLoss[0m : 11.03953
[1mStep[0m  [72/84], [94mLoss[0m : 10.66570
[1mStep[0m  [80/84], [94mLoss[0m : 10.38041

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.909, [92mTest[0m: 10.968, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.71924
[1mStep[0m  [8/84], [94mLoss[0m : 10.98989
[1mStep[0m  [16/84], [94mLoss[0m : 10.72333
[1mStep[0m  [24/84], [94mLoss[0m : 10.62173
[1mStep[0m  [32/84], [94mLoss[0m : 11.01942
[1mStep[0m  [40/84], [94mLoss[0m : 10.80043
[1mStep[0m  [48/84], [94mLoss[0m : 10.91856
[1mStep[0m  [56/84], [94mLoss[0m : 11.28246
[1mStep[0m  [64/84], [94mLoss[0m : 11.04327
[1mStep[0m  [72/84], [94mLoss[0m : 10.47501
[1mStep[0m  [80/84], [94mLoss[0m : 10.98082

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.882, [92mTest[0m: 10.916, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.63767
[1mStep[0m  [8/84], [94mLoss[0m : 11.05250
[1mStep[0m  [16/84], [94mLoss[0m : 11.41800
[1mStep[0m  [24/84], [94mLoss[0m : 10.42225
[1mStep[0m  [32/84], [94mLoss[0m : 11.42877
[1mStep[0m  [40/84], [94mLoss[0m : 10.68387
[1mStep[0m  [48/84], [94mLoss[0m : 10.21123
[1mStep[0m  [56/84], [94mLoss[0m : 10.68769
[1mStep[0m  [64/84], [94mLoss[0m : 11.04642
[1mStep[0m  [72/84], [94mLoss[0m : 10.54580
[1mStep[0m  [80/84], [94mLoss[0m : 10.18085

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.858, [92mTest[0m: 10.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.37632
[1mStep[0m  [8/84], [94mLoss[0m : 10.78643
[1mStep[0m  [16/84], [94mLoss[0m : 10.81045
[1mStep[0m  [24/84], [94mLoss[0m : 10.34236
[1mStep[0m  [32/84], [94mLoss[0m : 11.19367
[1mStep[0m  [40/84], [94mLoss[0m : 11.16521
[1mStep[0m  [48/84], [94mLoss[0m : 10.42106
[1mStep[0m  [56/84], [94mLoss[0m : 11.01917
[1mStep[0m  [64/84], [94mLoss[0m : 10.45779
[1mStep[0m  [72/84], [94mLoss[0m : 11.08072
[1mStep[0m  [80/84], [94mLoss[0m : 11.11502

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.836, [92mTest[0m: 10.853, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.25438
[1mStep[0m  [8/84], [94mLoss[0m : 10.45959
[1mStep[0m  [16/84], [94mLoss[0m : 10.26097
[1mStep[0m  [24/84], [94mLoss[0m : 10.82177
[1mStep[0m  [32/84], [94mLoss[0m : 10.88832
[1mStep[0m  [40/84], [94mLoss[0m : 11.47508
[1mStep[0m  [48/84], [94mLoss[0m : 10.49930
[1mStep[0m  [56/84], [94mLoss[0m : 10.89579
[1mStep[0m  [64/84], [94mLoss[0m : 10.88771
[1mStep[0m  [72/84], [94mLoss[0m : 11.32006
[1mStep[0m  [80/84], [94mLoss[0m : 10.46783

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.797, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.10981
[1mStep[0m  [8/84], [94mLoss[0m : 11.54623
[1mStep[0m  [16/84], [94mLoss[0m : 10.71377
[1mStep[0m  [24/84], [94mLoss[0m : 11.57321
[1mStep[0m  [32/84], [94mLoss[0m : 10.16003
[1mStep[0m  [40/84], [94mLoss[0m : 11.08563
[1mStep[0m  [48/84], [94mLoss[0m : 10.85129
[1mStep[0m  [56/84], [94mLoss[0m : 10.85750
[1mStep[0m  [64/84], [94mLoss[0m : 10.73134
[1mStep[0m  [72/84], [94mLoss[0m : 11.24877
[1mStep[0m  [80/84], [94mLoss[0m : 10.83098

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.774, [92mTest[0m: 10.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.16054
[1mStep[0m  [8/84], [94mLoss[0m : 11.10257
[1mStep[0m  [16/84], [94mLoss[0m : 10.44183
[1mStep[0m  [24/84], [94mLoss[0m : 10.62988
[1mStep[0m  [32/84], [94mLoss[0m : 10.54420
[1mStep[0m  [40/84], [94mLoss[0m : 10.65632
[1mStep[0m  [48/84], [94mLoss[0m : 11.11052
[1mStep[0m  [56/84], [94mLoss[0m : 10.54878
[1mStep[0m  [64/84], [94mLoss[0m : 10.45580
[1mStep[0m  [72/84], [94mLoss[0m : 10.53092
[1mStep[0m  [80/84], [94mLoss[0m : 11.00934

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.746, [92mTest[0m: 10.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.58665
[1mStep[0m  [8/84], [94mLoss[0m : 10.77397
[1mStep[0m  [16/84], [94mLoss[0m : 10.78534
[1mStep[0m  [24/84], [94mLoss[0m : 10.69300
[1mStep[0m  [32/84], [94mLoss[0m : 10.94714
[1mStep[0m  [40/84], [94mLoss[0m : 10.38160
[1mStep[0m  [48/84], [94mLoss[0m : 11.00568
[1mStep[0m  [56/84], [94mLoss[0m : 10.38305
[1mStep[0m  [64/84], [94mLoss[0m : 10.71281
[1mStep[0m  [72/84], [94mLoss[0m : 11.24903
[1mStep[0m  [80/84], [94mLoss[0m : 10.59617

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.717, [92mTest[0m: 10.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.67761
[1mStep[0m  [8/84], [94mLoss[0m : 10.50312
[1mStep[0m  [16/84], [94mLoss[0m : 11.24372
[1mStep[0m  [24/84], [94mLoss[0m : 10.34459
[1mStep[0m  [32/84], [94mLoss[0m : 11.15336
[1mStep[0m  [40/84], [94mLoss[0m : 10.76356
[1mStep[0m  [48/84], [94mLoss[0m : 10.93094
[1mStep[0m  [56/84], [94mLoss[0m : 10.66324
[1mStep[0m  [64/84], [94mLoss[0m : 10.48118
[1mStep[0m  [72/84], [94mLoss[0m : 10.25333
[1mStep[0m  [80/84], [94mLoss[0m : 10.81039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.688, [92mTest[0m: 10.644, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.20292
[1mStep[0m  [8/84], [94mLoss[0m : 10.39417
[1mStep[0m  [16/84], [94mLoss[0m : 10.35415
[1mStep[0m  [24/84], [94mLoss[0m : 10.46795
[1mStep[0m  [32/84], [94mLoss[0m : 10.92580
[1mStep[0m  [40/84], [94mLoss[0m : 10.77779
[1mStep[0m  [48/84], [94mLoss[0m : 10.51481
[1mStep[0m  [56/84], [94mLoss[0m : 9.80148
[1mStep[0m  [64/84], [94mLoss[0m : 10.81618
[1mStep[0m  [72/84], [94mLoss[0m : 11.10183
[1mStep[0m  [80/84], [94mLoss[0m : 10.52393

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.660, [92mTest[0m: 10.611, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.60701
[1mStep[0m  [8/84], [94mLoss[0m : 11.09966
[1mStep[0m  [16/84], [94mLoss[0m : 10.40463
[1mStep[0m  [24/84], [94mLoss[0m : 10.51233
[1mStep[0m  [32/84], [94mLoss[0m : 10.26027
[1mStep[0m  [40/84], [94mLoss[0m : 11.13355
[1mStep[0m  [48/84], [94mLoss[0m : 10.12654
[1mStep[0m  [56/84], [94mLoss[0m : 10.53984
[1mStep[0m  [64/84], [94mLoss[0m : 11.15057
[1mStep[0m  [72/84], [94mLoss[0m : 10.47274
[1mStep[0m  [80/84], [94mLoss[0m : 10.53443

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.637, [92mTest[0m: 10.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.33468
[1mStep[0m  [8/84], [94mLoss[0m : 10.41638
[1mStep[0m  [16/84], [94mLoss[0m : 10.45594
[1mStep[0m  [24/84], [94mLoss[0m : 11.01233
[1mStep[0m  [32/84], [94mLoss[0m : 10.08354
[1mStep[0m  [40/84], [94mLoss[0m : 10.53872
[1mStep[0m  [48/84], [94mLoss[0m : 10.41113
[1mStep[0m  [56/84], [94mLoss[0m : 10.18379
[1mStep[0m  [64/84], [94mLoss[0m : 10.83592
[1mStep[0m  [72/84], [94mLoss[0m : 11.25259
[1mStep[0m  [80/84], [94mLoss[0m : 10.25312

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.36279
[1mStep[0m  [8/84], [94mLoss[0m : 11.11599
[1mStep[0m  [16/84], [94mLoss[0m : 11.17784
[1mStep[0m  [24/84], [94mLoss[0m : 10.38496
[1mStep[0m  [32/84], [94mLoss[0m : 10.74161
[1mStep[0m  [40/84], [94mLoss[0m : 10.08780
[1mStep[0m  [48/84], [94mLoss[0m : 10.61425
[1mStep[0m  [56/84], [94mLoss[0m : 10.99114
[1mStep[0m  [64/84], [94mLoss[0m : 10.48920
[1mStep[0m  [72/84], [94mLoss[0m : 10.79072
[1mStep[0m  [80/84], [94mLoss[0m : 10.86186

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.585, [92mTest[0m: 10.510, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.07353
[1mStep[0m  [8/84], [94mLoss[0m : 10.66875
[1mStep[0m  [16/84], [94mLoss[0m : 11.02134
[1mStep[0m  [24/84], [94mLoss[0m : 10.27562
[1mStep[0m  [32/84], [94mLoss[0m : 11.73124
[1mStep[0m  [40/84], [94mLoss[0m : 10.37448
[1mStep[0m  [48/84], [94mLoss[0m : 10.40529
[1mStep[0m  [56/84], [94mLoss[0m : 10.94073
[1mStep[0m  [64/84], [94mLoss[0m : 10.46578
[1mStep[0m  [72/84], [94mLoss[0m : 10.52172
[1mStep[0m  [80/84], [94mLoss[0m : 10.68177

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.552, [92mTest[0m: 10.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.79706
[1mStep[0m  [8/84], [94mLoss[0m : 10.72426
[1mStep[0m  [16/84], [94mLoss[0m : 10.97706
[1mStep[0m  [24/84], [94mLoss[0m : 11.00542
[1mStep[0m  [32/84], [94mLoss[0m : 10.16446
[1mStep[0m  [40/84], [94mLoss[0m : 10.30699
[1mStep[0m  [48/84], [94mLoss[0m : 10.29967
[1mStep[0m  [56/84], [94mLoss[0m : 10.96563
[1mStep[0m  [64/84], [94mLoss[0m : 10.60478
[1mStep[0m  [72/84], [94mLoss[0m : 10.74917
[1mStep[0m  [80/84], [94mLoss[0m : 10.08731

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.527, [92mTest[0m: 10.422, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.50984
[1mStep[0m  [8/84], [94mLoss[0m : 10.94567
[1mStep[0m  [16/84], [94mLoss[0m : 10.60290
[1mStep[0m  [24/84], [94mLoss[0m : 10.58873
[1mStep[0m  [32/84], [94mLoss[0m : 9.92613
[1mStep[0m  [40/84], [94mLoss[0m : 10.96564
[1mStep[0m  [48/84], [94mLoss[0m : 10.82409
[1mStep[0m  [56/84], [94mLoss[0m : 10.53975
[1mStep[0m  [64/84], [94mLoss[0m : 10.50477
[1mStep[0m  [72/84], [94mLoss[0m : 10.77816
[1mStep[0m  [80/84], [94mLoss[0m : 11.14185

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.500, [92mTest[0m: 10.393, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.13595
[1mStep[0m  [8/84], [94mLoss[0m : 10.52859
[1mStep[0m  [16/84], [94mLoss[0m : 10.59021
[1mStep[0m  [24/84], [94mLoss[0m : 9.99310
[1mStep[0m  [32/84], [94mLoss[0m : 10.09142
[1mStep[0m  [40/84], [94mLoss[0m : 10.26574
[1mStep[0m  [48/84], [94mLoss[0m : 10.86300
[1mStep[0m  [56/84], [94mLoss[0m : 10.15961
[1mStep[0m  [64/84], [94mLoss[0m : 10.73110
[1mStep[0m  [72/84], [94mLoss[0m : 10.43343
[1mStep[0m  [80/84], [94mLoss[0m : 10.44492

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.473, [92mTest[0m: 10.374, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.49087
[1mStep[0m  [8/84], [94mLoss[0m : 10.79949
[1mStep[0m  [16/84], [94mLoss[0m : 10.39752
[1mStep[0m  [24/84], [94mLoss[0m : 10.32007
[1mStep[0m  [32/84], [94mLoss[0m : 10.80929
[1mStep[0m  [40/84], [94mLoss[0m : 10.28416
[1mStep[0m  [48/84], [94mLoss[0m : 10.60212
[1mStep[0m  [56/84], [94mLoss[0m : 10.88029
[1mStep[0m  [64/84], [94mLoss[0m : 10.05076
[1mStep[0m  [72/84], [94mLoss[0m : 10.78921
[1mStep[0m  [80/84], [94mLoss[0m : 10.42632

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.445, [92mTest[0m: 10.332, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.84692
[1mStep[0m  [8/84], [94mLoss[0m : 11.36082
[1mStep[0m  [16/84], [94mLoss[0m : 10.67612
[1mStep[0m  [24/84], [94mLoss[0m : 10.51544
[1mStep[0m  [32/84], [94mLoss[0m : 10.56156
[1mStep[0m  [40/84], [94mLoss[0m : 10.38469
[1mStep[0m  [48/84], [94mLoss[0m : 9.93365
[1mStep[0m  [56/84], [94mLoss[0m : 9.79812
[1mStep[0m  [64/84], [94mLoss[0m : 10.21285
[1mStep[0m  [72/84], [94mLoss[0m : 10.92486
[1mStep[0m  [80/84], [94mLoss[0m : 10.21428

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.411, [92mTest[0m: 10.306, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.83938
[1mStep[0m  [8/84], [94mLoss[0m : 10.49093
[1mStep[0m  [16/84], [94mLoss[0m : 10.55123
[1mStep[0m  [24/84], [94mLoss[0m : 10.51055
[1mStep[0m  [32/84], [94mLoss[0m : 10.85375
[1mStep[0m  [40/84], [94mLoss[0m : 10.91265
[1mStep[0m  [48/84], [94mLoss[0m : 10.54185
[1mStep[0m  [56/84], [94mLoss[0m : 10.30316
[1mStep[0m  [64/84], [94mLoss[0m : 10.18089
[1mStep[0m  [72/84], [94mLoss[0m : 10.75242
[1mStep[0m  [80/84], [94mLoss[0m : 10.34491

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.383, [92mTest[0m: 10.237, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.52680
[1mStep[0m  [8/84], [94mLoss[0m : 10.28581
[1mStep[0m  [16/84], [94mLoss[0m : 10.26028
[1mStep[0m  [24/84], [94mLoss[0m : 10.84303
[1mStep[0m  [32/84], [94mLoss[0m : 10.55785
[1mStep[0m  [40/84], [94mLoss[0m : 10.93137
[1mStep[0m  [48/84], [94mLoss[0m : 10.53415
[1mStep[0m  [56/84], [94mLoss[0m : 10.46262
[1mStep[0m  [64/84], [94mLoss[0m : 10.11897
[1mStep[0m  [72/84], [94mLoss[0m : 9.83000
[1mStep[0m  [80/84], [94mLoss[0m : 10.58174

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.353, [92mTest[0m: 10.163, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.93962
[1mStep[0m  [8/84], [94mLoss[0m : 10.13496
[1mStep[0m  [16/84], [94mLoss[0m : 10.37900
[1mStep[0m  [24/84], [94mLoss[0m : 10.02940
[1mStep[0m  [32/84], [94mLoss[0m : 10.22536
[1mStep[0m  [40/84], [94mLoss[0m : 10.17615
[1mStep[0m  [48/84], [94mLoss[0m : 10.31934
[1mStep[0m  [56/84], [94mLoss[0m : 11.17644
[1mStep[0m  [64/84], [94mLoss[0m : 9.47615
[1mStep[0m  [72/84], [94mLoss[0m : 10.02841
[1mStep[0m  [80/84], [94mLoss[0m : 9.93166

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.334, [92mTest[0m: 10.098, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.73919
[1mStep[0m  [8/84], [94mLoss[0m : 10.41582
[1mStep[0m  [16/84], [94mLoss[0m : 10.41971
[1mStep[0m  [24/84], [94mLoss[0m : 10.02355
[1mStep[0m  [32/84], [94mLoss[0m : 10.27903
[1mStep[0m  [40/84], [94mLoss[0m : 10.28592
[1mStep[0m  [48/84], [94mLoss[0m : 10.95127
[1mStep[0m  [56/84], [94mLoss[0m : 9.83337
[1mStep[0m  [64/84], [94mLoss[0m : 10.11902
[1mStep[0m  [72/84], [94mLoss[0m : 11.02284
[1mStep[0m  [80/84], [94mLoss[0m : 10.57750

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.300, [92mTest[0m: 10.085, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.32603
[1mStep[0m  [8/84], [94mLoss[0m : 9.80651
[1mStep[0m  [16/84], [94mLoss[0m : 10.49365
[1mStep[0m  [24/84], [94mLoss[0m : 10.75673
[1mStep[0m  [32/84], [94mLoss[0m : 10.25117
[1mStep[0m  [40/84], [94mLoss[0m : 10.33248
[1mStep[0m  [48/84], [94mLoss[0m : 10.08574
[1mStep[0m  [56/84], [94mLoss[0m : 10.11756
[1mStep[0m  [64/84], [94mLoss[0m : 9.91750
[1mStep[0m  [72/84], [94mLoss[0m : 10.06483
[1mStep[0m  [80/84], [94mLoss[0m : 10.02354

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.272, [92mTest[0m: 10.112, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.39943
[1mStep[0m  [8/84], [94mLoss[0m : 10.36709
[1mStep[0m  [16/84], [94mLoss[0m : 9.83768
[1mStep[0m  [24/84], [94mLoss[0m : 10.55131
[1mStep[0m  [32/84], [94mLoss[0m : 10.35438
[1mStep[0m  [40/84], [94mLoss[0m : 10.24618
[1mStep[0m  [48/84], [94mLoss[0m : 10.26005
[1mStep[0m  [56/84], [94mLoss[0m : 10.52694
[1mStep[0m  [64/84], [94mLoss[0m : 9.81203
[1mStep[0m  [72/84], [94mLoss[0m : 10.38120
[1mStep[0m  [80/84], [94mLoss[0m : 10.46250

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.256, [92mTest[0m: 10.049, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.09683
[1mStep[0m  [8/84], [94mLoss[0m : 10.46400
[1mStep[0m  [16/84], [94mLoss[0m : 10.03070
[1mStep[0m  [24/84], [94mLoss[0m : 10.22732
[1mStep[0m  [32/84], [94mLoss[0m : 10.91967
[1mStep[0m  [40/84], [94mLoss[0m : 10.50190
[1mStep[0m  [48/84], [94mLoss[0m : 10.77152
[1mStep[0m  [56/84], [94mLoss[0m : 9.68539
[1mStep[0m  [64/84], [94mLoss[0m : 10.17008
[1mStep[0m  [72/84], [94mLoss[0m : 10.08701
[1mStep[0m  [80/84], [94mLoss[0m : 10.21785

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.226, [92mTest[0m: 9.978, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.74757
[1mStep[0m  [8/84], [94mLoss[0m : 10.20411
[1mStep[0m  [16/84], [94mLoss[0m : 9.84971
[1mStep[0m  [24/84], [94mLoss[0m : 10.71785
[1mStep[0m  [32/84], [94mLoss[0m : 9.72015
[1mStep[0m  [40/84], [94mLoss[0m : 9.87126
[1mStep[0m  [48/84], [94mLoss[0m : 10.06263
[1mStep[0m  [56/84], [94mLoss[0m : 9.37458
[1mStep[0m  [64/84], [94mLoss[0m : 10.05030
[1mStep[0m  [72/84], [94mLoss[0m : 10.23886
[1mStep[0m  [80/84], [94mLoss[0m : 9.67043

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.194, [92mTest[0m: 9.964, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.15143
[1mStep[0m  [8/84], [94mLoss[0m : 10.00849
[1mStep[0m  [16/84], [94mLoss[0m : 10.47336
[1mStep[0m  [24/84], [94mLoss[0m : 10.58092
[1mStep[0m  [32/84], [94mLoss[0m : 10.60167
[1mStep[0m  [40/84], [94mLoss[0m : 9.61151
[1mStep[0m  [48/84], [94mLoss[0m : 9.87566
[1mStep[0m  [56/84], [94mLoss[0m : 10.31476
[1mStep[0m  [64/84], [94mLoss[0m : 10.24887
[1mStep[0m  [72/84], [94mLoss[0m : 10.69747
[1mStep[0m  [80/84], [94mLoss[0m : 10.39651

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.178, [92mTest[0m: 9.919, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.18927
[1mStep[0m  [8/84], [94mLoss[0m : 10.23557
[1mStep[0m  [16/84], [94mLoss[0m : 10.17581
[1mStep[0m  [24/84], [94mLoss[0m : 9.74167
[1mStep[0m  [32/84], [94mLoss[0m : 9.83182
[1mStep[0m  [40/84], [94mLoss[0m : 10.49336
[1mStep[0m  [48/84], [94mLoss[0m : 10.65529
[1mStep[0m  [56/84], [94mLoss[0m : 9.92530
[1mStep[0m  [64/84], [94mLoss[0m : 10.34536
[1mStep[0m  [72/84], [94mLoss[0m : 10.86866
[1mStep[0m  [80/84], [94mLoss[0m : 10.03580

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.145, [92mTest[0m: 9.931, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.826
====================================

Phase 1 - Evaluation MAE:  9.825995002474103
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.13199
[1mStep[0m  [8/84], [94mLoss[0m : 10.40229
[1mStep[0m  [16/84], [94mLoss[0m : 10.33330
[1mStep[0m  [24/84], [94mLoss[0m : 9.77345
[1mStep[0m  [32/84], [94mLoss[0m : 10.03335
[1mStep[0m  [40/84], [94mLoss[0m : 10.05700
[1mStep[0m  [48/84], [94mLoss[0m : 10.10510
[1mStep[0m  [56/84], [94mLoss[0m : 9.91761
[1mStep[0m  [64/84], [94mLoss[0m : 9.91567
[1mStep[0m  [72/84], [94mLoss[0m : 10.08071
[1mStep[0m  [80/84], [94mLoss[0m : 9.79928

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.094, [92mTest[0m: 9.840, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.71453
[1mStep[0m  [8/84], [94mLoss[0m : 9.94790
[1mStep[0m  [16/84], [94mLoss[0m : 10.21038
[1mStep[0m  [24/84], [94mLoss[0m : 10.47414
[1mStep[0m  [32/84], [94mLoss[0m : 10.06152
[1mStep[0m  [40/84], [94mLoss[0m : 9.97382
[1mStep[0m  [48/84], [94mLoss[0m : 10.18481
[1mStep[0m  [56/84], [94mLoss[0m : 10.37837
[1mStep[0m  [64/84], [94mLoss[0m : 9.91934
[1mStep[0m  [72/84], [94mLoss[0m : 9.62685
[1mStep[0m  [80/84], [94mLoss[0m : 10.27485

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.026, [92mTest[0m: 9.721, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.93955
[1mStep[0m  [8/84], [94mLoss[0m : 9.69107
[1mStep[0m  [16/84], [94mLoss[0m : 10.30776
[1mStep[0m  [24/84], [94mLoss[0m : 9.50901
[1mStep[0m  [32/84], [94mLoss[0m : 10.51339
[1mStep[0m  [40/84], [94mLoss[0m : 9.45352
[1mStep[0m  [48/84], [94mLoss[0m : 9.44507
[1mStep[0m  [56/84], [94mLoss[0m : 10.03971
[1mStep[0m  [64/84], [94mLoss[0m : 10.09672
[1mStep[0m  [72/84], [94mLoss[0m : 9.82243
[1mStep[0m  [80/84], [94mLoss[0m : 9.52746

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.942, [92mTest[0m: 9.742, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.66106
[1mStep[0m  [8/84], [94mLoss[0m : 9.49389
[1mStep[0m  [16/84], [94mLoss[0m : 9.89369
[1mStep[0m  [24/84], [94mLoss[0m : 9.47380
[1mStep[0m  [32/84], [94mLoss[0m : 9.71768
[1mStep[0m  [40/84], [94mLoss[0m : 9.07242
[1mStep[0m  [48/84], [94mLoss[0m : 9.90165
[1mStep[0m  [56/84], [94mLoss[0m : 9.61143
[1mStep[0m  [64/84], [94mLoss[0m : 10.17865
[1mStep[0m  [72/84], [94mLoss[0m : 10.21670
[1mStep[0m  [80/84], [94mLoss[0m : 9.88686

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.862, [92mTest[0m: 9.689, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.83879
[1mStep[0m  [8/84], [94mLoss[0m : 9.22924
[1mStep[0m  [16/84], [94mLoss[0m : 9.61677
[1mStep[0m  [24/84], [94mLoss[0m : 9.27137
[1mStep[0m  [32/84], [94mLoss[0m : 9.76586
[1mStep[0m  [40/84], [94mLoss[0m : 9.99529
[1mStep[0m  [48/84], [94mLoss[0m : 9.92301
[1mStep[0m  [56/84], [94mLoss[0m : 9.24457
[1mStep[0m  [64/84], [94mLoss[0m : 9.61080
[1mStep[0m  [72/84], [94mLoss[0m : 9.77617
[1mStep[0m  [80/84], [94mLoss[0m : 9.52011

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.783, [92mTest[0m: 9.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.87350
[1mStep[0m  [8/84], [94mLoss[0m : 9.61501
[1mStep[0m  [16/84], [94mLoss[0m : 9.93423
[1mStep[0m  [24/84], [94mLoss[0m : 9.89637
[1mStep[0m  [32/84], [94mLoss[0m : 9.53761
[1mStep[0m  [40/84], [94mLoss[0m : 10.22426
[1mStep[0m  [48/84], [94mLoss[0m : 9.54542
[1mStep[0m  [56/84], [94mLoss[0m : 9.23480
[1mStep[0m  [64/84], [94mLoss[0m : 9.44546
[1mStep[0m  [72/84], [94mLoss[0m : 9.67543
[1mStep[0m  [80/84], [94mLoss[0m : 9.74810

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.716, [92mTest[0m: 9.577, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.58517
[1mStep[0m  [8/84], [94mLoss[0m : 10.55685
[1mStep[0m  [16/84], [94mLoss[0m : 9.80200
[1mStep[0m  [24/84], [94mLoss[0m : 9.51894
[1mStep[0m  [32/84], [94mLoss[0m : 9.65870
[1mStep[0m  [40/84], [94mLoss[0m : 9.13409
[1mStep[0m  [48/84], [94mLoss[0m : 8.93267
[1mStep[0m  [56/84], [94mLoss[0m : 10.06130
[1mStep[0m  [64/84], [94mLoss[0m : 9.13496
[1mStep[0m  [72/84], [94mLoss[0m : 9.74761
[1mStep[0m  [80/84], [94mLoss[0m : 8.84219

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.642, [92mTest[0m: 9.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.88581
[1mStep[0m  [8/84], [94mLoss[0m : 9.72565
[1mStep[0m  [16/84], [94mLoss[0m : 9.63429
[1mStep[0m  [24/84], [94mLoss[0m : 9.61451
[1mStep[0m  [32/84], [94mLoss[0m : 9.46503
[1mStep[0m  [40/84], [94mLoss[0m : 9.24275
[1mStep[0m  [48/84], [94mLoss[0m : 9.90784
[1mStep[0m  [56/84], [94mLoss[0m : 9.49900
[1mStep[0m  [64/84], [94mLoss[0m : 9.47897
[1mStep[0m  [72/84], [94mLoss[0m : 9.75685
[1mStep[0m  [80/84], [94mLoss[0m : 9.53610

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.585, [92mTest[0m: 9.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.71232
[1mStep[0m  [8/84], [94mLoss[0m : 9.60384
[1mStep[0m  [16/84], [94mLoss[0m : 9.00541
[1mStep[0m  [24/84], [94mLoss[0m : 9.51266
[1mStep[0m  [32/84], [94mLoss[0m : 9.53016
[1mStep[0m  [40/84], [94mLoss[0m : 9.22238
[1mStep[0m  [48/84], [94mLoss[0m : 9.57439
[1mStep[0m  [56/84], [94mLoss[0m : 9.48919
[1mStep[0m  [64/84], [94mLoss[0m : 10.01998
[1mStep[0m  [72/84], [94mLoss[0m : 10.16557
[1mStep[0m  [80/84], [94mLoss[0m : 9.53857

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.519, [92mTest[0m: 9.334, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.79293
[1mStep[0m  [8/84], [94mLoss[0m : 9.82253
[1mStep[0m  [16/84], [94mLoss[0m : 9.60688
[1mStep[0m  [24/84], [94mLoss[0m : 9.46578
[1mStep[0m  [32/84], [94mLoss[0m : 9.60995
[1mStep[0m  [40/84], [94mLoss[0m : 9.47813
[1mStep[0m  [48/84], [94mLoss[0m : 9.83516
[1mStep[0m  [56/84], [94mLoss[0m : 9.42136
[1mStep[0m  [64/84], [94mLoss[0m : 9.27415
[1mStep[0m  [72/84], [94mLoss[0m : 9.94903
[1mStep[0m  [80/84], [94mLoss[0m : 9.24929

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.451, [92mTest[0m: 9.201, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.25673
[1mStep[0m  [8/84], [94mLoss[0m : 9.87357
[1mStep[0m  [16/84], [94mLoss[0m : 9.57082
[1mStep[0m  [24/84], [94mLoss[0m : 9.88393
[1mStep[0m  [32/84], [94mLoss[0m : 9.41725
[1mStep[0m  [40/84], [94mLoss[0m : 9.56893
[1mStep[0m  [48/84], [94mLoss[0m : 9.55651
[1mStep[0m  [56/84], [94mLoss[0m : 9.16366
[1mStep[0m  [64/84], [94mLoss[0m : 9.08869
[1mStep[0m  [72/84], [94mLoss[0m : 9.09112
[1mStep[0m  [80/84], [94mLoss[0m : 9.04657

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.376, [92mTest[0m: 9.235, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.98103
[1mStep[0m  [8/84], [94mLoss[0m : 9.14477
[1mStep[0m  [16/84], [94mLoss[0m : 9.57472
[1mStep[0m  [24/84], [94mLoss[0m : 9.97948
[1mStep[0m  [32/84], [94mLoss[0m : 9.64651
[1mStep[0m  [40/84], [94mLoss[0m : 8.75961
[1mStep[0m  [48/84], [94mLoss[0m : 10.00097
[1mStep[0m  [56/84], [94mLoss[0m : 9.12324
[1mStep[0m  [64/84], [94mLoss[0m : 9.06541
[1mStep[0m  [72/84], [94mLoss[0m : 9.08956
[1mStep[0m  [80/84], [94mLoss[0m : 9.32472

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.312, [92mTest[0m: 9.094, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.20027
[1mStep[0m  [8/84], [94mLoss[0m : 9.34008
[1mStep[0m  [16/84], [94mLoss[0m : 8.82664
[1mStep[0m  [24/84], [94mLoss[0m : 8.67568
[1mStep[0m  [32/84], [94mLoss[0m : 9.37366
[1mStep[0m  [40/84], [94mLoss[0m : 9.46757
[1mStep[0m  [48/84], [94mLoss[0m : 8.78060
[1mStep[0m  [56/84], [94mLoss[0m : 9.43123
[1mStep[0m  [64/84], [94mLoss[0m : 9.14188
[1mStep[0m  [72/84], [94mLoss[0m : 8.50207
[1mStep[0m  [80/84], [94mLoss[0m : 9.34659

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.227, [92mTest[0m: 9.033, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.09709
[1mStep[0m  [8/84], [94mLoss[0m : 9.12459
[1mStep[0m  [16/84], [94mLoss[0m : 9.18799
[1mStep[0m  [24/84], [94mLoss[0m : 9.03846
[1mStep[0m  [32/84], [94mLoss[0m : 9.20595
[1mStep[0m  [40/84], [94mLoss[0m : 9.75049
[1mStep[0m  [48/84], [94mLoss[0m : 9.22762
[1mStep[0m  [56/84], [94mLoss[0m : 9.27950
[1mStep[0m  [64/84], [94mLoss[0m : 9.03106
[1mStep[0m  [72/84], [94mLoss[0m : 8.99236
[1mStep[0m  [80/84], [94mLoss[0m : 9.52449

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.154, [92mTest[0m: 8.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.58837
[1mStep[0m  [8/84], [94mLoss[0m : 8.97283
[1mStep[0m  [16/84], [94mLoss[0m : 9.33003
[1mStep[0m  [24/84], [94mLoss[0m : 9.08090
[1mStep[0m  [32/84], [94mLoss[0m : 8.98227
[1mStep[0m  [40/84], [94mLoss[0m : 9.11528
[1mStep[0m  [48/84], [94mLoss[0m : 9.31885
[1mStep[0m  [56/84], [94mLoss[0m : 9.23329
[1mStep[0m  [64/84], [94mLoss[0m : 9.30768
[1mStep[0m  [72/84], [94mLoss[0m : 9.21072
[1mStep[0m  [80/84], [94mLoss[0m : 9.31720

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.087, [92mTest[0m: 8.824, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.67048
[1mStep[0m  [8/84], [94mLoss[0m : 9.41710
[1mStep[0m  [16/84], [94mLoss[0m : 9.13248
[1mStep[0m  [24/84], [94mLoss[0m : 8.20933
[1mStep[0m  [32/84], [94mLoss[0m : 8.77912
[1mStep[0m  [40/84], [94mLoss[0m : 9.31856
[1mStep[0m  [48/84], [94mLoss[0m : 9.03744
[1mStep[0m  [56/84], [94mLoss[0m : 9.51923
[1mStep[0m  [64/84], [94mLoss[0m : 9.15325
[1mStep[0m  [72/84], [94mLoss[0m : 8.87628
[1mStep[0m  [80/84], [94mLoss[0m : 9.19555

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.014, [92mTest[0m: 8.897, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.28725
[1mStep[0m  [8/84], [94mLoss[0m : 8.99020
[1mStep[0m  [16/84], [94mLoss[0m : 9.20734
[1mStep[0m  [24/84], [94mLoss[0m : 8.67783
[1mStep[0m  [32/84], [94mLoss[0m : 8.69296
[1mStep[0m  [40/84], [94mLoss[0m : 9.02332
[1mStep[0m  [48/84], [94mLoss[0m : 9.17585
[1mStep[0m  [56/84], [94mLoss[0m : 8.86208
[1mStep[0m  [64/84], [94mLoss[0m : 8.80510
[1mStep[0m  [72/84], [94mLoss[0m : 8.96437
[1mStep[0m  [80/84], [94mLoss[0m : 8.92665

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.923, [92mTest[0m: 8.566, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.52269
[1mStep[0m  [8/84], [94mLoss[0m : 9.06160
[1mStep[0m  [16/84], [94mLoss[0m : 8.70544
[1mStep[0m  [24/84], [94mLoss[0m : 8.65493
[1mStep[0m  [32/84], [94mLoss[0m : 9.45521
[1mStep[0m  [40/84], [94mLoss[0m : 8.56174
[1mStep[0m  [48/84], [94mLoss[0m : 8.12238
[1mStep[0m  [56/84], [94mLoss[0m : 9.25060
[1mStep[0m  [64/84], [94mLoss[0m : 8.91743
[1mStep[0m  [72/84], [94mLoss[0m : 8.72589
[1mStep[0m  [80/84], [94mLoss[0m : 9.21671

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.848, [92mTest[0m: 8.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.53601
[1mStep[0m  [8/84], [94mLoss[0m : 8.62373
[1mStep[0m  [16/84], [94mLoss[0m : 8.82144
[1mStep[0m  [24/84], [94mLoss[0m : 8.56374
[1mStep[0m  [32/84], [94mLoss[0m : 8.67734
[1mStep[0m  [40/84], [94mLoss[0m : 8.83379
[1mStep[0m  [48/84], [94mLoss[0m : 8.75750
[1mStep[0m  [56/84], [94mLoss[0m : 8.57213
[1mStep[0m  [64/84], [94mLoss[0m : 9.30393
[1mStep[0m  [72/84], [94mLoss[0m : 9.00592
[1mStep[0m  [80/84], [94mLoss[0m : 8.83308

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.776, [92mTest[0m: 8.589, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.10594
[1mStep[0m  [8/84], [94mLoss[0m : 8.66622
[1mStep[0m  [16/84], [94mLoss[0m : 8.33504
[1mStep[0m  [24/84], [94mLoss[0m : 8.72697
[1mStep[0m  [32/84], [94mLoss[0m : 8.88979
[1mStep[0m  [40/84], [94mLoss[0m : 8.72322
[1mStep[0m  [48/84], [94mLoss[0m : 8.82493
[1mStep[0m  [56/84], [94mLoss[0m : 8.11258
[1mStep[0m  [64/84], [94mLoss[0m : 8.91511
[1mStep[0m  [72/84], [94mLoss[0m : 8.45159
[1mStep[0m  [80/84], [94mLoss[0m : 8.29499

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.690, [92mTest[0m: 8.393, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.70880
[1mStep[0m  [8/84], [94mLoss[0m : 8.76225
[1mStep[0m  [16/84], [94mLoss[0m : 9.39437
[1mStep[0m  [24/84], [94mLoss[0m : 8.45304
[1mStep[0m  [32/84], [94mLoss[0m : 8.55502
[1mStep[0m  [40/84], [94mLoss[0m : 8.24401
[1mStep[0m  [48/84], [94mLoss[0m : 8.17428
[1mStep[0m  [56/84], [94mLoss[0m : 8.37858
[1mStep[0m  [64/84], [94mLoss[0m : 7.93149
[1mStep[0m  [72/84], [94mLoss[0m : 8.56638
[1mStep[0m  [80/84], [94mLoss[0m : 9.20589

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.610, [92mTest[0m: 8.279, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.11078
[1mStep[0m  [8/84], [94mLoss[0m : 8.65815
[1mStep[0m  [16/84], [94mLoss[0m : 8.92666
[1mStep[0m  [24/84], [94mLoss[0m : 8.04433
[1mStep[0m  [32/84], [94mLoss[0m : 9.11466
[1mStep[0m  [40/84], [94mLoss[0m : 8.67999
[1mStep[0m  [48/84], [94mLoss[0m : 8.56102
[1mStep[0m  [56/84], [94mLoss[0m : 8.75202
[1mStep[0m  [64/84], [94mLoss[0m : 8.43036
[1mStep[0m  [72/84], [94mLoss[0m : 8.70043
[1mStep[0m  [80/84], [94mLoss[0m : 8.79298

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.533, [92mTest[0m: 8.112, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.54191
[1mStep[0m  [8/84], [94mLoss[0m : 7.69252
[1mStep[0m  [16/84], [94mLoss[0m : 8.61806
[1mStep[0m  [24/84], [94mLoss[0m : 8.51903
[1mStep[0m  [32/84], [94mLoss[0m : 8.29195
[1mStep[0m  [40/84], [94mLoss[0m : 8.36114
[1mStep[0m  [48/84], [94mLoss[0m : 7.85593
[1mStep[0m  [56/84], [94mLoss[0m : 8.68777
[1mStep[0m  [64/84], [94mLoss[0m : 8.42137
[1mStep[0m  [72/84], [94mLoss[0m : 8.36980
[1mStep[0m  [80/84], [94mLoss[0m : 8.29258

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.466, [92mTest[0m: 8.158, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.33684
[1mStep[0m  [8/84], [94mLoss[0m : 8.33909
[1mStep[0m  [16/84], [94mLoss[0m : 7.95647
[1mStep[0m  [24/84], [94mLoss[0m : 7.93147
[1mStep[0m  [32/84], [94mLoss[0m : 8.51423
[1mStep[0m  [40/84], [94mLoss[0m : 8.93199
[1mStep[0m  [48/84], [94mLoss[0m : 8.84049
[1mStep[0m  [56/84], [94mLoss[0m : 8.88041
[1mStep[0m  [64/84], [94mLoss[0m : 8.28702
[1mStep[0m  [72/84], [94mLoss[0m : 8.10699
[1mStep[0m  [80/84], [94mLoss[0m : 8.56440

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.393, [92mTest[0m: 8.061, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.51968
[1mStep[0m  [8/84], [94mLoss[0m : 8.53122
[1mStep[0m  [16/84], [94mLoss[0m : 8.10940
[1mStep[0m  [24/84], [94mLoss[0m : 8.52142
[1mStep[0m  [32/84], [94mLoss[0m : 8.35300
[1mStep[0m  [40/84], [94mLoss[0m : 8.24848
[1mStep[0m  [48/84], [94mLoss[0m : 9.00504
[1mStep[0m  [56/84], [94mLoss[0m : 8.16973
[1mStep[0m  [64/84], [94mLoss[0m : 8.02540
[1mStep[0m  [72/84], [94mLoss[0m : 8.04426
[1mStep[0m  [80/84], [94mLoss[0m : 8.47594

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.326, [92mTest[0m: 8.101, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.65324
[1mStep[0m  [8/84], [94mLoss[0m : 8.58258
[1mStep[0m  [16/84], [94mLoss[0m : 8.13606
[1mStep[0m  [24/84], [94mLoss[0m : 7.92115
[1mStep[0m  [32/84], [94mLoss[0m : 8.07825
[1mStep[0m  [40/84], [94mLoss[0m : 8.45771
[1mStep[0m  [48/84], [94mLoss[0m : 7.67553
[1mStep[0m  [56/84], [94mLoss[0m : 8.37269
[1mStep[0m  [64/84], [94mLoss[0m : 8.26956
[1mStep[0m  [72/84], [94mLoss[0m : 8.55475
[1mStep[0m  [80/84], [94mLoss[0m : 9.08230

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.259, [92mTest[0m: 7.913, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.46109
[1mStep[0m  [8/84], [94mLoss[0m : 8.09615
[1mStep[0m  [16/84], [94mLoss[0m : 7.61901
[1mStep[0m  [24/84], [94mLoss[0m : 8.48798
[1mStep[0m  [32/84], [94mLoss[0m : 8.34659
[1mStep[0m  [40/84], [94mLoss[0m : 8.17581
[1mStep[0m  [48/84], [94mLoss[0m : 7.79582
[1mStep[0m  [56/84], [94mLoss[0m : 8.23073
[1mStep[0m  [64/84], [94mLoss[0m : 8.51420
[1mStep[0m  [72/84], [94mLoss[0m : 8.72473
[1mStep[0m  [80/84], [94mLoss[0m : 7.75598

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.192, [92mTest[0m: 7.749, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.62802
[1mStep[0m  [8/84], [94mLoss[0m : 8.32151
[1mStep[0m  [16/84], [94mLoss[0m : 7.63306
[1mStep[0m  [24/84], [94mLoss[0m : 7.72812
[1mStep[0m  [32/84], [94mLoss[0m : 8.17387
[1mStep[0m  [40/84], [94mLoss[0m : 8.16606
[1mStep[0m  [48/84], [94mLoss[0m : 8.50339
[1mStep[0m  [56/84], [94mLoss[0m : 8.33240
[1mStep[0m  [64/84], [94mLoss[0m : 7.61563
[1mStep[0m  [72/84], [94mLoss[0m : 7.77153
[1mStep[0m  [80/84], [94mLoss[0m : 7.65954

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.115, [92mTest[0m: 7.603, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.36479
[1mStep[0m  [8/84], [94mLoss[0m : 8.20272
[1mStep[0m  [16/84], [94mLoss[0m : 7.87850
[1mStep[0m  [24/84], [94mLoss[0m : 7.72167
[1mStep[0m  [32/84], [94mLoss[0m : 7.66827
[1mStep[0m  [40/84], [94mLoss[0m : 8.39593
[1mStep[0m  [48/84], [94mLoss[0m : 7.85743
[1mStep[0m  [56/84], [94mLoss[0m : 7.71509
[1mStep[0m  [64/84], [94mLoss[0m : 8.37809
[1mStep[0m  [72/84], [94mLoss[0m : 7.69560
[1mStep[0m  [80/84], [94mLoss[0m : 8.35836

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.041, [92mTest[0m: 7.616, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.12189
[1mStep[0m  [8/84], [94mLoss[0m : 7.64917
[1mStep[0m  [16/84], [94mLoss[0m : 8.29396
[1mStep[0m  [24/84], [94mLoss[0m : 8.35296
[1mStep[0m  [32/84], [94mLoss[0m : 7.62958
[1mStep[0m  [40/84], [94mLoss[0m : 7.73432
[1mStep[0m  [48/84], [94mLoss[0m : 7.70355
[1mStep[0m  [56/84], [94mLoss[0m : 8.32262
[1mStep[0m  [64/84], [94mLoss[0m : 7.92740
[1mStep[0m  [72/84], [94mLoss[0m : 8.13119
[1mStep[0m  [80/84], [94mLoss[0m : 8.31100

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.977, [92mTest[0m: 7.405, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.511
====================================

Phase 2 - Evaluation MAE:  7.511371425219944
MAE score P1      9.825995
MAE score P2      7.511371
loss              7.976609
learning_rate       0.0001
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 11.06158
[1mStep[0m  [8/84], [94mLoss[0m : 11.18894
[1mStep[0m  [16/84], [94mLoss[0m : 10.69128
[1mStep[0m  [24/84], [94mLoss[0m : 10.75849
[1mStep[0m  [32/84], [94mLoss[0m : 10.90385
[1mStep[0m  [40/84], [94mLoss[0m : 10.46110
[1mStep[0m  [48/84], [94mLoss[0m : 9.95807
[1mStep[0m  [56/84], [94mLoss[0m : 10.29493
[1mStep[0m  [64/84], [94mLoss[0m : 10.74050
[1mStep[0m  [72/84], [94mLoss[0m : 10.78018
[1mStep[0m  [80/84], [94mLoss[0m : 10.05354

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.596, [92mTest[0m: 10.863, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.23825
[1mStep[0m  [8/84], [94mLoss[0m : 10.66520
[1mStep[0m  [16/84], [94mLoss[0m : 10.60891
[1mStep[0m  [24/84], [94mLoss[0m : 10.95475
[1mStep[0m  [32/84], [94mLoss[0m : 10.23615
[1mStep[0m  [40/84], [94mLoss[0m : 10.06854
[1mStep[0m  [48/84], [94mLoss[0m : 10.29733
[1mStep[0m  [56/84], [94mLoss[0m : 10.49976
[1mStep[0m  [64/84], [94mLoss[0m : 9.91388
[1mStep[0m  [72/84], [94mLoss[0m : 9.71404
[1mStep[0m  [80/84], [94mLoss[0m : 9.94411

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.329, [92mTest[0m: 10.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.13171
[1mStep[0m  [8/84], [94mLoss[0m : 9.70636
[1mStep[0m  [16/84], [94mLoss[0m : 10.40419
[1mStep[0m  [24/84], [94mLoss[0m : 9.58682
[1mStep[0m  [32/84], [94mLoss[0m : 10.55538
[1mStep[0m  [40/84], [94mLoss[0m : 10.07996
[1mStep[0m  [48/84], [94mLoss[0m : 10.40252
[1mStep[0m  [56/84], [94mLoss[0m : 9.93578
[1mStep[0m  [64/84], [94mLoss[0m : 9.92722
[1mStep[0m  [72/84], [94mLoss[0m : 9.95702
[1mStep[0m  [80/84], [94mLoss[0m : 10.17400

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.046, [92mTest[0m: 10.355, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.00593
[1mStep[0m  [8/84], [94mLoss[0m : 9.28724
[1mStep[0m  [16/84], [94mLoss[0m : 9.95433
[1mStep[0m  [24/84], [94mLoss[0m : 9.91664
[1mStep[0m  [32/84], [94mLoss[0m : 10.10658
[1mStep[0m  [40/84], [94mLoss[0m : 9.68864
[1mStep[0m  [48/84], [94mLoss[0m : 10.00494
[1mStep[0m  [56/84], [94mLoss[0m : 9.55555
[1mStep[0m  [64/84], [94mLoss[0m : 9.80070
[1mStep[0m  [72/84], [94mLoss[0m : 9.20062
[1mStep[0m  [80/84], [94mLoss[0m : 9.81616

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.774, [92mTest[0m: 10.131, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.84542
[1mStep[0m  [8/84], [94mLoss[0m : 8.85690
[1mStep[0m  [16/84], [94mLoss[0m : 9.95574
[1mStep[0m  [24/84], [94mLoss[0m : 9.65258
[1mStep[0m  [32/84], [94mLoss[0m : 9.74651
[1mStep[0m  [40/84], [94mLoss[0m : 9.45396
[1mStep[0m  [48/84], [94mLoss[0m : 9.84333
[1mStep[0m  [56/84], [94mLoss[0m : 9.11714
[1mStep[0m  [64/84], [94mLoss[0m : 9.26656
[1mStep[0m  [72/84], [94mLoss[0m : 9.26432
[1mStep[0m  [80/84], [94mLoss[0m : 9.47735

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.516, [92mTest[0m: 9.922, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.20408
[1mStep[0m  [8/84], [94mLoss[0m : 9.50727
[1mStep[0m  [16/84], [94mLoss[0m : 9.24277
[1mStep[0m  [24/84], [94mLoss[0m : 8.73284
[1mStep[0m  [32/84], [94mLoss[0m : 9.04923
[1mStep[0m  [40/84], [94mLoss[0m : 8.70590
[1mStep[0m  [48/84], [94mLoss[0m : 9.28094
[1mStep[0m  [56/84], [94mLoss[0m : 9.12760
[1mStep[0m  [64/84], [94mLoss[0m : 8.76568
[1mStep[0m  [72/84], [94mLoss[0m : 9.55415
[1mStep[0m  [80/84], [94mLoss[0m : 8.97479

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.232, [92mTest[0m: 9.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.64054
[1mStep[0m  [8/84], [94mLoss[0m : 8.76218
[1mStep[0m  [16/84], [94mLoss[0m : 9.08187
[1mStep[0m  [24/84], [94mLoss[0m : 8.76042
[1mStep[0m  [32/84], [94mLoss[0m : 9.17323
[1mStep[0m  [40/84], [94mLoss[0m : 8.60842
[1mStep[0m  [48/84], [94mLoss[0m : 8.93637
[1mStep[0m  [56/84], [94mLoss[0m : 8.92611
[1mStep[0m  [64/84], [94mLoss[0m : 8.92437
[1mStep[0m  [72/84], [94mLoss[0m : 8.51454
[1mStep[0m  [80/84], [94mLoss[0m : 8.68999

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.961, [92mTest[0m: 9.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.72080
[1mStep[0m  [8/84], [94mLoss[0m : 8.91278
[1mStep[0m  [16/84], [94mLoss[0m : 9.15565
[1mStep[0m  [24/84], [94mLoss[0m : 8.46839
[1mStep[0m  [32/84], [94mLoss[0m : 8.52036
[1mStep[0m  [40/84], [94mLoss[0m : 9.01717
[1mStep[0m  [48/84], [94mLoss[0m : 9.31681
[1mStep[0m  [56/84], [94mLoss[0m : 8.45137
[1mStep[0m  [64/84], [94mLoss[0m : 9.31458
[1mStep[0m  [72/84], [94mLoss[0m : 8.88233
[1mStep[0m  [80/84], [94mLoss[0m : 8.70072

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.680, [92mTest[0m: 9.261, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.27142
[1mStep[0m  [8/84], [94mLoss[0m : 8.50241
[1mStep[0m  [16/84], [94mLoss[0m : 8.57784
[1mStep[0m  [24/84], [94mLoss[0m : 8.57784
[1mStep[0m  [32/84], [94mLoss[0m : 8.63942
[1mStep[0m  [40/84], [94mLoss[0m : 8.23403
[1mStep[0m  [48/84], [94mLoss[0m : 8.15007
[1mStep[0m  [56/84], [94mLoss[0m : 8.57073
[1mStep[0m  [64/84], [94mLoss[0m : 7.83644
[1mStep[0m  [72/84], [94mLoss[0m : 8.47468
[1mStep[0m  [80/84], [94mLoss[0m : 8.45356

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.430, [92mTest[0m: 9.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.34455
[1mStep[0m  [8/84], [94mLoss[0m : 8.29804
[1mStep[0m  [16/84], [94mLoss[0m : 8.71542
[1mStep[0m  [24/84], [94mLoss[0m : 8.28585
[1mStep[0m  [32/84], [94mLoss[0m : 8.24353
[1mStep[0m  [40/84], [94mLoss[0m : 8.38493
[1mStep[0m  [48/84], [94mLoss[0m : 7.94540
[1mStep[0m  [56/84], [94mLoss[0m : 8.15481
[1mStep[0m  [64/84], [94mLoss[0m : 8.40699
[1mStep[0m  [72/84], [94mLoss[0m : 8.25347
[1mStep[0m  [80/84], [94mLoss[0m : 7.50552

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.136, [92mTest[0m: 8.778, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.72806
[1mStep[0m  [8/84], [94mLoss[0m : 7.62378
[1mStep[0m  [16/84], [94mLoss[0m : 8.18366
[1mStep[0m  [24/84], [94mLoss[0m : 7.94455
[1mStep[0m  [32/84], [94mLoss[0m : 8.00062
[1mStep[0m  [40/84], [94mLoss[0m : 7.79289
[1mStep[0m  [48/84], [94mLoss[0m : 7.50611
[1mStep[0m  [56/84], [94mLoss[0m : 7.42740
[1mStep[0m  [64/84], [94mLoss[0m : 7.89899
[1mStep[0m  [72/84], [94mLoss[0m : 7.90040
[1mStep[0m  [80/84], [94mLoss[0m : 8.33620

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.870, [92mTest[0m: 8.582, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.02683
[1mStep[0m  [8/84], [94mLoss[0m : 7.69571
[1mStep[0m  [16/84], [94mLoss[0m : 7.25722
[1mStep[0m  [24/84], [94mLoss[0m : 7.55359
[1mStep[0m  [32/84], [94mLoss[0m : 8.06266
[1mStep[0m  [40/84], [94mLoss[0m : 7.44353
[1mStep[0m  [48/84], [94mLoss[0m : 7.25662
[1mStep[0m  [56/84], [94mLoss[0m : 7.43413
[1mStep[0m  [64/84], [94mLoss[0m : 7.33725
[1mStep[0m  [72/84], [94mLoss[0m : 7.01481
[1mStep[0m  [80/84], [94mLoss[0m : 7.81850

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.593, [92mTest[0m: 8.350, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.40260
[1mStep[0m  [8/84], [94mLoss[0m : 7.69346
[1mStep[0m  [16/84], [94mLoss[0m : 7.32437
[1mStep[0m  [24/84], [94mLoss[0m : 8.05099
[1mStep[0m  [32/84], [94mLoss[0m : 7.91342
[1mStep[0m  [40/84], [94mLoss[0m : 6.88798
[1mStep[0m  [48/84], [94mLoss[0m : 7.20473
[1mStep[0m  [56/84], [94mLoss[0m : 7.89355
[1mStep[0m  [64/84], [94mLoss[0m : 7.17199
[1mStep[0m  [72/84], [94mLoss[0m : 7.42148
[1mStep[0m  [80/84], [94mLoss[0m : 7.27575

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.347, [92mTest[0m: 8.121, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.77460
[1mStep[0m  [8/84], [94mLoss[0m : 7.06707
[1mStep[0m  [16/84], [94mLoss[0m : 7.72688
[1mStep[0m  [24/84], [94mLoss[0m : 7.76204
[1mStep[0m  [32/84], [94mLoss[0m : 7.11294
[1mStep[0m  [40/84], [94mLoss[0m : 7.06368
[1mStep[0m  [48/84], [94mLoss[0m : 6.94005
[1mStep[0m  [56/84], [94mLoss[0m : 7.66294
[1mStep[0m  [64/84], [94mLoss[0m : 6.87915
[1mStep[0m  [72/84], [94mLoss[0m : 7.10412
[1mStep[0m  [80/84], [94mLoss[0m : 6.82284

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.073, [92mTest[0m: 7.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.98744
[1mStep[0m  [8/84], [94mLoss[0m : 7.19485
[1mStep[0m  [16/84], [94mLoss[0m : 6.85109
[1mStep[0m  [24/84], [94mLoss[0m : 6.64866
[1mStep[0m  [32/84], [94mLoss[0m : 7.34638
[1mStep[0m  [40/84], [94mLoss[0m : 6.87045
[1mStep[0m  [48/84], [94mLoss[0m : 7.15573
[1mStep[0m  [56/84], [94mLoss[0m : 6.52863
[1mStep[0m  [64/84], [94mLoss[0m : 6.13294
[1mStep[0m  [72/84], [94mLoss[0m : 6.99302
[1mStep[0m  [80/84], [94mLoss[0m : 7.28514

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.824, [92mTest[0m: 7.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.84342
[1mStep[0m  [8/84], [94mLoss[0m : 6.64338
[1mStep[0m  [16/84], [94mLoss[0m : 6.77209
[1mStep[0m  [24/84], [94mLoss[0m : 6.71965
[1mStep[0m  [32/84], [94mLoss[0m : 6.48337
[1mStep[0m  [40/84], [94mLoss[0m : 6.29411
[1mStep[0m  [48/84], [94mLoss[0m : 7.09620
[1mStep[0m  [56/84], [94mLoss[0m : 5.94152
[1mStep[0m  [64/84], [94mLoss[0m : 6.12374
[1mStep[0m  [72/84], [94mLoss[0m : 5.74337
[1mStep[0m  [80/84], [94mLoss[0m : 7.18081

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.576, [92mTest[0m: 7.448, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.27660
[1mStep[0m  [8/84], [94mLoss[0m : 6.77847
[1mStep[0m  [16/84], [94mLoss[0m : 5.73035
[1mStep[0m  [24/84], [94mLoss[0m : 6.76157
[1mStep[0m  [32/84], [94mLoss[0m : 6.73896
[1mStep[0m  [40/84], [94mLoss[0m : 6.11265
[1mStep[0m  [48/84], [94mLoss[0m : 6.90001
[1mStep[0m  [56/84], [94mLoss[0m : 5.93886
[1mStep[0m  [64/84], [94mLoss[0m : 6.15762
[1mStep[0m  [72/84], [94mLoss[0m : 6.08788
[1mStep[0m  [80/84], [94mLoss[0m : 6.06359

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.338, [92mTest[0m: 7.165, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.99314
[1mStep[0m  [8/84], [94mLoss[0m : 6.06343
[1mStep[0m  [16/84], [94mLoss[0m : 6.33162
[1mStep[0m  [24/84], [94mLoss[0m : 6.30914
[1mStep[0m  [32/84], [94mLoss[0m : 6.36296
[1mStep[0m  [40/84], [94mLoss[0m : 6.52687
[1mStep[0m  [48/84], [94mLoss[0m : 6.15303
[1mStep[0m  [56/84], [94mLoss[0m : 6.37353
[1mStep[0m  [64/84], [94mLoss[0m : 5.82333
[1mStep[0m  [72/84], [94mLoss[0m : 6.09165
[1mStep[0m  [80/84], [94mLoss[0m : 5.24394

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.099, [92mTest[0m: 6.956, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.10493
[1mStep[0m  [8/84], [94mLoss[0m : 5.82674
[1mStep[0m  [16/84], [94mLoss[0m : 5.52893
[1mStep[0m  [24/84], [94mLoss[0m : 5.58573
[1mStep[0m  [32/84], [94mLoss[0m : 6.07391
[1mStep[0m  [40/84], [94mLoss[0m : 5.82943
[1mStep[0m  [48/84], [94mLoss[0m : 5.76183
[1mStep[0m  [56/84], [94mLoss[0m : 6.45920
[1mStep[0m  [64/84], [94mLoss[0m : 5.66611
[1mStep[0m  [72/84], [94mLoss[0m : 5.00657
[1mStep[0m  [80/84], [94mLoss[0m : 5.54001

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.836, [92mTest[0m: 6.704, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.25973
[1mStep[0m  [8/84], [94mLoss[0m : 5.16502
[1mStep[0m  [16/84], [94mLoss[0m : 5.25457
[1mStep[0m  [24/84], [94mLoss[0m : 5.82941
[1mStep[0m  [32/84], [94mLoss[0m : 5.91990
[1mStep[0m  [40/84], [94mLoss[0m : 5.61203
[1mStep[0m  [48/84], [94mLoss[0m : 5.47064
[1mStep[0m  [56/84], [94mLoss[0m : 5.27680
[1mStep[0m  [64/84], [94mLoss[0m : 5.35258
[1mStep[0m  [72/84], [94mLoss[0m : 5.52408
[1mStep[0m  [80/84], [94mLoss[0m : 6.12905

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.612, [92mTest[0m: 6.409, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.45954
[1mStep[0m  [8/84], [94mLoss[0m : 5.45024
[1mStep[0m  [16/84], [94mLoss[0m : 5.28272
[1mStep[0m  [24/84], [94mLoss[0m : 5.10526
[1mStep[0m  [32/84], [94mLoss[0m : 5.47206
[1mStep[0m  [40/84], [94mLoss[0m : 5.32639
[1mStep[0m  [48/84], [94mLoss[0m : 5.73675
[1mStep[0m  [56/84], [94mLoss[0m : 5.61878
[1mStep[0m  [64/84], [94mLoss[0m : 5.59991
[1mStep[0m  [72/84], [94mLoss[0m : 5.26486
[1mStep[0m  [80/84], [94mLoss[0m : 5.05980

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.376, [92mTest[0m: 6.201, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.36700
[1mStep[0m  [8/84], [94mLoss[0m : 5.37872
[1mStep[0m  [16/84], [94mLoss[0m : 5.09241
[1mStep[0m  [24/84], [94mLoss[0m : 5.24435
[1mStep[0m  [32/84], [94mLoss[0m : 5.49908
[1mStep[0m  [40/84], [94mLoss[0m : 5.66369
[1mStep[0m  [48/84], [94mLoss[0m : 5.69865
[1mStep[0m  [56/84], [94mLoss[0m : 5.00213
[1mStep[0m  [64/84], [94mLoss[0m : 5.21320
[1mStep[0m  [72/84], [94mLoss[0m : 4.60650
[1mStep[0m  [80/84], [94mLoss[0m : 5.19476

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.185, [92mTest[0m: 6.082, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.98418
[1mStep[0m  [8/84], [94mLoss[0m : 5.04922
[1mStep[0m  [16/84], [94mLoss[0m : 5.29955
[1mStep[0m  [24/84], [94mLoss[0m : 4.76939
[1mStep[0m  [32/84], [94mLoss[0m : 5.36335
[1mStep[0m  [40/84], [94mLoss[0m : 5.17843
[1mStep[0m  [48/84], [94mLoss[0m : 5.10131
[1mStep[0m  [56/84], [94mLoss[0m : 4.70666
[1mStep[0m  [64/84], [94mLoss[0m : 4.99148
[1mStep[0m  [72/84], [94mLoss[0m : 4.73460
[1mStep[0m  [80/84], [94mLoss[0m : 5.30341

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.984, [92mTest[0m: 5.804, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.24914
[1mStep[0m  [8/84], [94mLoss[0m : 5.02086
[1mStep[0m  [16/84], [94mLoss[0m : 4.82352
[1mStep[0m  [24/84], [94mLoss[0m : 4.44544
[1mStep[0m  [32/84], [94mLoss[0m : 4.54208
[1mStep[0m  [40/84], [94mLoss[0m : 5.20440
[1mStep[0m  [48/84], [94mLoss[0m : 4.94999
[1mStep[0m  [56/84], [94mLoss[0m : 5.06517
[1mStep[0m  [64/84], [94mLoss[0m : 4.62630
[1mStep[0m  [72/84], [94mLoss[0m : 4.77615
[1mStep[0m  [80/84], [94mLoss[0m : 4.98152

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.787, [92mTest[0m: 5.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.50797
[1mStep[0m  [8/84], [94mLoss[0m : 4.54347
[1mStep[0m  [16/84], [94mLoss[0m : 4.85972
[1mStep[0m  [24/84], [94mLoss[0m : 4.81284
[1mStep[0m  [32/84], [94mLoss[0m : 4.76531
[1mStep[0m  [40/84], [94mLoss[0m : 4.77341
[1mStep[0m  [48/84], [94mLoss[0m : 4.49578
[1mStep[0m  [56/84], [94mLoss[0m : 4.52069
[1mStep[0m  [64/84], [94mLoss[0m : 4.20880
[1mStep[0m  [72/84], [94mLoss[0m : 5.24603
[1mStep[0m  [80/84], [94mLoss[0m : 4.46880

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.594, [92mTest[0m: 5.407, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.70658
[1mStep[0m  [8/84], [94mLoss[0m : 4.70574
[1mStep[0m  [16/84], [94mLoss[0m : 4.86007
[1mStep[0m  [24/84], [94mLoss[0m : 4.06680
[1mStep[0m  [32/84], [94mLoss[0m : 4.65578
[1mStep[0m  [40/84], [94mLoss[0m : 4.68047
[1mStep[0m  [48/84], [94mLoss[0m : 4.83204
[1mStep[0m  [56/84], [94mLoss[0m : 4.50406
[1mStep[0m  [64/84], [94mLoss[0m : 5.29666
[1mStep[0m  [72/84], [94mLoss[0m : 4.25693
[1mStep[0m  [80/84], [94mLoss[0m : 4.20496

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.422, [92mTest[0m: 5.196, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.51351
[1mStep[0m  [8/84], [94mLoss[0m : 4.32741
[1mStep[0m  [16/84], [94mLoss[0m : 4.19827
[1mStep[0m  [24/84], [94mLoss[0m : 4.65971
[1mStep[0m  [32/84], [94mLoss[0m : 4.43610
[1mStep[0m  [40/84], [94mLoss[0m : 5.04328
[1mStep[0m  [48/84], [94mLoss[0m : 4.09698
[1mStep[0m  [56/84], [94mLoss[0m : 4.20718
[1mStep[0m  [64/84], [94mLoss[0m : 3.48937
[1mStep[0m  [72/84], [94mLoss[0m : 4.93282
[1mStep[0m  [80/84], [94mLoss[0m : 4.09458

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.280, [92mTest[0m: 5.019, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.78032
[1mStep[0m  [8/84], [94mLoss[0m : 4.06230
[1mStep[0m  [16/84], [94mLoss[0m : 4.12727
[1mStep[0m  [24/84], [94mLoss[0m : 4.61318
[1mStep[0m  [32/84], [94mLoss[0m : 4.21327
[1mStep[0m  [40/84], [94mLoss[0m : 4.47666
[1mStep[0m  [48/84], [94mLoss[0m : 4.16529
[1mStep[0m  [56/84], [94mLoss[0m : 4.42776
[1mStep[0m  [64/84], [94mLoss[0m : 4.49786
[1mStep[0m  [72/84], [94mLoss[0m : 4.63413
[1mStep[0m  [80/84], [94mLoss[0m : 3.91807

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.154, [92mTest[0m: 4.822, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.83714
[1mStep[0m  [8/84], [94mLoss[0m : 3.90204
[1mStep[0m  [16/84], [94mLoss[0m : 4.00634
[1mStep[0m  [24/84], [94mLoss[0m : 4.03198
[1mStep[0m  [32/84], [94mLoss[0m : 4.58393
[1mStep[0m  [40/84], [94mLoss[0m : 4.34843
[1mStep[0m  [48/84], [94mLoss[0m : 4.15356
[1mStep[0m  [56/84], [94mLoss[0m : 4.12707
[1mStep[0m  [64/84], [94mLoss[0m : 4.36425
[1mStep[0m  [72/84], [94mLoss[0m : 4.00538
[1mStep[0m  [80/84], [94mLoss[0m : 4.42842

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.998, [92mTest[0m: 4.629, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.02884
[1mStep[0m  [8/84], [94mLoss[0m : 3.97303
[1mStep[0m  [16/84], [94mLoss[0m : 3.71459
[1mStep[0m  [24/84], [94mLoss[0m : 3.50874
[1mStep[0m  [32/84], [94mLoss[0m : 3.95181
[1mStep[0m  [40/84], [94mLoss[0m : 4.04949
[1mStep[0m  [48/84], [94mLoss[0m : 4.24253
[1mStep[0m  [56/84], [94mLoss[0m : 3.87110
[1mStep[0m  [64/84], [94mLoss[0m : 3.96466
[1mStep[0m  [72/84], [94mLoss[0m : 3.45294
[1mStep[0m  [80/84], [94mLoss[0m : 3.94366

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.883, [92mTest[0m: 4.468, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.325
====================================

Phase 1 - Evaluation MAE:  4.325442501476833
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 4.16707
[1mStep[0m  [8/84], [94mLoss[0m : 3.96463
[1mStep[0m  [16/84], [94mLoss[0m : 3.84253
[1mStep[0m  [24/84], [94mLoss[0m : 3.84855
[1mStep[0m  [32/84], [94mLoss[0m : 3.81958
[1mStep[0m  [40/84], [94mLoss[0m : 3.91448
[1mStep[0m  [48/84], [94mLoss[0m : 4.00283
[1mStep[0m  [56/84], [94mLoss[0m : 3.73338
[1mStep[0m  [64/84], [94mLoss[0m : 3.77501
[1mStep[0m  [72/84], [94mLoss[0m : 4.07536
[1mStep[0m  [80/84], [94mLoss[0m : 3.56459

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.734, [92mTest[0m: 4.323, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.61955
[1mStep[0m  [8/84], [94mLoss[0m : 3.48936
[1mStep[0m  [16/84], [94mLoss[0m : 3.77436
[1mStep[0m  [24/84], [94mLoss[0m : 3.76321
[1mStep[0m  [32/84], [94mLoss[0m : 3.91931
[1mStep[0m  [40/84], [94mLoss[0m : 3.67046
[1mStep[0m  [48/84], [94mLoss[0m : 3.76235
[1mStep[0m  [56/84], [94mLoss[0m : 3.80789
[1mStep[0m  [64/84], [94mLoss[0m : 3.15149
[1mStep[0m  [72/84], [94mLoss[0m : 3.83760
[1mStep[0m  [80/84], [94mLoss[0m : 2.94480

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.616, [92mTest[0m: 4.022, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.29731
[1mStep[0m  [8/84], [94mLoss[0m : 3.71389
[1mStep[0m  [16/84], [94mLoss[0m : 3.61684
[1mStep[0m  [24/84], [94mLoss[0m : 3.06605
[1mStep[0m  [32/84], [94mLoss[0m : 3.62138
[1mStep[0m  [40/84], [94mLoss[0m : 3.87594
[1mStep[0m  [48/84], [94mLoss[0m : 3.05730
[1mStep[0m  [56/84], [94mLoss[0m : 3.12271
[1mStep[0m  [64/84], [94mLoss[0m : 3.63015
[1mStep[0m  [72/84], [94mLoss[0m : 3.74330
[1mStep[0m  [80/84], [94mLoss[0m : 3.31868

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.475, [92mTest[0m: 3.674, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.57332
[1mStep[0m  [8/84], [94mLoss[0m : 3.24253
[1mStep[0m  [16/84], [94mLoss[0m : 3.46325
[1mStep[0m  [24/84], [94mLoss[0m : 3.22275
[1mStep[0m  [32/84], [94mLoss[0m : 3.83383
[1mStep[0m  [40/84], [94mLoss[0m : 3.28500
[1mStep[0m  [48/84], [94mLoss[0m : 3.57669
[1mStep[0m  [56/84], [94mLoss[0m : 3.11912
[1mStep[0m  [64/84], [94mLoss[0m : 3.79722
[1mStep[0m  [72/84], [94mLoss[0m : 3.12235
[1mStep[0m  [80/84], [94mLoss[0m : 3.28692

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.387, [92mTest[0m: 3.359, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.87537
[1mStep[0m  [8/84], [94mLoss[0m : 3.22718
[1mStep[0m  [16/84], [94mLoss[0m : 3.35785
[1mStep[0m  [24/84], [94mLoss[0m : 3.55218
[1mStep[0m  [32/84], [94mLoss[0m : 3.13152
[1mStep[0m  [40/84], [94mLoss[0m : 3.33175
[1mStep[0m  [48/84], [94mLoss[0m : 3.24214
[1mStep[0m  [56/84], [94mLoss[0m : 3.86385
[1mStep[0m  [64/84], [94mLoss[0m : 3.21570
[1mStep[0m  [72/84], [94mLoss[0m : 3.36422
[1mStep[0m  [80/84], [94mLoss[0m : 3.38816

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.320, [92mTest[0m: 3.247, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.40158
[1mStep[0m  [8/84], [94mLoss[0m : 3.49616
[1mStep[0m  [16/84], [94mLoss[0m : 2.98100
[1mStep[0m  [24/84], [94mLoss[0m : 3.28668
[1mStep[0m  [32/84], [94mLoss[0m : 2.98759
[1mStep[0m  [40/84], [94mLoss[0m : 3.44483
[1mStep[0m  [48/84], [94mLoss[0m : 3.43450
[1mStep[0m  [56/84], [94mLoss[0m : 3.37278
[1mStep[0m  [64/84], [94mLoss[0m : 3.18789
[1mStep[0m  [72/84], [94mLoss[0m : 2.74000
[1mStep[0m  [80/84], [94mLoss[0m : 2.78708

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.237, [92mTest[0m: 3.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.42768
[1mStep[0m  [8/84], [94mLoss[0m : 3.25329
[1mStep[0m  [16/84], [94mLoss[0m : 3.26301
[1mStep[0m  [24/84], [94mLoss[0m : 2.93088
[1mStep[0m  [32/84], [94mLoss[0m : 3.51283
[1mStep[0m  [40/84], [94mLoss[0m : 3.19500
[1mStep[0m  [48/84], [94mLoss[0m : 2.79217
[1mStep[0m  [56/84], [94mLoss[0m : 3.31519
[1mStep[0m  [64/84], [94mLoss[0m : 3.01886
[1mStep[0m  [72/84], [94mLoss[0m : 3.05676
[1mStep[0m  [80/84], [94mLoss[0m : 3.00018

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.160, [92mTest[0m: 2.873, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.22295
[1mStep[0m  [8/84], [94mLoss[0m : 3.03451
[1mStep[0m  [16/84], [94mLoss[0m : 2.90378
[1mStep[0m  [24/84], [94mLoss[0m : 3.34258
[1mStep[0m  [32/84], [94mLoss[0m : 3.10629
[1mStep[0m  [40/84], [94mLoss[0m : 2.89522
[1mStep[0m  [48/84], [94mLoss[0m : 3.24985
[1mStep[0m  [56/84], [94mLoss[0m : 3.22822
[1mStep[0m  [64/84], [94mLoss[0m : 3.09767
[1mStep[0m  [72/84], [94mLoss[0m : 2.66497
[1mStep[0m  [80/84], [94mLoss[0m : 3.11880

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.111, [92mTest[0m: 2.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64359
[1mStep[0m  [8/84], [94mLoss[0m : 3.39526
[1mStep[0m  [16/84], [94mLoss[0m : 3.35479
[1mStep[0m  [24/84], [94mLoss[0m : 2.91419
[1mStep[0m  [32/84], [94mLoss[0m : 3.26817
[1mStep[0m  [40/84], [94mLoss[0m : 3.21304
[1mStep[0m  [48/84], [94mLoss[0m : 3.05148
[1mStep[0m  [56/84], [94mLoss[0m : 3.29246
[1mStep[0m  [64/84], [94mLoss[0m : 3.13700
[1mStep[0m  [72/84], [94mLoss[0m : 2.75310
[1mStep[0m  [80/84], [94mLoss[0m : 3.15688

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.033, [92mTest[0m: 2.703, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.00255
[1mStep[0m  [8/84], [94mLoss[0m : 2.88252
[1mStep[0m  [16/84], [94mLoss[0m : 3.11572
[1mStep[0m  [24/84], [94mLoss[0m : 2.60842
[1mStep[0m  [32/84], [94mLoss[0m : 2.96571
[1mStep[0m  [40/84], [94mLoss[0m : 3.21226
[1mStep[0m  [48/84], [94mLoss[0m : 2.51592
[1mStep[0m  [56/84], [94mLoss[0m : 3.27867
[1mStep[0m  [64/84], [94mLoss[0m : 3.02919
[1mStep[0m  [72/84], [94mLoss[0m : 2.65422
[1mStep[0m  [80/84], [94mLoss[0m : 3.17783

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.016, [92mTest[0m: 2.712, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66802
[1mStep[0m  [8/84], [94mLoss[0m : 2.83894
[1mStep[0m  [16/84], [94mLoss[0m : 3.15343
[1mStep[0m  [24/84], [94mLoss[0m : 2.98013
[1mStep[0m  [32/84], [94mLoss[0m : 3.09804
[1mStep[0m  [40/84], [94mLoss[0m : 2.31137
[1mStep[0m  [48/84], [94mLoss[0m : 2.80199
[1mStep[0m  [56/84], [94mLoss[0m : 3.06675
[1mStep[0m  [64/84], [94mLoss[0m : 3.27111
[1mStep[0m  [72/84], [94mLoss[0m : 3.03793
[1mStep[0m  [80/84], [94mLoss[0m : 3.49642

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.960, [92mTest[0m: 2.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93343
[1mStep[0m  [8/84], [94mLoss[0m : 2.68344
[1mStep[0m  [16/84], [94mLoss[0m : 3.09521
[1mStep[0m  [24/84], [94mLoss[0m : 2.94103
[1mStep[0m  [32/84], [94mLoss[0m : 3.06535
[1mStep[0m  [40/84], [94mLoss[0m : 2.56473
[1mStep[0m  [48/84], [94mLoss[0m : 3.19037
[1mStep[0m  [56/84], [94mLoss[0m : 3.24045
[1mStep[0m  [64/84], [94mLoss[0m : 2.55492
[1mStep[0m  [72/84], [94mLoss[0m : 2.79745
[1mStep[0m  [80/84], [94mLoss[0m : 2.70036

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.916, [92mTest[0m: 2.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.92117
[1mStep[0m  [8/84], [94mLoss[0m : 2.74043
[1mStep[0m  [16/84], [94mLoss[0m : 3.03913
[1mStep[0m  [24/84], [94mLoss[0m : 3.12020
[1mStep[0m  [32/84], [94mLoss[0m : 3.01538
[1mStep[0m  [40/84], [94mLoss[0m : 3.11525
[1mStep[0m  [48/84], [94mLoss[0m : 2.88163
[1mStep[0m  [56/84], [94mLoss[0m : 2.75163
[1mStep[0m  [64/84], [94mLoss[0m : 2.97282
[1mStep[0m  [72/84], [94mLoss[0m : 2.83491
[1mStep[0m  [80/84], [94mLoss[0m : 3.14655

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.908, [92mTest[0m: 2.648, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95611
[1mStep[0m  [8/84], [94mLoss[0m : 2.83508
[1mStep[0m  [16/84], [94mLoss[0m : 3.15467
[1mStep[0m  [24/84], [94mLoss[0m : 2.68597
[1mStep[0m  [32/84], [94mLoss[0m : 2.96969
[1mStep[0m  [40/84], [94mLoss[0m : 3.08354
[1mStep[0m  [48/84], [94mLoss[0m : 3.10465
[1mStep[0m  [56/84], [94mLoss[0m : 2.82405
[1mStep[0m  [64/84], [94mLoss[0m : 2.93910
[1mStep[0m  [72/84], [94mLoss[0m : 2.86128
[1mStep[0m  [80/84], [94mLoss[0m : 2.71381

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.884, [92mTest[0m: 2.688, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.94972
[1mStep[0m  [8/84], [94mLoss[0m : 3.18381
[1mStep[0m  [16/84], [94mLoss[0m : 2.86204
[1mStep[0m  [24/84], [94mLoss[0m : 2.66766
[1mStep[0m  [32/84], [94mLoss[0m : 2.64084
[1mStep[0m  [40/84], [94mLoss[0m : 2.80129
[1mStep[0m  [48/84], [94mLoss[0m : 2.84409
[1mStep[0m  [56/84], [94mLoss[0m : 3.19144
[1mStep[0m  [64/84], [94mLoss[0m : 2.67320
[1mStep[0m  [72/84], [94mLoss[0m : 2.67346
[1mStep[0m  [80/84], [94mLoss[0m : 3.16763

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.869, [92mTest[0m: 2.704, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.39573
[1mStep[0m  [8/84], [94mLoss[0m : 3.19514
[1mStep[0m  [16/84], [94mLoss[0m : 2.70139
[1mStep[0m  [24/84], [94mLoss[0m : 2.85343
[1mStep[0m  [32/84], [94mLoss[0m : 2.67409
[1mStep[0m  [40/84], [94mLoss[0m : 2.59065
[1mStep[0m  [48/84], [94mLoss[0m : 2.56389
[1mStep[0m  [56/84], [94mLoss[0m : 3.30870
[1mStep[0m  [64/84], [94mLoss[0m : 2.58442
[1mStep[0m  [72/84], [94mLoss[0m : 3.16567
[1mStep[0m  [80/84], [94mLoss[0m : 2.92029

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.855, [92mTest[0m: 2.712, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37196
[1mStep[0m  [8/84], [94mLoss[0m : 2.85555
[1mStep[0m  [16/84], [94mLoss[0m : 2.76553
[1mStep[0m  [24/84], [94mLoss[0m : 2.76287
[1mStep[0m  [32/84], [94mLoss[0m : 2.76425
[1mStep[0m  [40/84], [94mLoss[0m : 2.94863
[1mStep[0m  [48/84], [94mLoss[0m : 2.65621
[1mStep[0m  [56/84], [94mLoss[0m : 3.13868
[1mStep[0m  [64/84], [94mLoss[0m : 2.95883
[1mStep[0m  [72/84], [94mLoss[0m : 2.85044
[1mStep[0m  [80/84], [94mLoss[0m : 3.10541

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.816, [92mTest[0m: 2.649, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.07467
[1mStep[0m  [8/84], [94mLoss[0m : 2.96640
[1mStep[0m  [16/84], [94mLoss[0m : 2.96366
[1mStep[0m  [24/84], [94mLoss[0m : 2.96439
[1mStep[0m  [32/84], [94mLoss[0m : 2.63450
[1mStep[0m  [40/84], [94mLoss[0m : 2.81821
[1mStep[0m  [48/84], [94mLoss[0m : 2.99197
[1mStep[0m  [56/84], [94mLoss[0m : 2.82939
[1mStep[0m  [64/84], [94mLoss[0m : 2.63156
[1mStep[0m  [72/84], [94mLoss[0m : 2.46665
[1mStep[0m  [80/84], [94mLoss[0m : 2.45294

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.800, [92mTest[0m: 2.729, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96262
[1mStep[0m  [8/84], [94mLoss[0m : 3.04986
[1mStep[0m  [16/84], [94mLoss[0m : 2.90389
[1mStep[0m  [24/84], [94mLoss[0m : 2.68196
[1mStep[0m  [32/84], [94mLoss[0m : 2.96176
[1mStep[0m  [40/84], [94mLoss[0m : 2.67161
[1mStep[0m  [48/84], [94mLoss[0m : 3.33352
[1mStep[0m  [56/84], [94mLoss[0m : 3.10153
[1mStep[0m  [64/84], [94mLoss[0m : 3.05215
[1mStep[0m  [72/84], [94mLoss[0m : 3.08698
[1mStep[0m  [80/84], [94mLoss[0m : 2.91912

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.818, [92mTest[0m: 2.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97546
[1mStep[0m  [8/84], [94mLoss[0m : 3.16057
[1mStep[0m  [16/84], [94mLoss[0m : 2.91040
[1mStep[0m  [24/84], [94mLoss[0m : 2.81609
[1mStep[0m  [32/84], [94mLoss[0m : 2.61729
[1mStep[0m  [40/84], [94mLoss[0m : 3.16998
[1mStep[0m  [48/84], [94mLoss[0m : 2.59428
[1mStep[0m  [56/84], [94mLoss[0m : 2.52908
[1mStep[0m  [64/84], [94mLoss[0m : 2.48496
[1mStep[0m  [72/84], [94mLoss[0m : 2.60797
[1mStep[0m  [80/84], [94mLoss[0m : 2.57434

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.782, [92mTest[0m: 2.778, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.12260
[1mStep[0m  [8/84], [94mLoss[0m : 2.70225
[1mStep[0m  [16/84], [94mLoss[0m : 2.72612
[1mStep[0m  [24/84], [94mLoss[0m : 2.73206
[1mStep[0m  [32/84], [94mLoss[0m : 2.73994
[1mStep[0m  [40/84], [94mLoss[0m : 2.68846
[1mStep[0m  [48/84], [94mLoss[0m : 2.85035
[1mStep[0m  [56/84], [94mLoss[0m : 2.77873
[1mStep[0m  [64/84], [94mLoss[0m : 2.31178
[1mStep[0m  [72/84], [94mLoss[0m : 2.71658
[1mStep[0m  [80/84], [94mLoss[0m : 2.83816

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.768, [92mTest[0m: 2.853, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83172
[1mStep[0m  [8/84], [94mLoss[0m : 2.89635
[1mStep[0m  [16/84], [94mLoss[0m : 2.82809
[1mStep[0m  [24/84], [94mLoss[0m : 3.18454
[1mStep[0m  [32/84], [94mLoss[0m : 2.95865
[1mStep[0m  [40/84], [94mLoss[0m : 2.84359
[1mStep[0m  [48/84], [94mLoss[0m : 2.51424
[1mStep[0m  [56/84], [94mLoss[0m : 2.99683
[1mStep[0m  [64/84], [94mLoss[0m : 2.91524
[1mStep[0m  [72/84], [94mLoss[0m : 2.75019
[1mStep[0m  [80/84], [94mLoss[0m : 2.94328

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.763, [92mTest[0m: 2.798, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37837
[1mStep[0m  [8/84], [94mLoss[0m : 2.80756
[1mStep[0m  [16/84], [94mLoss[0m : 2.77358
[1mStep[0m  [24/84], [94mLoss[0m : 2.51480
[1mStep[0m  [32/84], [94mLoss[0m : 2.92532
[1mStep[0m  [40/84], [94mLoss[0m : 2.50310
[1mStep[0m  [48/84], [94mLoss[0m : 2.82203
[1mStep[0m  [56/84], [94mLoss[0m : 2.85356
[1mStep[0m  [64/84], [94mLoss[0m : 2.72192
[1mStep[0m  [72/84], [94mLoss[0m : 2.69443
[1mStep[0m  [80/84], [94mLoss[0m : 2.63218

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.792, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47797
[1mStep[0m  [8/84], [94mLoss[0m : 2.69810
[1mStep[0m  [16/84], [94mLoss[0m : 2.79942
[1mStep[0m  [24/84], [94mLoss[0m : 3.18264
[1mStep[0m  [32/84], [94mLoss[0m : 2.78851
[1mStep[0m  [40/84], [94mLoss[0m : 2.49826
[1mStep[0m  [48/84], [94mLoss[0m : 2.61637
[1mStep[0m  [56/84], [94mLoss[0m : 2.24305
[1mStep[0m  [64/84], [94mLoss[0m : 2.80262
[1mStep[0m  [72/84], [94mLoss[0m : 2.56222
[1mStep[0m  [80/84], [94mLoss[0m : 2.81569

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.729, [92mTest[0m: 2.823, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46445
[1mStep[0m  [8/84], [94mLoss[0m : 2.76045
[1mStep[0m  [16/84], [94mLoss[0m : 2.59540
[1mStep[0m  [24/84], [94mLoss[0m : 2.28676
[1mStep[0m  [32/84], [94mLoss[0m : 2.73135
[1mStep[0m  [40/84], [94mLoss[0m : 2.69023
[1mStep[0m  [48/84], [94mLoss[0m : 2.90247
[1mStep[0m  [56/84], [94mLoss[0m : 2.63441
[1mStep[0m  [64/84], [94mLoss[0m : 2.41723
[1mStep[0m  [72/84], [94mLoss[0m : 2.73861
[1mStep[0m  [80/84], [94mLoss[0m : 2.43538

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.800, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57304
[1mStep[0m  [8/84], [94mLoss[0m : 2.57854
[1mStep[0m  [16/84], [94mLoss[0m : 2.89817
[1mStep[0m  [24/84], [94mLoss[0m : 2.53519
[1mStep[0m  [32/84], [94mLoss[0m : 2.76594
[1mStep[0m  [40/84], [94mLoss[0m : 2.45678
[1mStep[0m  [48/84], [94mLoss[0m : 2.73917
[1mStep[0m  [56/84], [94mLoss[0m : 2.93353
[1mStep[0m  [64/84], [94mLoss[0m : 2.64933
[1mStep[0m  [72/84], [94mLoss[0m : 2.63547
[1mStep[0m  [80/84], [94mLoss[0m : 2.97408

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.780, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.03471
[1mStep[0m  [8/84], [94mLoss[0m : 2.66206
[1mStep[0m  [16/84], [94mLoss[0m : 2.83456
[1mStep[0m  [24/84], [94mLoss[0m : 2.73974
[1mStep[0m  [32/84], [94mLoss[0m : 2.71097
[1mStep[0m  [40/84], [94mLoss[0m : 2.60320
[1mStep[0m  [48/84], [94mLoss[0m : 2.51110
[1mStep[0m  [56/84], [94mLoss[0m : 2.45684
[1mStep[0m  [64/84], [94mLoss[0m : 2.65900
[1mStep[0m  [72/84], [94mLoss[0m : 3.05129
[1mStep[0m  [80/84], [94mLoss[0m : 2.70370

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.806, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29240
[1mStep[0m  [8/84], [94mLoss[0m : 2.62098
[1mStep[0m  [16/84], [94mLoss[0m : 2.78910
[1mStep[0m  [24/84], [94mLoss[0m : 2.71266
[1mStep[0m  [32/84], [94mLoss[0m : 2.81079
[1mStep[0m  [40/84], [94mLoss[0m : 2.64362
[1mStep[0m  [48/84], [94mLoss[0m : 2.41743
[1mStep[0m  [56/84], [94mLoss[0m : 2.66948
[1mStep[0m  [64/84], [94mLoss[0m : 2.87041
[1mStep[0m  [72/84], [94mLoss[0m : 2.94450
[1mStep[0m  [80/84], [94mLoss[0m : 2.49626

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.799, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.94655
[1mStep[0m  [8/84], [94mLoss[0m : 2.65319
[1mStep[0m  [16/84], [94mLoss[0m : 3.01554
[1mStep[0m  [24/84], [94mLoss[0m : 2.63188
[1mStep[0m  [32/84], [94mLoss[0m : 2.79328
[1mStep[0m  [40/84], [94mLoss[0m : 2.73773
[1mStep[0m  [48/84], [94mLoss[0m : 2.80267
[1mStep[0m  [56/84], [94mLoss[0m : 2.68748
[1mStep[0m  [64/84], [94mLoss[0m : 2.61712
[1mStep[0m  [72/84], [94mLoss[0m : 2.97772
[1mStep[0m  [80/84], [94mLoss[0m : 2.57353

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.817, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55464
[1mStep[0m  [8/84], [94mLoss[0m : 2.89283
[1mStep[0m  [16/84], [94mLoss[0m : 2.88614
[1mStep[0m  [24/84], [94mLoss[0m : 2.50676
[1mStep[0m  [32/84], [94mLoss[0m : 3.06211
[1mStep[0m  [40/84], [94mLoss[0m : 2.94917
[1mStep[0m  [48/84], [94mLoss[0m : 2.69810
[1mStep[0m  [56/84], [94mLoss[0m : 2.82539
[1mStep[0m  [64/84], [94mLoss[0m : 2.56832
[1mStep[0m  [72/84], [94mLoss[0m : 2.63572
[1mStep[0m  [80/84], [94mLoss[0m : 2.55024

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.742, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.830
====================================

Phase 2 - Evaluation MAE:  2.8295681306294034
MAE score P1      4.325443
MAE score P2      2.829568
loss               2.69549
learning_rate       0.0001
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.73017
[1mStep[0m  [8/84], [94mLoss[0m : 10.64328
[1mStep[0m  [16/84], [94mLoss[0m : 10.76740
[1mStep[0m  [24/84], [94mLoss[0m : 10.90564
[1mStep[0m  [32/84], [94mLoss[0m : 11.11845
[1mStep[0m  [40/84], [94mLoss[0m : 10.45674
[1mStep[0m  [48/84], [94mLoss[0m : 10.71930
[1mStep[0m  [56/84], [94mLoss[0m : 10.82691
[1mStep[0m  [64/84], [94mLoss[0m : 11.21610
[1mStep[0m  [72/84], [94mLoss[0m : 10.92344
[1mStep[0m  [80/84], [94mLoss[0m : 11.30133

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.965, [92mTest[0m: 11.008, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.21386
[1mStep[0m  [8/84], [94mLoss[0m : 11.07440
[1mStep[0m  [16/84], [94mLoss[0m : 10.94916
[1mStep[0m  [24/84], [94mLoss[0m : 10.92618
[1mStep[0m  [32/84], [94mLoss[0m : 10.35505
[1mStep[0m  [40/84], [94mLoss[0m : 10.45210
[1mStep[0m  [48/84], [94mLoss[0m : 10.62659
[1mStep[0m  [56/84], [94mLoss[0m : 11.24347
[1mStep[0m  [64/84], [94mLoss[0m : 10.54697
[1mStep[0m  [72/84], [94mLoss[0m : 11.03040
[1mStep[0m  [80/84], [94mLoss[0m : 10.78811

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.924, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.87478
[1mStep[0m  [8/84], [94mLoss[0m : 10.12110
[1mStep[0m  [16/84], [94mLoss[0m : 10.70264
[1mStep[0m  [24/84], [94mLoss[0m : 10.43306
[1mStep[0m  [32/84], [94mLoss[0m : 10.68321
[1mStep[0m  [40/84], [94mLoss[0m : 10.42695
[1mStep[0m  [48/84], [94mLoss[0m : 10.87250
[1mStep[0m  [56/84], [94mLoss[0m : 10.92026
[1mStep[0m  [64/84], [94mLoss[0m : 10.86495
[1mStep[0m  [72/84], [94mLoss[0m : 10.75882
[1mStep[0m  [80/84], [94mLoss[0m : 10.65619

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.769, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.88337
[1mStep[0m  [8/84], [94mLoss[0m : 10.58766
[1mStep[0m  [16/84], [94mLoss[0m : 11.24906
[1mStep[0m  [24/84], [94mLoss[0m : 10.90856
[1mStep[0m  [32/84], [94mLoss[0m : 10.22843
[1mStep[0m  [40/84], [94mLoss[0m : 10.86324
[1mStep[0m  [48/84], [94mLoss[0m : 10.54218
[1mStep[0m  [56/84], [94mLoss[0m : 10.24647
[1mStep[0m  [64/84], [94mLoss[0m : 10.26732
[1mStep[0m  [72/84], [94mLoss[0m : 11.08161
[1mStep[0m  [80/84], [94mLoss[0m : 10.46720

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.670, [92mTest[0m: 10.705, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.34947
[1mStep[0m  [8/84], [94mLoss[0m : 10.32804
[1mStep[0m  [16/84], [94mLoss[0m : 9.96954
[1mStep[0m  [24/84], [94mLoss[0m : 10.38635
[1mStep[0m  [32/84], [94mLoss[0m : 11.23928
[1mStep[0m  [40/84], [94mLoss[0m : 10.64375
[1mStep[0m  [48/84], [94mLoss[0m : 10.78255
[1mStep[0m  [56/84], [94mLoss[0m : 10.28745
[1mStep[0m  [64/84], [94mLoss[0m : 10.47634
[1mStep[0m  [72/84], [94mLoss[0m : 9.94747
[1mStep[0m  [80/84], [94mLoss[0m : 10.90175

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.581, [92mTest[0m: 10.572, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.60814
[1mStep[0m  [8/84], [94mLoss[0m : 10.44264
[1mStep[0m  [16/84], [94mLoss[0m : 11.01259
[1mStep[0m  [24/84], [94mLoss[0m : 10.60321
[1mStep[0m  [32/84], [94mLoss[0m : 10.43066
[1mStep[0m  [40/84], [94mLoss[0m : 10.22227
[1mStep[0m  [48/84], [94mLoss[0m : 10.18866
[1mStep[0m  [56/84], [94mLoss[0m : 9.73992
[1mStep[0m  [64/84], [94mLoss[0m : 10.12275
[1mStep[0m  [72/84], [94mLoss[0m : 10.62346
[1mStep[0m  [80/84], [94mLoss[0m : 10.92038

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.475, [92mTest[0m: 10.463, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.42473
[1mStep[0m  [8/84], [94mLoss[0m : 9.89736
[1mStep[0m  [16/84], [94mLoss[0m : 10.82053
[1mStep[0m  [24/84], [94mLoss[0m : 10.47760
[1mStep[0m  [32/84], [94mLoss[0m : 10.50634
[1mStep[0m  [40/84], [94mLoss[0m : 10.79388
[1mStep[0m  [48/84], [94mLoss[0m : 9.87809
[1mStep[0m  [56/84], [94mLoss[0m : 10.87959
[1mStep[0m  [64/84], [94mLoss[0m : 10.32633
[1mStep[0m  [72/84], [94mLoss[0m : 10.74086
[1mStep[0m  [80/84], [94mLoss[0m : 10.21055

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.365, [92mTest[0m: 10.323, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.39514
[1mStep[0m  [8/84], [94mLoss[0m : 10.55300
[1mStep[0m  [16/84], [94mLoss[0m : 10.37430
[1mStep[0m  [24/84], [94mLoss[0m : 11.08240
[1mStep[0m  [32/84], [94mLoss[0m : 9.97387
[1mStep[0m  [40/84], [94mLoss[0m : 10.36221
[1mStep[0m  [48/84], [94mLoss[0m : 10.27498
[1mStep[0m  [56/84], [94mLoss[0m : 10.03864
[1mStep[0m  [64/84], [94mLoss[0m : 10.00341
[1mStep[0m  [72/84], [94mLoss[0m : 10.33806
[1mStep[0m  [80/84], [94mLoss[0m : 10.40605

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.263, [92mTest[0m: 10.205, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.28536
[1mStep[0m  [8/84], [94mLoss[0m : 9.71931
[1mStep[0m  [16/84], [94mLoss[0m : 10.30457
[1mStep[0m  [24/84], [94mLoss[0m : 10.21583
[1mStep[0m  [32/84], [94mLoss[0m : 9.96925
[1mStep[0m  [40/84], [94mLoss[0m : 9.84693
[1mStep[0m  [48/84], [94mLoss[0m : 10.18611
[1mStep[0m  [56/84], [94mLoss[0m : 9.81798
[1mStep[0m  [64/84], [94mLoss[0m : 10.68765
[1mStep[0m  [72/84], [94mLoss[0m : 10.58159
[1mStep[0m  [80/84], [94mLoss[0m : 9.97337

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.153, [92mTest[0m: 10.092, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.29710
[1mStep[0m  [8/84], [94mLoss[0m : 10.04986
[1mStep[0m  [16/84], [94mLoss[0m : 9.61102
[1mStep[0m  [24/84], [94mLoss[0m : 10.27824
[1mStep[0m  [32/84], [94mLoss[0m : 10.29416
[1mStep[0m  [40/84], [94mLoss[0m : 9.89221
[1mStep[0m  [48/84], [94mLoss[0m : 9.70828
[1mStep[0m  [56/84], [94mLoss[0m : 10.27740
[1mStep[0m  [64/84], [94mLoss[0m : 9.98250
[1mStep[0m  [72/84], [94mLoss[0m : 10.06734
[1mStep[0m  [80/84], [94mLoss[0m : 9.87610

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.045, [92mTest[0m: 9.975, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.23594
[1mStep[0m  [8/84], [94mLoss[0m : 10.07657
[1mStep[0m  [16/84], [94mLoss[0m : 10.05015
[1mStep[0m  [24/84], [94mLoss[0m : 9.87415
[1mStep[0m  [32/84], [94mLoss[0m : 9.67716
[1mStep[0m  [40/84], [94mLoss[0m : 9.86127
[1mStep[0m  [48/84], [94mLoss[0m : 9.54392
[1mStep[0m  [56/84], [94mLoss[0m : 10.05276
[1mStep[0m  [64/84], [94mLoss[0m : 9.74564
[1mStep[0m  [72/84], [94mLoss[0m : 9.89066
[1mStep[0m  [80/84], [94mLoss[0m : 9.99515

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.941, [92mTest[0m: 9.841, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.45430
[1mStep[0m  [8/84], [94mLoss[0m : 10.69672
[1mStep[0m  [16/84], [94mLoss[0m : 9.99487
[1mStep[0m  [24/84], [94mLoss[0m : 9.60537
[1mStep[0m  [32/84], [94mLoss[0m : 9.85483
[1mStep[0m  [40/84], [94mLoss[0m : 9.49934
[1mStep[0m  [48/84], [94mLoss[0m : 9.91113
[1mStep[0m  [56/84], [94mLoss[0m : 9.64595
[1mStep[0m  [64/84], [94mLoss[0m : 9.88233
[1mStep[0m  [72/84], [94mLoss[0m : 10.04511
[1mStep[0m  [80/84], [94mLoss[0m : 9.89015

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.835, [92mTest[0m: 9.688, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.15819
[1mStep[0m  [8/84], [94mLoss[0m : 10.00785
[1mStep[0m  [16/84], [94mLoss[0m : 9.78064
[1mStep[0m  [24/84], [94mLoss[0m : 9.47619
[1mStep[0m  [32/84], [94mLoss[0m : 9.03048
[1mStep[0m  [40/84], [94mLoss[0m : 10.06086
[1mStep[0m  [48/84], [94mLoss[0m : 9.77088
[1mStep[0m  [56/84], [94mLoss[0m : 9.57380
[1mStep[0m  [64/84], [94mLoss[0m : 9.75791
[1mStep[0m  [72/84], [94mLoss[0m : 9.27602
[1mStep[0m  [80/84], [94mLoss[0m : 9.95308

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.711, [92mTest[0m: 9.567, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.91153
[1mStep[0m  [8/84], [94mLoss[0m : 9.17398
[1mStep[0m  [16/84], [94mLoss[0m : 9.91687
[1mStep[0m  [24/84], [94mLoss[0m : 9.23158
[1mStep[0m  [32/84], [94mLoss[0m : 9.67251
[1mStep[0m  [40/84], [94mLoss[0m : 9.61714
[1mStep[0m  [48/84], [94mLoss[0m : 9.20749
[1mStep[0m  [56/84], [94mLoss[0m : 9.71956
[1mStep[0m  [64/84], [94mLoss[0m : 9.81740
[1mStep[0m  [72/84], [94mLoss[0m : 9.75486
[1mStep[0m  [80/84], [94mLoss[0m : 9.70715

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.601, [92mTest[0m: 9.438, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.30381
[1mStep[0m  [8/84], [94mLoss[0m : 9.19373
[1mStep[0m  [16/84], [94mLoss[0m : 9.33338
[1mStep[0m  [24/84], [94mLoss[0m : 9.90191
[1mStep[0m  [32/84], [94mLoss[0m : 9.31775
[1mStep[0m  [40/84], [94mLoss[0m : 9.56494
[1mStep[0m  [48/84], [94mLoss[0m : 9.84367
[1mStep[0m  [56/84], [94mLoss[0m : 9.62432
[1mStep[0m  [64/84], [94mLoss[0m : 9.58769
[1mStep[0m  [72/84], [94mLoss[0m : 9.07711
[1mStep[0m  [80/84], [94mLoss[0m : 9.22215

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.473, [92mTest[0m: 9.281, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.14167
[1mStep[0m  [8/84], [94mLoss[0m : 9.00636
[1mStep[0m  [16/84], [94mLoss[0m : 9.02881
[1mStep[0m  [24/84], [94mLoss[0m : 9.20752
[1mStep[0m  [32/84], [94mLoss[0m : 9.41162
[1mStep[0m  [40/84], [94mLoss[0m : 9.32315
[1mStep[0m  [48/84], [94mLoss[0m : 9.37353
[1mStep[0m  [56/84], [94mLoss[0m : 9.43025
[1mStep[0m  [64/84], [94mLoss[0m : 8.69957
[1mStep[0m  [72/84], [94mLoss[0m : 9.12692
[1mStep[0m  [80/84], [94mLoss[0m : 9.61932

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.356, [92mTest[0m: 9.124, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.02135
[1mStep[0m  [8/84], [94mLoss[0m : 8.93592
[1mStep[0m  [16/84], [94mLoss[0m : 9.87493
[1mStep[0m  [24/84], [94mLoss[0m : 8.85021
[1mStep[0m  [32/84], [94mLoss[0m : 9.00716
[1mStep[0m  [40/84], [94mLoss[0m : 9.29486
[1mStep[0m  [48/84], [94mLoss[0m : 9.43386
[1mStep[0m  [56/84], [94mLoss[0m : 9.25948
[1mStep[0m  [64/84], [94mLoss[0m : 8.63227
[1mStep[0m  [72/84], [94mLoss[0m : 8.75166
[1mStep[0m  [80/84], [94mLoss[0m : 9.43316

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.222, [92mTest[0m: 8.973, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.18443
[1mStep[0m  [8/84], [94mLoss[0m : 9.30415
[1mStep[0m  [16/84], [94mLoss[0m : 9.22961
[1mStep[0m  [24/84], [94mLoss[0m : 8.86580
[1mStep[0m  [32/84], [94mLoss[0m : 9.44592
[1mStep[0m  [40/84], [94mLoss[0m : 9.42400
[1mStep[0m  [48/84], [94mLoss[0m : 9.02299
[1mStep[0m  [56/84], [94mLoss[0m : 8.84355
[1mStep[0m  [64/84], [94mLoss[0m : 9.49974
[1mStep[0m  [72/84], [94mLoss[0m : 9.02339
[1mStep[0m  [80/84], [94mLoss[0m : 8.71666

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.087, [92mTest[0m: 8.809, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.49406
[1mStep[0m  [8/84], [94mLoss[0m : 8.87384
[1mStep[0m  [16/84], [94mLoss[0m : 8.90304
[1mStep[0m  [24/84], [94mLoss[0m : 8.99803
[1mStep[0m  [32/84], [94mLoss[0m : 9.12889
[1mStep[0m  [40/84], [94mLoss[0m : 8.86678
[1mStep[0m  [48/84], [94mLoss[0m : 8.96818
[1mStep[0m  [56/84], [94mLoss[0m : 8.94394
[1mStep[0m  [64/84], [94mLoss[0m : 8.95108
[1mStep[0m  [72/84], [94mLoss[0m : 8.82168
[1mStep[0m  [80/84], [94mLoss[0m : 8.92363

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.939, [92mTest[0m: 8.652, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.14390
[1mStep[0m  [8/84], [94mLoss[0m : 9.45132
[1mStep[0m  [16/84], [94mLoss[0m : 8.77932
[1mStep[0m  [24/84], [94mLoss[0m : 9.27953
[1mStep[0m  [32/84], [94mLoss[0m : 8.99665
[1mStep[0m  [40/84], [94mLoss[0m : 8.56964
[1mStep[0m  [48/84], [94mLoss[0m : 8.22272
[1mStep[0m  [56/84], [94mLoss[0m : 8.61752
[1mStep[0m  [64/84], [94mLoss[0m : 8.65022
[1mStep[0m  [72/84], [94mLoss[0m : 8.60186
[1mStep[0m  [80/84], [94mLoss[0m : 8.77542

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.797, [92mTest[0m: 8.486, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.46618
[1mStep[0m  [8/84], [94mLoss[0m : 8.42623
[1mStep[0m  [16/84], [94mLoss[0m : 8.60724
[1mStep[0m  [24/84], [94mLoss[0m : 8.86823
[1mStep[0m  [32/84], [94mLoss[0m : 8.55164
[1mStep[0m  [40/84], [94mLoss[0m : 8.95398
[1mStep[0m  [48/84], [94mLoss[0m : 8.71351
[1mStep[0m  [56/84], [94mLoss[0m : 8.06537
[1mStep[0m  [64/84], [94mLoss[0m : 8.59475
[1mStep[0m  [72/84], [94mLoss[0m : 8.42219
[1mStep[0m  [80/84], [94mLoss[0m : 8.32172

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.642, [92mTest[0m: 8.323, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.76510
[1mStep[0m  [8/84], [94mLoss[0m : 8.74574
[1mStep[0m  [16/84], [94mLoss[0m : 8.55798
[1mStep[0m  [24/84], [94mLoss[0m : 8.58454
[1mStep[0m  [32/84], [94mLoss[0m : 9.00882
[1mStep[0m  [40/84], [94mLoss[0m : 8.10872
[1mStep[0m  [48/84], [94mLoss[0m : 8.63313
[1mStep[0m  [56/84], [94mLoss[0m : 8.63710
[1mStep[0m  [64/84], [94mLoss[0m : 8.43025
[1mStep[0m  [72/84], [94mLoss[0m : 8.52471
[1mStep[0m  [80/84], [94mLoss[0m : 7.85425

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.494, [92mTest[0m: 8.141, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.20939
[1mStep[0m  [8/84], [94mLoss[0m : 8.74885
[1mStep[0m  [16/84], [94mLoss[0m : 8.75916
[1mStep[0m  [24/84], [94mLoss[0m : 8.60916
[1mStep[0m  [32/84], [94mLoss[0m : 8.31806
[1mStep[0m  [40/84], [94mLoss[0m : 7.98686
[1mStep[0m  [48/84], [94mLoss[0m : 8.56604
[1mStep[0m  [56/84], [94mLoss[0m : 8.54918
[1mStep[0m  [64/84], [94mLoss[0m : 7.69917
[1mStep[0m  [72/84], [94mLoss[0m : 8.20887
[1mStep[0m  [80/84], [94mLoss[0m : 8.09478

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.332, [92mTest[0m: 7.948, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.48750
[1mStep[0m  [8/84], [94mLoss[0m : 8.41368
[1mStep[0m  [16/84], [94mLoss[0m : 7.83205
[1mStep[0m  [24/84], [94mLoss[0m : 8.31590
[1mStep[0m  [32/84], [94mLoss[0m : 8.26173
[1mStep[0m  [40/84], [94mLoss[0m : 8.31163
[1mStep[0m  [48/84], [94mLoss[0m : 7.84236
[1mStep[0m  [56/84], [94mLoss[0m : 7.37093
[1mStep[0m  [64/84], [94mLoss[0m : 7.83740
[1mStep[0m  [72/84], [94mLoss[0m : 8.28512
[1mStep[0m  [80/84], [94mLoss[0m : 8.39806

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.191, [92mTest[0m: 7.766, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.61495
[1mStep[0m  [8/84], [94mLoss[0m : 8.19676
[1mStep[0m  [16/84], [94mLoss[0m : 8.11577
[1mStep[0m  [24/84], [94mLoss[0m : 7.83948
[1mStep[0m  [32/84], [94mLoss[0m : 7.76160
[1mStep[0m  [40/84], [94mLoss[0m : 8.01357
[1mStep[0m  [48/84], [94mLoss[0m : 7.41754
[1mStep[0m  [56/84], [94mLoss[0m : 8.65360
[1mStep[0m  [64/84], [94mLoss[0m : 8.30259
[1mStep[0m  [72/84], [94mLoss[0m : 7.29571
[1mStep[0m  [80/84], [94mLoss[0m : 7.50495

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.013, [92mTest[0m: 7.565, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.57991
[1mStep[0m  [8/84], [94mLoss[0m : 7.82840
[1mStep[0m  [16/84], [94mLoss[0m : 8.22742
[1mStep[0m  [24/84], [94mLoss[0m : 7.97979
[1mStep[0m  [32/84], [94mLoss[0m : 7.81099
[1mStep[0m  [40/84], [94mLoss[0m : 7.61393
[1mStep[0m  [48/84], [94mLoss[0m : 7.53467
[1mStep[0m  [56/84], [94mLoss[0m : 7.47556
[1mStep[0m  [64/84], [94mLoss[0m : 7.75226
[1mStep[0m  [72/84], [94mLoss[0m : 7.23710
[1mStep[0m  [80/84], [94mLoss[0m : 7.47373

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.831, [92mTest[0m: 7.378, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.09817
[1mStep[0m  [8/84], [94mLoss[0m : 8.19922
[1mStep[0m  [16/84], [94mLoss[0m : 7.76948
[1mStep[0m  [24/84], [94mLoss[0m : 8.17967
[1mStep[0m  [32/84], [94mLoss[0m : 8.05799
[1mStep[0m  [40/84], [94mLoss[0m : 7.47787
[1mStep[0m  [48/84], [94mLoss[0m : 7.50386
[1mStep[0m  [56/84], [94mLoss[0m : 7.70922
[1mStep[0m  [64/84], [94mLoss[0m : 7.15758
[1mStep[0m  [72/84], [94mLoss[0m : 7.99493
[1mStep[0m  [80/84], [94mLoss[0m : 7.22980

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.640, [92mTest[0m: 7.161, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.44683
[1mStep[0m  [8/84], [94mLoss[0m : 7.68675
[1mStep[0m  [16/84], [94mLoss[0m : 7.80399
[1mStep[0m  [24/84], [94mLoss[0m : 7.57795
[1mStep[0m  [32/84], [94mLoss[0m : 7.54532
[1mStep[0m  [40/84], [94mLoss[0m : 7.64664
[1mStep[0m  [48/84], [94mLoss[0m : 7.06249
[1mStep[0m  [56/84], [94mLoss[0m : 7.46650
[1mStep[0m  [64/84], [94mLoss[0m : 7.49477
[1mStep[0m  [72/84], [94mLoss[0m : 7.16990
[1mStep[0m  [80/84], [94mLoss[0m : 7.10212

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.448, [92mTest[0m: 6.922, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.45336
[1mStep[0m  [8/84], [94mLoss[0m : 7.76006
[1mStep[0m  [16/84], [94mLoss[0m : 7.33186
[1mStep[0m  [24/84], [94mLoss[0m : 7.21387
[1mStep[0m  [32/84], [94mLoss[0m : 7.66056
[1mStep[0m  [40/84], [94mLoss[0m : 7.15564
[1mStep[0m  [48/84], [94mLoss[0m : 7.27652
[1mStep[0m  [56/84], [94mLoss[0m : 6.56400
[1mStep[0m  [64/84], [94mLoss[0m : 7.30215
[1mStep[0m  [72/84], [94mLoss[0m : 7.11766
[1mStep[0m  [80/84], [94mLoss[0m : 7.14425

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.247, [92mTest[0m: 6.724, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.34910
[1mStep[0m  [8/84], [94mLoss[0m : 6.68588
[1mStep[0m  [16/84], [94mLoss[0m : 6.87939
[1mStep[0m  [24/84], [94mLoss[0m : 7.23754
[1mStep[0m  [32/84], [94mLoss[0m : 7.40879
[1mStep[0m  [40/84], [94mLoss[0m : 6.93634
[1mStep[0m  [48/84], [94mLoss[0m : 6.85548
[1mStep[0m  [56/84], [94mLoss[0m : 7.54608
[1mStep[0m  [64/84], [94mLoss[0m : 6.76825
[1mStep[0m  [72/84], [94mLoss[0m : 6.86427
[1mStep[0m  [80/84], [94mLoss[0m : 6.72494

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.062, [92mTest[0m: 6.400, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.278
====================================

Phase 1 - Evaluation MAE:  6.278338210923331
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 6.83746
[1mStep[0m  [8/84], [94mLoss[0m : 6.98249
[1mStep[0m  [16/84], [94mLoss[0m : 6.55347
[1mStep[0m  [24/84], [94mLoss[0m : 6.82836
[1mStep[0m  [32/84], [94mLoss[0m : 7.29067
[1mStep[0m  [40/84], [94mLoss[0m : 6.49733
[1mStep[0m  [48/84], [94mLoss[0m : 6.59644
[1mStep[0m  [56/84], [94mLoss[0m : 7.14033
[1mStep[0m  [64/84], [94mLoss[0m : 6.42524
[1mStep[0m  [72/84], [94mLoss[0m : 7.09582
[1mStep[0m  [80/84], [94mLoss[0m : 6.94833

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.865, [92mTest[0m: 6.274, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.59989
[1mStep[0m  [8/84], [94mLoss[0m : 7.22557
[1mStep[0m  [16/84], [94mLoss[0m : 6.71436
[1mStep[0m  [24/84], [94mLoss[0m : 6.58904
[1mStep[0m  [32/84], [94mLoss[0m : 6.79950
[1mStep[0m  [40/84], [94mLoss[0m : 7.12453
[1mStep[0m  [48/84], [94mLoss[0m : 6.63478
[1mStep[0m  [56/84], [94mLoss[0m : 6.59565
[1mStep[0m  [64/84], [94mLoss[0m : 6.50447
[1mStep[0m  [72/84], [94mLoss[0m : 6.60788
[1mStep[0m  [80/84], [94mLoss[0m : 6.96747

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.662, [92mTest[0m: 5.912, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.45145
[1mStep[0m  [8/84], [94mLoss[0m : 6.52939
[1mStep[0m  [16/84], [94mLoss[0m : 6.12148
[1mStep[0m  [24/84], [94mLoss[0m : 5.88605
[1mStep[0m  [32/84], [94mLoss[0m : 6.64150
[1mStep[0m  [40/84], [94mLoss[0m : 6.38279
[1mStep[0m  [48/84], [94mLoss[0m : 6.21563
[1mStep[0m  [56/84], [94mLoss[0m : 6.20960
[1mStep[0m  [64/84], [94mLoss[0m : 6.43594
[1mStep[0m  [72/84], [94mLoss[0m : 6.20721
[1mStep[0m  [80/84], [94mLoss[0m : 5.79213

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.438, [92mTest[0m: 5.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.25172
[1mStep[0m  [8/84], [94mLoss[0m : 6.64584
[1mStep[0m  [16/84], [94mLoss[0m : 6.00164
[1mStep[0m  [24/84], [94mLoss[0m : 6.38705
[1mStep[0m  [32/84], [94mLoss[0m : 6.60590
[1mStep[0m  [40/84], [94mLoss[0m : 6.28722
[1mStep[0m  [48/84], [94mLoss[0m : 6.23786
[1mStep[0m  [56/84], [94mLoss[0m : 6.18752
[1mStep[0m  [64/84], [94mLoss[0m : 6.19531
[1mStep[0m  [72/84], [94mLoss[0m : 5.90989
[1mStep[0m  [80/84], [94mLoss[0m : 5.90382

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.226, [92mTest[0m: 5.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.68161
[1mStep[0m  [8/84], [94mLoss[0m : 5.63418
[1mStep[0m  [16/84], [94mLoss[0m : 5.88080
[1mStep[0m  [24/84], [94mLoss[0m : 6.18698
[1mStep[0m  [32/84], [94mLoss[0m : 6.05732
[1mStep[0m  [40/84], [94mLoss[0m : 6.20841
[1mStep[0m  [48/84], [94mLoss[0m : 5.74718
[1mStep[0m  [56/84], [94mLoss[0m : 6.34581
[1mStep[0m  [64/84], [94mLoss[0m : 6.24808
[1mStep[0m  [72/84], [94mLoss[0m : 5.89714
[1mStep[0m  [80/84], [94mLoss[0m : 5.74783

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.019, [92mTest[0m: 5.519, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.90224
[1mStep[0m  [8/84], [94mLoss[0m : 6.33549
[1mStep[0m  [16/84], [94mLoss[0m : 6.16209
[1mStep[0m  [24/84], [94mLoss[0m : 5.30320
[1mStep[0m  [32/84], [94mLoss[0m : 5.59236
[1mStep[0m  [40/84], [94mLoss[0m : 5.52547
[1mStep[0m  [48/84], [94mLoss[0m : 5.58633
[1mStep[0m  [56/84], [94mLoss[0m : 5.83985
[1mStep[0m  [64/84], [94mLoss[0m : 5.60794
[1mStep[0m  [72/84], [94mLoss[0m : 6.06628
[1mStep[0m  [80/84], [94mLoss[0m : 6.24677

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.824, [92mTest[0m: 5.349, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.77880
[1mStep[0m  [8/84], [94mLoss[0m : 5.66160
[1mStep[0m  [16/84], [94mLoss[0m : 6.22703
[1mStep[0m  [24/84], [94mLoss[0m : 5.53345
[1mStep[0m  [32/84], [94mLoss[0m : 5.49469
[1mStep[0m  [40/84], [94mLoss[0m : 5.56771
[1mStep[0m  [48/84], [94mLoss[0m : 5.58037
[1mStep[0m  [56/84], [94mLoss[0m : 5.58082
[1mStep[0m  [64/84], [94mLoss[0m : 5.40012
[1mStep[0m  [72/84], [94mLoss[0m : 5.55389
[1mStep[0m  [80/84], [94mLoss[0m : 5.91635

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.577, [92mTest[0m: 5.166, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.33021
[1mStep[0m  [8/84], [94mLoss[0m : 5.63583
[1mStep[0m  [16/84], [94mLoss[0m : 5.75840
[1mStep[0m  [24/84], [94mLoss[0m : 4.95059
[1mStep[0m  [32/84], [94mLoss[0m : 6.19306
[1mStep[0m  [40/84], [94mLoss[0m : 5.47226
[1mStep[0m  [48/84], [94mLoss[0m : 5.61848
[1mStep[0m  [56/84], [94mLoss[0m : 5.35604
[1mStep[0m  [64/84], [94mLoss[0m : 5.12039
[1mStep[0m  [72/84], [94mLoss[0m : 5.55977
[1mStep[0m  [80/84], [94mLoss[0m : 5.54974

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.405, [92mTest[0m: 5.080, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.80774
[1mStep[0m  [8/84], [94mLoss[0m : 5.33232
[1mStep[0m  [16/84], [94mLoss[0m : 5.57676
[1mStep[0m  [24/84], [94mLoss[0m : 5.10838
[1mStep[0m  [32/84], [94mLoss[0m : 5.08861
[1mStep[0m  [40/84], [94mLoss[0m : 5.53583
[1mStep[0m  [48/84], [94mLoss[0m : 5.24969
[1mStep[0m  [56/84], [94mLoss[0m : 4.60463
[1mStep[0m  [64/84], [94mLoss[0m : 5.01148
[1mStep[0m  [72/84], [94mLoss[0m : 5.22568
[1mStep[0m  [80/84], [94mLoss[0m : 5.48339

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.199, [92mTest[0m: 4.841, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.24782
[1mStep[0m  [8/84], [94mLoss[0m : 5.16416
[1mStep[0m  [16/84], [94mLoss[0m : 5.10505
[1mStep[0m  [24/84], [94mLoss[0m : 5.65833
[1mStep[0m  [32/84], [94mLoss[0m : 5.08802
[1mStep[0m  [40/84], [94mLoss[0m : 4.99793
[1mStep[0m  [48/84], [94mLoss[0m : 4.97712
[1mStep[0m  [56/84], [94mLoss[0m : 4.76357
[1mStep[0m  [64/84], [94mLoss[0m : 5.16046
[1mStep[0m  [72/84], [94mLoss[0m : 5.06344
[1mStep[0m  [80/84], [94mLoss[0m : 5.29965

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.995, [92mTest[0m: 4.687, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.84231
[1mStep[0m  [8/84], [94mLoss[0m : 4.64967
[1mStep[0m  [16/84], [94mLoss[0m : 4.84432
[1mStep[0m  [24/84], [94mLoss[0m : 4.97433
[1mStep[0m  [32/84], [94mLoss[0m : 4.30870
[1mStep[0m  [40/84], [94mLoss[0m : 5.41587
[1mStep[0m  [48/84], [94mLoss[0m : 4.85737
[1mStep[0m  [56/84], [94mLoss[0m : 4.55961
[1mStep[0m  [64/84], [94mLoss[0m : 4.42439
[1mStep[0m  [72/84], [94mLoss[0m : 4.77745
[1mStep[0m  [80/84], [94mLoss[0m : 4.69387

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.810, [92mTest[0m: 4.571, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.62866
[1mStep[0m  [8/84], [94mLoss[0m : 4.84626
[1mStep[0m  [16/84], [94mLoss[0m : 5.04812
[1mStep[0m  [24/84], [94mLoss[0m : 4.25374
[1mStep[0m  [32/84], [94mLoss[0m : 4.31477
[1mStep[0m  [40/84], [94mLoss[0m : 4.37060
[1mStep[0m  [48/84], [94mLoss[0m : 4.80799
[1mStep[0m  [56/84], [94mLoss[0m : 4.83837
[1mStep[0m  [64/84], [94mLoss[0m : 4.39915
[1mStep[0m  [72/84], [94mLoss[0m : 4.48620
[1mStep[0m  [80/84], [94mLoss[0m : 4.52418

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.621, [92mTest[0m: 4.468, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.52408
[1mStep[0m  [8/84], [94mLoss[0m : 4.56955
[1mStep[0m  [16/84], [94mLoss[0m : 4.94822
[1mStep[0m  [24/84], [94mLoss[0m : 4.39776
[1mStep[0m  [32/84], [94mLoss[0m : 4.90962
[1mStep[0m  [40/84], [94mLoss[0m : 4.34321
[1mStep[0m  [48/84], [94mLoss[0m : 4.68149
[1mStep[0m  [56/84], [94mLoss[0m : 4.84589
[1mStep[0m  [64/84], [94mLoss[0m : 4.36638
[1mStep[0m  [72/84], [94mLoss[0m : 4.34191
[1mStep[0m  [80/84], [94mLoss[0m : 4.43678

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.460, [92mTest[0m: 4.208, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.22499
[1mStep[0m  [8/84], [94mLoss[0m : 4.23201
[1mStep[0m  [16/84], [94mLoss[0m : 4.68605
[1mStep[0m  [24/84], [94mLoss[0m : 4.20154
[1mStep[0m  [32/84], [94mLoss[0m : 3.96897
[1mStep[0m  [40/84], [94mLoss[0m : 4.62146
[1mStep[0m  [48/84], [94mLoss[0m : 4.55035
[1mStep[0m  [56/84], [94mLoss[0m : 4.16686
[1mStep[0m  [64/84], [94mLoss[0m : 4.21809
[1mStep[0m  [72/84], [94mLoss[0m : 4.38707
[1mStep[0m  [80/84], [94mLoss[0m : 4.34693

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.312, [92mTest[0m: 4.013, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.43552
[1mStep[0m  [8/84], [94mLoss[0m : 4.53360
[1mStep[0m  [16/84], [94mLoss[0m : 3.92319
[1mStep[0m  [24/84], [94mLoss[0m : 4.14230
[1mStep[0m  [32/84], [94mLoss[0m : 3.73231
[1mStep[0m  [40/84], [94mLoss[0m : 4.34527
[1mStep[0m  [48/84], [94mLoss[0m : 3.89994
[1mStep[0m  [56/84], [94mLoss[0m : 3.97809
[1mStep[0m  [64/84], [94mLoss[0m : 3.94877
[1mStep[0m  [72/84], [94mLoss[0m : 4.44478
[1mStep[0m  [80/84], [94mLoss[0m : 3.98415

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.173, [92mTest[0m: 3.928, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.77912
[1mStep[0m  [8/84], [94mLoss[0m : 3.70942
[1mStep[0m  [16/84], [94mLoss[0m : 3.93681
[1mStep[0m  [24/84], [94mLoss[0m : 3.74652
[1mStep[0m  [32/84], [94mLoss[0m : 4.13372
[1mStep[0m  [40/84], [94mLoss[0m : 3.97545
[1mStep[0m  [48/84], [94mLoss[0m : 3.95826
[1mStep[0m  [56/84], [94mLoss[0m : 4.03319
[1mStep[0m  [64/84], [94mLoss[0m : 4.12246
[1mStep[0m  [72/84], [94mLoss[0m : 4.10656
[1mStep[0m  [80/84], [94mLoss[0m : 3.98847

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.016, [92mTest[0m: 3.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.91817
[1mStep[0m  [8/84], [94mLoss[0m : 4.02287
[1mStep[0m  [16/84], [94mLoss[0m : 3.45381
[1mStep[0m  [24/84], [94mLoss[0m : 3.95948
[1mStep[0m  [32/84], [94mLoss[0m : 3.88180
[1mStep[0m  [40/84], [94mLoss[0m : 4.16520
[1mStep[0m  [48/84], [94mLoss[0m : 3.50244
[1mStep[0m  [56/84], [94mLoss[0m : 4.15772
[1mStep[0m  [64/84], [94mLoss[0m : 3.69381
[1mStep[0m  [72/84], [94mLoss[0m : 3.69235
[1mStep[0m  [80/84], [94mLoss[0m : 3.96291

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.925, [92mTest[0m: 3.640, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.23167
[1mStep[0m  [8/84], [94mLoss[0m : 3.63053
[1mStep[0m  [16/84], [94mLoss[0m : 3.59387
[1mStep[0m  [24/84], [94mLoss[0m : 3.56652
[1mStep[0m  [32/84], [94mLoss[0m : 4.08545
[1mStep[0m  [40/84], [94mLoss[0m : 3.76086
[1mStep[0m  [48/84], [94mLoss[0m : 3.71446
[1mStep[0m  [56/84], [94mLoss[0m : 4.00651
[1mStep[0m  [64/84], [94mLoss[0m : 3.78918
[1mStep[0m  [72/84], [94mLoss[0m : 4.21541
[1mStep[0m  [80/84], [94mLoss[0m : 3.57185

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.794, [92mTest[0m: 3.420, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.80153
[1mStep[0m  [8/84], [94mLoss[0m : 3.47292
[1mStep[0m  [16/84], [94mLoss[0m : 3.82219
[1mStep[0m  [24/84], [94mLoss[0m : 3.80985
[1mStep[0m  [32/84], [94mLoss[0m : 3.92974
[1mStep[0m  [40/84], [94mLoss[0m : 3.32109
[1mStep[0m  [48/84], [94mLoss[0m : 3.62391
[1mStep[0m  [56/84], [94mLoss[0m : 3.62543
[1mStep[0m  [64/84], [94mLoss[0m : 3.89208
[1mStep[0m  [72/84], [94mLoss[0m : 3.72984
[1mStep[0m  [80/84], [94mLoss[0m : 3.42855

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.673, [92mTest[0m: 3.297, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.36253
[1mStep[0m  [8/84], [94mLoss[0m : 3.34984
[1mStep[0m  [16/84], [94mLoss[0m : 3.44275
[1mStep[0m  [24/84], [94mLoss[0m : 3.61647
[1mStep[0m  [32/84], [94mLoss[0m : 3.73506
[1mStep[0m  [40/84], [94mLoss[0m : 3.34621
[1mStep[0m  [48/84], [94mLoss[0m : 3.93702
[1mStep[0m  [56/84], [94mLoss[0m : 3.63077
[1mStep[0m  [64/84], [94mLoss[0m : 3.48190
[1mStep[0m  [72/84], [94mLoss[0m : 3.56667
[1mStep[0m  [80/84], [94mLoss[0m : 3.37019

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.583, [92mTest[0m: 3.277, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.20867
[1mStep[0m  [8/84], [94mLoss[0m : 3.25608
[1mStep[0m  [16/84], [94mLoss[0m : 3.52519
[1mStep[0m  [24/84], [94mLoss[0m : 3.39520
[1mStep[0m  [32/84], [94mLoss[0m : 3.69238
[1mStep[0m  [40/84], [94mLoss[0m : 3.41428
[1mStep[0m  [48/84], [94mLoss[0m : 3.37268
[1mStep[0m  [56/84], [94mLoss[0m : 3.54953
[1mStep[0m  [64/84], [94mLoss[0m : 3.42088
[1mStep[0m  [72/84], [94mLoss[0m : 3.38970
[1mStep[0m  [80/84], [94mLoss[0m : 3.75604

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.453, [92mTest[0m: 3.218, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90372
[1mStep[0m  [8/84], [94mLoss[0m : 3.40947
[1mStep[0m  [16/84], [94mLoss[0m : 3.82288
[1mStep[0m  [24/84], [94mLoss[0m : 3.30029
[1mStep[0m  [32/84], [94mLoss[0m : 3.77994
[1mStep[0m  [40/84], [94mLoss[0m : 3.53514
[1mStep[0m  [48/84], [94mLoss[0m : 3.38871
[1mStep[0m  [56/84], [94mLoss[0m : 3.61595
[1mStep[0m  [64/84], [94mLoss[0m : 3.21552
[1mStep[0m  [72/84], [94mLoss[0m : 3.38645
[1mStep[0m  [80/84], [94mLoss[0m : 3.82559

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.418, [92mTest[0m: 3.048, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.44090
[1mStep[0m  [8/84], [94mLoss[0m : 3.20135
[1mStep[0m  [16/84], [94mLoss[0m : 3.28825
[1mStep[0m  [24/84], [94mLoss[0m : 3.97178
[1mStep[0m  [32/84], [94mLoss[0m : 3.31938
[1mStep[0m  [40/84], [94mLoss[0m : 3.08346
[1mStep[0m  [48/84], [94mLoss[0m : 3.60953
[1mStep[0m  [56/84], [94mLoss[0m : 3.51807
[1mStep[0m  [64/84], [94mLoss[0m : 3.44062
[1mStep[0m  [72/84], [94mLoss[0m : 3.33365
[1mStep[0m  [80/84], [94mLoss[0m : 3.33614

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.337, [92mTest[0m: 3.031, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.17405
[1mStep[0m  [8/84], [94mLoss[0m : 3.11941
[1mStep[0m  [16/84], [94mLoss[0m : 3.61435
[1mStep[0m  [24/84], [94mLoss[0m : 3.41949
[1mStep[0m  [32/84], [94mLoss[0m : 3.22107
[1mStep[0m  [40/84], [94mLoss[0m : 3.49046
[1mStep[0m  [48/84], [94mLoss[0m : 3.03121
[1mStep[0m  [56/84], [94mLoss[0m : 3.47812
[1mStep[0m  [64/84], [94mLoss[0m : 2.85707
[1mStep[0m  [72/84], [94mLoss[0m : 3.30050
[1mStep[0m  [80/84], [94mLoss[0m : 3.36379

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.294, [92mTest[0m: 3.010, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89174
[1mStep[0m  [8/84], [94mLoss[0m : 3.07768
[1mStep[0m  [16/84], [94mLoss[0m : 3.44466
[1mStep[0m  [24/84], [94mLoss[0m : 3.33337
[1mStep[0m  [32/84], [94mLoss[0m : 3.34837
[1mStep[0m  [40/84], [94mLoss[0m : 3.53616
[1mStep[0m  [48/84], [94mLoss[0m : 3.52158
[1mStep[0m  [56/84], [94mLoss[0m : 2.92417
[1mStep[0m  [64/84], [94mLoss[0m : 3.12244
[1mStep[0m  [72/84], [94mLoss[0m : 3.75419
[1mStep[0m  [80/84], [94mLoss[0m : 3.26622

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.246, [92mTest[0m: 2.904, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.04321
[1mStep[0m  [8/84], [94mLoss[0m : 2.98731
[1mStep[0m  [16/84], [94mLoss[0m : 3.45575
[1mStep[0m  [24/84], [94mLoss[0m : 3.02310
[1mStep[0m  [32/84], [94mLoss[0m : 3.14478
[1mStep[0m  [40/84], [94mLoss[0m : 3.20304
[1mStep[0m  [48/84], [94mLoss[0m : 3.06735
[1mStep[0m  [56/84], [94mLoss[0m : 2.95230
[1mStep[0m  [64/84], [94mLoss[0m : 3.33804
[1mStep[0m  [72/84], [94mLoss[0m : 3.42906
[1mStep[0m  [80/84], [94mLoss[0m : 3.03138

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.199, [92mTest[0m: 2.858, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66245
[1mStep[0m  [8/84], [94mLoss[0m : 3.56132
[1mStep[0m  [16/84], [94mLoss[0m : 3.22119
[1mStep[0m  [24/84], [94mLoss[0m : 3.28676
[1mStep[0m  [32/84], [94mLoss[0m : 3.12056
[1mStep[0m  [40/84], [94mLoss[0m : 3.07736
[1mStep[0m  [48/84], [94mLoss[0m : 2.55268
[1mStep[0m  [56/84], [94mLoss[0m : 3.35362
[1mStep[0m  [64/84], [94mLoss[0m : 3.23599
[1mStep[0m  [72/84], [94mLoss[0m : 2.90100
[1mStep[0m  [80/84], [94mLoss[0m : 3.33518

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.147, [92mTest[0m: 2.825, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.23177
[1mStep[0m  [8/84], [94mLoss[0m : 2.89411
[1mStep[0m  [16/84], [94mLoss[0m : 3.12196
[1mStep[0m  [24/84], [94mLoss[0m : 3.14283
[1mStep[0m  [32/84], [94mLoss[0m : 3.30591
[1mStep[0m  [40/84], [94mLoss[0m : 2.68328
[1mStep[0m  [48/84], [94mLoss[0m : 2.82424
[1mStep[0m  [56/84], [94mLoss[0m : 3.35970
[1mStep[0m  [64/84], [94mLoss[0m : 3.16114
[1mStep[0m  [72/84], [94mLoss[0m : 3.37625
[1mStep[0m  [80/84], [94mLoss[0m : 3.02095

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.114, [92mTest[0m: 2.756, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.24775
[1mStep[0m  [8/84], [94mLoss[0m : 3.43643
[1mStep[0m  [16/84], [94mLoss[0m : 3.08188
[1mStep[0m  [24/84], [94mLoss[0m : 3.22423
[1mStep[0m  [32/84], [94mLoss[0m : 3.23057
[1mStep[0m  [40/84], [94mLoss[0m : 3.14735
[1mStep[0m  [48/84], [94mLoss[0m : 3.42729
[1mStep[0m  [56/84], [94mLoss[0m : 3.06525
[1mStep[0m  [64/84], [94mLoss[0m : 2.92950
[1mStep[0m  [72/84], [94mLoss[0m : 3.02754
[1mStep[0m  [80/84], [94mLoss[0m : 3.12234

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.076, [92mTest[0m: 2.780, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.21460
[1mStep[0m  [8/84], [94mLoss[0m : 3.00797
[1mStep[0m  [16/84], [94mLoss[0m : 3.11310
[1mStep[0m  [24/84], [94mLoss[0m : 3.19572
[1mStep[0m  [32/84], [94mLoss[0m : 3.36888
[1mStep[0m  [40/84], [94mLoss[0m : 2.89895
[1mStep[0m  [48/84], [94mLoss[0m : 2.89687
[1mStep[0m  [56/84], [94mLoss[0m : 2.94532
[1mStep[0m  [64/84], [94mLoss[0m : 2.95661
[1mStep[0m  [72/84], [94mLoss[0m : 3.11164
[1mStep[0m  [80/84], [94mLoss[0m : 2.99560

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.043, [92mTest[0m: 2.786, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.751
====================================

Phase 2 - Evaluation MAE:  2.751384343419756
MAE score P1       6.278338
MAE score P2       2.751384
loss               3.043396
learning_rate        0.0001
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.30463
[1mStep[0m  [8/84], [94mLoss[0m : 11.06861
[1mStep[0m  [16/84], [94mLoss[0m : 10.88440
[1mStep[0m  [24/84], [94mLoss[0m : 10.42288
[1mStep[0m  [32/84], [94mLoss[0m : 10.48158
[1mStep[0m  [40/84], [94mLoss[0m : 11.01439
[1mStep[0m  [48/84], [94mLoss[0m : 10.56581
[1mStep[0m  [56/84], [94mLoss[0m : 10.45474
[1mStep[0m  [64/84], [94mLoss[0m : 10.01160
[1mStep[0m  [72/84], [94mLoss[0m : 10.75568
[1mStep[0m  [80/84], [94mLoss[0m : 10.72693

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.876, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.98923
[1mStep[0m  [8/84], [94mLoss[0m : 9.77352
[1mStep[0m  [16/84], [94mLoss[0m : 9.82142
[1mStep[0m  [24/84], [94mLoss[0m : 10.15882
[1mStep[0m  [32/84], [94mLoss[0m : 9.92407
[1mStep[0m  [40/84], [94mLoss[0m : 10.26304
[1mStep[0m  [48/84], [94mLoss[0m : 9.65822
[1mStep[0m  [56/84], [94mLoss[0m : 9.31758
[1mStep[0m  [64/84], [94mLoss[0m : 9.68611
[1mStep[0m  [72/84], [94mLoss[0m : 10.39985
[1mStep[0m  [80/84], [94mLoss[0m : 9.87659

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.993, [92mTest[0m: 10.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.41635
[1mStep[0m  [8/84], [94mLoss[0m : 9.29943
[1mStep[0m  [16/84], [94mLoss[0m : 9.27115
[1mStep[0m  [24/84], [94mLoss[0m : 8.84186
[1mStep[0m  [32/84], [94mLoss[0m : 9.21125
[1mStep[0m  [40/84], [94mLoss[0m : 9.08674
[1mStep[0m  [48/84], [94mLoss[0m : 9.13270
[1mStep[0m  [56/84], [94mLoss[0m : 8.61470
[1mStep[0m  [64/84], [94mLoss[0m : 9.49410
[1mStep[0m  [72/84], [94mLoss[0m : 9.34867
[1mStep[0m  [80/84], [94mLoss[0m : 8.70782

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.258, [92mTest[0m: 9.875, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.12584
[1mStep[0m  [8/84], [94mLoss[0m : 8.86266
[1mStep[0m  [16/84], [94mLoss[0m : 8.58169
[1mStep[0m  [24/84], [94mLoss[0m : 8.84414
[1mStep[0m  [32/84], [94mLoss[0m : 8.68833
[1mStep[0m  [40/84], [94mLoss[0m : 8.83829
[1mStep[0m  [48/84], [94mLoss[0m : 8.11365
[1mStep[0m  [56/84], [94mLoss[0m : 8.12552
[1mStep[0m  [64/84], [94mLoss[0m : 8.97819
[1mStep[0m  [72/84], [94mLoss[0m : 7.65775
[1mStep[0m  [80/84], [94mLoss[0m : 8.32084

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.504, [92mTest[0m: 9.286, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.29414
[1mStep[0m  [8/84], [94mLoss[0m : 8.31770
[1mStep[0m  [16/84], [94mLoss[0m : 7.18509
[1mStep[0m  [24/84], [94mLoss[0m : 7.57667
[1mStep[0m  [32/84], [94mLoss[0m : 7.50557
[1mStep[0m  [40/84], [94mLoss[0m : 8.25682
[1mStep[0m  [48/84], [94mLoss[0m : 7.55889
[1mStep[0m  [56/84], [94mLoss[0m : 7.42003
[1mStep[0m  [64/84], [94mLoss[0m : 7.74329
[1mStep[0m  [72/84], [94mLoss[0m : 7.01735
[1mStep[0m  [80/84], [94mLoss[0m : 7.35610

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.736, [92mTest[0m: 8.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.91251
[1mStep[0m  [8/84], [94mLoss[0m : 7.25134
[1mStep[0m  [16/84], [94mLoss[0m : 6.89103
[1mStep[0m  [24/84], [94mLoss[0m : 7.44914
[1mStep[0m  [32/84], [94mLoss[0m : 7.35471
[1mStep[0m  [40/84], [94mLoss[0m : 7.09327
[1mStep[0m  [48/84], [94mLoss[0m : 6.52030
[1mStep[0m  [56/84], [94mLoss[0m : 7.19396
[1mStep[0m  [64/84], [94mLoss[0m : 7.00610
[1mStep[0m  [72/84], [94mLoss[0m : 6.30371
[1mStep[0m  [80/84], [94mLoss[0m : 6.49101

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.936, [92mTest[0m: 8.034, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.16895
[1mStep[0m  [8/84], [94mLoss[0m : 6.74253
[1mStep[0m  [16/84], [94mLoss[0m : 6.36193
[1mStep[0m  [24/84], [94mLoss[0m : 6.39972
[1mStep[0m  [32/84], [94mLoss[0m : 6.16833
[1mStep[0m  [40/84], [94mLoss[0m : 6.29803
[1mStep[0m  [48/84], [94mLoss[0m : 6.21736
[1mStep[0m  [56/84], [94mLoss[0m : 6.09743
[1mStep[0m  [64/84], [94mLoss[0m : 6.14636
[1mStep[0m  [72/84], [94mLoss[0m : 5.43793
[1mStep[0m  [80/84], [94mLoss[0m : 5.47436

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.148, [92mTest[0m: 7.340, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.97949
[1mStep[0m  [8/84], [94mLoss[0m : 5.85656
[1mStep[0m  [16/84], [94mLoss[0m : 6.01450
[1mStep[0m  [24/84], [94mLoss[0m : 5.18478
[1mStep[0m  [32/84], [94mLoss[0m : 4.87319
[1mStep[0m  [40/84], [94mLoss[0m : 5.45792
[1mStep[0m  [48/84], [94mLoss[0m : 5.67122
[1mStep[0m  [56/84], [94mLoss[0m : 5.26464
[1mStep[0m  [64/84], [94mLoss[0m : 4.47550
[1mStep[0m  [72/84], [94mLoss[0m : 4.96698
[1mStep[0m  [80/84], [94mLoss[0m : 5.33759

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.356, [92mTest[0m: 6.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.45652
[1mStep[0m  [8/84], [94mLoss[0m : 5.13829
[1mStep[0m  [16/84], [94mLoss[0m : 4.45059
[1mStep[0m  [24/84], [94mLoss[0m : 4.96396
[1mStep[0m  [32/84], [94mLoss[0m : 4.98622
[1mStep[0m  [40/84], [94mLoss[0m : 5.01446
[1mStep[0m  [48/84], [94mLoss[0m : 4.83187
[1mStep[0m  [56/84], [94mLoss[0m : 4.40864
[1mStep[0m  [64/84], [94mLoss[0m : 4.58810
[1mStep[0m  [72/84], [94mLoss[0m : 4.45465
[1mStep[0m  [80/84], [94mLoss[0m : 4.25434

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.642, [92mTest[0m: 5.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.41088
[1mStep[0m  [8/84], [94mLoss[0m : 3.93202
[1mStep[0m  [16/84], [94mLoss[0m : 4.06073
[1mStep[0m  [24/84], [94mLoss[0m : 3.90883
[1mStep[0m  [32/84], [94mLoss[0m : 3.92106
[1mStep[0m  [40/84], [94mLoss[0m : 4.11542
[1mStep[0m  [48/84], [94mLoss[0m : 4.26753
[1mStep[0m  [56/84], [94mLoss[0m : 3.43155
[1mStep[0m  [64/84], [94mLoss[0m : 3.85878
[1mStep[0m  [72/84], [94mLoss[0m : 3.60374
[1mStep[0m  [80/84], [94mLoss[0m : 3.56928

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.042, [92mTest[0m: 4.911, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.45707
[1mStep[0m  [8/84], [94mLoss[0m : 3.55510
[1mStep[0m  [16/84], [94mLoss[0m : 3.25768
[1mStep[0m  [24/84], [94mLoss[0m : 3.68258
[1mStep[0m  [32/84], [94mLoss[0m : 3.52423
[1mStep[0m  [40/84], [94mLoss[0m : 3.88480
[1mStep[0m  [48/84], [94mLoss[0m : 3.85328
[1mStep[0m  [56/84], [94mLoss[0m : 3.64408
[1mStep[0m  [64/84], [94mLoss[0m : 3.43848
[1mStep[0m  [72/84], [94mLoss[0m : 3.43849
[1mStep[0m  [80/84], [94mLoss[0m : 3.32211

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.567, [92mTest[0m: 4.256, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.41571
[1mStep[0m  [8/84], [94mLoss[0m : 3.36153
[1mStep[0m  [16/84], [94mLoss[0m : 3.17886
[1mStep[0m  [24/84], [94mLoss[0m : 3.05788
[1mStep[0m  [32/84], [94mLoss[0m : 3.46105
[1mStep[0m  [40/84], [94mLoss[0m : 3.16519
[1mStep[0m  [48/84], [94mLoss[0m : 2.88026
[1mStep[0m  [56/84], [94mLoss[0m : 3.52289
[1mStep[0m  [64/84], [94mLoss[0m : 3.20539
[1mStep[0m  [72/84], [94mLoss[0m : 2.72462
[1mStep[0m  [80/84], [94mLoss[0m : 3.33170

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.260, [92mTest[0m: 3.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.33777
[1mStep[0m  [8/84], [94mLoss[0m : 3.09420
[1mStep[0m  [16/84], [94mLoss[0m : 2.86645
[1mStep[0m  [24/84], [94mLoss[0m : 3.49841
[1mStep[0m  [32/84], [94mLoss[0m : 3.30068
[1mStep[0m  [40/84], [94mLoss[0m : 2.90996
[1mStep[0m  [48/84], [94mLoss[0m : 3.30802
[1mStep[0m  [56/84], [94mLoss[0m : 3.14136
[1mStep[0m  [64/84], [94mLoss[0m : 3.53836
[1mStep[0m  [72/84], [94mLoss[0m : 3.07399
[1mStep[0m  [80/84], [94mLoss[0m : 3.26238

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.054, [92mTest[0m: 3.345, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68785
[1mStep[0m  [8/84], [94mLoss[0m : 2.71746
[1mStep[0m  [16/84], [94mLoss[0m : 2.79573
[1mStep[0m  [24/84], [94mLoss[0m : 2.99498
[1mStep[0m  [32/84], [94mLoss[0m : 2.92942
[1mStep[0m  [40/84], [94mLoss[0m : 2.81972
[1mStep[0m  [48/84], [94mLoss[0m : 2.68064
[1mStep[0m  [56/84], [94mLoss[0m : 3.02152
[1mStep[0m  [64/84], [94mLoss[0m : 2.81996
[1mStep[0m  [72/84], [94mLoss[0m : 2.84043
[1mStep[0m  [80/84], [94mLoss[0m : 3.10001

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.886, [92mTest[0m: 3.054, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67861
[1mStep[0m  [8/84], [94mLoss[0m : 2.27595
[1mStep[0m  [16/84], [94mLoss[0m : 2.78864
[1mStep[0m  [24/84], [94mLoss[0m : 2.78837
[1mStep[0m  [32/84], [94mLoss[0m : 3.09107
[1mStep[0m  [40/84], [94mLoss[0m : 2.65996
[1mStep[0m  [48/84], [94mLoss[0m : 2.83660
[1mStep[0m  [56/84], [94mLoss[0m : 2.81188
[1mStep[0m  [64/84], [94mLoss[0m : 2.32512
[1mStep[0m  [72/84], [94mLoss[0m : 2.82192
[1mStep[0m  [80/84], [94mLoss[0m : 2.69419

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.805, [92mTest[0m: 2.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86874
[1mStep[0m  [8/84], [94mLoss[0m : 2.77704
[1mStep[0m  [16/84], [94mLoss[0m : 2.59324
[1mStep[0m  [24/84], [94mLoss[0m : 2.74824
[1mStep[0m  [32/84], [94mLoss[0m : 2.69032
[1mStep[0m  [40/84], [94mLoss[0m : 2.85300
[1mStep[0m  [48/84], [94mLoss[0m : 3.38539
[1mStep[0m  [56/84], [94mLoss[0m : 2.85174
[1mStep[0m  [64/84], [94mLoss[0m : 2.71140
[1mStep[0m  [72/84], [94mLoss[0m : 2.74036
[1mStep[0m  [80/84], [94mLoss[0m : 2.78962

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.790, [92mTest[0m: 2.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64590
[1mStep[0m  [8/84], [94mLoss[0m : 2.92467
[1mStep[0m  [16/84], [94mLoss[0m : 2.70331
[1mStep[0m  [24/84], [94mLoss[0m : 3.03085
[1mStep[0m  [32/84], [94mLoss[0m : 2.70884
[1mStep[0m  [40/84], [94mLoss[0m : 2.95981
[1mStep[0m  [48/84], [94mLoss[0m : 2.99354
[1mStep[0m  [56/84], [94mLoss[0m : 2.44839
[1mStep[0m  [64/84], [94mLoss[0m : 2.87223
[1mStep[0m  [72/84], [94mLoss[0m : 2.93355
[1mStep[0m  [80/84], [94mLoss[0m : 2.67976

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.737, [92mTest[0m: 2.660, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95619
[1mStep[0m  [8/84], [94mLoss[0m : 2.43096
[1mStep[0m  [16/84], [94mLoss[0m : 2.84719
[1mStep[0m  [24/84], [94mLoss[0m : 2.51781
[1mStep[0m  [32/84], [94mLoss[0m : 2.57945
[1mStep[0m  [40/84], [94mLoss[0m : 2.67125
[1mStep[0m  [48/84], [94mLoss[0m : 2.77725
[1mStep[0m  [56/84], [94mLoss[0m : 2.55590
[1mStep[0m  [64/84], [94mLoss[0m : 2.68479
[1mStep[0m  [72/84], [94mLoss[0m : 2.32812
[1mStep[0m  [80/84], [94mLoss[0m : 2.92931

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.04889
[1mStep[0m  [8/84], [94mLoss[0m : 2.69849
[1mStep[0m  [16/84], [94mLoss[0m : 2.71360
[1mStep[0m  [24/84], [94mLoss[0m : 2.78972
[1mStep[0m  [32/84], [94mLoss[0m : 2.55023
[1mStep[0m  [40/84], [94mLoss[0m : 2.53903
[1mStep[0m  [48/84], [94mLoss[0m : 2.39619
[1mStep[0m  [56/84], [94mLoss[0m : 2.63890
[1mStep[0m  [64/84], [94mLoss[0m : 2.79485
[1mStep[0m  [72/84], [94mLoss[0m : 2.84504
[1mStep[0m  [80/84], [94mLoss[0m : 2.98672

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.528, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41389
[1mStep[0m  [8/84], [94mLoss[0m : 2.71033
[1mStep[0m  [16/84], [94mLoss[0m : 2.53051
[1mStep[0m  [24/84], [94mLoss[0m : 2.87194
[1mStep[0m  [32/84], [94mLoss[0m : 2.65899
[1mStep[0m  [40/84], [94mLoss[0m : 2.35640
[1mStep[0m  [48/84], [94mLoss[0m : 3.21329
[1mStep[0m  [56/84], [94mLoss[0m : 2.73600
[1mStep[0m  [64/84], [94mLoss[0m : 2.84219
[1mStep[0m  [72/84], [94mLoss[0m : 2.71953
[1mStep[0m  [80/84], [94mLoss[0m : 2.74821

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.514, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96649
[1mStep[0m  [8/84], [94mLoss[0m : 2.73710
[1mStep[0m  [16/84], [94mLoss[0m : 2.44912
[1mStep[0m  [24/84], [94mLoss[0m : 2.81361
[1mStep[0m  [32/84], [94mLoss[0m : 2.53958
[1mStep[0m  [40/84], [94mLoss[0m : 2.73918
[1mStep[0m  [48/84], [94mLoss[0m : 2.38448
[1mStep[0m  [56/84], [94mLoss[0m : 2.72466
[1mStep[0m  [64/84], [94mLoss[0m : 3.10094
[1mStep[0m  [72/84], [94mLoss[0m : 2.42741
[1mStep[0m  [80/84], [94mLoss[0m : 2.58725

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.484, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80304
[1mStep[0m  [8/84], [94mLoss[0m : 2.52574
[1mStep[0m  [16/84], [94mLoss[0m : 2.33865
[1mStep[0m  [24/84], [94mLoss[0m : 2.66718
[1mStep[0m  [32/84], [94mLoss[0m : 2.56921
[1mStep[0m  [40/84], [94mLoss[0m : 2.69207
[1mStep[0m  [48/84], [94mLoss[0m : 2.54887
[1mStep[0m  [56/84], [94mLoss[0m : 2.65596
[1mStep[0m  [64/84], [94mLoss[0m : 2.53004
[1mStep[0m  [72/84], [94mLoss[0m : 2.54030
[1mStep[0m  [80/84], [94mLoss[0m : 2.58237

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.476, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61472
[1mStep[0m  [8/84], [94mLoss[0m : 2.75765
[1mStep[0m  [16/84], [94mLoss[0m : 2.60790
[1mStep[0m  [24/84], [94mLoss[0m : 2.41440
[1mStep[0m  [32/84], [94mLoss[0m : 2.71718
[1mStep[0m  [40/84], [94mLoss[0m : 2.74686
[1mStep[0m  [48/84], [94mLoss[0m : 2.71866
[1mStep[0m  [56/84], [94mLoss[0m : 2.92660
[1mStep[0m  [64/84], [94mLoss[0m : 2.56644
[1mStep[0m  [72/84], [94mLoss[0m : 2.46678
[1mStep[0m  [80/84], [94mLoss[0m : 2.28953

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.464, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74460
[1mStep[0m  [8/84], [94mLoss[0m : 2.84177
[1mStep[0m  [16/84], [94mLoss[0m : 2.40095
[1mStep[0m  [24/84], [94mLoss[0m : 2.77505
[1mStep[0m  [32/84], [94mLoss[0m : 2.66983
[1mStep[0m  [40/84], [94mLoss[0m : 2.70577
[1mStep[0m  [48/84], [94mLoss[0m : 2.90543
[1mStep[0m  [56/84], [94mLoss[0m : 2.61554
[1mStep[0m  [64/84], [94mLoss[0m : 2.77403
[1mStep[0m  [72/84], [94mLoss[0m : 2.71802
[1mStep[0m  [80/84], [94mLoss[0m : 2.60552

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.449, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33888
[1mStep[0m  [8/84], [94mLoss[0m : 2.50910
[1mStep[0m  [16/84], [94mLoss[0m : 2.69632
[1mStep[0m  [24/84], [94mLoss[0m : 2.56366
[1mStep[0m  [32/84], [94mLoss[0m : 2.67302
[1mStep[0m  [40/84], [94mLoss[0m : 2.32170
[1mStep[0m  [48/84], [94mLoss[0m : 2.51519
[1mStep[0m  [56/84], [94mLoss[0m : 2.96980
[1mStep[0m  [64/84], [94mLoss[0m : 2.56570
[1mStep[0m  [72/84], [94mLoss[0m : 2.82971
[1mStep[0m  [80/84], [94mLoss[0m : 2.76547

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.436, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45962
[1mStep[0m  [8/84], [94mLoss[0m : 2.87467
[1mStep[0m  [16/84], [94mLoss[0m : 2.89709
[1mStep[0m  [24/84], [94mLoss[0m : 2.71134
[1mStep[0m  [32/84], [94mLoss[0m : 2.66573
[1mStep[0m  [40/84], [94mLoss[0m : 2.64816
[1mStep[0m  [48/84], [94mLoss[0m : 2.65925
[1mStep[0m  [56/84], [94mLoss[0m : 2.61859
[1mStep[0m  [64/84], [94mLoss[0m : 2.49813
[1mStep[0m  [72/84], [94mLoss[0m : 2.83061
[1mStep[0m  [80/84], [94mLoss[0m : 2.70013

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71261
[1mStep[0m  [8/84], [94mLoss[0m : 2.89144
[1mStep[0m  [16/84], [94mLoss[0m : 2.76404
[1mStep[0m  [24/84], [94mLoss[0m : 2.23853
[1mStep[0m  [32/84], [94mLoss[0m : 2.48148
[1mStep[0m  [40/84], [94mLoss[0m : 2.66589
[1mStep[0m  [48/84], [94mLoss[0m : 2.60896
[1mStep[0m  [56/84], [94mLoss[0m : 2.62317
[1mStep[0m  [64/84], [94mLoss[0m : 2.26068
[1mStep[0m  [72/84], [94mLoss[0m : 2.56189
[1mStep[0m  [80/84], [94mLoss[0m : 2.88144

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.422, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81169
[1mStep[0m  [8/84], [94mLoss[0m : 2.71292
[1mStep[0m  [16/84], [94mLoss[0m : 2.72741
[1mStep[0m  [24/84], [94mLoss[0m : 2.51147
[1mStep[0m  [32/84], [94mLoss[0m : 2.55213
[1mStep[0m  [40/84], [94mLoss[0m : 2.91409
[1mStep[0m  [48/84], [94mLoss[0m : 2.50420
[1mStep[0m  [56/84], [94mLoss[0m : 2.82770
[1mStep[0m  [64/84], [94mLoss[0m : 2.58415
[1mStep[0m  [72/84], [94mLoss[0m : 2.52635
[1mStep[0m  [80/84], [94mLoss[0m : 2.83891

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71767
[1mStep[0m  [8/84], [94mLoss[0m : 2.29673
[1mStep[0m  [16/84], [94mLoss[0m : 2.66384
[1mStep[0m  [24/84], [94mLoss[0m : 2.74152
[1mStep[0m  [32/84], [94mLoss[0m : 2.38794
[1mStep[0m  [40/84], [94mLoss[0m : 2.34178
[1mStep[0m  [48/84], [94mLoss[0m : 2.67475
[1mStep[0m  [56/84], [94mLoss[0m : 2.79628
[1mStep[0m  [64/84], [94mLoss[0m : 2.31488
[1mStep[0m  [72/84], [94mLoss[0m : 2.52784
[1mStep[0m  [80/84], [94mLoss[0m : 2.63031

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.424, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48897
[1mStep[0m  [8/84], [94mLoss[0m : 2.54500
[1mStep[0m  [16/84], [94mLoss[0m : 2.79874
[1mStep[0m  [24/84], [94mLoss[0m : 2.70152
[1mStep[0m  [32/84], [94mLoss[0m : 2.68073
[1mStep[0m  [40/84], [94mLoss[0m : 2.91722
[1mStep[0m  [48/84], [94mLoss[0m : 2.33086
[1mStep[0m  [56/84], [94mLoss[0m : 2.74551
[1mStep[0m  [64/84], [94mLoss[0m : 2.60851
[1mStep[0m  [72/84], [94mLoss[0m : 2.49442
[1mStep[0m  [80/84], [94mLoss[0m : 2.56692

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.410, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.409
====================================

Phase 1 - Evaluation MAE:  2.4091549004827226
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.73152
[1mStep[0m  [8/84], [94mLoss[0m : 2.61361
[1mStep[0m  [16/84], [94mLoss[0m : 2.51478
[1mStep[0m  [24/84], [94mLoss[0m : 2.80711
[1mStep[0m  [32/84], [94mLoss[0m : 2.62533
[1mStep[0m  [40/84], [94mLoss[0m : 2.52941
[1mStep[0m  [48/84], [94mLoss[0m : 2.84303
[1mStep[0m  [56/84], [94mLoss[0m : 2.48954
[1mStep[0m  [64/84], [94mLoss[0m : 2.42955
[1mStep[0m  [72/84], [94mLoss[0m : 2.61761
[1mStep[0m  [80/84], [94mLoss[0m : 2.71298

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.408, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37014
[1mStep[0m  [8/84], [94mLoss[0m : 2.74610
[1mStep[0m  [16/84], [94mLoss[0m : 2.60850
[1mStep[0m  [24/84], [94mLoss[0m : 2.45702
[1mStep[0m  [32/84], [94mLoss[0m : 3.01131
[1mStep[0m  [40/84], [94mLoss[0m : 2.67499
[1mStep[0m  [48/84], [94mLoss[0m : 2.79595
[1mStep[0m  [56/84], [94mLoss[0m : 2.83617
[1mStep[0m  [64/84], [94mLoss[0m : 3.18760
[1mStep[0m  [72/84], [94mLoss[0m : 2.78428
[1mStep[0m  [80/84], [94mLoss[0m : 2.50761

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.745, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99242
[1mStep[0m  [8/84], [94mLoss[0m : 2.86626
[1mStep[0m  [16/84], [94mLoss[0m : 2.62031
[1mStep[0m  [24/84], [94mLoss[0m : 2.76300
[1mStep[0m  [32/84], [94mLoss[0m : 2.47021
[1mStep[0m  [40/84], [94mLoss[0m : 2.35200
[1mStep[0m  [48/84], [94mLoss[0m : 2.81917
[1mStep[0m  [56/84], [94mLoss[0m : 2.82018
[1mStep[0m  [64/84], [94mLoss[0m : 2.81365
[1mStep[0m  [72/84], [94mLoss[0m : 3.04788
[1mStep[0m  [80/84], [94mLoss[0m : 2.92142

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.984, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61653
[1mStep[0m  [8/84], [94mLoss[0m : 2.50723
[1mStep[0m  [16/84], [94mLoss[0m : 2.60199
[1mStep[0m  [24/84], [94mLoss[0m : 2.70972
[1mStep[0m  [32/84], [94mLoss[0m : 2.54757
[1mStep[0m  [40/84], [94mLoss[0m : 2.60641
[1mStep[0m  [48/84], [94mLoss[0m : 2.42959
[1mStep[0m  [56/84], [94mLoss[0m : 2.16544
[1mStep[0m  [64/84], [94mLoss[0m : 2.41885
[1mStep[0m  [72/84], [94mLoss[0m : 2.69981
[1mStep[0m  [80/84], [94mLoss[0m : 2.23043

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.635, [92mTest[0m: 3.187, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70734
[1mStep[0m  [8/84], [94mLoss[0m : 2.56244
[1mStep[0m  [16/84], [94mLoss[0m : 2.80827
[1mStep[0m  [24/84], [94mLoss[0m : 2.36021
[1mStep[0m  [32/84], [94mLoss[0m : 2.55310
[1mStep[0m  [40/84], [94mLoss[0m : 2.19332
[1mStep[0m  [48/84], [94mLoss[0m : 2.60543
[1mStep[0m  [56/84], [94mLoss[0m : 2.41310
[1mStep[0m  [64/84], [94mLoss[0m : 2.67622
[1mStep[0m  [72/84], [94mLoss[0m : 2.35063
[1mStep[0m  [80/84], [94mLoss[0m : 2.37083

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.640, [92mTest[0m: 3.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57758
[1mStep[0m  [8/84], [94mLoss[0m : 2.73121
[1mStep[0m  [16/84], [94mLoss[0m : 2.63579
[1mStep[0m  [24/84], [94mLoss[0m : 2.68360
[1mStep[0m  [32/84], [94mLoss[0m : 2.58393
[1mStep[0m  [40/84], [94mLoss[0m : 2.19701
[1mStep[0m  [48/84], [94mLoss[0m : 2.56196
[1mStep[0m  [56/84], [94mLoss[0m : 2.79787
[1mStep[0m  [64/84], [94mLoss[0m : 2.94788
[1mStep[0m  [72/84], [94mLoss[0m : 2.42995
[1mStep[0m  [80/84], [94mLoss[0m : 2.56173

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.598, [92mTest[0m: 3.254, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58081
[1mStep[0m  [8/84], [94mLoss[0m : 2.31657
[1mStep[0m  [16/84], [94mLoss[0m : 2.81508
[1mStep[0m  [24/84], [94mLoss[0m : 2.59233
[1mStep[0m  [32/84], [94mLoss[0m : 2.55602
[1mStep[0m  [40/84], [94mLoss[0m : 2.41629
[1mStep[0m  [48/84], [94mLoss[0m : 2.73519
[1mStep[0m  [56/84], [94mLoss[0m : 2.51481
[1mStep[0m  [64/84], [94mLoss[0m : 2.81636
[1mStep[0m  [72/84], [94mLoss[0m : 2.55116
[1mStep[0m  [80/84], [94mLoss[0m : 2.73801

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.610, [92mTest[0m: 3.277, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60521
[1mStep[0m  [8/84], [94mLoss[0m : 2.69418
[1mStep[0m  [16/84], [94mLoss[0m : 2.76050
[1mStep[0m  [24/84], [94mLoss[0m : 2.77032
[1mStep[0m  [32/84], [94mLoss[0m : 2.73156
[1mStep[0m  [40/84], [94mLoss[0m : 2.40485
[1mStep[0m  [48/84], [94mLoss[0m : 2.77634
[1mStep[0m  [56/84], [94mLoss[0m : 2.97264
[1mStep[0m  [64/84], [94mLoss[0m : 2.51701
[1mStep[0m  [72/84], [94mLoss[0m : 2.63178
[1mStep[0m  [80/84], [94mLoss[0m : 2.70358

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.607, [92mTest[0m: 3.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57057
[1mStep[0m  [8/84], [94mLoss[0m : 2.46601
[1mStep[0m  [16/84], [94mLoss[0m : 2.40081
[1mStep[0m  [24/84], [94mLoss[0m : 2.50628
[1mStep[0m  [32/84], [94mLoss[0m : 2.60092
[1mStep[0m  [40/84], [94mLoss[0m : 2.82556
[1mStep[0m  [48/84], [94mLoss[0m : 2.63743
[1mStep[0m  [56/84], [94mLoss[0m : 2.59597
[1mStep[0m  [64/84], [94mLoss[0m : 2.45427
[1mStep[0m  [72/84], [94mLoss[0m : 2.78383
[1mStep[0m  [80/84], [94mLoss[0m : 2.55171

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.584, [92mTest[0m: 3.222, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24661
[1mStep[0m  [8/84], [94mLoss[0m : 2.69654
[1mStep[0m  [16/84], [94mLoss[0m : 2.55047
[1mStep[0m  [24/84], [94mLoss[0m : 2.50729
[1mStep[0m  [32/84], [94mLoss[0m : 2.50427
[1mStep[0m  [40/84], [94mLoss[0m : 2.63188
[1mStep[0m  [48/84], [94mLoss[0m : 2.38277
[1mStep[0m  [56/84], [94mLoss[0m : 2.42182
[1mStep[0m  [64/84], [94mLoss[0m : 2.54089
[1mStep[0m  [72/84], [94mLoss[0m : 2.68442
[1mStep[0m  [80/84], [94mLoss[0m : 2.91435

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.568, [92mTest[0m: 3.342, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66962
[1mStep[0m  [8/84], [94mLoss[0m : 2.41651
[1mStep[0m  [16/84], [94mLoss[0m : 2.59806
[1mStep[0m  [24/84], [94mLoss[0m : 2.85402
[1mStep[0m  [32/84], [94mLoss[0m : 2.73379
[1mStep[0m  [40/84], [94mLoss[0m : 2.40912
[1mStep[0m  [48/84], [94mLoss[0m : 2.53229
[1mStep[0m  [56/84], [94mLoss[0m : 2.21675
[1mStep[0m  [64/84], [94mLoss[0m : 2.56730
[1mStep[0m  [72/84], [94mLoss[0m : 2.40049
[1mStep[0m  [80/84], [94mLoss[0m : 2.48397

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.578, [92mTest[0m: 3.132, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80326
[1mStep[0m  [8/84], [94mLoss[0m : 2.65389
[1mStep[0m  [16/84], [94mLoss[0m : 2.70071
[1mStep[0m  [24/84], [94mLoss[0m : 2.54914
[1mStep[0m  [32/84], [94mLoss[0m : 2.55274
[1mStep[0m  [40/84], [94mLoss[0m : 2.73013
[1mStep[0m  [48/84], [94mLoss[0m : 2.78377
[1mStep[0m  [56/84], [94mLoss[0m : 2.85101
[1mStep[0m  [64/84], [94mLoss[0m : 2.37599
[1mStep[0m  [72/84], [94mLoss[0m : 2.39268
[1mStep[0m  [80/84], [94mLoss[0m : 2.53732

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.564, [92mTest[0m: 3.132, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42087
[1mStep[0m  [8/84], [94mLoss[0m : 2.53245
[1mStep[0m  [16/84], [94mLoss[0m : 2.69259
[1mStep[0m  [24/84], [94mLoss[0m : 2.81836
[1mStep[0m  [32/84], [94mLoss[0m : 2.83002
[1mStep[0m  [40/84], [94mLoss[0m : 2.69474
[1mStep[0m  [48/84], [94mLoss[0m : 2.57619
[1mStep[0m  [56/84], [94mLoss[0m : 2.55246
[1mStep[0m  [64/84], [94mLoss[0m : 2.24359
[1mStep[0m  [72/84], [94mLoss[0m : 3.06114
[1mStep[0m  [80/84], [94mLoss[0m : 2.78654

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.977, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61814
[1mStep[0m  [8/84], [94mLoss[0m : 2.68174
[1mStep[0m  [16/84], [94mLoss[0m : 2.71461
[1mStep[0m  [24/84], [94mLoss[0m : 2.49302
[1mStep[0m  [32/84], [94mLoss[0m : 2.55533
[1mStep[0m  [40/84], [94mLoss[0m : 2.52336
[1mStep[0m  [48/84], [94mLoss[0m : 2.49182
[1mStep[0m  [56/84], [94mLoss[0m : 2.62987
[1mStep[0m  [64/84], [94mLoss[0m : 2.63342
[1mStep[0m  [72/84], [94mLoss[0m : 2.54022
[1mStep[0m  [80/84], [94mLoss[0m : 2.48203

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.945, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55822
[1mStep[0m  [8/84], [94mLoss[0m : 2.64347
[1mStep[0m  [16/84], [94mLoss[0m : 2.67904
[1mStep[0m  [24/84], [94mLoss[0m : 2.30586
[1mStep[0m  [32/84], [94mLoss[0m : 2.83201
[1mStep[0m  [40/84], [94mLoss[0m : 2.54405
[1mStep[0m  [48/84], [94mLoss[0m : 2.47216
[1mStep[0m  [56/84], [94mLoss[0m : 2.67344
[1mStep[0m  [64/84], [94mLoss[0m : 2.66534
[1mStep[0m  [72/84], [94mLoss[0m : 2.72277
[1mStep[0m  [80/84], [94mLoss[0m : 2.16278

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.880, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62065
[1mStep[0m  [8/84], [94mLoss[0m : 2.33353
[1mStep[0m  [16/84], [94mLoss[0m : 2.50559
[1mStep[0m  [24/84], [94mLoss[0m : 2.45552
[1mStep[0m  [32/84], [94mLoss[0m : 2.57519
[1mStep[0m  [40/84], [94mLoss[0m : 2.60156
[1mStep[0m  [48/84], [94mLoss[0m : 2.64478
[1mStep[0m  [56/84], [94mLoss[0m : 2.57583
[1mStep[0m  [64/84], [94mLoss[0m : 2.31764
[1mStep[0m  [72/84], [94mLoss[0m : 2.66954
[1mStep[0m  [80/84], [94mLoss[0m : 2.75412

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.540, [92mTest[0m: 3.076, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61242
[1mStep[0m  [8/84], [94mLoss[0m : 2.10494
[1mStep[0m  [16/84], [94mLoss[0m : 2.48331
[1mStep[0m  [24/84], [94mLoss[0m : 2.45389
[1mStep[0m  [32/84], [94mLoss[0m : 2.47166
[1mStep[0m  [40/84], [94mLoss[0m : 2.32273
[1mStep[0m  [48/84], [94mLoss[0m : 2.55683
[1mStep[0m  [56/84], [94mLoss[0m : 2.41848
[1mStep[0m  [64/84], [94mLoss[0m : 2.52255
[1mStep[0m  [72/84], [94mLoss[0m : 2.39494
[1mStep[0m  [80/84], [94mLoss[0m : 2.39681

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.523, [92mTest[0m: 3.033, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67839
[1mStep[0m  [8/84], [94mLoss[0m : 2.83767
[1mStep[0m  [16/84], [94mLoss[0m : 2.39919
[1mStep[0m  [24/84], [94mLoss[0m : 2.40549
[1mStep[0m  [32/84], [94mLoss[0m : 2.30396
[1mStep[0m  [40/84], [94mLoss[0m : 2.52583
[1mStep[0m  [48/84], [94mLoss[0m : 2.74233
[1mStep[0m  [56/84], [94mLoss[0m : 2.36370
[1mStep[0m  [64/84], [94mLoss[0m : 2.58390
[1mStep[0m  [72/84], [94mLoss[0m : 2.28741
[1mStep[0m  [80/84], [94mLoss[0m : 2.44512

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.970, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56073
[1mStep[0m  [8/84], [94mLoss[0m : 2.36413
[1mStep[0m  [16/84], [94mLoss[0m : 2.11040
[1mStep[0m  [24/84], [94mLoss[0m : 2.36818
[1mStep[0m  [32/84], [94mLoss[0m : 2.40406
[1mStep[0m  [40/84], [94mLoss[0m : 2.56063
[1mStep[0m  [48/84], [94mLoss[0m : 2.50238
[1mStep[0m  [56/84], [94mLoss[0m : 2.31889
[1mStep[0m  [64/84], [94mLoss[0m : 2.80884
[1mStep[0m  [72/84], [94mLoss[0m : 2.57723
[1mStep[0m  [80/84], [94mLoss[0m : 2.49515

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.877, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38012
[1mStep[0m  [8/84], [94mLoss[0m : 3.00449
[1mStep[0m  [16/84], [94mLoss[0m : 2.67160
[1mStep[0m  [24/84], [94mLoss[0m : 2.81151
[1mStep[0m  [32/84], [94mLoss[0m : 2.41628
[1mStep[0m  [40/84], [94mLoss[0m : 2.42686
[1mStep[0m  [48/84], [94mLoss[0m : 2.53893
[1mStep[0m  [56/84], [94mLoss[0m : 2.53698
[1mStep[0m  [64/84], [94mLoss[0m : 2.61764
[1mStep[0m  [72/84], [94mLoss[0m : 2.24871
[1mStep[0m  [80/84], [94mLoss[0m : 2.55552

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.822, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49742
[1mStep[0m  [8/84], [94mLoss[0m : 2.63680
[1mStep[0m  [16/84], [94mLoss[0m : 2.53416
[1mStep[0m  [24/84], [94mLoss[0m : 2.47571
[1mStep[0m  [32/84], [94mLoss[0m : 2.03164
[1mStep[0m  [40/84], [94mLoss[0m : 2.66145
[1mStep[0m  [48/84], [94mLoss[0m : 2.36898
[1mStep[0m  [56/84], [94mLoss[0m : 2.29847
[1mStep[0m  [64/84], [94mLoss[0m : 2.52982
[1mStep[0m  [72/84], [94mLoss[0m : 2.37774
[1mStep[0m  [80/84], [94mLoss[0m : 2.32240

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.786, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33811
[1mStep[0m  [8/84], [94mLoss[0m : 2.42630
[1mStep[0m  [16/84], [94mLoss[0m : 2.22455
[1mStep[0m  [24/84], [94mLoss[0m : 2.42813
[1mStep[0m  [32/84], [94mLoss[0m : 2.54163
[1mStep[0m  [40/84], [94mLoss[0m : 2.55914
[1mStep[0m  [48/84], [94mLoss[0m : 2.54173
[1mStep[0m  [56/84], [94mLoss[0m : 2.33027
[1mStep[0m  [64/84], [94mLoss[0m : 2.22580
[1mStep[0m  [72/84], [94mLoss[0m : 2.41373
[1mStep[0m  [80/84], [94mLoss[0m : 2.58036

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.781, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50130
[1mStep[0m  [8/84], [94mLoss[0m : 2.35269
[1mStep[0m  [16/84], [94mLoss[0m : 2.61557
[1mStep[0m  [24/84], [94mLoss[0m : 2.74618
[1mStep[0m  [32/84], [94mLoss[0m : 2.72293
[1mStep[0m  [40/84], [94mLoss[0m : 2.73885
[1mStep[0m  [48/84], [94mLoss[0m : 2.66802
[1mStep[0m  [56/84], [94mLoss[0m : 2.43915
[1mStep[0m  [64/84], [94mLoss[0m : 2.31580
[1mStep[0m  [72/84], [94mLoss[0m : 2.43587
[1mStep[0m  [80/84], [94mLoss[0m : 2.48511

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.745, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60555
[1mStep[0m  [8/84], [94mLoss[0m : 2.59533
[1mStep[0m  [16/84], [94mLoss[0m : 2.57174
[1mStep[0m  [24/84], [94mLoss[0m : 2.68566
[1mStep[0m  [32/84], [94mLoss[0m : 2.74592
[1mStep[0m  [40/84], [94mLoss[0m : 2.45953
[1mStep[0m  [48/84], [94mLoss[0m : 2.81669
[1mStep[0m  [56/84], [94mLoss[0m : 2.58367
[1mStep[0m  [64/84], [94mLoss[0m : 2.59913
[1mStep[0m  [72/84], [94mLoss[0m : 2.32389
[1mStep[0m  [80/84], [94mLoss[0m : 2.28901

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.621, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63751
[1mStep[0m  [8/84], [94mLoss[0m : 2.85376
[1mStep[0m  [16/84], [94mLoss[0m : 2.57512
[1mStep[0m  [24/84], [94mLoss[0m : 2.64757
[1mStep[0m  [32/84], [94mLoss[0m : 2.79534
[1mStep[0m  [40/84], [94mLoss[0m : 2.43324
[1mStep[0m  [48/84], [94mLoss[0m : 2.47146
[1mStep[0m  [56/84], [94mLoss[0m : 2.22414
[1mStep[0m  [64/84], [94mLoss[0m : 2.65669
[1mStep[0m  [72/84], [94mLoss[0m : 2.56804
[1mStep[0m  [80/84], [94mLoss[0m : 2.39494

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.727, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41043
[1mStep[0m  [8/84], [94mLoss[0m : 2.41249
[1mStep[0m  [16/84], [94mLoss[0m : 2.29571
[1mStep[0m  [24/84], [94mLoss[0m : 2.21551
[1mStep[0m  [32/84], [94mLoss[0m : 2.43854
[1mStep[0m  [40/84], [94mLoss[0m : 2.41027
[1mStep[0m  [48/84], [94mLoss[0m : 2.45842
[1mStep[0m  [56/84], [94mLoss[0m : 2.67407
[1mStep[0m  [64/84], [94mLoss[0m : 2.57144
[1mStep[0m  [72/84], [94mLoss[0m : 2.62343
[1mStep[0m  [80/84], [94mLoss[0m : 2.50688

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.643, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34812
[1mStep[0m  [8/84], [94mLoss[0m : 2.33885
[1mStep[0m  [16/84], [94mLoss[0m : 2.74206
[1mStep[0m  [24/84], [94mLoss[0m : 2.53452
[1mStep[0m  [32/84], [94mLoss[0m : 2.49466
[1mStep[0m  [40/84], [94mLoss[0m : 2.35598
[1mStep[0m  [48/84], [94mLoss[0m : 2.54440
[1mStep[0m  [56/84], [94mLoss[0m : 2.50432
[1mStep[0m  [64/84], [94mLoss[0m : 2.56869
[1mStep[0m  [72/84], [94mLoss[0m : 2.37592
[1mStep[0m  [80/84], [94mLoss[0m : 2.32436

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.608, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58560
[1mStep[0m  [8/84], [94mLoss[0m : 2.25592
[1mStep[0m  [16/84], [94mLoss[0m : 2.46304
[1mStep[0m  [24/84], [94mLoss[0m : 2.48968
[1mStep[0m  [32/84], [94mLoss[0m : 2.56185
[1mStep[0m  [40/84], [94mLoss[0m : 2.22088
[1mStep[0m  [48/84], [94mLoss[0m : 2.21460
[1mStep[0m  [56/84], [94mLoss[0m : 2.32877
[1mStep[0m  [64/84], [94mLoss[0m : 2.37853
[1mStep[0m  [72/84], [94mLoss[0m : 2.62467
[1mStep[0m  [80/84], [94mLoss[0m : 2.46833

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.605, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59455
[1mStep[0m  [8/84], [94mLoss[0m : 2.50937
[1mStep[0m  [16/84], [94mLoss[0m : 2.39791
[1mStep[0m  [24/84], [94mLoss[0m : 2.28709
[1mStep[0m  [32/84], [94mLoss[0m : 2.81924
[1mStep[0m  [40/84], [94mLoss[0m : 2.30084
[1mStep[0m  [48/84], [94mLoss[0m : 2.30654
[1mStep[0m  [56/84], [94mLoss[0m : 2.31595
[1mStep[0m  [64/84], [94mLoss[0m : 2.24676
[1mStep[0m  [72/84], [94mLoss[0m : 2.36631
[1mStep[0m  [80/84], [94mLoss[0m : 2.67509

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.611, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34064
[1mStep[0m  [8/84], [94mLoss[0m : 2.24081
[1mStep[0m  [16/84], [94mLoss[0m : 2.37731
[1mStep[0m  [24/84], [94mLoss[0m : 2.34422
[1mStep[0m  [32/84], [94mLoss[0m : 2.26123
[1mStep[0m  [40/84], [94mLoss[0m : 2.22303
[1mStep[0m  [48/84], [94mLoss[0m : 2.64419
[1mStep[0m  [56/84], [94mLoss[0m : 2.42358
[1mStep[0m  [64/84], [94mLoss[0m : 2.16695
[1mStep[0m  [72/84], [94mLoss[0m : 2.52320
[1mStep[0m  [80/84], [94mLoss[0m : 2.75238

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.582, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.575
====================================

Phase 2 - Evaluation MAE:  2.574856460094452
MAE score P1       2.409155
MAE score P2       2.574856
loss               2.424628
learning_rate        0.0001
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.48210
[1mStep[0m  [8/84], [94mLoss[0m : 10.57041
[1mStep[0m  [16/84], [94mLoss[0m : 10.43029
[1mStep[0m  [24/84], [94mLoss[0m : 10.85578
[1mStep[0m  [32/84], [94mLoss[0m : 10.70933
[1mStep[0m  [40/84], [94mLoss[0m : 10.60364
[1mStep[0m  [48/84], [94mLoss[0m : 11.70563
[1mStep[0m  [56/84], [94mLoss[0m : 10.67222
[1mStep[0m  [64/84], [94mLoss[0m : 11.11341
[1mStep[0m  [72/84], [94mLoss[0m : 10.48229
[1mStep[0m  [80/84], [94mLoss[0m : 10.93710

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.864, [92mTest[0m: 10.857, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.86260
[1mStep[0m  [8/84], [94mLoss[0m : 10.55992
[1mStep[0m  [16/84], [94mLoss[0m : 10.35872
[1mStep[0m  [24/84], [94mLoss[0m : 10.84953
[1mStep[0m  [32/84], [94mLoss[0m : 10.93853
[1mStep[0m  [40/84], [94mLoss[0m : 10.48393
[1mStep[0m  [48/84], [94mLoss[0m : 10.62660
[1mStep[0m  [56/84], [94mLoss[0m : 11.03275
[1mStep[0m  [64/84], [94mLoss[0m : 10.45519
[1mStep[0m  [72/84], [94mLoss[0m : 10.53673
[1mStep[0m  [80/84], [94mLoss[0m : 10.83840

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.849, [92mTest[0m: 10.834, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.96969
[1mStep[0m  [8/84], [94mLoss[0m : 10.87001
[1mStep[0m  [16/84], [94mLoss[0m : 10.92740
[1mStep[0m  [24/84], [94mLoss[0m : 10.85242
[1mStep[0m  [32/84], [94mLoss[0m : 10.91653
[1mStep[0m  [40/84], [94mLoss[0m : 11.28394
[1mStep[0m  [48/84], [94mLoss[0m : 11.09274
[1mStep[0m  [56/84], [94mLoss[0m : 10.27465
[1mStep[0m  [64/84], [94mLoss[0m : 11.21923
[1mStep[0m  [72/84], [94mLoss[0m : 10.86084
[1mStep[0m  [80/84], [94mLoss[0m : 10.75630

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.839, [92mTest[0m: 10.808, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.42973
[1mStep[0m  [8/84], [94mLoss[0m : 10.62737
[1mStep[0m  [16/84], [94mLoss[0m : 11.01464
[1mStep[0m  [24/84], [94mLoss[0m : 11.17931
[1mStep[0m  [32/84], [94mLoss[0m : 11.21725
[1mStep[0m  [40/84], [94mLoss[0m : 10.68801
[1mStep[0m  [48/84], [94mLoss[0m : 11.06176
[1mStep[0m  [56/84], [94mLoss[0m : 11.10259
[1mStep[0m  [64/84], [94mLoss[0m : 10.52000
[1mStep[0m  [72/84], [94mLoss[0m : 11.39460
[1mStep[0m  [80/84], [94mLoss[0m : 10.42853

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.18822
[1mStep[0m  [8/84], [94mLoss[0m : 10.65854
[1mStep[0m  [16/84], [94mLoss[0m : 10.62751
[1mStep[0m  [24/84], [94mLoss[0m : 10.85783
[1mStep[0m  [32/84], [94mLoss[0m : 10.92189
[1mStep[0m  [40/84], [94mLoss[0m : 10.43286
[1mStep[0m  [48/84], [94mLoss[0m : 10.59639
[1mStep[0m  [56/84], [94mLoss[0m : 11.07169
[1mStep[0m  [64/84], [94mLoss[0m : 10.79117
[1mStep[0m  [72/84], [94mLoss[0m : 10.79284
[1mStep[0m  [80/84], [94mLoss[0m : 11.15818

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.815, [92mTest[0m: 10.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.92631
[1mStep[0m  [8/84], [94mLoss[0m : 10.41408
[1mStep[0m  [16/84], [94mLoss[0m : 10.45548
[1mStep[0m  [24/84], [94mLoss[0m : 10.72197
[1mStep[0m  [32/84], [94mLoss[0m : 10.32835
[1mStep[0m  [40/84], [94mLoss[0m : 10.88011
[1mStep[0m  [48/84], [94mLoss[0m : 10.70561
[1mStep[0m  [56/84], [94mLoss[0m : 11.08881
[1mStep[0m  [64/84], [94mLoss[0m : 10.71744
[1mStep[0m  [72/84], [94mLoss[0m : 10.76213
[1mStep[0m  [80/84], [94mLoss[0m : 10.96531

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.809, [92mTest[0m: 10.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.00794
[1mStep[0m  [8/84], [94mLoss[0m : 11.03598
[1mStep[0m  [16/84], [94mLoss[0m : 11.03481
[1mStep[0m  [24/84], [94mLoss[0m : 10.64322
[1mStep[0m  [32/84], [94mLoss[0m : 11.14888
[1mStep[0m  [40/84], [94mLoss[0m : 10.33084
[1mStep[0m  [48/84], [94mLoss[0m : 10.67678
[1mStep[0m  [56/84], [94mLoss[0m : 10.76532
[1mStep[0m  [64/84], [94mLoss[0m : 10.82845
[1mStep[0m  [72/84], [94mLoss[0m : 11.16712
[1mStep[0m  [80/84], [94mLoss[0m : 10.49395

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.799, [92mTest[0m: 10.759, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.14784
[1mStep[0m  [8/84], [94mLoss[0m : 10.83233
[1mStep[0m  [16/84], [94mLoss[0m : 11.04657
[1mStep[0m  [24/84], [94mLoss[0m : 10.76943
[1mStep[0m  [32/84], [94mLoss[0m : 10.42326
[1mStep[0m  [40/84], [94mLoss[0m : 11.29350
[1mStep[0m  [48/84], [94mLoss[0m : 10.92691
[1mStep[0m  [56/84], [94mLoss[0m : 10.75650
[1mStep[0m  [64/84], [94mLoss[0m : 10.71598
[1mStep[0m  [72/84], [94mLoss[0m : 10.21366
[1mStep[0m  [80/84], [94mLoss[0m : 11.04259

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.791, [92mTest[0m: 10.728, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.90577
[1mStep[0m  [8/84], [94mLoss[0m : 10.91393
[1mStep[0m  [16/84], [94mLoss[0m : 10.73247
[1mStep[0m  [24/84], [94mLoss[0m : 10.89779
[1mStep[0m  [32/84], [94mLoss[0m : 10.97608
[1mStep[0m  [40/84], [94mLoss[0m : 10.78677
[1mStep[0m  [48/84], [94mLoss[0m : 10.20752
[1mStep[0m  [56/84], [94mLoss[0m : 11.06883
[1mStep[0m  [64/84], [94mLoss[0m : 11.11216
[1mStep[0m  [72/84], [94mLoss[0m : 10.96832
[1mStep[0m  [80/84], [94mLoss[0m : 10.82803

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.784, [92mTest[0m: 10.744, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.50936
[1mStep[0m  [8/84], [94mLoss[0m : 10.71580
[1mStep[0m  [16/84], [94mLoss[0m : 10.60607
[1mStep[0m  [24/84], [94mLoss[0m : 10.37563
[1mStep[0m  [32/84], [94mLoss[0m : 11.15811
[1mStep[0m  [40/84], [94mLoss[0m : 10.63096
[1mStep[0m  [48/84], [94mLoss[0m : 11.27272
[1mStep[0m  [56/84], [94mLoss[0m : 10.13937
[1mStep[0m  [64/84], [94mLoss[0m : 10.71180
[1mStep[0m  [72/84], [94mLoss[0m : 10.80313
[1mStep[0m  [80/84], [94mLoss[0m : 10.85280

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.760, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.67605
[1mStep[0m  [8/84], [94mLoss[0m : 10.78450
[1mStep[0m  [16/84], [94mLoss[0m : 10.46105
[1mStep[0m  [24/84], [94mLoss[0m : 10.88949
[1mStep[0m  [32/84], [94mLoss[0m : 10.57088
[1mStep[0m  [40/84], [94mLoss[0m : 10.97461
[1mStep[0m  [48/84], [94mLoss[0m : 11.19368
[1mStep[0m  [56/84], [94mLoss[0m : 10.64196
[1mStep[0m  [64/84], [94mLoss[0m : 10.28901
[1mStep[0m  [72/84], [94mLoss[0m : 10.85386
[1mStep[0m  [80/84], [94mLoss[0m : 10.78326

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.755, [92mTest[0m: 10.715, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.33550
[1mStep[0m  [8/84], [94mLoss[0m : 10.75249
[1mStep[0m  [16/84], [94mLoss[0m : 10.71083
[1mStep[0m  [24/84], [94mLoss[0m : 11.26554
[1mStep[0m  [32/84], [94mLoss[0m : 10.61682
[1mStep[0m  [40/84], [94mLoss[0m : 11.04616
[1mStep[0m  [48/84], [94mLoss[0m : 10.58117
[1mStep[0m  [56/84], [94mLoss[0m : 10.75351
[1mStep[0m  [64/84], [94mLoss[0m : 10.28750
[1mStep[0m  [72/84], [94mLoss[0m : 10.67633
[1mStep[0m  [80/84], [94mLoss[0m : 10.54204

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.69262
[1mStep[0m  [8/84], [94mLoss[0m : 10.91305
[1mStep[0m  [16/84], [94mLoss[0m : 10.85256
[1mStep[0m  [24/84], [94mLoss[0m : 10.85859
[1mStep[0m  [32/84], [94mLoss[0m : 11.29182
[1mStep[0m  [40/84], [94mLoss[0m : 10.85146
[1mStep[0m  [48/84], [94mLoss[0m : 10.74737
[1mStep[0m  [56/84], [94mLoss[0m : 10.69478
[1mStep[0m  [64/84], [94mLoss[0m : 10.62205
[1mStep[0m  [72/84], [94mLoss[0m : 10.08684
[1mStep[0m  [80/84], [94mLoss[0m : 11.22454

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.739, [92mTest[0m: 10.674, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.45350
[1mStep[0m  [8/84], [94mLoss[0m : 10.71408
[1mStep[0m  [16/84], [94mLoss[0m : 10.68303
[1mStep[0m  [24/84], [94mLoss[0m : 10.54407
[1mStep[0m  [32/84], [94mLoss[0m : 10.35271
[1mStep[0m  [40/84], [94mLoss[0m : 11.26993
[1mStep[0m  [48/84], [94mLoss[0m : 10.26482
[1mStep[0m  [56/84], [94mLoss[0m : 10.66630
[1mStep[0m  [64/84], [94mLoss[0m : 10.69865
[1mStep[0m  [72/84], [94mLoss[0m : 10.81159
[1mStep[0m  [80/84], [94mLoss[0m : 11.00666

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.724, [92mTest[0m: 10.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.69132
[1mStep[0m  [8/84], [94mLoss[0m : 10.37734
[1mStep[0m  [16/84], [94mLoss[0m : 11.25149
[1mStep[0m  [24/84], [94mLoss[0m : 10.54348
[1mStep[0m  [32/84], [94mLoss[0m : 10.75253
[1mStep[0m  [40/84], [94mLoss[0m : 10.50002
[1mStep[0m  [48/84], [94mLoss[0m : 10.55650
[1mStep[0m  [56/84], [94mLoss[0m : 10.01343
[1mStep[0m  [64/84], [94mLoss[0m : 11.31750
[1mStep[0m  [72/84], [94mLoss[0m : 10.18497
[1mStep[0m  [80/84], [94mLoss[0m : 10.17642

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.712, [92mTest[0m: 10.672, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.57593
[1mStep[0m  [8/84], [94mLoss[0m : 11.02560
[1mStep[0m  [16/84], [94mLoss[0m : 10.75286
[1mStep[0m  [24/84], [94mLoss[0m : 10.40965
[1mStep[0m  [32/84], [94mLoss[0m : 10.97723
[1mStep[0m  [40/84], [94mLoss[0m : 10.78029
[1mStep[0m  [48/84], [94mLoss[0m : 10.12315
[1mStep[0m  [56/84], [94mLoss[0m : 10.62960
[1mStep[0m  [64/84], [94mLoss[0m : 10.86319
[1mStep[0m  [72/84], [94mLoss[0m : 10.49556
[1mStep[0m  [80/84], [94mLoss[0m : 10.83421

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.704, [92mTest[0m: 10.625, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.11815
[1mStep[0m  [8/84], [94mLoss[0m : 10.97934
[1mStep[0m  [16/84], [94mLoss[0m : 10.12298
[1mStep[0m  [24/84], [94mLoss[0m : 10.68626
[1mStep[0m  [32/84], [94mLoss[0m : 10.39370
[1mStep[0m  [40/84], [94mLoss[0m : 10.83130
[1mStep[0m  [48/84], [94mLoss[0m : 10.87566
[1mStep[0m  [56/84], [94mLoss[0m : 11.17181
[1mStep[0m  [64/84], [94mLoss[0m : 10.29039
[1mStep[0m  [72/84], [94mLoss[0m : 10.32385
[1mStep[0m  [80/84], [94mLoss[0m : 10.47073

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.695, [92mTest[0m: 10.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.07432
[1mStep[0m  [8/84], [94mLoss[0m : 10.70691
[1mStep[0m  [16/84], [94mLoss[0m : 10.40392
[1mStep[0m  [24/84], [94mLoss[0m : 10.49040
[1mStep[0m  [32/84], [94mLoss[0m : 11.46049
[1mStep[0m  [40/84], [94mLoss[0m : 10.52634
[1mStep[0m  [48/84], [94mLoss[0m : 10.55372
[1mStep[0m  [56/84], [94mLoss[0m : 10.38037
[1mStep[0m  [64/84], [94mLoss[0m : 10.30702
[1mStep[0m  [72/84], [94mLoss[0m : 10.73332
[1mStep[0m  [80/84], [94mLoss[0m : 10.53230

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.680, [92mTest[0m: 10.616, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.14784
[1mStep[0m  [8/84], [94mLoss[0m : 10.17663
[1mStep[0m  [16/84], [94mLoss[0m : 10.36067
[1mStep[0m  [24/84], [94mLoss[0m : 10.29124
[1mStep[0m  [32/84], [94mLoss[0m : 10.67510
[1mStep[0m  [40/84], [94mLoss[0m : 11.02513
[1mStep[0m  [48/84], [94mLoss[0m : 10.80556
[1mStep[0m  [56/84], [94mLoss[0m : 10.69311
[1mStep[0m  [64/84], [94mLoss[0m : 10.18925
[1mStep[0m  [72/84], [94mLoss[0m : 10.51284
[1mStep[0m  [80/84], [94mLoss[0m : 10.31228

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.663, [92mTest[0m: 10.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.12497
[1mStep[0m  [8/84], [94mLoss[0m : 10.61736
[1mStep[0m  [16/84], [94mLoss[0m : 10.63454
[1mStep[0m  [24/84], [94mLoss[0m : 10.59839
[1mStep[0m  [32/84], [94mLoss[0m : 10.93849
[1mStep[0m  [40/84], [94mLoss[0m : 10.54447
[1mStep[0m  [48/84], [94mLoss[0m : 10.61727
[1mStep[0m  [56/84], [94mLoss[0m : 10.61636
[1mStep[0m  [64/84], [94mLoss[0m : 10.92841
[1mStep[0m  [72/84], [94mLoss[0m : 10.38853
[1mStep[0m  [80/84], [94mLoss[0m : 10.42783

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.655, [92mTest[0m: 10.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.96858
[1mStep[0m  [8/84], [94mLoss[0m : 10.72745
[1mStep[0m  [16/84], [94mLoss[0m : 10.41969
[1mStep[0m  [24/84], [94mLoss[0m : 10.91891
[1mStep[0m  [32/84], [94mLoss[0m : 10.72593
[1mStep[0m  [40/84], [94mLoss[0m : 10.42829
[1mStep[0m  [48/84], [94mLoss[0m : 10.42010
[1mStep[0m  [56/84], [94mLoss[0m : 10.06469
[1mStep[0m  [64/84], [94mLoss[0m : 11.02426
[1mStep[0m  [72/84], [94mLoss[0m : 10.61982
[1mStep[0m  [80/84], [94mLoss[0m : 10.93445

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.652, [92mTest[0m: 10.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.18345
[1mStep[0m  [8/84], [94mLoss[0m : 10.16806
[1mStep[0m  [16/84], [94mLoss[0m : 10.68027
[1mStep[0m  [24/84], [94mLoss[0m : 10.72519
[1mStep[0m  [32/84], [94mLoss[0m : 10.27273
[1mStep[0m  [40/84], [94mLoss[0m : 10.57163
[1mStep[0m  [48/84], [94mLoss[0m : 10.54543
[1mStep[0m  [56/84], [94mLoss[0m : 10.44444
[1mStep[0m  [64/84], [94mLoss[0m : 10.95678
[1mStep[0m  [72/84], [94mLoss[0m : 10.56135
[1mStep[0m  [80/84], [94mLoss[0m : 10.73998

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.58150
[1mStep[0m  [8/84], [94mLoss[0m : 10.81111
[1mStep[0m  [16/84], [94mLoss[0m : 10.08286
[1mStep[0m  [24/84], [94mLoss[0m : 10.34461
[1mStep[0m  [32/84], [94mLoss[0m : 10.80760
[1mStep[0m  [40/84], [94mLoss[0m : 10.46338
[1mStep[0m  [48/84], [94mLoss[0m : 10.24424
[1mStep[0m  [56/84], [94mLoss[0m : 10.90034
[1mStep[0m  [64/84], [94mLoss[0m : 10.73208
[1mStep[0m  [72/84], [94mLoss[0m : 10.99606
[1mStep[0m  [80/84], [94mLoss[0m : 9.95561

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.625, [92mTest[0m: 10.552, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.68704
[1mStep[0m  [8/84], [94mLoss[0m : 10.33593
[1mStep[0m  [16/84], [94mLoss[0m : 10.79742
[1mStep[0m  [24/84], [94mLoss[0m : 10.78555
[1mStep[0m  [32/84], [94mLoss[0m : 10.31199
[1mStep[0m  [40/84], [94mLoss[0m : 10.57266
[1mStep[0m  [48/84], [94mLoss[0m : 10.39012
[1mStep[0m  [56/84], [94mLoss[0m : 11.02982
[1mStep[0m  [64/84], [94mLoss[0m : 11.13575
[1mStep[0m  [72/84], [94mLoss[0m : 10.44334
[1mStep[0m  [80/84], [94mLoss[0m : 10.59479

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.618, [92mTest[0m: 10.562, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.80333
[1mStep[0m  [8/84], [94mLoss[0m : 10.67571
[1mStep[0m  [16/84], [94mLoss[0m : 10.44468
[1mStep[0m  [24/84], [94mLoss[0m : 10.70757
[1mStep[0m  [32/84], [94mLoss[0m : 10.03251
[1mStep[0m  [40/84], [94mLoss[0m : 10.55083
[1mStep[0m  [48/84], [94mLoss[0m : 10.80122
[1mStep[0m  [56/84], [94mLoss[0m : 10.49966
[1mStep[0m  [64/84], [94mLoss[0m : 10.60500
[1mStep[0m  [72/84], [94mLoss[0m : 10.28926
[1mStep[0m  [80/84], [94mLoss[0m : 10.64919

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.608, [92mTest[0m: 10.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.46409
[1mStep[0m  [8/84], [94mLoss[0m : 10.51963
[1mStep[0m  [16/84], [94mLoss[0m : 10.76547
[1mStep[0m  [24/84], [94mLoss[0m : 10.62685
[1mStep[0m  [32/84], [94mLoss[0m : 10.72651
[1mStep[0m  [40/84], [94mLoss[0m : 10.97062
[1mStep[0m  [48/84], [94mLoss[0m : 11.19244
[1mStep[0m  [56/84], [94mLoss[0m : 10.53254
[1mStep[0m  [64/84], [94mLoss[0m : 10.52256
[1mStep[0m  [72/84], [94mLoss[0m : 10.69403
[1mStep[0m  [80/84], [94mLoss[0m : 10.55803

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.594, [92mTest[0m: 10.529, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.52253
[1mStep[0m  [8/84], [94mLoss[0m : 10.40019
[1mStep[0m  [16/84], [94mLoss[0m : 10.81761
[1mStep[0m  [24/84], [94mLoss[0m : 10.67002
[1mStep[0m  [32/84], [94mLoss[0m : 10.70457
[1mStep[0m  [40/84], [94mLoss[0m : 10.51306
[1mStep[0m  [48/84], [94mLoss[0m : 10.62831
[1mStep[0m  [56/84], [94mLoss[0m : 10.77111
[1mStep[0m  [64/84], [94mLoss[0m : 10.11344
[1mStep[0m  [72/84], [94mLoss[0m : 10.64428
[1mStep[0m  [80/84], [94mLoss[0m : 10.56739

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.582, [92mTest[0m: 10.503, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.73527
[1mStep[0m  [8/84], [94mLoss[0m : 11.00210
[1mStep[0m  [16/84], [94mLoss[0m : 10.69151
[1mStep[0m  [24/84], [94mLoss[0m : 10.89721
[1mStep[0m  [32/84], [94mLoss[0m : 10.40242
[1mStep[0m  [40/84], [94mLoss[0m : 10.36237
[1mStep[0m  [48/84], [94mLoss[0m : 10.20615
[1mStep[0m  [56/84], [94mLoss[0m : 10.90991
[1mStep[0m  [64/84], [94mLoss[0m : 10.62170
[1mStep[0m  [72/84], [94mLoss[0m : 10.78362
[1mStep[0m  [80/84], [94mLoss[0m : 11.07370

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.582, [92mTest[0m: 10.521, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.41520
[1mStep[0m  [8/84], [94mLoss[0m : 10.44986
[1mStep[0m  [16/84], [94mLoss[0m : 10.59079
[1mStep[0m  [24/84], [94mLoss[0m : 10.60038
[1mStep[0m  [32/84], [94mLoss[0m : 10.31291
[1mStep[0m  [40/84], [94mLoss[0m : 9.93456
[1mStep[0m  [48/84], [94mLoss[0m : 11.07762
[1mStep[0m  [56/84], [94mLoss[0m : 10.26887
[1mStep[0m  [64/84], [94mLoss[0m : 10.50419
[1mStep[0m  [72/84], [94mLoss[0m : 10.37111
[1mStep[0m  [80/84], [94mLoss[0m : 10.67692

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.566, [92mTest[0m: 10.473, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.29701
[1mStep[0m  [8/84], [94mLoss[0m : 10.32439
[1mStep[0m  [16/84], [94mLoss[0m : 10.63933
[1mStep[0m  [24/84], [94mLoss[0m : 10.43044
[1mStep[0m  [32/84], [94mLoss[0m : 10.71220
[1mStep[0m  [40/84], [94mLoss[0m : 10.73628
[1mStep[0m  [48/84], [94mLoss[0m : 10.32646
[1mStep[0m  [56/84], [94mLoss[0m : 10.71965
[1mStep[0m  [64/84], [94mLoss[0m : 10.53244
[1mStep[0m  [72/84], [94mLoss[0m : 10.29039
[1mStep[0m  [80/84], [94mLoss[0m : 10.22530

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.470, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.451
====================================

Phase 1 - Evaluation MAE:  10.450863429478236
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.72695
[1mStep[0m  [8/84], [94mLoss[0m : 10.57431
[1mStep[0m  [16/84], [94mLoss[0m : 11.20263
[1mStep[0m  [24/84], [94mLoss[0m : 10.35972
[1mStep[0m  [32/84], [94mLoss[0m : 10.80297
[1mStep[0m  [40/84], [94mLoss[0m : 10.41560
[1mStep[0m  [48/84], [94mLoss[0m : 10.79477
[1mStep[0m  [56/84], [94mLoss[0m : 10.37761
[1mStep[0m  [64/84], [94mLoss[0m : 10.48598
[1mStep[0m  [72/84], [94mLoss[0m : 9.99298
[1mStep[0m  [80/84], [94mLoss[0m : 10.42226

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.551, [92mTest[0m: 10.458, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.13485
[1mStep[0m  [8/84], [94mLoss[0m : 10.82585
[1mStep[0m  [16/84], [94mLoss[0m : 10.43999
[1mStep[0m  [24/84], [94mLoss[0m : 10.60923
[1mStep[0m  [32/84], [94mLoss[0m : 10.20223
[1mStep[0m  [40/84], [94mLoss[0m : 10.44353
[1mStep[0m  [48/84], [94mLoss[0m : 10.43660
[1mStep[0m  [56/84], [94mLoss[0m : 10.74501
[1mStep[0m  [64/84], [94mLoss[0m : 10.70166
[1mStep[0m  [72/84], [94mLoss[0m : 10.24679
[1mStep[0m  [80/84], [94mLoss[0m : 10.91171

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.522, [92mTest[0m: 10.463, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.71682
[1mStep[0m  [8/84], [94mLoss[0m : 10.48632
[1mStep[0m  [16/84], [94mLoss[0m : 11.07468
[1mStep[0m  [24/84], [94mLoss[0m : 9.73411
[1mStep[0m  [32/84], [94mLoss[0m : 10.93712
[1mStep[0m  [40/84], [94mLoss[0m : 10.13423
[1mStep[0m  [48/84], [94mLoss[0m : 10.90677
[1mStep[0m  [56/84], [94mLoss[0m : 10.43748
[1mStep[0m  [64/84], [94mLoss[0m : 10.27735
[1mStep[0m  [72/84], [94mLoss[0m : 10.25374
[1mStep[0m  [80/84], [94mLoss[0m : 10.49940

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.523, [92mTest[0m: 10.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.83434
[1mStep[0m  [8/84], [94mLoss[0m : 10.39973
[1mStep[0m  [16/84], [94mLoss[0m : 10.20353
[1mStep[0m  [24/84], [94mLoss[0m : 10.46441
[1mStep[0m  [32/84], [94mLoss[0m : 10.19755
[1mStep[0m  [40/84], [94mLoss[0m : 10.80742
[1mStep[0m  [48/84], [94mLoss[0m : 10.85866
[1mStep[0m  [56/84], [94mLoss[0m : 11.02641
[1mStep[0m  [64/84], [94mLoss[0m : 10.42566
[1mStep[0m  [72/84], [94mLoss[0m : 10.57593
[1mStep[0m  [80/84], [94mLoss[0m : 10.58512

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.48503
[1mStep[0m  [8/84], [94mLoss[0m : 10.18975
[1mStep[0m  [16/84], [94mLoss[0m : 10.22499
[1mStep[0m  [24/84], [94mLoss[0m : 10.34648
[1mStep[0m  [32/84], [94mLoss[0m : 10.39204
[1mStep[0m  [40/84], [94mLoss[0m : 10.72745
[1mStep[0m  [48/84], [94mLoss[0m : 10.48161
[1mStep[0m  [56/84], [94mLoss[0m : 10.44338
[1mStep[0m  [64/84], [94mLoss[0m : 10.26682
[1mStep[0m  [72/84], [94mLoss[0m : 10.08212
[1mStep[0m  [80/84], [94mLoss[0m : 10.93435

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.496, [92mTest[0m: 10.379, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.96756
[1mStep[0m  [8/84], [94mLoss[0m : 11.04710
[1mStep[0m  [16/84], [94mLoss[0m : 10.26548
[1mStep[0m  [24/84], [94mLoss[0m : 10.97851
[1mStep[0m  [32/84], [94mLoss[0m : 10.39989
[1mStep[0m  [40/84], [94mLoss[0m : 10.17408
[1mStep[0m  [48/84], [94mLoss[0m : 10.71001
[1mStep[0m  [56/84], [94mLoss[0m : 10.80935
[1mStep[0m  [64/84], [94mLoss[0m : 10.45074
[1mStep[0m  [72/84], [94mLoss[0m : 10.66073
[1mStep[0m  [80/84], [94mLoss[0m : 10.78473

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.486, [92mTest[0m: 10.404, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.09802
[1mStep[0m  [8/84], [94mLoss[0m : 10.38235
[1mStep[0m  [16/84], [94mLoss[0m : 10.62911
[1mStep[0m  [24/84], [94mLoss[0m : 10.54587
[1mStep[0m  [32/84], [94mLoss[0m : 10.75073
[1mStep[0m  [40/84], [94mLoss[0m : 10.21536
[1mStep[0m  [48/84], [94mLoss[0m : 10.52127
[1mStep[0m  [56/84], [94mLoss[0m : 10.79954
[1mStep[0m  [64/84], [94mLoss[0m : 10.56187
[1mStep[0m  [72/84], [94mLoss[0m : 10.55842
[1mStep[0m  [80/84], [94mLoss[0m : 10.49658

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.460, [92mTest[0m: 10.379, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.20993
[1mStep[0m  [8/84], [94mLoss[0m : 10.10904
[1mStep[0m  [16/84], [94mLoss[0m : 10.53982
[1mStep[0m  [24/84], [94mLoss[0m : 10.60255
[1mStep[0m  [32/84], [94mLoss[0m : 10.39430
[1mStep[0m  [40/84], [94mLoss[0m : 11.00853
[1mStep[0m  [48/84], [94mLoss[0m : 10.72671
[1mStep[0m  [56/84], [94mLoss[0m : 10.52514
[1mStep[0m  [64/84], [94mLoss[0m : 10.47392
[1mStep[0m  [72/84], [94mLoss[0m : 10.50584
[1mStep[0m  [80/84], [94mLoss[0m : 10.54208

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.458, [92mTest[0m: 10.311, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.47808
[1mStep[0m  [8/84], [94mLoss[0m : 10.36284
[1mStep[0m  [16/84], [94mLoss[0m : 10.50908
[1mStep[0m  [24/84], [94mLoss[0m : 10.51895
[1mStep[0m  [32/84], [94mLoss[0m : 10.66547
[1mStep[0m  [40/84], [94mLoss[0m : 10.45766
[1mStep[0m  [48/84], [94mLoss[0m : 9.87808
[1mStep[0m  [56/84], [94mLoss[0m : 9.81499
[1mStep[0m  [64/84], [94mLoss[0m : 10.53759
[1mStep[0m  [72/84], [94mLoss[0m : 10.42724
[1mStep[0m  [80/84], [94mLoss[0m : 10.38515

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.442, [92mTest[0m: 10.339, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.14810
[1mStep[0m  [8/84], [94mLoss[0m : 9.85487
[1mStep[0m  [16/84], [94mLoss[0m : 10.53614
[1mStep[0m  [24/84], [94mLoss[0m : 10.81006
[1mStep[0m  [32/84], [94mLoss[0m : 10.37586
[1mStep[0m  [40/84], [94mLoss[0m : 10.44986
[1mStep[0m  [48/84], [94mLoss[0m : 10.96679
[1mStep[0m  [56/84], [94mLoss[0m : 10.30977
[1mStep[0m  [64/84], [94mLoss[0m : 10.84990
[1mStep[0m  [72/84], [94mLoss[0m : 11.27922
[1mStep[0m  [80/84], [94mLoss[0m : 10.35931

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.442, [92mTest[0m: 10.345, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.65629
[1mStep[0m  [8/84], [94mLoss[0m : 10.20264
[1mStep[0m  [16/84], [94mLoss[0m : 10.03195
[1mStep[0m  [24/84], [94mLoss[0m : 10.24662
[1mStep[0m  [32/84], [94mLoss[0m : 10.26747
[1mStep[0m  [40/84], [94mLoss[0m : 10.36278
[1mStep[0m  [48/84], [94mLoss[0m : 10.87886
[1mStep[0m  [56/84], [94mLoss[0m : 9.95050
[1mStep[0m  [64/84], [94mLoss[0m : 10.60358
[1mStep[0m  [72/84], [94mLoss[0m : 10.70061
[1mStep[0m  [80/84], [94mLoss[0m : 10.16087

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.420, [92mTest[0m: 10.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.70433
[1mStep[0m  [8/84], [94mLoss[0m : 10.47587
[1mStep[0m  [16/84], [94mLoss[0m : 10.37585
[1mStep[0m  [24/84], [94mLoss[0m : 10.34265
[1mStep[0m  [32/84], [94mLoss[0m : 10.56059
[1mStep[0m  [40/84], [94mLoss[0m : 9.86394
[1mStep[0m  [48/84], [94mLoss[0m : 11.04831
[1mStep[0m  [56/84], [94mLoss[0m : 10.11156
[1mStep[0m  [64/84], [94mLoss[0m : 10.80277
[1mStep[0m  [72/84], [94mLoss[0m : 10.50621
[1mStep[0m  [80/84], [94mLoss[0m : 10.48913

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.414, [92mTest[0m: 10.289, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.05864
[1mStep[0m  [8/84], [94mLoss[0m : 10.21618
[1mStep[0m  [16/84], [94mLoss[0m : 10.20271
[1mStep[0m  [24/84], [94mLoss[0m : 10.80261
[1mStep[0m  [32/84], [94mLoss[0m : 10.30233
[1mStep[0m  [40/84], [94mLoss[0m : 10.58933
[1mStep[0m  [48/84], [94mLoss[0m : 10.36069
[1mStep[0m  [56/84], [94mLoss[0m : 10.75124
[1mStep[0m  [64/84], [94mLoss[0m : 10.44620
[1mStep[0m  [72/84], [94mLoss[0m : 10.65017
[1mStep[0m  [80/84], [94mLoss[0m : 10.46529

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.263, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.04727
[1mStep[0m  [8/84], [94mLoss[0m : 10.07540
[1mStep[0m  [16/84], [94mLoss[0m : 10.69661
[1mStep[0m  [24/84], [94mLoss[0m : 9.96559
[1mStep[0m  [32/84], [94mLoss[0m : 10.51532
[1mStep[0m  [40/84], [94mLoss[0m : 10.63793
[1mStep[0m  [48/84], [94mLoss[0m : 11.04094
[1mStep[0m  [56/84], [94mLoss[0m : 10.33221
[1mStep[0m  [64/84], [94mLoss[0m : 10.34433
[1mStep[0m  [72/84], [94mLoss[0m : 10.25013
[1mStep[0m  [80/84], [94mLoss[0m : 9.87777

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.387, [92mTest[0m: 10.254, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.88864
[1mStep[0m  [8/84], [94mLoss[0m : 10.38082
[1mStep[0m  [16/84], [94mLoss[0m : 10.27910
[1mStep[0m  [24/84], [94mLoss[0m : 10.62924
[1mStep[0m  [32/84], [94mLoss[0m : 10.02521
[1mStep[0m  [40/84], [94mLoss[0m : 9.80491
[1mStep[0m  [48/84], [94mLoss[0m : 10.59221
[1mStep[0m  [56/84], [94mLoss[0m : 9.87534
[1mStep[0m  [64/84], [94mLoss[0m : 10.73582
[1mStep[0m  [72/84], [94mLoss[0m : 10.77797
[1mStep[0m  [80/84], [94mLoss[0m : 10.69901

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.377, [92mTest[0m: 10.240, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.90637
[1mStep[0m  [8/84], [94mLoss[0m : 9.96199
[1mStep[0m  [16/84], [94mLoss[0m : 10.42080
[1mStep[0m  [24/84], [94mLoss[0m : 9.95534
[1mStep[0m  [32/84], [94mLoss[0m : 10.77904
[1mStep[0m  [40/84], [94mLoss[0m : 10.35206
[1mStep[0m  [48/84], [94mLoss[0m : 10.91855
[1mStep[0m  [56/84], [94mLoss[0m : 10.18735
[1mStep[0m  [64/84], [94mLoss[0m : 9.85874
[1mStep[0m  [72/84], [94mLoss[0m : 10.27673
[1mStep[0m  [80/84], [94mLoss[0m : 10.35701

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.367, [92mTest[0m: 10.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.97624
[1mStep[0m  [8/84], [94mLoss[0m : 9.90719
[1mStep[0m  [16/84], [94mLoss[0m : 10.47787
[1mStep[0m  [24/84], [94mLoss[0m : 10.05009
[1mStep[0m  [32/84], [94mLoss[0m : 10.05978
[1mStep[0m  [40/84], [94mLoss[0m : 10.19364
[1mStep[0m  [48/84], [94mLoss[0m : 10.17397
[1mStep[0m  [56/84], [94mLoss[0m : 10.45138
[1mStep[0m  [64/84], [94mLoss[0m : 9.88873
[1mStep[0m  [72/84], [94mLoss[0m : 10.76079
[1mStep[0m  [80/84], [94mLoss[0m : 10.42154

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.348, [92mTest[0m: 10.211, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.01849
[1mStep[0m  [8/84], [94mLoss[0m : 10.19860
[1mStep[0m  [16/84], [94mLoss[0m : 10.21885
[1mStep[0m  [24/84], [94mLoss[0m : 10.47856
[1mStep[0m  [32/84], [94mLoss[0m : 10.71830
[1mStep[0m  [40/84], [94mLoss[0m : 10.79208
[1mStep[0m  [48/84], [94mLoss[0m : 10.31429
[1mStep[0m  [56/84], [94mLoss[0m : 10.10264
[1mStep[0m  [64/84], [94mLoss[0m : 9.90141
[1mStep[0m  [72/84], [94mLoss[0m : 9.83664
[1mStep[0m  [80/84], [94mLoss[0m : 10.22840

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.341, [92mTest[0m: 10.235, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.25170
[1mStep[0m  [8/84], [94mLoss[0m : 10.46026
[1mStep[0m  [16/84], [94mLoss[0m : 10.41115
[1mStep[0m  [24/84], [94mLoss[0m : 10.38208
[1mStep[0m  [32/84], [94mLoss[0m : 10.43122
[1mStep[0m  [40/84], [94mLoss[0m : 10.03502
[1mStep[0m  [48/84], [94mLoss[0m : 9.89930
[1mStep[0m  [56/84], [94mLoss[0m : 10.61802
[1mStep[0m  [64/84], [94mLoss[0m : 9.77946
[1mStep[0m  [72/84], [94mLoss[0m : 10.56029
[1mStep[0m  [80/84], [94mLoss[0m : 9.97645

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.336, [92mTest[0m: 10.210, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.11252
[1mStep[0m  [8/84], [94mLoss[0m : 10.02445
[1mStep[0m  [16/84], [94mLoss[0m : 10.08318
[1mStep[0m  [24/84], [94mLoss[0m : 10.79238
[1mStep[0m  [32/84], [94mLoss[0m : 9.97128
[1mStep[0m  [40/84], [94mLoss[0m : 10.38262
[1mStep[0m  [48/84], [94mLoss[0m : 10.36291
[1mStep[0m  [56/84], [94mLoss[0m : 10.32350
[1mStep[0m  [64/84], [94mLoss[0m : 9.93493
[1mStep[0m  [72/84], [94mLoss[0m : 10.89640
[1mStep[0m  [80/84], [94mLoss[0m : 10.35901

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.322, [92mTest[0m: 10.188, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.60065
[1mStep[0m  [8/84], [94mLoss[0m : 10.45910
[1mStep[0m  [16/84], [94mLoss[0m : 10.13936
[1mStep[0m  [24/84], [94mLoss[0m : 10.49712
[1mStep[0m  [32/84], [94mLoss[0m : 10.32225
[1mStep[0m  [40/84], [94mLoss[0m : 10.25826
[1mStep[0m  [48/84], [94mLoss[0m : 10.13062
[1mStep[0m  [56/84], [94mLoss[0m : 10.15411
[1mStep[0m  [64/84], [94mLoss[0m : 10.32013
[1mStep[0m  [72/84], [94mLoss[0m : 9.87135
[1mStep[0m  [80/84], [94mLoss[0m : 10.37702

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.310, [92mTest[0m: 10.142, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.12025
[1mStep[0m  [8/84], [94mLoss[0m : 9.99000
[1mStep[0m  [16/84], [94mLoss[0m : 10.97214
[1mStep[0m  [24/84], [94mLoss[0m : 10.54886
[1mStep[0m  [32/84], [94mLoss[0m : 10.26857
[1mStep[0m  [40/84], [94mLoss[0m : 10.12844
[1mStep[0m  [48/84], [94mLoss[0m : 10.18032
[1mStep[0m  [56/84], [94mLoss[0m : 10.71131
[1mStep[0m  [64/84], [94mLoss[0m : 10.53353
[1mStep[0m  [72/84], [94mLoss[0m : 10.37388
[1mStep[0m  [80/84], [94mLoss[0m : 10.77081

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.293, [92mTest[0m: 10.152, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.62464
[1mStep[0m  [8/84], [94mLoss[0m : 10.56076
[1mStep[0m  [16/84], [94mLoss[0m : 9.98987
[1mStep[0m  [24/84], [94mLoss[0m : 10.28144
[1mStep[0m  [32/84], [94mLoss[0m : 10.98025
[1mStep[0m  [40/84], [94mLoss[0m : 10.53800
[1mStep[0m  [48/84], [94mLoss[0m : 10.22859
[1mStep[0m  [56/84], [94mLoss[0m : 10.07830
[1mStep[0m  [64/84], [94mLoss[0m : 10.30712
[1mStep[0m  [72/84], [94mLoss[0m : 9.74328
[1mStep[0m  [80/84], [94mLoss[0m : 10.63788

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.289, [92mTest[0m: 10.116, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.58806
[1mStep[0m  [8/84], [94mLoss[0m : 10.23389
[1mStep[0m  [16/84], [94mLoss[0m : 9.85472
[1mStep[0m  [24/84], [94mLoss[0m : 10.53417
[1mStep[0m  [32/84], [94mLoss[0m : 10.38291
[1mStep[0m  [40/84], [94mLoss[0m : 10.51120
[1mStep[0m  [48/84], [94mLoss[0m : 10.51645
[1mStep[0m  [56/84], [94mLoss[0m : 10.46445
[1mStep[0m  [64/84], [94mLoss[0m : 10.40947
[1mStep[0m  [72/84], [94mLoss[0m : 9.53783
[1mStep[0m  [80/84], [94mLoss[0m : 10.24885

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.263, [92mTest[0m: 10.098, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.22392
[1mStep[0m  [8/84], [94mLoss[0m : 10.15668
[1mStep[0m  [16/84], [94mLoss[0m : 10.05654
[1mStep[0m  [24/84], [94mLoss[0m : 9.73242
[1mStep[0m  [32/84], [94mLoss[0m : 10.41155
[1mStep[0m  [40/84], [94mLoss[0m : 9.97848
[1mStep[0m  [48/84], [94mLoss[0m : 10.25097
[1mStep[0m  [56/84], [94mLoss[0m : 10.40487
[1mStep[0m  [64/84], [94mLoss[0m : 10.45860
[1mStep[0m  [72/84], [94mLoss[0m : 10.23448
[1mStep[0m  [80/84], [94mLoss[0m : 10.50399

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.266, [92mTest[0m: 10.132, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.63928
[1mStep[0m  [8/84], [94mLoss[0m : 10.52795
[1mStep[0m  [16/84], [94mLoss[0m : 10.65260
[1mStep[0m  [24/84], [94mLoss[0m : 9.86922
[1mStep[0m  [32/84], [94mLoss[0m : 10.23466
[1mStep[0m  [40/84], [94mLoss[0m : 10.59389
[1mStep[0m  [48/84], [94mLoss[0m : 10.92739
[1mStep[0m  [56/84], [94mLoss[0m : 10.24619
[1mStep[0m  [64/84], [94mLoss[0m : 10.43025
[1mStep[0m  [72/84], [94mLoss[0m : 10.23937
[1mStep[0m  [80/84], [94mLoss[0m : 10.18936

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.251, [92mTest[0m: 10.134, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.66494
[1mStep[0m  [8/84], [94mLoss[0m : 10.43211
[1mStep[0m  [16/84], [94mLoss[0m : 10.52324
[1mStep[0m  [24/84], [94mLoss[0m : 9.98335
[1mStep[0m  [32/84], [94mLoss[0m : 10.48869
[1mStep[0m  [40/84], [94mLoss[0m : 10.34239
[1mStep[0m  [48/84], [94mLoss[0m : 10.05717
[1mStep[0m  [56/84], [94mLoss[0m : 10.62261
[1mStep[0m  [64/84], [94mLoss[0m : 10.54103
[1mStep[0m  [72/84], [94mLoss[0m : 9.97284
[1mStep[0m  [80/84], [94mLoss[0m : 10.17105

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.229, [92mTest[0m: 10.063, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.26171
[1mStep[0m  [8/84], [94mLoss[0m : 10.05432
[1mStep[0m  [16/84], [94mLoss[0m : 9.98590
[1mStep[0m  [24/84], [94mLoss[0m : 9.90787
[1mStep[0m  [32/84], [94mLoss[0m : 10.04808
[1mStep[0m  [40/84], [94mLoss[0m : 9.71069
[1mStep[0m  [48/84], [94mLoss[0m : 10.38840
[1mStep[0m  [56/84], [94mLoss[0m : 10.39645
[1mStep[0m  [64/84], [94mLoss[0m : 10.18603
[1mStep[0m  [72/84], [94mLoss[0m : 9.65272
[1mStep[0m  [80/84], [94mLoss[0m : 10.19872

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.220, [92mTest[0m: 10.091, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.63549
[1mStep[0m  [8/84], [94mLoss[0m : 10.00346
[1mStep[0m  [16/84], [94mLoss[0m : 10.24135
[1mStep[0m  [24/84], [94mLoss[0m : 10.15651
[1mStep[0m  [32/84], [94mLoss[0m : 10.71329
[1mStep[0m  [40/84], [94mLoss[0m : 9.89677
[1mStep[0m  [48/84], [94mLoss[0m : 10.24481
[1mStep[0m  [56/84], [94mLoss[0m : 10.15430
[1mStep[0m  [64/84], [94mLoss[0m : 10.26550
[1mStep[0m  [72/84], [94mLoss[0m : 10.05705
[1mStep[0m  [80/84], [94mLoss[0m : 10.14524

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.209, [92mTest[0m: 10.051, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.27328
[1mStep[0m  [8/84], [94mLoss[0m : 10.90603
[1mStep[0m  [16/84], [94mLoss[0m : 9.87088
[1mStep[0m  [24/84], [94mLoss[0m : 10.02482
[1mStep[0m  [32/84], [94mLoss[0m : 10.20359
[1mStep[0m  [40/84], [94mLoss[0m : 10.23840
[1mStep[0m  [48/84], [94mLoss[0m : 9.96876
[1mStep[0m  [56/84], [94mLoss[0m : 9.96546
[1mStep[0m  [64/84], [94mLoss[0m : 10.50882
[1mStep[0m  [72/84], [94mLoss[0m : 10.22083
[1mStep[0m  [80/84], [94mLoss[0m : 10.04954

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.201, [92mTest[0m: 10.033, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.043
====================================

Phase 2 - Evaluation MAE:  10.043271541595459
MAE score P1       10.450863
MAE score P2       10.043272
loss               10.201185
learning_rate         0.0001
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.1
weight_decay           0.001
Name: 16, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.84611
[1mStep[0m  [8/84], [94mLoss[0m : 10.96537
[1mStep[0m  [16/84], [94mLoss[0m : 11.02182
[1mStep[0m  [24/84], [94mLoss[0m : 11.01349
[1mStep[0m  [32/84], [94mLoss[0m : 11.13193
[1mStep[0m  [40/84], [94mLoss[0m : 10.64203
[1mStep[0m  [48/84], [94mLoss[0m : 11.04974
[1mStep[0m  [56/84], [94mLoss[0m : 11.49926
[1mStep[0m  [64/84], [94mLoss[0m : 10.69143
[1mStep[0m  [72/84], [94mLoss[0m : 10.32283
[1mStep[0m  [80/84], [94mLoss[0m : 10.43089

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.08497
[1mStep[0m  [8/84], [94mLoss[0m : 11.09550
[1mStep[0m  [16/84], [94mLoss[0m : 10.84009
[1mStep[0m  [24/84], [94mLoss[0m : 10.50994
[1mStep[0m  [32/84], [94mLoss[0m : 10.79673
[1mStep[0m  [40/84], [94mLoss[0m : 10.98967
[1mStep[0m  [48/84], [94mLoss[0m : 11.33303
[1mStep[0m  [56/84], [94mLoss[0m : 10.27807
[1mStep[0m  [64/84], [94mLoss[0m : 10.66666
[1mStep[0m  [72/84], [94mLoss[0m : 11.06334
[1mStep[0m  [80/84], [94mLoss[0m : 11.44793

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.837, [92mTest[0m: 10.871, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.31805
[1mStep[0m  [8/84], [94mLoss[0m : 10.80834
[1mStep[0m  [16/84], [94mLoss[0m : 10.81364
[1mStep[0m  [24/84], [94mLoss[0m : 10.40825
[1mStep[0m  [32/84], [94mLoss[0m : 10.60152
[1mStep[0m  [40/84], [94mLoss[0m : 11.16866
[1mStep[0m  [48/84], [94mLoss[0m : 10.19357
[1mStep[0m  [56/84], [94mLoss[0m : 10.83752
[1mStep[0m  [64/84], [94mLoss[0m : 10.65115
[1mStep[0m  [72/84], [94mLoss[0m : 10.81216
[1mStep[0m  [80/84], [94mLoss[0m : 10.37356

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.811, [92mTest[0m: 10.839, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.62973
[1mStep[0m  [8/84], [94mLoss[0m : 9.93369
[1mStep[0m  [16/84], [94mLoss[0m : 10.71288
[1mStep[0m  [24/84], [94mLoss[0m : 11.32285
[1mStep[0m  [32/84], [94mLoss[0m : 11.11161
[1mStep[0m  [40/84], [94mLoss[0m : 10.61506
[1mStep[0m  [48/84], [94mLoss[0m : 10.09855
[1mStep[0m  [56/84], [94mLoss[0m : 10.52230
[1mStep[0m  [64/84], [94mLoss[0m : 10.82493
[1mStep[0m  [72/84], [94mLoss[0m : 10.31177
[1mStep[0m  [80/84], [94mLoss[0m : 10.91681

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.786, [92mTest[0m: 10.809, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.13661
[1mStep[0m  [8/84], [94mLoss[0m : 10.53365
[1mStep[0m  [16/84], [94mLoss[0m : 11.10666
[1mStep[0m  [24/84], [94mLoss[0m : 10.65129
[1mStep[0m  [32/84], [94mLoss[0m : 11.06606
[1mStep[0m  [40/84], [94mLoss[0m : 10.76258
[1mStep[0m  [48/84], [94mLoss[0m : 10.70631
[1mStep[0m  [56/84], [94mLoss[0m : 11.08341
[1mStep[0m  [64/84], [94mLoss[0m : 11.05190
[1mStep[0m  [72/84], [94mLoss[0m : 10.83722
[1mStep[0m  [80/84], [94mLoss[0m : 11.24073

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.760, [92mTest[0m: 10.775, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.46865
[1mStep[0m  [8/84], [94mLoss[0m : 10.64580
[1mStep[0m  [16/84], [94mLoss[0m : 10.58658
[1mStep[0m  [24/84], [94mLoss[0m : 10.78944
[1mStep[0m  [32/84], [94mLoss[0m : 10.53824
[1mStep[0m  [40/84], [94mLoss[0m : 11.38233
[1mStep[0m  [48/84], [94mLoss[0m : 10.64856
[1mStep[0m  [56/84], [94mLoss[0m : 10.82190
[1mStep[0m  [64/84], [94mLoss[0m : 10.90544
[1mStep[0m  [72/84], [94mLoss[0m : 11.03241
[1mStep[0m  [80/84], [94mLoss[0m : 10.75166

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.728, [92mTest[0m: 10.742, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.61112
[1mStep[0m  [8/84], [94mLoss[0m : 11.14143
[1mStep[0m  [16/84], [94mLoss[0m : 10.85916
[1mStep[0m  [24/84], [94mLoss[0m : 10.94914
[1mStep[0m  [32/84], [94mLoss[0m : 10.97509
[1mStep[0m  [40/84], [94mLoss[0m : 10.16681
[1mStep[0m  [48/84], [94mLoss[0m : 10.40638
[1mStep[0m  [56/84], [94mLoss[0m : 10.38161
[1mStep[0m  [64/84], [94mLoss[0m : 11.02258
[1mStep[0m  [72/84], [94mLoss[0m : 10.67696
[1mStep[0m  [80/84], [94mLoss[0m : 10.50275

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.52180
[1mStep[0m  [8/84], [94mLoss[0m : 10.82078
[1mStep[0m  [16/84], [94mLoss[0m : 10.85537
[1mStep[0m  [24/84], [94mLoss[0m : 10.72114
[1mStep[0m  [32/84], [94mLoss[0m : 11.05600
[1mStep[0m  [40/84], [94mLoss[0m : 11.53223
[1mStep[0m  [48/84], [94mLoss[0m : 11.07451
[1mStep[0m  [56/84], [94mLoss[0m : 10.83045
[1mStep[0m  [64/84], [94mLoss[0m : 10.75445
[1mStep[0m  [72/84], [94mLoss[0m : 10.88654
[1mStep[0m  [80/84], [94mLoss[0m : 10.74438

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.690, [92mTest[0m: 10.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.56486
[1mStep[0m  [8/84], [94mLoss[0m : 10.43304
[1mStep[0m  [16/84], [94mLoss[0m : 10.61489
[1mStep[0m  [24/84], [94mLoss[0m : 10.56138
[1mStep[0m  [32/84], [94mLoss[0m : 10.57044
[1mStep[0m  [40/84], [94mLoss[0m : 10.96052
[1mStep[0m  [48/84], [94mLoss[0m : 10.25374
[1mStep[0m  [56/84], [94mLoss[0m : 10.92693
[1mStep[0m  [64/84], [94mLoss[0m : 10.73689
[1mStep[0m  [72/84], [94mLoss[0m : 10.69644
[1mStep[0m  [80/84], [94mLoss[0m : 10.65501

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.668, [92mTest[0m: 10.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.25724
[1mStep[0m  [8/84], [94mLoss[0m : 10.38302
[1mStep[0m  [16/84], [94mLoss[0m : 10.90253
[1mStep[0m  [24/84], [94mLoss[0m : 10.28201
[1mStep[0m  [32/84], [94mLoss[0m : 10.39883
[1mStep[0m  [40/84], [94mLoss[0m : 10.44423
[1mStep[0m  [48/84], [94mLoss[0m : 10.94975
[1mStep[0m  [56/84], [94mLoss[0m : 10.94739
[1mStep[0m  [64/84], [94mLoss[0m : 10.66367
[1mStep[0m  [72/84], [94mLoss[0m : 10.58793
[1mStep[0m  [80/84], [94mLoss[0m : 10.43410

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.75190
[1mStep[0m  [8/84], [94mLoss[0m : 10.42592
[1mStep[0m  [16/84], [94mLoss[0m : 10.81830
[1mStep[0m  [24/84], [94mLoss[0m : 10.96858
[1mStep[0m  [32/84], [94mLoss[0m : 10.83593
[1mStep[0m  [40/84], [94mLoss[0m : 10.54046
[1mStep[0m  [48/84], [94mLoss[0m : 10.38865
[1mStep[0m  [56/84], [94mLoss[0m : 10.57051
[1mStep[0m  [64/84], [94mLoss[0m : 11.48463
[1mStep[0m  [72/84], [94mLoss[0m : 10.63008
[1mStep[0m  [80/84], [94mLoss[0m : 10.53516

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.623, [92mTest[0m: 10.596, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.20298
[1mStep[0m  [8/84], [94mLoss[0m : 10.51014
[1mStep[0m  [16/84], [94mLoss[0m : 10.26205
[1mStep[0m  [24/84], [94mLoss[0m : 10.61417
[1mStep[0m  [32/84], [94mLoss[0m : 10.66082
[1mStep[0m  [40/84], [94mLoss[0m : 10.94800
[1mStep[0m  [48/84], [94mLoss[0m : 10.91772
[1mStep[0m  [56/84], [94mLoss[0m : 10.78459
[1mStep[0m  [64/84], [94mLoss[0m : 10.18994
[1mStep[0m  [72/84], [94mLoss[0m : 10.33566
[1mStep[0m  [80/84], [94mLoss[0m : 10.22461

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.593, [92mTest[0m: 10.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.76817
[1mStep[0m  [8/84], [94mLoss[0m : 10.53568
[1mStep[0m  [16/84], [94mLoss[0m : 10.24201
[1mStep[0m  [24/84], [94mLoss[0m : 10.70794
[1mStep[0m  [32/84], [94mLoss[0m : 10.47881
[1mStep[0m  [40/84], [94mLoss[0m : 10.26291
[1mStep[0m  [48/84], [94mLoss[0m : 10.69971
[1mStep[0m  [56/84], [94mLoss[0m : 10.67186
[1mStep[0m  [64/84], [94mLoss[0m : 9.98074
[1mStep[0m  [72/84], [94mLoss[0m : 10.65683
[1mStep[0m  [80/84], [94mLoss[0m : 10.73611

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.553, [92mTest[0m: 10.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.88317
[1mStep[0m  [8/84], [94mLoss[0m : 10.53355
[1mStep[0m  [16/84], [94mLoss[0m : 9.99762
[1mStep[0m  [24/84], [94mLoss[0m : 10.48530
[1mStep[0m  [32/84], [94mLoss[0m : 10.54590
[1mStep[0m  [40/84], [94mLoss[0m : 9.79962
[1mStep[0m  [48/84], [94mLoss[0m : 10.06256
[1mStep[0m  [56/84], [94mLoss[0m : 10.39860
[1mStep[0m  [64/84], [94mLoss[0m : 10.41390
[1mStep[0m  [72/84], [94mLoss[0m : 10.34544
[1mStep[0m  [80/84], [94mLoss[0m : 10.46655

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.539, [92mTest[0m: 10.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.21884
[1mStep[0m  [8/84], [94mLoss[0m : 10.42812
[1mStep[0m  [16/84], [94mLoss[0m : 10.93592
[1mStep[0m  [24/84], [94mLoss[0m : 10.32881
[1mStep[0m  [32/84], [94mLoss[0m : 10.62613
[1mStep[0m  [40/84], [94mLoss[0m : 10.67665
[1mStep[0m  [48/84], [94mLoss[0m : 11.35596
[1mStep[0m  [56/84], [94mLoss[0m : 10.65913
[1mStep[0m  [64/84], [94mLoss[0m : 10.22803
[1mStep[0m  [72/84], [94mLoss[0m : 10.88151
[1mStep[0m  [80/84], [94mLoss[0m : 10.27260

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.513, [92mTest[0m: 10.425, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.48372
[1mStep[0m  [8/84], [94mLoss[0m : 10.14583
[1mStep[0m  [16/84], [94mLoss[0m : 10.39753
[1mStep[0m  [24/84], [94mLoss[0m : 11.23192
[1mStep[0m  [32/84], [94mLoss[0m : 10.31521
[1mStep[0m  [40/84], [94mLoss[0m : 10.48594
[1mStep[0m  [48/84], [94mLoss[0m : 10.27772
[1mStep[0m  [56/84], [94mLoss[0m : 10.45078
[1mStep[0m  [64/84], [94mLoss[0m : 10.97652
[1mStep[0m  [72/84], [94mLoss[0m : 10.40045
[1mStep[0m  [80/84], [94mLoss[0m : 10.01312

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.485, [92mTest[0m: 10.406, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.37810
[1mStep[0m  [8/84], [94mLoss[0m : 10.52073
[1mStep[0m  [16/84], [94mLoss[0m : 10.00411
[1mStep[0m  [24/84], [94mLoss[0m : 10.60619
[1mStep[0m  [32/84], [94mLoss[0m : 9.99642
[1mStep[0m  [40/84], [94mLoss[0m : 10.28433
[1mStep[0m  [48/84], [94mLoss[0m : 10.47904
[1mStep[0m  [56/84], [94mLoss[0m : 10.64698
[1mStep[0m  [64/84], [94mLoss[0m : 10.61406
[1mStep[0m  [72/84], [94mLoss[0m : 10.07290
[1mStep[0m  [80/84], [94mLoss[0m : 10.00766

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.459, [92mTest[0m: 10.362, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.15109
[1mStep[0m  [8/84], [94mLoss[0m : 10.41007
[1mStep[0m  [16/84], [94mLoss[0m : 10.40693
[1mStep[0m  [24/84], [94mLoss[0m : 10.15956
[1mStep[0m  [32/84], [94mLoss[0m : 10.04271
[1mStep[0m  [40/84], [94mLoss[0m : 10.45667
[1mStep[0m  [48/84], [94mLoss[0m : 10.41960
[1mStep[0m  [56/84], [94mLoss[0m : 10.59352
[1mStep[0m  [64/84], [94mLoss[0m : 10.18797
[1mStep[0m  [72/84], [94mLoss[0m : 10.58689
[1mStep[0m  [80/84], [94mLoss[0m : 10.20878

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.350, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.61017
[1mStep[0m  [8/84], [94mLoss[0m : 10.10854
[1mStep[0m  [16/84], [94mLoss[0m : 10.12676
[1mStep[0m  [24/84], [94mLoss[0m : 10.78581
[1mStep[0m  [32/84], [94mLoss[0m : 10.83195
[1mStep[0m  [40/84], [94mLoss[0m : 10.54693
[1mStep[0m  [48/84], [94mLoss[0m : 10.06196
[1mStep[0m  [56/84], [94mLoss[0m : 10.77710
[1mStep[0m  [64/84], [94mLoss[0m : 10.12776
[1mStep[0m  [72/84], [94mLoss[0m : 10.45423
[1mStep[0m  [80/84], [94mLoss[0m : 10.15095

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.407, [92mTest[0m: 10.280, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.84264
[1mStep[0m  [8/84], [94mLoss[0m : 10.13105
[1mStep[0m  [16/84], [94mLoss[0m : 10.91983
[1mStep[0m  [24/84], [94mLoss[0m : 10.00260
[1mStep[0m  [32/84], [94mLoss[0m : 10.39444
[1mStep[0m  [40/84], [94mLoss[0m : 10.22480
[1mStep[0m  [48/84], [94mLoss[0m : 10.03557
[1mStep[0m  [56/84], [94mLoss[0m : 10.32260
[1mStep[0m  [64/84], [94mLoss[0m : 10.15911
[1mStep[0m  [72/84], [94mLoss[0m : 10.39628
[1mStep[0m  [80/84], [94mLoss[0m : 10.27217

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.382, [92mTest[0m: 10.293, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.92864
[1mStep[0m  [8/84], [94mLoss[0m : 10.19741
[1mStep[0m  [16/84], [94mLoss[0m : 9.87848
[1mStep[0m  [24/84], [94mLoss[0m : 10.00238
[1mStep[0m  [32/84], [94mLoss[0m : 10.53638
[1mStep[0m  [40/84], [94mLoss[0m : 9.89621
[1mStep[0m  [48/84], [94mLoss[0m : 10.65422
[1mStep[0m  [56/84], [94mLoss[0m : 10.36644
[1mStep[0m  [64/84], [94mLoss[0m : 10.75616
[1mStep[0m  [72/84], [94mLoss[0m : 10.36646
[1mStep[0m  [80/84], [94mLoss[0m : 10.18442

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.355, [92mTest[0m: 10.236, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.66067
[1mStep[0m  [8/84], [94mLoss[0m : 10.24657
[1mStep[0m  [16/84], [94mLoss[0m : 10.73961
[1mStep[0m  [24/84], [94mLoss[0m : 10.70735
[1mStep[0m  [32/84], [94mLoss[0m : 10.24453
[1mStep[0m  [40/84], [94mLoss[0m : 9.95708
[1mStep[0m  [48/84], [94mLoss[0m : 10.51905
[1mStep[0m  [56/84], [94mLoss[0m : 10.27794
[1mStep[0m  [64/84], [94mLoss[0m : 10.53358
[1mStep[0m  [72/84], [94mLoss[0m : 9.83618
[1mStep[0m  [80/84], [94mLoss[0m : 10.98268

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.332, [92mTest[0m: 10.167, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.17041
[1mStep[0m  [8/84], [94mLoss[0m : 9.99611
[1mStep[0m  [16/84], [94mLoss[0m : 10.55108
[1mStep[0m  [24/84], [94mLoss[0m : 10.41627
[1mStep[0m  [32/84], [94mLoss[0m : 10.38903
[1mStep[0m  [40/84], [94mLoss[0m : 10.21379
[1mStep[0m  [48/84], [94mLoss[0m : 10.68391
[1mStep[0m  [56/84], [94mLoss[0m : 10.58305
[1mStep[0m  [64/84], [94mLoss[0m : 9.83438
[1mStep[0m  [72/84], [94mLoss[0m : 10.31104
[1mStep[0m  [80/84], [94mLoss[0m : 10.16860

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.312, [92mTest[0m: 10.134, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.08444
[1mStep[0m  [8/84], [94mLoss[0m : 10.07520
[1mStep[0m  [16/84], [94mLoss[0m : 10.51968
[1mStep[0m  [24/84], [94mLoss[0m : 10.26570
[1mStep[0m  [32/84], [94mLoss[0m : 9.77040
[1mStep[0m  [40/84], [94mLoss[0m : 10.17272
[1mStep[0m  [48/84], [94mLoss[0m : 10.06559
[1mStep[0m  [56/84], [94mLoss[0m : 9.87099
[1mStep[0m  [64/84], [94mLoss[0m : 10.45412
[1mStep[0m  [72/84], [94mLoss[0m : 10.59709
[1mStep[0m  [80/84], [94mLoss[0m : 9.96558

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.291, [92mTest[0m: 10.124, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.85199
[1mStep[0m  [8/84], [94mLoss[0m : 10.44166
[1mStep[0m  [16/84], [94mLoss[0m : 10.41364
[1mStep[0m  [24/84], [94mLoss[0m : 10.43505
[1mStep[0m  [32/84], [94mLoss[0m : 10.16833
[1mStep[0m  [40/84], [94mLoss[0m : 9.71161
[1mStep[0m  [48/84], [94mLoss[0m : 10.16537
[1mStep[0m  [56/84], [94mLoss[0m : 10.97525
[1mStep[0m  [64/84], [94mLoss[0m : 10.30031
[1mStep[0m  [72/84], [94mLoss[0m : 10.63761
[1mStep[0m  [80/84], [94mLoss[0m : 9.92438

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.261, [92mTest[0m: 10.093, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.50271
[1mStep[0m  [8/84], [94mLoss[0m : 9.37717
[1mStep[0m  [16/84], [94mLoss[0m : 9.86036
[1mStep[0m  [24/84], [94mLoss[0m : 9.75197
[1mStep[0m  [32/84], [94mLoss[0m : 10.49503
[1mStep[0m  [40/84], [94mLoss[0m : 10.58757
[1mStep[0m  [48/84], [94mLoss[0m : 10.48192
[1mStep[0m  [56/84], [94mLoss[0m : 10.29062
[1mStep[0m  [64/84], [94mLoss[0m : 10.06267
[1mStep[0m  [72/84], [94mLoss[0m : 10.78555
[1mStep[0m  [80/84], [94mLoss[0m : 10.19493

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.240, [92mTest[0m: 10.075, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.41694
[1mStep[0m  [8/84], [94mLoss[0m : 10.28562
[1mStep[0m  [16/84], [94mLoss[0m : 10.23909
[1mStep[0m  [24/84], [94mLoss[0m : 9.68047
[1mStep[0m  [32/84], [94mLoss[0m : 10.16304
[1mStep[0m  [40/84], [94mLoss[0m : 10.23479
[1mStep[0m  [48/84], [94mLoss[0m : 10.34035
[1mStep[0m  [56/84], [94mLoss[0m : 10.29038
[1mStep[0m  [64/84], [94mLoss[0m : 10.63941
[1mStep[0m  [72/84], [94mLoss[0m : 10.36630
[1mStep[0m  [80/84], [94mLoss[0m : 9.59204

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.218, [92mTest[0m: 10.057, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.81383
[1mStep[0m  [8/84], [94mLoss[0m : 10.25366
[1mStep[0m  [16/84], [94mLoss[0m : 10.21611
[1mStep[0m  [24/84], [94mLoss[0m : 10.28564
[1mStep[0m  [32/84], [94mLoss[0m : 10.48722
[1mStep[0m  [40/84], [94mLoss[0m : 10.20218
[1mStep[0m  [48/84], [94mLoss[0m : 10.44215
[1mStep[0m  [56/84], [94mLoss[0m : 10.21312
[1mStep[0m  [64/84], [94mLoss[0m : 9.64352
[1mStep[0m  [72/84], [94mLoss[0m : 10.16860
[1mStep[0m  [80/84], [94mLoss[0m : 10.28301

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.183, [92mTest[0m: 10.031, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.96025
[1mStep[0m  [8/84], [94mLoss[0m : 10.17875
[1mStep[0m  [16/84], [94mLoss[0m : 10.44787
[1mStep[0m  [24/84], [94mLoss[0m : 10.34492
[1mStep[0m  [32/84], [94mLoss[0m : 10.29420
[1mStep[0m  [40/84], [94mLoss[0m : 10.65714
[1mStep[0m  [48/84], [94mLoss[0m : 9.69418
[1mStep[0m  [56/84], [94mLoss[0m : 10.20257
[1mStep[0m  [64/84], [94mLoss[0m : 10.05440
[1mStep[0m  [72/84], [94mLoss[0m : 10.30070
[1mStep[0m  [80/84], [94mLoss[0m : 10.05294

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.172, [92mTest[0m: 9.969, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.75588
[1mStep[0m  [8/84], [94mLoss[0m : 9.75517
[1mStep[0m  [16/84], [94mLoss[0m : 10.46247
[1mStep[0m  [24/84], [94mLoss[0m : 10.48760
[1mStep[0m  [32/84], [94mLoss[0m : 9.65857
[1mStep[0m  [40/84], [94mLoss[0m : 10.34081
[1mStep[0m  [48/84], [94mLoss[0m : 10.82315
[1mStep[0m  [56/84], [94mLoss[0m : 10.16110
[1mStep[0m  [64/84], [94mLoss[0m : 9.74490
[1mStep[0m  [72/84], [94mLoss[0m : 10.16500
[1mStep[0m  [80/84], [94mLoss[0m : 10.35538

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.145, [92mTest[0m: 9.947, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.938
====================================

Phase 1 - Evaluation MAE:  9.938068662370954
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.16939
[1mStep[0m  [8/84], [94mLoss[0m : 10.09811
[1mStep[0m  [16/84], [94mLoss[0m : 10.40541
[1mStep[0m  [24/84], [94mLoss[0m : 9.96121
[1mStep[0m  [32/84], [94mLoss[0m : 10.42054
[1mStep[0m  [40/84], [94mLoss[0m : 10.29864
[1mStep[0m  [48/84], [94mLoss[0m : 9.68236
[1mStep[0m  [56/84], [94mLoss[0m : 10.40698
[1mStep[0m  [64/84], [94mLoss[0m : 10.16117
[1mStep[0m  [72/84], [94mLoss[0m : 10.67008
[1mStep[0m  [80/84], [94mLoss[0m : 10.20557

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.109, [92mTest[0m: 9.929, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.82981
[1mStep[0m  [8/84], [94mLoss[0m : 10.30004
[1mStep[0m  [16/84], [94mLoss[0m : 9.64445
[1mStep[0m  [24/84], [94mLoss[0m : 10.14021
[1mStep[0m  [32/84], [94mLoss[0m : 9.45809
[1mStep[0m  [40/84], [94mLoss[0m : 9.99748
[1mStep[0m  [48/84], [94mLoss[0m : 9.83621
[1mStep[0m  [56/84], [94mLoss[0m : 10.17480
[1mStep[0m  [64/84], [94mLoss[0m : 10.22452
[1mStep[0m  [72/84], [94mLoss[0m : 10.23420
[1mStep[0m  [80/84], [94mLoss[0m : 9.59319

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.067, [92mTest[0m: 9.896, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.75938
[1mStep[0m  [8/84], [94mLoss[0m : 9.71704
[1mStep[0m  [16/84], [94mLoss[0m : 10.09238
[1mStep[0m  [24/84], [94mLoss[0m : 10.16674
[1mStep[0m  [32/84], [94mLoss[0m : 9.76101
[1mStep[0m  [40/84], [94mLoss[0m : 10.23154
[1mStep[0m  [48/84], [94mLoss[0m : 9.67207
[1mStep[0m  [56/84], [94mLoss[0m : 10.23845
[1mStep[0m  [64/84], [94mLoss[0m : 10.55127
[1mStep[0m  [72/84], [94mLoss[0m : 10.27207
[1mStep[0m  [80/84], [94mLoss[0m : 10.16288

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.014, [92mTest[0m: 9.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.27785
[1mStep[0m  [8/84], [94mLoss[0m : 9.80090
[1mStep[0m  [16/84], [94mLoss[0m : 10.08035
[1mStep[0m  [24/84], [94mLoss[0m : 9.85021
[1mStep[0m  [32/84], [94mLoss[0m : 9.35405
[1mStep[0m  [40/84], [94mLoss[0m : 9.71038
[1mStep[0m  [48/84], [94mLoss[0m : 9.66006
[1mStep[0m  [56/84], [94mLoss[0m : 9.62544
[1mStep[0m  [64/84], [94mLoss[0m : 10.55268
[1mStep[0m  [72/84], [94mLoss[0m : 10.29307
[1mStep[0m  [80/84], [94mLoss[0m : 9.86337

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.948, [92mTest[0m: 9.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.06181
[1mStep[0m  [8/84], [94mLoss[0m : 9.48152
[1mStep[0m  [16/84], [94mLoss[0m : 9.56089
[1mStep[0m  [24/84], [94mLoss[0m : 10.11640
[1mStep[0m  [32/84], [94mLoss[0m : 9.56440
[1mStep[0m  [40/84], [94mLoss[0m : 10.10929
[1mStep[0m  [48/84], [94mLoss[0m : 9.65778
[1mStep[0m  [56/84], [94mLoss[0m : 9.69189
[1mStep[0m  [64/84], [94mLoss[0m : 10.30075
[1mStep[0m  [72/84], [94mLoss[0m : 9.91848
[1mStep[0m  [80/84], [94mLoss[0m : 9.69534

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.902, [92mTest[0m: 9.758, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.55976
[1mStep[0m  [8/84], [94mLoss[0m : 9.73109
[1mStep[0m  [16/84], [94mLoss[0m : 9.48144
[1mStep[0m  [24/84], [94mLoss[0m : 9.71206
[1mStep[0m  [32/84], [94mLoss[0m : 9.40276
[1mStep[0m  [40/84], [94mLoss[0m : 10.34009
[1mStep[0m  [48/84], [94mLoss[0m : 10.01468
[1mStep[0m  [56/84], [94mLoss[0m : 9.81973
[1mStep[0m  [64/84], [94mLoss[0m : 9.60918
[1mStep[0m  [72/84], [94mLoss[0m : 10.19922
[1mStep[0m  [80/84], [94mLoss[0m : 10.06175

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.850, [92mTest[0m: 9.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.42697
[1mStep[0m  [8/84], [94mLoss[0m : 9.37115
[1mStep[0m  [16/84], [94mLoss[0m : 9.31373
[1mStep[0m  [24/84], [94mLoss[0m : 9.37265
[1mStep[0m  [32/84], [94mLoss[0m : 9.16755
[1mStep[0m  [40/84], [94mLoss[0m : 10.29117
[1mStep[0m  [48/84], [94mLoss[0m : 10.05451
[1mStep[0m  [56/84], [94mLoss[0m : 9.82141
[1mStep[0m  [64/84], [94mLoss[0m : 10.00520
[1mStep[0m  [72/84], [94mLoss[0m : 9.98636
[1mStep[0m  [80/84], [94mLoss[0m : 10.17817

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.807, [92mTest[0m: 9.702, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.47452
[1mStep[0m  [8/84], [94mLoss[0m : 9.66308
[1mStep[0m  [16/84], [94mLoss[0m : 9.53864
[1mStep[0m  [24/84], [94mLoss[0m : 9.68796
[1mStep[0m  [32/84], [94mLoss[0m : 9.88939
[1mStep[0m  [40/84], [94mLoss[0m : 10.37183
[1mStep[0m  [48/84], [94mLoss[0m : 9.63844
[1mStep[0m  [56/84], [94mLoss[0m : 9.80319
[1mStep[0m  [64/84], [94mLoss[0m : 8.81842
[1mStep[0m  [72/84], [94mLoss[0m : 9.43886
[1mStep[0m  [80/84], [94mLoss[0m : 10.08498

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.747, [92mTest[0m: 9.661, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.03590
[1mStep[0m  [8/84], [94mLoss[0m : 9.94534
[1mStep[0m  [16/84], [94mLoss[0m : 9.34815
[1mStep[0m  [24/84], [94mLoss[0m : 10.18587
[1mStep[0m  [32/84], [94mLoss[0m : 9.84039
[1mStep[0m  [40/84], [94mLoss[0m : 9.71413
[1mStep[0m  [48/84], [94mLoss[0m : 9.78085
[1mStep[0m  [56/84], [94mLoss[0m : 10.23391
[1mStep[0m  [64/84], [94mLoss[0m : 10.04330
[1mStep[0m  [72/84], [94mLoss[0m : 9.35499
[1mStep[0m  [80/84], [94mLoss[0m : 9.50451

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.698, [92mTest[0m: 9.527, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.41278
[1mStep[0m  [8/84], [94mLoss[0m : 9.94545
[1mStep[0m  [16/84], [94mLoss[0m : 9.53863
[1mStep[0m  [24/84], [94mLoss[0m : 9.54331
[1mStep[0m  [32/84], [94mLoss[0m : 8.86950
[1mStep[0m  [40/84], [94mLoss[0m : 9.96488
[1mStep[0m  [48/84], [94mLoss[0m : 9.91047
[1mStep[0m  [56/84], [94mLoss[0m : 9.99658
[1mStep[0m  [64/84], [94mLoss[0m : 9.73423
[1mStep[0m  [72/84], [94mLoss[0m : 9.99704
[1mStep[0m  [80/84], [94mLoss[0m : 9.57370

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.658, [92mTest[0m: 9.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.53632
[1mStep[0m  [8/84], [94mLoss[0m : 9.72287
[1mStep[0m  [16/84], [94mLoss[0m : 9.97101
[1mStep[0m  [24/84], [94mLoss[0m : 9.83810
[1mStep[0m  [32/84], [94mLoss[0m : 9.79640
[1mStep[0m  [40/84], [94mLoss[0m : 9.32127
[1mStep[0m  [48/84], [94mLoss[0m : 9.06677
[1mStep[0m  [56/84], [94mLoss[0m : 9.35103
[1mStep[0m  [64/84], [94mLoss[0m : 9.82455
[1mStep[0m  [72/84], [94mLoss[0m : 9.85252
[1mStep[0m  [80/84], [94mLoss[0m : 9.07858

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.608, [92mTest[0m: 9.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.48659
[1mStep[0m  [8/84], [94mLoss[0m : 9.79573
[1mStep[0m  [16/84], [94mLoss[0m : 9.78600
[1mStep[0m  [24/84], [94mLoss[0m : 9.23561
[1mStep[0m  [32/84], [94mLoss[0m : 10.11378
[1mStep[0m  [40/84], [94mLoss[0m : 8.93276
[1mStep[0m  [48/84], [94mLoss[0m : 9.48616
[1mStep[0m  [56/84], [94mLoss[0m : 9.69483
[1mStep[0m  [64/84], [94mLoss[0m : 9.20587
[1mStep[0m  [72/84], [94mLoss[0m : 9.61543
[1mStep[0m  [80/84], [94mLoss[0m : 9.83563

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.557, [92mTest[0m: 9.330, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.19419
[1mStep[0m  [8/84], [94mLoss[0m : 9.45207
[1mStep[0m  [16/84], [94mLoss[0m : 9.06756
[1mStep[0m  [24/84], [94mLoss[0m : 9.31589
[1mStep[0m  [32/84], [94mLoss[0m : 9.67091
[1mStep[0m  [40/84], [94mLoss[0m : 10.24181
[1mStep[0m  [48/84], [94mLoss[0m : 9.12829
[1mStep[0m  [56/84], [94mLoss[0m : 10.13414
[1mStep[0m  [64/84], [94mLoss[0m : 9.63321
[1mStep[0m  [72/84], [94mLoss[0m : 9.74911
[1mStep[0m  [80/84], [94mLoss[0m : 9.17491

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.517, [92mTest[0m: 9.328, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.28741
[1mStep[0m  [8/84], [94mLoss[0m : 9.55625
[1mStep[0m  [16/84], [94mLoss[0m : 9.76065
[1mStep[0m  [24/84], [94mLoss[0m : 9.38892
[1mStep[0m  [32/84], [94mLoss[0m : 9.36365
[1mStep[0m  [40/84], [94mLoss[0m : 9.40054
[1mStep[0m  [48/84], [94mLoss[0m : 8.76060
[1mStep[0m  [56/84], [94mLoss[0m : 10.27084
[1mStep[0m  [64/84], [94mLoss[0m : 8.94620
[1mStep[0m  [72/84], [94mLoss[0m : 9.18423
[1mStep[0m  [80/84], [94mLoss[0m : 9.63042

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.468, [92mTest[0m: 9.350, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.66116
[1mStep[0m  [8/84], [94mLoss[0m : 10.19513
[1mStep[0m  [16/84], [94mLoss[0m : 9.87688
[1mStep[0m  [24/84], [94mLoss[0m : 9.17349
[1mStep[0m  [32/84], [94mLoss[0m : 9.36383
[1mStep[0m  [40/84], [94mLoss[0m : 9.30524
[1mStep[0m  [48/84], [94mLoss[0m : 9.27485
[1mStep[0m  [56/84], [94mLoss[0m : 9.20241
[1mStep[0m  [64/84], [94mLoss[0m : 9.28456
[1mStep[0m  [72/84], [94mLoss[0m : 9.00646
[1mStep[0m  [80/84], [94mLoss[0m : 9.67184

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.419, [92mTest[0m: 9.248, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.74423
[1mStep[0m  [8/84], [94mLoss[0m : 9.27044
[1mStep[0m  [16/84], [94mLoss[0m : 9.37164
[1mStep[0m  [24/84], [94mLoss[0m : 9.52112
[1mStep[0m  [32/84], [94mLoss[0m : 9.22961
[1mStep[0m  [40/84], [94mLoss[0m : 9.97999
[1mStep[0m  [48/84], [94mLoss[0m : 9.41688
[1mStep[0m  [56/84], [94mLoss[0m : 8.83662
[1mStep[0m  [64/84], [94mLoss[0m : 9.05826
[1mStep[0m  [72/84], [94mLoss[0m : 9.62260
[1mStep[0m  [80/84], [94mLoss[0m : 9.17452

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.361, [92mTest[0m: 9.128, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.03088
[1mStep[0m  [8/84], [94mLoss[0m : 9.27851
[1mStep[0m  [16/84], [94mLoss[0m : 9.01986
[1mStep[0m  [24/84], [94mLoss[0m : 9.26552
[1mStep[0m  [32/84], [94mLoss[0m : 9.25352
[1mStep[0m  [40/84], [94mLoss[0m : 9.24922
[1mStep[0m  [48/84], [94mLoss[0m : 9.23371
[1mStep[0m  [56/84], [94mLoss[0m : 9.19234
[1mStep[0m  [64/84], [94mLoss[0m : 8.97321
[1mStep[0m  [72/84], [94mLoss[0m : 9.32067
[1mStep[0m  [80/84], [94mLoss[0m : 8.56160

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.304, [92mTest[0m: 9.152, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.86835
[1mStep[0m  [8/84], [94mLoss[0m : 9.09428
[1mStep[0m  [16/84], [94mLoss[0m : 9.36685
[1mStep[0m  [24/84], [94mLoss[0m : 8.84724
[1mStep[0m  [32/84], [94mLoss[0m : 9.08370
[1mStep[0m  [40/84], [94mLoss[0m : 9.72814
[1mStep[0m  [48/84], [94mLoss[0m : 9.33644
[1mStep[0m  [56/84], [94mLoss[0m : 9.55072
[1mStep[0m  [64/84], [94mLoss[0m : 8.84738
[1mStep[0m  [72/84], [94mLoss[0m : 9.20444
[1mStep[0m  [80/84], [94mLoss[0m : 9.32662

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.255, [92mTest[0m: 9.025, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.53523
[1mStep[0m  [8/84], [94mLoss[0m : 9.09655
[1mStep[0m  [16/84], [94mLoss[0m : 9.25941
[1mStep[0m  [24/84], [94mLoss[0m : 8.57064
[1mStep[0m  [32/84], [94mLoss[0m : 9.21622
[1mStep[0m  [40/84], [94mLoss[0m : 8.96438
[1mStep[0m  [48/84], [94mLoss[0m : 9.06515
[1mStep[0m  [56/84], [94mLoss[0m : 9.38821
[1mStep[0m  [64/84], [94mLoss[0m : 8.95261
[1mStep[0m  [72/84], [94mLoss[0m : 9.35183
[1mStep[0m  [80/84], [94mLoss[0m : 9.50401

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.205, [92mTest[0m: 8.925, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.99337
[1mStep[0m  [8/84], [94mLoss[0m : 9.29741
[1mStep[0m  [16/84], [94mLoss[0m : 8.91837
[1mStep[0m  [24/84], [94mLoss[0m : 8.94028
[1mStep[0m  [32/84], [94mLoss[0m : 8.98694
[1mStep[0m  [40/84], [94mLoss[0m : 9.26039
[1mStep[0m  [48/84], [94mLoss[0m : 9.24924
[1mStep[0m  [56/84], [94mLoss[0m : 8.83335
[1mStep[0m  [64/84], [94mLoss[0m : 9.27134
[1mStep[0m  [72/84], [94mLoss[0m : 9.24101
[1mStep[0m  [80/84], [94mLoss[0m : 8.63097

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.156, [92mTest[0m: 8.977, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.19395
[1mStep[0m  [8/84], [94mLoss[0m : 8.65617
[1mStep[0m  [16/84], [94mLoss[0m : 8.99798
[1mStep[0m  [24/84], [94mLoss[0m : 8.59139
[1mStep[0m  [32/84], [94mLoss[0m : 9.24103
[1mStep[0m  [40/84], [94mLoss[0m : 9.26569
[1mStep[0m  [48/84], [94mLoss[0m : 9.19363
[1mStep[0m  [56/84], [94mLoss[0m : 8.83697
[1mStep[0m  [64/84], [94mLoss[0m : 8.97181
[1mStep[0m  [72/84], [94mLoss[0m : 8.68903
[1mStep[0m  [80/84], [94mLoss[0m : 8.88629

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.097, [92mTest[0m: 8.870, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.00858
[1mStep[0m  [8/84], [94mLoss[0m : 8.33897
[1mStep[0m  [16/84], [94mLoss[0m : 8.75338
[1mStep[0m  [24/84], [94mLoss[0m : 9.23987
[1mStep[0m  [32/84], [94mLoss[0m : 9.18178
[1mStep[0m  [40/84], [94mLoss[0m : 9.49986
[1mStep[0m  [48/84], [94mLoss[0m : 8.92612
[1mStep[0m  [56/84], [94mLoss[0m : 9.45698
[1mStep[0m  [64/84], [94mLoss[0m : 9.21599
[1mStep[0m  [72/84], [94mLoss[0m : 9.19557
[1mStep[0m  [80/84], [94mLoss[0m : 9.16199

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.063, [92mTest[0m: 8.776, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.93212
[1mStep[0m  [8/84], [94mLoss[0m : 9.49908
[1mStep[0m  [16/84], [94mLoss[0m : 9.11217
[1mStep[0m  [24/84], [94mLoss[0m : 9.71331
[1mStep[0m  [32/84], [94mLoss[0m : 9.04881
[1mStep[0m  [40/84], [94mLoss[0m : 8.56899
[1mStep[0m  [48/84], [94mLoss[0m : 8.59277
[1mStep[0m  [56/84], [94mLoss[0m : 8.94742
[1mStep[0m  [64/84], [94mLoss[0m : 9.55210
[1mStep[0m  [72/84], [94mLoss[0m : 9.42904
[1mStep[0m  [80/84], [94mLoss[0m : 8.61101

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.998, [92mTest[0m: 8.732, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.07323
[1mStep[0m  [8/84], [94mLoss[0m : 9.25752
[1mStep[0m  [16/84], [94mLoss[0m : 8.69692
[1mStep[0m  [24/84], [94mLoss[0m : 9.05058
[1mStep[0m  [32/84], [94mLoss[0m : 9.44738
[1mStep[0m  [40/84], [94mLoss[0m : 9.19209
[1mStep[0m  [48/84], [94mLoss[0m : 9.20775
[1mStep[0m  [56/84], [94mLoss[0m : 9.36998
[1mStep[0m  [64/84], [94mLoss[0m : 8.93416
[1mStep[0m  [72/84], [94mLoss[0m : 9.20535
[1mStep[0m  [80/84], [94mLoss[0m : 9.06255

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.944, [92mTest[0m: 8.840, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.67507
[1mStep[0m  [8/84], [94mLoss[0m : 8.86123
[1mStep[0m  [16/84], [94mLoss[0m : 8.86323
[1mStep[0m  [24/84], [94mLoss[0m : 8.96663
[1mStep[0m  [32/84], [94mLoss[0m : 9.13031
[1mStep[0m  [40/84], [94mLoss[0m : 8.74288
[1mStep[0m  [48/84], [94mLoss[0m : 8.65695
[1mStep[0m  [56/84], [94mLoss[0m : 8.97951
[1mStep[0m  [64/84], [94mLoss[0m : 9.37900
[1mStep[0m  [72/84], [94mLoss[0m : 8.91222
[1mStep[0m  [80/84], [94mLoss[0m : 9.35495

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.903, [92mTest[0m: 8.632, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.75688
[1mStep[0m  [8/84], [94mLoss[0m : 8.61530
[1mStep[0m  [16/84], [94mLoss[0m : 8.85203
[1mStep[0m  [24/84], [94mLoss[0m : 9.28256
[1mStep[0m  [32/84], [94mLoss[0m : 9.00022
[1mStep[0m  [40/84], [94mLoss[0m : 8.97691
[1mStep[0m  [48/84], [94mLoss[0m : 8.85795
[1mStep[0m  [56/84], [94mLoss[0m : 8.44079
[1mStep[0m  [64/84], [94mLoss[0m : 8.30560
[1mStep[0m  [72/84], [94mLoss[0m : 8.67800
[1mStep[0m  [80/84], [94mLoss[0m : 8.15904

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.844, [92mTest[0m: 8.635, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.63761
[1mStep[0m  [8/84], [94mLoss[0m : 9.03961
[1mStep[0m  [16/84], [94mLoss[0m : 8.48933
[1mStep[0m  [24/84], [94mLoss[0m : 9.21041
[1mStep[0m  [32/84], [94mLoss[0m : 9.02967
[1mStep[0m  [40/84], [94mLoss[0m : 8.92866
[1mStep[0m  [48/84], [94mLoss[0m : 8.77356
[1mStep[0m  [56/84], [94mLoss[0m : 8.89164
[1mStep[0m  [64/84], [94mLoss[0m : 9.31187
[1mStep[0m  [72/84], [94mLoss[0m : 8.57352
[1mStep[0m  [80/84], [94mLoss[0m : 9.22198

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.789, [92mTest[0m: 8.565, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.84534
[1mStep[0m  [8/84], [94mLoss[0m : 8.70603
[1mStep[0m  [16/84], [94mLoss[0m : 8.86386
[1mStep[0m  [24/84], [94mLoss[0m : 8.59710
[1mStep[0m  [32/84], [94mLoss[0m : 8.34506
[1mStep[0m  [40/84], [94mLoss[0m : 8.99778
[1mStep[0m  [48/84], [94mLoss[0m : 8.61962
[1mStep[0m  [56/84], [94mLoss[0m : 8.45826
[1mStep[0m  [64/84], [94mLoss[0m : 9.43801
[1mStep[0m  [72/84], [94mLoss[0m : 8.93985
[1mStep[0m  [80/84], [94mLoss[0m : 9.09382

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.727, [92mTest[0m: 8.488, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.03754
[1mStep[0m  [8/84], [94mLoss[0m : 9.08078
[1mStep[0m  [16/84], [94mLoss[0m : 9.07520
[1mStep[0m  [24/84], [94mLoss[0m : 8.93896
[1mStep[0m  [32/84], [94mLoss[0m : 8.52240
[1mStep[0m  [40/84], [94mLoss[0m : 8.78207
[1mStep[0m  [48/84], [94mLoss[0m : 8.75082
[1mStep[0m  [56/84], [94mLoss[0m : 8.87165
[1mStep[0m  [64/84], [94mLoss[0m : 8.92558
[1mStep[0m  [72/84], [94mLoss[0m : 8.14898
[1mStep[0m  [80/84], [94mLoss[0m : 8.38674

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.679, [92mTest[0m: 8.417, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.67639
[1mStep[0m  [8/84], [94mLoss[0m : 8.10586
[1mStep[0m  [16/84], [94mLoss[0m : 8.25245
[1mStep[0m  [24/84], [94mLoss[0m : 8.84357
[1mStep[0m  [32/84], [94mLoss[0m : 8.82032
[1mStep[0m  [40/84], [94mLoss[0m : 8.42953
[1mStep[0m  [48/84], [94mLoss[0m : 8.50762
[1mStep[0m  [56/84], [94mLoss[0m : 8.64222
[1mStep[0m  [64/84], [94mLoss[0m : 8.60501
[1mStep[0m  [72/84], [94mLoss[0m : 8.00683
[1mStep[0m  [80/84], [94mLoss[0m : 8.73788

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.627, [92mTest[0m: 8.303, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.253
====================================

Phase 2 - Evaluation MAE:  8.253225479807172
MAE score P1      9.938069
MAE score P2      8.253225
loss              8.626815
learning_rate       0.0001
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 17, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.57689
[1mStep[0m  [16/169], [94mLoss[0m : 10.13870
[1mStep[0m  [32/169], [94mLoss[0m : 10.58673
[1mStep[0m  [48/169], [94mLoss[0m : 11.54284
[1mStep[0m  [64/169], [94mLoss[0m : 11.05299
[1mStep[0m  [80/169], [94mLoss[0m : 10.30174
[1mStep[0m  [96/169], [94mLoss[0m : 11.72624
[1mStep[0m  [112/169], [94mLoss[0m : 10.45043
[1mStep[0m  [128/169], [94mLoss[0m : 9.89479
[1mStep[0m  [144/169], [94mLoss[0m : 10.36926
[1mStep[0m  [160/169], [94mLoss[0m : 10.89429

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.679, [92mTest[0m: 10.973, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.52255
[1mStep[0m  [16/169], [94mLoss[0m : 10.76931
[1mStep[0m  [32/169], [94mLoss[0m : 11.25666
[1mStep[0m  [48/169], [94mLoss[0m : 10.68334
[1mStep[0m  [64/169], [94mLoss[0m : 10.75125
[1mStep[0m  [80/169], [94mLoss[0m : 10.91521
[1mStep[0m  [96/169], [94mLoss[0m : 9.77410
[1mStep[0m  [112/169], [94mLoss[0m : 10.37978
[1mStep[0m  [128/169], [94mLoss[0m : 10.54433
[1mStep[0m  [144/169], [94mLoss[0m : 9.94049
[1mStep[0m  [160/169], [94mLoss[0m : 10.00870

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.508, [92mTest[0m: 10.709, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.34767
[1mStep[0m  [16/169], [94mLoss[0m : 10.38292
[1mStep[0m  [32/169], [94mLoss[0m : 10.97864
[1mStep[0m  [48/169], [94mLoss[0m : 10.45371
[1mStep[0m  [64/169], [94mLoss[0m : 9.10717
[1mStep[0m  [80/169], [94mLoss[0m : 9.83786
[1mStep[0m  [96/169], [94mLoss[0m : 10.13074
[1mStep[0m  [112/169], [94mLoss[0m : 11.16129
[1mStep[0m  [128/169], [94mLoss[0m : 10.34882
[1mStep[0m  [144/169], [94mLoss[0m : 9.85142
[1mStep[0m  [160/169], [94mLoss[0m : 10.03423

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.349, [92mTest[0m: 10.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.83027
[1mStep[0m  [16/169], [94mLoss[0m : 10.33732
[1mStep[0m  [32/169], [94mLoss[0m : 9.22689
[1mStep[0m  [48/169], [94mLoss[0m : 10.22817
[1mStep[0m  [64/169], [94mLoss[0m : 10.64252
[1mStep[0m  [80/169], [94mLoss[0m : 10.55633
[1mStep[0m  [96/169], [94mLoss[0m : 10.24021
[1mStep[0m  [112/169], [94mLoss[0m : 9.67737
[1mStep[0m  [128/169], [94mLoss[0m : 10.59736
[1mStep[0m  [144/169], [94mLoss[0m : 10.34692
[1mStep[0m  [160/169], [94mLoss[0m : 10.53961

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.176, [92mTest[0m: 10.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.73089
[1mStep[0m  [16/169], [94mLoss[0m : 9.77528
[1mStep[0m  [32/169], [94mLoss[0m : 10.24389
[1mStep[0m  [48/169], [94mLoss[0m : 9.76600
[1mStep[0m  [64/169], [94mLoss[0m : 10.03057
[1mStep[0m  [80/169], [94mLoss[0m : 10.20166
[1mStep[0m  [96/169], [94mLoss[0m : 9.84370
[1mStep[0m  [112/169], [94mLoss[0m : 10.42670
[1mStep[0m  [128/169], [94mLoss[0m : 9.47451
[1mStep[0m  [144/169], [94mLoss[0m : 9.77888
[1mStep[0m  [160/169], [94mLoss[0m : 10.10789

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.011, [92mTest[0m: 10.305, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.43322
[1mStep[0m  [16/169], [94mLoss[0m : 9.74316
[1mStep[0m  [32/169], [94mLoss[0m : 9.71048
[1mStep[0m  [48/169], [94mLoss[0m : 9.81430
[1mStep[0m  [64/169], [94mLoss[0m : 9.57857
[1mStep[0m  [80/169], [94mLoss[0m : 10.20999
[1mStep[0m  [96/169], [94mLoss[0m : 10.13788
[1mStep[0m  [112/169], [94mLoss[0m : 10.26609
[1mStep[0m  [128/169], [94mLoss[0m : 10.15696
[1mStep[0m  [144/169], [94mLoss[0m : 9.35857
[1mStep[0m  [160/169], [94mLoss[0m : 10.04111

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.848, [92mTest[0m: 10.166, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.69412
[1mStep[0m  [16/169], [94mLoss[0m : 9.57746
[1mStep[0m  [32/169], [94mLoss[0m : 9.55097
[1mStep[0m  [48/169], [94mLoss[0m : 11.35086
[1mStep[0m  [64/169], [94mLoss[0m : 9.47757
[1mStep[0m  [80/169], [94mLoss[0m : 9.07962
[1mStep[0m  [96/169], [94mLoss[0m : 8.68329
[1mStep[0m  [112/169], [94mLoss[0m : 9.73816
[1mStep[0m  [128/169], [94mLoss[0m : 9.61926
[1mStep[0m  [144/169], [94mLoss[0m : 9.97848
[1mStep[0m  [160/169], [94mLoss[0m : 9.50226

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.671, [92mTest[0m: 10.051, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.87913
[1mStep[0m  [16/169], [94mLoss[0m : 8.59488
[1mStep[0m  [32/169], [94mLoss[0m : 9.71369
[1mStep[0m  [48/169], [94mLoss[0m : 10.31038
[1mStep[0m  [64/169], [94mLoss[0m : 9.66375
[1mStep[0m  [80/169], [94mLoss[0m : 9.25959
[1mStep[0m  [96/169], [94mLoss[0m : 9.52868
[1mStep[0m  [112/169], [94mLoss[0m : 9.51729
[1mStep[0m  [128/169], [94mLoss[0m : 9.61668
[1mStep[0m  [144/169], [94mLoss[0m : 9.65421
[1mStep[0m  [160/169], [94mLoss[0m : 9.27005

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.509, [92mTest[0m: 9.940, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.15236
[1mStep[0m  [16/169], [94mLoss[0m : 9.58334
[1mStep[0m  [32/169], [94mLoss[0m : 9.46099
[1mStep[0m  [48/169], [94mLoss[0m : 8.41638
[1mStep[0m  [64/169], [94mLoss[0m : 9.04589
[1mStep[0m  [80/169], [94mLoss[0m : 9.16637
[1mStep[0m  [96/169], [94mLoss[0m : 9.30746
[1mStep[0m  [112/169], [94mLoss[0m : 8.74072
[1mStep[0m  [128/169], [94mLoss[0m : 9.11687
[1mStep[0m  [144/169], [94mLoss[0m : 9.21115
[1mStep[0m  [160/169], [94mLoss[0m : 9.67317

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.343, [92mTest[0m: 9.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.67290
[1mStep[0m  [16/169], [94mLoss[0m : 9.38792
[1mStep[0m  [32/169], [94mLoss[0m : 8.93021
[1mStep[0m  [48/169], [94mLoss[0m : 9.91191
[1mStep[0m  [64/169], [94mLoss[0m : 9.45135
[1mStep[0m  [80/169], [94mLoss[0m : 8.37570
[1mStep[0m  [96/169], [94mLoss[0m : 9.12218
[1mStep[0m  [112/169], [94mLoss[0m : 9.99093
[1mStep[0m  [128/169], [94mLoss[0m : 9.40388
[1mStep[0m  [144/169], [94mLoss[0m : 9.58813
[1mStep[0m  [160/169], [94mLoss[0m : 9.23431

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.168, [92mTest[0m: 9.691, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.22020
[1mStep[0m  [16/169], [94mLoss[0m : 9.14238
[1mStep[0m  [32/169], [94mLoss[0m : 9.01623
[1mStep[0m  [48/169], [94mLoss[0m : 9.97755
[1mStep[0m  [64/169], [94mLoss[0m : 8.75135
[1mStep[0m  [80/169], [94mLoss[0m : 9.71065
[1mStep[0m  [96/169], [94mLoss[0m : 9.41503
[1mStep[0m  [112/169], [94mLoss[0m : 10.01993
[1mStep[0m  [128/169], [94mLoss[0m : 8.37169
[1mStep[0m  [144/169], [94mLoss[0m : 9.08838
[1mStep[0m  [160/169], [94mLoss[0m : 9.50585

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.995, [92mTest[0m: 9.546, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.97739
[1mStep[0m  [16/169], [94mLoss[0m : 8.81972
[1mStep[0m  [32/169], [94mLoss[0m : 8.66308
[1mStep[0m  [48/169], [94mLoss[0m : 8.77754
[1mStep[0m  [64/169], [94mLoss[0m : 8.80297
[1mStep[0m  [80/169], [94mLoss[0m : 8.32272
[1mStep[0m  [96/169], [94mLoss[0m : 8.40658
[1mStep[0m  [112/169], [94mLoss[0m : 8.69054
[1mStep[0m  [128/169], [94mLoss[0m : 9.23737
[1mStep[0m  [144/169], [94mLoss[0m : 8.16750
[1mStep[0m  [160/169], [94mLoss[0m : 9.08802

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.824, [92mTest[0m: 9.430, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.77183
[1mStep[0m  [16/169], [94mLoss[0m : 8.32097
[1mStep[0m  [32/169], [94mLoss[0m : 9.19055
[1mStep[0m  [48/169], [94mLoss[0m : 7.83100
[1mStep[0m  [64/169], [94mLoss[0m : 9.05021
[1mStep[0m  [80/169], [94mLoss[0m : 8.57247
[1mStep[0m  [96/169], [94mLoss[0m : 8.35348
[1mStep[0m  [112/169], [94mLoss[0m : 8.39407
[1mStep[0m  [128/169], [94mLoss[0m : 8.07513
[1mStep[0m  [144/169], [94mLoss[0m : 8.34551
[1mStep[0m  [160/169], [94mLoss[0m : 7.62653

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.648, [92mTest[0m: 9.264, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.62552
[1mStep[0m  [16/169], [94mLoss[0m : 8.35338
[1mStep[0m  [32/169], [94mLoss[0m : 7.95644
[1mStep[0m  [48/169], [94mLoss[0m : 8.34059
[1mStep[0m  [64/169], [94mLoss[0m : 8.65272
[1mStep[0m  [80/169], [94mLoss[0m : 8.44180
[1mStep[0m  [96/169], [94mLoss[0m : 7.90273
[1mStep[0m  [112/169], [94mLoss[0m : 8.14209
[1mStep[0m  [128/169], [94mLoss[0m : 8.64099
[1mStep[0m  [144/169], [94mLoss[0m : 8.41849
[1mStep[0m  [160/169], [94mLoss[0m : 8.32522

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.476, [92mTest[0m: 9.169, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.22282
[1mStep[0m  [16/169], [94mLoss[0m : 8.67836
[1mStep[0m  [32/169], [94mLoss[0m : 8.33557
[1mStep[0m  [48/169], [94mLoss[0m : 9.07441
[1mStep[0m  [64/169], [94mLoss[0m : 8.28996
[1mStep[0m  [80/169], [94mLoss[0m : 8.73640
[1mStep[0m  [96/169], [94mLoss[0m : 7.76340
[1mStep[0m  [112/169], [94mLoss[0m : 8.00323
[1mStep[0m  [128/169], [94mLoss[0m : 8.13064
[1mStep[0m  [144/169], [94mLoss[0m : 8.55186
[1mStep[0m  [160/169], [94mLoss[0m : 8.59079

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.301, [92mTest[0m: 9.002, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.06482
[1mStep[0m  [16/169], [94mLoss[0m : 8.51653
[1mStep[0m  [32/169], [94mLoss[0m : 8.64920
[1mStep[0m  [48/169], [94mLoss[0m : 7.99433
[1mStep[0m  [64/169], [94mLoss[0m : 7.69404
[1mStep[0m  [80/169], [94mLoss[0m : 8.54670
[1mStep[0m  [96/169], [94mLoss[0m : 8.86378
[1mStep[0m  [112/169], [94mLoss[0m : 8.28180
[1mStep[0m  [128/169], [94mLoss[0m : 8.09235
[1mStep[0m  [144/169], [94mLoss[0m : 7.67998
[1mStep[0m  [160/169], [94mLoss[0m : 7.87531

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.127, [92mTest[0m: 8.900, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.01793
[1mStep[0m  [16/169], [94mLoss[0m : 8.36434
[1mStep[0m  [32/169], [94mLoss[0m : 7.87216
[1mStep[0m  [48/169], [94mLoss[0m : 8.18493
[1mStep[0m  [64/169], [94mLoss[0m : 7.56632
[1mStep[0m  [80/169], [94mLoss[0m : 8.58605
[1mStep[0m  [96/169], [94mLoss[0m : 8.48697
[1mStep[0m  [112/169], [94mLoss[0m : 8.36630
[1mStep[0m  [128/169], [94mLoss[0m : 8.04914
[1mStep[0m  [144/169], [94mLoss[0m : 7.91405
[1mStep[0m  [160/169], [94mLoss[0m : 7.79339

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.946, [92mTest[0m: 8.778, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.29250
[1mStep[0m  [16/169], [94mLoss[0m : 7.83492
[1mStep[0m  [32/169], [94mLoss[0m : 7.35097
[1mStep[0m  [48/169], [94mLoss[0m : 7.50746
[1mStep[0m  [64/169], [94mLoss[0m : 8.16975
[1mStep[0m  [80/169], [94mLoss[0m : 7.53421
[1mStep[0m  [96/169], [94mLoss[0m : 7.19739
[1mStep[0m  [112/169], [94mLoss[0m : 7.76160
[1mStep[0m  [128/169], [94mLoss[0m : 7.88047
[1mStep[0m  [144/169], [94mLoss[0m : 7.40360
[1mStep[0m  [160/169], [94mLoss[0m : 6.90532

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.769, [92mTest[0m: 8.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.45190
[1mStep[0m  [16/169], [94mLoss[0m : 8.03975
[1mStep[0m  [32/169], [94mLoss[0m : 7.72543
[1mStep[0m  [48/169], [94mLoss[0m : 7.65341
[1mStep[0m  [64/169], [94mLoss[0m : 7.74514
[1mStep[0m  [80/169], [94mLoss[0m : 7.54701
[1mStep[0m  [96/169], [94mLoss[0m : 7.58067
[1mStep[0m  [112/169], [94mLoss[0m : 8.30423
[1mStep[0m  [128/169], [94mLoss[0m : 7.13451
[1mStep[0m  [144/169], [94mLoss[0m : 8.33112
[1mStep[0m  [160/169], [94mLoss[0m : 7.05170

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.597, [92mTest[0m: 8.482, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.99443
[1mStep[0m  [16/169], [94mLoss[0m : 6.81391
[1mStep[0m  [32/169], [94mLoss[0m : 7.38677
[1mStep[0m  [48/169], [94mLoss[0m : 7.47826
[1mStep[0m  [64/169], [94mLoss[0m : 7.10200
[1mStep[0m  [80/169], [94mLoss[0m : 7.21820
[1mStep[0m  [96/169], [94mLoss[0m : 7.53141
[1mStep[0m  [112/169], [94mLoss[0m : 7.27381
[1mStep[0m  [128/169], [94mLoss[0m : 8.28349
[1mStep[0m  [144/169], [94mLoss[0m : 8.54846
[1mStep[0m  [160/169], [94mLoss[0m : 7.69911

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.408, [92mTest[0m: 8.407, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.81765
[1mStep[0m  [16/169], [94mLoss[0m : 6.83877
[1mStep[0m  [32/169], [94mLoss[0m : 7.46172
[1mStep[0m  [48/169], [94mLoss[0m : 7.27538
[1mStep[0m  [64/169], [94mLoss[0m : 6.38201
[1mStep[0m  [80/169], [94mLoss[0m : 7.82952
[1mStep[0m  [96/169], [94mLoss[0m : 7.02825
[1mStep[0m  [112/169], [94mLoss[0m : 7.04320
[1mStep[0m  [128/169], [94mLoss[0m : 6.97814
[1mStep[0m  [144/169], [94mLoss[0m : 6.93469
[1mStep[0m  [160/169], [94mLoss[0m : 7.47579

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.243, [92mTest[0m: 8.261, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.33515
[1mStep[0m  [16/169], [94mLoss[0m : 7.49465
[1mStep[0m  [32/169], [94mLoss[0m : 6.49546
[1mStep[0m  [48/169], [94mLoss[0m : 7.85318
[1mStep[0m  [64/169], [94mLoss[0m : 6.43736
[1mStep[0m  [80/169], [94mLoss[0m : 6.87888
[1mStep[0m  [96/169], [94mLoss[0m : 7.48143
[1mStep[0m  [112/169], [94mLoss[0m : 7.51042
[1mStep[0m  [128/169], [94mLoss[0m : 7.85999
[1mStep[0m  [144/169], [94mLoss[0m : 7.47993
[1mStep[0m  [160/169], [94mLoss[0m : 7.55421

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.088, [92mTest[0m: 8.022, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.90534
[1mStep[0m  [16/169], [94mLoss[0m : 7.41935
[1mStep[0m  [32/169], [94mLoss[0m : 6.47162
[1mStep[0m  [48/169], [94mLoss[0m : 7.27056
[1mStep[0m  [64/169], [94mLoss[0m : 7.52508
[1mStep[0m  [80/169], [94mLoss[0m : 6.92668
[1mStep[0m  [96/169], [94mLoss[0m : 6.86736
[1mStep[0m  [112/169], [94mLoss[0m : 6.34936
[1mStep[0m  [128/169], [94mLoss[0m : 7.00597
[1mStep[0m  [144/169], [94mLoss[0m : 7.00054
[1mStep[0m  [160/169], [94mLoss[0m : 6.92613

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.915, [92mTest[0m: 7.953, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.03218
[1mStep[0m  [16/169], [94mLoss[0m : 6.92407
[1mStep[0m  [32/169], [94mLoss[0m : 6.88359
[1mStep[0m  [48/169], [94mLoss[0m : 6.86137
[1mStep[0m  [64/169], [94mLoss[0m : 6.73966
[1mStep[0m  [80/169], [94mLoss[0m : 6.39753
[1mStep[0m  [96/169], [94mLoss[0m : 6.79014
[1mStep[0m  [112/169], [94mLoss[0m : 7.34659
[1mStep[0m  [128/169], [94mLoss[0m : 6.41703
[1mStep[0m  [144/169], [94mLoss[0m : 7.53150
[1mStep[0m  [160/169], [94mLoss[0m : 6.50570

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.763, [92mTest[0m: 7.818, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.74248
[1mStep[0m  [16/169], [94mLoss[0m : 6.61811
[1mStep[0m  [32/169], [94mLoss[0m : 6.15262
[1mStep[0m  [48/169], [94mLoss[0m : 6.07233
[1mStep[0m  [64/169], [94mLoss[0m : 6.98924
[1mStep[0m  [80/169], [94mLoss[0m : 7.25576
[1mStep[0m  [96/169], [94mLoss[0m : 7.09319
[1mStep[0m  [112/169], [94mLoss[0m : 6.37011
[1mStep[0m  [128/169], [94mLoss[0m : 5.38607
[1mStep[0m  [144/169], [94mLoss[0m : 6.19450
[1mStep[0m  [160/169], [94mLoss[0m : 6.39226

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.597, [92mTest[0m: 7.687, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.22713
[1mStep[0m  [16/169], [94mLoss[0m : 6.16705
[1mStep[0m  [32/169], [94mLoss[0m : 5.96491
[1mStep[0m  [48/169], [94mLoss[0m : 5.78330
[1mStep[0m  [64/169], [94mLoss[0m : 5.67125
[1mStep[0m  [80/169], [94mLoss[0m : 5.85032
[1mStep[0m  [96/169], [94mLoss[0m : 5.96072
[1mStep[0m  [112/169], [94mLoss[0m : 6.48826
[1mStep[0m  [128/169], [94mLoss[0m : 6.55158
[1mStep[0m  [144/169], [94mLoss[0m : 6.71991
[1mStep[0m  [160/169], [94mLoss[0m : 6.22750

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.426, [92mTest[0m: 7.547, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.10697
[1mStep[0m  [16/169], [94mLoss[0m : 6.42579
[1mStep[0m  [32/169], [94mLoss[0m : 6.77685
[1mStep[0m  [48/169], [94mLoss[0m : 6.05070
[1mStep[0m  [64/169], [94mLoss[0m : 6.18839
[1mStep[0m  [80/169], [94mLoss[0m : 6.26742
[1mStep[0m  [96/169], [94mLoss[0m : 6.88807
[1mStep[0m  [112/169], [94mLoss[0m : 5.33981
[1mStep[0m  [128/169], [94mLoss[0m : 5.33234
[1mStep[0m  [144/169], [94mLoss[0m : 5.64584
[1mStep[0m  [160/169], [94mLoss[0m : 5.86477

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.279, [92mTest[0m: 7.391, [96mlr[0m: 9e-05
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.138
====================================

Phase 1 - Evaluation MAE:  7.137886711529323
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 6.02102
[1mStep[0m  [16/169], [94mLoss[0m : 6.93499
[1mStep[0m  [32/169], [94mLoss[0m : 6.55630
[1mStep[0m  [48/169], [94mLoss[0m : 6.01049
[1mStep[0m  [64/169], [94mLoss[0m : 5.08935
[1mStep[0m  [80/169], [94mLoss[0m : 6.11620
[1mStep[0m  [96/169], [94mLoss[0m : 5.82660
[1mStep[0m  [112/169], [94mLoss[0m : 6.09003
[1mStep[0m  [128/169], [94mLoss[0m : 6.53039
[1mStep[0m  [144/169], [94mLoss[0m : 6.87883
[1mStep[0m  [160/169], [94mLoss[0m : 6.02040

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.090, [92mTest[0m: 7.137, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.979
====================================

Phase 2 - Evaluation MAE:  6.979154757090977
MAE score P1       7.137887
MAE score P2       6.979155
loss               6.090403
learning_rate        0.0001
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.1
weight_decay          0.001
Name: 18, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.70820
[1mStep[0m  [8/84], [94mLoss[0m : 10.50866
[1mStep[0m  [16/84], [94mLoss[0m : 10.78387
[1mStep[0m  [24/84], [94mLoss[0m : 10.52014
[1mStep[0m  [32/84], [94mLoss[0m : 10.48570
[1mStep[0m  [40/84], [94mLoss[0m : 10.95356
[1mStep[0m  [48/84], [94mLoss[0m : 10.20410
[1mStep[0m  [56/84], [94mLoss[0m : 10.74207
[1mStep[0m  [64/84], [94mLoss[0m : 9.71952
[1mStep[0m  [72/84], [94mLoss[0m : 10.10329
[1mStep[0m  [80/84], [94mLoss[0m : 9.97430

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.515, [92mTest[0m: 10.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.07128
[1mStep[0m  [8/84], [94mLoss[0m : 10.08916
[1mStep[0m  [16/84], [94mLoss[0m : 10.24361
[1mStep[0m  [24/84], [94mLoss[0m : 9.57569
[1mStep[0m  [32/84], [94mLoss[0m : 9.52627
[1mStep[0m  [40/84], [94mLoss[0m : 10.44976
[1mStep[0m  [48/84], [94mLoss[0m : 9.14841
[1mStep[0m  [56/84], [94mLoss[0m : 9.43438
[1mStep[0m  [64/84], [94mLoss[0m : 10.03288
[1mStep[0m  [72/84], [94mLoss[0m : 9.72003
[1mStep[0m  [80/84], [94mLoss[0m : 9.18589

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.802, [92mTest[0m: 10.179, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.27565
[1mStep[0m  [8/84], [94mLoss[0m : 9.57648
[1mStep[0m  [16/84], [94mLoss[0m : 9.44007
[1mStep[0m  [24/84], [94mLoss[0m : 9.40056
[1mStep[0m  [32/84], [94mLoss[0m : 9.48911
[1mStep[0m  [40/84], [94mLoss[0m : 8.79086
[1mStep[0m  [48/84], [94mLoss[0m : 8.90708
[1mStep[0m  [56/84], [94mLoss[0m : 8.65917
[1mStep[0m  [64/84], [94mLoss[0m : 8.98311
[1mStep[0m  [72/84], [94mLoss[0m : 8.81332
[1mStep[0m  [80/84], [94mLoss[0m : 8.26229

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.097, [92mTest[0m: 9.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.59974
[1mStep[0m  [8/84], [94mLoss[0m : 9.02013
[1mStep[0m  [16/84], [94mLoss[0m : 9.10907
[1mStep[0m  [24/84], [94mLoss[0m : 8.64382
[1mStep[0m  [32/84], [94mLoss[0m : 8.22198
[1mStep[0m  [40/84], [94mLoss[0m : 8.66222
[1mStep[0m  [48/84], [94mLoss[0m : 8.01075
[1mStep[0m  [56/84], [94mLoss[0m : 7.94217
[1mStep[0m  [64/84], [94mLoss[0m : 7.85937
[1mStep[0m  [72/84], [94mLoss[0m : 7.91909
[1mStep[0m  [80/84], [94mLoss[0m : 7.53687

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.393, [92mTest[0m: 8.741, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.78254
[1mStep[0m  [8/84], [94mLoss[0m : 7.75924
[1mStep[0m  [16/84], [94mLoss[0m : 8.34517
[1mStep[0m  [24/84], [94mLoss[0m : 7.82832
[1mStep[0m  [32/84], [94mLoss[0m : 7.33984
[1mStep[0m  [40/84], [94mLoss[0m : 7.68166
[1mStep[0m  [48/84], [94mLoss[0m : 7.60157
[1mStep[0m  [56/84], [94mLoss[0m : 7.02882
[1mStep[0m  [64/84], [94mLoss[0m : 8.02398
[1mStep[0m  [72/84], [94mLoss[0m : 7.33738
[1mStep[0m  [80/84], [94mLoss[0m : 7.01708

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.675, [92mTest[0m: 8.058, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.43314
[1mStep[0m  [8/84], [94mLoss[0m : 7.14786
[1mStep[0m  [16/84], [94mLoss[0m : 7.31989
[1mStep[0m  [24/84], [94mLoss[0m : 7.21905
[1mStep[0m  [32/84], [94mLoss[0m : 6.81280
[1mStep[0m  [40/84], [94mLoss[0m : 6.91571
[1mStep[0m  [48/84], [94mLoss[0m : 6.75113
[1mStep[0m  [56/84], [94mLoss[0m : 6.60743
[1mStep[0m  [64/84], [94mLoss[0m : 6.97110
[1mStep[0m  [72/84], [94mLoss[0m : 6.99317
[1mStep[0m  [80/84], [94mLoss[0m : 6.40422

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.976, [92mTest[0m: 7.334, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.70271
[1mStep[0m  [8/84], [94mLoss[0m : 6.64259
[1mStep[0m  [16/84], [94mLoss[0m : 7.01857
[1mStep[0m  [24/84], [94mLoss[0m : 6.05168
[1mStep[0m  [32/84], [94mLoss[0m : 5.82783
[1mStep[0m  [40/84], [94mLoss[0m : 6.11614
[1mStep[0m  [48/84], [94mLoss[0m : 5.97056
[1mStep[0m  [56/84], [94mLoss[0m : 6.85607
[1mStep[0m  [64/84], [94mLoss[0m : 5.94238
[1mStep[0m  [72/84], [94mLoss[0m : 6.55891
[1mStep[0m  [80/84], [94mLoss[0m : 5.58892

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.270, [92mTest[0m: 6.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.89937
[1mStep[0m  [8/84], [94mLoss[0m : 6.14088
[1mStep[0m  [16/84], [94mLoss[0m : 5.70499
[1mStep[0m  [24/84], [94mLoss[0m : 5.74839
[1mStep[0m  [32/84], [94mLoss[0m : 5.64072
[1mStep[0m  [40/84], [94mLoss[0m : 5.67978
[1mStep[0m  [48/84], [94mLoss[0m : 5.53619
[1mStep[0m  [56/84], [94mLoss[0m : 5.55540
[1mStep[0m  [64/84], [94mLoss[0m : 5.33674
[1mStep[0m  [72/84], [94mLoss[0m : 4.81029
[1mStep[0m  [80/84], [94mLoss[0m : 5.15275

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.574, [92mTest[0m: 5.941, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.24838
[1mStep[0m  [8/84], [94mLoss[0m : 5.07437
[1mStep[0m  [16/84], [94mLoss[0m : 4.82498
[1mStep[0m  [24/84], [94mLoss[0m : 4.92799
[1mStep[0m  [32/84], [94mLoss[0m : 5.73061
[1mStep[0m  [40/84], [94mLoss[0m : 4.82669
[1mStep[0m  [48/84], [94mLoss[0m : 4.76529
[1mStep[0m  [56/84], [94mLoss[0m : 4.67669
[1mStep[0m  [64/84], [94mLoss[0m : 4.40388
[1mStep[0m  [72/84], [94mLoss[0m : 4.41313
[1mStep[0m  [80/84], [94mLoss[0m : 4.74049

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.960, [92mTest[0m: 5.275, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.64559
[1mStep[0m  [8/84], [94mLoss[0m : 4.63107
[1mStep[0m  [16/84], [94mLoss[0m : 4.87214
[1mStep[0m  [24/84], [94mLoss[0m : 4.59451
[1mStep[0m  [32/84], [94mLoss[0m : 4.27578
[1mStep[0m  [40/84], [94mLoss[0m : 4.24760
[1mStep[0m  [48/84], [94mLoss[0m : 4.50167
[1mStep[0m  [56/84], [94mLoss[0m : 4.63418
[1mStep[0m  [64/84], [94mLoss[0m : 4.22814
[1mStep[0m  [72/84], [94mLoss[0m : 4.41986
[1mStep[0m  [80/84], [94mLoss[0m : 4.00454

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.460, [92mTest[0m: 4.716, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.90824
[1mStep[0m  [8/84], [94mLoss[0m : 3.83070
[1mStep[0m  [16/84], [94mLoss[0m : 4.70917
[1mStep[0m  [24/84], [94mLoss[0m : 4.25047
[1mStep[0m  [32/84], [94mLoss[0m : 4.25312
[1mStep[0m  [40/84], [94mLoss[0m : 3.79624
[1mStep[0m  [48/84], [94mLoss[0m : 4.03562
[1mStep[0m  [56/84], [94mLoss[0m : 3.72939
[1mStep[0m  [64/84], [94mLoss[0m : 3.95064
[1mStep[0m  [72/84], [94mLoss[0m : 3.77679
[1mStep[0m  [80/84], [94mLoss[0m : 3.88095

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.059, [92mTest[0m: 4.256, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.34116
[1mStep[0m  [8/84], [94mLoss[0m : 3.83056
[1mStep[0m  [16/84], [94mLoss[0m : 3.77720
[1mStep[0m  [24/84], [94mLoss[0m : 4.17462
[1mStep[0m  [32/84], [94mLoss[0m : 3.49792
[1mStep[0m  [40/84], [94mLoss[0m : 3.87978
[1mStep[0m  [48/84], [94mLoss[0m : 3.43787
[1mStep[0m  [56/84], [94mLoss[0m : 3.36816
[1mStep[0m  [64/84], [94mLoss[0m : 3.77126
[1mStep[0m  [72/84], [94mLoss[0m : 3.89439
[1mStep[0m  [80/84], [94mLoss[0m : 3.51459

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.735, [92mTest[0m: 3.902, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.76616
[1mStep[0m  [8/84], [94mLoss[0m : 3.73949
[1mStep[0m  [16/84], [94mLoss[0m : 3.86660
[1mStep[0m  [24/84], [94mLoss[0m : 3.37939
[1mStep[0m  [32/84], [94mLoss[0m : 3.64722
[1mStep[0m  [40/84], [94mLoss[0m : 2.98792
[1mStep[0m  [48/84], [94mLoss[0m : 3.64451
[1mStep[0m  [56/84], [94mLoss[0m : 3.62330
[1mStep[0m  [64/84], [94mLoss[0m : 3.50614
[1mStep[0m  [72/84], [94mLoss[0m : 3.62125
[1mStep[0m  [80/84], [94mLoss[0m : 3.17926

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.513, [92mTest[0m: 3.624, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.80112
[1mStep[0m  [8/84], [94mLoss[0m : 2.96421
[1mStep[0m  [16/84], [94mLoss[0m : 3.59377
[1mStep[0m  [24/84], [94mLoss[0m : 3.25645
[1mStep[0m  [32/84], [94mLoss[0m : 3.33555
[1mStep[0m  [40/84], [94mLoss[0m : 3.22499
[1mStep[0m  [48/84], [94mLoss[0m : 3.06342
[1mStep[0m  [56/84], [94mLoss[0m : 3.19743
[1mStep[0m  [64/84], [94mLoss[0m : 3.10386
[1mStep[0m  [72/84], [94mLoss[0m : 3.48449
[1mStep[0m  [80/84], [94mLoss[0m : 3.34882

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.347, [92mTest[0m: 3.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.27561
[1mStep[0m  [8/84], [94mLoss[0m : 3.71614
[1mStep[0m  [16/84], [94mLoss[0m : 3.12455
[1mStep[0m  [24/84], [94mLoss[0m : 3.13132
[1mStep[0m  [32/84], [94mLoss[0m : 2.84154
[1mStep[0m  [40/84], [94mLoss[0m : 3.25708
[1mStep[0m  [48/84], [94mLoss[0m : 3.06164
[1mStep[0m  [56/84], [94mLoss[0m : 3.05274
[1mStep[0m  [64/84], [94mLoss[0m : 3.13106
[1mStep[0m  [72/84], [94mLoss[0m : 3.24141
[1mStep[0m  [80/84], [94mLoss[0m : 3.33274

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.220, [92mTest[0m: 3.260, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.60888
[1mStep[0m  [8/84], [94mLoss[0m : 3.15012
[1mStep[0m  [16/84], [94mLoss[0m : 3.20019
[1mStep[0m  [24/84], [94mLoss[0m : 3.00321
[1mStep[0m  [32/84], [94mLoss[0m : 3.26524
[1mStep[0m  [40/84], [94mLoss[0m : 2.69098
[1mStep[0m  [48/84], [94mLoss[0m : 2.95862
[1mStep[0m  [56/84], [94mLoss[0m : 3.22958
[1mStep[0m  [64/84], [94mLoss[0m : 3.21624
[1mStep[0m  [72/84], [94mLoss[0m : 3.32393
[1mStep[0m  [80/84], [94mLoss[0m : 3.03847

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.128, [92mTest[0m: 3.143, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.19301
[1mStep[0m  [8/84], [94mLoss[0m : 3.04100
[1mStep[0m  [16/84], [94mLoss[0m : 3.52448
[1mStep[0m  [24/84], [94mLoss[0m : 3.00202
[1mStep[0m  [32/84], [94mLoss[0m : 3.28269
[1mStep[0m  [40/84], [94mLoss[0m : 2.95073
[1mStep[0m  [48/84], [94mLoss[0m : 3.31403
[1mStep[0m  [56/84], [94mLoss[0m : 3.23487
[1mStep[0m  [64/84], [94mLoss[0m : 3.35753
[1mStep[0m  [72/84], [94mLoss[0m : 3.11580
[1mStep[0m  [80/84], [94mLoss[0m : 3.25471

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.055, [92mTest[0m: 3.065, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63197
[1mStep[0m  [8/84], [94mLoss[0m : 2.72831
[1mStep[0m  [16/84], [94mLoss[0m : 3.04548
[1mStep[0m  [24/84], [94mLoss[0m : 3.09652
[1mStep[0m  [32/84], [94mLoss[0m : 3.05335
[1mStep[0m  [40/84], [94mLoss[0m : 2.84811
[1mStep[0m  [48/84], [94mLoss[0m : 3.08968
[1mStep[0m  [56/84], [94mLoss[0m : 3.16472
[1mStep[0m  [64/84], [94mLoss[0m : 2.51879
[1mStep[0m  [72/84], [94mLoss[0m : 3.02522
[1mStep[0m  [80/84], [94mLoss[0m : 3.22960

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.990, [92mTest[0m: 2.995, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61607
[1mStep[0m  [8/84], [94mLoss[0m : 2.50688
[1mStep[0m  [16/84], [94mLoss[0m : 3.23605
[1mStep[0m  [24/84], [94mLoss[0m : 3.17868
[1mStep[0m  [32/84], [94mLoss[0m : 3.10952
[1mStep[0m  [40/84], [94mLoss[0m : 2.87890
[1mStep[0m  [48/84], [94mLoss[0m : 2.68610
[1mStep[0m  [56/84], [94mLoss[0m : 2.71028
[1mStep[0m  [64/84], [94mLoss[0m : 3.17243
[1mStep[0m  [72/84], [94mLoss[0m : 3.07668
[1mStep[0m  [80/84], [94mLoss[0m : 2.92279

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.950, [92mTest[0m: 2.934, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75602
[1mStep[0m  [8/84], [94mLoss[0m : 2.84591
[1mStep[0m  [16/84], [94mLoss[0m : 3.35014
[1mStep[0m  [24/84], [94mLoss[0m : 3.61625
[1mStep[0m  [32/84], [94mLoss[0m : 3.08095
[1mStep[0m  [40/84], [94mLoss[0m : 2.69362
[1mStep[0m  [48/84], [94mLoss[0m : 3.04451
[1mStep[0m  [56/84], [94mLoss[0m : 2.89703
[1mStep[0m  [64/84], [94mLoss[0m : 3.08833
[1mStep[0m  [72/84], [94mLoss[0m : 2.95188
[1mStep[0m  [80/84], [94mLoss[0m : 2.82720

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.910, [92mTest[0m: 2.896, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.09612
[1mStep[0m  [8/84], [94mLoss[0m : 2.73707
[1mStep[0m  [16/84], [94mLoss[0m : 2.94531
[1mStep[0m  [24/84], [94mLoss[0m : 2.78366
[1mStep[0m  [32/84], [94mLoss[0m : 2.49782
[1mStep[0m  [40/84], [94mLoss[0m : 2.86356
[1mStep[0m  [48/84], [94mLoss[0m : 2.62154
[1mStep[0m  [56/84], [94mLoss[0m : 2.98041
[1mStep[0m  [64/84], [94mLoss[0m : 3.11005
[1mStep[0m  [72/84], [94mLoss[0m : 3.03569
[1mStep[0m  [80/84], [94mLoss[0m : 2.71335

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.885, [92mTest[0m: 2.860, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.85921
[1mStep[0m  [8/84], [94mLoss[0m : 2.66102
[1mStep[0m  [16/84], [94mLoss[0m : 3.34125
[1mStep[0m  [24/84], [94mLoss[0m : 2.86956
[1mStep[0m  [32/84], [94mLoss[0m : 2.95779
[1mStep[0m  [40/84], [94mLoss[0m : 2.75531
[1mStep[0m  [48/84], [94mLoss[0m : 2.74739
[1mStep[0m  [56/84], [94mLoss[0m : 2.73537
[1mStep[0m  [64/84], [94mLoss[0m : 2.79131
[1mStep[0m  [72/84], [94mLoss[0m : 2.78988
[1mStep[0m  [80/84], [94mLoss[0m : 2.55160

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.856, [92mTest[0m: 2.826, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95606
[1mStep[0m  [8/84], [94mLoss[0m : 2.98136
[1mStep[0m  [16/84], [94mLoss[0m : 2.67739
[1mStep[0m  [24/84], [94mLoss[0m : 2.66457
[1mStep[0m  [32/84], [94mLoss[0m : 2.85969
[1mStep[0m  [40/84], [94mLoss[0m : 2.56967
[1mStep[0m  [48/84], [94mLoss[0m : 2.65765
[1mStep[0m  [56/84], [94mLoss[0m : 2.54444
[1mStep[0m  [64/84], [94mLoss[0m : 2.85089
[1mStep[0m  [72/84], [94mLoss[0m : 3.04111
[1mStep[0m  [80/84], [94mLoss[0m : 2.90202

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.839, [92mTest[0m: 2.803, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81479
[1mStep[0m  [8/84], [94mLoss[0m : 2.61067
[1mStep[0m  [16/84], [94mLoss[0m : 2.90600
[1mStep[0m  [24/84], [94mLoss[0m : 2.43248
[1mStep[0m  [32/84], [94mLoss[0m : 2.84446
[1mStep[0m  [40/84], [94mLoss[0m : 2.88626
[1mStep[0m  [48/84], [94mLoss[0m : 3.05822
[1mStep[0m  [56/84], [94mLoss[0m : 2.76421
[1mStep[0m  [64/84], [94mLoss[0m : 2.57168
[1mStep[0m  [72/84], [94mLoss[0m : 2.99886
[1mStep[0m  [80/84], [94mLoss[0m : 3.14715

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.812, [92mTest[0m: 2.782, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.05372
[1mStep[0m  [8/84], [94mLoss[0m : 2.81928
[1mStep[0m  [16/84], [94mLoss[0m : 2.83841
[1mStep[0m  [24/84], [94mLoss[0m : 2.73911
[1mStep[0m  [32/84], [94mLoss[0m : 3.03641
[1mStep[0m  [40/84], [94mLoss[0m : 2.99536
[1mStep[0m  [48/84], [94mLoss[0m : 3.04812
[1mStep[0m  [56/84], [94mLoss[0m : 2.55204
[1mStep[0m  [64/84], [94mLoss[0m : 2.21705
[1mStep[0m  [72/84], [94mLoss[0m : 2.46678
[1mStep[0m  [80/84], [94mLoss[0m : 2.41471

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.800, [92mTest[0m: 2.758, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55687
[1mStep[0m  [8/84], [94mLoss[0m : 2.67452
[1mStep[0m  [16/84], [94mLoss[0m : 2.59425
[1mStep[0m  [24/84], [94mLoss[0m : 3.02066
[1mStep[0m  [32/84], [94mLoss[0m : 2.76914
[1mStep[0m  [40/84], [94mLoss[0m : 2.98437
[1mStep[0m  [48/84], [94mLoss[0m : 2.76661
[1mStep[0m  [56/84], [94mLoss[0m : 2.99716
[1mStep[0m  [64/84], [94mLoss[0m : 2.84960
[1mStep[0m  [72/84], [94mLoss[0m : 2.86040
[1mStep[0m  [80/84], [94mLoss[0m : 2.84776

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.773, [92mTest[0m: 2.743, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71764
[1mStep[0m  [8/84], [94mLoss[0m : 2.98380
[1mStep[0m  [16/84], [94mLoss[0m : 2.92644
[1mStep[0m  [24/84], [94mLoss[0m : 2.67242
[1mStep[0m  [32/84], [94mLoss[0m : 2.94358
[1mStep[0m  [40/84], [94mLoss[0m : 2.77750
[1mStep[0m  [48/84], [94mLoss[0m : 2.72307
[1mStep[0m  [56/84], [94mLoss[0m : 2.55912
[1mStep[0m  [64/84], [94mLoss[0m : 2.90660
[1mStep[0m  [72/84], [94mLoss[0m : 3.14221
[1mStep[0m  [80/84], [94mLoss[0m : 2.81962

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.760, [92mTest[0m: 2.723, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74865
[1mStep[0m  [8/84], [94mLoss[0m : 2.67167
[1mStep[0m  [16/84], [94mLoss[0m : 2.80141
[1mStep[0m  [24/84], [94mLoss[0m : 2.78680
[1mStep[0m  [32/84], [94mLoss[0m : 3.17584
[1mStep[0m  [40/84], [94mLoss[0m : 2.70662
[1mStep[0m  [48/84], [94mLoss[0m : 2.58128
[1mStep[0m  [56/84], [94mLoss[0m : 2.53542
[1mStep[0m  [64/84], [94mLoss[0m : 2.62016
[1mStep[0m  [72/84], [94mLoss[0m : 2.89702
[1mStep[0m  [80/84], [94mLoss[0m : 2.62159

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.714, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63661
[1mStep[0m  [8/84], [94mLoss[0m : 2.71519
[1mStep[0m  [16/84], [94mLoss[0m : 3.00661
[1mStep[0m  [24/84], [94mLoss[0m : 2.62202
[1mStep[0m  [32/84], [94mLoss[0m : 2.37349
[1mStep[0m  [40/84], [94mLoss[0m : 2.87398
[1mStep[0m  [48/84], [94mLoss[0m : 3.19197
[1mStep[0m  [56/84], [94mLoss[0m : 3.09017
[1mStep[0m  [64/84], [94mLoss[0m : 2.67733
[1mStep[0m  [72/84], [94mLoss[0m : 2.81341
[1mStep[0m  [80/84], [94mLoss[0m : 2.82466

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.693, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61802
[1mStep[0m  [8/84], [94mLoss[0m : 2.37389
[1mStep[0m  [16/84], [94mLoss[0m : 2.37654
[1mStep[0m  [24/84], [94mLoss[0m : 2.88585
[1mStep[0m  [32/84], [94mLoss[0m : 2.88897
[1mStep[0m  [40/84], [94mLoss[0m : 2.78550
[1mStep[0m  [48/84], [94mLoss[0m : 2.73157
[1mStep[0m  [56/84], [94mLoss[0m : 2.62202
[1mStep[0m  [64/84], [94mLoss[0m : 2.66702
[1mStep[0m  [72/84], [94mLoss[0m : 3.08366
[1mStep[0m  [80/84], [94mLoss[0m : 2.42892

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.673, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.670
====================================

Phase 1 - Evaluation MAE:  2.670325142996652
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.76726
[1mStep[0m  [8/84], [94mLoss[0m : 2.43594
[1mStep[0m  [16/84], [94mLoss[0m : 3.00771
[1mStep[0m  [24/84], [94mLoss[0m : 2.87696
[1mStep[0m  [32/84], [94mLoss[0m : 3.01414
[1mStep[0m  [40/84], [94mLoss[0m : 2.66023
[1mStep[0m  [48/84], [94mLoss[0m : 2.80905
[1mStep[0m  [56/84], [94mLoss[0m : 2.55219
[1mStep[0m  [64/84], [94mLoss[0m : 2.47882
[1mStep[0m  [72/84], [94mLoss[0m : 2.82561
[1mStep[0m  [80/84], [94mLoss[0m : 2.74596

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.674, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83846
[1mStep[0m  [8/84], [94mLoss[0m : 2.80840
[1mStep[0m  [16/84], [94mLoss[0m : 2.65145
[1mStep[0m  [24/84], [94mLoss[0m : 2.69965
[1mStep[0m  [32/84], [94mLoss[0m : 2.74525
[1mStep[0m  [40/84], [94mLoss[0m : 3.06876
[1mStep[0m  [48/84], [94mLoss[0m : 2.41200
[1mStep[0m  [56/84], [94mLoss[0m : 2.69720
[1mStep[0m  [64/84], [94mLoss[0m : 2.43888
[1mStep[0m  [72/84], [94mLoss[0m : 2.66802
[1mStep[0m  [80/84], [94mLoss[0m : 2.87603

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.645, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74528
[1mStep[0m  [8/84], [94mLoss[0m : 2.64685
[1mStep[0m  [16/84], [94mLoss[0m : 2.52096
[1mStep[0m  [24/84], [94mLoss[0m : 3.10277
[1mStep[0m  [32/84], [94mLoss[0m : 2.67214
[1mStep[0m  [40/84], [94mLoss[0m : 2.76638
[1mStep[0m  [48/84], [94mLoss[0m : 2.66894
[1mStep[0m  [56/84], [94mLoss[0m : 2.51722
[1mStep[0m  [64/84], [94mLoss[0m : 2.50339
[1mStep[0m  [72/84], [94mLoss[0m : 2.98051
[1mStep[0m  [80/84], [94mLoss[0m : 2.97341

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43316
[1mStep[0m  [8/84], [94mLoss[0m : 2.77754
[1mStep[0m  [16/84], [94mLoss[0m : 2.85840
[1mStep[0m  [24/84], [94mLoss[0m : 2.70538
[1mStep[0m  [32/84], [94mLoss[0m : 2.87745
[1mStep[0m  [40/84], [94mLoss[0m : 2.90655
[1mStep[0m  [48/84], [94mLoss[0m : 3.06959
[1mStep[0m  [56/84], [94mLoss[0m : 2.74068
[1mStep[0m  [64/84], [94mLoss[0m : 2.38872
[1mStep[0m  [72/84], [94mLoss[0m : 2.69704
[1mStep[0m  [80/84], [94mLoss[0m : 2.74248

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.656, [92mTest[0m: 2.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67093
[1mStep[0m  [8/84], [94mLoss[0m : 2.62569
[1mStep[0m  [16/84], [94mLoss[0m : 2.42071
[1mStep[0m  [24/84], [94mLoss[0m : 2.69975
[1mStep[0m  [32/84], [94mLoss[0m : 2.81717
[1mStep[0m  [40/84], [94mLoss[0m : 2.93451
[1mStep[0m  [48/84], [94mLoss[0m : 2.85818
[1mStep[0m  [56/84], [94mLoss[0m : 2.74970
[1mStep[0m  [64/84], [94mLoss[0m : 2.71115
[1mStep[0m  [72/84], [94mLoss[0m : 2.51021
[1mStep[0m  [80/84], [94mLoss[0m : 2.59985

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39989
[1mStep[0m  [8/84], [94mLoss[0m : 2.33647
[1mStep[0m  [16/84], [94mLoss[0m : 2.65144
[1mStep[0m  [24/84], [94mLoss[0m : 2.26155
[1mStep[0m  [32/84], [94mLoss[0m : 2.63900
[1mStep[0m  [40/84], [94mLoss[0m : 2.47018
[1mStep[0m  [48/84], [94mLoss[0m : 2.72999
[1mStep[0m  [56/84], [94mLoss[0m : 2.52910
[1mStep[0m  [64/84], [94mLoss[0m : 2.69895
[1mStep[0m  [72/84], [94mLoss[0m : 2.57780
[1mStep[0m  [80/84], [94mLoss[0m : 2.65507

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78295
[1mStep[0m  [8/84], [94mLoss[0m : 2.42474
[1mStep[0m  [16/84], [94mLoss[0m : 2.72841
[1mStep[0m  [24/84], [94mLoss[0m : 2.68440
[1mStep[0m  [32/84], [94mLoss[0m : 2.75510
[1mStep[0m  [40/84], [94mLoss[0m : 2.64857
[1mStep[0m  [48/84], [94mLoss[0m : 2.58495
[1mStep[0m  [56/84], [94mLoss[0m : 2.72357
[1mStep[0m  [64/84], [94mLoss[0m : 2.93010
[1mStep[0m  [72/84], [94mLoss[0m : 2.59077
[1mStep[0m  [80/84], [94mLoss[0m : 2.82693

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72533
[1mStep[0m  [8/84], [94mLoss[0m : 2.59417
[1mStep[0m  [16/84], [94mLoss[0m : 2.60383
[1mStep[0m  [24/84], [94mLoss[0m : 2.45587
[1mStep[0m  [32/84], [94mLoss[0m : 2.80863
[1mStep[0m  [40/84], [94mLoss[0m : 2.75746
[1mStep[0m  [48/84], [94mLoss[0m : 2.79668
[1mStep[0m  [56/84], [94mLoss[0m : 2.95637
[1mStep[0m  [64/84], [94mLoss[0m : 2.76341
[1mStep[0m  [72/84], [94mLoss[0m : 2.72206
[1mStep[0m  [80/84], [94mLoss[0m : 2.66548

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.548, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95814
[1mStep[0m  [8/84], [94mLoss[0m : 2.35110
[1mStep[0m  [16/84], [94mLoss[0m : 2.77642
[1mStep[0m  [24/84], [94mLoss[0m : 2.66900
[1mStep[0m  [32/84], [94mLoss[0m : 2.35241
[1mStep[0m  [40/84], [94mLoss[0m : 2.41563
[1mStep[0m  [48/84], [94mLoss[0m : 2.56630
[1mStep[0m  [56/84], [94mLoss[0m : 2.50150
[1mStep[0m  [64/84], [94mLoss[0m : 2.15930
[1mStep[0m  [72/84], [94mLoss[0m : 2.34224
[1mStep[0m  [80/84], [94mLoss[0m : 2.71244

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.533, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29301
[1mStep[0m  [8/84], [94mLoss[0m : 3.03180
[1mStep[0m  [16/84], [94mLoss[0m : 2.44387
[1mStep[0m  [24/84], [94mLoss[0m : 2.46495
[1mStep[0m  [32/84], [94mLoss[0m : 2.50212
[1mStep[0m  [40/84], [94mLoss[0m : 2.54593
[1mStep[0m  [48/84], [94mLoss[0m : 2.76431
[1mStep[0m  [56/84], [94mLoss[0m : 2.56318
[1mStep[0m  [64/84], [94mLoss[0m : 2.41234
[1mStep[0m  [72/84], [94mLoss[0m : 2.58303
[1mStep[0m  [80/84], [94mLoss[0m : 2.81691

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.525, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86807
[1mStep[0m  [8/84], [94mLoss[0m : 2.70420
[1mStep[0m  [16/84], [94mLoss[0m : 2.77277
[1mStep[0m  [24/84], [94mLoss[0m : 2.72404
[1mStep[0m  [32/84], [94mLoss[0m : 2.65576
[1mStep[0m  [40/84], [94mLoss[0m : 2.52786
[1mStep[0m  [48/84], [94mLoss[0m : 2.38432
[1mStep[0m  [56/84], [94mLoss[0m : 2.40889
[1mStep[0m  [64/84], [94mLoss[0m : 2.64351
[1mStep[0m  [72/84], [94mLoss[0m : 2.56888
[1mStep[0m  [80/84], [94mLoss[0m : 2.28281

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.514, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84351
[1mStep[0m  [8/84], [94mLoss[0m : 2.23822
[1mStep[0m  [16/84], [94mLoss[0m : 2.87845
[1mStep[0m  [24/84], [94mLoss[0m : 2.66314
[1mStep[0m  [32/84], [94mLoss[0m : 2.64633
[1mStep[0m  [40/84], [94mLoss[0m : 2.59724
[1mStep[0m  [48/84], [94mLoss[0m : 2.32792
[1mStep[0m  [56/84], [94mLoss[0m : 2.74495
[1mStep[0m  [64/84], [94mLoss[0m : 3.10032
[1mStep[0m  [72/84], [94mLoss[0m : 2.71049
[1mStep[0m  [80/84], [94mLoss[0m : 2.46626

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39356
[1mStep[0m  [8/84], [94mLoss[0m : 2.42585
[1mStep[0m  [16/84], [94mLoss[0m : 2.77171
[1mStep[0m  [24/84], [94mLoss[0m : 2.84797
[1mStep[0m  [32/84], [94mLoss[0m : 2.66264
[1mStep[0m  [40/84], [94mLoss[0m : 2.81347
[1mStep[0m  [48/84], [94mLoss[0m : 2.33653
[1mStep[0m  [56/84], [94mLoss[0m : 2.67238
[1mStep[0m  [64/84], [94mLoss[0m : 2.36233
[1mStep[0m  [72/84], [94mLoss[0m : 2.56753
[1mStep[0m  [80/84], [94mLoss[0m : 2.57494

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.505, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70298
[1mStep[0m  [8/84], [94mLoss[0m : 2.56990
[1mStep[0m  [16/84], [94mLoss[0m : 2.45096
[1mStep[0m  [24/84], [94mLoss[0m : 2.39477
[1mStep[0m  [32/84], [94mLoss[0m : 2.61286
[1mStep[0m  [40/84], [94mLoss[0m : 2.44626
[1mStep[0m  [48/84], [94mLoss[0m : 2.54723
[1mStep[0m  [56/84], [94mLoss[0m : 2.41939
[1mStep[0m  [64/84], [94mLoss[0m : 2.63996
[1mStep[0m  [72/84], [94mLoss[0m : 2.71536
[1mStep[0m  [80/84], [94mLoss[0m : 2.61699

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55558
[1mStep[0m  [8/84], [94mLoss[0m : 2.39654
[1mStep[0m  [16/84], [94mLoss[0m : 2.41961
[1mStep[0m  [24/84], [94mLoss[0m : 2.56020
[1mStep[0m  [32/84], [94mLoss[0m : 2.59277
[1mStep[0m  [40/84], [94mLoss[0m : 2.73087
[1mStep[0m  [48/84], [94mLoss[0m : 2.55682
[1mStep[0m  [56/84], [94mLoss[0m : 2.43218
[1mStep[0m  [64/84], [94mLoss[0m : 2.18031
[1mStep[0m  [72/84], [94mLoss[0m : 2.31095
[1mStep[0m  [80/84], [94mLoss[0m : 2.66916

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.492, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78595
[1mStep[0m  [8/84], [94mLoss[0m : 2.75809
[1mStep[0m  [16/84], [94mLoss[0m : 2.78424
[1mStep[0m  [24/84], [94mLoss[0m : 2.83371
[1mStep[0m  [32/84], [94mLoss[0m : 2.51401
[1mStep[0m  [40/84], [94mLoss[0m : 2.55843
[1mStep[0m  [48/84], [94mLoss[0m : 2.85453
[1mStep[0m  [56/84], [94mLoss[0m : 2.48739
[1mStep[0m  [64/84], [94mLoss[0m : 2.28719
[1mStep[0m  [72/84], [94mLoss[0m : 2.44731
[1mStep[0m  [80/84], [94mLoss[0m : 2.56863

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37921
[1mStep[0m  [8/84], [94mLoss[0m : 2.52636
[1mStep[0m  [16/84], [94mLoss[0m : 2.40901
[1mStep[0m  [24/84], [94mLoss[0m : 2.61270
[1mStep[0m  [32/84], [94mLoss[0m : 2.40631
[1mStep[0m  [40/84], [94mLoss[0m : 2.31098
[1mStep[0m  [48/84], [94mLoss[0m : 2.37841
[1mStep[0m  [56/84], [94mLoss[0m : 2.40220
[1mStep[0m  [64/84], [94mLoss[0m : 2.74802
[1mStep[0m  [72/84], [94mLoss[0m : 2.27986
[1mStep[0m  [80/84], [94mLoss[0m : 2.43716

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60796
[1mStep[0m  [8/84], [94mLoss[0m : 2.53440
[1mStep[0m  [16/84], [94mLoss[0m : 2.47713
[1mStep[0m  [24/84], [94mLoss[0m : 2.28357
[1mStep[0m  [32/84], [94mLoss[0m : 2.71336
[1mStep[0m  [40/84], [94mLoss[0m : 2.67230
[1mStep[0m  [48/84], [94mLoss[0m : 2.59854
[1mStep[0m  [56/84], [94mLoss[0m : 2.53638
[1mStep[0m  [64/84], [94mLoss[0m : 2.39684
[1mStep[0m  [72/84], [94mLoss[0m : 2.64935
[1mStep[0m  [80/84], [94mLoss[0m : 2.82491

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81197
[1mStep[0m  [8/84], [94mLoss[0m : 2.34464
[1mStep[0m  [16/84], [94mLoss[0m : 2.57645
[1mStep[0m  [24/84], [94mLoss[0m : 2.65988
[1mStep[0m  [32/84], [94mLoss[0m : 2.85120
[1mStep[0m  [40/84], [94mLoss[0m : 2.82216
[1mStep[0m  [48/84], [94mLoss[0m : 2.43932
[1mStep[0m  [56/84], [94mLoss[0m : 2.63483
[1mStep[0m  [64/84], [94mLoss[0m : 2.47702
[1mStep[0m  [72/84], [94mLoss[0m : 2.18461
[1mStep[0m  [80/84], [94mLoss[0m : 2.44290

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32757
[1mStep[0m  [8/84], [94mLoss[0m : 2.89617
[1mStep[0m  [16/84], [94mLoss[0m : 2.57010
[1mStep[0m  [24/84], [94mLoss[0m : 2.49441
[1mStep[0m  [32/84], [94mLoss[0m : 2.64774
[1mStep[0m  [40/84], [94mLoss[0m : 2.79559
[1mStep[0m  [48/84], [94mLoss[0m : 2.39799
[1mStep[0m  [56/84], [94mLoss[0m : 2.37413
[1mStep[0m  [64/84], [94mLoss[0m : 2.46404
[1mStep[0m  [72/84], [94mLoss[0m : 2.42463
[1mStep[0m  [80/84], [94mLoss[0m : 2.17972

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.474, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41523
[1mStep[0m  [8/84], [94mLoss[0m : 2.42333
[1mStep[0m  [16/84], [94mLoss[0m : 2.38402
[1mStep[0m  [24/84], [94mLoss[0m : 2.84081
[1mStep[0m  [32/84], [94mLoss[0m : 2.17487
[1mStep[0m  [40/84], [94mLoss[0m : 2.49769
[1mStep[0m  [48/84], [94mLoss[0m : 2.58731
[1mStep[0m  [56/84], [94mLoss[0m : 2.44280
[1mStep[0m  [64/84], [94mLoss[0m : 2.44973
[1mStep[0m  [72/84], [94mLoss[0m : 2.66571
[1mStep[0m  [80/84], [94mLoss[0m : 2.64846

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.464, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43247
[1mStep[0m  [8/84], [94mLoss[0m : 2.66032
[1mStep[0m  [16/84], [94mLoss[0m : 2.56598
[1mStep[0m  [24/84], [94mLoss[0m : 2.62212
[1mStep[0m  [32/84], [94mLoss[0m : 2.24885
[1mStep[0m  [40/84], [94mLoss[0m : 2.64153
[1mStep[0m  [48/84], [94mLoss[0m : 2.51244
[1mStep[0m  [56/84], [94mLoss[0m : 2.72487
[1mStep[0m  [64/84], [94mLoss[0m : 2.52805
[1mStep[0m  [72/84], [94mLoss[0m : 2.47336
[1mStep[0m  [80/84], [94mLoss[0m : 2.40678

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57934
[1mStep[0m  [8/84], [94mLoss[0m : 2.35420
[1mStep[0m  [16/84], [94mLoss[0m : 2.70676
[1mStep[0m  [24/84], [94mLoss[0m : 2.72799
[1mStep[0m  [32/84], [94mLoss[0m : 2.70967
[1mStep[0m  [40/84], [94mLoss[0m : 2.98893
[1mStep[0m  [48/84], [94mLoss[0m : 2.50055
[1mStep[0m  [56/84], [94mLoss[0m : 2.42472
[1mStep[0m  [64/84], [94mLoss[0m : 2.47683
[1mStep[0m  [72/84], [94mLoss[0m : 2.52414
[1mStep[0m  [80/84], [94mLoss[0m : 2.43297

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45792
[1mStep[0m  [8/84], [94mLoss[0m : 2.74297
[1mStep[0m  [16/84], [94mLoss[0m : 2.60900
[1mStep[0m  [24/84], [94mLoss[0m : 2.76390
[1mStep[0m  [32/84], [94mLoss[0m : 2.67654
[1mStep[0m  [40/84], [94mLoss[0m : 2.34796
[1mStep[0m  [48/84], [94mLoss[0m : 2.80092
[1mStep[0m  [56/84], [94mLoss[0m : 2.70611
[1mStep[0m  [64/84], [94mLoss[0m : 3.06474
[1mStep[0m  [72/84], [94mLoss[0m : 2.60911
[1mStep[0m  [80/84], [94mLoss[0m : 2.59704

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.458, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59786
[1mStep[0m  [8/84], [94mLoss[0m : 2.51404
[1mStep[0m  [16/84], [94mLoss[0m : 2.53027
[1mStep[0m  [24/84], [94mLoss[0m : 2.27601
[1mStep[0m  [32/84], [94mLoss[0m : 2.67287
[1mStep[0m  [40/84], [94mLoss[0m : 1.93760
[1mStep[0m  [48/84], [94mLoss[0m : 2.66019
[1mStep[0m  [56/84], [94mLoss[0m : 2.49822
[1mStep[0m  [64/84], [94mLoss[0m : 2.38390
[1mStep[0m  [72/84], [94mLoss[0m : 2.17550
[1mStep[0m  [80/84], [94mLoss[0m : 2.39725

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49251
[1mStep[0m  [8/84], [94mLoss[0m : 2.18513
[1mStep[0m  [16/84], [94mLoss[0m : 2.32956
[1mStep[0m  [24/84], [94mLoss[0m : 2.26691
[1mStep[0m  [32/84], [94mLoss[0m : 2.38382
[1mStep[0m  [40/84], [94mLoss[0m : 2.27579
[1mStep[0m  [48/84], [94mLoss[0m : 2.42128
[1mStep[0m  [56/84], [94mLoss[0m : 2.82024
[1mStep[0m  [64/84], [94mLoss[0m : 2.69791
[1mStep[0m  [72/84], [94mLoss[0m : 2.16112
[1mStep[0m  [80/84], [94mLoss[0m : 2.54603

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26087
[1mStep[0m  [8/84], [94mLoss[0m : 2.46877
[1mStep[0m  [16/84], [94mLoss[0m : 2.35654
[1mStep[0m  [24/84], [94mLoss[0m : 2.27287
[1mStep[0m  [32/84], [94mLoss[0m : 2.56394
[1mStep[0m  [40/84], [94mLoss[0m : 2.32359
[1mStep[0m  [48/84], [94mLoss[0m : 2.41662
[1mStep[0m  [56/84], [94mLoss[0m : 2.35219
[1mStep[0m  [64/84], [94mLoss[0m : 3.05102
[1mStep[0m  [72/84], [94mLoss[0m : 2.41103
[1mStep[0m  [80/84], [94mLoss[0m : 2.65587

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.446, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41363
[1mStep[0m  [8/84], [94mLoss[0m : 2.07810
[1mStep[0m  [16/84], [94mLoss[0m : 2.57048
[1mStep[0m  [24/84], [94mLoss[0m : 2.51031
[1mStep[0m  [32/84], [94mLoss[0m : 2.20414
[1mStep[0m  [40/84], [94mLoss[0m : 2.61973
[1mStep[0m  [48/84], [94mLoss[0m : 2.64344
[1mStep[0m  [56/84], [94mLoss[0m : 2.77452
[1mStep[0m  [64/84], [94mLoss[0m : 2.35825
[1mStep[0m  [72/84], [94mLoss[0m : 2.26689
[1mStep[0m  [80/84], [94mLoss[0m : 2.55179

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.440, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53355
[1mStep[0m  [8/84], [94mLoss[0m : 2.28445
[1mStep[0m  [16/84], [94mLoss[0m : 2.20204
[1mStep[0m  [24/84], [94mLoss[0m : 2.51683
[1mStep[0m  [32/84], [94mLoss[0m : 2.26992
[1mStep[0m  [40/84], [94mLoss[0m : 2.29596
[1mStep[0m  [48/84], [94mLoss[0m : 2.30384
[1mStep[0m  [56/84], [94mLoss[0m : 2.34317
[1mStep[0m  [64/84], [94mLoss[0m : 2.65860
[1mStep[0m  [72/84], [94mLoss[0m : 2.46118
[1mStep[0m  [80/84], [94mLoss[0m : 2.88805

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.431, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54006
[1mStep[0m  [8/84], [94mLoss[0m : 2.30475
[1mStep[0m  [16/84], [94mLoss[0m : 2.46299
[1mStep[0m  [24/84], [94mLoss[0m : 2.44918
[1mStep[0m  [32/84], [94mLoss[0m : 2.28510
[1mStep[0m  [40/84], [94mLoss[0m : 2.22667
[1mStep[0m  [48/84], [94mLoss[0m : 2.32134
[1mStep[0m  [56/84], [94mLoss[0m : 2.67105
[1mStep[0m  [64/84], [94mLoss[0m : 2.72268
[1mStep[0m  [72/84], [94mLoss[0m : 2.46618
[1mStep[0m  [80/84], [94mLoss[0m : 2.41998

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.431, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.434
====================================

Phase 2 - Evaluation MAE:  2.4336648668561662
MAE score P1      2.670325
MAE score P2      2.433665
loss              2.493094
learning_rate       0.0001
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 19, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 9.77827
[1mStep[0m  [8/84], [94mLoss[0m : 10.47696
[1mStep[0m  [16/84], [94mLoss[0m : 10.16384
[1mStep[0m  [24/84], [94mLoss[0m : 10.07308
[1mStep[0m  [32/84], [94mLoss[0m : 10.07783
[1mStep[0m  [40/84], [94mLoss[0m : 9.27272
[1mStep[0m  [48/84], [94mLoss[0m : 9.73596
[1mStep[0m  [56/84], [94mLoss[0m : 9.35357
[1mStep[0m  [64/84], [94mLoss[0m : 9.78630
[1mStep[0m  [72/84], [94mLoss[0m : 10.37031
[1mStep[0m  [80/84], [94mLoss[0m : 9.77322

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.947, [92mTest[0m: 10.243, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.17353
[1mStep[0m  [8/84], [94mLoss[0m : 10.00959
[1mStep[0m  [16/84], [94mLoss[0m : 9.53746
[1mStep[0m  [24/84], [94mLoss[0m : 9.62982
[1mStep[0m  [32/84], [94mLoss[0m : 9.51264
[1mStep[0m  [40/84], [94mLoss[0m : 9.32442
[1mStep[0m  [48/84], [94mLoss[0m : 9.46347
[1mStep[0m  [56/84], [94mLoss[0m : 9.62049
[1mStep[0m  [64/84], [94mLoss[0m : 8.99298
[1mStep[0m  [72/84], [94mLoss[0m : 9.01098
[1mStep[0m  [80/84], [94mLoss[0m : 8.99637

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.363, [92mTest[0m: 9.651, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.08302
[1mStep[0m  [8/84], [94mLoss[0m : 9.08940
[1mStep[0m  [16/84], [94mLoss[0m : 9.15599
[1mStep[0m  [24/84], [94mLoss[0m : 8.68055
[1mStep[0m  [32/84], [94mLoss[0m : 9.16243
[1mStep[0m  [40/84], [94mLoss[0m : 8.41156
[1mStep[0m  [48/84], [94mLoss[0m : 8.76922
[1mStep[0m  [56/84], [94mLoss[0m : 8.61589
[1mStep[0m  [64/84], [94mLoss[0m : 9.13776
[1mStep[0m  [72/84], [94mLoss[0m : 8.55126
[1mStep[0m  [80/84], [94mLoss[0m : 8.70940

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.767, [92mTest[0m: 9.065, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.32289
[1mStep[0m  [8/84], [94mLoss[0m : 8.74280
[1mStep[0m  [16/84], [94mLoss[0m : 8.19726
[1mStep[0m  [24/84], [94mLoss[0m : 8.03727
[1mStep[0m  [32/84], [94mLoss[0m : 8.23941
[1mStep[0m  [40/84], [94mLoss[0m : 8.01086
[1mStep[0m  [48/84], [94mLoss[0m : 8.24429
[1mStep[0m  [56/84], [94mLoss[0m : 8.73609
[1mStep[0m  [64/84], [94mLoss[0m : 7.81760
[1mStep[0m  [72/84], [94mLoss[0m : 8.29602
[1mStep[0m  [80/84], [94mLoss[0m : 7.83701

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.171, [92mTest[0m: 8.451, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.42601
[1mStep[0m  [8/84], [94mLoss[0m : 7.83094
[1mStep[0m  [16/84], [94mLoss[0m : 7.91275
[1mStep[0m  [24/84], [94mLoss[0m : 7.86185
[1mStep[0m  [32/84], [94mLoss[0m : 7.56477
[1mStep[0m  [40/84], [94mLoss[0m : 7.09939
[1mStep[0m  [48/84], [94mLoss[0m : 7.77412
[1mStep[0m  [56/84], [94mLoss[0m : 7.07690
[1mStep[0m  [64/84], [94mLoss[0m : 6.99379
[1mStep[0m  [72/84], [94mLoss[0m : 7.43855
[1mStep[0m  [80/84], [94mLoss[0m : 7.20111

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.577, [92mTest[0m: 7.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.65683
[1mStep[0m  [8/84], [94mLoss[0m : 7.30890
[1mStep[0m  [16/84], [94mLoss[0m : 7.47064
[1mStep[0m  [24/84], [94mLoss[0m : 7.03687
[1mStep[0m  [32/84], [94mLoss[0m : 7.13856
[1mStep[0m  [40/84], [94mLoss[0m : 6.61135
[1mStep[0m  [48/84], [94mLoss[0m : 6.23417
[1mStep[0m  [56/84], [94mLoss[0m : 6.97471
[1mStep[0m  [64/84], [94mLoss[0m : 6.64383
[1mStep[0m  [72/84], [94mLoss[0m : 7.26168
[1mStep[0m  [80/84], [94mLoss[0m : 6.75830

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.989, [92mTest[0m: 7.271, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.94842
[1mStep[0m  [8/84], [94mLoss[0m : 6.96259
[1mStep[0m  [16/84], [94mLoss[0m : 6.28548
[1mStep[0m  [24/84], [94mLoss[0m : 6.15264
[1mStep[0m  [32/84], [94mLoss[0m : 6.88178
[1mStep[0m  [40/84], [94mLoss[0m : 6.47186
[1mStep[0m  [48/84], [94mLoss[0m : 6.27799
[1mStep[0m  [56/84], [94mLoss[0m : 6.32413
[1mStep[0m  [64/84], [94mLoss[0m : 6.72117
[1mStep[0m  [72/84], [94mLoss[0m : 6.02022
[1mStep[0m  [80/84], [94mLoss[0m : 6.55568

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.389, [92mTest[0m: 6.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.10092
[1mStep[0m  [8/84], [94mLoss[0m : 6.41897
[1mStep[0m  [16/84], [94mLoss[0m : 5.67683
[1mStep[0m  [24/84], [94mLoss[0m : 5.75121
[1mStep[0m  [32/84], [94mLoss[0m : 5.93669
[1mStep[0m  [40/84], [94mLoss[0m : 5.89111
[1mStep[0m  [48/84], [94mLoss[0m : 5.89303
[1mStep[0m  [56/84], [94mLoss[0m : 5.24360
[1mStep[0m  [64/84], [94mLoss[0m : 6.29431
[1mStep[0m  [72/84], [94mLoss[0m : 5.85370
[1mStep[0m  [80/84], [94mLoss[0m : 5.39283

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.801, [92mTest[0m: 6.067, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.92942
[1mStep[0m  [8/84], [94mLoss[0m : 5.67153
[1mStep[0m  [16/84], [94mLoss[0m : 5.23233
[1mStep[0m  [24/84], [94mLoss[0m : 5.73541
[1mStep[0m  [32/84], [94mLoss[0m : 4.88888
[1mStep[0m  [40/84], [94mLoss[0m : 5.20307
[1mStep[0m  [48/84], [94mLoss[0m : 5.44518
[1mStep[0m  [56/84], [94mLoss[0m : 5.31503
[1mStep[0m  [64/84], [94mLoss[0m : 5.13622
[1mStep[0m  [72/84], [94mLoss[0m : 4.46245
[1mStep[0m  [80/84], [94mLoss[0m : 5.47155

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.231, [92mTest[0m: 5.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.89934
[1mStep[0m  [8/84], [94mLoss[0m : 4.51827
[1mStep[0m  [16/84], [94mLoss[0m : 5.42938
[1mStep[0m  [24/84], [94mLoss[0m : 5.64170
[1mStep[0m  [32/84], [94mLoss[0m : 4.89329
[1mStep[0m  [40/84], [94mLoss[0m : 4.73323
[1mStep[0m  [48/84], [94mLoss[0m : 4.63183
[1mStep[0m  [56/84], [94mLoss[0m : 4.76280
[1mStep[0m  [64/84], [94mLoss[0m : 4.56546
[1mStep[0m  [72/84], [94mLoss[0m : 4.21366
[1mStep[0m  [80/84], [94mLoss[0m : 4.58115

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.742, [92mTest[0m: 4.974, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.87690
[1mStep[0m  [8/84], [94mLoss[0m : 4.24234
[1mStep[0m  [16/84], [94mLoss[0m : 4.22997
[1mStep[0m  [24/84], [94mLoss[0m : 4.59011
[1mStep[0m  [32/84], [94mLoss[0m : 3.83190
[1mStep[0m  [40/84], [94mLoss[0m : 4.41574
[1mStep[0m  [48/84], [94mLoss[0m : 4.39940
[1mStep[0m  [56/84], [94mLoss[0m : 4.12007
[1mStep[0m  [64/84], [94mLoss[0m : 3.73905
[1mStep[0m  [72/84], [94mLoss[0m : 4.55486
[1mStep[0m  [80/84], [94mLoss[0m : 3.88645

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.313, [92mTest[0m: 4.514, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.33170
[1mStep[0m  [8/84], [94mLoss[0m : 4.02741
[1mStep[0m  [16/84], [94mLoss[0m : 3.84710
[1mStep[0m  [24/84], [94mLoss[0m : 4.43079
[1mStep[0m  [32/84], [94mLoss[0m : 4.11293
[1mStep[0m  [40/84], [94mLoss[0m : 4.17090
[1mStep[0m  [48/84], [94mLoss[0m : 3.60333
[1mStep[0m  [56/84], [94mLoss[0m : 3.60534
[1mStep[0m  [64/84], [94mLoss[0m : 4.23698
[1mStep[0m  [72/84], [94mLoss[0m : 3.92058
[1mStep[0m  [80/84], [94mLoss[0m : 3.55546

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.967, [92mTest[0m: 4.138, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.86668
[1mStep[0m  [8/84], [94mLoss[0m : 3.81788
[1mStep[0m  [16/84], [94mLoss[0m : 3.10660
[1mStep[0m  [24/84], [94mLoss[0m : 3.44189
[1mStep[0m  [32/84], [94mLoss[0m : 3.67449
[1mStep[0m  [40/84], [94mLoss[0m : 3.37750
[1mStep[0m  [48/84], [94mLoss[0m : 3.80763
[1mStep[0m  [56/84], [94mLoss[0m : 3.86565
[1mStep[0m  [64/84], [94mLoss[0m : 3.97299
[1mStep[0m  [72/84], [94mLoss[0m : 3.55706
[1mStep[0m  [80/84], [94mLoss[0m : 3.65740

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.706, [92mTest[0m: 3.809, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.55892
[1mStep[0m  [8/84], [94mLoss[0m : 3.52907
[1mStep[0m  [16/84], [94mLoss[0m : 3.65716
[1mStep[0m  [24/84], [94mLoss[0m : 3.62829
[1mStep[0m  [32/84], [94mLoss[0m : 3.53224
[1mStep[0m  [40/84], [94mLoss[0m : 3.59768
[1mStep[0m  [48/84], [94mLoss[0m : 3.46211
[1mStep[0m  [56/84], [94mLoss[0m : 3.58049
[1mStep[0m  [64/84], [94mLoss[0m : 3.46297
[1mStep[0m  [72/84], [94mLoss[0m : 3.39565
[1mStep[0m  [80/84], [94mLoss[0m : 3.63914

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.493, [92mTest[0m: 3.569, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.44556
[1mStep[0m  [8/84], [94mLoss[0m : 3.53680
[1mStep[0m  [16/84], [94mLoss[0m : 3.25970
[1mStep[0m  [24/84], [94mLoss[0m : 3.15005
[1mStep[0m  [32/84], [94mLoss[0m : 3.21609
[1mStep[0m  [40/84], [94mLoss[0m : 3.19826
[1mStep[0m  [48/84], [94mLoss[0m : 3.20352
[1mStep[0m  [56/84], [94mLoss[0m : 3.14642
[1mStep[0m  [64/84], [94mLoss[0m : 4.10004
[1mStep[0m  [72/84], [94mLoss[0m : 3.26473
[1mStep[0m  [80/84], [94mLoss[0m : 3.22448

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.323, [92mTest[0m: 3.377, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.54089
[1mStep[0m  [8/84], [94mLoss[0m : 3.40419
[1mStep[0m  [16/84], [94mLoss[0m : 3.20348
[1mStep[0m  [24/84], [94mLoss[0m : 3.24388
[1mStep[0m  [32/84], [94mLoss[0m : 2.94507
[1mStep[0m  [40/84], [94mLoss[0m : 3.23094
[1mStep[0m  [48/84], [94mLoss[0m : 3.37898
[1mStep[0m  [56/84], [94mLoss[0m : 2.61581
[1mStep[0m  [64/84], [94mLoss[0m : 2.98567
[1mStep[0m  [72/84], [94mLoss[0m : 3.01639
[1mStep[0m  [80/84], [94mLoss[0m : 2.50562

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.205, [92mTest[0m: 3.224, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.71317
[1mStep[0m  [8/84], [94mLoss[0m : 2.67960
[1mStep[0m  [16/84], [94mLoss[0m : 3.19618
[1mStep[0m  [24/84], [94mLoss[0m : 3.24155
[1mStep[0m  [32/84], [94mLoss[0m : 3.11808
[1mStep[0m  [40/84], [94mLoss[0m : 2.78353
[1mStep[0m  [48/84], [94mLoss[0m : 2.93547
[1mStep[0m  [56/84], [94mLoss[0m : 3.28264
[1mStep[0m  [64/84], [94mLoss[0m : 3.22902
[1mStep[0m  [72/84], [94mLoss[0m : 2.58535
[1mStep[0m  [80/84], [94mLoss[0m : 2.94701

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.100, [92mTest[0m: 3.112, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.04110
[1mStep[0m  [8/84], [94mLoss[0m : 3.14786
[1mStep[0m  [16/84], [94mLoss[0m : 2.97585
[1mStep[0m  [24/84], [94mLoss[0m : 3.05705
[1mStep[0m  [32/84], [94mLoss[0m : 3.22346
[1mStep[0m  [40/84], [94mLoss[0m : 3.12690
[1mStep[0m  [48/84], [94mLoss[0m : 3.35626
[1mStep[0m  [56/84], [94mLoss[0m : 3.10995
[1mStep[0m  [64/84], [94mLoss[0m : 2.75149
[1mStep[0m  [72/84], [94mLoss[0m : 3.24094
[1mStep[0m  [80/84], [94mLoss[0m : 3.08749

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.026, [92mTest[0m: 3.030, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.94982
[1mStep[0m  [8/84], [94mLoss[0m : 2.94119
[1mStep[0m  [16/84], [94mLoss[0m : 3.13981
[1mStep[0m  [24/84], [94mLoss[0m : 3.31764
[1mStep[0m  [32/84], [94mLoss[0m : 2.99725
[1mStep[0m  [40/84], [94mLoss[0m : 3.07767
[1mStep[0m  [48/84], [94mLoss[0m : 3.00767
[1mStep[0m  [56/84], [94mLoss[0m : 2.93451
[1mStep[0m  [64/84], [94mLoss[0m : 3.05142
[1mStep[0m  [72/84], [94mLoss[0m : 2.93361
[1mStep[0m  [80/84], [94mLoss[0m : 2.50227

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.963, [92mTest[0m: 2.959, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.00580
[1mStep[0m  [8/84], [94mLoss[0m : 2.64854
[1mStep[0m  [16/84], [94mLoss[0m : 3.01246
[1mStep[0m  [24/84], [94mLoss[0m : 3.26929
[1mStep[0m  [32/84], [94mLoss[0m : 2.62349
[1mStep[0m  [40/84], [94mLoss[0m : 2.82217
[1mStep[0m  [48/84], [94mLoss[0m : 2.86655
[1mStep[0m  [56/84], [94mLoss[0m : 2.51121
[1mStep[0m  [64/84], [94mLoss[0m : 2.92499
[1mStep[0m  [72/84], [94mLoss[0m : 2.86300
[1mStep[0m  [80/84], [94mLoss[0m : 2.74337

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.910, [92mTest[0m: 2.902, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80875
[1mStep[0m  [8/84], [94mLoss[0m : 2.93155
[1mStep[0m  [16/84], [94mLoss[0m : 3.09714
[1mStep[0m  [24/84], [94mLoss[0m : 2.98268
[1mStep[0m  [32/84], [94mLoss[0m : 2.82321
[1mStep[0m  [40/84], [94mLoss[0m : 3.01223
[1mStep[0m  [48/84], [94mLoss[0m : 2.71215
[1mStep[0m  [56/84], [94mLoss[0m : 2.54489
[1mStep[0m  [64/84], [94mLoss[0m : 2.87407
[1mStep[0m  [72/84], [94mLoss[0m : 3.10274
[1mStep[0m  [80/84], [94mLoss[0m : 3.07717

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.871, [92mTest[0m: 2.852, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83135
[1mStep[0m  [8/84], [94mLoss[0m : 2.77504
[1mStep[0m  [16/84], [94mLoss[0m : 2.92699
[1mStep[0m  [24/84], [94mLoss[0m : 2.93694
[1mStep[0m  [32/84], [94mLoss[0m : 2.67764
[1mStep[0m  [40/84], [94mLoss[0m : 2.94670
[1mStep[0m  [48/84], [94mLoss[0m : 3.32416
[1mStep[0m  [56/84], [94mLoss[0m : 2.97191
[1mStep[0m  [64/84], [94mLoss[0m : 2.73267
[1mStep[0m  [72/84], [94mLoss[0m : 3.20934
[1mStep[0m  [80/84], [94mLoss[0m : 2.77122

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.839, [92mTest[0m: 2.818, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.02087
[1mStep[0m  [8/84], [94mLoss[0m : 2.68663
[1mStep[0m  [16/84], [94mLoss[0m : 2.79821
[1mStep[0m  [24/84], [94mLoss[0m : 2.79681
[1mStep[0m  [32/84], [94mLoss[0m : 3.08823
[1mStep[0m  [40/84], [94mLoss[0m : 2.66587
[1mStep[0m  [48/84], [94mLoss[0m : 2.59862
[1mStep[0m  [56/84], [94mLoss[0m : 2.97180
[1mStep[0m  [64/84], [94mLoss[0m : 2.72907
[1mStep[0m  [72/84], [94mLoss[0m : 2.92289
[1mStep[0m  [80/84], [94mLoss[0m : 2.83554

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.812, [92mTest[0m: 2.785, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.92448
[1mStep[0m  [8/84], [94mLoss[0m : 3.14046
[1mStep[0m  [16/84], [94mLoss[0m : 2.92311
[1mStep[0m  [24/84], [94mLoss[0m : 2.93051
[1mStep[0m  [32/84], [94mLoss[0m : 2.77707
[1mStep[0m  [40/84], [94mLoss[0m : 2.64072
[1mStep[0m  [48/84], [94mLoss[0m : 2.77494
[1mStep[0m  [56/84], [94mLoss[0m : 2.49267
[1mStep[0m  [64/84], [94mLoss[0m : 2.99049
[1mStep[0m  [72/84], [94mLoss[0m : 2.45210
[1mStep[0m  [80/84], [94mLoss[0m : 2.73682

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.781, [92mTest[0m: 2.755, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39890
[1mStep[0m  [8/84], [94mLoss[0m : 2.44952
[1mStep[0m  [16/84], [94mLoss[0m : 2.81086
[1mStep[0m  [24/84], [94mLoss[0m : 2.50377
[1mStep[0m  [32/84], [94mLoss[0m : 2.71396
[1mStep[0m  [40/84], [94mLoss[0m : 2.97337
[1mStep[0m  [48/84], [94mLoss[0m : 2.61256
[1mStep[0m  [56/84], [94mLoss[0m : 2.73644
[1mStep[0m  [64/84], [94mLoss[0m : 2.89004
[1mStep[0m  [72/84], [94mLoss[0m : 2.69181
[1mStep[0m  [80/84], [94mLoss[0m : 2.88041

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.761, [92mTest[0m: 2.725, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90370
[1mStep[0m  [8/84], [94mLoss[0m : 2.81163
[1mStep[0m  [16/84], [94mLoss[0m : 2.77196
[1mStep[0m  [24/84], [94mLoss[0m : 3.12178
[1mStep[0m  [32/84], [94mLoss[0m : 2.43688
[1mStep[0m  [40/84], [94mLoss[0m : 2.63675
[1mStep[0m  [48/84], [94mLoss[0m : 2.96516
[1mStep[0m  [56/84], [94mLoss[0m : 2.85342
[1mStep[0m  [64/84], [94mLoss[0m : 2.93927
[1mStep[0m  [72/84], [94mLoss[0m : 2.66615
[1mStep[0m  [80/84], [94mLoss[0m : 2.68253

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.713, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74241
[1mStep[0m  [8/84], [94mLoss[0m : 2.66405
[1mStep[0m  [16/84], [94mLoss[0m : 2.98098
[1mStep[0m  [24/84], [94mLoss[0m : 3.05798
[1mStep[0m  [32/84], [94mLoss[0m : 2.43316
[1mStep[0m  [40/84], [94mLoss[0m : 2.58938
[1mStep[0m  [48/84], [94mLoss[0m : 2.50935
[1mStep[0m  [56/84], [94mLoss[0m : 2.72033
[1mStep[0m  [64/84], [94mLoss[0m : 2.72045
[1mStep[0m  [72/84], [94mLoss[0m : 2.62822
[1mStep[0m  [80/84], [94mLoss[0m : 2.85812

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.690, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67721
[1mStep[0m  [8/84], [94mLoss[0m : 2.86896
[1mStep[0m  [16/84], [94mLoss[0m : 2.96015
[1mStep[0m  [24/84], [94mLoss[0m : 3.01627
[1mStep[0m  [32/84], [94mLoss[0m : 2.80683
[1mStep[0m  [40/84], [94mLoss[0m : 2.71970
[1mStep[0m  [48/84], [94mLoss[0m : 3.07312
[1mStep[0m  [56/84], [94mLoss[0m : 2.73232
[1mStep[0m  [64/84], [94mLoss[0m : 2.32870
[1mStep[0m  [72/84], [94mLoss[0m : 2.35484
[1mStep[0m  [80/84], [94mLoss[0m : 2.51941

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.667, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45385
[1mStep[0m  [8/84], [94mLoss[0m : 2.51193
[1mStep[0m  [16/84], [94mLoss[0m : 2.77969
[1mStep[0m  [24/84], [94mLoss[0m : 2.82316
[1mStep[0m  [32/84], [94mLoss[0m : 2.85552
[1mStep[0m  [40/84], [94mLoss[0m : 2.69175
[1mStep[0m  [48/84], [94mLoss[0m : 2.68109
[1mStep[0m  [56/84], [94mLoss[0m : 2.71370
[1mStep[0m  [64/84], [94mLoss[0m : 2.57040
[1mStep[0m  [72/84], [94mLoss[0m : 2.70122
[1mStep[0m  [80/84], [94mLoss[0m : 2.60088

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.653, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.92648
[1mStep[0m  [8/84], [94mLoss[0m : 2.88582
[1mStep[0m  [16/84], [94mLoss[0m : 2.55489
[1mStep[0m  [24/84], [94mLoss[0m : 2.89745
[1mStep[0m  [32/84], [94mLoss[0m : 2.65707
[1mStep[0m  [40/84], [94mLoss[0m : 2.51142
[1mStep[0m  [48/84], [94mLoss[0m : 2.86407
[1mStep[0m  [56/84], [94mLoss[0m : 2.70702
[1mStep[0m  [64/84], [94mLoss[0m : 2.51909
[1mStep[0m  [72/84], [94mLoss[0m : 2.56859
[1mStep[0m  [80/84], [94mLoss[0m : 2.69833

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.641, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.627
====================================

Phase 1 - Evaluation MAE:  2.6270645345960344
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.69686
[1mStep[0m  [8/84], [94mLoss[0m : 2.58911
[1mStep[0m  [16/84], [94mLoss[0m : 2.34160
[1mStep[0m  [24/84], [94mLoss[0m : 2.72882
[1mStep[0m  [32/84], [94mLoss[0m : 2.72062
[1mStep[0m  [40/84], [94mLoss[0m : 2.58230
[1mStep[0m  [48/84], [94mLoss[0m : 2.77662
[1mStep[0m  [56/84], [94mLoss[0m : 2.79713
[1mStep[0m  [64/84], [94mLoss[0m : 2.70978
[1mStep[0m  [72/84], [94mLoss[0m : 2.71761
[1mStep[0m  [80/84], [94mLoss[0m : 2.72121

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95091
[1mStep[0m  [8/84], [94mLoss[0m : 2.82064
[1mStep[0m  [16/84], [94mLoss[0m : 2.71339
[1mStep[0m  [24/84], [94mLoss[0m : 2.21485
[1mStep[0m  [32/84], [94mLoss[0m : 2.60409
[1mStep[0m  [40/84], [94mLoss[0m : 2.54723
[1mStep[0m  [48/84], [94mLoss[0m : 3.00064
[1mStep[0m  [56/84], [94mLoss[0m : 2.83112
[1mStep[0m  [64/84], [94mLoss[0m : 2.62366
[1mStep[0m  [72/84], [94mLoss[0m : 2.49941
[1mStep[0m  [80/84], [94mLoss[0m : 2.51551

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.608, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53708
[1mStep[0m  [8/84], [94mLoss[0m : 2.82809
[1mStep[0m  [16/84], [94mLoss[0m : 2.49693
[1mStep[0m  [24/84], [94mLoss[0m : 2.73564
[1mStep[0m  [32/84], [94mLoss[0m : 2.53698
[1mStep[0m  [40/84], [94mLoss[0m : 2.50041
[1mStep[0m  [48/84], [94mLoss[0m : 3.13991
[1mStep[0m  [56/84], [94mLoss[0m : 2.64206
[1mStep[0m  [64/84], [94mLoss[0m : 2.56313
[1mStep[0m  [72/84], [94mLoss[0m : 2.92784
[1mStep[0m  [80/84], [94mLoss[0m : 2.64165

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.586, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71856
[1mStep[0m  [8/84], [94mLoss[0m : 2.69328
[1mStep[0m  [16/84], [94mLoss[0m : 2.59960
[1mStep[0m  [24/84], [94mLoss[0m : 2.94316
[1mStep[0m  [32/84], [94mLoss[0m : 2.54311
[1mStep[0m  [40/84], [94mLoss[0m : 2.51206
[1mStep[0m  [48/84], [94mLoss[0m : 2.70879
[1mStep[0m  [56/84], [94mLoss[0m : 2.45145
[1mStep[0m  [64/84], [94mLoss[0m : 2.58364
[1mStep[0m  [72/84], [94mLoss[0m : 2.65573
[1mStep[0m  [80/84], [94mLoss[0m : 2.77243

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.566, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.82438
[1mStep[0m  [8/84], [94mLoss[0m : 2.56314
[1mStep[0m  [16/84], [94mLoss[0m : 2.73558
[1mStep[0m  [24/84], [94mLoss[0m : 2.48460
[1mStep[0m  [32/84], [94mLoss[0m : 2.28961
[1mStep[0m  [40/84], [94mLoss[0m : 2.52600
[1mStep[0m  [48/84], [94mLoss[0m : 2.52293
[1mStep[0m  [56/84], [94mLoss[0m : 2.66764
[1mStep[0m  [64/84], [94mLoss[0m : 2.69120
[1mStep[0m  [72/84], [94mLoss[0m : 2.75356
[1mStep[0m  [80/84], [94mLoss[0m : 2.54323

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.555, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56134
[1mStep[0m  [8/84], [94mLoss[0m : 2.58860
[1mStep[0m  [16/84], [94mLoss[0m : 2.80872
[1mStep[0m  [24/84], [94mLoss[0m : 2.59167
[1mStep[0m  [32/84], [94mLoss[0m : 2.50605
[1mStep[0m  [40/84], [94mLoss[0m : 2.69312
[1mStep[0m  [48/84], [94mLoss[0m : 2.66587
[1mStep[0m  [56/84], [94mLoss[0m : 2.49321
[1mStep[0m  [64/84], [94mLoss[0m : 2.45396
[1mStep[0m  [72/84], [94mLoss[0m : 2.65936
[1mStep[0m  [80/84], [94mLoss[0m : 2.54697

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.543, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37640
[1mStep[0m  [8/84], [94mLoss[0m : 2.67197
[1mStep[0m  [16/84], [94mLoss[0m : 2.45604
[1mStep[0m  [24/84], [94mLoss[0m : 2.36712
[1mStep[0m  [32/84], [94mLoss[0m : 2.45982
[1mStep[0m  [40/84], [94mLoss[0m : 2.73209
[1mStep[0m  [48/84], [94mLoss[0m : 2.41222
[1mStep[0m  [56/84], [94mLoss[0m : 2.69772
[1mStep[0m  [64/84], [94mLoss[0m : 2.37214
[1mStep[0m  [72/84], [94mLoss[0m : 2.50670
[1mStep[0m  [80/84], [94mLoss[0m : 2.47699

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.537, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42956
[1mStep[0m  [8/84], [94mLoss[0m : 2.43551
[1mStep[0m  [16/84], [94mLoss[0m : 2.53907
[1mStep[0m  [24/84], [94mLoss[0m : 2.47088
[1mStep[0m  [32/84], [94mLoss[0m : 2.41594
[1mStep[0m  [40/84], [94mLoss[0m : 2.51031
[1mStep[0m  [48/84], [94mLoss[0m : 2.41457
[1mStep[0m  [56/84], [94mLoss[0m : 2.72218
[1mStep[0m  [64/84], [94mLoss[0m : 2.60147
[1mStep[0m  [72/84], [94mLoss[0m : 2.49722
[1mStep[0m  [80/84], [94mLoss[0m : 2.79709

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93350
[1mStep[0m  [8/84], [94mLoss[0m : 2.69015
[1mStep[0m  [16/84], [94mLoss[0m : 2.34261
[1mStep[0m  [24/84], [94mLoss[0m : 2.35475
[1mStep[0m  [32/84], [94mLoss[0m : 2.67690
[1mStep[0m  [40/84], [94mLoss[0m : 2.55557
[1mStep[0m  [48/84], [94mLoss[0m : 2.46741
[1mStep[0m  [56/84], [94mLoss[0m : 2.41327
[1mStep[0m  [64/84], [94mLoss[0m : 2.26835
[1mStep[0m  [72/84], [94mLoss[0m : 2.31987
[1mStep[0m  [80/84], [94mLoss[0m : 2.52075

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.512, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79343
[1mStep[0m  [8/84], [94mLoss[0m : 2.66378
[1mStep[0m  [16/84], [94mLoss[0m : 2.48286
[1mStep[0m  [24/84], [94mLoss[0m : 2.62296
[1mStep[0m  [32/84], [94mLoss[0m : 2.43547
[1mStep[0m  [40/84], [94mLoss[0m : 2.87110
[1mStep[0m  [48/84], [94mLoss[0m : 2.82848
[1mStep[0m  [56/84], [94mLoss[0m : 2.56770
[1mStep[0m  [64/84], [94mLoss[0m : 2.44364
[1mStep[0m  [72/84], [94mLoss[0m : 2.86337
[1mStep[0m  [80/84], [94mLoss[0m : 2.66944

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.502, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36355
[1mStep[0m  [8/84], [94mLoss[0m : 2.42287
[1mStep[0m  [16/84], [94mLoss[0m : 2.71773
[1mStep[0m  [24/84], [94mLoss[0m : 2.47020
[1mStep[0m  [32/84], [94mLoss[0m : 2.34053
[1mStep[0m  [40/84], [94mLoss[0m : 2.52646
[1mStep[0m  [48/84], [94mLoss[0m : 2.06833
[1mStep[0m  [56/84], [94mLoss[0m : 2.73822
[1mStep[0m  [64/84], [94mLoss[0m : 2.41360
[1mStep[0m  [72/84], [94mLoss[0m : 2.69145
[1mStep[0m  [80/84], [94mLoss[0m : 2.36100

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37830
[1mStep[0m  [8/84], [94mLoss[0m : 2.81582
[1mStep[0m  [16/84], [94mLoss[0m : 2.55838
[1mStep[0m  [24/84], [94mLoss[0m : 2.76873
[1mStep[0m  [32/84], [94mLoss[0m : 2.72549
[1mStep[0m  [40/84], [94mLoss[0m : 2.49683
[1mStep[0m  [48/84], [94mLoss[0m : 2.62543
[1mStep[0m  [56/84], [94mLoss[0m : 2.44976
[1mStep[0m  [64/84], [94mLoss[0m : 2.57537
[1mStep[0m  [72/84], [94mLoss[0m : 2.51421
[1mStep[0m  [80/84], [94mLoss[0m : 2.25695

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60991
[1mStep[0m  [8/84], [94mLoss[0m : 2.41223
[1mStep[0m  [16/84], [94mLoss[0m : 2.62709
[1mStep[0m  [24/84], [94mLoss[0m : 2.71287
[1mStep[0m  [32/84], [94mLoss[0m : 2.66452
[1mStep[0m  [40/84], [94mLoss[0m : 2.78170
[1mStep[0m  [48/84], [94mLoss[0m : 2.54731
[1mStep[0m  [56/84], [94mLoss[0m : 2.40027
[1mStep[0m  [64/84], [94mLoss[0m : 2.49422
[1mStep[0m  [72/84], [94mLoss[0m : 2.42192
[1mStep[0m  [80/84], [94mLoss[0m : 2.68256

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40733
[1mStep[0m  [8/84], [94mLoss[0m : 2.61932
[1mStep[0m  [16/84], [94mLoss[0m : 2.57866
[1mStep[0m  [24/84], [94mLoss[0m : 2.50046
[1mStep[0m  [32/84], [94mLoss[0m : 2.34686
[1mStep[0m  [40/84], [94mLoss[0m : 2.45715
[1mStep[0m  [48/84], [94mLoss[0m : 2.34802
[1mStep[0m  [56/84], [94mLoss[0m : 2.54285
[1mStep[0m  [64/84], [94mLoss[0m : 2.39950
[1mStep[0m  [72/84], [94mLoss[0m : 2.62304
[1mStep[0m  [80/84], [94mLoss[0m : 2.49884

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.477, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59482
[1mStep[0m  [8/84], [94mLoss[0m : 2.45398
[1mStep[0m  [16/84], [94mLoss[0m : 2.59280
[1mStep[0m  [24/84], [94mLoss[0m : 2.79891
[1mStep[0m  [32/84], [94mLoss[0m : 2.98115
[1mStep[0m  [40/84], [94mLoss[0m : 2.61416
[1mStep[0m  [48/84], [94mLoss[0m : 2.50254
[1mStep[0m  [56/84], [94mLoss[0m : 2.47527
[1mStep[0m  [64/84], [94mLoss[0m : 2.73459
[1mStep[0m  [72/84], [94mLoss[0m : 2.45821
[1mStep[0m  [80/84], [94mLoss[0m : 2.68235

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44938
[1mStep[0m  [8/84], [94mLoss[0m : 2.60963
[1mStep[0m  [16/84], [94mLoss[0m : 2.47862
[1mStep[0m  [24/84], [94mLoss[0m : 2.81123
[1mStep[0m  [32/84], [94mLoss[0m : 2.42653
[1mStep[0m  [40/84], [94mLoss[0m : 2.80088
[1mStep[0m  [48/84], [94mLoss[0m : 2.71377
[1mStep[0m  [56/84], [94mLoss[0m : 2.74929
[1mStep[0m  [64/84], [94mLoss[0m : 2.46484
[1mStep[0m  [72/84], [94mLoss[0m : 2.74805
[1mStep[0m  [80/84], [94mLoss[0m : 2.71404

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.473, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30999
[1mStep[0m  [8/84], [94mLoss[0m : 2.37916
[1mStep[0m  [16/84], [94mLoss[0m : 2.80475
[1mStep[0m  [24/84], [94mLoss[0m : 2.54067
[1mStep[0m  [32/84], [94mLoss[0m : 2.48388
[1mStep[0m  [40/84], [94mLoss[0m : 2.69660
[1mStep[0m  [48/84], [94mLoss[0m : 2.71159
[1mStep[0m  [56/84], [94mLoss[0m : 2.56610
[1mStep[0m  [64/84], [94mLoss[0m : 2.36209
[1mStep[0m  [72/84], [94mLoss[0m : 2.63528
[1mStep[0m  [80/84], [94mLoss[0m : 2.46133

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.463, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46166
[1mStep[0m  [8/84], [94mLoss[0m : 2.48544
[1mStep[0m  [16/84], [94mLoss[0m : 2.46006
[1mStep[0m  [24/84], [94mLoss[0m : 2.48902
[1mStep[0m  [32/84], [94mLoss[0m : 2.31411
[1mStep[0m  [40/84], [94mLoss[0m : 2.47458
[1mStep[0m  [48/84], [94mLoss[0m : 2.47915
[1mStep[0m  [56/84], [94mLoss[0m : 2.64255
[1mStep[0m  [64/84], [94mLoss[0m : 2.86691
[1mStep[0m  [72/84], [94mLoss[0m : 2.60806
[1mStep[0m  [80/84], [94mLoss[0m : 2.64721

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55216
[1mStep[0m  [8/84], [94mLoss[0m : 2.68125
[1mStep[0m  [16/84], [94mLoss[0m : 2.32991
[1mStep[0m  [24/84], [94mLoss[0m : 2.71643
[1mStep[0m  [32/84], [94mLoss[0m : 2.61617
[1mStep[0m  [40/84], [94mLoss[0m : 2.56302
[1mStep[0m  [48/84], [94mLoss[0m : 2.55154
[1mStep[0m  [56/84], [94mLoss[0m : 2.38249
[1mStep[0m  [64/84], [94mLoss[0m : 2.33771
[1mStep[0m  [72/84], [94mLoss[0m : 2.46657
[1mStep[0m  [80/84], [94mLoss[0m : 2.15258

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21144
[1mStep[0m  [8/84], [94mLoss[0m : 2.60112
[1mStep[0m  [16/84], [94mLoss[0m : 2.53549
[1mStep[0m  [24/84], [94mLoss[0m : 2.40845
[1mStep[0m  [32/84], [94mLoss[0m : 2.63260
[1mStep[0m  [40/84], [94mLoss[0m : 2.73217
[1mStep[0m  [48/84], [94mLoss[0m : 2.24553
[1mStep[0m  [56/84], [94mLoss[0m : 2.43499
[1mStep[0m  [64/84], [94mLoss[0m : 2.57672
[1mStep[0m  [72/84], [94mLoss[0m : 2.58086
[1mStep[0m  [80/84], [94mLoss[0m : 2.62189

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.467, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24340
[1mStep[0m  [8/84], [94mLoss[0m : 2.53498
[1mStep[0m  [16/84], [94mLoss[0m : 2.43591
[1mStep[0m  [24/84], [94mLoss[0m : 2.64410
[1mStep[0m  [32/84], [94mLoss[0m : 2.72600
[1mStep[0m  [40/84], [94mLoss[0m : 2.38952
[1mStep[0m  [48/84], [94mLoss[0m : 2.43672
[1mStep[0m  [56/84], [94mLoss[0m : 2.66025
[1mStep[0m  [64/84], [94mLoss[0m : 2.44031
[1mStep[0m  [72/84], [94mLoss[0m : 2.63365
[1mStep[0m  [80/84], [94mLoss[0m : 2.15659

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66904
[1mStep[0m  [8/84], [94mLoss[0m : 2.40436
[1mStep[0m  [16/84], [94mLoss[0m : 2.60001
[1mStep[0m  [24/84], [94mLoss[0m : 2.23133
[1mStep[0m  [32/84], [94mLoss[0m : 2.49647
[1mStep[0m  [40/84], [94mLoss[0m : 2.44075
[1mStep[0m  [48/84], [94mLoss[0m : 2.24871
[1mStep[0m  [56/84], [94mLoss[0m : 2.76638
[1mStep[0m  [64/84], [94mLoss[0m : 2.45828
[1mStep[0m  [72/84], [94mLoss[0m : 2.27087
[1mStep[0m  [80/84], [94mLoss[0m : 2.53189

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55628
[1mStep[0m  [8/84], [94mLoss[0m : 2.38531
[1mStep[0m  [16/84], [94mLoss[0m : 2.50808
[1mStep[0m  [24/84], [94mLoss[0m : 2.33941
[1mStep[0m  [32/84], [94mLoss[0m : 2.13762
[1mStep[0m  [40/84], [94mLoss[0m : 2.35507
[1mStep[0m  [48/84], [94mLoss[0m : 2.41666
[1mStep[0m  [56/84], [94mLoss[0m : 2.66023
[1mStep[0m  [64/84], [94mLoss[0m : 2.14641
[1mStep[0m  [72/84], [94mLoss[0m : 2.49408
[1mStep[0m  [80/84], [94mLoss[0m : 2.38145

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38843
[1mStep[0m  [8/84], [94mLoss[0m : 2.16646
[1mStep[0m  [16/84], [94mLoss[0m : 2.48456
[1mStep[0m  [24/84], [94mLoss[0m : 2.26569
[1mStep[0m  [32/84], [94mLoss[0m : 2.77377
[1mStep[0m  [40/84], [94mLoss[0m : 2.67348
[1mStep[0m  [48/84], [94mLoss[0m : 2.48712
[1mStep[0m  [56/84], [94mLoss[0m : 2.46299
[1mStep[0m  [64/84], [94mLoss[0m : 2.47717
[1mStep[0m  [72/84], [94mLoss[0m : 2.43786
[1mStep[0m  [80/84], [94mLoss[0m : 2.30565

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68908
[1mStep[0m  [8/84], [94mLoss[0m : 2.37884
[1mStep[0m  [16/84], [94mLoss[0m : 2.68724
[1mStep[0m  [24/84], [94mLoss[0m : 2.57817
[1mStep[0m  [32/84], [94mLoss[0m : 2.43643
[1mStep[0m  [40/84], [94mLoss[0m : 2.27703
[1mStep[0m  [48/84], [94mLoss[0m : 2.79922
[1mStep[0m  [56/84], [94mLoss[0m : 2.65111
[1mStep[0m  [64/84], [94mLoss[0m : 2.24568
[1mStep[0m  [72/84], [94mLoss[0m : 2.43222
[1mStep[0m  [80/84], [94mLoss[0m : 2.50321

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38905
[1mStep[0m  [8/84], [94mLoss[0m : 2.44366
[1mStep[0m  [16/84], [94mLoss[0m : 2.42984
[1mStep[0m  [24/84], [94mLoss[0m : 2.31082
[1mStep[0m  [32/84], [94mLoss[0m : 2.34078
[1mStep[0m  [40/84], [94mLoss[0m : 2.16271
[1mStep[0m  [48/84], [94mLoss[0m : 2.78310
[1mStep[0m  [56/84], [94mLoss[0m : 2.54553
[1mStep[0m  [64/84], [94mLoss[0m : 2.58383
[1mStep[0m  [72/84], [94mLoss[0m : 2.36692
[1mStep[0m  [80/84], [94mLoss[0m : 2.45064

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.442, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48573
[1mStep[0m  [8/84], [94mLoss[0m : 2.32114
[1mStep[0m  [16/84], [94mLoss[0m : 2.37582
[1mStep[0m  [24/84], [94mLoss[0m : 2.23970
[1mStep[0m  [32/84], [94mLoss[0m : 2.60754
[1mStep[0m  [40/84], [94mLoss[0m : 2.34439
[1mStep[0m  [48/84], [94mLoss[0m : 2.71020
[1mStep[0m  [56/84], [94mLoss[0m : 2.65208
[1mStep[0m  [64/84], [94mLoss[0m : 2.60797
[1mStep[0m  [72/84], [94mLoss[0m : 2.92119
[1mStep[0m  [80/84], [94mLoss[0m : 2.82478

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.450, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46750
[1mStep[0m  [8/84], [94mLoss[0m : 2.58488
[1mStep[0m  [16/84], [94mLoss[0m : 2.44671
[1mStep[0m  [24/84], [94mLoss[0m : 2.69131
[1mStep[0m  [32/84], [94mLoss[0m : 2.13735
[1mStep[0m  [40/84], [94mLoss[0m : 2.33991
[1mStep[0m  [48/84], [94mLoss[0m : 2.54136
[1mStep[0m  [56/84], [94mLoss[0m : 2.36067
[1mStep[0m  [64/84], [94mLoss[0m : 2.82062
[1mStep[0m  [72/84], [94mLoss[0m : 2.41831
[1mStep[0m  [80/84], [94mLoss[0m : 2.37801

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23724
[1mStep[0m  [8/84], [94mLoss[0m : 2.55863
[1mStep[0m  [16/84], [94mLoss[0m : 2.51416
[1mStep[0m  [24/84], [94mLoss[0m : 2.34266
[1mStep[0m  [32/84], [94mLoss[0m : 2.39709
[1mStep[0m  [40/84], [94mLoss[0m : 2.57846
[1mStep[0m  [48/84], [94mLoss[0m : 2.60558
[1mStep[0m  [56/84], [94mLoss[0m : 2.62176
[1mStep[0m  [64/84], [94mLoss[0m : 2.68890
[1mStep[0m  [72/84], [94mLoss[0m : 2.76172
[1mStep[0m  [80/84], [94mLoss[0m : 2.44289

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.451, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29783
[1mStep[0m  [8/84], [94mLoss[0m : 2.31535
[1mStep[0m  [16/84], [94mLoss[0m : 2.48039
[1mStep[0m  [24/84], [94mLoss[0m : 2.33637
[1mStep[0m  [32/84], [94mLoss[0m : 2.71050
[1mStep[0m  [40/84], [94mLoss[0m : 2.73197
[1mStep[0m  [48/84], [94mLoss[0m : 2.53838
[1mStep[0m  [56/84], [94mLoss[0m : 2.61190
[1mStep[0m  [64/84], [94mLoss[0m : 2.61427
[1mStep[0m  [72/84], [94mLoss[0m : 2.51301
[1mStep[0m  [80/84], [94mLoss[0m : 2.43573

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.434, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.440
====================================

Phase 2 - Evaluation MAE:  2.439824734415327
MAE score P1      2.627065
MAE score P2      2.439825
loss              2.479219
learning_rate       0.0001
batch_size             128
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 20, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.53118
[1mStep[0m  [8/84], [94mLoss[0m : 10.59081
[1mStep[0m  [16/84], [94mLoss[0m : 10.54707
[1mStep[0m  [24/84], [94mLoss[0m : 10.07597
[1mStep[0m  [32/84], [94mLoss[0m : 10.15758
[1mStep[0m  [40/84], [94mLoss[0m : 9.78864
[1mStep[0m  [48/84], [94mLoss[0m : 9.51425
[1mStep[0m  [56/84], [94mLoss[0m : 9.96243
[1mStep[0m  [64/84], [94mLoss[0m : 8.70914
[1mStep[0m  [72/84], [94mLoss[0m : 8.45945
[1mStep[0m  [80/84], [94mLoss[0m : 8.69112

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.766, [92mTest[0m: 10.677, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.34023
[1mStep[0m  [8/84], [94mLoss[0m : 8.18409
[1mStep[0m  [16/84], [94mLoss[0m : 7.87809
[1mStep[0m  [24/84], [94mLoss[0m : 7.92931
[1mStep[0m  [32/84], [94mLoss[0m : 7.66597
[1mStep[0m  [40/84], [94mLoss[0m : 7.26689
[1mStep[0m  [48/84], [94mLoss[0m : 7.74311
[1mStep[0m  [56/84], [94mLoss[0m : 7.15783
[1mStep[0m  [64/84], [94mLoss[0m : 7.07410
[1mStep[0m  [72/84], [94mLoss[0m : 6.92596
[1mStep[0m  [80/84], [94mLoss[0m : 6.19885

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.614, [92mTest[0m: 8.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.34719
[1mStep[0m  [8/84], [94mLoss[0m : 6.39824
[1mStep[0m  [16/84], [94mLoss[0m : 5.89452
[1mStep[0m  [24/84], [94mLoss[0m : 5.99052
[1mStep[0m  [32/84], [94mLoss[0m : 5.53824
[1mStep[0m  [40/84], [94mLoss[0m : 5.49968
[1mStep[0m  [48/84], [94mLoss[0m : 5.00849
[1mStep[0m  [56/84], [94mLoss[0m : 4.90380
[1mStep[0m  [64/84], [94mLoss[0m : 4.94105
[1mStep[0m  [72/84], [94mLoss[0m : 4.56539
[1mStep[0m  [80/84], [94mLoss[0m : 4.06350

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.477, [92mTest[0m: 6.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.41958
[1mStep[0m  [8/84], [94mLoss[0m : 4.62510
[1mStep[0m  [16/84], [94mLoss[0m : 4.20837
[1mStep[0m  [24/84], [94mLoss[0m : 4.19774
[1mStep[0m  [32/84], [94mLoss[0m : 3.88276
[1mStep[0m  [40/84], [94mLoss[0m : 4.00428
[1mStep[0m  [48/84], [94mLoss[0m : 3.76105
[1mStep[0m  [56/84], [94mLoss[0m : 3.54288
[1mStep[0m  [64/84], [94mLoss[0m : 4.26801
[1mStep[0m  [72/84], [94mLoss[0m : 3.66520
[1mStep[0m  [80/84], [94mLoss[0m : 3.37970

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.974, [92mTest[0m: 4.520, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.45433
[1mStep[0m  [8/84], [94mLoss[0m : 3.40468
[1mStep[0m  [16/84], [94mLoss[0m : 3.96927
[1mStep[0m  [24/84], [94mLoss[0m : 3.15349
[1mStep[0m  [32/84], [94mLoss[0m : 3.28345
[1mStep[0m  [40/84], [94mLoss[0m : 3.26932
[1mStep[0m  [48/84], [94mLoss[0m : 2.99365
[1mStep[0m  [56/84], [94mLoss[0m : 3.09551
[1mStep[0m  [64/84], [94mLoss[0m : 3.42143
[1mStep[0m  [72/84], [94mLoss[0m : 3.13334
[1mStep[0m  [80/84], [94mLoss[0m : 3.24547

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.284, [92mTest[0m: 3.458, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.51424
[1mStep[0m  [8/84], [94mLoss[0m : 3.52729
[1mStep[0m  [16/84], [94mLoss[0m : 2.92139
[1mStep[0m  [24/84], [94mLoss[0m : 3.08111
[1mStep[0m  [32/84], [94mLoss[0m : 2.81468
[1mStep[0m  [40/84], [94mLoss[0m : 2.68998
[1mStep[0m  [48/84], [94mLoss[0m : 2.74749
[1mStep[0m  [56/84], [94mLoss[0m : 3.00289
[1mStep[0m  [64/84], [94mLoss[0m : 3.06787
[1mStep[0m  [72/84], [94mLoss[0m : 2.64043
[1mStep[0m  [80/84], [94mLoss[0m : 2.65467

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.988, [92mTest[0m: 2.999, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67600
[1mStep[0m  [8/84], [94mLoss[0m : 3.15363
[1mStep[0m  [16/84], [94mLoss[0m : 2.74299
[1mStep[0m  [24/84], [94mLoss[0m : 2.68127
[1mStep[0m  [32/84], [94mLoss[0m : 2.75175
[1mStep[0m  [40/84], [94mLoss[0m : 3.23050
[1mStep[0m  [48/84], [94mLoss[0m : 2.91614
[1mStep[0m  [56/84], [94mLoss[0m : 2.92539
[1mStep[0m  [64/84], [94mLoss[0m : 2.80528
[1mStep[0m  [72/84], [94mLoss[0m : 3.04055
[1mStep[0m  [80/84], [94mLoss[0m : 2.88842

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.837, [92mTest[0m: 2.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75266
[1mStep[0m  [8/84], [94mLoss[0m : 2.72460
[1mStep[0m  [16/84], [94mLoss[0m : 2.66123
[1mStep[0m  [24/84], [94mLoss[0m : 2.48832
[1mStep[0m  [32/84], [94mLoss[0m : 2.75162
[1mStep[0m  [40/84], [94mLoss[0m : 2.84904
[1mStep[0m  [48/84], [94mLoss[0m : 3.04633
[1mStep[0m  [56/84], [94mLoss[0m : 2.57965
[1mStep[0m  [64/84], [94mLoss[0m : 2.55792
[1mStep[0m  [72/84], [94mLoss[0m : 2.37051
[1mStep[0m  [80/84], [94mLoss[0m : 2.99787

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97834
[1mStep[0m  [8/84], [94mLoss[0m : 2.59279
[1mStep[0m  [16/84], [94mLoss[0m : 2.82201
[1mStep[0m  [24/84], [94mLoss[0m : 2.66060
[1mStep[0m  [32/84], [94mLoss[0m : 2.53945
[1mStep[0m  [40/84], [94mLoss[0m : 2.45939
[1mStep[0m  [48/84], [94mLoss[0m : 2.65598
[1mStep[0m  [56/84], [94mLoss[0m : 2.68125
[1mStep[0m  [64/84], [94mLoss[0m : 2.55547
[1mStep[0m  [72/84], [94mLoss[0m : 2.34659
[1mStep[0m  [80/84], [94mLoss[0m : 2.84646

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72840
[1mStep[0m  [8/84], [94mLoss[0m : 2.53657
[1mStep[0m  [16/84], [94mLoss[0m : 2.73337
[1mStep[0m  [24/84], [94mLoss[0m : 2.79163
[1mStep[0m  [32/84], [94mLoss[0m : 2.75881
[1mStep[0m  [40/84], [94mLoss[0m : 2.37277
[1mStep[0m  [48/84], [94mLoss[0m : 2.82229
[1mStep[0m  [56/84], [94mLoss[0m : 2.71999
[1mStep[0m  [64/84], [94mLoss[0m : 2.55786
[1mStep[0m  [72/84], [94mLoss[0m : 2.45282
[1mStep[0m  [80/84], [94mLoss[0m : 2.67959

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.512, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31278
[1mStep[0m  [8/84], [94mLoss[0m : 2.45005
[1mStep[0m  [16/84], [94mLoss[0m : 2.13447
[1mStep[0m  [24/84], [94mLoss[0m : 2.57123
[1mStep[0m  [32/84], [94mLoss[0m : 2.72982
[1mStep[0m  [40/84], [94mLoss[0m : 2.84969
[1mStep[0m  [48/84], [94mLoss[0m : 2.65511
[1mStep[0m  [56/84], [94mLoss[0m : 2.58303
[1mStep[0m  [64/84], [94mLoss[0m : 2.37726
[1mStep[0m  [72/84], [94mLoss[0m : 2.56641
[1mStep[0m  [80/84], [94mLoss[0m : 2.75224

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51927
[1mStep[0m  [8/84], [94mLoss[0m : 2.38243
[1mStep[0m  [16/84], [94mLoss[0m : 2.79888
[1mStep[0m  [24/84], [94mLoss[0m : 2.30862
[1mStep[0m  [32/84], [94mLoss[0m : 2.81931
[1mStep[0m  [40/84], [94mLoss[0m : 2.66835
[1mStep[0m  [48/84], [94mLoss[0m : 2.50181
[1mStep[0m  [56/84], [94mLoss[0m : 2.76372
[1mStep[0m  [64/84], [94mLoss[0m : 2.93135
[1mStep[0m  [72/84], [94mLoss[0m : 2.93642
[1mStep[0m  [80/84], [94mLoss[0m : 2.47020

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.464, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53367
[1mStep[0m  [8/84], [94mLoss[0m : 2.53202
[1mStep[0m  [16/84], [94mLoss[0m : 2.60879
[1mStep[0m  [24/84], [94mLoss[0m : 2.53026
[1mStep[0m  [32/84], [94mLoss[0m : 2.59670
[1mStep[0m  [40/84], [94mLoss[0m : 2.47703
[1mStep[0m  [48/84], [94mLoss[0m : 2.72342
[1mStep[0m  [56/84], [94mLoss[0m : 2.52145
[1mStep[0m  [64/84], [94mLoss[0m : 2.49621
[1mStep[0m  [72/84], [94mLoss[0m : 2.36963
[1mStep[0m  [80/84], [94mLoss[0m : 2.71106

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.455, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.92206
[1mStep[0m  [8/84], [94mLoss[0m : 2.57578
[1mStep[0m  [16/84], [94mLoss[0m : 2.53295
[1mStep[0m  [24/84], [94mLoss[0m : 2.89473
[1mStep[0m  [32/84], [94mLoss[0m : 2.70403
[1mStep[0m  [40/84], [94mLoss[0m : 2.82766
[1mStep[0m  [48/84], [94mLoss[0m : 2.23028
[1mStep[0m  [56/84], [94mLoss[0m : 2.47485
[1mStep[0m  [64/84], [94mLoss[0m : 2.59657
[1mStep[0m  [72/84], [94mLoss[0m : 2.52754
[1mStep[0m  [80/84], [94mLoss[0m : 2.54618

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.446, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58591
[1mStep[0m  [8/84], [94mLoss[0m : 2.79430
[1mStep[0m  [16/84], [94mLoss[0m : 2.67956
[1mStep[0m  [24/84], [94mLoss[0m : 2.67502
[1mStep[0m  [32/84], [94mLoss[0m : 2.48668
[1mStep[0m  [40/84], [94mLoss[0m : 2.85528
[1mStep[0m  [48/84], [94mLoss[0m : 2.46166
[1mStep[0m  [56/84], [94mLoss[0m : 2.53945
[1mStep[0m  [64/84], [94mLoss[0m : 2.38344
[1mStep[0m  [72/84], [94mLoss[0m : 2.89412
[1mStep[0m  [80/84], [94mLoss[0m : 2.48232

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.448, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23356
[1mStep[0m  [8/84], [94mLoss[0m : 2.46843
[1mStep[0m  [16/84], [94mLoss[0m : 2.51534
[1mStep[0m  [24/84], [94mLoss[0m : 2.80841
[1mStep[0m  [32/84], [94mLoss[0m : 2.68043
[1mStep[0m  [40/84], [94mLoss[0m : 2.57624
[1mStep[0m  [48/84], [94mLoss[0m : 2.65994
[1mStep[0m  [56/84], [94mLoss[0m : 2.47552
[1mStep[0m  [64/84], [94mLoss[0m : 2.70554
[1mStep[0m  [72/84], [94mLoss[0m : 2.71800
[1mStep[0m  [80/84], [94mLoss[0m : 2.54913

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.438, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59445
[1mStep[0m  [8/84], [94mLoss[0m : 2.20430
[1mStep[0m  [16/84], [94mLoss[0m : 2.49453
[1mStep[0m  [24/84], [94mLoss[0m : 2.76237
[1mStep[0m  [32/84], [94mLoss[0m : 2.66525
[1mStep[0m  [40/84], [94mLoss[0m : 2.51506
[1mStep[0m  [48/84], [94mLoss[0m : 2.51979
[1mStep[0m  [56/84], [94mLoss[0m : 2.20217
[1mStep[0m  [64/84], [94mLoss[0m : 2.83196
[1mStep[0m  [72/84], [94mLoss[0m : 2.54745
[1mStep[0m  [80/84], [94mLoss[0m : 2.50809

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.432, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51595
[1mStep[0m  [8/84], [94mLoss[0m : 2.86298
[1mStep[0m  [16/84], [94mLoss[0m : 2.80134
[1mStep[0m  [24/84], [94mLoss[0m : 2.53342
[1mStep[0m  [32/84], [94mLoss[0m : 2.36722
[1mStep[0m  [40/84], [94mLoss[0m : 2.81187
[1mStep[0m  [48/84], [94mLoss[0m : 2.71588
[1mStep[0m  [56/84], [94mLoss[0m : 2.53355
[1mStep[0m  [64/84], [94mLoss[0m : 2.61939
[1mStep[0m  [72/84], [94mLoss[0m : 2.64848
[1mStep[0m  [80/84], [94mLoss[0m : 2.50684

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47708
[1mStep[0m  [8/84], [94mLoss[0m : 2.47925
[1mStep[0m  [16/84], [94mLoss[0m : 2.47795
[1mStep[0m  [24/84], [94mLoss[0m : 2.51004
[1mStep[0m  [32/84], [94mLoss[0m : 2.88519
[1mStep[0m  [40/84], [94mLoss[0m : 2.37711
[1mStep[0m  [48/84], [94mLoss[0m : 2.41223
[1mStep[0m  [56/84], [94mLoss[0m : 2.55166
[1mStep[0m  [64/84], [94mLoss[0m : 2.45623
[1mStep[0m  [72/84], [94mLoss[0m : 2.70058
[1mStep[0m  [80/84], [94mLoss[0m : 2.57128

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49359
[1mStep[0m  [8/84], [94mLoss[0m : 2.75093
[1mStep[0m  [16/84], [94mLoss[0m : 2.60911
[1mStep[0m  [24/84], [94mLoss[0m : 2.30262
[1mStep[0m  [32/84], [94mLoss[0m : 2.55954
[1mStep[0m  [40/84], [94mLoss[0m : 2.60529
[1mStep[0m  [48/84], [94mLoss[0m : 2.68353
[1mStep[0m  [56/84], [94mLoss[0m : 2.64634
[1mStep[0m  [64/84], [94mLoss[0m : 2.33442
[1mStep[0m  [72/84], [94mLoss[0m : 2.60319
[1mStep[0m  [80/84], [94mLoss[0m : 2.50117

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.430, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.86843
[1mStep[0m  [8/84], [94mLoss[0m : 2.79940
[1mStep[0m  [16/84], [94mLoss[0m : 2.45017
[1mStep[0m  [24/84], [94mLoss[0m : 2.49425
[1mStep[0m  [32/84], [94mLoss[0m : 2.54068
[1mStep[0m  [40/84], [94mLoss[0m : 2.15493
[1mStep[0m  [48/84], [94mLoss[0m : 2.42774
[1mStep[0m  [56/84], [94mLoss[0m : 2.55038
[1mStep[0m  [64/84], [94mLoss[0m : 2.34786
[1mStep[0m  [72/84], [94mLoss[0m : 2.37859
[1mStep[0m  [80/84], [94mLoss[0m : 2.69314

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.412, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56954
[1mStep[0m  [8/84], [94mLoss[0m : 2.71764
[1mStep[0m  [16/84], [94mLoss[0m : 2.68265
[1mStep[0m  [24/84], [94mLoss[0m : 2.44428
[1mStep[0m  [32/84], [94mLoss[0m : 2.25521
[1mStep[0m  [40/84], [94mLoss[0m : 2.61834
[1mStep[0m  [48/84], [94mLoss[0m : 2.89618
[1mStep[0m  [56/84], [94mLoss[0m : 2.76586
[1mStep[0m  [64/84], [94mLoss[0m : 2.35037
[1mStep[0m  [72/84], [94mLoss[0m : 2.58875
[1mStep[0m  [80/84], [94mLoss[0m : 2.81268

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.413, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49457
[1mStep[0m  [8/84], [94mLoss[0m : 2.75495
[1mStep[0m  [16/84], [94mLoss[0m : 2.52684
[1mStep[0m  [24/84], [94mLoss[0m : 2.29823
[1mStep[0m  [32/84], [94mLoss[0m : 2.47965
[1mStep[0m  [40/84], [94mLoss[0m : 2.61860
[1mStep[0m  [48/84], [94mLoss[0m : 2.63713
[1mStep[0m  [56/84], [94mLoss[0m : 2.69823
[1mStep[0m  [64/84], [94mLoss[0m : 2.14514
[1mStep[0m  [72/84], [94mLoss[0m : 2.23098
[1mStep[0m  [80/84], [94mLoss[0m : 2.65217

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.423, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40562
[1mStep[0m  [8/84], [94mLoss[0m : 2.44230
[1mStep[0m  [16/84], [94mLoss[0m : 2.60365
[1mStep[0m  [24/84], [94mLoss[0m : 2.37071
[1mStep[0m  [32/84], [94mLoss[0m : 2.31966
[1mStep[0m  [40/84], [94mLoss[0m : 2.37602
[1mStep[0m  [48/84], [94mLoss[0m : 2.43954
[1mStep[0m  [56/84], [94mLoss[0m : 2.74012
[1mStep[0m  [64/84], [94mLoss[0m : 2.60120
[1mStep[0m  [72/84], [94mLoss[0m : 2.64381
[1mStep[0m  [80/84], [94mLoss[0m : 2.56079

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.420, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62168
[1mStep[0m  [8/84], [94mLoss[0m : 2.55367
[1mStep[0m  [16/84], [94mLoss[0m : 2.58253
[1mStep[0m  [24/84], [94mLoss[0m : 2.67109
[1mStep[0m  [32/84], [94mLoss[0m : 2.52343
[1mStep[0m  [40/84], [94mLoss[0m : 2.49266
[1mStep[0m  [48/84], [94mLoss[0m : 2.47225
[1mStep[0m  [56/84], [94mLoss[0m : 2.32067
[1mStep[0m  [64/84], [94mLoss[0m : 2.55411
[1mStep[0m  [72/84], [94mLoss[0m : 2.61984
[1mStep[0m  [80/84], [94mLoss[0m : 2.38234

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.409, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53538
[1mStep[0m  [8/84], [94mLoss[0m : 2.86551
[1mStep[0m  [16/84], [94mLoss[0m : 2.33644
[1mStep[0m  [24/84], [94mLoss[0m : 2.61818
[1mStep[0m  [32/84], [94mLoss[0m : 2.63202
[1mStep[0m  [40/84], [94mLoss[0m : 2.65856
[1mStep[0m  [48/84], [94mLoss[0m : 2.71441
[1mStep[0m  [56/84], [94mLoss[0m : 2.68361
[1mStep[0m  [64/84], [94mLoss[0m : 2.66598
[1mStep[0m  [72/84], [94mLoss[0m : 2.31430
[1mStep[0m  [80/84], [94mLoss[0m : 2.35685

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.408, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29019
[1mStep[0m  [8/84], [94mLoss[0m : 2.63035
[1mStep[0m  [16/84], [94mLoss[0m : 2.21259
[1mStep[0m  [24/84], [94mLoss[0m : 2.38397
[1mStep[0m  [32/84], [94mLoss[0m : 2.60393
[1mStep[0m  [40/84], [94mLoss[0m : 2.65635
[1mStep[0m  [48/84], [94mLoss[0m : 2.56671
[1mStep[0m  [56/84], [94mLoss[0m : 2.48681
[1mStep[0m  [64/84], [94mLoss[0m : 2.31517
[1mStep[0m  [72/84], [94mLoss[0m : 2.85258
[1mStep[0m  [80/84], [94mLoss[0m : 2.53240

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.408, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60140
[1mStep[0m  [8/84], [94mLoss[0m : 2.32777
[1mStep[0m  [16/84], [94mLoss[0m : 2.60309
[1mStep[0m  [24/84], [94mLoss[0m : 2.56739
[1mStep[0m  [32/84], [94mLoss[0m : 2.54326
[1mStep[0m  [40/84], [94mLoss[0m : 3.03351
[1mStep[0m  [48/84], [94mLoss[0m : 2.28323
[1mStep[0m  [56/84], [94mLoss[0m : 2.30232
[1mStep[0m  [64/84], [94mLoss[0m : 2.83796
[1mStep[0m  [72/84], [94mLoss[0m : 2.63633
[1mStep[0m  [80/84], [94mLoss[0m : 2.24165

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.404, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55764
[1mStep[0m  [8/84], [94mLoss[0m : 2.36500
[1mStep[0m  [16/84], [94mLoss[0m : 2.42158
[1mStep[0m  [24/84], [94mLoss[0m : 2.29170
[1mStep[0m  [32/84], [94mLoss[0m : 2.17385
[1mStep[0m  [40/84], [94mLoss[0m : 2.37549
[1mStep[0m  [48/84], [94mLoss[0m : 2.69790
[1mStep[0m  [56/84], [94mLoss[0m : 2.37454
[1mStep[0m  [64/84], [94mLoss[0m : 2.46346
[1mStep[0m  [72/84], [94mLoss[0m : 2.50238
[1mStep[0m  [80/84], [94mLoss[0m : 2.38781

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.400, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61324
[1mStep[0m  [8/84], [94mLoss[0m : 2.46115
[1mStep[0m  [16/84], [94mLoss[0m : 2.53542
[1mStep[0m  [24/84], [94mLoss[0m : 2.71597
[1mStep[0m  [32/84], [94mLoss[0m : 2.51736
[1mStep[0m  [40/84], [94mLoss[0m : 2.81957
[1mStep[0m  [48/84], [94mLoss[0m : 2.51067
[1mStep[0m  [56/84], [94mLoss[0m : 2.33307
[1mStep[0m  [64/84], [94mLoss[0m : 2.48295
[1mStep[0m  [72/84], [94mLoss[0m : 2.69440
[1mStep[0m  [80/84], [94mLoss[0m : 2.34386

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.397, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.392504555838449
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.69662
[1mStep[0m  [8/84], [94mLoss[0m : 2.32339
[1mStep[0m  [16/84], [94mLoss[0m : 2.77592
[1mStep[0m  [24/84], [94mLoss[0m : 2.28753
[1mStep[0m  [32/84], [94mLoss[0m : 2.71370
[1mStep[0m  [40/84], [94mLoss[0m : 2.52176
[1mStep[0m  [48/84], [94mLoss[0m : 2.51410
[1mStep[0m  [56/84], [94mLoss[0m : 2.12487
[1mStep[0m  [64/84], [94mLoss[0m : 3.12969
[1mStep[0m  [72/84], [94mLoss[0m : 2.60234
[1mStep[0m  [80/84], [94mLoss[0m : 2.38703

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.387, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46658
[1mStep[0m  [8/84], [94mLoss[0m : 2.40337
[1mStep[0m  [16/84], [94mLoss[0m : 2.34446
[1mStep[0m  [24/84], [94mLoss[0m : 2.12425
[1mStep[0m  [32/84], [94mLoss[0m : 2.77948
[1mStep[0m  [40/84], [94mLoss[0m : 2.80662
[1mStep[0m  [48/84], [94mLoss[0m : 2.60890
[1mStep[0m  [56/84], [94mLoss[0m : 2.70755
[1mStep[0m  [64/84], [94mLoss[0m : 2.26001
[1mStep[0m  [72/84], [94mLoss[0m : 2.59755
[1mStep[0m  [80/84], [94mLoss[0m : 2.68695

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.404, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52907
[1mStep[0m  [8/84], [94mLoss[0m : 2.58384
[1mStep[0m  [16/84], [94mLoss[0m : 2.65018
[1mStep[0m  [24/84], [94mLoss[0m : 2.62912
[1mStep[0m  [32/84], [94mLoss[0m : 2.65714
[1mStep[0m  [40/84], [94mLoss[0m : 2.59159
[1mStep[0m  [48/84], [94mLoss[0m : 2.47015
[1mStep[0m  [56/84], [94mLoss[0m : 2.60887
[1mStep[0m  [64/84], [94mLoss[0m : 2.74542
[1mStep[0m  [72/84], [94mLoss[0m : 2.36061
[1mStep[0m  [80/84], [94mLoss[0m : 2.25677

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.432, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55270
[1mStep[0m  [8/84], [94mLoss[0m : 2.28061
[1mStep[0m  [16/84], [94mLoss[0m : 2.57758
[1mStep[0m  [24/84], [94mLoss[0m : 2.43471
[1mStep[0m  [32/84], [94mLoss[0m : 2.52074
[1mStep[0m  [40/84], [94mLoss[0m : 2.44831
[1mStep[0m  [48/84], [94mLoss[0m : 2.50454
[1mStep[0m  [56/84], [94mLoss[0m : 2.67298
[1mStep[0m  [64/84], [94mLoss[0m : 2.69708
[1mStep[0m  [72/84], [94mLoss[0m : 2.48758
[1mStep[0m  [80/84], [94mLoss[0m : 2.22740

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40324
[1mStep[0m  [8/84], [94mLoss[0m : 2.58466
[1mStep[0m  [16/84], [94mLoss[0m : 2.61524
[1mStep[0m  [24/84], [94mLoss[0m : 2.37621
[1mStep[0m  [32/84], [94mLoss[0m : 2.74839
[1mStep[0m  [40/84], [94mLoss[0m : 2.31867
[1mStep[0m  [48/84], [94mLoss[0m : 2.43293
[1mStep[0m  [56/84], [94mLoss[0m : 3.00189
[1mStep[0m  [64/84], [94mLoss[0m : 2.65535
[1mStep[0m  [72/84], [94mLoss[0m : 2.62460
[1mStep[0m  [80/84], [94mLoss[0m : 2.78224

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.452, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43868
[1mStep[0m  [8/84], [94mLoss[0m : 2.67805
[1mStep[0m  [16/84], [94mLoss[0m : 2.43978
[1mStep[0m  [24/84], [94mLoss[0m : 2.30162
[1mStep[0m  [32/84], [94mLoss[0m : 2.52756
[1mStep[0m  [40/84], [94mLoss[0m : 2.73159
[1mStep[0m  [48/84], [94mLoss[0m : 2.80662
[1mStep[0m  [56/84], [94mLoss[0m : 2.49261
[1mStep[0m  [64/84], [94mLoss[0m : 2.51827
[1mStep[0m  [72/84], [94mLoss[0m : 2.53720
[1mStep[0m  [80/84], [94mLoss[0m : 2.60256

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.432, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30083
[1mStep[0m  [8/84], [94mLoss[0m : 2.29479
[1mStep[0m  [16/84], [94mLoss[0m : 2.57286
[1mStep[0m  [24/84], [94mLoss[0m : 2.59103
[1mStep[0m  [32/84], [94mLoss[0m : 2.51668
[1mStep[0m  [40/84], [94mLoss[0m : 2.74023
[1mStep[0m  [48/84], [94mLoss[0m : 2.39326
[1mStep[0m  [56/84], [94mLoss[0m : 2.35385
[1mStep[0m  [64/84], [94mLoss[0m : 2.19870
[1mStep[0m  [72/84], [94mLoss[0m : 2.32013
[1mStep[0m  [80/84], [94mLoss[0m : 2.80650

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.493, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20842
[1mStep[0m  [8/84], [94mLoss[0m : 2.47015
[1mStep[0m  [16/84], [94mLoss[0m : 2.45422
[1mStep[0m  [24/84], [94mLoss[0m : 2.43901
[1mStep[0m  [32/84], [94mLoss[0m : 2.83182
[1mStep[0m  [40/84], [94mLoss[0m : 2.38216
[1mStep[0m  [48/84], [94mLoss[0m : 2.49132
[1mStep[0m  [56/84], [94mLoss[0m : 2.29206
[1mStep[0m  [64/84], [94mLoss[0m : 2.21722
[1mStep[0m  [72/84], [94mLoss[0m : 2.44777
[1mStep[0m  [80/84], [94mLoss[0m : 2.36194

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.455, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43594
[1mStep[0m  [8/84], [94mLoss[0m : 2.24866
[1mStep[0m  [16/84], [94mLoss[0m : 2.41818
[1mStep[0m  [24/84], [94mLoss[0m : 2.29983
[1mStep[0m  [32/84], [94mLoss[0m : 2.38408
[1mStep[0m  [40/84], [94mLoss[0m : 2.43224
[1mStep[0m  [48/84], [94mLoss[0m : 2.59999
[1mStep[0m  [56/84], [94mLoss[0m : 2.46564
[1mStep[0m  [64/84], [94mLoss[0m : 2.48689
[1mStep[0m  [72/84], [94mLoss[0m : 2.64043
[1mStep[0m  [80/84], [94mLoss[0m : 2.41681

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53377
[1mStep[0m  [8/84], [94mLoss[0m : 2.12894
[1mStep[0m  [16/84], [94mLoss[0m : 2.14996
[1mStep[0m  [24/84], [94mLoss[0m : 2.71131
[1mStep[0m  [32/84], [94mLoss[0m : 2.38405
[1mStep[0m  [40/84], [94mLoss[0m : 2.59226
[1mStep[0m  [48/84], [94mLoss[0m : 2.77772
[1mStep[0m  [56/84], [94mLoss[0m : 2.51398
[1mStep[0m  [64/84], [94mLoss[0m : 2.52392
[1mStep[0m  [72/84], [94mLoss[0m : 2.49991
[1mStep[0m  [80/84], [94mLoss[0m : 2.33186

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.467, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66584
[1mStep[0m  [8/84], [94mLoss[0m : 2.59682
[1mStep[0m  [16/84], [94mLoss[0m : 2.48719
[1mStep[0m  [24/84], [94mLoss[0m : 2.69056
[1mStep[0m  [32/84], [94mLoss[0m : 2.53398
[1mStep[0m  [40/84], [94mLoss[0m : 2.56043
[1mStep[0m  [48/84], [94mLoss[0m : 2.46191
[1mStep[0m  [56/84], [94mLoss[0m : 2.47657
[1mStep[0m  [64/84], [94mLoss[0m : 2.24967
[1mStep[0m  [72/84], [94mLoss[0m : 2.66590
[1mStep[0m  [80/84], [94mLoss[0m : 2.76768

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.471, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69543
[1mStep[0m  [8/84], [94mLoss[0m : 2.28376
[1mStep[0m  [16/84], [94mLoss[0m : 2.81524
[1mStep[0m  [24/84], [94mLoss[0m : 2.57946
[1mStep[0m  [32/84], [94mLoss[0m : 2.32358
[1mStep[0m  [40/84], [94mLoss[0m : 2.42184
[1mStep[0m  [48/84], [94mLoss[0m : 2.41044
[1mStep[0m  [56/84], [94mLoss[0m : 2.30450
[1mStep[0m  [64/84], [94mLoss[0m : 2.38785
[1mStep[0m  [72/84], [94mLoss[0m : 2.57883
[1mStep[0m  [80/84], [94mLoss[0m : 2.27118

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.467, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39470
[1mStep[0m  [8/84], [94mLoss[0m : 2.55445
[1mStep[0m  [16/84], [94mLoss[0m : 2.67666
[1mStep[0m  [24/84], [94mLoss[0m : 2.37735
[1mStep[0m  [32/84], [94mLoss[0m : 2.54177
[1mStep[0m  [40/84], [94mLoss[0m : 2.28860
[1mStep[0m  [48/84], [94mLoss[0m : 2.52198
[1mStep[0m  [56/84], [94mLoss[0m : 2.29856
[1mStep[0m  [64/84], [94mLoss[0m : 2.36797
[1mStep[0m  [72/84], [94mLoss[0m : 2.51023
[1mStep[0m  [80/84], [94mLoss[0m : 2.34058

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.488, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37816
[1mStep[0m  [8/84], [94mLoss[0m : 2.41509
[1mStep[0m  [16/84], [94mLoss[0m : 2.41427
[1mStep[0m  [24/84], [94mLoss[0m : 2.60814
[1mStep[0m  [32/84], [94mLoss[0m : 2.57654
[1mStep[0m  [40/84], [94mLoss[0m : 2.24294
[1mStep[0m  [48/84], [94mLoss[0m : 2.36044
[1mStep[0m  [56/84], [94mLoss[0m : 2.51725
[1mStep[0m  [64/84], [94mLoss[0m : 2.50158
[1mStep[0m  [72/84], [94mLoss[0m : 2.33951
[1mStep[0m  [80/84], [94mLoss[0m : 2.46300

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.448, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34577
[1mStep[0m  [8/84], [94mLoss[0m : 2.51559
[1mStep[0m  [16/84], [94mLoss[0m : 2.19603
[1mStep[0m  [24/84], [94mLoss[0m : 2.29941
[1mStep[0m  [32/84], [94mLoss[0m : 2.52432
[1mStep[0m  [40/84], [94mLoss[0m : 2.59822
[1mStep[0m  [48/84], [94mLoss[0m : 2.47675
[1mStep[0m  [56/84], [94mLoss[0m : 2.40248
[1mStep[0m  [64/84], [94mLoss[0m : 2.26694
[1mStep[0m  [72/84], [94mLoss[0m : 2.34856
[1mStep[0m  [80/84], [94mLoss[0m : 2.43745

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.458, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41760
[1mStep[0m  [8/84], [94mLoss[0m : 2.59662
[1mStep[0m  [16/84], [94mLoss[0m : 2.40640
[1mStep[0m  [24/84], [94mLoss[0m : 2.65154
[1mStep[0m  [32/84], [94mLoss[0m : 2.40065
[1mStep[0m  [40/84], [94mLoss[0m : 2.43944
[1mStep[0m  [48/84], [94mLoss[0m : 2.56953
[1mStep[0m  [56/84], [94mLoss[0m : 2.23360
[1mStep[0m  [64/84], [94mLoss[0m : 2.56177
[1mStep[0m  [72/84], [94mLoss[0m : 2.68534
[1mStep[0m  [80/84], [94mLoss[0m : 2.20290

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.475, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40611
[1mStep[0m  [8/84], [94mLoss[0m : 2.29282
[1mStep[0m  [16/84], [94mLoss[0m : 2.81946
[1mStep[0m  [24/84], [94mLoss[0m : 2.53478
[1mStep[0m  [32/84], [94mLoss[0m : 2.50626
[1mStep[0m  [40/84], [94mLoss[0m : 2.47409
[1mStep[0m  [48/84], [94mLoss[0m : 2.52858
[1mStep[0m  [56/84], [94mLoss[0m : 2.55559
[1mStep[0m  [64/84], [94mLoss[0m : 2.42957
[1mStep[0m  [72/84], [94mLoss[0m : 2.43980
[1mStep[0m  [80/84], [94mLoss[0m : 2.51945

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.442, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36443
[1mStep[0m  [8/84], [94mLoss[0m : 2.67728
[1mStep[0m  [16/84], [94mLoss[0m : 2.47417
[1mStep[0m  [24/84], [94mLoss[0m : 2.32742
[1mStep[0m  [32/84], [94mLoss[0m : 2.38480
[1mStep[0m  [40/84], [94mLoss[0m : 2.15738
[1mStep[0m  [48/84], [94mLoss[0m : 2.26991
[1mStep[0m  [56/84], [94mLoss[0m : 2.56894
[1mStep[0m  [64/84], [94mLoss[0m : 2.69287
[1mStep[0m  [72/84], [94mLoss[0m : 2.30820
[1mStep[0m  [80/84], [94mLoss[0m : 2.37899

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.496, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17920
[1mStep[0m  [8/84], [94mLoss[0m : 2.72556
[1mStep[0m  [16/84], [94mLoss[0m : 2.27440
[1mStep[0m  [24/84], [94mLoss[0m : 2.43380
[1mStep[0m  [32/84], [94mLoss[0m : 2.16686
[1mStep[0m  [40/84], [94mLoss[0m : 2.44088
[1mStep[0m  [48/84], [94mLoss[0m : 2.58767
[1mStep[0m  [56/84], [94mLoss[0m : 2.49796
[1mStep[0m  [64/84], [94mLoss[0m : 2.45291
[1mStep[0m  [72/84], [94mLoss[0m : 2.36967
[1mStep[0m  [80/84], [94mLoss[0m : 2.58313

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.524, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38391
[1mStep[0m  [8/84], [94mLoss[0m : 2.43277
[1mStep[0m  [16/84], [94mLoss[0m : 2.25262
[1mStep[0m  [24/84], [94mLoss[0m : 2.23545
[1mStep[0m  [32/84], [94mLoss[0m : 2.58178
[1mStep[0m  [40/84], [94mLoss[0m : 2.36989
[1mStep[0m  [48/84], [94mLoss[0m : 2.43825
[1mStep[0m  [56/84], [94mLoss[0m : 2.57169
[1mStep[0m  [64/84], [94mLoss[0m : 2.43224
[1mStep[0m  [72/84], [94mLoss[0m : 2.63621
[1mStep[0m  [80/84], [94mLoss[0m : 2.57222

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.457, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62910
[1mStep[0m  [8/84], [94mLoss[0m : 2.61369
[1mStep[0m  [16/84], [94mLoss[0m : 2.45277
[1mStep[0m  [24/84], [94mLoss[0m : 2.40984
[1mStep[0m  [32/84], [94mLoss[0m : 2.27782
[1mStep[0m  [40/84], [94mLoss[0m : 2.51218
[1mStep[0m  [48/84], [94mLoss[0m : 2.47599
[1mStep[0m  [56/84], [94mLoss[0m : 2.33459
[1mStep[0m  [64/84], [94mLoss[0m : 2.34445
[1mStep[0m  [72/84], [94mLoss[0m : 2.47960
[1mStep[0m  [80/84], [94mLoss[0m : 2.21279

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.502, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57619
[1mStep[0m  [8/84], [94mLoss[0m : 2.35005
[1mStep[0m  [16/84], [94mLoss[0m : 2.44259
[1mStep[0m  [24/84], [94mLoss[0m : 2.39799
[1mStep[0m  [32/84], [94mLoss[0m : 2.48378
[1mStep[0m  [40/84], [94mLoss[0m : 2.45353
[1mStep[0m  [48/84], [94mLoss[0m : 2.61918
[1mStep[0m  [56/84], [94mLoss[0m : 2.37328
[1mStep[0m  [64/84], [94mLoss[0m : 2.42132
[1mStep[0m  [72/84], [94mLoss[0m : 2.56109
[1mStep[0m  [80/84], [94mLoss[0m : 2.31184

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.488, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53995
[1mStep[0m  [8/84], [94mLoss[0m : 2.63668
[1mStep[0m  [16/84], [94mLoss[0m : 2.61573
[1mStep[0m  [24/84], [94mLoss[0m : 2.19405
[1mStep[0m  [32/84], [94mLoss[0m : 2.41044
[1mStep[0m  [40/84], [94mLoss[0m : 2.43282
[1mStep[0m  [48/84], [94mLoss[0m : 2.60475
[1mStep[0m  [56/84], [94mLoss[0m : 2.55729
[1mStep[0m  [64/84], [94mLoss[0m : 2.34866
[1mStep[0m  [72/84], [94mLoss[0m : 2.36049
[1mStep[0m  [80/84], [94mLoss[0m : 2.51438

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.548, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49240
[1mStep[0m  [8/84], [94mLoss[0m : 2.42136
[1mStep[0m  [16/84], [94mLoss[0m : 2.45329
[1mStep[0m  [24/84], [94mLoss[0m : 2.53349
[1mStep[0m  [32/84], [94mLoss[0m : 2.34353
[1mStep[0m  [40/84], [94mLoss[0m : 2.65092
[1mStep[0m  [48/84], [94mLoss[0m : 2.52078
[1mStep[0m  [56/84], [94mLoss[0m : 2.54751
[1mStep[0m  [64/84], [94mLoss[0m : 2.55583
[1mStep[0m  [72/84], [94mLoss[0m : 2.70251
[1mStep[0m  [80/84], [94mLoss[0m : 2.45413

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.513, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53033
[1mStep[0m  [8/84], [94mLoss[0m : 2.53030
[1mStep[0m  [16/84], [94mLoss[0m : 2.29445
[1mStep[0m  [24/84], [94mLoss[0m : 2.30109
[1mStep[0m  [32/84], [94mLoss[0m : 2.44317
[1mStep[0m  [40/84], [94mLoss[0m : 2.31598
[1mStep[0m  [48/84], [94mLoss[0m : 2.56146
[1mStep[0m  [56/84], [94mLoss[0m : 2.33669
[1mStep[0m  [64/84], [94mLoss[0m : 2.34516
[1mStep[0m  [72/84], [94mLoss[0m : 2.83245
[1mStep[0m  [80/84], [94mLoss[0m : 2.50878

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59866
[1mStep[0m  [8/84], [94mLoss[0m : 2.30231
[1mStep[0m  [16/84], [94mLoss[0m : 2.29801
[1mStep[0m  [24/84], [94mLoss[0m : 2.36940
[1mStep[0m  [32/84], [94mLoss[0m : 2.28204
[1mStep[0m  [40/84], [94mLoss[0m : 2.54086
[1mStep[0m  [48/84], [94mLoss[0m : 2.34433
[1mStep[0m  [56/84], [94mLoss[0m : 2.29204
[1mStep[0m  [64/84], [94mLoss[0m : 2.45882
[1mStep[0m  [72/84], [94mLoss[0m : 2.36642
[1mStep[0m  [80/84], [94mLoss[0m : 2.48911

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.464, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30462
[1mStep[0m  [8/84], [94mLoss[0m : 2.80153
[1mStep[0m  [16/84], [94mLoss[0m : 2.18410
[1mStep[0m  [24/84], [94mLoss[0m : 2.40155
[1mStep[0m  [32/84], [94mLoss[0m : 2.54070
[1mStep[0m  [40/84], [94mLoss[0m : 2.61778
[1mStep[0m  [48/84], [94mLoss[0m : 2.76170
[1mStep[0m  [56/84], [94mLoss[0m : 2.43122
[1mStep[0m  [64/84], [94mLoss[0m : 2.63642
[1mStep[0m  [72/84], [94mLoss[0m : 2.30260
[1mStep[0m  [80/84], [94mLoss[0m : 2.29228

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.476, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52505
[1mStep[0m  [8/84], [94mLoss[0m : 2.45322
[1mStep[0m  [16/84], [94mLoss[0m : 2.23929
[1mStep[0m  [24/84], [94mLoss[0m : 2.19216
[1mStep[0m  [32/84], [94mLoss[0m : 2.30951
[1mStep[0m  [40/84], [94mLoss[0m : 2.23676
[1mStep[0m  [48/84], [94mLoss[0m : 2.55319
[1mStep[0m  [56/84], [94mLoss[0m : 2.40485
[1mStep[0m  [64/84], [94mLoss[0m : 2.71320
[1mStep[0m  [72/84], [94mLoss[0m : 2.25737
[1mStep[0m  [80/84], [94mLoss[0m : 2.44691

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.433, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52201
[1mStep[0m  [8/84], [94mLoss[0m : 2.38276
[1mStep[0m  [16/84], [94mLoss[0m : 2.61068
[1mStep[0m  [24/84], [94mLoss[0m : 2.49040
[1mStep[0m  [32/84], [94mLoss[0m : 2.22157
[1mStep[0m  [40/84], [94mLoss[0m : 2.46750
[1mStep[0m  [48/84], [94mLoss[0m : 2.64770
[1mStep[0m  [56/84], [94mLoss[0m : 2.56051
[1mStep[0m  [64/84], [94mLoss[0m : 2.11606
[1mStep[0m  [72/84], [94mLoss[0m : 2.53761
[1mStep[0m  [80/84], [94mLoss[0m : 2.78915

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.475, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39799
[1mStep[0m  [8/84], [94mLoss[0m : 2.48384
[1mStep[0m  [16/84], [94mLoss[0m : 2.27992
[1mStep[0m  [24/84], [94mLoss[0m : 2.53395
[1mStep[0m  [32/84], [94mLoss[0m : 2.31531
[1mStep[0m  [40/84], [94mLoss[0m : 2.25337
[1mStep[0m  [48/84], [94mLoss[0m : 2.64221
[1mStep[0m  [56/84], [94mLoss[0m : 2.59633
[1mStep[0m  [64/84], [94mLoss[0m : 2.27004
[1mStep[0m  [72/84], [94mLoss[0m : 2.42339
[1mStep[0m  [80/84], [94mLoss[0m : 2.29315

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.484, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.537
====================================

Phase 2 - Evaluation MAE:  2.5369447469711304
MAE score P1        2.392505
MAE score P2        2.536945
loss                2.397713
learning_rate         0.0001
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 21, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.70615
[1mStep[0m  [8/84], [94mLoss[0m : 10.72489
[1mStep[0m  [16/84], [94mLoss[0m : 10.95514
[1mStep[0m  [24/84], [94mLoss[0m : 11.57405
[1mStep[0m  [32/84], [94mLoss[0m : 10.80211
[1mStep[0m  [40/84], [94mLoss[0m : 10.37627
[1mStep[0m  [48/84], [94mLoss[0m : 10.60397
[1mStep[0m  [56/84], [94mLoss[0m : 10.32165
[1mStep[0m  [64/84], [94mLoss[0m : 11.14419
[1mStep[0m  [72/84], [94mLoss[0m : 10.36245
[1mStep[0m  [80/84], [94mLoss[0m : 10.25696

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.581, [92mTest[0m: 10.879, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.99420
[1mStep[0m  [8/84], [94mLoss[0m : 10.12792
[1mStep[0m  [16/84], [94mLoss[0m : 10.14171
[1mStep[0m  [24/84], [94mLoss[0m : 9.74882
[1mStep[0m  [32/84], [94mLoss[0m : 9.27019
[1mStep[0m  [40/84], [94mLoss[0m : 10.06228
[1mStep[0m  [48/84], [94mLoss[0m : 9.90577
[1mStep[0m  [56/84], [94mLoss[0m : 9.76791
[1mStep[0m  [64/84], [94mLoss[0m : 10.30989
[1mStep[0m  [72/84], [94mLoss[0m : 9.66003
[1mStep[0m  [80/84], [94mLoss[0m : 9.23584

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.863, [92mTest[0m: 10.492, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.97637
[1mStep[0m  [8/84], [94mLoss[0m : 9.64942
[1mStep[0m  [16/84], [94mLoss[0m : 9.52218
[1mStep[0m  [24/84], [94mLoss[0m : 8.89947
[1mStep[0m  [32/84], [94mLoss[0m : 9.50212
[1mStep[0m  [40/84], [94mLoss[0m : 9.49689
[1mStep[0m  [48/84], [94mLoss[0m : 9.05680
[1mStep[0m  [56/84], [94mLoss[0m : 8.57164
[1mStep[0m  [64/84], [94mLoss[0m : 8.73730
[1mStep[0m  [72/84], [94mLoss[0m : 8.42040
[1mStep[0m  [80/84], [94mLoss[0m : 8.51839

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.103, [92mTest[0m: 10.014, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.50889
[1mStep[0m  [8/84], [94mLoss[0m : 8.62974
[1mStep[0m  [16/84], [94mLoss[0m : 8.51060
[1mStep[0m  [24/84], [94mLoss[0m : 8.29094
[1mStep[0m  [32/84], [94mLoss[0m : 8.71303
[1mStep[0m  [40/84], [94mLoss[0m : 8.60751
[1mStep[0m  [48/84], [94mLoss[0m : 8.76763
[1mStep[0m  [56/84], [94mLoss[0m : 8.40998
[1mStep[0m  [64/84], [94mLoss[0m : 8.35470
[1mStep[0m  [72/84], [94mLoss[0m : 7.78036
[1mStep[0m  [80/84], [94mLoss[0m : 7.28112

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.334, [92mTest[0m: 9.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.95994
[1mStep[0m  [8/84], [94mLoss[0m : 7.40419
[1mStep[0m  [16/84], [94mLoss[0m : 7.39999
[1mStep[0m  [24/84], [94mLoss[0m : 8.01844
[1mStep[0m  [32/84], [94mLoss[0m : 7.52825
[1mStep[0m  [40/84], [94mLoss[0m : 8.18707
[1mStep[0m  [48/84], [94mLoss[0m : 8.00588
[1mStep[0m  [56/84], [94mLoss[0m : 8.08047
[1mStep[0m  [64/84], [94mLoss[0m : 7.63548
[1mStep[0m  [72/84], [94mLoss[0m : 7.37819
[1mStep[0m  [80/84], [94mLoss[0m : 7.32556

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.572, [92mTest[0m: 8.975, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.31962
[1mStep[0m  [8/84], [94mLoss[0m : 6.68083
[1mStep[0m  [16/84], [94mLoss[0m : 6.52149
[1mStep[0m  [24/84], [94mLoss[0m : 6.96194
[1mStep[0m  [32/84], [94mLoss[0m : 7.27155
[1mStep[0m  [40/84], [94mLoss[0m : 6.56374
[1mStep[0m  [48/84], [94mLoss[0m : 6.95910
[1mStep[0m  [56/84], [94mLoss[0m : 6.57287
[1mStep[0m  [64/84], [94mLoss[0m : 6.60171
[1mStep[0m  [72/84], [94mLoss[0m : 6.21929
[1mStep[0m  [80/84], [94mLoss[0m : 6.78119

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.781, [92mTest[0m: 8.460, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.58716
[1mStep[0m  [8/84], [94mLoss[0m : 6.39028
[1mStep[0m  [16/84], [94mLoss[0m : 6.27081
[1mStep[0m  [24/84], [94mLoss[0m : 6.45115
[1mStep[0m  [32/84], [94mLoss[0m : 6.35088
[1mStep[0m  [40/84], [94mLoss[0m : 5.64073
[1mStep[0m  [48/84], [94mLoss[0m : 5.65975
[1mStep[0m  [56/84], [94mLoss[0m : 5.63839
[1mStep[0m  [64/84], [94mLoss[0m : 5.42855
[1mStep[0m  [72/84], [94mLoss[0m : 5.94592
[1mStep[0m  [80/84], [94mLoss[0m : 5.83562

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.015, [92mTest[0m: 7.901, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.63752
[1mStep[0m  [8/84], [94mLoss[0m : 5.09717
[1mStep[0m  [16/84], [94mLoss[0m : 5.47260
[1mStep[0m  [24/84], [94mLoss[0m : 5.62127
[1mStep[0m  [32/84], [94mLoss[0m : 5.64693
[1mStep[0m  [40/84], [94mLoss[0m : 5.21701
[1mStep[0m  [48/84], [94mLoss[0m : 5.76273
[1mStep[0m  [56/84], [94mLoss[0m : 4.62031
[1mStep[0m  [64/84], [94mLoss[0m : 5.65417
[1mStep[0m  [72/84], [94mLoss[0m : 5.31886
[1mStep[0m  [80/84], [94mLoss[0m : 4.51110

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.277, [92mTest[0m: 7.225, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.60423
[1mStep[0m  [8/84], [94mLoss[0m : 4.90271
[1mStep[0m  [16/84], [94mLoss[0m : 4.99098
[1mStep[0m  [24/84], [94mLoss[0m : 5.01364
[1mStep[0m  [32/84], [94mLoss[0m : 4.59378
[1mStep[0m  [40/84], [94mLoss[0m : 4.53595
[1mStep[0m  [48/84], [94mLoss[0m : 4.35421
[1mStep[0m  [56/84], [94mLoss[0m : 4.59410
[1mStep[0m  [64/84], [94mLoss[0m : 4.39284
[1mStep[0m  [72/84], [94mLoss[0m : 4.48020
[1mStep[0m  [80/84], [94mLoss[0m : 4.83599

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.643, [92mTest[0m: 6.480, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 8 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.837
====================================

Phase 1 - Evaluation MAE:  5.837413430213928
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 4.93929
[1mStep[0m  [8/84], [94mLoss[0m : 4.28150
[1mStep[0m  [16/84], [94mLoss[0m : 4.33177
[1mStep[0m  [24/84], [94mLoss[0m : 3.64616
[1mStep[0m  [32/84], [94mLoss[0m : 4.48906
[1mStep[0m  [40/84], [94mLoss[0m : 4.03555
[1mStep[0m  [48/84], [94mLoss[0m : 4.57886
[1mStep[0m  [56/84], [94mLoss[0m : 4.44020
[1mStep[0m  [64/84], [94mLoss[0m : 4.61785
[1mStep[0m  [72/84], [94mLoss[0m : 4.25031
[1mStep[0m  [80/84], [94mLoss[0m : 4.12367

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.165, [92mTest[0m: 5.847, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.871
====================================

Phase 2 - Evaluation MAE:  3.8709391185215543
MAE score P1       5.837413
MAE score P2       3.870939
loss               4.164599
learning_rate        0.0001
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay         0.0001
Name: 22, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.18216
[1mStep[0m  [16/169], [94mLoss[0m : 10.39021
[1mStep[0m  [32/169], [94mLoss[0m : 10.81777
[1mStep[0m  [48/169], [94mLoss[0m : 10.75332
[1mStep[0m  [64/169], [94mLoss[0m : 10.12588
[1mStep[0m  [80/169], [94mLoss[0m : 10.65252
[1mStep[0m  [96/169], [94mLoss[0m : 11.20472
[1mStep[0m  [112/169], [94mLoss[0m : 11.20538
[1mStep[0m  [128/169], [94mLoss[0m : 10.80033
[1mStep[0m  [144/169], [94mLoss[0m : 11.18680
[1mStep[0m  [160/169], [94mLoss[0m : 11.00000

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.871, [92mTest[0m: 11.064, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 11.17668
[1mStep[0m  [16/169], [94mLoss[0m : 10.07241
[1mStep[0m  [32/169], [94mLoss[0m : 11.26663
[1mStep[0m  [48/169], [94mLoss[0m : 10.85671
[1mStep[0m  [64/169], [94mLoss[0m : 10.45202
[1mStep[0m  [80/169], [94mLoss[0m : 10.18171
[1mStep[0m  [96/169], [94mLoss[0m : 10.69989
[1mStep[0m  [112/169], [94mLoss[0m : 10.65077
[1mStep[0m  [128/169], [94mLoss[0m : 10.72336
[1mStep[0m  [144/169], [94mLoss[0m : 11.53509
[1mStep[0m  [160/169], [94mLoss[0m : 10.90866

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.712, [92mTest[0m: 10.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.63518
[1mStep[0m  [16/169], [94mLoss[0m : 10.34041
[1mStep[0m  [32/169], [94mLoss[0m : 10.59412
[1mStep[0m  [48/169], [94mLoss[0m : 10.26316
[1mStep[0m  [64/169], [94mLoss[0m : 10.12341
[1mStep[0m  [80/169], [94mLoss[0m : 11.11326
[1mStep[0m  [96/169], [94mLoss[0m : 11.16060
[1mStep[0m  [112/169], [94mLoss[0m : 10.37499
[1mStep[0m  [128/169], [94mLoss[0m : 11.01422
[1mStep[0m  [144/169], [94mLoss[0m : 10.01325
[1mStep[0m  [160/169], [94mLoss[0m : 10.07109

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.546, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.90819
[1mStep[0m  [16/169], [94mLoss[0m : 10.18709
[1mStep[0m  [32/169], [94mLoss[0m : 10.55340
[1mStep[0m  [48/169], [94mLoss[0m : 10.48697
[1mStep[0m  [64/169], [94mLoss[0m : 9.82761
[1mStep[0m  [80/169], [94mLoss[0m : 11.22192
[1mStep[0m  [96/169], [94mLoss[0m : 9.88282
[1mStep[0m  [112/169], [94mLoss[0m : 9.78317
[1mStep[0m  [128/169], [94mLoss[0m : 10.39900
[1mStep[0m  [144/169], [94mLoss[0m : 9.92542
[1mStep[0m  [160/169], [94mLoss[0m : 10.65865

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.381, [92mTest[0m: 10.588, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.55467
[1mStep[0m  [16/169], [94mLoss[0m : 10.60692
[1mStep[0m  [32/169], [94mLoss[0m : 10.73766
[1mStep[0m  [48/169], [94mLoss[0m : 9.95288
[1mStep[0m  [64/169], [94mLoss[0m : 9.74651
[1mStep[0m  [80/169], [94mLoss[0m : 11.10635
[1mStep[0m  [96/169], [94mLoss[0m : 10.23026
[1mStep[0m  [112/169], [94mLoss[0m : 10.73947
[1mStep[0m  [128/169], [94mLoss[0m : 9.29887
[1mStep[0m  [144/169], [94mLoss[0m : 10.12382
[1mStep[0m  [160/169], [94mLoss[0m : 9.61777

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.215, [92mTest[0m: 10.455, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.39669
[1mStep[0m  [16/169], [94mLoss[0m : 9.74609
[1mStep[0m  [32/169], [94mLoss[0m : 9.92478
[1mStep[0m  [48/169], [94mLoss[0m : 9.97969
[1mStep[0m  [64/169], [94mLoss[0m : 10.38994
[1mStep[0m  [80/169], [94mLoss[0m : 10.13129
[1mStep[0m  [96/169], [94mLoss[0m : 10.15133
[1mStep[0m  [112/169], [94mLoss[0m : 10.54550
[1mStep[0m  [128/169], [94mLoss[0m : 10.03078
[1mStep[0m  [144/169], [94mLoss[0m : 9.41262
[1mStep[0m  [160/169], [94mLoss[0m : 9.45066

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.053, [92mTest[0m: 10.349, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.76016
[1mStep[0m  [16/169], [94mLoss[0m : 9.85947
[1mStep[0m  [32/169], [94mLoss[0m : 10.05670
[1mStep[0m  [48/169], [94mLoss[0m : 9.35704
[1mStep[0m  [64/169], [94mLoss[0m : 9.11524
[1mStep[0m  [80/169], [94mLoss[0m : 9.56111
[1mStep[0m  [96/169], [94mLoss[0m : 10.45029
[1mStep[0m  [112/169], [94mLoss[0m : 10.66038
[1mStep[0m  [128/169], [94mLoss[0m : 10.13673
[1mStep[0m  [144/169], [94mLoss[0m : 9.91383
[1mStep[0m  [160/169], [94mLoss[0m : 9.26795

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.890, [92mTest[0m: 10.229, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.00915
[1mStep[0m  [16/169], [94mLoss[0m : 9.80842
[1mStep[0m  [32/169], [94mLoss[0m : 9.60036
[1mStep[0m  [48/169], [94mLoss[0m : 9.64599
[1mStep[0m  [64/169], [94mLoss[0m : 9.81312
[1mStep[0m  [80/169], [94mLoss[0m : 8.83533
[1mStep[0m  [96/169], [94mLoss[0m : 9.92795
[1mStep[0m  [112/169], [94mLoss[0m : 10.68839
[1mStep[0m  [128/169], [94mLoss[0m : 8.96004
[1mStep[0m  [144/169], [94mLoss[0m : 9.30482
[1mStep[0m  [160/169], [94mLoss[0m : 9.28740

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.719, [92mTest[0m: 10.070, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.89921
[1mStep[0m  [16/169], [94mLoss[0m : 8.93784
[1mStep[0m  [32/169], [94mLoss[0m : 8.92060
[1mStep[0m  [48/169], [94mLoss[0m : 9.95350
[1mStep[0m  [64/169], [94mLoss[0m : 10.19928
[1mStep[0m  [80/169], [94mLoss[0m : 9.77543
[1mStep[0m  [96/169], [94mLoss[0m : 9.16906
[1mStep[0m  [112/169], [94mLoss[0m : 9.33262
[1mStep[0m  [128/169], [94mLoss[0m : 9.62267
[1mStep[0m  [144/169], [94mLoss[0m : 8.57900
[1mStep[0m  [160/169], [94mLoss[0m : 10.15649

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.554, [92mTest[0m: 9.988, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.65597
[1mStep[0m  [16/169], [94mLoss[0m : 9.53723
[1mStep[0m  [32/169], [94mLoss[0m : 9.06723
[1mStep[0m  [48/169], [94mLoss[0m : 10.34277
[1mStep[0m  [64/169], [94mLoss[0m : 9.80240
[1mStep[0m  [80/169], [94mLoss[0m : 8.62504
[1mStep[0m  [96/169], [94mLoss[0m : 10.32428
[1mStep[0m  [112/169], [94mLoss[0m : 8.68095
[1mStep[0m  [128/169], [94mLoss[0m : 9.19069
[1mStep[0m  [144/169], [94mLoss[0m : 9.84205
[1mStep[0m  [160/169], [94mLoss[0m : 8.70008

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.384, [92mTest[0m: 9.824, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.27229
[1mStep[0m  [16/169], [94mLoss[0m : 9.11215
[1mStep[0m  [32/169], [94mLoss[0m : 9.10977
[1mStep[0m  [48/169], [94mLoss[0m : 9.27038
[1mStep[0m  [64/169], [94mLoss[0m : 9.48235
[1mStep[0m  [80/169], [94mLoss[0m : 9.13619
[1mStep[0m  [96/169], [94mLoss[0m : 9.50997
[1mStep[0m  [112/169], [94mLoss[0m : 10.02186
[1mStep[0m  [128/169], [94mLoss[0m : 9.28151
[1mStep[0m  [144/169], [94mLoss[0m : 9.61086
[1mStep[0m  [160/169], [94mLoss[0m : 10.41589

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.209, [92mTest[0m: 9.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.40003
[1mStep[0m  [16/169], [94mLoss[0m : 9.37231
[1mStep[0m  [32/169], [94mLoss[0m : 9.37080
[1mStep[0m  [48/169], [94mLoss[0m : 9.63638
[1mStep[0m  [64/169], [94mLoss[0m : 9.02587
[1mStep[0m  [80/169], [94mLoss[0m : 8.98893
[1mStep[0m  [96/169], [94mLoss[0m : 9.12138
[1mStep[0m  [112/169], [94mLoss[0m : 8.71531
[1mStep[0m  [128/169], [94mLoss[0m : 10.03703
[1mStep[0m  [144/169], [94mLoss[0m : 8.68462
[1mStep[0m  [160/169], [94mLoss[0m : 9.62842

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.043, [92mTest[0m: 9.579, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.86223
[1mStep[0m  [16/169], [94mLoss[0m : 8.09221
[1mStep[0m  [32/169], [94mLoss[0m : 8.76554
[1mStep[0m  [48/169], [94mLoss[0m : 8.37309
[1mStep[0m  [64/169], [94mLoss[0m : 10.05556
[1mStep[0m  [80/169], [94mLoss[0m : 9.06925
[1mStep[0m  [96/169], [94mLoss[0m : 8.98323
[1mStep[0m  [112/169], [94mLoss[0m : 8.76683
[1mStep[0m  [128/169], [94mLoss[0m : 8.58934
[1mStep[0m  [144/169], [94mLoss[0m : 8.08688
[1mStep[0m  [160/169], [94mLoss[0m : 8.61174

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.866, [92mTest[0m: 9.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.22927
[1mStep[0m  [16/169], [94mLoss[0m : 9.88101
[1mStep[0m  [32/169], [94mLoss[0m : 9.03630
[1mStep[0m  [48/169], [94mLoss[0m : 8.84686
[1mStep[0m  [64/169], [94mLoss[0m : 8.41878
[1mStep[0m  [80/169], [94mLoss[0m : 8.23719
[1mStep[0m  [96/169], [94mLoss[0m : 8.26296
[1mStep[0m  [112/169], [94mLoss[0m : 8.88549
[1mStep[0m  [128/169], [94mLoss[0m : 9.73859
[1mStep[0m  [144/169], [94mLoss[0m : 8.34429
[1mStep[0m  [160/169], [94mLoss[0m : 8.56609

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.688, [92mTest[0m: 9.298, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.29685
[1mStep[0m  [16/169], [94mLoss[0m : 8.47422
[1mStep[0m  [32/169], [94mLoss[0m : 9.22127
[1mStep[0m  [48/169], [94mLoss[0m : 8.89446
[1mStep[0m  [64/169], [94mLoss[0m : 8.60693
[1mStep[0m  [80/169], [94mLoss[0m : 8.95683
[1mStep[0m  [96/169], [94mLoss[0m : 8.09447
[1mStep[0m  [112/169], [94mLoss[0m : 8.18907
[1mStep[0m  [128/169], [94mLoss[0m : 8.07383
[1mStep[0m  [144/169], [94mLoss[0m : 8.37429
[1mStep[0m  [160/169], [94mLoss[0m : 8.96989

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.523, [92mTest[0m: 9.170, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.32504
[1mStep[0m  [16/169], [94mLoss[0m : 8.45354
[1mStep[0m  [32/169], [94mLoss[0m : 7.85187
[1mStep[0m  [48/169], [94mLoss[0m : 8.16625
[1mStep[0m  [64/169], [94mLoss[0m : 8.04713
[1mStep[0m  [80/169], [94mLoss[0m : 8.88915
[1mStep[0m  [96/169], [94mLoss[0m : 8.02706
[1mStep[0m  [112/169], [94mLoss[0m : 8.35408
[1mStep[0m  [128/169], [94mLoss[0m : 7.99363
[1mStep[0m  [144/169], [94mLoss[0m : 8.45365
[1mStep[0m  [160/169], [94mLoss[0m : 7.92879

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.356, [92mTest[0m: 9.037, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.48703
[1mStep[0m  [16/169], [94mLoss[0m : 8.55052
[1mStep[0m  [32/169], [94mLoss[0m : 7.87439
[1mStep[0m  [48/169], [94mLoss[0m : 8.25439
[1mStep[0m  [64/169], [94mLoss[0m : 7.88143
[1mStep[0m  [80/169], [94mLoss[0m : 8.25737
[1mStep[0m  [96/169], [94mLoss[0m : 7.96700
[1mStep[0m  [112/169], [94mLoss[0m : 7.58777
[1mStep[0m  [128/169], [94mLoss[0m : 9.28942
[1mStep[0m  [144/169], [94mLoss[0m : 8.65750
[1mStep[0m  [160/169], [94mLoss[0m : 8.41819

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.174, [92mTest[0m: 8.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.55989
[1mStep[0m  [16/169], [94mLoss[0m : 8.60308
[1mStep[0m  [32/169], [94mLoss[0m : 8.13511
[1mStep[0m  [48/169], [94mLoss[0m : 7.78175
[1mStep[0m  [64/169], [94mLoss[0m : 7.21867
[1mStep[0m  [80/169], [94mLoss[0m : 7.48267
[1mStep[0m  [96/169], [94mLoss[0m : 7.70132
[1mStep[0m  [112/169], [94mLoss[0m : 8.30319
[1mStep[0m  [128/169], [94mLoss[0m : 7.72028
[1mStep[0m  [144/169], [94mLoss[0m : 8.36229
[1mStep[0m  [160/169], [94mLoss[0m : 7.93552

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.001, [92mTest[0m: 8.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.51824
[1mStep[0m  [16/169], [94mLoss[0m : 8.11886
[1mStep[0m  [32/169], [94mLoss[0m : 7.82492
[1mStep[0m  [48/169], [94mLoss[0m : 7.95212
[1mStep[0m  [64/169], [94mLoss[0m : 7.90918
[1mStep[0m  [80/169], [94mLoss[0m : 7.88102
[1mStep[0m  [96/169], [94mLoss[0m : 7.58505
[1mStep[0m  [112/169], [94mLoss[0m : 7.56030
[1mStep[0m  [128/169], [94mLoss[0m : 7.69105
[1mStep[0m  [144/169], [94mLoss[0m : 7.75647
[1mStep[0m  [160/169], [94mLoss[0m : 8.06533

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.826, [92mTest[0m: 8.609, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.73589
[1mStep[0m  [16/169], [94mLoss[0m : 7.77513
[1mStep[0m  [32/169], [94mLoss[0m : 6.86761
[1mStep[0m  [48/169], [94mLoss[0m : 8.92077
[1mStep[0m  [64/169], [94mLoss[0m : 8.21754
[1mStep[0m  [80/169], [94mLoss[0m : 7.82481
[1mStep[0m  [96/169], [94mLoss[0m : 6.59089
[1mStep[0m  [112/169], [94mLoss[0m : 7.14718
[1mStep[0m  [128/169], [94mLoss[0m : 8.15619
[1mStep[0m  [144/169], [94mLoss[0m : 8.38340
[1mStep[0m  [160/169], [94mLoss[0m : 8.26122

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.637, [92mTest[0m: 8.508, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.34560
[1mStep[0m  [16/169], [94mLoss[0m : 7.87759
[1mStep[0m  [32/169], [94mLoss[0m : 7.71751
[1mStep[0m  [48/169], [94mLoss[0m : 7.82686
[1mStep[0m  [64/169], [94mLoss[0m : 7.22325
[1mStep[0m  [80/169], [94mLoss[0m : 7.13789
[1mStep[0m  [96/169], [94mLoss[0m : 7.57263
[1mStep[0m  [112/169], [94mLoss[0m : 7.24308
[1mStep[0m  [128/169], [94mLoss[0m : 8.15015
[1mStep[0m  [144/169], [94mLoss[0m : 6.24030
[1mStep[0m  [160/169], [94mLoss[0m : 7.48833

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.472, [92mTest[0m: 8.338, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.20866
[1mStep[0m  [16/169], [94mLoss[0m : 7.71199
[1mStep[0m  [32/169], [94mLoss[0m : 8.07895
[1mStep[0m  [48/169], [94mLoss[0m : 7.57273
[1mStep[0m  [64/169], [94mLoss[0m : 7.38008
[1mStep[0m  [80/169], [94mLoss[0m : 7.74448
[1mStep[0m  [96/169], [94mLoss[0m : 6.45517
[1mStep[0m  [112/169], [94mLoss[0m : 6.48008
[1mStep[0m  [128/169], [94mLoss[0m : 8.19555
[1mStep[0m  [144/169], [94mLoss[0m : 7.40566
[1mStep[0m  [160/169], [94mLoss[0m : 7.41016

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.306, [92mTest[0m: 8.206, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.67411
[1mStep[0m  [16/169], [94mLoss[0m : 7.46114
[1mStep[0m  [32/169], [94mLoss[0m : 6.95864
[1mStep[0m  [48/169], [94mLoss[0m : 7.32638
[1mStep[0m  [64/169], [94mLoss[0m : 6.48670
[1mStep[0m  [80/169], [94mLoss[0m : 7.20811
[1mStep[0m  [96/169], [94mLoss[0m : 7.29544
[1mStep[0m  [112/169], [94mLoss[0m : 6.77457
[1mStep[0m  [128/169], [94mLoss[0m : 7.18406
[1mStep[0m  [144/169], [94mLoss[0m : 7.51644
[1mStep[0m  [160/169], [94mLoss[0m : 6.56643

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.149, [92mTest[0m: 8.060, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.76087
[1mStep[0m  [16/169], [94mLoss[0m : 7.28843
[1mStep[0m  [32/169], [94mLoss[0m : 7.13409
[1mStep[0m  [48/169], [94mLoss[0m : 6.96408
[1mStep[0m  [64/169], [94mLoss[0m : 6.90398
[1mStep[0m  [80/169], [94mLoss[0m : 7.42188
[1mStep[0m  [96/169], [94mLoss[0m : 7.26433
[1mStep[0m  [112/169], [94mLoss[0m : 7.92957
[1mStep[0m  [128/169], [94mLoss[0m : 7.02566
[1mStep[0m  [144/169], [94mLoss[0m : 7.89289
[1mStep[0m  [160/169], [94mLoss[0m : 7.20981

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.971, [92mTest[0m: 7.852, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.11275
[1mStep[0m  [16/169], [94mLoss[0m : 7.16542
[1mStep[0m  [32/169], [94mLoss[0m : 7.21588
[1mStep[0m  [48/169], [94mLoss[0m : 6.39992
[1mStep[0m  [64/169], [94mLoss[0m : 7.41291
[1mStep[0m  [80/169], [94mLoss[0m : 6.91074
[1mStep[0m  [96/169], [94mLoss[0m : 6.46478
[1mStep[0m  [112/169], [94mLoss[0m : 6.75688
[1mStep[0m  [128/169], [94mLoss[0m : 6.81390
[1mStep[0m  [144/169], [94mLoss[0m : 7.29411
[1mStep[0m  [160/169], [94mLoss[0m : 6.83864

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.821, [92mTest[0m: 7.782, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.27950
[1mStep[0m  [16/169], [94mLoss[0m : 5.91956
[1mStep[0m  [32/169], [94mLoss[0m : 6.73765
[1mStep[0m  [48/169], [94mLoss[0m : 6.99837
[1mStep[0m  [64/169], [94mLoss[0m : 7.00723
[1mStep[0m  [80/169], [94mLoss[0m : 7.38414
[1mStep[0m  [96/169], [94mLoss[0m : 6.72176
[1mStep[0m  [112/169], [94mLoss[0m : 6.68757
[1mStep[0m  [128/169], [94mLoss[0m : 7.14485
[1mStep[0m  [144/169], [94mLoss[0m : 7.16697
[1mStep[0m  [160/169], [94mLoss[0m : 6.59132

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.649, [92mTest[0m: 7.638, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.43469
[1mStep[0m  [16/169], [94mLoss[0m : 6.51255
[1mStep[0m  [32/169], [94mLoss[0m : 6.67325
[1mStep[0m  [48/169], [94mLoss[0m : 6.35269
[1mStep[0m  [64/169], [94mLoss[0m : 6.57584
[1mStep[0m  [80/169], [94mLoss[0m : 6.99843
[1mStep[0m  [96/169], [94mLoss[0m : 5.65034
[1mStep[0m  [112/169], [94mLoss[0m : 6.64133
[1mStep[0m  [128/169], [94mLoss[0m : 6.87239
[1mStep[0m  [144/169], [94mLoss[0m : 6.11430
[1mStep[0m  [160/169], [94mLoss[0m : 6.38279

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.484, [92mTest[0m: 7.445, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.89986
[1mStep[0m  [16/169], [94mLoss[0m : 6.19466
[1mStep[0m  [32/169], [94mLoss[0m : 6.21913
[1mStep[0m  [48/169], [94mLoss[0m : 6.37864
[1mStep[0m  [64/169], [94mLoss[0m : 6.03195
[1mStep[0m  [80/169], [94mLoss[0m : 6.13322
[1mStep[0m  [96/169], [94mLoss[0m : 5.94430
[1mStep[0m  [112/169], [94mLoss[0m : 6.67457
[1mStep[0m  [128/169], [94mLoss[0m : 5.61829
[1mStep[0m  [144/169], [94mLoss[0m : 6.33003
[1mStep[0m  [160/169], [94mLoss[0m : 6.30326

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.333, [92mTest[0m: 7.337, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.90713
[1mStep[0m  [16/169], [94mLoss[0m : 6.40820
[1mStep[0m  [32/169], [94mLoss[0m : 5.93286
[1mStep[0m  [48/169], [94mLoss[0m : 5.74931
[1mStep[0m  [64/169], [94mLoss[0m : 6.07728
[1mStep[0m  [80/169], [94mLoss[0m : 6.11948
[1mStep[0m  [96/169], [94mLoss[0m : 6.04092
[1mStep[0m  [112/169], [94mLoss[0m : 6.35421
[1mStep[0m  [128/169], [94mLoss[0m : 5.81125
[1mStep[0m  [144/169], [94mLoss[0m : 6.26577
[1mStep[0m  [160/169], [94mLoss[0m : 5.87791

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.163, [92mTest[0m: 7.134, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.21211
[1mStep[0m  [16/169], [94mLoss[0m : 5.86240
[1mStep[0m  [32/169], [94mLoss[0m : 5.40257
[1mStep[0m  [48/169], [94mLoss[0m : 5.80605
[1mStep[0m  [64/169], [94mLoss[0m : 6.35466
[1mStep[0m  [80/169], [94mLoss[0m : 6.48143
[1mStep[0m  [96/169], [94mLoss[0m : 6.31520
[1mStep[0m  [112/169], [94mLoss[0m : 5.45738
[1mStep[0m  [128/169], [94mLoss[0m : 5.82546
[1mStep[0m  [144/169], [94mLoss[0m : 5.76608
[1mStep[0m  [160/169], [94mLoss[0m : 5.84688

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.003, [92mTest[0m: 6.976, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.771
====================================

Phase 1 - Evaluation MAE:  6.771206089428493
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 5.60331
[1mStep[0m  [16/169], [94mLoss[0m : 5.59794
[1mStep[0m  [32/169], [94mLoss[0m : 6.20666
[1mStep[0m  [48/169], [94mLoss[0m : 5.78796
[1mStep[0m  [64/169], [94mLoss[0m : 6.05557
[1mStep[0m  [80/169], [94mLoss[0m : 5.82981
[1mStep[0m  [96/169], [94mLoss[0m : 6.09453
[1mStep[0m  [112/169], [94mLoss[0m : 6.17090
[1mStep[0m  [128/169], [94mLoss[0m : 5.31324
[1mStep[0m  [144/169], [94mLoss[0m : 5.71991
[1mStep[0m  [160/169], [94mLoss[0m : 5.78773

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.822, [92mTest[0m: 6.770, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.10505
[1mStep[0m  [16/169], [94mLoss[0m : 5.53471
[1mStep[0m  [32/169], [94mLoss[0m : 6.43498
[1mStep[0m  [48/169], [94mLoss[0m : 5.54539
[1mStep[0m  [64/169], [94mLoss[0m : 6.39110
[1mStep[0m  [80/169], [94mLoss[0m : 4.67632
[1mStep[0m  [96/169], [94mLoss[0m : 5.86169
[1mStep[0m  [112/169], [94mLoss[0m : 6.05877
[1mStep[0m  [128/169], [94mLoss[0m : 5.67763
[1mStep[0m  [144/169], [94mLoss[0m : 4.98628
[1mStep[0m  [160/169], [94mLoss[0m : 5.30147

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.619, [92mTest[0m: 6.609, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.85680
[1mStep[0m  [16/169], [94mLoss[0m : 5.66319
[1mStep[0m  [32/169], [94mLoss[0m : 5.16565
[1mStep[0m  [48/169], [94mLoss[0m : 5.14371
[1mStep[0m  [64/169], [94mLoss[0m : 5.63414
[1mStep[0m  [80/169], [94mLoss[0m : 5.90359
[1mStep[0m  [96/169], [94mLoss[0m : 4.46541
[1mStep[0m  [112/169], [94mLoss[0m : 5.23647
[1mStep[0m  [128/169], [94mLoss[0m : 6.22902
[1mStep[0m  [144/169], [94mLoss[0m : 4.94550
[1mStep[0m  [160/169], [94mLoss[0m : 5.72492

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.430, [92mTest[0m: 6.378, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.78879
[1mStep[0m  [16/169], [94mLoss[0m : 4.85922
[1mStep[0m  [32/169], [94mLoss[0m : 5.17176
[1mStep[0m  [48/169], [94mLoss[0m : 5.38882
[1mStep[0m  [64/169], [94mLoss[0m : 5.47815
[1mStep[0m  [80/169], [94mLoss[0m : 5.55691
[1mStep[0m  [96/169], [94mLoss[0m : 5.67730
[1mStep[0m  [112/169], [94mLoss[0m : 5.17705
[1mStep[0m  [128/169], [94mLoss[0m : 5.19424
[1mStep[0m  [144/169], [94mLoss[0m : 5.77210
[1mStep[0m  [160/169], [94mLoss[0m : 5.33528

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.254, [92mTest[0m: 6.074, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.68659
[1mStep[0m  [16/169], [94mLoss[0m : 4.56131
[1mStep[0m  [32/169], [94mLoss[0m : 5.25722
[1mStep[0m  [48/169], [94mLoss[0m : 5.09434
[1mStep[0m  [64/169], [94mLoss[0m : 5.15811
[1mStep[0m  [80/169], [94mLoss[0m : 5.02458
[1mStep[0m  [96/169], [94mLoss[0m : 4.57096
[1mStep[0m  [112/169], [94mLoss[0m : 4.57414
[1mStep[0m  [128/169], [94mLoss[0m : 5.29424
[1mStep[0m  [144/169], [94mLoss[0m : 5.85253
[1mStep[0m  [160/169], [94mLoss[0m : 4.60165

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.074, [92mTest[0m: 5.801, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.78334
[1mStep[0m  [16/169], [94mLoss[0m : 4.61211
[1mStep[0m  [32/169], [94mLoss[0m : 5.28675
[1mStep[0m  [48/169], [94mLoss[0m : 4.04421
[1mStep[0m  [64/169], [94mLoss[0m : 4.18302
[1mStep[0m  [80/169], [94mLoss[0m : 4.66411
[1mStep[0m  [96/169], [94mLoss[0m : 5.47876
[1mStep[0m  [112/169], [94mLoss[0m : 5.21817
[1mStep[0m  [128/169], [94mLoss[0m : 4.84613
[1mStep[0m  [144/169], [94mLoss[0m : 4.37456
[1mStep[0m  [160/169], [94mLoss[0m : 4.90488

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.912, [92mTest[0m: 5.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.01133
[1mStep[0m  [16/169], [94mLoss[0m : 5.15322
[1mStep[0m  [32/169], [94mLoss[0m : 5.42507
[1mStep[0m  [48/169], [94mLoss[0m : 4.78366
[1mStep[0m  [64/169], [94mLoss[0m : 3.88105
[1mStep[0m  [80/169], [94mLoss[0m : 4.23765
[1mStep[0m  [96/169], [94mLoss[0m : 4.70300
[1mStep[0m  [112/169], [94mLoss[0m : 4.41717
[1mStep[0m  [128/169], [94mLoss[0m : 5.05700
[1mStep[0m  [144/169], [94mLoss[0m : 4.33725
[1mStep[0m  [160/169], [94mLoss[0m : 4.26636

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.748, [92mTest[0m: 5.331, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.79807
[1mStep[0m  [16/169], [94mLoss[0m : 4.60210
[1mStep[0m  [32/169], [94mLoss[0m : 4.40409
[1mStep[0m  [48/169], [94mLoss[0m : 5.37305
[1mStep[0m  [64/169], [94mLoss[0m : 4.90419
[1mStep[0m  [80/169], [94mLoss[0m : 4.85717
[1mStep[0m  [96/169], [94mLoss[0m : 4.74323
[1mStep[0m  [112/169], [94mLoss[0m : 3.96176
[1mStep[0m  [128/169], [94mLoss[0m : 4.79388
[1mStep[0m  [144/169], [94mLoss[0m : 4.36061
[1mStep[0m  [160/169], [94mLoss[0m : 4.18276

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.580, [92mTest[0m: 5.114, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.32229
[1mStep[0m  [16/169], [94mLoss[0m : 3.76277
[1mStep[0m  [32/169], [94mLoss[0m : 4.10741
[1mStep[0m  [48/169], [94mLoss[0m : 3.99019
[1mStep[0m  [64/169], [94mLoss[0m : 4.04594
[1mStep[0m  [80/169], [94mLoss[0m : 4.67090
[1mStep[0m  [96/169], [94mLoss[0m : 4.65141
[1mStep[0m  [112/169], [94mLoss[0m : 4.18918
[1mStep[0m  [128/169], [94mLoss[0m : 4.39076
[1mStep[0m  [144/169], [94mLoss[0m : 4.62070
[1mStep[0m  [160/169], [94mLoss[0m : 4.10365

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.442, [92mTest[0m: 4.791, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.61788
[1mStep[0m  [16/169], [94mLoss[0m : 3.85046
[1mStep[0m  [32/169], [94mLoss[0m : 4.50367
[1mStep[0m  [48/169], [94mLoss[0m : 3.86061
[1mStep[0m  [64/169], [94mLoss[0m : 4.65761
[1mStep[0m  [80/169], [94mLoss[0m : 4.50249
[1mStep[0m  [96/169], [94mLoss[0m : 4.64467
[1mStep[0m  [112/169], [94mLoss[0m : 4.05629
[1mStep[0m  [128/169], [94mLoss[0m : 5.03775
[1mStep[0m  [144/169], [94mLoss[0m : 4.19697
[1mStep[0m  [160/169], [94mLoss[0m : 4.10601

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.308, [92mTest[0m: 4.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.92804
[1mStep[0m  [16/169], [94mLoss[0m : 4.65028
[1mStep[0m  [32/169], [94mLoss[0m : 4.35785
[1mStep[0m  [48/169], [94mLoss[0m : 3.83919
[1mStep[0m  [64/169], [94mLoss[0m : 3.62045
[1mStep[0m  [80/169], [94mLoss[0m : 4.33512
[1mStep[0m  [96/169], [94mLoss[0m : 4.07878
[1mStep[0m  [112/169], [94mLoss[0m : 4.15647
[1mStep[0m  [128/169], [94mLoss[0m : 3.58845
[1mStep[0m  [144/169], [94mLoss[0m : 4.99039
[1mStep[0m  [160/169], [94mLoss[0m : 4.36192

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.179, [92mTest[0m: 4.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.51039
[1mStep[0m  [16/169], [94mLoss[0m : 4.26470
[1mStep[0m  [32/169], [94mLoss[0m : 4.22395
[1mStep[0m  [48/169], [94mLoss[0m : 4.11693
[1mStep[0m  [64/169], [94mLoss[0m : 4.10609
[1mStep[0m  [80/169], [94mLoss[0m : 4.56153
[1mStep[0m  [96/169], [94mLoss[0m : 4.05698
[1mStep[0m  [112/169], [94mLoss[0m : 3.54593
[1mStep[0m  [128/169], [94mLoss[0m : 3.88420
[1mStep[0m  [144/169], [94mLoss[0m : 3.44469
[1mStep[0m  [160/169], [94mLoss[0m : 3.85229

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.050, [92mTest[0m: 4.071, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.17976
[1mStep[0m  [16/169], [94mLoss[0m : 3.78376
[1mStep[0m  [32/169], [94mLoss[0m : 3.42663
[1mStep[0m  [48/169], [94mLoss[0m : 3.62287
[1mStep[0m  [64/169], [94mLoss[0m : 3.08984
[1mStep[0m  [80/169], [94mLoss[0m : 4.11727
[1mStep[0m  [96/169], [94mLoss[0m : 4.03321
[1mStep[0m  [112/169], [94mLoss[0m : 3.72134
[1mStep[0m  [128/169], [94mLoss[0m : 3.71094
[1mStep[0m  [144/169], [94mLoss[0m : 3.63086
[1mStep[0m  [160/169], [94mLoss[0m : 3.51073

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.943, [92mTest[0m: 3.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.82300
[1mStep[0m  [16/169], [94mLoss[0m : 3.39972
[1mStep[0m  [32/169], [94mLoss[0m : 3.82114
[1mStep[0m  [48/169], [94mLoss[0m : 3.64742
[1mStep[0m  [64/169], [94mLoss[0m : 3.65679
[1mStep[0m  [80/169], [94mLoss[0m : 3.25463
[1mStep[0m  [96/169], [94mLoss[0m : 4.30869
[1mStep[0m  [112/169], [94mLoss[0m : 2.73076
[1mStep[0m  [128/169], [94mLoss[0m : 4.19548
[1mStep[0m  [144/169], [94mLoss[0m : 3.74558
[1mStep[0m  [160/169], [94mLoss[0m : 4.32973

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.806, [92mTest[0m: 3.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.37361
[1mStep[0m  [16/169], [94mLoss[0m : 3.64250
[1mStep[0m  [32/169], [94mLoss[0m : 3.49474
[1mStep[0m  [48/169], [94mLoss[0m : 3.28515
[1mStep[0m  [64/169], [94mLoss[0m : 3.63571
[1mStep[0m  [80/169], [94mLoss[0m : 3.49247
[1mStep[0m  [96/169], [94mLoss[0m : 4.00805
[1mStep[0m  [112/169], [94mLoss[0m : 3.61250
[1mStep[0m  [128/169], [94mLoss[0m : 3.62274
[1mStep[0m  [144/169], [94mLoss[0m : 3.67042
[1mStep[0m  [160/169], [94mLoss[0m : 2.91445

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.714, [92mTest[0m: 3.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.74509
[1mStep[0m  [16/169], [94mLoss[0m : 3.94786
[1mStep[0m  [32/169], [94mLoss[0m : 4.44327
[1mStep[0m  [48/169], [94mLoss[0m : 4.29877
[1mStep[0m  [64/169], [94mLoss[0m : 3.01768
[1mStep[0m  [80/169], [94mLoss[0m : 3.33992
[1mStep[0m  [96/169], [94mLoss[0m : 3.56287
[1mStep[0m  [112/169], [94mLoss[0m : 3.15103
[1mStep[0m  [128/169], [94mLoss[0m : 3.35578
[1mStep[0m  [144/169], [94mLoss[0m : 3.50749
[1mStep[0m  [160/169], [94mLoss[0m : 4.10737

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.623, [92mTest[0m: 3.282, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.56982
[1mStep[0m  [16/169], [94mLoss[0m : 3.62820
[1mStep[0m  [32/169], [94mLoss[0m : 3.54328
[1mStep[0m  [48/169], [94mLoss[0m : 4.01808
[1mStep[0m  [64/169], [94mLoss[0m : 3.52362
[1mStep[0m  [80/169], [94mLoss[0m : 2.65669
[1mStep[0m  [96/169], [94mLoss[0m : 3.18699
[1mStep[0m  [112/169], [94mLoss[0m : 2.88742
[1mStep[0m  [128/169], [94mLoss[0m : 3.53294
[1mStep[0m  [144/169], [94mLoss[0m : 3.91109
[1mStep[0m  [160/169], [94mLoss[0m : 3.32648

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.534, [92mTest[0m: 3.174, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.40657
[1mStep[0m  [16/169], [94mLoss[0m : 3.67348
[1mStep[0m  [32/169], [94mLoss[0m : 3.68709
[1mStep[0m  [48/169], [94mLoss[0m : 3.41384
[1mStep[0m  [64/169], [94mLoss[0m : 2.53830
[1mStep[0m  [80/169], [94mLoss[0m : 3.76733
[1mStep[0m  [96/169], [94mLoss[0m : 3.38277
[1mStep[0m  [112/169], [94mLoss[0m : 2.54470
[1mStep[0m  [128/169], [94mLoss[0m : 4.42314
[1mStep[0m  [144/169], [94mLoss[0m : 3.31302
[1mStep[0m  [160/169], [94mLoss[0m : 3.77371

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.455, [92mTest[0m: 2.977, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.83687
[1mStep[0m  [16/169], [94mLoss[0m : 3.63879
[1mStep[0m  [32/169], [94mLoss[0m : 3.65752
[1mStep[0m  [48/169], [94mLoss[0m : 3.49702
[1mStep[0m  [64/169], [94mLoss[0m : 3.30714
[1mStep[0m  [80/169], [94mLoss[0m : 3.06671
[1mStep[0m  [96/169], [94mLoss[0m : 3.93187
[1mStep[0m  [112/169], [94mLoss[0m : 3.17385
[1mStep[0m  [128/169], [94mLoss[0m : 3.42158
[1mStep[0m  [144/169], [94mLoss[0m : 3.65209
[1mStep[0m  [160/169], [94mLoss[0m : 3.53072

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.379, [92mTest[0m: 3.033, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.27668
[1mStep[0m  [16/169], [94mLoss[0m : 3.48893
[1mStep[0m  [32/169], [94mLoss[0m : 3.49584
[1mStep[0m  [48/169], [94mLoss[0m : 3.43400
[1mStep[0m  [64/169], [94mLoss[0m : 3.20445
[1mStep[0m  [80/169], [94mLoss[0m : 3.17163
[1mStep[0m  [96/169], [94mLoss[0m : 4.06063
[1mStep[0m  [112/169], [94mLoss[0m : 3.44585
[1mStep[0m  [128/169], [94mLoss[0m : 3.15394
[1mStep[0m  [144/169], [94mLoss[0m : 3.94117
[1mStep[0m  [160/169], [94mLoss[0m : 3.46004

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.308, [92mTest[0m: 2.866, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.35875
[1mStep[0m  [16/169], [94mLoss[0m : 3.19647
[1mStep[0m  [32/169], [94mLoss[0m : 3.24371
[1mStep[0m  [48/169], [94mLoss[0m : 3.36579
[1mStep[0m  [64/169], [94mLoss[0m : 3.56545
[1mStep[0m  [80/169], [94mLoss[0m : 3.46497
[1mStep[0m  [96/169], [94mLoss[0m : 3.36102
[1mStep[0m  [112/169], [94mLoss[0m : 2.70563
[1mStep[0m  [128/169], [94mLoss[0m : 2.45561
[1mStep[0m  [144/169], [94mLoss[0m : 4.19538
[1mStep[0m  [160/169], [94mLoss[0m : 3.88762

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.244, [92mTest[0m: 2.807, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80101
[1mStep[0m  [16/169], [94mLoss[0m : 2.95585
[1mStep[0m  [32/169], [94mLoss[0m : 3.64725
[1mStep[0m  [48/169], [94mLoss[0m : 2.94789
[1mStep[0m  [64/169], [94mLoss[0m : 2.89135
[1mStep[0m  [80/169], [94mLoss[0m : 3.53600
[1mStep[0m  [96/169], [94mLoss[0m : 3.07649
[1mStep[0m  [112/169], [94mLoss[0m : 3.15994
[1mStep[0m  [128/169], [94mLoss[0m : 2.97975
[1mStep[0m  [144/169], [94mLoss[0m : 3.24700
[1mStep[0m  [160/169], [94mLoss[0m : 2.94649

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.211, [92mTest[0m: 2.784, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71992
[1mStep[0m  [16/169], [94mLoss[0m : 3.21909
[1mStep[0m  [32/169], [94mLoss[0m : 3.75482
[1mStep[0m  [48/169], [94mLoss[0m : 2.98407
[1mStep[0m  [64/169], [94mLoss[0m : 3.31581
[1mStep[0m  [80/169], [94mLoss[0m : 3.80699
[1mStep[0m  [96/169], [94mLoss[0m : 3.64886
[1mStep[0m  [112/169], [94mLoss[0m : 3.00441
[1mStep[0m  [128/169], [94mLoss[0m : 3.62367
[1mStep[0m  [144/169], [94mLoss[0m : 3.77348
[1mStep[0m  [160/169], [94mLoss[0m : 3.30071

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.141, [92mTest[0m: 2.750, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.25543
[1mStep[0m  [16/169], [94mLoss[0m : 2.99374
[1mStep[0m  [32/169], [94mLoss[0m : 3.17100
[1mStep[0m  [48/169], [94mLoss[0m : 3.34491
[1mStep[0m  [64/169], [94mLoss[0m : 3.11314
[1mStep[0m  [80/169], [94mLoss[0m : 2.98197
[1mStep[0m  [96/169], [94mLoss[0m : 2.70739
[1mStep[0m  [112/169], [94mLoss[0m : 3.05838
[1mStep[0m  [128/169], [94mLoss[0m : 2.98720
[1mStep[0m  [144/169], [94mLoss[0m : 2.90210
[1mStep[0m  [160/169], [94mLoss[0m : 2.79309

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.114, [92mTest[0m: 2.672, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.02229
[1mStep[0m  [16/169], [94mLoss[0m : 2.93597
[1mStep[0m  [32/169], [94mLoss[0m : 2.99318
[1mStep[0m  [48/169], [94mLoss[0m : 3.48481
[1mStep[0m  [64/169], [94mLoss[0m : 2.76671
[1mStep[0m  [80/169], [94mLoss[0m : 2.67484
[1mStep[0m  [96/169], [94mLoss[0m : 3.09884
[1mStep[0m  [112/169], [94mLoss[0m : 3.28108
[1mStep[0m  [128/169], [94mLoss[0m : 3.46313
[1mStep[0m  [144/169], [94mLoss[0m : 2.86736
[1mStep[0m  [160/169], [94mLoss[0m : 3.42877

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.068, [92mTest[0m: 2.663, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.53668
[1mStep[0m  [16/169], [94mLoss[0m : 3.23005
[1mStep[0m  [32/169], [94mLoss[0m : 3.13150
[1mStep[0m  [48/169], [94mLoss[0m : 3.28893
[1mStep[0m  [64/169], [94mLoss[0m : 3.21572
[1mStep[0m  [80/169], [94mLoss[0m : 2.79337
[1mStep[0m  [96/169], [94mLoss[0m : 2.96318
[1mStep[0m  [112/169], [94mLoss[0m : 3.19992
[1mStep[0m  [128/169], [94mLoss[0m : 3.12078
[1mStep[0m  [144/169], [94mLoss[0m : 3.16114
[1mStep[0m  [160/169], [94mLoss[0m : 2.32798

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.044, [92mTest[0m: 2.648, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.12101
[1mStep[0m  [16/169], [94mLoss[0m : 2.74790
[1mStep[0m  [32/169], [94mLoss[0m : 3.02682
[1mStep[0m  [48/169], [94mLoss[0m : 3.22480
[1mStep[0m  [64/169], [94mLoss[0m : 2.77400
[1mStep[0m  [80/169], [94mLoss[0m : 2.80167
[1mStep[0m  [96/169], [94mLoss[0m : 3.13809
[1mStep[0m  [112/169], [94mLoss[0m : 3.56968
[1mStep[0m  [128/169], [94mLoss[0m : 2.82593
[1mStep[0m  [144/169], [94mLoss[0m : 3.16254
[1mStep[0m  [160/169], [94mLoss[0m : 3.03830

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.017, [92mTest[0m: 2.624, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88960
[1mStep[0m  [16/169], [94mLoss[0m : 2.50125
[1mStep[0m  [32/169], [94mLoss[0m : 3.63881
[1mStep[0m  [48/169], [94mLoss[0m : 2.67341
[1mStep[0m  [64/169], [94mLoss[0m : 3.57981
[1mStep[0m  [80/169], [94mLoss[0m : 3.09998
[1mStep[0m  [96/169], [94mLoss[0m : 3.12081
[1mStep[0m  [112/169], [94mLoss[0m : 2.64888
[1mStep[0m  [128/169], [94mLoss[0m : 3.73545
[1mStep[0m  [144/169], [94mLoss[0m : 2.77209
[1mStep[0m  [160/169], [94mLoss[0m : 3.52765

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.968, [92mTest[0m: 2.623, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.48345
[1mStep[0m  [16/169], [94mLoss[0m : 2.84256
[1mStep[0m  [32/169], [94mLoss[0m : 3.17565
[1mStep[0m  [48/169], [94mLoss[0m : 2.97979
[1mStep[0m  [64/169], [94mLoss[0m : 3.23132
[1mStep[0m  [80/169], [94mLoss[0m : 3.12392
[1mStep[0m  [96/169], [94mLoss[0m : 2.85564
[1mStep[0m  [112/169], [94mLoss[0m : 2.94310
[1mStep[0m  [128/169], [94mLoss[0m : 2.74539
[1mStep[0m  [144/169], [94mLoss[0m : 2.55610
[1mStep[0m  [160/169], [94mLoss[0m : 2.67908

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.928, [92mTest[0m: 2.613, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80839
[1mStep[0m  [16/169], [94mLoss[0m : 2.51910
[1mStep[0m  [32/169], [94mLoss[0m : 2.91436
[1mStep[0m  [48/169], [94mLoss[0m : 3.29033
[1mStep[0m  [64/169], [94mLoss[0m : 2.81575
[1mStep[0m  [80/169], [94mLoss[0m : 2.39911
[1mStep[0m  [96/169], [94mLoss[0m : 2.60053
[1mStep[0m  [112/169], [94mLoss[0m : 2.99651
[1mStep[0m  [128/169], [94mLoss[0m : 3.35443
[1mStep[0m  [144/169], [94mLoss[0m : 3.25043
[1mStep[0m  [160/169], [94mLoss[0m : 2.75344

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.912, [92mTest[0m: 2.571, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.573
====================================

Phase 2 - Evaluation MAE:  2.572642113481249
MAE score P1       6.771206
MAE score P2       2.572642
loss               2.911937
learning_rate        0.0001
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 23, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.49292
[1mStep[0m  [16/169], [94mLoss[0m : 10.89326
[1mStep[0m  [32/169], [94mLoss[0m : 10.60865
[1mStep[0m  [48/169], [94mLoss[0m : 10.47865
[1mStep[0m  [64/169], [94mLoss[0m : 10.65191
[1mStep[0m  [80/169], [94mLoss[0m : 10.95426
[1mStep[0m  [96/169], [94mLoss[0m : 9.90764
[1mStep[0m  [112/169], [94mLoss[0m : 9.78739
[1mStep[0m  [128/169], [94mLoss[0m : 9.21632
[1mStep[0m  [144/169], [94mLoss[0m : 8.94322
[1mStep[0m  [160/169], [94mLoss[0m : 9.05943

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.087, [92mTest[0m: 11.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.56443
[1mStep[0m  [16/169], [94mLoss[0m : 9.23087
[1mStep[0m  [32/169], [94mLoss[0m : 8.29050
[1mStep[0m  [48/169], [94mLoss[0m : 8.52854
[1mStep[0m  [64/169], [94mLoss[0m : 7.85298
[1mStep[0m  [80/169], [94mLoss[0m : 7.14850
[1mStep[0m  [96/169], [94mLoss[0m : 7.94130
[1mStep[0m  [112/169], [94mLoss[0m : 7.88402
[1mStep[0m  [128/169], [94mLoss[0m : 7.10581
[1mStep[0m  [144/169], [94mLoss[0m : 7.14582
[1mStep[0m  [160/169], [94mLoss[0m : 7.10930

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.788, [92mTest[0m: 8.934, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.40808
[1mStep[0m  [16/169], [94mLoss[0m : 5.82221
[1mStep[0m  [32/169], [94mLoss[0m : 6.19771
[1mStep[0m  [48/169], [94mLoss[0m : 5.52235
[1mStep[0m  [64/169], [94mLoss[0m : 5.62663
[1mStep[0m  [80/169], [94mLoss[0m : 5.39154
[1mStep[0m  [96/169], [94mLoss[0m : 5.55297
[1mStep[0m  [112/169], [94mLoss[0m : 5.17562
[1mStep[0m  [128/169], [94mLoss[0m : 5.96741
[1mStep[0m  [144/169], [94mLoss[0m : 5.13657
[1mStep[0m  [160/169], [94mLoss[0m : 5.27687

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.571, [92mTest[0m: 6.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.86984
[1mStep[0m  [16/169], [94mLoss[0m : 4.88882
[1mStep[0m  [32/169], [94mLoss[0m : 4.24615
[1mStep[0m  [48/169], [94mLoss[0m : 3.79615
[1mStep[0m  [64/169], [94mLoss[0m : 4.16323
[1mStep[0m  [80/169], [94mLoss[0m : 4.13297
[1mStep[0m  [96/169], [94mLoss[0m : 4.01995
[1mStep[0m  [112/169], [94mLoss[0m : 3.92977
[1mStep[0m  [128/169], [94mLoss[0m : 3.76189
[1mStep[0m  [144/169], [94mLoss[0m : 3.63905
[1mStep[0m  [160/169], [94mLoss[0m : 4.31069

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.002, [92mTest[0m: 4.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.83812
[1mStep[0m  [16/169], [94mLoss[0m : 3.24800
[1mStep[0m  [32/169], [94mLoss[0m : 3.46313
[1mStep[0m  [48/169], [94mLoss[0m : 3.65395
[1mStep[0m  [64/169], [94mLoss[0m : 3.47404
[1mStep[0m  [80/169], [94mLoss[0m : 3.58317
[1mStep[0m  [96/169], [94mLoss[0m : 3.03107
[1mStep[0m  [112/169], [94mLoss[0m : 2.91755
[1mStep[0m  [128/169], [94mLoss[0m : 2.83625
[1mStep[0m  [144/169], [94mLoss[0m : 2.81680
[1mStep[0m  [160/169], [94mLoss[0m : 3.29729

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.267, [92mTest[0m: 3.380, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.01966
[1mStep[0m  [16/169], [94mLoss[0m : 3.43130
[1mStep[0m  [32/169], [94mLoss[0m : 3.61834
[1mStep[0m  [48/169], [94mLoss[0m : 2.85069
[1mStep[0m  [64/169], [94mLoss[0m : 2.85111
[1mStep[0m  [80/169], [94mLoss[0m : 3.06619
[1mStep[0m  [96/169], [94mLoss[0m : 2.84972
[1mStep[0m  [112/169], [94mLoss[0m : 3.04820
[1mStep[0m  [128/169], [94mLoss[0m : 2.90914
[1mStep[0m  [144/169], [94mLoss[0m : 2.74077
[1mStep[0m  [160/169], [94mLoss[0m : 2.93912

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.956, [92mTest[0m: 2.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.15677
[1mStep[0m  [16/169], [94mLoss[0m : 3.40093
[1mStep[0m  [32/169], [94mLoss[0m : 2.86725
[1mStep[0m  [48/169], [94mLoss[0m : 2.98074
[1mStep[0m  [64/169], [94mLoss[0m : 3.12714
[1mStep[0m  [80/169], [94mLoss[0m : 2.73090
[1mStep[0m  [96/169], [94mLoss[0m : 2.78347
[1mStep[0m  [112/169], [94mLoss[0m : 2.80365
[1mStep[0m  [128/169], [94mLoss[0m : 2.68933
[1mStep[0m  [144/169], [94mLoss[0m : 3.08118
[1mStep[0m  [160/169], [94mLoss[0m : 2.87453

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.808, [92mTest[0m: 2.596, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72576
[1mStep[0m  [16/169], [94mLoss[0m : 3.04621
[1mStep[0m  [32/169], [94mLoss[0m : 2.59047
[1mStep[0m  [48/169], [94mLoss[0m : 2.51296
[1mStep[0m  [64/169], [94mLoss[0m : 2.50370
[1mStep[0m  [80/169], [94mLoss[0m : 2.31727
[1mStep[0m  [96/169], [94mLoss[0m : 2.40851
[1mStep[0m  [112/169], [94mLoss[0m : 2.58083
[1mStep[0m  [128/169], [94mLoss[0m : 2.90507
[1mStep[0m  [144/169], [94mLoss[0m : 2.89485
[1mStep[0m  [160/169], [94mLoss[0m : 2.78467

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.492, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36424
[1mStep[0m  [16/169], [94mLoss[0m : 2.89436
[1mStep[0m  [32/169], [94mLoss[0m : 3.16446
[1mStep[0m  [48/169], [94mLoss[0m : 3.13202
[1mStep[0m  [64/169], [94mLoss[0m : 2.97250
[1mStep[0m  [80/169], [94mLoss[0m : 2.36446
[1mStep[0m  [96/169], [94mLoss[0m : 2.46544
[1mStep[0m  [112/169], [94mLoss[0m : 2.69396
[1mStep[0m  [128/169], [94mLoss[0m : 2.34337
[1mStep[0m  [144/169], [94mLoss[0m : 3.03157
[1mStep[0m  [160/169], [94mLoss[0m : 2.93809

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.457, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74143
[1mStep[0m  [16/169], [94mLoss[0m : 2.77698
[1mStep[0m  [32/169], [94mLoss[0m : 2.74945
[1mStep[0m  [48/169], [94mLoss[0m : 2.50840
[1mStep[0m  [64/169], [94mLoss[0m : 2.20499
[1mStep[0m  [80/169], [94mLoss[0m : 2.57067
[1mStep[0m  [96/169], [94mLoss[0m : 2.56954
[1mStep[0m  [112/169], [94mLoss[0m : 2.64741
[1mStep[0m  [128/169], [94mLoss[0m : 2.37532
[1mStep[0m  [144/169], [94mLoss[0m : 2.39184
[1mStep[0m  [160/169], [94mLoss[0m : 2.86114

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80887
[1mStep[0m  [16/169], [94mLoss[0m : 2.51939
[1mStep[0m  [32/169], [94mLoss[0m : 2.83822
[1mStep[0m  [48/169], [94mLoss[0m : 2.35530
[1mStep[0m  [64/169], [94mLoss[0m : 2.67816
[1mStep[0m  [80/169], [94mLoss[0m : 2.51296
[1mStep[0m  [96/169], [94mLoss[0m : 2.50175
[1mStep[0m  [112/169], [94mLoss[0m : 2.60985
[1mStep[0m  [128/169], [94mLoss[0m : 2.54326
[1mStep[0m  [144/169], [94mLoss[0m : 2.80450
[1mStep[0m  [160/169], [94mLoss[0m : 2.15970

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.435, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23303
[1mStep[0m  [16/169], [94mLoss[0m : 2.51110
[1mStep[0m  [32/169], [94mLoss[0m : 2.76537
[1mStep[0m  [48/169], [94mLoss[0m : 2.37751
[1mStep[0m  [64/169], [94mLoss[0m : 2.62449
[1mStep[0m  [80/169], [94mLoss[0m : 2.54826
[1mStep[0m  [96/169], [94mLoss[0m : 2.85135
[1mStep[0m  [112/169], [94mLoss[0m : 2.67275
[1mStep[0m  [128/169], [94mLoss[0m : 2.94505
[1mStep[0m  [144/169], [94mLoss[0m : 2.70160
[1mStep[0m  [160/169], [94mLoss[0m : 2.69085

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.695, [92mTest[0m: 2.434, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94370
[1mStep[0m  [16/169], [94mLoss[0m : 2.69457
[1mStep[0m  [32/169], [94mLoss[0m : 2.52921
[1mStep[0m  [48/169], [94mLoss[0m : 3.25348
[1mStep[0m  [64/169], [94mLoss[0m : 2.98759
[1mStep[0m  [80/169], [94mLoss[0m : 2.94689
[1mStep[0m  [96/169], [94mLoss[0m : 2.65104
[1mStep[0m  [112/169], [94mLoss[0m : 2.57020
[1mStep[0m  [128/169], [94mLoss[0m : 2.76130
[1mStep[0m  [144/169], [94mLoss[0m : 2.72833
[1mStep[0m  [160/169], [94mLoss[0m : 2.99707

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84086
[1mStep[0m  [16/169], [94mLoss[0m : 2.53981
[1mStep[0m  [32/169], [94mLoss[0m : 2.65322
[1mStep[0m  [48/169], [94mLoss[0m : 2.54073
[1mStep[0m  [64/169], [94mLoss[0m : 2.69964
[1mStep[0m  [80/169], [94mLoss[0m : 2.66987
[1mStep[0m  [96/169], [94mLoss[0m : 2.60963
[1mStep[0m  [112/169], [94mLoss[0m : 2.86671
[1mStep[0m  [128/169], [94mLoss[0m : 2.99946
[1mStep[0m  [144/169], [94mLoss[0m : 2.45074
[1mStep[0m  [160/169], [94mLoss[0m : 2.42428

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.420, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37896
[1mStep[0m  [16/169], [94mLoss[0m : 2.55997
[1mStep[0m  [32/169], [94mLoss[0m : 2.59642
[1mStep[0m  [48/169], [94mLoss[0m : 2.51272
[1mStep[0m  [64/169], [94mLoss[0m : 2.48311
[1mStep[0m  [80/169], [94mLoss[0m : 2.61934
[1mStep[0m  [96/169], [94mLoss[0m : 3.27470
[1mStep[0m  [112/169], [94mLoss[0m : 2.89787
[1mStep[0m  [128/169], [94mLoss[0m : 2.81602
[1mStep[0m  [144/169], [94mLoss[0m : 2.38329
[1mStep[0m  [160/169], [94mLoss[0m : 2.57045

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.412, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66118
[1mStep[0m  [16/169], [94mLoss[0m : 2.88970
[1mStep[0m  [32/169], [94mLoss[0m : 2.70904
[1mStep[0m  [48/169], [94mLoss[0m : 2.84686
[1mStep[0m  [64/169], [94mLoss[0m : 3.16948
[1mStep[0m  [80/169], [94mLoss[0m : 2.66847
[1mStep[0m  [96/169], [94mLoss[0m : 2.74552
[1mStep[0m  [112/169], [94mLoss[0m : 2.64310
[1mStep[0m  [128/169], [94mLoss[0m : 2.97167
[1mStep[0m  [144/169], [94mLoss[0m : 2.71380
[1mStep[0m  [160/169], [94mLoss[0m : 2.92109

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.399, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48076
[1mStep[0m  [16/169], [94mLoss[0m : 2.84194
[1mStep[0m  [32/169], [94mLoss[0m : 2.69824
[1mStep[0m  [48/169], [94mLoss[0m : 2.83667
[1mStep[0m  [64/169], [94mLoss[0m : 3.16754
[1mStep[0m  [80/169], [94mLoss[0m : 3.16418
[1mStep[0m  [96/169], [94mLoss[0m : 2.36147
[1mStep[0m  [112/169], [94mLoss[0m : 3.03361
[1mStep[0m  [128/169], [94mLoss[0m : 2.78299
[1mStep[0m  [144/169], [94mLoss[0m : 2.35784
[1mStep[0m  [160/169], [94mLoss[0m : 2.83210

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03152
[1mStep[0m  [16/169], [94mLoss[0m : 2.75003
[1mStep[0m  [32/169], [94mLoss[0m : 2.54507
[1mStep[0m  [48/169], [94mLoss[0m : 3.04024
[1mStep[0m  [64/169], [94mLoss[0m : 2.83769
[1mStep[0m  [80/169], [94mLoss[0m : 2.55487
[1mStep[0m  [96/169], [94mLoss[0m : 2.15276
[1mStep[0m  [112/169], [94mLoss[0m : 2.99370
[1mStep[0m  [128/169], [94mLoss[0m : 2.35984
[1mStep[0m  [144/169], [94mLoss[0m : 2.47582
[1mStep[0m  [160/169], [94mLoss[0m : 2.11042

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.401, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67259
[1mStep[0m  [16/169], [94mLoss[0m : 2.64745
[1mStep[0m  [32/169], [94mLoss[0m : 2.69521
[1mStep[0m  [48/169], [94mLoss[0m : 2.65838
[1mStep[0m  [64/169], [94mLoss[0m : 2.96244
[1mStep[0m  [80/169], [94mLoss[0m : 2.87190
[1mStep[0m  [96/169], [94mLoss[0m : 2.69648
[1mStep[0m  [112/169], [94mLoss[0m : 2.82707
[1mStep[0m  [128/169], [94mLoss[0m : 2.58974
[1mStep[0m  [144/169], [94mLoss[0m : 2.60616
[1mStep[0m  [160/169], [94mLoss[0m : 3.04009

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.391, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90680
[1mStep[0m  [16/169], [94mLoss[0m : 2.65788
[1mStep[0m  [32/169], [94mLoss[0m : 2.83802
[1mStep[0m  [48/169], [94mLoss[0m : 2.36745
[1mStep[0m  [64/169], [94mLoss[0m : 2.37675
[1mStep[0m  [80/169], [94mLoss[0m : 2.25454
[1mStep[0m  [96/169], [94mLoss[0m : 2.74141
[1mStep[0m  [112/169], [94mLoss[0m : 2.53164
[1mStep[0m  [128/169], [94mLoss[0m : 2.24817
[1mStep[0m  [144/169], [94mLoss[0m : 2.81588
[1mStep[0m  [160/169], [94mLoss[0m : 2.89377

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.394, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90539
[1mStep[0m  [16/169], [94mLoss[0m : 2.73044
[1mStep[0m  [32/169], [94mLoss[0m : 2.83968
[1mStep[0m  [48/169], [94mLoss[0m : 2.84387
[1mStep[0m  [64/169], [94mLoss[0m : 3.11861
[1mStep[0m  [80/169], [94mLoss[0m : 2.36504
[1mStep[0m  [96/169], [94mLoss[0m : 2.62751
[1mStep[0m  [112/169], [94mLoss[0m : 2.88107
[1mStep[0m  [128/169], [94mLoss[0m : 2.74404
[1mStep[0m  [144/169], [94mLoss[0m : 2.60852
[1mStep[0m  [160/169], [94mLoss[0m : 2.59286

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.396, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46357
[1mStep[0m  [16/169], [94mLoss[0m : 2.37100
[1mStep[0m  [32/169], [94mLoss[0m : 2.69757
[1mStep[0m  [48/169], [94mLoss[0m : 3.32049
[1mStep[0m  [64/169], [94mLoss[0m : 2.51564
[1mStep[0m  [80/169], [94mLoss[0m : 2.49598
[1mStep[0m  [96/169], [94mLoss[0m : 2.69843
[1mStep[0m  [112/169], [94mLoss[0m : 3.07076
[1mStep[0m  [128/169], [94mLoss[0m : 3.27035
[1mStep[0m  [144/169], [94mLoss[0m : 2.84024
[1mStep[0m  [160/169], [94mLoss[0m : 2.35129

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.388, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.17878
[1mStep[0m  [16/169], [94mLoss[0m : 2.43152
[1mStep[0m  [32/169], [94mLoss[0m : 3.04386
[1mStep[0m  [48/169], [94mLoss[0m : 3.06823
[1mStep[0m  [64/169], [94mLoss[0m : 3.05220
[1mStep[0m  [80/169], [94mLoss[0m : 2.73511
[1mStep[0m  [96/169], [94mLoss[0m : 2.60047
[1mStep[0m  [112/169], [94mLoss[0m : 2.72068
[1mStep[0m  [128/169], [94mLoss[0m : 2.84764
[1mStep[0m  [144/169], [94mLoss[0m : 2.48568
[1mStep[0m  [160/169], [94mLoss[0m : 2.67376

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.387, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90813
[1mStep[0m  [16/169], [94mLoss[0m : 2.54745
[1mStep[0m  [32/169], [94mLoss[0m : 2.24893
[1mStep[0m  [48/169], [94mLoss[0m : 2.85906
[1mStep[0m  [64/169], [94mLoss[0m : 2.58346
[1mStep[0m  [80/169], [94mLoss[0m : 2.85194
[1mStep[0m  [96/169], [94mLoss[0m : 2.36949
[1mStep[0m  [112/169], [94mLoss[0m : 2.23043
[1mStep[0m  [128/169], [94mLoss[0m : 2.52513
[1mStep[0m  [144/169], [94mLoss[0m : 2.59925
[1mStep[0m  [160/169], [94mLoss[0m : 2.95823

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.385, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50601
[1mStep[0m  [16/169], [94mLoss[0m : 2.59891
[1mStep[0m  [32/169], [94mLoss[0m : 3.04947
[1mStep[0m  [48/169], [94mLoss[0m : 2.81856
[1mStep[0m  [64/169], [94mLoss[0m : 2.11980
[1mStep[0m  [80/169], [94mLoss[0m : 2.79170
[1mStep[0m  [96/169], [94mLoss[0m : 2.84438
[1mStep[0m  [112/169], [94mLoss[0m : 2.48719
[1mStep[0m  [128/169], [94mLoss[0m : 2.60556
[1mStep[0m  [144/169], [94mLoss[0m : 2.25320
[1mStep[0m  [160/169], [94mLoss[0m : 2.80916

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.377, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67215
[1mStep[0m  [16/169], [94mLoss[0m : 2.37074
[1mStep[0m  [32/169], [94mLoss[0m : 3.00941
[1mStep[0m  [48/169], [94mLoss[0m : 2.84761
[1mStep[0m  [64/169], [94mLoss[0m : 2.43051
[1mStep[0m  [80/169], [94mLoss[0m : 2.74004
[1mStep[0m  [96/169], [94mLoss[0m : 3.32923
[1mStep[0m  [112/169], [94mLoss[0m : 3.20136
[1mStep[0m  [128/169], [94mLoss[0m : 2.81807
[1mStep[0m  [144/169], [94mLoss[0m : 3.07256
[1mStep[0m  [160/169], [94mLoss[0m : 2.76108

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.377, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27143
[1mStep[0m  [16/169], [94mLoss[0m : 2.46485
[1mStep[0m  [32/169], [94mLoss[0m : 2.55783
[1mStep[0m  [48/169], [94mLoss[0m : 2.80346
[1mStep[0m  [64/169], [94mLoss[0m : 2.48862
[1mStep[0m  [80/169], [94mLoss[0m : 2.89887
[1mStep[0m  [96/169], [94mLoss[0m : 2.82283
[1mStep[0m  [112/169], [94mLoss[0m : 2.57810
[1mStep[0m  [128/169], [94mLoss[0m : 2.74379
[1mStep[0m  [144/169], [94mLoss[0m : 2.54753
[1mStep[0m  [160/169], [94mLoss[0m : 2.61157

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.372, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46285
[1mStep[0m  [16/169], [94mLoss[0m : 2.21433
[1mStep[0m  [32/169], [94mLoss[0m : 3.17480
[1mStep[0m  [48/169], [94mLoss[0m : 2.83032
[1mStep[0m  [64/169], [94mLoss[0m : 3.10282
[1mStep[0m  [80/169], [94mLoss[0m : 2.46294
[1mStep[0m  [96/169], [94mLoss[0m : 2.57324
[1mStep[0m  [112/169], [94mLoss[0m : 3.22065
[1mStep[0m  [128/169], [94mLoss[0m : 2.85235
[1mStep[0m  [144/169], [94mLoss[0m : 2.58446
[1mStep[0m  [160/169], [94mLoss[0m : 2.80252

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.370, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91419
[1mStep[0m  [16/169], [94mLoss[0m : 2.58298
[1mStep[0m  [32/169], [94mLoss[0m : 2.60936
[1mStep[0m  [48/169], [94mLoss[0m : 2.62799
[1mStep[0m  [64/169], [94mLoss[0m : 2.73639
[1mStep[0m  [80/169], [94mLoss[0m : 2.39182
[1mStep[0m  [96/169], [94mLoss[0m : 3.23434
[1mStep[0m  [112/169], [94mLoss[0m : 2.38843
[1mStep[0m  [128/169], [94mLoss[0m : 2.11836
[1mStep[0m  [144/169], [94mLoss[0m : 3.07844
[1mStep[0m  [160/169], [94mLoss[0m : 2.33635

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53026
[1mStep[0m  [16/169], [94mLoss[0m : 2.49134
[1mStep[0m  [32/169], [94mLoss[0m : 3.08719
[1mStep[0m  [48/169], [94mLoss[0m : 2.58086
[1mStep[0m  [64/169], [94mLoss[0m : 2.27258
[1mStep[0m  [80/169], [94mLoss[0m : 2.20019
[1mStep[0m  [96/169], [94mLoss[0m : 3.02341
[1mStep[0m  [112/169], [94mLoss[0m : 2.78981
[1mStep[0m  [128/169], [94mLoss[0m : 2.36167
[1mStep[0m  [144/169], [94mLoss[0m : 2.75373
[1mStep[0m  [160/169], [94mLoss[0m : 2.52792

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.374, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.384
====================================

Phase 1 - Evaluation MAE:  2.3844156669718877
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.95806
[1mStep[0m  [16/169], [94mLoss[0m : 2.74153
[1mStep[0m  [32/169], [94mLoss[0m : 2.30933
[1mStep[0m  [48/169], [94mLoss[0m : 3.14841
[1mStep[0m  [64/169], [94mLoss[0m : 2.28603
[1mStep[0m  [80/169], [94mLoss[0m : 2.77868
[1mStep[0m  [96/169], [94mLoss[0m : 2.73455
[1mStep[0m  [112/169], [94mLoss[0m : 2.95153
[1mStep[0m  [128/169], [94mLoss[0m : 2.80421
[1mStep[0m  [144/169], [94mLoss[0m : 2.67519
[1mStep[0m  [160/169], [94mLoss[0m : 2.73483

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.380, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36930
[1mStep[0m  [16/169], [94mLoss[0m : 2.75539
[1mStep[0m  [32/169], [94mLoss[0m : 3.10333
[1mStep[0m  [48/169], [94mLoss[0m : 2.70059
[1mStep[0m  [64/169], [94mLoss[0m : 2.48947
[1mStep[0m  [80/169], [94mLoss[0m : 2.47145
[1mStep[0m  [96/169], [94mLoss[0m : 2.94016
[1mStep[0m  [112/169], [94mLoss[0m : 2.79692
[1mStep[0m  [128/169], [94mLoss[0m : 2.49288
[1mStep[0m  [144/169], [94mLoss[0m : 2.59263
[1mStep[0m  [160/169], [94mLoss[0m : 2.86605

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68345
[1mStep[0m  [16/169], [94mLoss[0m : 2.67860
[1mStep[0m  [32/169], [94mLoss[0m : 2.35909
[1mStep[0m  [48/169], [94mLoss[0m : 2.86579
[1mStep[0m  [64/169], [94mLoss[0m : 2.88184
[1mStep[0m  [80/169], [94mLoss[0m : 2.67010
[1mStep[0m  [96/169], [94mLoss[0m : 2.50375
[1mStep[0m  [112/169], [94mLoss[0m : 2.47360
[1mStep[0m  [128/169], [94mLoss[0m : 2.50786
[1mStep[0m  [144/169], [94mLoss[0m : 2.47932
[1mStep[0m  [160/169], [94mLoss[0m : 2.57861

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81780
[1mStep[0m  [16/169], [94mLoss[0m : 2.95396
[1mStep[0m  [32/169], [94mLoss[0m : 3.01171
[1mStep[0m  [48/169], [94mLoss[0m : 2.78416
[1mStep[0m  [64/169], [94mLoss[0m : 2.86279
[1mStep[0m  [80/169], [94mLoss[0m : 2.64995
[1mStep[0m  [96/169], [94mLoss[0m : 2.06307
[1mStep[0m  [112/169], [94mLoss[0m : 2.23092
[1mStep[0m  [128/169], [94mLoss[0m : 2.50536
[1mStep[0m  [144/169], [94mLoss[0m : 2.23874
[1mStep[0m  [160/169], [94mLoss[0m : 2.77135

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58027
[1mStep[0m  [16/169], [94mLoss[0m : 2.29886
[1mStep[0m  [32/169], [94mLoss[0m : 2.26952
[1mStep[0m  [48/169], [94mLoss[0m : 2.98378
[1mStep[0m  [64/169], [94mLoss[0m : 2.55987
[1mStep[0m  [80/169], [94mLoss[0m : 3.16227
[1mStep[0m  [96/169], [94mLoss[0m : 2.61384
[1mStep[0m  [112/169], [94mLoss[0m : 3.20476
[1mStep[0m  [128/169], [94mLoss[0m : 2.42348
[1mStep[0m  [144/169], [94mLoss[0m : 2.24978
[1mStep[0m  [160/169], [94mLoss[0m : 3.07266

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.623, [92mTest[0m: 2.553, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64909
[1mStep[0m  [16/169], [94mLoss[0m : 2.50828
[1mStep[0m  [32/169], [94mLoss[0m : 2.85429
[1mStep[0m  [48/169], [94mLoss[0m : 2.47348
[1mStep[0m  [64/169], [94mLoss[0m : 2.44657
[1mStep[0m  [80/169], [94mLoss[0m : 2.54871
[1mStep[0m  [96/169], [94mLoss[0m : 2.39443
[1mStep[0m  [112/169], [94mLoss[0m : 2.97790
[1mStep[0m  [128/169], [94mLoss[0m : 2.10508
[1mStep[0m  [144/169], [94mLoss[0m : 2.52445
[1mStep[0m  [160/169], [94mLoss[0m : 2.74104

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.579, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61663
[1mStep[0m  [16/169], [94mLoss[0m : 3.07950
[1mStep[0m  [32/169], [94mLoss[0m : 2.65774
[1mStep[0m  [48/169], [94mLoss[0m : 2.84245
[1mStep[0m  [64/169], [94mLoss[0m : 2.96937
[1mStep[0m  [80/169], [94mLoss[0m : 2.32133
[1mStep[0m  [96/169], [94mLoss[0m : 2.62851
[1mStep[0m  [112/169], [94mLoss[0m : 2.52445
[1mStep[0m  [128/169], [94mLoss[0m : 2.59250
[1mStep[0m  [144/169], [94mLoss[0m : 2.45665
[1mStep[0m  [160/169], [94mLoss[0m : 2.81577

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.625, [92mTest[0m: 2.622, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31136
[1mStep[0m  [16/169], [94mLoss[0m : 2.69058
[1mStep[0m  [32/169], [94mLoss[0m : 2.37666
[1mStep[0m  [48/169], [94mLoss[0m : 2.33710
[1mStep[0m  [64/169], [94mLoss[0m : 2.64268
[1mStep[0m  [80/169], [94mLoss[0m : 2.94702
[1mStep[0m  [96/169], [94mLoss[0m : 2.93585
[1mStep[0m  [112/169], [94mLoss[0m : 2.54010
[1mStep[0m  [128/169], [94mLoss[0m : 2.12037
[1mStep[0m  [144/169], [94mLoss[0m : 2.68765
[1mStep[0m  [160/169], [94mLoss[0m : 2.46700

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.584, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59055
[1mStep[0m  [16/169], [94mLoss[0m : 3.01571
[1mStep[0m  [32/169], [94mLoss[0m : 2.38878
[1mStep[0m  [48/169], [94mLoss[0m : 2.86015
[1mStep[0m  [64/169], [94mLoss[0m : 2.33109
[1mStep[0m  [80/169], [94mLoss[0m : 2.48115
[1mStep[0m  [96/169], [94mLoss[0m : 2.59662
[1mStep[0m  [112/169], [94mLoss[0m : 2.81202
[1mStep[0m  [128/169], [94mLoss[0m : 2.73500
[1mStep[0m  [144/169], [94mLoss[0m : 2.60876
[1mStep[0m  [160/169], [94mLoss[0m : 2.35204

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.655, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46431
[1mStep[0m  [16/169], [94mLoss[0m : 2.89904
[1mStep[0m  [32/169], [94mLoss[0m : 2.43356
[1mStep[0m  [48/169], [94mLoss[0m : 2.26908
[1mStep[0m  [64/169], [94mLoss[0m : 3.10443
[1mStep[0m  [80/169], [94mLoss[0m : 2.47857
[1mStep[0m  [96/169], [94mLoss[0m : 2.81510
[1mStep[0m  [112/169], [94mLoss[0m : 2.47581
[1mStep[0m  [128/169], [94mLoss[0m : 2.90129
[1mStep[0m  [144/169], [94mLoss[0m : 2.92030
[1mStep[0m  [160/169], [94mLoss[0m : 2.62753

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72862
[1mStep[0m  [16/169], [94mLoss[0m : 2.26591
[1mStep[0m  [32/169], [94mLoss[0m : 2.52064
[1mStep[0m  [48/169], [94mLoss[0m : 2.90339
[1mStep[0m  [64/169], [94mLoss[0m : 2.59758
[1mStep[0m  [80/169], [94mLoss[0m : 2.18770
[1mStep[0m  [96/169], [94mLoss[0m : 2.56608
[1mStep[0m  [112/169], [94mLoss[0m : 2.74146
[1mStep[0m  [128/169], [94mLoss[0m : 2.87707
[1mStep[0m  [144/169], [94mLoss[0m : 2.72474
[1mStep[0m  [160/169], [94mLoss[0m : 2.85392

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.588, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71717
[1mStep[0m  [16/169], [94mLoss[0m : 2.47381
[1mStep[0m  [32/169], [94mLoss[0m : 2.04616
[1mStep[0m  [48/169], [94mLoss[0m : 1.95113
[1mStep[0m  [64/169], [94mLoss[0m : 2.71483
[1mStep[0m  [80/169], [94mLoss[0m : 2.32290
[1mStep[0m  [96/169], [94mLoss[0m : 2.92799
[1mStep[0m  [112/169], [94mLoss[0m : 2.93121
[1mStep[0m  [128/169], [94mLoss[0m : 2.78497
[1mStep[0m  [144/169], [94mLoss[0m : 2.60971
[1mStep[0m  [160/169], [94mLoss[0m : 2.62346

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.579, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00322
[1mStep[0m  [16/169], [94mLoss[0m : 2.43324
[1mStep[0m  [32/169], [94mLoss[0m : 2.30456
[1mStep[0m  [48/169], [94mLoss[0m : 2.58783
[1mStep[0m  [64/169], [94mLoss[0m : 2.36957
[1mStep[0m  [80/169], [94mLoss[0m : 2.65402
[1mStep[0m  [96/169], [94mLoss[0m : 2.82503
[1mStep[0m  [112/169], [94mLoss[0m : 2.42702
[1mStep[0m  [128/169], [94mLoss[0m : 2.79560
[1mStep[0m  [144/169], [94mLoss[0m : 2.62753
[1mStep[0m  [160/169], [94mLoss[0m : 2.61287

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.623, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57839
[1mStep[0m  [16/169], [94mLoss[0m : 2.68050
[1mStep[0m  [32/169], [94mLoss[0m : 2.60576
[1mStep[0m  [48/169], [94mLoss[0m : 2.85459
[1mStep[0m  [64/169], [94mLoss[0m : 2.75596
[1mStep[0m  [80/169], [94mLoss[0m : 2.00951
[1mStep[0m  [96/169], [94mLoss[0m : 2.56266
[1mStep[0m  [112/169], [94mLoss[0m : 2.69729
[1mStep[0m  [128/169], [94mLoss[0m : 2.71873
[1mStep[0m  [144/169], [94mLoss[0m : 2.68735
[1mStep[0m  [160/169], [94mLoss[0m : 2.42964

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31811
[1mStep[0m  [16/169], [94mLoss[0m : 2.73208
[1mStep[0m  [32/169], [94mLoss[0m : 3.29471
[1mStep[0m  [48/169], [94mLoss[0m : 2.62047
[1mStep[0m  [64/169], [94mLoss[0m : 2.20589
[1mStep[0m  [80/169], [94mLoss[0m : 2.59163
[1mStep[0m  [96/169], [94mLoss[0m : 2.45714
[1mStep[0m  [112/169], [94mLoss[0m : 2.47001
[1mStep[0m  [128/169], [94mLoss[0m : 2.46972
[1mStep[0m  [144/169], [94mLoss[0m : 2.31136
[1mStep[0m  [160/169], [94mLoss[0m : 2.95988

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.679, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55881
[1mStep[0m  [16/169], [94mLoss[0m : 2.41228
[1mStep[0m  [32/169], [94mLoss[0m : 2.40749
[1mStep[0m  [48/169], [94mLoss[0m : 2.43281
[1mStep[0m  [64/169], [94mLoss[0m : 2.03105
[1mStep[0m  [80/169], [94mLoss[0m : 2.44322
[1mStep[0m  [96/169], [94mLoss[0m : 2.62873
[1mStep[0m  [112/169], [94mLoss[0m : 2.13246
[1mStep[0m  [128/169], [94mLoss[0m : 2.25333
[1mStep[0m  [144/169], [94mLoss[0m : 2.86961
[1mStep[0m  [160/169], [94mLoss[0m : 2.61024

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35869
[1mStep[0m  [16/169], [94mLoss[0m : 2.39704
[1mStep[0m  [32/169], [94mLoss[0m : 2.42752
[1mStep[0m  [48/169], [94mLoss[0m : 2.56252
[1mStep[0m  [64/169], [94mLoss[0m : 2.51221
[1mStep[0m  [80/169], [94mLoss[0m : 2.64029
[1mStep[0m  [96/169], [94mLoss[0m : 3.35424
[1mStep[0m  [112/169], [94mLoss[0m : 2.61242
[1mStep[0m  [128/169], [94mLoss[0m : 2.36676
[1mStep[0m  [144/169], [94mLoss[0m : 2.51673
[1mStep[0m  [160/169], [94mLoss[0m : 2.11498

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.666, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54213
[1mStep[0m  [16/169], [94mLoss[0m : 2.39862
[1mStep[0m  [32/169], [94mLoss[0m : 2.90863
[1mStep[0m  [48/169], [94mLoss[0m : 2.26993
[1mStep[0m  [64/169], [94mLoss[0m : 2.21832
[1mStep[0m  [80/169], [94mLoss[0m : 3.21620
[1mStep[0m  [96/169], [94mLoss[0m : 2.79033
[1mStep[0m  [112/169], [94mLoss[0m : 2.40268
[1mStep[0m  [128/169], [94mLoss[0m : 2.74000
[1mStep[0m  [144/169], [94mLoss[0m : 2.52048
[1mStep[0m  [160/169], [94mLoss[0m : 3.02907

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.678, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53530
[1mStep[0m  [16/169], [94mLoss[0m : 2.22311
[1mStep[0m  [32/169], [94mLoss[0m : 2.45259
[1mStep[0m  [48/169], [94mLoss[0m : 3.14254
[1mStep[0m  [64/169], [94mLoss[0m : 2.34267
[1mStep[0m  [80/169], [94mLoss[0m : 2.19068
[1mStep[0m  [96/169], [94mLoss[0m : 2.61001
[1mStep[0m  [112/169], [94mLoss[0m : 2.20776
[1mStep[0m  [128/169], [94mLoss[0m : 2.54526
[1mStep[0m  [144/169], [94mLoss[0m : 2.76024
[1mStep[0m  [160/169], [94mLoss[0m : 2.53926

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.01350
[1mStep[0m  [16/169], [94mLoss[0m : 2.45508
[1mStep[0m  [32/169], [94mLoss[0m : 2.50391
[1mStep[0m  [48/169], [94mLoss[0m : 2.34062
[1mStep[0m  [64/169], [94mLoss[0m : 2.67284
[1mStep[0m  [80/169], [94mLoss[0m : 2.81718
[1mStep[0m  [96/169], [94mLoss[0m : 2.36284
[1mStep[0m  [112/169], [94mLoss[0m : 2.67118
[1mStep[0m  [128/169], [94mLoss[0m : 2.48734
[1mStep[0m  [144/169], [94mLoss[0m : 2.76073
[1mStep[0m  [160/169], [94mLoss[0m : 2.68144

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.652, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78196
[1mStep[0m  [16/169], [94mLoss[0m : 2.53011
[1mStep[0m  [32/169], [94mLoss[0m : 2.37973
[1mStep[0m  [48/169], [94mLoss[0m : 2.36175
[1mStep[0m  [64/169], [94mLoss[0m : 2.72939
[1mStep[0m  [80/169], [94mLoss[0m : 2.91174
[1mStep[0m  [96/169], [94mLoss[0m : 2.80326
[1mStep[0m  [112/169], [94mLoss[0m : 2.27901
[1mStep[0m  [128/169], [94mLoss[0m : 2.42806
[1mStep[0m  [144/169], [94mLoss[0m : 2.87674
[1mStep[0m  [160/169], [94mLoss[0m : 2.70271

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.686, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27427
[1mStep[0m  [16/169], [94mLoss[0m : 3.15335
[1mStep[0m  [32/169], [94mLoss[0m : 2.84074
[1mStep[0m  [48/169], [94mLoss[0m : 2.54598
[1mStep[0m  [64/169], [94mLoss[0m : 2.35441
[1mStep[0m  [80/169], [94mLoss[0m : 2.63647
[1mStep[0m  [96/169], [94mLoss[0m : 2.94420
[1mStep[0m  [112/169], [94mLoss[0m : 2.50177
[1mStep[0m  [128/169], [94mLoss[0m : 3.00847
[1mStep[0m  [144/169], [94mLoss[0m : 2.72566
[1mStep[0m  [160/169], [94mLoss[0m : 2.78342

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.689, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34062
[1mStep[0m  [16/169], [94mLoss[0m : 2.52109
[1mStep[0m  [32/169], [94mLoss[0m : 2.43568
[1mStep[0m  [48/169], [94mLoss[0m : 2.98686
[1mStep[0m  [64/169], [94mLoss[0m : 2.02937
[1mStep[0m  [80/169], [94mLoss[0m : 2.79235
[1mStep[0m  [96/169], [94mLoss[0m : 2.53356
[1mStep[0m  [112/169], [94mLoss[0m : 2.83447
[1mStep[0m  [128/169], [94mLoss[0m : 2.42464
[1mStep[0m  [144/169], [94mLoss[0m : 2.60145
[1mStep[0m  [160/169], [94mLoss[0m : 2.19846

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.709, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77293
[1mStep[0m  [16/169], [94mLoss[0m : 2.75492
[1mStep[0m  [32/169], [94mLoss[0m : 2.51775
[1mStep[0m  [48/169], [94mLoss[0m : 2.12104
[1mStep[0m  [64/169], [94mLoss[0m : 2.97111
[1mStep[0m  [80/169], [94mLoss[0m : 1.98778
[1mStep[0m  [96/169], [94mLoss[0m : 2.51491
[1mStep[0m  [112/169], [94mLoss[0m : 2.26598
[1mStep[0m  [128/169], [94mLoss[0m : 2.41597
[1mStep[0m  [144/169], [94mLoss[0m : 2.59850
[1mStep[0m  [160/169], [94mLoss[0m : 2.98591

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.672, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10345
[1mStep[0m  [16/169], [94mLoss[0m : 2.80781
[1mStep[0m  [32/169], [94mLoss[0m : 2.57158
[1mStep[0m  [48/169], [94mLoss[0m : 2.46047
[1mStep[0m  [64/169], [94mLoss[0m : 2.61266
[1mStep[0m  [80/169], [94mLoss[0m : 2.21755
[1mStep[0m  [96/169], [94mLoss[0m : 2.60086
[1mStep[0m  [112/169], [94mLoss[0m : 2.36052
[1mStep[0m  [128/169], [94mLoss[0m : 2.50419
[1mStep[0m  [144/169], [94mLoss[0m : 2.48975
[1mStep[0m  [160/169], [94mLoss[0m : 2.45884

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.689, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43828
[1mStep[0m  [16/169], [94mLoss[0m : 2.55195
[1mStep[0m  [32/169], [94mLoss[0m : 2.58587
[1mStep[0m  [48/169], [94mLoss[0m : 2.17862
[1mStep[0m  [64/169], [94mLoss[0m : 2.72053
[1mStep[0m  [80/169], [94mLoss[0m : 2.58709
[1mStep[0m  [96/169], [94mLoss[0m : 2.58909
[1mStep[0m  [112/169], [94mLoss[0m : 2.38045
[1mStep[0m  [128/169], [94mLoss[0m : 2.52800
[1mStep[0m  [144/169], [94mLoss[0m : 2.84978
[1mStep[0m  [160/169], [94mLoss[0m : 2.19493

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.703, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47504
[1mStep[0m  [16/169], [94mLoss[0m : 1.82184
[1mStep[0m  [32/169], [94mLoss[0m : 2.21784
[1mStep[0m  [48/169], [94mLoss[0m : 2.27218
[1mStep[0m  [64/169], [94mLoss[0m : 2.88844
[1mStep[0m  [80/169], [94mLoss[0m : 2.03677
[1mStep[0m  [96/169], [94mLoss[0m : 2.73980
[1mStep[0m  [112/169], [94mLoss[0m : 2.65922
[1mStep[0m  [128/169], [94mLoss[0m : 2.66611
[1mStep[0m  [144/169], [94mLoss[0m : 2.23297
[1mStep[0m  [160/169], [94mLoss[0m : 2.63749

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.718, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27059
[1mStep[0m  [16/169], [94mLoss[0m : 2.77819
[1mStep[0m  [32/169], [94mLoss[0m : 2.79909
[1mStep[0m  [48/169], [94mLoss[0m : 2.82548
[1mStep[0m  [64/169], [94mLoss[0m : 2.23791
[1mStep[0m  [80/169], [94mLoss[0m : 2.73851
[1mStep[0m  [96/169], [94mLoss[0m : 2.59165
[1mStep[0m  [112/169], [94mLoss[0m : 2.24378
[1mStep[0m  [128/169], [94mLoss[0m : 1.99548
[1mStep[0m  [144/169], [94mLoss[0m : 2.61085
[1mStep[0m  [160/169], [94mLoss[0m : 2.92349

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.700, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11399
[1mStep[0m  [16/169], [94mLoss[0m : 2.34279
[1mStep[0m  [32/169], [94mLoss[0m : 2.70773
[1mStep[0m  [48/169], [94mLoss[0m : 2.91376
[1mStep[0m  [64/169], [94mLoss[0m : 2.39568
[1mStep[0m  [80/169], [94mLoss[0m : 2.27787
[1mStep[0m  [96/169], [94mLoss[0m : 2.26782
[1mStep[0m  [112/169], [94mLoss[0m : 2.25734
[1mStep[0m  [128/169], [94mLoss[0m : 2.53448
[1mStep[0m  [144/169], [94mLoss[0m : 2.40631
[1mStep[0m  [160/169], [94mLoss[0m : 2.01701

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.751, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60625
[1mStep[0m  [16/169], [94mLoss[0m : 2.67921
[1mStep[0m  [32/169], [94mLoss[0m : 2.62060
[1mStep[0m  [48/169], [94mLoss[0m : 2.46198
[1mStep[0m  [64/169], [94mLoss[0m : 2.18444
[1mStep[0m  [80/169], [94mLoss[0m : 2.19249
[1mStep[0m  [96/169], [94mLoss[0m : 2.50526
[1mStep[0m  [112/169], [94mLoss[0m : 2.59695
[1mStep[0m  [128/169], [94mLoss[0m : 2.71587
[1mStep[0m  [144/169], [94mLoss[0m : 2.53549
[1mStep[0m  [160/169], [94mLoss[0m : 2.34699

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.678, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.729
====================================

Phase 2 - Evaluation MAE:  2.7288392654487064
MAE score P1       2.384416
MAE score P2       2.728839
loss               2.453171
learning_rate        0.0001
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 24, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.80957
[1mStep[0m  [4/42], [94mLoss[0m : 10.82719
[1mStep[0m  [8/42], [94mLoss[0m : 10.81540
[1mStep[0m  [12/42], [94mLoss[0m : 10.61902
[1mStep[0m  [16/42], [94mLoss[0m : 10.72198
[1mStep[0m  [20/42], [94mLoss[0m : 10.19223
[1mStep[0m  [24/42], [94mLoss[0m : 9.99718
[1mStep[0m  [28/42], [94mLoss[0m : 9.66462
[1mStep[0m  [32/42], [94mLoss[0m : 10.00208
[1mStep[0m  [36/42], [94mLoss[0m : 9.20469
[1mStep[0m  [40/42], [94mLoss[0m : 9.31714

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.271, [92mTest[0m: 10.882, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.33669
[1mStep[0m  [4/42], [94mLoss[0m : 9.05886
[1mStep[0m  [8/42], [94mLoss[0m : 8.83887
[1mStep[0m  [12/42], [94mLoss[0m : 8.63446
[1mStep[0m  [16/42], [94mLoss[0m : 8.71502
[1mStep[0m  [20/42], [94mLoss[0m : 8.44945
[1mStep[0m  [24/42], [94mLoss[0m : 8.47615
[1mStep[0m  [28/42], [94mLoss[0m : 8.00872
[1mStep[0m  [32/42], [94mLoss[0m : 8.13311
[1mStep[0m  [36/42], [94mLoss[0m : 8.24051
[1mStep[0m  [40/42], [94mLoss[0m : 7.51589

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.449, [92mTest[0m: 9.703, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.48132
[1mStep[0m  [4/42], [94mLoss[0m : 7.33247
[1mStep[0m  [8/42], [94mLoss[0m : 7.00187
[1mStep[0m  [12/42], [94mLoss[0m : 6.73865
[1mStep[0m  [16/42], [94mLoss[0m : 6.77406
[1mStep[0m  [20/42], [94mLoss[0m : 6.42143
[1mStep[0m  [24/42], [94mLoss[0m : 6.72268
[1mStep[0m  [28/42], [94mLoss[0m : 6.51069
[1mStep[0m  [32/42], [94mLoss[0m : 6.18030
[1mStep[0m  [36/42], [94mLoss[0m : 5.85689
[1mStep[0m  [40/42], [94mLoss[0m : 6.12807

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.605, [92mTest[0m: 8.061, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.78734
[1mStep[0m  [4/42], [94mLoss[0m : 5.68901
[1mStep[0m  [8/42], [94mLoss[0m : 5.63788
[1mStep[0m  [12/42], [94mLoss[0m : 5.19057
[1mStep[0m  [16/42], [94mLoss[0m : 5.20785
[1mStep[0m  [20/42], [94mLoss[0m : 5.37657
[1mStep[0m  [24/42], [94mLoss[0m : 5.10619
[1mStep[0m  [28/42], [94mLoss[0m : 4.57833
[1mStep[0m  [32/42], [94mLoss[0m : 4.91971
[1mStep[0m  [36/42], [94mLoss[0m : 4.34428
[1mStep[0m  [40/42], [94mLoss[0m : 4.53672

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.131, [92mTest[0m: 6.587, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.51032
[1mStep[0m  [4/42], [94mLoss[0m : 4.49718
[1mStep[0m  [8/42], [94mLoss[0m : 3.91976
[1mStep[0m  [12/42], [94mLoss[0m : 4.27913
[1mStep[0m  [16/42], [94mLoss[0m : 4.22969
[1mStep[0m  [20/42], [94mLoss[0m : 3.98728
[1mStep[0m  [24/42], [94mLoss[0m : 3.78964
[1mStep[0m  [28/42], [94mLoss[0m : 3.77270
[1mStep[0m  [32/42], [94mLoss[0m : 4.17050
[1mStep[0m  [36/42], [94mLoss[0m : 3.85517
[1mStep[0m  [40/42], [94mLoss[0m : 3.79153

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.076, [92mTest[0m: 5.321, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.78059
[1mStep[0m  [4/42], [94mLoss[0m : 3.61770
[1mStep[0m  [8/42], [94mLoss[0m : 3.40130
[1mStep[0m  [12/42], [94mLoss[0m : 3.33333
[1mStep[0m  [16/42], [94mLoss[0m : 3.62791
[1mStep[0m  [20/42], [94mLoss[0m : 3.36237
[1mStep[0m  [24/42], [94mLoss[0m : 3.14494
[1mStep[0m  [28/42], [94mLoss[0m : 3.19930
[1mStep[0m  [32/42], [94mLoss[0m : 3.30006
[1mStep[0m  [36/42], [94mLoss[0m : 3.26866
[1mStep[0m  [40/42], [94mLoss[0m : 3.10157

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.349, [92mTest[0m: 4.332, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.97542
[1mStep[0m  [4/42], [94mLoss[0m : 3.10396
[1mStep[0m  [8/42], [94mLoss[0m : 2.73534
[1mStep[0m  [12/42], [94mLoss[0m : 2.94090
[1mStep[0m  [16/42], [94mLoss[0m : 3.06275
[1mStep[0m  [20/42], [94mLoss[0m : 3.01282
[1mStep[0m  [24/42], [94mLoss[0m : 2.91159
[1mStep[0m  [28/42], [94mLoss[0m : 2.61282
[1mStep[0m  [32/42], [94mLoss[0m : 2.81631
[1mStep[0m  [36/42], [94mLoss[0m : 2.89670
[1mStep[0m  [40/42], [94mLoss[0m : 2.68329

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.952, [92mTest[0m: 3.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71130
[1mStep[0m  [4/42], [94mLoss[0m : 2.74671
[1mStep[0m  [8/42], [94mLoss[0m : 2.85725
[1mStep[0m  [12/42], [94mLoss[0m : 3.06699
[1mStep[0m  [16/42], [94mLoss[0m : 2.73262
[1mStep[0m  [20/42], [94mLoss[0m : 2.89593
[1mStep[0m  [24/42], [94mLoss[0m : 2.52729
[1mStep[0m  [28/42], [94mLoss[0m : 2.67156
[1mStep[0m  [32/42], [94mLoss[0m : 2.65373
[1mStep[0m  [36/42], [94mLoss[0m : 2.62880
[1mStep[0m  [40/42], [94mLoss[0m : 2.74432

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.755, [92mTest[0m: 3.117, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51581
[1mStep[0m  [4/42], [94mLoss[0m : 2.91686
[1mStep[0m  [8/42], [94mLoss[0m : 2.99998
[1mStep[0m  [12/42], [94mLoss[0m : 2.53067
[1mStep[0m  [16/42], [94mLoss[0m : 2.71576
[1mStep[0m  [20/42], [94mLoss[0m : 2.58750
[1mStep[0m  [24/42], [94mLoss[0m : 3.02458
[1mStep[0m  [28/42], [94mLoss[0m : 2.59285
[1mStep[0m  [32/42], [94mLoss[0m : 2.68359
[1mStep[0m  [36/42], [94mLoss[0m : 2.62132
[1mStep[0m  [40/42], [94mLoss[0m : 2.54058

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.687, [92mTest[0m: 2.885, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56796
[1mStep[0m  [4/42], [94mLoss[0m : 2.52020
[1mStep[0m  [8/42], [94mLoss[0m : 2.66888
[1mStep[0m  [12/42], [94mLoss[0m : 2.61765
[1mStep[0m  [16/42], [94mLoss[0m : 2.60795
[1mStep[0m  [20/42], [94mLoss[0m : 2.77124
[1mStep[0m  [24/42], [94mLoss[0m : 2.66084
[1mStep[0m  [28/42], [94mLoss[0m : 2.47699
[1mStep[0m  [32/42], [94mLoss[0m : 2.62316
[1mStep[0m  [36/42], [94mLoss[0m : 2.64466
[1mStep[0m  [40/42], [94mLoss[0m : 2.59139

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.770, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56388
[1mStep[0m  [4/42], [94mLoss[0m : 2.57426
[1mStep[0m  [8/42], [94mLoss[0m : 2.71202
[1mStep[0m  [12/42], [94mLoss[0m : 2.64120
[1mStep[0m  [16/42], [94mLoss[0m : 2.72578
[1mStep[0m  [20/42], [94mLoss[0m : 2.71605
[1mStep[0m  [24/42], [94mLoss[0m : 2.62956
[1mStep[0m  [28/42], [94mLoss[0m : 2.53151
[1mStep[0m  [32/42], [94mLoss[0m : 2.76044
[1mStep[0m  [36/42], [94mLoss[0m : 2.44317
[1mStep[0m  [40/42], [94mLoss[0m : 2.42830

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40882
[1mStep[0m  [4/42], [94mLoss[0m : 2.60100
[1mStep[0m  [8/42], [94mLoss[0m : 2.72405
[1mStep[0m  [12/42], [94mLoss[0m : 2.61002
[1mStep[0m  [16/42], [94mLoss[0m : 2.60285
[1mStep[0m  [20/42], [94mLoss[0m : 2.85352
[1mStep[0m  [24/42], [94mLoss[0m : 2.48368
[1mStep[0m  [28/42], [94mLoss[0m : 2.57207
[1mStep[0m  [32/42], [94mLoss[0m : 2.75259
[1mStep[0m  [36/42], [94mLoss[0m : 2.54862
[1mStep[0m  [40/42], [94mLoss[0m : 2.65294

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53773
[1mStep[0m  [4/42], [94mLoss[0m : 2.60208
[1mStep[0m  [8/42], [94mLoss[0m : 2.43260
[1mStep[0m  [12/42], [94mLoss[0m : 2.85899
[1mStep[0m  [16/42], [94mLoss[0m : 2.46237
[1mStep[0m  [20/42], [94mLoss[0m : 2.76924
[1mStep[0m  [24/42], [94mLoss[0m : 2.77867
[1mStep[0m  [28/42], [94mLoss[0m : 2.56209
[1mStep[0m  [32/42], [94mLoss[0m : 2.54532
[1mStep[0m  [36/42], [94mLoss[0m : 2.73527
[1mStep[0m  [40/42], [94mLoss[0m : 2.58654

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.644, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63386
[1mStep[0m  [4/42], [94mLoss[0m : 2.46429
[1mStep[0m  [8/42], [94mLoss[0m : 2.58628
[1mStep[0m  [12/42], [94mLoss[0m : 2.53521
[1mStep[0m  [16/42], [94mLoss[0m : 2.71576
[1mStep[0m  [20/42], [94mLoss[0m : 2.58027
[1mStep[0m  [24/42], [94mLoss[0m : 2.73339
[1mStep[0m  [28/42], [94mLoss[0m : 2.58595
[1mStep[0m  [32/42], [94mLoss[0m : 2.67757
[1mStep[0m  [36/42], [94mLoss[0m : 2.35269
[1mStep[0m  [40/42], [94mLoss[0m : 2.43237

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.664, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48558
[1mStep[0m  [4/42], [94mLoss[0m : 2.41606
[1mStep[0m  [8/42], [94mLoss[0m : 2.45560
[1mStep[0m  [12/42], [94mLoss[0m : 2.55219
[1mStep[0m  [16/42], [94mLoss[0m : 2.52051
[1mStep[0m  [20/42], [94mLoss[0m : 2.76209
[1mStep[0m  [24/42], [94mLoss[0m : 2.69900
[1mStep[0m  [28/42], [94mLoss[0m : 2.71118
[1mStep[0m  [32/42], [94mLoss[0m : 2.54898
[1mStep[0m  [36/42], [94mLoss[0m : 2.32595
[1mStep[0m  [40/42], [94mLoss[0m : 2.37970

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.617, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47233
[1mStep[0m  [4/42], [94mLoss[0m : 2.77224
[1mStep[0m  [8/42], [94mLoss[0m : 2.62860
[1mStep[0m  [12/42], [94mLoss[0m : 2.47667
[1mStep[0m  [16/42], [94mLoss[0m : 2.41457
[1mStep[0m  [20/42], [94mLoss[0m : 2.58292
[1mStep[0m  [24/42], [94mLoss[0m : 2.61147
[1mStep[0m  [28/42], [94mLoss[0m : 2.56838
[1mStep[0m  [32/42], [94mLoss[0m : 2.60536
[1mStep[0m  [36/42], [94mLoss[0m : 2.56714
[1mStep[0m  [40/42], [94mLoss[0m : 2.51570

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.610, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54111
[1mStep[0m  [4/42], [94mLoss[0m : 2.57784
[1mStep[0m  [8/42], [94mLoss[0m : 2.58339
[1mStep[0m  [12/42], [94mLoss[0m : 2.41315
[1mStep[0m  [16/42], [94mLoss[0m : 2.60215
[1mStep[0m  [20/42], [94mLoss[0m : 2.79051
[1mStep[0m  [24/42], [94mLoss[0m : 2.41675
[1mStep[0m  [28/42], [94mLoss[0m : 2.51932
[1mStep[0m  [32/42], [94mLoss[0m : 2.47617
[1mStep[0m  [36/42], [94mLoss[0m : 2.34327
[1mStep[0m  [40/42], [94mLoss[0m : 2.51165

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.636, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61930
[1mStep[0m  [4/42], [94mLoss[0m : 2.62332
[1mStep[0m  [8/42], [94mLoss[0m : 2.53112
[1mStep[0m  [12/42], [94mLoss[0m : 2.39773
[1mStep[0m  [16/42], [94mLoss[0m : 2.61366
[1mStep[0m  [20/42], [94mLoss[0m : 2.62133
[1mStep[0m  [24/42], [94mLoss[0m : 2.70873
[1mStep[0m  [28/42], [94mLoss[0m : 2.60237
[1mStep[0m  [32/42], [94mLoss[0m : 2.46226
[1mStep[0m  [36/42], [94mLoss[0m : 2.46876
[1mStep[0m  [40/42], [94mLoss[0m : 2.76623

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.598, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42099
[1mStep[0m  [4/42], [94mLoss[0m : 2.52338
[1mStep[0m  [8/42], [94mLoss[0m : 2.33947
[1mStep[0m  [12/42], [94mLoss[0m : 2.59667
[1mStep[0m  [16/42], [94mLoss[0m : 2.55396
[1mStep[0m  [20/42], [94mLoss[0m : 2.51406
[1mStep[0m  [24/42], [94mLoss[0m : 2.62390
[1mStep[0m  [28/42], [94mLoss[0m : 2.57732
[1mStep[0m  [32/42], [94mLoss[0m : 2.45698
[1mStep[0m  [36/42], [94mLoss[0m : 2.66330
[1mStep[0m  [40/42], [94mLoss[0m : 2.79735

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58679
[1mStep[0m  [4/42], [94mLoss[0m : 2.58019
[1mStep[0m  [8/42], [94mLoss[0m : 2.45716
[1mStep[0m  [12/42], [94mLoss[0m : 2.38992
[1mStep[0m  [16/42], [94mLoss[0m : 2.51979
[1mStep[0m  [20/42], [94mLoss[0m : 2.53906
[1mStep[0m  [24/42], [94mLoss[0m : 2.42459
[1mStep[0m  [28/42], [94mLoss[0m : 2.39509
[1mStep[0m  [32/42], [94mLoss[0m : 2.54478
[1mStep[0m  [36/42], [94mLoss[0m : 2.61246
[1mStep[0m  [40/42], [94mLoss[0m : 2.50440

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.589, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75285
[1mStep[0m  [4/42], [94mLoss[0m : 2.50317
[1mStep[0m  [8/42], [94mLoss[0m : 2.53378
[1mStep[0m  [12/42], [94mLoss[0m : 2.49014
[1mStep[0m  [16/42], [94mLoss[0m : 2.74457
[1mStep[0m  [20/42], [94mLoss[0m : 2.55085
[1mStep[0m  [24/42], [94mLoss[0m : 2.60161
[1mStep[0m  [28/42], [94mLoss[0m : 2.34498
[1mStep[0m  [32/42], [94mLoss[0m : 2.60402
[1mStep[0m  [36/42], [94mLoss[0m : 2.40395
[1mStep[0m  [40/42], [94mLoss[0m : 2.43555

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.594, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51917
[1mStep[0m  [4/42], [94mLoss[0m : 2.48967
[1mStep[0m  [8/42], [94mLoss[0m : 2.68869
[1mStep[0m  [12/42], [94mLoss[0m : 2.75858
[1mStep[0m  [16/42], [94mLoss[0m : 2.63438
[1mStep[0m  [20/42], [94mLoss[0m : 2.75695
[1mStep[0m  [24/42], [94mLoss[0m : 2.44608
[1mStep[0m  [28/42], [94mLoss[0m : 2.32396
[1mStep[0m  [32/42], [94mLoss[0m : 2.45450
[1mStep[0m  [36/42], [94mLoss[0m : 2.30527
[1mStep[0m  [40/42], [94mLoss[0m : 2.71958

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.574, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50273
[1mStep[0m  [4/42], [94mLoss[0m : 2.56439
[1mStep[0m  [8/42], [94mLoss[0m : 2.60491
[1mStep[0m  [12/42], [94mLoss[0m : 2.61936
[1mStep[0m  [16/42], [94mLoss[0m : 2.36714
[1mStep[0m  [20/42], [94mLoss[0m : 2.54964
[1mStep[0m  [24/42], [94mLoss[0m : 2.26016
[1mStep[0m  [28/42], [94mLoss[0m : 2.42205
[1mStep[0m  [32/42], [94mLoss[0m : 2.55947
[1mStep[0m  [36/42], [94mLoss[0m : 2.32250
[1mStep[0m  [40/42], [94mLoss[0m : 2.46697

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.590, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46752
[1mStep[0m  [4/42], [94mLoss[0m : 2.59289
[1mStep[0m  [8/42], [94mLoss[0m : 2.43578
[1mStep[0m  [12/42], [94mLoss[0m : 2.66014
[1mStep[0m  [16/42], [94mLoss[0m : 2.66565
[1mStep[0m  [20/42], [94mLoss[0m : 2.43825
[1mStep[0m  [24/42], [94mLoss[0m : 2.51871
[1mStep[0m  [28/42], [94mLoss[0m : 2.67050
[1mStep[0m  [32/42], [94mLoss[0m : 2.41371
[1mStep[0m  [36/42], [94mLoss[0m : 2.34361
[1mStep[0m  [40/42], [94mLoss[0m : 2.51881

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.562, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61894
[1mStep[0m  [4/42], [94mLoss[0m : 2.46242
[1mStep[0m  [8/42], [94mLoss[0m : 2.65585
[1mStep[0m  [12/42], [94mLoss[0m : 2.69622
[1mStep[0m  [16/42], [94mLoss[0m : 2.10842
[1mStep[0m  [20/42], [94mLoss[0m : 2.52464
[1mStep[0m  [24/42], [94mLoss[0m : 2.59665
[1mStep[0m  [28/42], [94mLoss[0m : 2.42479
[1mStep[0m  [32/42], [94mLoss[0m : 2.70513
[1mStep[0m  [36/42], [94mLoss[0m : 2.46173
[1mStep[0m  [40/42], [94mLoss[0m : 2.49890

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52462
[1mStep[0m  [4/42], [94mLoss[0m : 2.48210
[1mStep[0m  [8/42], [94mLoss[0m : 2.49877
[1mStep[0m  [12/42], [94mLoss[0m : 2.52133
[1mStep[0m  [16/42], [94mLoss[0m : 2.64330
[1mStep[0m  [20/42], [94mLoss[0m : 2.48887
[1mStep[0m  [24/42], [94mLoss[0m : 2.37205
[1mStep[0m  [28/42], [94mLoss[0m : 2.39282
[1mStep[0m  [32/42], [94mLoss[0m : 2.55973
[1mStep[0m  [36/42], [94mLoss[0m : 2.60992
[1mStep[0m  [40/42], [94mLoss[0m : 2.38215

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42598
[1mStep[0m  [4/42], [94mLoss[0m : 2.25148
[1mStep[0m  [8/42], [94mLoss[0m : 2.77620
[1mStep[0m  [12/42], [94mLoss[0m : 2.32551
[1mStep[0m  [16/42], [94mLoss[0m : 2.51290
[1mStep[0m  [20/42], [94mLoss[0m : 2.54610
[1mStep[0m  [24/42], [94mLoss[0m : 2.48435
[1mStep[0m  [28/42], [94mLoss[0m : 2.65121
[1mStep[0m  [32/42], [94mLoss[0m : 2.59425
[1mStep[0m  [36/42], [94mLoss[0m : 2.49964
[1mStep[0m  [40/42], [94mLoss[0m : 2.37089

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.574, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70073
[1mStep[0m  [4/42], [94mLoss[0m : 2.45172
[1mStep[0m  [8/42], [94mLoss[0m : 2.44000
[1mStep[0m  [12/42], [94mLoss[0m : 2.38074
[1mStep[0m  [16/42], [94mLoss[0m : 2.39294
[1mStep[0m  [20/42], [94mLoss[0m : 2.39458
[1mStep[0m  [24/42], [94mLoss[0m : 2.53460
[1mStep[0m  [28/42], [94mLoss[0m : 2.37717
[1mStep[0m  [32/42], [94mLoss[0m : 2.76451
[1mStep[0m  [36/42], [94mLoss[0m : 2.57119
[1mStep[0m  [40/42], [94mLoss[0m : 2.62805

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.566, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39485
[1mStep[0m  [4/42], [94mLoss[0m : 2.22315
[1mStep[0m  [8/42], [94mLoss[0m : 2.37877
[1mStep[0m  [12/42], [94mLoss[0m : 2.43607
[1mStep[0m  [16/42], [94mLoss[0m : 2.39720
[1mStep[0m  [20/42], [94mLoss[0m : 2.35692
[1mStep[0m  [24/42], [94mLoss[0m : 2.40701
[1mStep[0m  [28/42], [94mLoss[0m : 2.32339
[1mStep[0m  [32/42], [94mLoss[0m : 2.53642
[1mStep[0m  [36/42], [94mLoss[0m : 2.37581
[1mStep[0m  [40/42], [94mLoss[0m : 2.53998

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.589, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46180
[1mStep[0m  [4/42], [94mLoss[0m : 2.34114
[1mStep[0m  [8/42], [94mLoss[0m : 2.38347
[1mStep[0m  [12/42], [94mLoss[0m : 2.53113
[1mStep[0m  [16/42], [94mLoss[0m : 2.46202
[1mStep[0m  [20/42], [94mLoss[0m : 2.58533
[1mStep[0m  [24/42], [94mLoss[0m : 2.52068
[1mStep[0m  [28/42], [94mLoss[0m : 2.54035
[1mStep[0m  [32/42], [94mLoss[0m : 2.51446
[1mStep[0m  [36/42], [94mLoss[0m : 2.37103
[1mStep[0m  [40/42], [94mLoss[0m : 2.49886

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.576, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.568
====================================

Phase 1 - Evaluation MAE:  2.5679926531655446
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.64097
[1mStep[0m  [4/42], [94mLoss[0m : 2.41648
[1mStep[0m  [8/42], [94mLoss[0m : 2.35886
[1mStep[0m  [12/42], [94mLoss[0m : 2.70022
[1mStep[0m  [16/42], [94mLoss[0m : 2.48213
[1mStep[0m  [20/42], [94mLoss[0m : 2.42088
[1mStep[0m  [24/42], [94mLoss[0m : 2.50512
[1mStep[0m  [28/42], [94mLoss[0m : 2.53450
[1mStep[0m  [32/42], [94mLoss[0m : 2.37855
[1mStep[0m  [36/42], [94mLoss[0m : 2.49206
[1mStep[0m  [40/42], [94mLoss[0m : 2.37379

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64183
[1mStep[0m  [4/42], [94mLoss[0m : 2.51373
[1mStep[0m  [8/42], [94mLoss[0m : 2.42459
[1mStep[0m  [12/42], [94mLoss[0m : 2.44675
[1mStep[0m  [16/42], [94mLoss[0m : 2.64570
[1mStep[0m  [20/42], [94mLoss[0m : 2.49106
[1mStep[0m  [24/42], [94mLoss[0m : 2.60634
[1mStep[0m  [28/42], [94mLoss[0m : 2.60332
[1mStep[0m  [32/42], [94mLoss[0m : 2.41241
[1mStep[0m  [36/42], [94mLoss[0m : 2.40674
[1mStep[0m  [40/42], [94mLoss[0m : 2.37190

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42892
[1mStep[0m  [4/42], [94mLoss[0m : 2.60958
[1mStep[0m  [8/42], [94mLoss[0m : 2.41940
[1mStep[0m  [12/42], [94mLoss[0m : 2.50742
[1mStep[0m  [16/42], [94mLoss[0m : 2.46660
[1mStep[0m  [20/42], [94mLoss[0m : 2.65719
[1mStep[0m  [24/42], [94mLoss[0m : 2.39974
[1mStep[0m  [28/42], [94mLoss[0m : 2.72815
[1mStep[0m  [32/42], [94mLoss[0m : 2.46772
[1mStep[0m  [36/42], [94mLoss[0m : 2.49072
[1mStep[0m  [40/42], [94mLoss[0m : 2.52014

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.473, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44153
[1mStep[0m  [4/42], [94mLoss[0m : 2.43347
[1mStep[0m  [8/42], [94mLoss[0m : 2.48604
[1mStep[0m  [12/42], [94mLoss[0m : 2.46056
[1mStep[0m  [16/42], [94mLoss[0m : 2.33110
[1mStep[0m  [20/42], [94mLoss[0m : 2.41543
[1mStep[0m  [24/42], [94mLoss[0m : 2.44858
[1mStep[0m  [28/42], [94mLoss[0m : 2.12730
[1mStep[0m  [32/42], [94mLoss[0m : 2.23933
[1mStep[0m  [36/42], [94mLoss[0m : 2.34061
[1mStep[0m  [40/42], [94mLoss[0m : 2.61063

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.455, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30857
[1mStep[0m  [4/42], [94mLoss[0m : 2.16038
[1mStep[0m  [8/42], [94mLoss[0m : 2.30943
[1mStep[0m  [12/42], [94mLoss[0m : 2.40891
[1mStep[0m  [16/42], [94mLoss[0m : 2.66976
[1mStep[0m  [20/42], [94mLoss[0m : 2.28045
[1mStep[0m  [24/42], [94mLoss[0m : 2.60670
[1mStep[0m  [28/42], [94mLoss[0m : 2.37093
[1mStep[0m  [32/42], [94mLoss[0m : 2.49310
[1mStep[0m  [36/42], [94mLoss[0m : 2.48956
[1mStep[0m  [40/42], [94mLoss[0m : 2.40076

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.476, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31058
[1mStep[0m  [4/42], [94mLoss[0m : 2.39822
[1mStep[0m  [8/42], [94mLoss[0m : 2.37193
[1mStep[0m  [12/42], [94mLoss[0m : 2.47973
[1mStep[0m  [16/42], [94mLoss[0m : 2.50925
[1mStep[0m  [20/42], [94mLoss[0m : 2.13136
[1mStep[0m  [24/42], [94mLoss[0m : 2.60175
[1mStep[0m  [28/42], [94mLoss[0m : 2.26644
[1mStep[0m  [32/42], [94mLoss[0m : 2.35741
[1mStep[0m  [36/42], [94mLoss[0m : 2.73462
[1mStep[0m  [40/42], [94mLoss[0m : 2.57972

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.571, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43024
[1mStep[0m  [4/42], [94mLoss[0m : 2.36412
[1mStep[0m  [8/42], [94mLoss[0m : 2.63694
[1mStep[0m  [12/42], [94mLoss[0m : 2.49633
[1mStep[0m  [16/42], [94mLoss[0m : 2.53075
[1mStep[0m  [20/42], [94mLoss[0m : 2.38991
[1mStep[0m  [24/42], [94mLoss[0m : 2.52269
[1mStep[0m  [28/42], [94mLoss[0m : 2.54362
[1mStep[0m  [32/42], [94mLoss[0m : 2.36420
[1mStep[0m  [36/42], [94mLoss[0m : 2.13339
[1mStep[0m  [40/42], [94mLoss[0m : 2.42736

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28673
[1mStep[0m  [4/42], [94mLoss[0m : 2.46580
[1mStep[0m  [8/42], [94mLoss[0m : 2.39655
[1mStep[0m  [12/42], [94mLoss[0m : 2.53225
[1mStep[0m  [16/42], [94mLoss[0m : 2.29717
[1mStep[0m  [20/42], [94mLoss[0m : 2.42271
[1mStep[0m  [24/42], [94mLoss[0m : 2.39077
[1mStep[0m  [28/42], [94mLoss[0m : 2.30407
[1mStep[0m  [32/42], [94mLoss[0m : 2.42694
[1mStep[0m  [36/42], [94mLoss[0m : 2.55585
[1mStep[0m  [40/42], [94mLoss[0m : 2.46243

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31112
[1mStep[0m  [4/42], [94mLoss[0m : 2.36021
[1mStep[0m  [8/42], [94mLoss[0m : 2.48882
[1mStep[0m  [12/42], [94mLoss[0m : 2.43870
[1mStep[0m  [16/42], [94mLoss[0m : 2.44084
[1mStep[0m  [20/42], [94mLoss[0m : 2.65180
[1mStep[0m  [24/42], [94mLoss[0m : 2.25578
[1mStep[0m  [28/42], [94mLoss[0m : 2.39131
[1mStep[0m  [32/42], [94mLoss[0m : 2.58071
[1mStep[0m  [36/42], [94mLoss[0m : 2.55737
[1mStep[0m  [40/42], [94mLoss[0m : 2.12416

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.10965
[1mStep[0m  [4/42], [94mLoss[0m : 2.39741
[1mStep[0m  [8/42], [94mLoss[0m : 2.58140
[1mStep[0m  [12/42], [94mLoss[0m : 2.23880
[1mStep[0m  [16/42], [94mLoss[0m : 2.57078
[1mStep[0m  [20/42], [94mLoss[0m : 2.36028
[1mStep[0m  [24/42], [94mLoss[0m : 2.38382
[1mStep[0m  [28/42], [94mLoss[0m : 2.20135
[1mStep[0m  [32/42], [94mLoss[0m : 2.53706
[1mStep[0m  [36/42], [94mLoss[0m : 2.28643
[1mStep[0m  [40/42], [94mLoss[0m : 2.30241

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.699, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38844
[1mStep[0m  [4/42], [94mLoss[0m : 2.14302
[1mStep[0m  [8/42], [94mLoss[0m : 2.38339
[1mStep[0m  [12/42], [94mLoss[0m : 2.35916
[1mStep[0m  [16/42], [94mLoss[0m : 2.13625
[1mStep[0m  [20/42], [94mLoss[0m : 2.42352
[1mStep[0m  [24/42], [94mLoss[0m : 2.40088
[1mStep[0m  [28/42], [94mLoss[0m : 2.39016
[1mStep[0m  [32/42], [94mLoss[0m : 2.40572
[1mStep[0m  [36/42], [94mLoss[0m : 2.35408
[1mStep[0m  [40/42], [94mLoss[0m : 2.15893

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36778
[1mStep[0m  [4/42], [94mLoss[0m : 2.34477
[1mStep[0m  [8/42], [94mLoss[0m : 2.34021
[1mStep[0m  [12/42], [94mLoss[0m : 2.34930
[1mStep[0m  [16/42], [94mLoss[0m : 2.18278
[1mStep[0m  [20/42], [94mLoss[0m : 2.34382
[1mStep[0m  [24/42], [94mLoss[0m : 2.47638
[1mStep[0m  [28/42], [94mLoss[0m : 2.42236
[1mStep[0m  [32/42], [94mLoss[0m : 2.33392
[1mStep[0m  [36/42], [94mLoss[0m : 2.33281
[1mStep[0m  [40/42], [94mLoss[0m : 2.22424

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26808
[1mStep[0m  [4/42], [94mLoss[0m : 2.47152
[1mStep[0m  [8/42], [94mLoss[0m : 2.51144
[1mStep[0m  [12/42], [94mLoss[0m : 2.36771
[1mStep[0m  [16/42], [94mLoss[0m : 2.39183
[1mStep[0m  [20/42], [94mLoss[0m : 2.49023
[1mStep[0m  [24/42], [94mLoss[0m : 2.48639
[1mStep[0m  [28/42], [94mLoss[0m : 2.37748
[1mStep[0m  [32/42], [94mLoss[0m : 2.52088
[1mStep[0m  [36/42], [94mLoss[0m : 2.59585
[1mStep[0m  [40/42], [94mLoss[0m : 2.40070

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38320
[1mStep[0m  [4/42], [94mLoss[0m : 2.44666
[1mStep[0m  [8/42], [94mLoss[0m : 2.45384
[1mStep[0m  [12/42], [94mLoss[0m : 2.20540
[1mStep[0m  [16/42], [94mLoss[0m : 2.46859
[1mStep[0m  [20/42], [94mLoss[0m : 2.40281
[1mStep[0m  [24/42], [94mLoss[0m : 2.16758
[1mStep[0m  [28/42], [94mLoss[0m : 2.38383
[1mStep[0m  [32/42], [94mLoss[0m : 2.30863
[1mStep[0m  [36/42], [94mLoss[0m : 2.52902
[1mStep[0m  [40/42], [94mLoss[0m : 2.12213

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.20159
[1mStep[0m  [4/42], [94mLoss[0m : 2.51462
[1mStep[0m  [8/42], [94mLoss[0m : 2.21899
[1mStep[0m  [12/42], [94mLoss[0m : 2.38259
[1mStep[0m  [16/42], [94mLoss[0m : 2.25178
[1mStep[0m  [20/42], [94mLoss[0m : 2.49798
[1mStep[0m  [24/42], [94mLoss[0m : 2.35640
[1mStep[0m  [28/42], [94mLoss[0m : 2.18917
[1mStep[0m  [32/42], [94mLoss[0m : 2.26849
[1mStep[0m  [36/42], [94mLoss[0m : 2.52931
[1mStep[0m  [40/42], [94mLoss[0m : 2.60578

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.533, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33129
[1mStep[0m  [4/42], [94mLoss[0m : 2.28210
[1mStep[0m  [8/42], [94mLoss[0m : 2.35449
[1mStep[0m  [12/42], [94mLoss[0m : 2.56125
[1mStep[0m  [16/42], [94mLoss[0m : 2.36283
[1mStep[0m  [20/42], [94mLoss[0m : 2.57263
[1mStep[0m  [24/42], [94mLoss[0m : 2.40229
[1mStep[0m  [28/42], [94mLoss[0m : 2.41411
[1mStep[0m  [32/42], [94mLoss[0m : 2.45874
[1mStep[0m  [36/42], [94mLoss[0m : 2.31006
[1mStep[0m  [40/42], [94mLoss[0m : 2.32993

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.552, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17416
[1mStep[0m  [4/42], [94mLoss[0m : 2.35780
[1mStep[0m  [8/42], [94mLoss[0m : 2.19642
[1mStep[0m  [12/42], [94mLoss[0m : 2.32881
[1mStep[0m  [16/42], [94mLoss[0m : 2.20528
[1mStep[0m  [20/42], [94mLoss[0m : 2.30216
[1mStep[0m  [24/42], [94mLoss[0m : 2.52592
[1mStep[0m  [28/42], [94mLoss[0m : 2.42118
[1mStep[0m  [32/42], [94mLoss[0m : 2.46408
[1mStep[0m  [36/42], [94mLoss[0m : 2.40714
[1mStep[0m  [40/42], [94mLoss[0m : 2.46442

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.559, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51091
[1mStep[0m  [4/42], [94mLoss[0m : 2.38094
[1mStep[0m  [8/42], [94mLoss[0m : 2.36113
[1mStep[0m  [12/42], [94mLoss[0m : 2.29342
[1mStep[0m  [16/42], [94mLoss[0m : 2.22426
[1mStep[0m  [20/42], [94mLoss[0m : 2.48257
[1mStep[0m  [24/42], [94mLoss[0m : 2.54532
[1mStep[0m  [28/42], [94mLoss[0m : 2.35453
[1mStep[0m  [32/42], [94mLoss[0m : 2.50840
[1mStep[0m  [36/42], [94mLoss[0m : 2.13782
[1mStep[0m  [40/42], [94mLoss[0m : 2.24496

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.540, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.08349
[1mStep[0m  [4/42], [94mLoss[0m : 2.44381
[1mStep[0m  [8/42], [94mLoss[0m : 2.43585
[1mStep[0m  [12/42], [94mLoss[0m : 2.46021
[1mStep[0m  [16/42], [94mLoss[0m : 2.26677
[1mStep[0m  [20/42], [94mLoss[0m : 2.44558
[1mStep[0m  [24/42], [94mLoss[0m : 2.31461
[1mStep[0m  [28/42], [94mLoss[0m : 2.22764
[1mStep[0m  [32/42], [94mLoss[0m : 2.21765
[1mStep[0m  [36/42], [94mLoss[0m : 2.20120
[1mStep[0m  [40/42], [94mLoss[0m : 2.46098

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.648, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24617
[1mStep[0m  [4/42], [94mLoss[0m : 2.42924
[1mStep[0m  [8/42], [94mLoss[0m : 2.42383
[1mStep[0m  [12/42], [94mLoss[0m : 2.38377
[1mStep[0m  [16/42], [94mLoss[0m : 2.25538
[1mStep[0m  [20/42], [94mLoss[0m : 2.30868
[1mStep[0m  [24/42], [94mLoss[0m : 2.16077
[1mStep[0m  [28/42], [94mLoss[0m : 2.21086
[1mStep[0m  [32/42], [94mLoss[0m : 2.32601
[1mStep[0m  [36/42], [94mLoss[0m : 2.27220
[1mStep[0m  [40/42], [94mLoss[0m : 2.36739

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.679, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35317
[1mStep[0m  [4/42], [94mLoss[0m : 2.29175
[1mStep[0m  [8/42], [94mLoss[0m : 2.27891
[1mStep[0m  [12/42], [94mLoss[0m : 2.35773
[1mStep[0m  [16/42], [94mLoss[0m : 2.34146
[1mStep[0m  [20/42], [94mLoss[0m : 2.58681
[1mStep[0m  [24/42], [94mLoss[0m : 2.44895
[1mStep[0m  [28/42], [94mLoss[0m : 2.14534
[1mStep[0m  [32/42], [94mLoss[0m : 2.03198
[1mStep[0m  [36/42], [94mLoss[0m : 2.30742
[1mStep[0m  [40/42], [94mLoss[0m : 2.05512

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.627, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40633
[1mStep[0m  [4/42], [94mLoss[0m : 2.44618
[1mStep[0m  [8/42], [94mLoss[0m : 2.36457
[1mStep[0m  [12/42], [94mLoss[0m : 2.17145
[1mStep[0m  [16/42], [94mLoss[0m : 2.41835
[1mStep[0m  [20/42], [94mLoss[0m : 2.32786
[1mStep[0m  [24/42], [94mLoss[0m : 2.18225
[1mStep[0m  [28/42], [94mLoss[0m : 1.99848
[1mStep[0m  [32/42], [94mLoss[0m : 2.32836
[1mStep[0m  [36/42], [94mLoss[0m : 2.30701
[1mStep[0m  [40/42], [94mLoss[0m : 2.19368

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.628, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.35790
[1mStep[0m  [4/42], [94mLoss[0m : 2.42817
[1mStep[0m  [8/42], [94mLoss[0m : 2.33267
[1mStep[0m  [12/42], [94mLoss[0m : 2.48998
[1mStep[0m  [16/42], [94mLoss[0m : 2.31562
[1mStep[0m  [20/42], [94mLoss[0m : 2.26757
[1mStep[0m  [24/42], [94mLoss[0m : 2.17122
[1mStep[0m  [28/42], [94mLoss[0m : 2.53074
[1mStep[0m  [32/42], [94mLoss[0m : 2.27443
[1mStep[0m  [36/42], [94mLoss[0m : 2.54543
[1mStep[0m  [40/42], [94mLoss[0m : 2.39082

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.613, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46288
[1mStep[0m  [4/42], [94mLoss[0m : 2.11235
[1mStep[0m  [8/42], [94mLoss[0m : 2.23816
[1mStep[0m  [12/42], [94mLoss[0m : 2.00814
[1mStep[0m  [16/42], [94mLoss[0m : 2.17790
[1mStep[0m  [20/42], [94mLoss[0m : 2.29250
[1mStep[0m  [24/42], [94mLoss[0m : 2.23846
[1mStep[0m  [28/42], [94mLoss[0m : 2.36595
[1mStep[0m  [32/42], [94mLoss[0m : 2.35847
[1mStep[0m  [36/42], [94mLoss[0m : 2.25004
[1mStep[0m  [40/42], [94mLoss[0m : 2.29821

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.676, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.28823
[1mStep[0m  [4/42], [94mLoss[0m : 2.49788
[1mStep[0m  [8/42], [94mLoss[0m : 2.44957
[1mStep[0m  [12/42], [94mLoss[0m : 2.18810
[1mStep[0m  [16/42], [94mLoss[0m : 2.10554
[1mStep[0m  [20/42], [94mLoss[0m : 2.11953
[1mStep[0m  [24/42], [94mLoss[0m : 2.38260
[1mStep[0m  [28/42], [94mLoss[0m : 2.32612
[1mStep[0m  [32/42], [94mLoss[0m : 2.44217
[1mStep[0m  [36/42], [94mLoss[0m : 2.17946
[1mStep[0m  [40/42], [94mLoss[0m : 2.35495

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.635, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31302
[1mStep[0m  [4/42], [94mLoss[0m : 2.26561
[1mStep[0m  [8/42], [94mLoss[0m : 2.38901
[1mStep[0m  [12/42], [94mLoss[0m : 2.35028
[1mStep[0m  [16/42], [94mLoss[0m : 2.49274
[1mStep[0m  [20/42], [94mLoss[0m : 2.31873
[1mStep[0m  [24/42], [94mLoss[0m : 2.60484
[1mStep[0m  [28/42], [94mLoss[0m : 2.22541
[1mStep[0m  [32/42], [94mLoss[0m : 2.25024
[1mStep[0m  [36/42], [94mLoss[0m : 2.49384
[1mStep[0m  [40/42], [94mLoss[0m : 2.28495

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36821
[1mStep[0m  [4/42], [94mLoss[0m : 2.34280
[1mStep[0m  [8/42], [94mLoss[0m : 2.18005
[1mStep[0m  [12/42], [94mLoss[0m : 2.17646
[1mStep[0m  [16/42], [94mLoss[0m : 2.27831
[1mStep[0m  [20/42], [94mLoss[0m : 2.51235
[1mStep[0m  [24/42], [94mLoss[0m : 2.01629
[1mStep[0m  [28/42], [94mLoss[0m : 2.55058
[1mStep[0m  [32/42], [94mLoss[0m : 2.09265
[1mStep[0m  [36/42], [94mLoss[0m : 2.28774
[1mStep[0m  [40/42], [94mLoss[0m : 2.45442

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14249
[1mStep[0m  [4/42], [94mLoss[0m : 2.13098
[1mStep[0m  [8/42], [94mLoss[0m : 2.38776
[1mStep[0m  [12/42], [94mLoss[0m : 2.33460
[1mStep[0m  [16/42], [94mLoss[0m : 2.14940
[1mStep[0m  [20/42], [94mLoss[0m : 2.34740
[1mStep[0m  [24/42], [94mLoss[0m : 2.43858
[1mStep[0m  [28/42], [94mLoss[0m : 2.21234
[1mStep[0m  [32/42], [94mLoss[0m : 2.48843
[1mStep[0m  [36/42], [94mLoss[0m : 2.10929
[1mStep[0m  [40/42], [94mLoss[0m : 2.28797

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.584, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37962
[1mStep[0m  [4/42], [94mLoss[0m : 2.25991
[1mStep[0m  [8/42], [94mLoss[0m : 2.06592
[1mStep[0m  [12/42], [94mLoss[0m : 2.12556
[1mStep[0m  [16/42], [94mLoss[0m : 2.34861
[1mStep[0m  [20/42], [94mLoss[0m : 2.06719
[1mStep[0m  [24/42], [94mLoss[0m : 2.16206
[1mStep[0m  [28/42], [94mLoss[0m : 2.27479
[1mStep[0m  [32/42], [94mLoss[0m : 2.15661
[1mStep[0m  [36/42], [94mLoss[0m : 2.29228
[1mStep[0m  [40/42], [94mLoss[0m : 2.24828

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.511, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24492
[1mStep[0m  [4/42], [94mLoss[0m : 2.36509
[1mStep[0m  [8/42], [94mLoss[0m : 2.34582
[1mStep[0m  [12/42], [94mLoss[0m : 2.33949
[1mStep[0m  [16/42], [94mLoss[0m : 2.36905
[1mStep[0m  [20/42], [94mLoss[0m : 2.08915
[1mStep[0m  [24/42], [94mLoss[0m : 2.25317
[1mStep[0m  [28/42], [94mLoss[0m : 2.36199
[1mStep[0m  [32/42], [94mLoss[0m : 2.03516
[1mStep[0m  [36/42], [94mLoss[0m : 2.08097
[1mStep[0m  [40/42], [94mLoss[0m : 2.16283

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.567, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.539
====================================

Phase 2 - Evaluation MAE:  2.538934792791094
MAE score P1      2.567993
MAE score P2      2.538935
loss              2.271551
learning_rate       0.0001
batch_size             256
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 25, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.09219
[1mStep[0m  [8/84], [94mLoss[0m : 10.44427
[1mStep[0m  [16/84], [94mLoss[0m : 11.17915
[1mStep[0m  [24/84], [94mLoss[0m : 10.56775
[1mStep[0m  [32/84], [94mLoss[0m : 10.73274
[1mStep[0m  [40/84], [94mLoss[0m : 10.30980
[1mStep[0m  [48/84], [94mLoss[0m : 10.68631
[1mStep[0m  [56/84], [94mLoss[0m : 10.73350
[1mStep[0m  [64/84], [94mLoss[0m : 9.70387
[1mStep[0m  [72/84], [94mLoss[0m : 9.90113
[1mStep[0m  [80/84], [94mLoss[0m : 9.50075

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.304, [92mTest[0m: 10.846, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.47352
[1mStep[0m  [8/84], [94mLoss[0m : 10.27427
[1mStep[0m  [16/84], [94mLoss[0m : 9.00110
[1mStep[0m  [24/84], [94mLoss[0m : 8.86549
[1mStep[0m  [32/84], [94mLoss[0m : 8.75186
[1mStep[0m  [40/84], [94mLoss[0m : 9.52445
[1mStep[0m  [48/84], [94mLoss[0m : 9.13990
[1mStep[0m  [56/84], [94mLoss[0m : 8.70425
[1mStep[0m  [64/84], [94mLoss[0m : 9.06278
[1mStep[0m  [72/84], [94mLoss[0m : 8.81257
[1mStep[0m  [80/84], [94mLoss[0m : 9.00268

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.945, [92mTest[0m: 9.997, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.78034
[1mStep[0m  [8/84], [94mLoss[0m : 7.98545
[1mStep[0m  [16/84], [94mLoss[0m : 8.13293
[1mStep[0m  [24/84], [94mLoss[0m : 8.15817
[1mStep[0m  [32/84], [94mLoss[0m : 7.60111
[1mStep[0m  [40/84], [94mLoss[0m : 8.10484
[1mStep[0m  [48/84], [94mLoss[0m : 7.32282
[1mStep[0m  [56/84], [94mLoss[0m : 7.67044
[1mStep[0m  [64/84], [94mLoss[0m : 7.25027
[1mStep[0m  [72/84], [94mLoss[0m : 7.08783
[1mStep[0m  [80/84], [94mLoss[0m : 6.58049

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.563, [92mTest[0m: 9.063, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.02083
[1mStep[0m  [8/84], [94mLoss[0m : 7.10388
[1mStep[0m  [16/84], [94mLoss[0m : 6.41642
[1mStep[0m  [24/84], [94mLoss[0m : 6.11191
[1mStep[0m  [32/84], [94mLoss[0m : 6.49031
[1mStep[0m  [40/84], [94mLoss[0m : 6.58506
[1mStep[0m  [48/84], [94mLoss[0m : 5.89191
[1mStep[0m  [56/84], [94mLoss[0m : 6.16702
[1mStep[0m  [64/84], [94mLoss[0m : 6.15132
[1mStep[0m  [72/84], [94mLoss[0m : 5.83140
[1mStep[0m  [80/84], [94mLoss[0m : 5.91165

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.212, [92mTest[0m: 8.110, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.89268
[1mStep[0m  [8/84], [94mLoss[0m : 5.27019
[1mStep[0m  [16/84], [94mLoss[0m : 5.72805
[1mStep[0m  [24/84], [94mLoss[0m : 5.39308
[1mStep[0m  [32/84], [94mLoss[0m : 4.61106
[1mStep[0m  [40/84], [94mLoss[0m : 4.92715
[1mStep[0m  [48/84], [94mLoss[0m : 4.59881
[1mStep[0m  [56/84], [94mLoss[0m : 5.02845
[1mStep[0m  [64/84], [94mLoss[0m : 5.06753
[1mStep[0m  [72/84], [94mLoss[0m : 4.71386
[1mStep[0m  [80/84], [94mLoss[0m : 4.05264

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.017, [92mTest[0m: 7.149, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.62747
[1mStep[0m  [8/84], [94mLoss[0m : 4.18974
[1mStep[0m  [16/84], [94mLoss[0m : 4.76028
[1mStep[0m  [24/84], [94mLoss[0m : 3.57448
[1mStep[0m  [32/84], [94mLoss[0m : 4.15610
[1mStep[0m  [40/84], [94mLoss[0m : 4.66816
[1mStep[0m  [48/84], [94mLoss[0m : 4.09927
[1mStep[0m  [56/84], [94mLoss[0m : 4.38286
[1mStep[0m  [64/84], [94mLoss[0m : 3.88617
[1mStep[0m  [72/84], [94mLoss[0m : 3.50326
[1mStep[0m  [80/84], [94mLoss[0m : 3.49772

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.066, [92mTest[0m: 6.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.80853
[1mStep[0m  [8/84], [94mLoss[0m : 3.49629
[1mStep[0m  [16/84], [94mLoss[0m : 3.53332
[1mStep[0m  [24/84], [94mLoss[0m : 3.29860
[1mStep[0m  [32/84], [94mLoss[0m : 3.49264
[1mStep[0m  [40/84], [94mLoss[0m : 3.53294
[1mStep[0m  [48/84], [94mLoss[0m : 3.54814
[1mStep[0m  [56/84], [94mLoss[0m : 3.65958
[1mStep[0m  [64/84], [94mLoss[0m : 2.96420
[1mStep[0m  [72/84], [94mLoss[0m : 3.80365
[1mStep[0m  [80/84], [94mLoss[0m : 3.29218

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.438, [92mTest[0m: 4.984, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 6 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.316
====================================

Phase 1 - Evaluation MAE:  4.31553145817348
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 3.10322
[1mStep[0m  [8/84], [94mLoss[0m : 3.16581
[1mStep[0m  [16/84], [94mLoss[0m : 2.81855
[1mStep[0m  [24/84], [94mLoss[0m : 3.21586
[1mStep[0m  [32/84], [94mLoss[0m : 3.16636
[1mStep[0m  [40/84], [94mLoss[0m : 2.92187
[1mStep[0m  [48/84], [94mLoss[0m : 3.04858
[1mStep[0m  [56/84], [94mLoss[0m : 3.02379
[1mStep[0m  [64/84], [94mLoss[0m : 3.05328
[1mStep[0m  [72/84], [94mLoss[0m : 3.15546
[1mStep[0m  [80/84], [94mLoss[0m : 3.10184

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.096, [92mTest[0m: 4.313, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.871
====================================

Phase 2 - Evaluation MAE:  2.8710666554314748
MAE score P1        4.315531
MAE score P2        2.871067
loss                3.095904
learning_rate         0.0001
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 26, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.85551
[1mStep[0m  [8/84], [94mLoss[0m : 10.91436
[1mStep[0m  [16/84], [94mLoss[0m : 11.44780
[1mStep[0m  [24/84], [94mLoss[0m : 11.03571
[1mStep[0m  [32/84], [94mLoss[0m : 11.38496
[1mStep[0m  [40/84], [94mLoss[0m : 11.17710
[1mStep[0m  [48/84], [94mLoss[0m : 11.44571
[1mStep[0m  [56/84], [94mLoss[0m : 10.86920
[1mStep[0m  [64/84], [94mLoss[0m : 10.78709
[1mStep[0m  [72/84], [94mLoss[0m : 10.63049
[1mStep[0m  [80/84], [94mLoss[0m : 10.56580

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.878, [92mTest[0m: 10.938, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 11.36220
[1mStep[0m  [8/84], [94mLoss[0m : 10.63318
[1mStep[0m  [16/84], [94mLoss[0m : 10.20840
[1mStep[0m  [24/84], [94mLoss[0m : 10.08673
[1mStep[0m  [32/84], [94mLoss[0m : 10.91876
[1mStep[0m  [40/84], [94mLoss[0m : 10.31773
[1mStep[0m  [48/84], [94mLoss[0m : 10.68682
[1mStep[0m  [56/84], [94mLoss[0m : 10.60946
[1mStep[0m  [64/84], [94mLoss[0m : 10.65056
[1mStep[0m  [72/84], [94mLoss[0m : 10.49230
[1mStep[0m  [80/84], [94mLoss[0m : 9.76135

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.516, [92mTest[0m: 10.723, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.78215
[1mStep[0m  [8/84], [94mLoss[0m : 9.69697
[1mStep[0m  [16/84], [94mLoss[0m : 9.95154
[1mStep[0m  [24/84], [94mLoss[0m : 10.77609
[1mStep[0m  [32/84], [94mLoss[0m : 10.27621
[1mStep[0m  [40/84], [94mLoss[0m : 10.36965
[1mStep[0m  [48/84], [94mLoss[0m : 10.00617
[1mStep[0m  [56/84], [94mLoss[0m : 10.30234
[1mStep[0m  [64/84], [94mLoss[0m : 10.44639
[1mStep[0m  [72/84], [94mLoss[0m : 9.87615
[1mStep[0m  [80/84], [94mLoss[0m : 10.38921

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.160, [92mTest[0m: 10.419, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.52279
[1mStep[0m  [8/84], [94mLoss[0m : 10.40251
[1mStep[0m  [16/84], [94mLoss[0m : 9.69128
[1mStep[0m  [24/84], [94mLoss[0m : 10.27932
[1mStep[0m  [32/84], [94mLoss[0m : 9.69928
[1mStep[0m  [40/84], [94mLoss[0m : 9.30070
[1mStep[0m  [48/84], [94mLoss[0m : 9.71806
[1mStep[0m  [56/84], [94mLoss[0m : 10.52699
[1mStep[0m  [64/84], [94mLoss[0m : 10.33607
[1mStep[0m  [72/84], [94mLoss[0m : 9.28641
[1mStep[0m  [80/84], [94mLoss[0m : 10.26598

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.793, [92mTest[0m: 10.129, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.85719
[1mStep[0m  [8/84], [94mLoss[0m : 9.51590
[1mStep[0m  [16/84], [94mLoss[0m : 9.22961
[1mStep[0m  [24/84], [94mLoss[0m : 9.37886
[1mStep[0m  [32/84], [94mLoss[0m : 9.96277
[1mStep[0m  [40/84], [94mLoss[0m : 9.52897
[1mStep[0m  [48/84], [94mLoss[0m : 9.51418
[1mStep[0m  [56/84], [94mLoss[0m : 9.88490
[1mStep[0m  [64/84], [94mLoss[0m : 9.19892
[1mStep[0m  [72/84], [94mLoss[0m : 9.45293
[1mStep[0m  [80/84], [94mLoss[0m : 9.22552

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.431, [92mTest[0m: 9.841, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.16703
[1mStep[0m  [8/84], [94mLoss[0m : 9.24188
[1mStep[0m  [16/84], [94mLoss[0m : 8.99876
[1mStep[0m  [24/84], [94mLoss[0m : 8.79188
[1mStep[0m  [32/84], [94mLoss[0m : 8.99088
[1mStep[0m  [40/84], [94mLoss[0m : 9.46739
[1mStep[0m  [48/84], [94mLoss[0m : 8.98490
[1mStep[0m  [56/84], [94mLoss[0m : 8.84404
[1mStep[0m  [64/84], [94mLoss[0m : 9.05315
[1mStep[0m  [72/84], [94mLoss[0m : 9.16516
[1mStep[0m  [80/84], [94mLoss[0m : 8.87188

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.083, [92mTest[0m: 9.550, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.73149
[1mStep[0m  [8/84], [94mLoss[0m : 9.16889
[1mStep[0m  [16/84], [94mLoss[0m : 9.29453
[1mStep[0m  [24/84], [94mLoss[0m : 9.05268
[1mStep[0m  [32/84], [94mLoss[0m : 8.40796
[1mStep[0m  [40/84], [94mLoss[0m : 9.08202
[1mStep[0m  [48/84], [94mLoss[0m : 8.88673
[1mStep[0m  [56/84], [94mLoss[0m : 8.83106
[1mStep[0m  [64/84], [94mLoss[0m : 8.69386
[1mStep[0m  [72/84], [94mLoss[0m : 9.09777
[1mStep[0m  [80/84], [94mLoss[0m : 8.30291

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.712, [92mTest[0m: 9.262, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.30182
[1mStep[0m  [8/84], [94mLoss[0m : 8.37984
[1mStep[0m  [16/84], [94mLoss[0m : 8.24218
[1mStep[0m  [24/84], [94mLoss[0m : 8.42805
[1mStep[0m  [32/84], [94mLoss[0m : 8.38966
[1mStep[0m  [40/84], [94mLoss[0m : 8.66978
[1mStep[0m  [48/84], [94mLoss[0m : 8.39644
[1mStep[0m  [56/84], [94mLoss[0m : 8.32990
[1mStep[0m  [64/84], [94mLoss[0m : 8.38342
[1mStep[0m  [72/84], [94mLoss[0m : 7.79869
[1mStep[0m  [80/84], [94mLoss[0m : 8.41154

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.351, [92mTest[0m: 8.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.85147
[1mStep[0m  [8/84], [94mLoss[0m : 8.57946
[1mStep[0m  [16/84], [94mLoss[0m : 7.74269
[1mStep[0m  [24/84], [94mLoss[0m : 7.59147
[1mStep[0m  [32/84], [94mLoss[0m : 8.00651
[1mStep[0m  [40/84], [94mLoss[0m : 8.46337
[1mStep[0m  [48/84], [94mLoss[0m : 8.08152
[1mStep[0m  [56/84], [94mLoss[0m : 7.56575
[1mStep[0m  [64/84], [94mLoss[0m : 8.03616
[1mStep[0m  [72/84], [94mLoss[0m : 7.45036
[1mStep[0m  [80/84], [94mLoss[0m : 8.26637

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.989, [92mTest[0m: 8.679, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.36416
[1mStep[0m  [8/84], [94mLoss[0m : 7.35703
[1mStep[0m  [16/84], [94mLoss[0m : 7.45634
[1mStep[0m  [24/84], [94mLoss[0m : 7.46501
[1mStep[0m  [32/84], [94mLoss[0m : 7.78437
[1mStep[0m  [40/84], [94mLoss[0m : 7.77760
[1mStep[0m  [48/84], [94mLoss[0m : 7.04173
[1mStep[0m  [56/84], [94mLoss[0m : 7.34038
[1mStep[0m  [64/84], [94mLoss[0m : 7.10845
[1mStep[0m  [72/84], [94mLoss[0m : 8.24336
[1mStep[0m  [80/84], [94mLoss[0m : 7.36773

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.644, [92mTest[0m: 8.384, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.14438
[1mStep[0m  [8/84], [94mLoss[0m : 7.57398
[1mStep[0m  [16/84], [94mLoss[0m : 6.72931
[1mStep[0m  [24/84], [94mLoss[0m : 7.97067
[1mStep[0m  [32/84], [94mLoss[0m : 7.71917
[1mStep[0m  [40/84], [94mLoss[0m : 7.10876
[1mStep[0m  [48/84], [94mLoss[0m : 7.35193
[1mStep[0m  [56/84], [94mLoss[0m : 7.48623
[1mStep[0m  [64/84], [94mLoss[0m : 7.96384
[1mStep[0m  [72/84], [94mLoss[0m : 6.85250
[1mStep[0m  [80/84], [94mLoss[0m : 6.86100

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.298, [92mTest[0m: 8.066, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.59487
[1mStep[0m  [8/84], [94mLoss[0m : 7.59789
[1mStep[0m  [16/84], [94mLoss[0m : 7.52131
[1mStep[0m  [24/84], [94mLoss[0m : 7.15329
[1mStep[0m  [32/84], [94mLoss[0m : 7.12568
[1mStep[0m  [40/84], [94mLoss[0m : 7.16607
[1mStep[0m  [48/84], [94mLoss[0m : 7.03784
[1mStep[0m  [56/84], [94mLoss[0m : 7.13354
[1mStep[0m  [64/84], [94mLoss[0m : 6.57562
[1mStep[0m  [72/84], [94mLoss[0m : 7.01767
[1mStep[0m  [80/84], [94mLoss[0m : 6.39854

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.969, [92mTest[0m: 7.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.05610
[1mStep[0m  [8/84], [94mLoss[0m : 6.83460
[1mStep[0m  [16/84], [94mLoss[0m : 6.47061
[1mStep[0m  [24/84], [94mLoss[0m : 6.68914
[1mStep[0m  [32/84], [94mLoss[0m : 6.76758
[1mStep[0m  [40/84], [94mLoss[0m : 7.28618
[1mStep[0m  [48/84], [94mLoss[0m : 6.36468
[1mStep[0m  [56/84], [94mLoss[0m : 6.79761
[1mStep[0m  [64/84], [94mLoss[0m : 6.63591
[1mStep[0m  [72/84], [94mLoss[0m : 7.20832
[1mStep[0m  [80/84], [94mLoss[0m : 5.96726

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.654, [92mTest[0m: 7.499, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.25742
[1mStep[0m  [8/84], [94mLoss[0m : 6.41195
[1mStep[0m  [16/84], [94mLoss[0m : 6.30227
[1mStep[0m  [24/84], [94mLoss[0m : 6.42031
[1mStep[0m  [32/84], [94mLoss[0m : 6.60943
[1mStep[0m  [40/84], [94mLoss[0m : 6.91763
[1mStep[0m  [48/84], [94mLoss[0m : 6.37586
[1mStep[0m  [56/84], [94mLoss[0m : 6.51723
[1mStep[0m  [64/84], [94mLoss[0m : 6.61724
[1mStep[0m  [72/84], [94mLoss[0m : 6.01984
[1mStep[0m  [80/84], [94mLoss[0m : 6.40310

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.350, [92mTest[0m: 7.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.11033
[1mStep[0m  [8/84], [94mLoss[0m : 5.87261
[1mStep[0m  [16/84], [94mLoss[0m : 6.53486
[1mStep[0m  [24/84], [94mLoss[0m : 5.75702
[1mStep[0m  [32/84], [94mLoss[0m : 5.77970
[1mStep[0m  [40/84], [94mLoss[0m : 6.09591
[1mStep[0m  [48/84], [94mLoss[0m : 6.21560
[1mStep[0m  [56/84], [94mLoss[0m : 5.52742
[1mStep[0m  [64/84], [94mLoss[0m : 5.56381
[1mStep[0m  [72/84], [94mLoss[0m : 5.97764
[1mStep[0m  [80/84], [94mLoss[0m : 5.80200

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.057, [92mTest[0m: 7.024, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.38649
[1mStep[0m  [8/84], [94mLoss[0m : 5.86755
[1mStep[0m  [16/84], [94mLoss[0m : 5.74118
[1mStep[0m  [24/84], [94mLoss[0m : 5.97810
[1mStep[0m  [32/84], [94mLoss[0m : 5.47745
[1mStep[0m  [40/84], [94mLoss[0m : 5.61154
[1mStep[0m  [48/84], [94mLoss[0m : 5.94283
[1mStep[0m  [56/84], [94mLoss[0m : 5.79576
[1mStep[0m  [64/84], [94mLoss[0m : 5.34016
[1mStep[0m  [72/84], [94mLoss[0m : 5.55079
[1mStep[0m  [80/84], [94mLoss[0m : 5.75426

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.783, [92mTest[0m: 6.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.03819
[1mStep[0m  [8/84], [94mLoss[0m : 5.88618
[1mStep[0m  [16/84], [94mLoss[0m : 5.62268
[1mStep[0m  [24/84], [94mLoss[0m : 5.57650
[1mStep[0m  [32/84], [94mLoss[0m : 5.77279
[1mStep[0m  [40/84], [94mLoss[0m : 5.83939
[1mStep[0m  [48/84], [94mLoss[0m : 5.64412
[1mStep[0m  [56/84], [94mLoss[0m : 5.32830
[1mStep[0m  [64/84], [94mLoss[0m : 5.84160
[1mStep[0m  [72/84], [94mLoss[0m : 5.68543
[1mStep[0m  [80/84], [94mLoss[0m : 5.52337

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.518, [92mTest[0m: 6.455, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.17894
[1mStep[0m  [8/84], [94mLoss[0m : 5.34935
[1mStep[0m  [16/84], [94mLoss[0m : 4.88786
[1mStep[0m  [24/84], [94mLoss[0m : 4.76024
[1mStep[0m  [32/84], [94mLoss[0m : 4.88924
[1mStep[0m  [40/84], [94mLoss[0m : 5.32277
[1mStep[0m  [48/84], [94mLoss[0m : 5.67175
[1mStep[0m  [56/84], [94mLoss[0m : 5.04405
[1mStep[0m  [64/84], [94mLoss[0m : 5.56236
[1mStep[0m  [72/84], [94mLoss[0m : 4.91299
[1mStep[0m  [80/84], [94mLoss[0m : 5.41489

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.259, [92mTest[0m: 6.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.50891
[1mStep[0m  [8/84], [94mLoss[0m : 4.63764
[1mStep[0m  [16/84], [94mLoss[0m : 4.99574
[1mStep[0m  [24/84], [94mLoss[0m : 4.17090
[1mStep[0m  [32/84], [94mLoss[0m : 5.22643
[1mStep[0m  [40/84], [94mLoss[0m : 5.00795
[1mStep[0m  [48/84], [94mLoss[0m : 4.55155
[1mStep[0m  [56/84], [94mLoss[0m : 5.43758
[1mStep[0m  [64/84], [94mLoss[0m : 4.91033
[1mStep[0m  [72/84], [94mLoss[0m : 4.74081
[1mStep[0m  [80/84], [94mLoss[0m : 4.73955

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.020, [92mTest[0m: 5.926, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.24808
[1mStep[0m  [8/84], [94mLoss[0m : 4.85745
[1mStep[0m  [16/84], [94mLoss[0m : 4.76497
[1mStep[0m  [24/84], [94mLoss[0m : 4.80806
[1mStep[0m  [32/84], [94mLoss[0m : 4.66523
[1mStep[0m  [40/84], [94mLoss[0m : 4.72856
[1mStep[0m  [48/84], [94mLoss[0m : 4.70880
[1mStep[0m  [56/84], [94mLoss[0m : 4.73950
[1mStep[0m  [64/84], [94mLoss[0m : 4.67910
[1mStep[0m  [72/84], [94mLoss[0m : 4.87300
[1mStep[0m  [80/84], [94mLoss[0m : 3.97509

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.794, [92mTest[0m: 5.705, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.50120
[1mStep[0m  [8/84], [94mLoss[0m : 4.86270
[1mStep[0m  [16/84], [94mLoss[0m : 3.92969
[1mStep[0m  [24/84], [94mLoss[0m : 4.72858
[1mStep[0m  [32/84], [94mLoss[0m : 5.31032
[1mStep[0m  [40/84], [94mLoss[0m : 4.39708
[1mStep[0m  [48/84], [94mLoss[0m : 5.01515
[1mStep[0m  [56/84], [94mLoss[0m : 5.16467
[1mStep[0m  [64/84], [94mLoss[0m : 4.92713
[1mStep[0m  [72/84], [94mLoss[0m : 4.60931
[1mStep[0m  [80/84], [94mLoss[0m : 3.94745

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.599, [92mTest[0m: 5.526, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.42106
[1mStep[0m  [8/84], [94mLoss[0m : 4.67684
[1mStep[0m  [16/84], [94mLoss[0m : 5.25517
[1mStep[0m  [24/84], [94mLoss[0m : 4.63310
[1mStep[0m  [32/84], [94mLoss[0m : 4.62997
[1mStep[0m  [40/84], [94mLoss[0m : 4.10481
[1mStep[0m  [48/84], [94mLoss[0m : 4.54851
[1mStep[0m  [56/84], [94mLoss[0m : 4.58179
[1mStep[0m  [64/84], [94mLoss[0m : 4.03664
[1mStep[0m  [72/84], [94mLoss[0m : 4.09168
[1mStep[0m  [80/84], [94mLoss[0m : 4.05254

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.417, [92mTest[0m: 5.270, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.07056
[1mStep[0m  [8/84], [94mLoss[0m : 3.75862
[1mStep[0m  [16/84], [94mLoss[0m : 4.28413
[1mStep[0m  [24/84], [94mLoss[0m : 4.32558
[1mStep[0m  [32/84], [94mLoss[0m : 4.20475
[1mStep[0m  [40/84], [94mLoss[0m : 4.60962
[1mStep[0m  [48/84], [94mLoss[0m : 3.98483
[1mStep[0m  [56/84], [94mLoss[0m : 4.25015
[1mStep[0m  [64/84], [94mLoss[0m : 4.16039
[1mStep[0m  [72/84], [94mLoss[0m : 4.65875
[1mStep[0m  [80/84], [94mLoss[0m : 4.50139

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.234, [92mTest[0m: 5.102, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.85446
[1mStep[0m  [8/84], [94mLoss[0m : 4.63326
[1mStep[0m  [16/84], [94mLoss[0m : 4.40338
[1mStep[0m  [24/84], [94mLoss[0m : 4.02283
[1mStep[0m  [32/84], [94mLoss[0m : 4.01612
[1mStep[0m  [40/84], [94mLoss[0m : 3.92947
[1mStep[0m  [48/84], [94mLoss[0m : 4.40553
[1mStep[0m  [56/84], [94mLoss[0m : 3.95628
[1mStep[0m  [64/84], [94mLoss[0m : 3.85846
[1mStep[0m  [72/84], [94mLoss[0m : 4.35770
[1mStep[0m  [80/84], [94mLoss[0m : 3.73497

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.085, [92mTest[0m: 4.967, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.11541
[1mStep[0m  [8/84], [94mLoss[0m : 3.89364
[1mStep[0m  [16/84], [94mLoss[0m : 3.97038
[1mStep[0m  [24/84], [94mLoss[0m : 4.26988
[1mStep[0m  [32/84], [94mLoss[0m : 3.96329
[1mStep[0m  [40/84], [94mLoss[0m : 4.17002
[1mStep[0m  [48/84], [94mLoss[0m : 3.64330
[1mStep[0m  [56/84], [94mLoss[0m : 3.90669
[1mStep[0m  [64/84], [94mLoss[0m : 3.93954
[1mStep[0m  [72/84], [94mLoss[0m : 3.90097
[1mStep[0m  [80/84], [94mLoss[0m : 3.93534

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.943, [92mTest[0m: 4.725, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.63474
[1mStep[0m  [8/84], [94mLoss[0m : 3.53800
[1mStep[0m  [16/84], [94mLoss[0m : 3.58811
[1mStep[0m  [24/84], [94mLoss[0m : 3.52381
[1mStep[0m  [32/84], [94mLoss[0m : 3.58559
[1mStep[0m  [40/84], [94mLoss[0m : 4.19635
[1mStep[0m  [48/84], [94mLoss[0m : 3.96333
[1mStep[0m  [56/84], [94mLoss[0m : 3.59446
[1mStep[0m  [64/84], [94mLoss[0m : 3.49582
[1mStep[0m  [72/84], [94mLoss[0m : 3.85215
[1mStep[0m  [80/84], [94mLoss[0m : 3.99980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.781, [92mTest[0m: 4.536, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.95830
[1mStep[0m  [8/84], [94mLoss[0m : 3.30469
[1mStep[0m  [16/84], [94mLoss[0m : 3.80871
[1mStep[0m  [24/84], [94mLoss[0m : 4.41955
[1mStep[0m  [32/84], [94mLoss[0m : 4.23318
[1mStep[0m  [40/84], [94mLoss[0m : 4.00168
[1mStep[0m  [48/84], [94mLoss[0m : 4.34658
[1mStep[0m  [56/84], [94mLoss[0m : 3.85639
[1mStep[0m  [64/84], [94mLoss[0m : 3.38026
[1mStep[0m  [72/84], [94mLoss[0m : 3.33116
[1mStep[0m  [80/84], [94mLoss[0m : 3.12230

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.680, [92mTest[0m: 4.419, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.62490
[1mStep[0m  [8/84], [94mLoss[0m : 3.77697
[1mStep[0m  [16/84], [94mLoss[0m : 3.39013
[1mStep[0m  [24/84], [94mLoss[0m : 3.61773
[1mStep[0m  [32/84], [94mLoss[0m : 3.88750
[1mStep[0m  [40/84], [94mLoss[0m : 3.83744
[1mStep[0m  [48/84], [94mLoss[0m : 3.70146
[1mStep[0m  [56/84], [94mLoss[0m : 3.49076
[1mStep[0m  [64/84], [94mLoss[0m : 3.69176
[1mStep[0m  [72/84], [94mLoss[0m : 2.99651
[1mStep[0m  [80/84], [94mLoss[0m : 3.36809

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.544, [92mTest[0m: 4.204, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.74858
[1mStep[0m  [8/84], [94mLoss[0m : 3.06042
[1mStep[0m  [16/84], [94mLoss[0m : 3.52989
[1mStep[0m  [24/84], [94mLoss[0m : 3.40560
[1mStep[0m  [32/84], [94mLoss[0m : 3.49304
[1mStep[0m  [40/84], [94mLoss[0m : 2.94099
[1mStep[0m  [48/84], [94mLoss[0m : 3.52366
[1mStep[0m  [56/84], [94mLoss[0m : 3.84843
[1mStep[0m  [64/84], [94mLoss[0m : 3.04801
[1mStep[0m  [72/84], [94mLoss[0m : 3.23406
[1mStep[0m  [80/84], [94mLoss[0m : 3.21784

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.439, [92mTest[0m: 4.084, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.48384
[1mStep[0m  [8/84], [94mLoss[0m : 3.21905
[1mStep[0m  [16/84], [94mLoss[0m : 3.72907
[1mStep[0m  [24/84], [94mLoss[0m : 2.93700
[1mStep[0m  [32/84], [94mLoss[0m : 3.42958
[1mStep[0m  [40/84], [94mLoss[0m : 3.38288
[1mStep[0m  [48/84], [94mLoss[0m : 3.16496
[1mStep[0m  [56/84], [94mLoss[0m : 3.30800
[1mStep[0m  [64/84], [94mLoss[0m : 3.31324
[1mStep[0m  [72/84], [94mLoss[0m : 3.47007
[1mStep[0m  [80/84], [94mLoss[0m : 3.51896

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.344, [92mTest[0m: 3.943, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.799
====================================

Phase 1 - Evaluation MAE:  3.798631719180516
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 3.46184
[1mStep[0m  [8/84], [94mLoss[0m : 3.58318
[1mStep[0m  [16/84], [94mLoss[0m : 3.35543
[1mStep[0m  [24/84], [94mLoss[0m : 2.86631
[1mStep[0m  [32/84], [94mLoss[0m : 3.24122
[1mStep[0m  [40/84], [94mLoss[0m : 3.25814
[1mStep[0m  [48/84], [94mLoss[0m : 3.55295
[1mStep[0m  [56/84], [94mLoss[0m : 3.12993
[1mStep[0m  [64/84], [94mLoss[0m : 2.61101
[1mStep[0m  [72/84], [94mLoss[0m : 3.38028
[1mStep[0m  [80/84], [94mLoss[0m : 3.17448

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.236, [92mTest[0m: 3.802, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.24967
[1mStep[0m  [8/84], [94mLoss[0m : 3.16119
[1mStep[0m  [16/84], [94mLoss[0m : 3.61667
[1mStep[0m  [24/84], [94mLoss[0m : 3.27404
[1mStep[0m  [32/84], [94mLoss[0m : 3.45669
[1mStep[0m  [40/84], [94mLoss[0m : 2.88805
[1mStep[0m  [48/84], [94mLoss[0m : 3.08357
[1mStep[0m  [56/84], [94mLoss[0m : 2.93218
[1mStep[0m  [64/84], [94mLoss[0m : 2.82804
[1mStep[0m  [72/84], [94mLoss[0m : 3.08151
[1mStep[0m  [80/84], [94mLoss[0m : 3.41080

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.122, [92mTest[0m: 3.596, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.17096
[1mStep[0m  [8/84], [94mLoss[0m : 2.94188
[1mStep[0m  [16/84], [94mLoss[0m : 2.77055
[1mStep[0m  [24/84], [94mLoss[0m : 3.08052
[1mStep[0m  [32/84], [94mLoss[0m : 2.89319
[1mStep[0m  [40/84], [94mLoss[0m : 3.14871
[1mStep[0m  [48/84], [94mLoss[0m : 2.78898
[1mStep[0m  [56/84], [94mLoss[0m : 3.19592
[1mStep[0m  [64/84], [94mLoss[0m : 3.10130
[1mStep[0m  [72/84], [94mLoss[0m : 3.14488
[1mStep[0m  [80/84], [94mLoss[0m : 3.02969

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.038, [92mTest[0m: 3.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.29152
[1mStep[0m  [8/84], [94mLoss[0m : 3.06544
[1mStep[0m  [16/84], [94mLoss[0m : 2.72059
[1mStep[0m  [24/84], [94mLoss[0m : 2.78292
[1mStep[0m  [32/84], [94mLoss[0m : 3.23509
[1mStep[0m  [40/84], [94mLoss[0m : 2.64116
[1mStep[0m  [48/84], [94mLoss[0m : 3.05990
[1mStep[0m  [56/84], [94mLoss[0m : 2.49591
[1mStep[0m  [64/84], [94mLoss[0m : 3.32205
[1mStep[0m  [72/84], [94mLoss[0m : 2.86724
[1mStep[0m  [80/84], [94mLoss[0m : 2.80519

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.957, [92mTest[0m: 3.293, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99446
[1mStep[0m  [8/84], [94mLoss[0m : 3.18301
[1mStep[0m  [16/84], [94mLoss[0m : 2.82846
[1mStep[0m  [24/84], [94mLoss[0m : 2.77737
[1mStep[0m  [32/84], [94mLoss[0m : 2.75966
[1mStep[0m  [40/84], [94mLoss[0m : 2.64573
[1mStep[0m  [48/84], [94mLoss[0m : 2.57283
[1mStep[0m  [56/84], [94mLoss[0m : 3.02712
[1mStep[0m  [64/84], [94mLoss[0m : 3.17146
[1mStep[0m  [72/84], [94mLoss[0m : 2.92061
[1mStep[0m  [80/84], [94mLoss[0m : 2.55163

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.915, [92mTest[0m: 3.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.19981
[1mStep[0m  [8/84], [94mLoss[0m : 2.78953
[1mStep[0m  [16/84], [94mLoss[0m : 2.71558
[1mStep[0m  [24/84], [94mLoss[0m : 2.91444
[1mStep[0m  [32/84], [94mLoss[0m : 3.04745
[1mStep[0m  [40/84], [94mLoss[0m : 2.25770
[1mStep[0m  [48/84], [94mLoss[0m : 2.76587
[1mStep[0m  [56/84], [94mLoss[0m : 2.87366
[1mStep[0m  [64/84], [94mLoss[0m : 2.91019
[1mStep[0m  [72/84], [94mLoss[0m : 2.55098
[1mStep[0m  [80/84], [94mLoss[0m : 2.86093

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.860, [92mTest[0m: 3.047, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52247
[1mStep[0m  [8/84], [94mLoss[0m : 2.81595
[1mStep[0m  [16/84], [94mLoss[0m : 2.93550
[1mStep[0m  [24/84], [94mLoss[0m : 2.87064
[1mStep[0m  [32/84], [94mLoss[0m : 2.57916
[1mStep[0m  [40/84], [94mLoss[0m : 2.90033
[1mStep[0m  [48/84], [94mLoss[0m : 2.69983
[1mStep[0m  [56/84], [94mLoss[0m : 2.90415
[1mStep[0m  [64/84], [94mLoss[0m : 2.67354
[1mStep[0m  [72/84], [94mLoss[0m : 2.88040
[1mStep[0m  [80/84], [94mLoss[0m : 2.81357

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.797, [92mTest[0m: 2.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.20172
[1mStep[0m  [8/84], [94mLoss[0m : 2.88036
[1mStep[0m  [16/84], [94mLoss[0m : 2.85730
[1mStep[0m  [24/84], [94mLoss[0m : 2.77280
[1mStep[0m  [32/84], [94mLoss[0m : 2.51877
[1mStep[0m  [40/84], [94mLoss[0m : 2.72586
[1mStep[0m  [48/84], [94mLoss[0m : 2.69225
[1mStep[0m  [56/84], [94mLoss[0m : 2.93356
[1mStep[0m  [64/84], [94mLoss[0m : 2.83275
[1mStep[0m  [72/84], [94mLoss[0m : 2.74238
[1mStep[0m  [80/84], [94mLoss[0m : 2.77836

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.779, [92mTest[0m: 2.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95313
[1mStep[0m  [8/84], [94mLoss[0m : 2.88930
[1mStep[0m  [16/84], [94mLoss[0m : 2.94440
[1mStep[0m  [24/84], [94mLoss[0m : 2.68014
[1mStep[0m  [32/84], [94mLoss[0m : 2.81935
[1mStep[0m  [40/84], [94mLoss[0m : 2.96581
[1mStep[0m  [48/84], [94mLoss[0m : 2.64572
[1mStep[0m  [56/84], [94mLoss[0m : 2.51790
[1mStep[0m  [64/84], [94mLoss[0m : 2.65063
[1mStep[0m  [72/84], [94mLoss[0m : 2.78023
[1mStep[0m  [80/84], [94mLoss[0m : 3.14532

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.799, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55347
[1mStep[0m  [8/84], [94mLoss[0m : 2.92317
[1mStep[0m  [16/84], [94mLoss[0m : 2.87948
[1mStep[0m  [24/84], [94mLoss[0m : 2.39112
[1mStep[0m  [32/84], [94mLoss[0m : 2.64793
[1mStep[0m  [40/84], [94mLoss[0m : 3.05422
[1mStep[0m  [48/84], [94mLoss[0m : 2.49753
[1mStep[0m  [56/84], [94mLoss[0m : 2.59627
[1mStep[0m  [64/84], [94mLoss[0m : 3.06524
[1mStep[0m  [72/84], [94mLoss[0m : 2.41722
[1mStep[0m  [80/84], [94mLoss[0m : 2.41087

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.723, [92mTest[0m: 2.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14128
[1mStep[0m  [8/84], [94mLoss[0m : 3.14116
[1mStep[0m  [16/84], [94mLoss[0m : 2.32233
[1mStep[0m  [24/84], [94mLoss[0m : 2.47553
[1mStep[0m  [32/84], [94mLoss[0m : 2.69320
[1mStep[0m  [40/84], [94mLoss[0m : 2.86001
[1mStep[0m  [48/84], [94mLoss[0m : 2.90459
[1mStep[0m  [56/84], [94mLoss[0m : 2.50757
[1mStep[0m  [64/84], [94mLoss[0m : 2.61183
[1mStep[0m  [72/84], [94mLoss[0m : 2.51193
[1mStep[0m  [80/84], [94mLoss[0m : 2.73761

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.692, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67150
[1mStep[0m  [8/84], [94mLoss[0m : 2.75547
[1mStep[0m  [16/84], [94mLoss[0m : 3.07588
[1mStep[0m  [24/84], [94mLoss[0m : 2.83575
[1mStep[0m  [32/84], [94mLoss[0m : 2.71432
[1mStep[0m  [40/84], [94mLoss[0m : 2.68648
[1mStep[0m  [48/84], [94mLoss[0m : 2.70934
[1mStep[0m  [56/84], [94mLoss[0m : 2.88583
[1mStep[0m  [64/84], [94mLoss[0m : 2.61401
[1mStep[0m  [72/84], [94mLoss[0m : 2.75394
[1mStep[0m  [80/84], [94mLoss[0m : 2.54635

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59665
[1mStep[0m  [8/84], [94mLoss[0m : 2.57152
[1mStep[0m  [16/84], [94mLoss[0m : 2.50909
[1mStep[0m  [24/84], [94mLoss[0m : 2.53562
[1mStep[0m  [32/84], [94mLoss[0m : 2.71403
[1mStep[0m  [40/84], [94mLoss[0m : 2.63724
[1mStep[0m  [48/84], [94mLoss[0m : 2.95480
[1mStep[0m  [56/84], [94mLoss[0m : 2.69219
[1mStep[0m  [64/84], [94mLoss[0m : 2.91725
[1mStep[0m  [72/84], [94mLoss[0m : 2.49774
[1mStep[0m  [80/84], [94mLoss[0m : 2.29784

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.633, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37675
[1mStep[0m  [8/84], [94mLoss[0m : 2.77823
[1mStep[0m  [16/84], [94mLoss[0m : 2.85407
[1mStep[0m  [24/84], [94mLoss[0m : 3.16750
[1mStep[0m  [32/84], [94mLoss[0m : 2.30228
[1mStep[0m  [40/84], [94mLoss[0m : 2.56939
[1mStep[0m  [48/84], [94mLoss[0m : 2.57517
[1mStep[0m  [56/84], [94mLoss[0m : 2.87708
[1mStep[0m  [64/84], [94mLoss[0m : 2.91504
[1mStep[0m  [72/84], [94mLoss[0m : 2.71925
[1mStep[0m  [80/84], [94mLoss[0m : 2.81794

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.604, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59966
[1mStep[0m  [8/84], [94mLoss[0m : 2.61321
[1mStep[0m  [16/84], [94mLoss[0m : 2.94619
[1mStep[0m  [24/84], [94mLoss[0m : 2.63285
[1mStep[0m  [32/84], [94mLoss[0m : 2.55785
[1mStep[0m  [40/84], [94mLoss[0m : 2.59319
[1mStep[0m  [48/84], [94mLoss[0m : 2.69370
[1mStep[0m  [56/84], [94mLoss[0m : 2.88250
[1mStep[0m  [64/84], [94mLoss[0m : 2.57322
[1mStep[0m  [72/84], [94mLoss[0m : 2.54816
[1mStep[0m  [80/84], [94mLoss[0m : 2.98915

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63108
[1mStep[0m  [8/84], [94mLoss[0m : 2.80760
[1mStep[0m  [16/84], [94mLoss[0m : 2.44864
[1mStep[0m  [24/84], [94mLoss[0m : 2.41111
[1mStep[0m  [32/84], [94mLoss[0m : 2.69023
[1mStep[0m  [40/84], [94mLoss[0m : 2.35526
[1mStep[0m  [48/84], [94mLoss[0m : 2.52739
[1mStep[0m  [56/84], [94mLoss[0m : 2.15725
[1mStep[0m  [64/84], [94mLoss[0m : 3.08677
[1mStep[0m  [72/84], [94mLoss[0m : 2.34113
[1mStep[0m  [80/84], [94mLoss[0m : 2.66723

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.573, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65047
[1mStep[0m  [8/84], [94mLoss[0m : 2.67479
[1mStep[0m  [16/84], [94mLoss[0m : 2.84881
[1mStep[0m  [24/84], [94mLoss[0m : 2.60989
[1mStep[0m  [32/84], [94mLoss[0m : 2.67843
[1mStep[0m  [40/84], [94mLoss[0m : 2.31354
[1mStep[0m  [48/84], [94mLoss[0m : 2.66042
[1mStep[0m  [56/84], [94mLoss[0m : 2.44694
[1mStep[0m  [64/84], [94mLoss[0m : 2.81449
[1mStep[0m  [72/84], [94mLoss[0m : 2.32425
[1mStep[0m  [80/84], [94mLoss[0m : 2.48047

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.576, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65086
[1mStep[0m  [8/84], [94mLoss[0m : 2.61562
[1mStep[0m  [16/84], [94mLoss[0m : 2.61314
[1mStep[0m  [24/84], [94mLoss[0m : 2.57438
[1mStep[0m  [32/84], [94mLoss[0m : 2.47894
[1mStep[0m  [40/84], [94mLoss[0m : 2.56549
[1mStep[0m  [48/84], [94mLoss[0m : 2.66328
[1mStep[0m  [56/84], [94mLoss[0m : 2.37057
[1mStep[0m  [64/84], [94mLoss[0m : 2.75693
[1mStep[0m  [72/84], [94mLoss[0m : 2.69927
[1mStep[0m  [80/84], [94mLoss[0m : 2.39691

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.574, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87882
[1mStep[0m  [8/84], [94mLoss[0m : 2.33220
[1mStep[0m  [16/84], [94mLoss[0m : 2.49101
[1mStep[0m  [24/84], [94mLoss[0m : 2.72480
[1mStep[0m  [32/84], [94mLoss[0m : 2.73471
[1mStep[0m  [40/84], [94mLoss[0m : 2.72110
[1mStep[0m  [48/84], [94mLoss[0m : 2.40655
[1mStep[0m  [56/84], [94mLoss[0m : 2.52224
[1mStep[0m  [64/84], [94mLoss[0m : 2.58040
[1mStep[0m  [72/84], [94mLoss[0m : 2.72254
[1mStep[0m  [80/84], [94mLoss[0m : 2.72989

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.572, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.08987
[1mStep[0m  [8/84], [94mLoss[0m : 2.47680
[1mStep[0m  [16/84], [94mLoss[0m : 2.73737
[1mStep[0m  [24/84], [94mLoss[0m : 2.21107
[1mStep[0m  [32/84], [94mLoss[0m : 2.38119
[1mStep[0m  [40/84], [94mLoss[0m : 2.56500
[1mStep[0m  [48/84], [94mLoss[0m : 2.42015
[1mStep[0m  [56/84], [94mLoss[0m : 2.73370
[1mStep[0m  [64/84], [94mLoss[0m : 2.25961
[1mStep[0m  [72/84], [94mLoss[0m : 2.89132
[1mStep[0m  [80/84], [94mLoss[0m : 2.91586

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.563, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.92719
[1mStep[0m  [8/84], [94mLoss[0m : 2.30065
[1mStep[0m  [16/84], [94mLoss[0m : 2.79298
[1mStep[0m  [24/84], [94mLoss[0m : 2.57366
[1mStep[0m  [32/84], [94mLoss[0m : 2.17212
[1mStep[0m  [40/84], [94mLoss[0m : 2.45309
[1mStep[0m  [48/84], [94mLoss[0m : 2.34017
[1mStep[0m  [56/84], [94mLoss[0m : 2.42460
[1mStep[0m  [64/84], [94mLoss[0m : 2.65016
[1mStep[0m  [72/84], [94mLoss[0m : 2.54605
[1mStep[0m  [80/84], [94mLoss[0m : 2.41975

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.551, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75562
[1mStep[0m  [8/84], [94mLoss[0m : 2.85752
[1mStep[0m  [16/84], [94mLoss[0m : 2.45799
[1mStep[0m  [24/84], [94mLoss[0m : 2.59198
[1mStep[0m  [32/84], [94mLoss[0m : 2.45528
[1mStep[0m  [40/84], [94mLoss[0m : 2.62611
[1mStep[0m  [48/84], [94mLoss[0m : 2.73741
[1mStep[0m  [56/84], [94mLoss[0m : 2.88844
[1mStep[0m  [64/84], [94mLoss[0m : 2.94575
[1mStep[0m  [72/84], [94mLoss[0m : 2.21025
[1mStep[0m  [80/84], [94mLoss[0m : 2.57592

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44296
[1mStep[0m  [8/84], [94mLoss[0m : 2.46934
[1mStep[0m  [16/84], [94mLoss[0m : 2.50147
[1mStep[0m  [24/84], [94mLoss[0m : 2.52599
[1mStep[0m  [32/84], [94mLoss[0m : 2.31138
[1mStep[0m  [40/84], [94mLoss[0m : 2.44847
[1mStep[0m  [48/84], [94mLoss[0m : 2.48970
[1mStep[0m  [56/84], [94mLoss[0m : 2.28775
[1mStep[0m  [64/84], [94mLoss[0m : 2.71198
[1mStep[0m  [72/84], [94mLoss[0m : 2.75448
[1mStep[0m  [80/84], [94mLoss[0m : 2.41704

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.561, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71422
[1mStep[0m  [8/84], [94mLoss[0m : 2.55545
[1mStep[0m  [16/84], [94mLoss[0m : 2.39007
[1mStep[0m  [24/84], [94mLoss[0m : 2.54187
[1mStep[0m  [32/84], [94mLoss[0m : 2.68806
[1mStep[0m  [40/84], [94mLoss[0m : 2.47911
[1mStep[0m  [48/84], [94mLoss[0m : 2.53067
[1mStep[0m  [56/84], [94mLoss[0m : 2.48632
[1mStep[0m  [64/84], [94mLoss[0m : 2.69076
[1mStep[0m  [72/84], [94mLoss[0m : 2.30730
[1mStep[0m  [80/84], [94mLoss[0m : 2.53646

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.572, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56076
[1mStep[0m  [8/84], [94mLoss[0m : 2.26233
[1mStep[0m  [16/84], [94mLoss[0m : 2.33111
[1mStep[0m  [24/84], [94mLoss[0m : 2.18308
[1mStep[0m  [32/84], [94mLoss[0m : 2.27027
[1mStep[0m  [40/84], [94mLoss[0m : 2.29040
[1mStep[0m  [48/84], [94mLoss[0m : 2.55534
[1mStep[0m  [56/84], [94mLoss[0m : 2.35756
[1mStep[0m  [64/84], [94mLoss[0m : 2.28264
[1mStep[0m  [72/84], [94mLoss[0m : 2.34172
[1mStep[0m  [80/84], [94mLoss[0m : 2.53154

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.557, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22049
[1mStep[0m  [8/84], [94mLoss[0m : 2.70943
[1mStep[0m  [16/84], [94mLoss[0m : 2.59206
[1mStep[0m  [24/84], [94mLoss[0m : 2.21660
[1mStep[0m  [32/84], [94mLoss[0m : 2.64867
[1mStep[0m  [40/84], [94mLoss[0m : 2.73632
[1mStep[0m  [48/84], [94mLoss[0m : 2.70691
[1mStep[0m  [56/84], [94mLoss[0m : 2.63133
[1mStep[0m  [64/84], [94mLoss[0m : 2.47790
[1mStep[0m  [72/84], [94mLoss[0m : 2.74489
[1mStep[0m  [80/84], [94mLoss[0m : 2.46924

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.548, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22611
[1mStep[0m  [8/84], [94mLoss[0m : 2.58278
[1mStep[0m  [16/84], [94mLoss[0m : 2.63240
[1mStep[0m  [24/84], [94mLoss[0m : 2.92513
[1mStep[0m  [32/84], [94mLoss[0m : 2.62255
[1mStep[0m  [40/84], [94mLoss[0m : 2.48352
[1mStep[0m  [48/84], [94mLoss[0m : 2.51394
[1mStep[0m  [56/84], [94mLoss[0m : 2.64454
[1mStep[0m  [64/84], [94mLoss[0m : 2.55114
[1mStep[0m  [72/84], [94mLoss[0m : 2.22628
[1mStep[0m  [80/84], [94mLoss[0m : 2.53356

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.570, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35942
[1mStep[0m  [8/84], [94mLoss[0m : 2.53382
[1mStep[0m  [16/84], [94mLoss[0m : 2.70757
[1mStep[0m  [24/84], [94mLoss[0m : 2.75743
[1mStep[0m  [32/84], [94mLoss[0m : 2.72450
[1mStep[0m  [40/84], [94mLoss[0m : 2.47136
[1mStep[0m  [48/84], [94mLoss[0m : 2.66421
[1mStep[0m  [56/84], [94mLoss[0m : 2.31423
[1mStep[0m  [64/84], [94mLoss[0m : 2.37878
[1mStep[0m  [72/84], [94mLoss[0m : 2.75567
[1mStep[0m  [80/84], [94mLoss[0m : 2.36630

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.551, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81394
[1mStep[0m  [8/84], [94mLoss[0m : 2.40522
[1mStep[0m  [16/84], [94mLoss[0m : 2.64706
[1mStep[0m  [24/84], [94mLoss[0m : 2.54760
[1mStep[0m  [32/84], [94mLoss[0m : 2.42381
[1mStep[0m  [40/84], [94mLoss[0m : 2.22244
[1mStep[0m  [48/84], [94mLoss[0m : 2.52694
[1mStep[0m  [56/84], [94mLoss[0m : 2.53190
[1mStep[0m  [64/84], [94mLoss[0m : 2.50344
[1mStep[0m  [72/84], [94mLoss[0m : 2.47663
[1mStep[0m  [80/84], [94mLoss[0m : 2.67207

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.559, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43777
[1mStep[0m  [8/84], [94mLoss[0m : 2.66113
[1mStep[0m  [16/84], [94mLoss[0m : 2.62900
[1mStep[0m  [24/84], [94mLoss[0m : 2.41708
[1mStep[0m  [32/84], [94mLoss[0m : 2.58947
[1mStep[0m  [40/84], [94mLoss[0m : 2.39973
[1mStep[0m  [48/84], [94mLoss[0m : 2.32132
[1mStep[0m  [56/84], [94mLoss[0m : 2.50489
[1mStep[0m  [64/84], [94mLoss[0m : 2.25687
[1mStep[0m  [72/84], [94mLoss[0m : 2.56902
[1mStep[0m  [80/84], [94mLoss[0m : 2.48576

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.556, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.528
====================================

Phase 2 - Evaluation MAE:  2.5277224693979536
MAE score P1      3.798632
MAE score P2      2.527722
loss              2.508895
learning_rate       0.0001
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 27, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.86022
[1mStep[0m  [8/84], [94mLoss[0m : 10.39759
[1mStep[0m  [16/84], [94mLoss[0m : 10.75133
[1mStep[0m  [24/84], [94mLoss[0m : 10.71134
[1mStep[0m  [32/84], [94mLoss[0m : 10.34873
[1mStep[0m  [40/84], [94mLoss[0m : 10.28165
[1mStep[0m  [48/84], [94mLoss[0m : 9.86137
[1mStep[0m  [56/84], [94mLoss[0m : 9.77287
[1mStep[0m  [64/84], [94mLoss[0m : 10.03115
[1mStep[0m  [72/84], [94mLoss[0m : 10.04861
[1mStep[0m  [80/84], [94mLoss[0m : 9.75680

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.378, [92mTest[0m: 10.885, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.89206
[1mStep[0m  [8/84], [94mLoss[0m : 9.75141
[1mStep[0m  [16/84], [94mLoss[0m : 9.40853
[1mStep[0m  [24/84], [94mLoss[0m : 9.58934
[1mStep[0m  [32/84], [94mLoss[0m : 9.18863
[1mStep[0m  [40/84], [94mLoss[0m : 8.87945
[1mStep[0m  [48/84], [94mLoss[0m : 9.55035
[1mStep[0m  [56/84], [94mLoss[0m : 9.35389
[1mStep[0m  [64/84], [94mLoss[0m : 8.64704
[1mStep[0m  [72/84], [94mLoss[0m : 9.24997
[1mStep[0m  [80/84], [94mLoss[0m : 8.45768

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.258, [92mTest[0m: 9.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.46176
[1mStep[0m  [8/84], [94mLoss[0m : 7.84141
[1mStep[0m  [16/84], [94mLoss[0m : 8.84996
[1mStep[0m  [24/84], [94mLoss[0m : 8.26061
[1mStep[0m  [32/84], [94mLoss[0m : 8.06378
[1mStep[0m  [40/84], [94mLoss[0m : 7.80057
[1mStep[0m  [48/84], [94mLoss[0m : 8.14786
[1mStep[0m  [56/84], [94mLoss[0m : 7.72623
[1mStep[0m  [64/84], [94mLoss[0m : 7.62661
[1mStep[0m  [72/84], [94mLoss[0m : 7.14986
[1mStep[0m  [80/84], [94mLoss[0m : 7.37864

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.118, [92mTest[0m: 8.658, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.64331
[1mStep[0m  [8/84], [94mLoss[0m : 7.77458
[1mStep[0m  [16/84], [94mLoss[0m : 7.83799
[1mStep[0m  [24/84], [94mLoss[0m : 7.06963
[1mStep[0m  [32/84], [94mLoss[0m : 7.24751
[1mStep[0m  [40/84], [94mLoss[0m : 6.96150
[1mStep[0m  [48/84], [94mLoss[0m : 7.16357
[1mStep[0m  [56/84], [94mLoss[0m : 6.85797
[1mStep[0m  [64/84], [94mLoss[0m : 6.41430
[1mStep[0m  [72/84], [94mLoss[0m : 6.06316
[1mStep[0m  [80/84], [94mLoss[0m : 6.03991

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.985, [92mTest[0m: 7.528, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.35626
[1mStep[0m  [8/84], [94mLoss[0m : 6.55034
[1mStep[0m  [16/84], [94mLoss[0m : 6.96528
[1mStep[0m  [24/84], [94mLoss[0m : 5.67538
[1mStep[0m  [32/84], [94mLoss[0m : 5.77223
[1mStep[0m  [40/84], [94mLoss[0m : 5.61264
[1mStep[0m  [48/84], [94mLoss[0m : 5.58729
[1mStep[0m  [56/84], [94mLoss[0m : 6.09408
[1mStep[0m  [64/84], [94mLoss[0m : 5.58649
[1mStep[0m  [72/84], [94mLoss[0m : 5.46227
[1mStep[0m  [80/84], [94mLoss[0m : 5.57450

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.870, [92mTest[0m: 6.371, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.00991
[1mStep[0m  [8/84], [94mLoss[0m : 6.02840
[1mStep[0m  [16/84], [94mLoss[0m : 4.50774
[1mStep[0m  [24/84], [94mLoss[0m : 5.48308
[1mStep[0m  [32/84], [94mLoss[0m : 4.70498
[1mStep[0m  [40/84], [94mLoss[0m : 4.67744
[1mStep[0m  [48/84], [94mLoss[0m : 4.66731
[1mStep[0m  [56/84], [94mLoss[0m : 4.36552
[1mStep[0m  [64/84], [94mLoss[0m : 4.12931
[1mStep[0m  [72/84], [94mLoss[0m : 4.98266
[1mStep[0m  [80/84], [94mLoss[0m : 4.19367

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.858, [92mTest[0m: 5.284, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.31325
[1mStep[0m  [8/84], [94mLoss[0m : 4.18841
[1mStep[0m  [16/84], [94mLoss[0m : 4.21402
[1mStep[0m  [24/84], [94mLoss[0m : 4.43350
[1mStep[0m  [32/84], [94mLoss[0m : 4.16867
[1mStep[0m  [40/84], [94mLoss[0m : 4.19397
[1mStep[0m  [48/84], [94mLoss[0m : 4.03798
[1mStep[0m  [56/84], [94mLoss[0m : 3.60937
[1mStep[0m  [64/84], [94mLoss[0m : 4.80749
[1mStep[0m  [72/84], [94mLoss[0m : 4.13476
[1mStep[0m  [80/84], [94mLoss[0m : 4.65407

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.113, [92mTest[0m: 4.355, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.97735
[1mStep[0m  [8/84], [94mLoss[0m : 3.97418
[1mStep[0m  [16/84], [94mLoss[0m : 3.51745
[1mStep[0m  [24/84], [94mLoss[0m : 3.74435
[1mStep[0m  [32/84], [94mLoss[0m : 3.33640
[1mStep[0m  [40/84], [94mLoss[0m : 3.70592
[1mStep[0m  [48/84], [94mLoss[0m : 3.64731
[1mStep[0m  [56/84], [94mLoss[0m : 3.66827
[1mStep[0m  [64/84], [94mLoss[0m : 3.87741
[1mStep[0m  [72/84], [94mLoss[0m : 3.56759
[1mStep[0m  [80/84], [94mLoss[0m : 3.42517

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.587, [92mTest[0m: 3.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.01931
[1mStep[0m  [8/84], [94mLoss[0m : 3.15180
[1mStep[0m  [16/84], [94mLoss[0m : 3.00753
[1mStep[0m  [24/84], [94mLoss[0m : 2.81983
[1mStep[0m  [32/84], [94mLoss[0m : 3.33718
[1mStep[0m  [40/84], [94mLoss[0m : 3.39891
[1mStep[0m  [48/84], [94mLoss[0m : 3.20122
[1mStep[0m  [56/84], [94mLoss[0m : 3.02784
[1mStep[0m  [64/84], [94mLoss[0m : 3.23261
[1mStep[0m  [72/84], [94mLoss[0m : 3.23768
[1mStep[0m  [80/84], [94mLoss[0m : 3.03703

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.280, [92mTest[0m: 3.276, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.22763
[1mStep[0m  [8/84], [94mLoss[0m : 3.38344
[1mStep[0m  [16/84], [94mLoss[0m : 3.14585
[1mStep[0m  [24/84], [94mLoss[0m : 3.06166
[1mStep[0m  [32/84], [94mLoss[0m : 3.12944
[1mStep[0m  [40/84], [94mLoss[0m : 3.47479
[1mStep[0m  [48/84], [94mLoss[0m : 2.97591
[1mStep[0m  [56/84], [94mLoss[0m : 3.38471
[1mStep[0m  [64/84], [94mLoss[0m : 2.96895
[1mStep[0m  [72/84], [94mLoss[0m : 3.04948
[1mStep[0m  [80/84], [94mLoss[0m : 2.84619

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.050, [92mTest[0m: 2.971, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.90867
[1mStep[0m  [8/84], [94mLoss[0m : 3.37090
[1mStep[0m  [16/84], [94mLoss[0m : 2.96736
[1mStep[0m  [24/84], [94mLoss[0m : 2.84430
[1mStep[0m  [32/84], [94mLoss[0m : 2.87168
[1mStep[0m  [40/84], [94mLoss[0m : 2.66663
[1mStep[0m  [48/84], [94mLoss[0m : 2.70647
[1mStep[0m  [56/84], [94mLoss[0m : 3.10395
[1mStep[0m  [64/84], [94mLoss[0m : 2.63688
[1mStep[0m  [72/84], [94mLoss[0m : 3.09341
[1mStep[0m  [80/84], [94mLoss[0m : 3.16911

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.901, [92mTest[0m: 2.781, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77632
[1mStep[0m  [8/84], [94mLoss[0m : 3.38377
[1mStep[0m  [16/84], [94mLoss[0m : 2.79557
[1mStep[0m  [24/84], [94mLoss[0m : 3.12102
[1mStep[0m  [32/84], [94mLoss[0m : 2.92868
[1mStep[0m  [40/84], [94mLoss[0m : 2.82835
[1mStep[0m  [48/84], [94mLoss[0m : 3.28847
[1mStep[0m  [56/84], [94mLoss[0m : 2.58319
[1mStep[0m  [64/84], [94mLoss[0m : 2.71756
[1mStep[0m  [72/84], [94mLoss[0m : 2.82356
[1mStep[0m  [80/84], [94mLoss[0m : 2.88616

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.807, [92mTest[0m: 2.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.21115
[1mStep[0m  [8/84], [94mLoss[0m : 2.66363
[1mStep[0m  [16/84], [94mLoss[0m : 2.96985
[1mStep[0m  [24/84], [94mLoss[0m : 3.18274
[1mStep[0m  [32/84], [94mLoss[0m : 2.84148
[1mStep[0m  [40/84], [94mLoss[0m : 2.67186
[1mStep[0m  [48/84], [94mLoss[0m : 2.58882
[1mStep[0m  [56/84], [94mLoss[0m : 2.64422
[1mStep[0m  [64/84], [94mLoss[0m : 2.79955
[1mStep[0m  [72/84], [94mLoss[0m : 3.08694
[1mStep[0m  [80/84], [94mLoss[0m : 3.06485

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.746, [92mTest[0m: 2.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99304
[1mStep[0m  [8/84], [94mLoss[0m : 2.53668
[1mStep[0m  [16/84], [94mLoss[0m : 2.63876
[1mStep[0m  [24/84], [94mLoss[0m : 2.55686
[1mStep[0m  [32/84], [94mLoss[0m : 2.96757
[1mStep[0m  [40/84], [94mLoss[0m : 2.92375
[1mStep[0m  [48/84], [94mLoss[0m : 2.74364
[1mStep[0m  [56/84], [94mLoss[0m : 2.82011
[1mStep[0m  [64/84], [94mLoss[0m : 2.87553
[1mStep[0m  [72/84], [94mLoss[0m : 2.51398
[1mStep[0m  [80/84], [94mLoss[0m : 2.44131

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.502, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.91218
[1mStep[0m  [8/84], [94mLoss[0m : 2.41735
[1mStep[0m  [16/84], [94mLoss[0m : 2.68607
[1mStep[0m  [24/84], [94mLoss[0m : 2.83809
[1mStep[0m  [32/84], [94mLoss[0m : 3.01844
[1mStep[0m  [40/84], [94mLoss[0m : 3.07842
[1mStep[0m  [48/84], [94mLoss[0m : 2.86182
[1mStep[0m  [56/84], [94mLoss[0m : 2.38320
[1mStep[0m  [64/84], [94mLoss[0m : 2.65192
[1mStep[0m  [72/84], [94mLoss[0m : 2.77900
[1mStep[0m  [80/84], [94mLoss[0m : 2.62780

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.91900
[1mStep[0m  [8/84], [94mLoss[0m : 2.42842
[1mStep[0m  [16/84], [94mLoss[0m : 2.82782
[1mStep[0m  [24/84], [94mLoss[0m : 2.56467
[1mStep[0m  [32/84], [94mLoss[0m : 2.91924
[1mStep[0m  [40/84], [94mLoss[0m : 2.70269
[1mStep[0m  [48/84], [94mLoss[0m : 2.95463
[1mStep[0m  [56/84], [94mLoss[0m : 2.63164
[1mStep[0m  [64/84], [94mLoss[0m : 2.42412
[1mStep[0m  [72/84], [94mLoss[0m : 2.68703
[1mStep[0m  [80/84], [94mLoss[0m : 2.87843

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.455, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68281
[1mStep[0m  [8/84], [94mLoss[0m : 2.58344
[1mStep[0m  [16/84], [94mLoss[0m : 2.57387
[1mStep[0m  [24/84], [94mLoss[0m : 2.71207
[1mStep[0m  [32/84], [94mLoss[0m : 2.24642
[1mStep[0m  [40/84], [94mLoss[0m : 2.44871
[1mStep[0m  [48/84], [94mLoss[0m : 2.60888
[1mStep[0m  [56/84], [94mLoss[0m : 2.61590
[1mStep[0m  [64/84], [94mLoss[0m : 2.70927
[1mStep[0m  [72/84], [94mLoss[0m : 2.82781
[1mStep[0m  [80/84], [94mLoss[0m : 2.38577

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68538
[1mStep[0m  [8/84], [94mLoss[0m : 2.59061
[1mStep[0m  [16/84], [94mLoss[0m : 2.38619
[1mStep[0m  [24/84], [94mLoss[0m : 2.36354
[1mStep[0m  [32/84], [94mLoss[0m : 2.91710
[1mStep[0m  [40/84], [94mLoss[0m : 2.85209
[1mStep[0m  [48/84], [94mLoss[0m : 2.61990
[1mStep[0m  [56/84], [94mLoss[0m : 2.55803
[1mStep[0m  [64/84], [94mLoss[0m : 2.37267
[1mStep[0m  [72/84], [94mLoss[0m : 2.76817
[1mStep[0m  [80/84], [94mLoss[0m : 2.37971

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.437, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80244
[1mStep[0m  [8/84], [94mLoss[0m : 2.44595
[1mStep[0m  [16/84], [94mLoss[0m : 2.43957
[1mStep[0m  [24/84], [94mLoss[0m : 2.31192
[1mStep[0m  [32/84], [94mLoss[0m : 2.45578
[1mStep[0m  [40/84], [94mLoss[0m : 2.79858
[1mStep[0m  [48/84], [94mLoss[0m : 2.51433
[1mStep[0m  [56/84], [94mLoss[0m : 2.52122
[1mStep[0m  [64/84], [94mLoss[0m : 2.25362
[1mStep[0m  [72/84], [94mLoss[0m : 3.07014
[1mStep[0m  [80/84], [94mLoss[0m : 2.85464

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.431, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32079
[1mStep[0m  [8/84], [94mLoss[0m : 2.51942
[1mStep[0m  [16/84], [94mLoss[0m : 2.65483
[1mStep[0m  [24/84], [94mLoss[0m : 2.41058
[1mStep[0m  [32/84], [94mLoss[0m : 2.51157
[1mStep[0m  [40/84], [94mLoss[0m : 2.72257
[1mStep[0m  [48/84], [94mLoss[0m : 2.93846
[1mStep[0m  [56/84], [94mLoss[0m : 2.80232
[1mStep[0m  [64/84], [94mLoss[0m : 2.76089
[1mStep[0m  [72/84], [94mLoss[0m : 3.08385
[1mStep[0m  [80/84], [94mLoss[0m : 2.62911

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.424, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52702
[1mStep[0m  [8/84], [94mLoss[0m : 2.80063
[1mStep[0m  [16/84], [94mLoss[0m : 2.52529
[1mStep[0m  [24/84], [94mLoss[0m : 2.50336
[1mStep[0m  [32/84], [94mLoss[0m : 2.74279
[1mStep[0m  [40/84], [94mLoss[0m : 2.89607
[1mStep[0m  [48/84], [94mLoss[0m : 2.58838
[1mStep[0m  [56/84], [94mLoss[0m : 2.95412
[1mStep[0m  [64/84], [94mLoss[0m : 2.67659
[1mStep[0m  [72/84], [94mLoss[0m : 2.80875
[1mStep[0m  [80/84], [94mLoss[0m : 2.53931

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.423, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.95209
[1mStep[0m  [8/84], [94mLoss[0m : 2.63527
[1mStep[0m  [16/84], [94mLoss[0m : 2.43443
[1mStep[0m  [24/84], [94mLoss[0m : 2.54623
[1mStep[0m  [32/84], [94mLoss[0m : 2.42394
[1mStep[0m  [40/84], [94mLoss[0m : 2.86094
[1mStep[0m  [48/84], [94mLoss[0m : 2.63496
[1mStep[0m  [56/84], [94mLoss[0m : 2.44231
[1mStep[0m  [64/84], [94mLoss[0m : 2.72624
[1mStep[0m  [72/84], [94mLoss[0m : 2.42001
[1mStep[0m  [80/84], [94mLoss[0m : 2.77064

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.417, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58122
[1mStep[0m  [8/84], [94mLoss[0m : 2.52862
[1mStep[0m  [16/84], [94mLoss[0m : 2.73106
[1mStep[0m  [24/84], [94mLoss[0m : 2.41246
[1mStep[0m  [32/84], [94mLoss[0m : 2.72729
[1mStep[0m  [40/84], [94mLoss[0m : 2.60738
[1mStep[0m  [48/84], [94mLoss[0m : 2.95590
[1mStep[0m  [56/84], [94mLoss[0m : 2.77741
[1mStep[0m  [64/84], [94mLoss[0m : 2.90328
[1mStep[0m  [72/84], [94mLoss[0m : 2.62861
[1mStep[0m  [80/84], [94mLoss[0m : 2.55783

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.417, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42234
[1mStep[0m  [8/84], [94mLoss[0m : 3.01463
[1mStep[0m  [16/84], [94mLoss[0m : 2.59356
[1mStep[0m  [24/84], [94mLoss[0m : 2.62095
[1mStep[0m  [32/84], [94mLoss[0m : 2.69345
[1mStep[0m  [40/84], [94mLoss[0m : 2.73600
[1mStep[0m  [48/84], [94mLoss[0m : 2.60374
[1mStep[0m  [56/84], [94mLoss[0m : 2.46889
[1mStep[0m  [64/84], [94mLoss[0m : 2.41490
[1mStep[0m  [72/84], [94mLoss[0m : 2.73203
[1mStep[0m  [80/84], [94mLoss[0m : 2.63336

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.408, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72768
[1mStep[0m  [8/84], [94mLoss[0m : 2.63495
[1mStep[0m  [16/84], [94mLoss[0m : 2.77422
[1mStep[0m  [24/84], [94mLoss[0m : 2.63897
[1mStep[0m  [32/84], [94mLoss[0m : 2.65099
[1mStep[0m  [40/84], [94mLoss[0m : 2.68131
[1mStep[0m  [48/84], [94mLoss[0m : 2.80904
[1mStep[0m  [56/84], [94mLoss[0m : 2.45380
[1mStep[0m  [64/84], [94mLoss[0m : 2.82752
[1mStep[0m  [72/84], [94mLoss[0m : 2.60853
[1mStep[0m  [80/84], [94mLoss[0m : 2.59476

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.411, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62454
[1mStep[0m  [8/84], [94mLoss[0m : 2.69283
[1mStep[0m  [16/84], [94mLoss[0m : 2.31786
[1mStep[0m  [24/84], [94mLoss[0m : 2.46015
[1mStep[0m  [32/84], [94mLoss[0m : 2.59313
[1mStep[0m  [40/84], [94mLoss[0m : 2.43921
[1mStep[0m  [48/84], [94mLoss[0m : 2.66846
[1mStep[0m  [56/84], [94mLoss[0m : 2.46220
[1mStep[0m  [64/84], [94mLoss[0m : 2.75280
[1mStep[0m  [72/84], [94mLoss[0m : 2.70575
[1mStep[0m  [80/84], [94mLoss[0m : 2.81420

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.410, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77815
[1mStep[0m  [8/84], [94mLoss[0m : 2.58950
[1mStep[0m  [16/84], [94mLoss[0m : 2.68586
[1mStep[0m  [24/84], [94mLoss[0m : 2.68605
[1mStep[0m  [32/84], [94mLoss[0m : 2.69571
[1mStep[0m  [40/84], [94mLoss[0m : 2.72667
[1mStep[0m  [48/84], [94mLoss[0m : 2.67255
[1mStep[0m  [56/84], [94mLoss[0m : 2.56947
[1mStep[0m  [64/84], [94mLoss[0m : 2.41255
[1mStep[0m  [72/84], [94mLoss[0m : 2.72228
[1mStep[0m  [80/84], [94mLoss[0m : 2.36290

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.409, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69761
[1mStep[0m  [8/84], [94mLoss[0m : 2.44295
[1mStep[0m  [16/84], [94mLoss[0m : 2.44269
[1mStep[0m  [24/84], [94mLoss[0m : 2.75484
[1mStep[0m  [32/84], [94mLoss[0m : 2.51464
[1mStep[0m  [40/84], [94mLoss[0m : 2.64919
[1mStep[0m  [48/84], [94mLoss[0m : 2.30789
[1mStep[0m  [56/84], [94mLoss[0m : 2.33063
[1mStep[0m  [64/84], [94mLoss[0m : 2.37639
[1mStep[0m  [72/84], [94mLoss[0m : 2.67383
[1mStep[0m  [80/84], [94mLoss[0m : 2.48077

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.403, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83659
[1mStep[0m  [8/84], [94mLoss[0m : 2.56612
[1mStep[0m  [16/84], [94mLoss[0m : 2.49092
[1mStep[0m  [24/84], [94mLoss[0m : 2.93797
[1mStep[0m  [32/84], [94mLoss[0m : 3.03065
[1mStep[0m  [40/84], [94mLoss[0m : 3.09029
[1mStep[0m  [48/84], [94mLoss[0m : 2.82767
[1mStep[0m  [56/84], [94mLoss[0m : 2.58640
[1mStep[0m  [64/84], [94mLoss[0m : 2.82274
[1mStep[0m  [72/84], [94mLoss[0m : 2.40034
[1mStep[0m  [80/84], [94mLoss[0m : 2.43473

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.589, [92mTest[0m: 2.388, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61811
[1mStep[0m  [8/84], [94mLoss[0m : 2.75581
[1mStep[0m  [16/84], [94mLoss[0m : 2.73343
[1mStep[0m  [24/84], [94mLoss[0m : 2.70686
[1mStep[0m  [32/84], [94mLoss[0m : 2.68012
[1mStep[0m  [40/84], [94mLoss[0m : 2.59094
[1mStep[0m  [48/84], [94mLoss[0m : 2.70864
[1mStep[0m  [56/84], [94mLoss[0m : 2.65771
[1mStep[0m  [64/84], [94mLoss[0m : 2.26448
[1mStep[0m  [72/84], [94mLoss[0m : 2.25574
[1mStep[0m  [80/84], [94mLoss[0m : 2.67453

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.400, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 1 - Evaluation MAE:  2.3971544929913113
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.47053
[1mStep[0m  [8/84], [94mLoss[0m : 2.77420
[1mStep[0m  [16/84], [94mLoss[0m : 2.68512
[1mStep[0m  [24/84], [94mLoss[0m : 2.89333
[1mStep[0m  [32/84], [94mLoss[0m : 2.24528
[1mStep[0m  [40/84], [94mLoss[0m : 2.03252
[1mStep[0m  [48/84], [94mLoss[0m : 2.58384
[1mStep[0m  [56/84], [94mLoss[0m : 2.59945
[1mStep[0m  [64/84], [94mLoss[0m : 2.55999
[1mStep[0m  [72/84], [94mLoss[0m : 2.48800
[1mStep[0m  [80/84], [94mLoss[0m : 2.36280

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.398, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70368
[1mStep[0m  [8/84], [94mLoss[0m : 2.49298
[1mStep[0m  [16/84], [94mLoss[0m : 2.29395
[1mStep[0m  [24/84], [94mLoss[0m : 2.95147
[1mStep[0m  [32/84], [94mLoss[0m : 2.79404
[1mStep[0m  [40/84], [94mLoss[0m : 2.37829
[1mStep[0m  [48/84], [94mLoss[0m : 2.47359
[1mStep[0m  [56/84], [94mLoss[0m : 2.42470
[1mStep[0m  [64/84], [94mLoss[0m : 2.62229
[1mStep[0m  [72/84], [94mLoss[0m : 3.13110
[1mStep[0m  [80/84], [94mLoss[0m : 2.73634

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.402, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56001
[1mStep[0m  [8/84], [94mLoss[0m : 2.54067
[1mStep[0m  [16/84], [94mLoss[0m : 2.65666
[1mStep[0m  [24/84], [94mLoss[0m : 2.76086
[1mStep[0m  [32/84], [94mLoss[0m : 2.68585
[1mStep[0m  [40/84], [94mLoss[0m : 2.66481
[1mStep[0m  [48/84], [94mLoss[0m : 2.41946
[1mStep[0m  [56/84], [94mLoss[0m : 2.60129
[1mStep[0m  [64/84], [94mLoss[0m : 2.52980
[1mStep[0m  [72/84], [94mLoss[0m : 2.61618
[1mStep[0m  [80/84], [94mLoss[0m : 2.26892

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.404, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78131
[1mStep[0m  [8/84], [94mLoss[0m : 2.78029
[1mStep[0m  [16/84], [94mLoss[0m : 2.14562
[1mStep[0m  [24/84], [94mLoss[0m : 2.27163
[1mStep[0m  [32/84], [94mLoss[0m : 2.55043
[1mStep[0m  [40/84], [94mLoss[0m : 2.67301
[1mStep[0m  [48/84], [94mLoss[0m : 2.51752
[1mStep[0m  [56/84], [94mLoss[0m : 2.70899
[1mStep[0m  [64/84], [94mLoss[0m : 2.41645
[1mStep[0m  [72/84], [94mLoss[0m : 2.76181
[1mStep[0m  [80/84], [94mLoss[0m : 2.72804

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.403, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53926
[1mStep[0m  [8/84], [94mLoss[0m : 2.61640
[1mStep[0m  [16/84], [94mLoss[0m : 2.46599
[1mStep[0m  [24/84], [94mLoss[0m : 2.76722
[1mStep[0m  [32/84], [94mLoss[0m : 2.73684
[1mStep[0m  [40/84], [94mLoss[0m : 2.68566
[1mStep[0m  [48/84], [94mLoss[0m : 2.52455
[1mStep[0m  [56/84], [94mLoss[0m : 2.49252
[1mStep[0m  [64/84], [94mLoss[0m : 2.61295
[1mStep[0m  [72/84], [94mLoss[0m : 2.54062
[1mStep[0m  [80/84], [94mLoss[0m : 2.74619

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.403, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72640
[1mStep[0m  [8/84], [94mLoss[0m : 2.64646
[1mStep[0m  [16/84], [94mLoss[0m : 2.49008
[1mStep[0m  [24/84], [94mLoss[0m : 2.24666
[1mStep[0m  [32/84], [94mLoss[0m : 2.34272
[1mStep[0m  [40/84], [94mLoss[0m : 2.69066
[1mStep[0m  [48/84], [94mLoss[0m : 2.47927
[1mStep[0m  [56/84], [94mLoss[0m : 2.59627
[1mStep[0m  [64/84], [94mLoss[0m : 2.34515
[1mStep[0m  [72/84], [94mLoss[0m : 2.47726
[1mStep[0m  [80/84], [94mLoss[0m : 2.54453

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.401, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50673
[1mStep[0m  [8/84], [94mLoss[0m : 2.46862
[1mStep[0m  [16/84], [94mLoss[0m : 2.22349
[1mStep[0m  [24/84], [94mLoss[0m : 3.02282
[1mStep[0m  [32/84], [94mLoss[0m : 2.49757
[1mStep[0m  [40/84], [94mLoss[0m : 2.39350
[1mStep[0m  [48/84], [94mLoss[0m : 2.67030
[1mStep[0m  [56/84], [94mLoss[0m : 2.41286
[1mStep[0m  [64/84], [94mLoss[0m : 2.46499
[1mStep[0m  [72/84], [94mLoss[0m : 2.60171
[1mStep[0m  [80/84], [94mLoss[0m : 2.87143

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.381, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49485
[1mStep[0m  [8/84], [94mLoss[0m : 2.60566
[1mStep[0m  [16/84], [94mLoss[0m : 2.78801
[1mStep[0m  [24/84], [94mLoss[0m : 2.78176
[1mStep[0m  [32/84], [94mLoss[0m : 2.62872
[1mStep[0m  [40/84], [94mLoss[0m : 2.81285
[1mStep[0m  [48/84], [94mLoss[0m : 2.62823
[1mStep[0m  [56/84], [94mLoss[0m : 2.59391
[1mStep[0m  [64/84], [94mLoss[0m : 2.25724
[1mStep[0m  [72/84], [94mLoss[0m : 2.55961
[1mStep[0m  [80/84], [94mLoss[0m : 2.27859

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.422, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75871
[1mStep[0m  [8/84], [94mLoss[0m : 2.32708
[1mStep[0m  [16/84], [94mLoss[0m : 2.85054
[1mStep[0m  [24/84], [94mLoss[0m : 2.46968
[1mStep[0m  [32/84], [94mLoss[0m : 2.46709
[1mStep[0m  [40/84], [94mLoss[0m : 2.58437
[1mStep[0m  [48/84], [94mLoss[0m : 2.55865
[1mStep[0m  [56/84], [94mLoss[0m : 2.79921
[1mStep[0m  [64/84], [94mLoss[0m : 2.39459
[1mStep[0m  [72/84], [94mLoss[0m : 2.57961
[1mStep[0m  [80/84], [94mLoss[0m : 2.77883

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.411, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36988
[1mStep[0m  [8/84], [94mLoss[0m : 2.49895
[1mStep[0m  [16/84], [94mLoss[0m : 2.61153
[1mStep[0m  [24/84], [94mLoss[0m : 2.50646
[1mStep[0m  [32/84], [94mLoss[0m : 2.47137
[1mStep[0m  [40/84], [94mLoss[0m : 2.46712
[1mStep[0m  [48/84], [94mLoss[0m : 2.61502
[1mStep[0m  [56/84], [94mLoss[0m : 2.43863
[1mStep[0m  [64/84], [94mLoss[0m : 2.71453
[1mStep[0m  [72/84], [94mLoss[0m : 2.78480
[1mStep[0m  [80/84], [94mLoss[0m : 2.46964

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.425, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55207
[1mStep[0m  [8/84], [94mLoss[0m : 2.48015
[1mStep[0m  [16/84], [94mLoss[0m : 2.68531
[1mStep[0m  [24/84], [94mLoss[0m : 2.63539
[1mStep[0m  [32/84], [94mLoss[0m : 2.73860
[1mStep[0m  [40/84], [94mLoss[0m : 2.58034
[1mStep[0m  [48/84], [94mLoss[0m : 2.41717
[1mStep[0m  [56/84], [94mLoss[0m : 2.38310
[1mStep[0m  [64/84], [94mLoss[0m : 2.53447
[1mStep[0m  [72/84], [94mLoss[0m : 2.18207
[1mStep[0m  [80/84], [94mLoss[0m : 2.29596

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.468, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63288
[1mStep[0m  [8/84], [94mLoss[0m : 2.60393
[1mStep[0m  [16/84], [94mLoss[0m : 2.80897
[1mStep[0m  [24/84], [94mLoss[0m : 2.50844
[1mStep[0m  [32/84], [94mLoss[0m : 2.44328
[1mStep[0m  [40/84], [94mLoss[0m : 2.39650
[1mStep[0m  [48/84], [94mLoss[0m : 2.27882
[1mStep[0m  [56/84], [94mLoss[0m : 2.82858
[1mStep[0m  [64/84], [94mLoss[0m : 2.96648
[1mStep[0m  [72/84], [94mLoss[0m : 2.85688
[1mStep[0m  [80/84], [94mLoss[0m : 2.54608

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.464, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37308
[1mStep[0m  [8/84], [94mLoss[0m : 2.38547
[1mStep[0m  [16/84], [94mLoss[0m : 2.50503
[1mStep[0m  [24/84], [94mLoss[0m : 2.60222
[1mStep[0m  [32/84], [94mLoss[0m : 2.71765
[1mStep[0m  [40/84], [94mLoss[0m : 2.21478
[1mStep[0m  [48/84], [94mLoss[0m : 2.74023
[1mStep[0m  [56/84], [94mLoss[0m : 2.63357
[1mStep[0m  [64/84], [94mLoss[0m : 2.87578
[1mStep[0m  [72/84], [94mLoss[0m : 2.41581
[1mStep[0m  [80/84], [94mLoss[0m : 2.71842

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.484, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57070
[1mStep[0m  [8/84], [94mLoss[0m : 2.21644
[1mStep[0m  [16/84], [94mLoss[0m : 2.74093
[1mStep[0m  [24/84], [94mLoss[0m : 2.22744
[1mStep[0m  [32/84], [94mLoss[0m : 2.61342
[1mStep[0m  [40/84], [94mLoss[0m : 2.52404
[1mStep[0m  [48/84], [94mLoss[0m : 2.68724
[1mStep[0m  [56/84], [94mLoss[0m : 2.73132
[1mStep[0m  [64/84], [94mLoss[0m : 2.59490
[1mStep[0m  [72/84], [94mLoss[0m : 2.42925
[1mStep[0m  [80/84], [94mLoss[0m : 2.49233

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.432, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63645
[1mStep[0m  [8/84], [94mLoss[0m : 2.64407
[1mStep[0m  [16/84], [94mLoss[0m : 2.31818
[1mStep[0m  [24/84], [94mLoss[0m : 2.96815
[1mStep[0m  [32/84], [94mLoss[0m : 2.23626
[1mStep[0m  [40/84], [94mLoss[0m : 2.74840
[1mStep[0m  [48/84], [94mLoss[0m : 2.57451
[1mStep[0m  [56/84], [94mLoss[0m : 2.35624
[1mStep[0m  [64/84], [94mLoss[0m : 2.97450
[1mStep[0m  [72/84], [94mLoss[0m : 2.59458
[1mStep[0m  [80/84], [94mLoss[0m : 2.63043

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.453, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60748
[1mStep[0m  [8/84], [94mLoss[0m : 2.72275
[1mStep[0m  [16/84], [94mLoss[0m : 2.61346
[1mStep[0m  [24/84], [94mLoss[0m : 2.51068
[1mStep[0m  [32/84], [94mLoss[0m : 2.83592
[1mStep[0m  [40/84], [94mLoss[0m : 2.44423
[1mStep[0m  [48/84], [94mLoss[0m : 2.59360
[1mStep[0m  [56/84], [94mLoss[0m : 2.33421
[1mStep[0m  [64/84], [94mLoss[0m : 2.30684
[1mStep[0m  [72/84], [94mLoss[0m : 2.46404
[1mStep[0m  [80/84], [94mLoss[0m : 2.64259

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.476, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48145
[1mStep[0m  [8/84], [94mLoss[0m : 2.54744
[1mStep[0m  [16/84], [94mLoss[0m : 2.36203
[1mStep[0m  [24/84], [94mLoss[0m : 2.14670
[1mStep[0m  [32/84], [94mLoss[0m : 2.43999
[1mStep[0m  [40/84], [94mLoss[0m : 2.43634
[1mStep[0m  [48/84], [94mLoss[0m : 2.10827
[1mStep[0m  [56/84], [94mLoss[0m : 2.44242
[1mStep[0m  [64/84], [94mLoss[0m : 2.36416
[1mStep[0m  [72/84], [94mLoss[0m : 2.20748
[1mStep[0m  [80/84], [94mLoss[0m : 2.42101

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.438, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54549
[1mStep[0m  [8/84], [94mLoss[0m : 2.72872
[1mStep[0m  [16/84], [94mLoss[0m : 2.56739
[1mStep[0m  [24/84], [94mLoss[0m : 2.47233
[1mStep[0m  [32/84], [94mLoss[0m : 2.36289
[1mStep[0m  [40/84], [94mLoss[0m : 2.47453
[1mStep[0m  [48/84], [94mLoss[0m : 2.44251
[1mStep[0m  [56/84], [94mLoss[0m : 2.65991
[1mStep[0m  [64/84], [94mLoss[0m : 2.47947
[1mStep[0m  [72/84], [94mLoss[0m : 2.47762
[1mStep[0m  [80/84], [94mLoss[0m : 2.55189

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.420, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32807
[1mStep[0m  [8/84], [94mLoss[0m : 2.10954
[1mStep[0m  [16/84], [94mLoss[0m : 2.49479
[1mStep[0m  [24/84], [94mLoss[0m : 2.60598
[1mStep[0m  [32/84], [94mLoss[0m : 2.65053
[1mStep[0m  [40/84], [94mLoss[0m : 2.27533
[1mStep[0m  [48/84], [94mLoss[0m : 2.44227
[1mStep[0m  [56/84], [94mLoss[0m : 2.46997
[1mStep[0m  [64/84], [94mLoss[0m : 2.75896
[1mStep[0m  [72/84], [94mLoss[0m : 2.65600
[1mStep[0m  [80/84], [94mLoss[0m : 2.29192

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.442, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58451
[1mStep[0m  [8/84], [94mLoss[0m : 2.88455
[1mStep[0m  [16/84], [94mLoss[0m : 2.68647
[1mStep[0m  [24/84], [94mLoss[0m : 2.59288
[1mStep[0m  [32/84], [94mLoss[0m : 2.55503
[1mStep[0m  [40/84], [94mLoss[0m : 2.34680
[1mStep[0m  [48/84], [94mLoss[0m : 2.54604
[1mStep[0m  [56/84], [94mLoss[0m : 2.54711
[1mStep[0m  [64/84], [94mLoss[0m : 2.48960
[1mStep[0m  [72/84], [94mLoss[0m : 2.53261
[1mStep[0m  [80/84], [94mLoss[0m : 2.26533

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.390, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55524
[1mStep[0m  [8/84], [94mLoss[0m : 2.32386
[1mStep[0m  [16/84], [94mLoss[0m : 2.62015
[1mStep[0m  [24/84], [94mLoss[0m : 2.61670
[1mStep[0m  [32/84], [94mLoss[0m : 2.59349
[1mStep[0m  [40/84], [94mLoss[0m : 2.32530
[1mStep[0m  [48/84], [94mLoss[0m : 2.48563
[1mStep[0m  [56/84], [94mLoss[0m : 2.75085
[1mStep[0m  [64/84], [94mLoss[0m : 2.40187
[1mStep[0m  [72/84], [94mLoss[0m : 2.30688
[1mStep[0m  [80/84], [94mLoss[0m : 2.43364

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80113
[1mStep[0m  [8/84], [94mLoss[0m : 2.45857
[1mStep[0m  [16/84], [94mLoss[0m : 2.41251
[1mStep[0m  [24/84], [94mLoss[0m : 2.44656
[1mStep[0m  [32/84], [94mLoss[0m : 2.21718
[1mStep[0m  [40/84], [94mLoss[0m : 2.30623
[1mStep[0m  [48/84], [94mLoss[0m : 2.79237
[1mStep[0m  [56/84], [94mLoss[0m : 2.20355
[1mStep[0m  [64/84], [94mLoss[0m : 2.31463
[1mStep[0m  [72/84], [94mLoss[0m : 2.46238
[1mStep[0m  [80/84], [94mLoss[0m : 2.48594

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.428, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28887
[1mStep[0m  [8/84], [94mLoss[0m : 2.51516
[1mStep[0m  [16/84], [94mLoss[0m : 2.48678
[1mStep[0m  [24/84], [94mLoss[0m : 2.39660
[1mStep[0m  [32/84], [94mLoss[0m : 2.42129
[1mStep[0m  [40/84], [94mLoss[0m : 2.25816
[1mStep[0m  [48/84], [94mLoss[0m : 2.44076
[1mStep[0m  [56/84], [94mLoss[0m : 2.36737
[1mStep[0m  [64/84], [94mLoss[0m : 2.10032
[1mStep[0m  [72/84], [94mLoss[0m : 2.54075
[1mStep[0m  [80/84], [94mLoss[0m : 2.61728

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.414, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40572
[1mStep[0m  [8/84], [94mLoss[0m : 2.47668
[1mStep[0m  [16/84], [94mLoss[0m : 2.87842
[1mStep[0m  [24/84], [94mLoss[0m : 2.65460
[1mStep[0m  [32/84], [94mLoss[0m : 2.07161
[1mStep[0m  [40/84], [94mLoss[0m : 2.49415
[1mStep[0m  [48/84], [94mLoss[0m : 2.66276
[1mStep[0m  [56/84], [94mLoss[0m : 2.47580
[1mStep[0m  [64/84], [94mLoss[0m : 2.36616
[1mStep[0m  [72/84], [94mLoss[0m : 2.51516
[1mStep[0m  [80/84], [94mLoss[0m : 2.18412

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26144
[1mStep[0m  [8/84], [94mLoss[0m : 2.67180
[1mStep[0m  [16/84], [94mLoss[0m : 2.61797
[1mStep[0m  [24/84], [94mLoss[0m : 2.28470
[1mStep[0m  [32/84], [94mLoss[0m : 2.20416
[1mStep[0m  [40/84], [94mLoss[0m : 2.07892
[1mStep[0m  [48/84], [94mLoss[0m : 2.38490
[1mStep[0m  [56/84], [94mLoss[0m : 2.68415
[1mStep[0m  [64/84], [94mLoss[0m : 2.42779
[1mStep[0m  [72/84], [94mLoss[0m : 2.26998
[1mStep[0m  [80/84], [94mLoss[0m : 3.04624

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.472, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52894
[1mStep[0m  [8/84], [94mLoss[0m : 2.16597
[1mStep[0m  [16/84], [94mLoss[0m : 2.60691
[1mStep[0m  [24/84], [94mLoss[0m : 2.18047
[1mStep[0m  [32/84], [94mLoss[0m : 2.48757
[1mStep[0m  [40/84], [94mLoss[0m : 2.61989
[1mStep[0m  [48/84], [94mLoss[0m : 2.58513
[1mStep[0m  [56/84], [94mLoss[0m : 2.40950
[1mStep[0m  [64/84], [94mLoss[0m : 2.22734
[1mStep[0m  [72/84], [94mLoss[0m : 2.20787
[1mStep[0m  [80/84], [94mLoss[0m : 2.53198

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.477, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63718
[1mStep[0m  [8/84], [94mLoss[0m : 2.37565
[1mStep[0m  [16/84], [94mLoss[0m : 2.52371
[1mStep[0m  [24/84], [94mLoss[0m : 2.18526
[1mStep[0m  [32/84], [94mLoss[0m : 2.40676
[1mStep[0m  [40/84], [94mLoss[0m : 2.16865
[1mStep[0m  [48/84], [94mLoss[0m : 2.47910
[1mStep[0m  [56/84], [94mLoss[0m : 2.54140
[1mStep[0m  [64/84], [94mLoss[0m : 2.51103
[1mStep[0m  [72/84], [94mLoss[0m : 2.48786
[1mStep[0m  [80/84], [94mLoss[0m : 2.18061

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.414, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41931
[1mStep[0m  [8/84], [94mLoss[0m : 2.23905
[1mStep[0m  [16/84], [94mLoss[0m : 2.44083
[1mStep[0m  [24/84], [94mLoss[0m : 2.49026
[1mStep[0m  [32/84], [94mLoss[0m : 2.46769
[1mStep[0m  [40/84], [94mLoss[0m : 2.34073
[1mStep[0m  [48/84], [94mLoss[0m : 2.23893
[1mStep[0m  [56/84], [94mLoss[0m : 2.43244
[1mStep[0m  [64/84], [94mLoss[0m : 2.23662
[1mStep[0m  [72/84], [94mLoss[0m : 2.11532
[1mStep[0m  [80/84], [94mLoss[0m : 2.58310

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.504, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46479
[1mStep[0m  [8/84], [94mLoss[0m : 2.36167
[1mStep[0m  [16/84], [94mLoss[0m : 2.34848
[1mStep[0m  [24/84], [94mLoss[0m : 2.73983
[1mStep[0m  [32/84], [94mLoss[0m : 2.68565
[1mStep[0m  [40/84], [94mLoss[0m : 2.54579
[1mStep[0m  [48/84], [94mLoss[0m : 2.49146
[1mStep[0m  [56/84], [94mLoss[0m : 2.43866
[1mStep[0m  [64/84], [94mLoss[0m : 2.33157
[1mStep[0m  [72/84], [94mLoss[0m : 2.45224
[1mStep[0m  [80/84], [94mLoss[0m : 2.42842

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49878
[1mStep[0m  [8/84], [94mLoss[0m : 2.38109
[1mStep[0m  [16/84], [94mLoss[0m : 2.38210
[1mStep[0m  [24/84], [94mLoss[0m : 2.41573
[1mStep[0m  [32/84], [94mLoss[0m : 2.37810
[1mStep[0m  [40/84], [94mLoss[0m : 2.60781
[1mStep[0m  [48/84], [94mLoss[0m : 2.56645
[1mStep[0m  [56/84], [94mLoss[0m : 2.42928
[1mStep[0m  [64/84], [94mLoss[0m : 2.62760
[1mStep[0m  [72/84], [94mLoss[0m : 2.33934
[1mStep[0m  [80/84], [94mLoss[0m : 2.62259

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.414, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.515
====================================

Phase 2 - Evaluation MAE:  2.515297157423837
MAE score P1       2.397154
MAE score P2       2.515297
loss               2.467335
learning_rate        0.0001
batch_size              128
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 28, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
